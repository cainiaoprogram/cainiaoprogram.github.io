<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>CVPR 2023 | GigaGAN：效果不差Stable Diffusion，速度远优！ - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="CVPR 2023 | GigaGAN：效果不差Stable Diffusion，速度远优！" />
<meta property="og:description" content="本文来源 机器之心 编辑：张倩、小舟
在文生图领域，扩散模型似乎已经一统天下，让曾经也风头无两的 GAN 显得有些过时。但两相比较，GAN 依然存在不可磨灭的优势。这使得一些研究者在这一方向上持续努力，并取得了非常实用的成果。相关论文已被 CVPR 2023 接收。
图像生成是当前 AIGC 领域最热门的方向之一。近期发布的图像生成模型如 DALL・E 2、Imagen、Stable Diffusion 等等，开创了图像生成的新时代，实现了前所未有的图像质量和模型灵活性水平。扩散模型也成为目前占据主导地位的范式。然而，扩散模型依赖于迭代推理，这是一把双刃剑，因为迭代方法可以实现具有简单目标的稳定训练，但推理过程需要高昂的计算成本。
在扩散模型之前，生成对抗网络（GAN）是图像生成模型中常用的基础架构。相比于扩散模型，GAN 通过单个前向传递生成图像，因此本质上是更高效的，但由于训练过程的不稳定性，扩展 GAN 需要仔细调整网络架构和训练因素。因此，GAN 擅长对单个或多个对象类进行建模，但扩展到复杂数据集（更不用说现实世界）则极具挑战性。因此，超大型模型、数据和计算资源现在都专用于扩散模型和自回归模型。
但作为一种高效的生成方法，许多研究者并没有完全放弃 GAN 方法。例如，最近英伟达提出了 StyleGAN-T 模型；港中文等用基于 GAN 的方法生成流畅视频，这些都是 CV 研究者在 GAN 上做的进一步尝试。
现在，在一篇 CVPR 2023 论文中，来自 POSTECH、卡耐基梅隆大学和 Adobe 研究院的研究者们联合探究了关于 GAN 的几个重要问题，包括：
GAN 能否继续扩大规模并从大量资源中受益，GAN 遇到瓶颈了吗？
是什么阻止了 GAN 的进一步扩展，我们能否克服这些障碍？
论文链接：https://arxiv.org/abs/2303.05511
项目链接：https://mingukkang.github.io/GigaGAN/
值得注意的是，CycleGAN 的主要作者、曾获 2018 年 ACM SIGGRAPH 最佳博士论文奖的朱俊彦是这篇 CVPR 论文的第二作者。
该研究首先使用 StyleGAN2 进行实验，并观察到简单地扩展主干网络会导致训练不稳定。基于此，研究者确定了几个关键问题，并提出了一种在增加模型容量的同时稳定训练的技术。
首先，该研究通过保留一组滤波器（filter）并采用特定于样本的线性组合来有效地扩展生成器的容量。该研究还采用了扩散上下文（diffusion context）中常用的几种技术，并证实它们为 GAN 带来了类似的好处。例如，将自注意力（仅图像）和交叉注意力（图像 - 文本）与卷积层交织在一起可以提高模型性能。
该研究还重新引入了多尺度训练，并提出一种新方案来改进图像 - 文本对齐和生成输出的低频细节。多尺度训练允许基于 GAN 的生成器更有效地使用低分辨率块中的参数，从而实现了更好的图像 - 文本对齐和图像质量。经过仔细调整后，该研究提出了十亿参数的新模型 GigaGAN，并在大型数据集（例如 LAION2B-en）上实现了稳定和可扩展的训练，实验结果如下图 1 所示。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/3c1f49765f6a2f2bf7038f106dedfa04/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-03-11T15:14:31+08:00" />
<meta property="article:modified_time" content="2023-03-11T15:14:31+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">CVPR 2023 | GigaGAN：效果不差Stable Diffusion，速度远优！</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p style="text-align:center;"><strong>本文来源  机器之心  编辑：张倩、小舟<br></strong></p> 
 <blockquote> 
  <p>在文生图领域，扩散模型似乎已经一统天下，让曾经也风头无两的 GAN 显得有些过时。但两相比较，GAN 依然存在不可磨灭的优势。这使得一些研究者在这一方向上持续努力，并取得了非常实用的成果。相关论文已被 CVPR 2023 接收。</p> 
 </blockquote> 
 <p style="text-align:justify;">图像生成是当前 AIGC 领域最热门的方向之一。近期发布的图像生成模型如 DALL・E 2、Imagen、Stable Diffusion 等等，开创了图像生成的新时代，实现了前所未有的图像质量和模型灵活性水平。扩散模型也成为目前占据主导地位的范式。然而，扩散模型依赖于迭代推理，这是一把双刃剑，因为迭代方法可以实现具有简单目标的稳定训练，但推理过程需要高昂的计算成本。</p> 
 <p style="text-align:justify;">在扩散模型之前，生成对抗网络（GAN）是图像生成模型中常用的基础架构。相比于扩散模型，GAN 通过单个前向传递生成图像，因此本质上是更高效的，但由于训练过程的不稳定性，扩展 GAN 需要仔细调整网络架构和训练因素。因此，GAN 擅长对单个或多个对象类进行建模，但扩展到复杂数据集（更不用说现实世界）则极具挑战性。因此，超大型模型、数据和计算资源现在都专用于扩散模型和自回归模型。</p> 
 <p style="text-align:justify;">但作为一种高效的生成方法，许多研究者并没有完全放弃 GAN 方法。例如，最近英伟达提出了 StyleGAN-T 模型；港中文等用基于 GAN 的方法生成流畅视频，这些都是 CV 研究者在 GAN 上做的进一步尝试。</p> 
 <p style="text-align:justify;">现在，在一篇 CVPR 2023 论文中，来自 POSTECH、卡耐基梅隆大学和 Adobe 研究院的研究者们联合探究了关于 GAN 的几个重要问题，包括：</p> 
 <ul><li><p style="text-align:justify;">GAN 能否继续扩大规模并从大量资源中受益，GAN 遇到瓶颈了吗？</p></li><li><p style="text-align:justify;">是什么阻止了 GAN 的进一步扩展，我们能否克服这些障碍？</p></li></ul> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/0b/48/zhM2ToNm_o.png" alt="5bbc7a378afe979da65015ddcd298a7c.png"></p> 
 <ul><li><p style="text-align:left;">论文链接：https://arxiv.org/abs/2303.05511</p></li><li><p style="text-align:left;">项目链接：https://mingukkang.github.io/GigaGAN/</p></li></ul> 
 <p style="text-align:justify;">值得注意的是，CycleGAN 的主要作者、曾获 2018 年 ACM SIGGRAPH 最佳博士论文奖的朱俊彦是这篇 CVPR 论文的第二作者。</p> 
 <p style="text-align:justify;">该研究首先使用 StyleGAN2 进行实验，并观察到简单地扩展主干网络会导致训练不稳定。基于此，研究者确定了几个关键问题，并提出了一种在增加模型容量的同时稳定训练的技术。</p> 
 <p style="text-align:justify;">首先，该研究通过保留一组滤波器（filter）并采用特定于样本的线性组合来有效地扩展生成器的容量。该研究还采用了扩散上下文（diffusion context）中常用的几种技术，并证实它们为 GAN 带来了类似的好处。例如，将自注意力（仅图像）和交叉注意力（图像 - 文本）与卷积层交织在一起可以提高模型性能。</p> 
 <p style="text-align:justify;">该研究还重新引入了多尺度训练，并提出一种新方案来改进图像 - 文本对齐和生成输出的低频细节。多尺度训练允许基于 GAN 的生成器更有效地使用低分辨率块中的参数，从而实现了更好的图像 - 文本对齐和图像质量。经过仔细调整后，该研究提出了十亿参数的新模型 GigaGAN，并在大型数据集（例如 LAION2B-en）上实现了稳定和可扩展的训练，实验结果如下图 1 所示。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/06/95/S5cyV6fE_o.png" alt="2241ea36156daaebb0d6e1b5f1110102.png"></p> 
 <p style="text-align:justify;">此外，该研究还采用了多阶段方法 [14, 104]，首先以 64 × 64 的低分辨率生成图像，然后再上采样到 512 × 512 分辨率。这两个网络是模块化的，并且足够强大，能够以即插即用的方式使用。</p> 
 <p style="text-align:justify;">该研究表明，基于文本条件的 GAN 上采样网络可以用作基础扩散模型的高效且更高质量的上采样器，如下图 2 和图 3 所示。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/ce/fc/8RyjZQRC_o.png" alt="c5baaa76bc60d7d539c7cb5164a12a9e.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/58/71/fByltA8c_o.png" alt="610d8081f402176bb8a5cb54d17b705e.png"></p> 
 <p style="text-align:justify;">上述改进使 GigaGAN 远远超越了以前的 GAN：比 StyleGAN2 大 36 倍，比 StyleGAN-XL 和 XMC-GAN 大 6 倍。虽然 GigaGAN 十亿（1B）的参数量仍然低于近期的大型合成模型，例如 Imagen (3.0B)、DALL・E 2 (5.5B) 和 Parti (20B)，但研究者表示他们尚未观察到关于模型大小的质量饱和。</p> 
 <p style="text-align:justify;">GigaGAN 在 COCO2014 数据集上实现了 9.09 的零样本 FID，低于 DALL・E 2、Parti-750M 和 Stable Diffusion。</p> 
 <p style="text-align:justify;">此外，与扩散模型和自回归模型相比，GigaGAN 具有三大实用优势。首先，它的速度快了几十倍，在 0.13 秒内生成了 512 像素的图像（图 1）。其次，它可以在 3.66 秒内合成 4k 分辨率的超高分辨率图像。第三，它具有可控的潜在向量空间，适用于经过充分研究的可控图像合成应用，例如风格混合（图 6）、prompt 插值（图 7）和 prompt 混合（图 8）。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/34/c2/TnpEJWSH_o.png" alt="e2079889dcba8c7162ee5685ddcbb8c9.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/12/98/X6Lapj3x_o.png" alt="76049c4e31322f93894048112880ed95.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/de/b4/shnP7X4D_o.png" alt="45155b53b26276f30eb4fe9886fc8049.png"></p> 
 <p style="text-align:justify;">该研究成功地在数十亿现实世界图像上训练了基于 GAN 的十亿参数规模模型 GigaGAN。这表明 GAN 仍然是文本到图像合成的可行选择，研究人员们应考虑将其用于未来的积极扩展。</p> 
 <p style="text-align:center;"><strong>方法概览</strong></p> 
 <p style="text-align:justify;">研究者训练了一个生成器 G (z, c)，在给定一个潜在编码 z∼N (0, 1)∈R^128 和文本调节信号 c 的情况下，预测一个图像 x∈R^(H×W×3)。他们使用一个判别器 D (x, c) 来判断生成的图像的真实性，与训练数据库 D 中的样本相比较，后者包含图像 - 文本对。</p> 
 <p style="text-align:justify;">尽管 GAN 可以成功地在单类和多类数据集上生成真实的图像，但在互联网图像上进行开放式文本条件合成仍然面临挑战。研究者假设，目前的限制源于其对卷积层的依赖。也就是说，同样的卷积滤波器被用来为图像所有位置上的所有文本条件进行通用图像合成函数建模，这是个挑战。有鉴于此，研究者试图通过根据输入条件动态选择卷积滤波器，并通过注意力机制捕捉长程依赖，为参数化注入更多的表现力。</p> 
 <p style="text-align:justify;">GigaGAN 高容量文本 - 图像生成器如下图 4 所示。首先，研究者使用预训练的 CLIP 模型和学习过的编码器 T 来提取文本嵌入。使用交叉注意力将局部文本描述符提供给生成器。全局文本描述符，连同潜在编码 z，被送入风格映射网络 M 以产生风格码 w。风格码使用论文中的风格 - 自适应内核选择调节主生成器，如右侧所示。</p> 
 <p style="text-align:justify;">生成器通过将中间特征转换为 RGB 图像来输出一个图像金字塔。为了达到更高的容量，研究者在每个尺度上使用多个注意力层和卷积层（附录 A2）。他们还使用了一个单独的上采样器模型，该模型未在此图中显示。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/d6/bc/7rJGQiio_o.png" alt="47ba816f6e91cd1859db5b2ad3bf4bb2.png"></p> 
 <p style="text-align:justify;">判别器由两个分支组成，用于处理图像和文本调节 t_D。文本分支对文本的处理与生成器类似（图 4）。图像分支接收一个图像金字塔，并对每个图像尺度进行独立预测。此外，预测是在下采样层的所有后续尺度上进行的，这使得它成为一个多尺度输入、多尺度输出（MS-I/O）的判别器。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/96/9c/6T4o7q8R_o.png" alt="293cbf1bc464e56fd8aa4490c0bbc949.png"></p> 
 <p style="text-align:center;"><strong>实验结果</strong></p> 
 <p style="text-align:justify;">在论文中，作者记录了五个不同的实验。</p> 
 <p style="text-align:justify;">在第一个实验中，他们通过逐个纳入每个技术组件来展示所提方法的有效性。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/d2/f3/IMpjr0Om_o.png" alt="81199c48f11233c8f52e1c25fc2945e0.png"></p> 
 <p style="text-align:justify;">在第二个实验中，他们测试了模型文生图的能力，结果显示，GigaGAN 表现出与 Stable Diffusion（SD-v1.5）相当的 FID，同时产生的结果比扩散或自回归模型快得多。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/eb/f8/PmrV1G4X_o.png" alt="47c92327fee533db42ef1150db4f7f1d.png"></p> 
 <p style="text-align:justify;">在第三个实验中，他们将 GigaGAN 与基于蒸馏的扩散模型进行比较，结果显示，GigaGAN 能比基于蒸馏的扩散模型更快地合成更高质量的图像。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/a1/5b/NT17Xm9H_o.png" alt="d6c184f64feef29259a574378c29e3e7.png"></p> 
 <p style="text-align:justify;">在第四个实验中，他们验证了 GigaGAN 的上采样器在有条件和无条件的超分辨率任务中相比其他上采样器的优势。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/a1/eb/BCKg1hPy_o.png" alt="0a157e771d6519d1e6ae2f1a889cc4c1.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/3c/1a/7IYBA4wg_o.png" alt="5b5107587f36fde03b15120f9708d828.png"></p> 
 <p style="text-align:justify;">最后，他们展示了自己提出的大规模 GAN 模型仍然享受 GAN 的连续和解纠缠的潜在空间操作，从而实现了新的图像编辑模式。图表请参见上文中的图 6 和图 8。</p> 
 <p style="text-align:center;"><strong>猜您喜欢：</strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">深入浅出stable diffusion：AI作画技术背后的潜在扩散模型论文解读</a></strong><strong><br></strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/33/d5/s6VIUcp4_o.png" alt="74a3ad1339a031e592ee73e39e999a6b.png"> <strong><a href="" rel="nofollow">戳我，查看GAN的系列专辑~！</a></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">一顿午饭外卖，成为CV视觉的前沿弄潮儿！</a></strong><strong><br></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">最新最全100篇汇总！生成扩散模型Diffusion Models</a></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">ECCV2022 | 生成对抗网络GAN部分论文汇总</a><br></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">CVPR 2022 | 25+方向、最新50篇GAN论文</a><br></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow"> ICCV 2021 | 35个主题GAN论文汇总</a><br></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">超110篇！CVPR 2021最全GAN论文梳理</a></strong><strong><br></strong></p> 
 <p style="text-align:center;"><a href="" rel="nofollow"><strong>超100篇！CVPR 2020最全GAN论文梳理</strong></a></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">拆解组新的GAN：解耦表征MixNMatch</a><br></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">StarGAN第2版：多域多样性图像生成</a></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">附下载 | 《可解释的机器学习》中文版</a><br></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">附下载 |《TensorFlow 2.0 深度学习算法实战》</a><br></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">附下载 |《计算机视觉中的数学方法》分享</a></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">《基于深度学习的表面缺陷检测方法综述》</a><br></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">《零样本图像分类综述: 十年进展》</a><br></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">《基于深度神经网络的少样本学习综述》</a></p> 
 <blockquote> 
  <p>《礼记·学记》有云：独学而无友，则孤陋而寡闻</p> 
 </blockquote> 
 <p style="text-align:center;"><strong>欢迎加入 GAN/扩散模型 —交流微信群 ！</strong></p> 
 <p>扫描下面二维码，添加运营小妹好友，拉你进群。发送申请时，请备注，<strong>格式为：</strong><strong>研究方向+</strong><strong>地区+学校/公司+姓名</strong>。如 <strong>扩散模型+北京+北航+吴彦祖</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/a3/b3/DPD7vYTA_o.png" alt="276ad7d3544a600fe933b509dd928c1e.jpeg"></p> 
 <p style="text-align:center;">请备注格式：<strong>研究方向+</strong><strong>地区+学校/公司+姓名</strong></p> 
 <p><strong>点击 </strong><strong><strong><a href="" rel="nofollow">一顿午饭外卖，成为CV视觉的前沿弄潮儿！</a></strong></strong><strong>，领取优惠券，加入 </strong><strong>AI生成创作与计算机视觉</strong><strong> 知识星球！</strong></p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/3f5101a2d8bea63ec815c996e716a21b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【打造家庭服务器系列01】无桌面版Ubuntu 22.04 连接wifi</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/9c1820814ec91f40c1197a57815e99bd/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">一起学习交换机的基本原理</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>