<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>3天快速入门python机器学习（黑马xxx） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="3天快速入门python机器学习（黑马xxx）" />
<meta property="og:description" content="目录 一、 机器学习概述1.1 人工智能概述1.1.1介绍1.1.2 机器学习、深度学习能做些什么1.1.3 人工智能阶段课程安排 1.2 什么是机器学习1.2.3 数据集构成 1.3 机器学习算法分类1.3.1 分类1.3.2 小练习1.3.3 机器学习算法分类 1.4 机器学习开发流程1.5 学习框架和资料介绍1.5.1 机器学习库和框架1.5.2 书籍资料 二、特征工程2.1 数据集2.1.1 可用数据集（针对本次三天学习而言）2.1.2 sklearn数据集2.1.3 数据集的划分 2.2 特征工程介绍2.2.1 为什么需要特征工程(Feature Engineering)2.2.2 什么是特征工程2.2.3 特征工程的位置与数据处理的比较 2.3 特征提取2.3.1 特征提取2.3.2 字典特征提取2.3.3 上述总结2.3.3文本特征提取2.3.5 总结 2.4 特征预处理2.4.1 什么是特征预处理2.4.2 归一化2.4.3 标准化 2.5 特征降维2.5.1 降维 - 降低维度2.5.2 降维的两种方式2.5.3 特征选择2.5.4 低方差特征过滤 2.6 主成分分析2.6.1 什么是主成分分析(PCA)2.6.2 案例：探究用户对物品类别的喜好细分 三、分类算法3.1 sklearn转换器和估计器3.1.1 转换器 - 特征工程的父类3.1.2 估计器(sklearn机器学习算法的实现) 3.2 K-近邻算法（KNN）3.2.1 什么是K-近邻算法3.2.2 k-邻近算法API3.2.3 案例1：鸢尾花种类预测3.2.4 K-近邻总结 3.3 模型选择与调优3.3.1 什么是交叉验证(cross validation)3.3.2 超参数搜索-网格搜索(Grid Search)3." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/49ebc45fcfba5f51f7db00a592cbeeb9/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-31T22:13:24+08:00" />
<meta property="article:modified_time" content="2022-07-31T22:13:24+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">3天快速入门python机器学习（黑马xxx）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>目录</h4> 
 <ul><li><a href="#__2" rel="nofollow">一、 机器学习概述</a></li><li><ul><li><a href="#11__3" rel="nofollow">1.1 人工智能概述</a></li><li><ul><li><a href="#111_4" rel="nofollow">1.1.1介绍</a></li><li><a href="#112__11" rel="nofollow">1.1.2 机器学习、深度学习能做些什么</a></li><li><a href="#113__16" rel="nofollow">1.1.3 人工智能阶段课程安排</a></li></ul> 
   </li><li><a href="#12__19" rel="nofollow">1.2 什么是机器学习</a></li><li><ul><li><a href="#123__24" rel="nofollow">1.2.3 数据集构成</a></li></ul> 
   </li><li><a href="#13__29" rel="nofollow">1.3 机器学习算法分类</a></li><li><ul><li><a href="#131__30" rel="nofollow">1.3.1 分类</a></li><li><a href="#132__41" rel="nofollow">1.3.2 小练习</a></li><li><a href="#133__47" rel="nofollow">1.3.3 机器学习算法分类</a></li></ul> 
   </li><li><a href="#14__50" rel="nofollow">1.4 机器学习开发流程</a></li><li><a href="#15__60" rel="nofollow">1.5 学习框架和资料介绍</a></li><li><ul><li><a href="#151__73" rel="nofollow">1.5.1 机器学习库和框架</a></li><li><a href="#152__78" rel="nofollow">1.5.2 书籍资料</a></li></ul> 
  </li></ul> 
  </li><li><a href="#_82" rel="nofollow">二、特征工程</a></li><li><ul><li><a href="#21__83" rel="nofollow">2.1 数据集</a></li><li><ul><li><a href="#211__86" rel="nofollow">2.1.1 可用数据集（针对本次三天学习而言）</a></li><li><a href="#212_sklearn_110" rel="nofollow">2.1.2 sklearn数据集</a></li><li><a href="#213__124" rel="nofollow">2.1.3 数据集的划分</a></li></ul> 
   </li><li><a href="#22__134" rel="nofollow">2.2 特征工程介绍</a></li><li><ul><li><a href="#221_Feature_Engineering_136" rel="nofollow">2.2.1 为什么需要特征工程(Feature Engineering)</a></li><li><a href="#222__139" rel="nofollow">2.2.2 什么是特征工程</a></li><li><a href="#223__141" rel="nofollow">2.2.3 特征工程的位置与数据处理的比较</a></li></ul> 
   </li><li><a href="#23__149" rel="nofollow">2.3 特征提取</a></li><li><ul><li><a href="#231__152" rel="nofollow">2.3.1 特征提取</a></li><li><a href="#232__154" rel="nofollow">2.3.2 字典特征提取</a></li><li><a href="#233__172" rel="nofollow">2.3.3 上述总结</a></li><li><a href="#233_182" rel="nofollow">2.3.3文本特征提取</a></li><li><a href="#235__224" rel="nofollow">2.3.5 总结</a></li></ul> 
   </li><li><a href="#24__228" rel="nofollow">2.4 特征预处理</a></li><li><ul><li><a href="#241__230" rel="nofollow">2.4.1 什么是特征预处理</a></li><li><a href="#242__236" rel="nofollow">2.4.2 归一化</a></li><li><a href="#243__244" rel="nofollow">2.4.3 标准化</a></li></ul> 
   </li><li><a href="#25__255" rel="nofollow">2.5 特征降维</a></li><li><ul><li><a href="#251____256" rel="nofollow">2.5.1 降维 - 降低维度</a></li><li><a href="#252__268" rel="nofollow">2.5.2 降维的两种方式</a></li><li><a href="#253__273" rel="nofollow">2.5.3 特征选择</a></li><li><a href="#254__294" rel="nofollow">2.5.4 低方差特征过滤</a></li></ul> 
   </li><li><a href="#26__304" rel="nofollow">2.6 主成分分析</a></li><li><ul><li><a href="#261_PCA_305" rel="nofollow">2.6.1 什么是主成分分析(PCA)</a></li><li><a href="#262__314" rel="nofollow">2.6.2 案例：探究用户对物品类别的喜好细分</a></li></ul> 
  </li></ul> 
  </li><li><a href="#_323" rel="nofollow">三、分类算法</a></li><li><ul><li><a href="#31_sklearn_333" rel="nofollow">3.1 sklearn转换器和估计器</a></li><li><ul><li><a href="#311____336" rel="nofollow">3.1.1 转换器 - 特征工程的父类</a></li><li><a href="#312_sklearn_347" rel="nofollow">3.1.2 估计器(sklearn机器学习算法的实现)</a></li></ul> 
   </li><li><a href="#32_KKNN_358" rel="nofollow">3.2 K-近邻算法（KNN）</a></li><li><ul><li><a href="#321_K_359" rel="nofollow">3.2.1 什么是K-近邻算法</a></li><li><a href="#322_kAPI_385" rel="nofollow">3.2.2 k-邻近算法API</a></li><li><a href="#323_1_392" rel="nofollow">3.2.3 案例1：鸢尾花种类预测</a></li><li><a href="#324_K_402" rel="nofollow">3.2.4 K-近邻总结</a></li></ul> 
   </li><li><a href="#33__408" rel="nofollow">3.3 模型选择与调优</a></li><li><ul><li><a href="#331_cross_validation_409" rel="nofollow">3.3.1 什么是交叉验证(cross validation)</a></li><li><a href="#332_Grid_Search_414" rel="nofollow">3.3.2 超参数搜索-网格搜索(Grid Search)</a></li><li><a href="#333_K_421" rel="nofollow">3.3.3 鸢尾花案例增加K值调优</a></li><li><a href="#324_facebook_424" rel="nofollow">3.2.4 案例：预测facebook签到位置</a></li></ul> 
   </li><li><a href="#34__445" rel="nofollow">3.4 朴素贝叶斯算法</a></li><li><ul><li><a href="#341__447" rel="nofollow">3.4.1 什么是朴素贝叶斯分类方法</a></li><li><a href="#342__452" rel="nofollow">3.4.2 概率基础</a></li><li><a href="#343__456" rel="nofollow">3.4.3 联合概率、条件概率与相互独立</a></li><li><a href="#344__465" rel="nofollow">3.4.4 贝叶斯公式</a></li><li><a href="#346_20_484" rel="nofollow">3.4.6 案例：20类新闻分类</a></li><li><a href="#347__493" rel="nofollow">3.4.7 朴素贝叶斯算法总结</a></li></ul> 
   </li><li><a href="#35__496" rel="nofollow">3.5 决策树</a></li><li><ul><li><a href="#351__498" rel="nofollow">3.5.1 认识决策树</a></li><li><a href="#352__501" rel="nofollow">3.5.2 决策树分类原理详解</a></li><li><a href="#353_API_511" rel="nofollow">3.5.3 据册数API</a></li><li><a href="#354__525" rel="nofollow">3.5.4 决策树对鸢尾花分类</a></li><li><a href="#355__529" rel="nofollow">3.5.5 决策树可视化</a></li><li><a href="#356__537" rel="nofollow">3.5.6 决策树总结</a></li><li><a href="#357__544" rel="nofollow">3.5.7 案例：泰坦尼克号乘客生存预测</a></li></ul> 
   </li><li><a href="#36__560" rel="nofollow">3.6 集成学习方法之随机森林</a></li><li><ul><li><a href="#361__561" rel="nofollow">3.6.1 什么是集成学习方法</a></li><li><a href="#362__563" rel="nofollow">3.6.2 什么是随机森林</a></li><li><a href="#363__568" rel="nofollow">3.6.3 随机森林原理过程</a></li><li><a href="#364_API_588" rel="nofollow">3.6.4 API</a></li><li><a href="#366__592" rel="nofollow">3.6.6 总结</a></li></ul> 
  </li></ul> 
  </li><li><a href="#_596" rel="nofollow">四、回归和聚类</a></li><li><ul><li><a href="#41__609" rel="nofollow">4.1 线性回归</a></li><li><ul><li><a href="#411__612" rel="nofollow">4.1.1 线性回归的原理</a></li><li><a href="#412__615" rel="nofollow">4.1.2 什么是线性回归</a></li><li><a href="#412__640" rel="nofollow">4.1.2 线性回归的损失和优化原理（理解记忆）</a></li><li><a href="#413_API_665" rel="nofollow">4.1.3 线性回归API</a></li><li><a href="#414__670" rel="nofollow">4.1.4 波士顿房价预测</a></li><li><a href="#415__680" rel="nofollow">4.1.5 回归的性能评估</a></li><li><a href="#416__684" rel="nofollow">4.1.6 正规方程和梯度下降对比</a></li><li><a href="#417__688" rel="nofollow">4.1.7 总结</a></li></ul> 
   </li><li><a href="#42__696" rel="nofollow">4.2 欠拟合与过拟合</a></li><li><ul><li><a href="#421__698" rel="nofollow">4.2.1 什么是过拟合与欠拟合</a></li><li><a href="#422__712" rel="nofollow">4.2.2 原因及解决方法</a></li></ul> 
   </li><li><a href="#43__723" rel="nofollow">4.3 线性回归的改进-岭回归</a></li><li><ul><li><a href="#431_L2_725" rel="nofollow">4.3.1 带有L2正则化的线性回归-岭回归</a></li></ul> 
   </li><li><a href="#44__736" rel="nofollow">4.4 分类算法-逻辑回归与二分类</a></li><li><ul><li><a href="#441__737" rel="nofollow">4.4.1 逻辑回归的应用场景</a></li><li><a href="#442__746" rel="nofollow">4.4.2 逻辑回归的原理</a></li><li><a href="#444__778" rel="nofollow">4.4.4 案例：癌症分类预测-良／恶性乳腺癌肿瘤预测</a></li><li><a href="#445__791" rel="nofollow">4.4.5 分类的评估方法</a></li></ul> 
   </li><li><a href="#45__830" rel="nofollow">4.5 模型保存和加载</a></li><li><ul><li><a href="#451_sklearnAPI_832" rel="nofollow">4.5.1 sklearn模型的保存和加载API</a></li></ul> 
   </li><li><a href="#46_Kmeans_837" rel="nofollow">4.6 无监督学习-K-means算法</a></li><li><ul><li><a href="#461__838" rel="nofollow">4.6.1 什么是无监督学习</a></li><li><a href="#462__842" rel="nofollow">4.6.2 无监督学习包含算法</a></li><li><a href="#463_Kmeans_847" rel="nofollow">4.6.3 K-means原理</a></li><li><a href="#464_API_857" rel="nofollow">4.6.4 API</a></li><li><a href="#465_kmeansInstacart_Market_860" rel="nofollow">4.6.5 案例：k-means对Instacart Market用户聚类</a></li><li><a href="#466_Kmeans_870" rel="nofollow">4.6.6 Kmeans性能评估指标</a></li><li><a href="#467_Kmeans_885" rel="nofollow">4.6.7 K-means总结</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="__2"></a>一、 机器学习概述</h2> 
<h3><a id="11__3"></a>1.1 人工智能概述</h3> 
<h4><a id="111_4"></a>1.1.1介绍</h4> 
<p>让机器下棋（1950年人工智能），过滤垃圾邮件（1980年机器学习），图像识别（2010年深度学习）<br> 达特茅斯会议-人工智能的起点<br> 机器学习是人工智能的一个实现途径<br> 深度学习是机器学习的一个方法发展而来</p> 
<p>用机器来模仿人类学习以及其他方面的智能</p> 
<h4><a id="112__11"></a>1.1.2 机器学习、深度学习能做些什么</h4> 
<p>传统预测：量化投资、广告推荐、销量预测<br> 图像识别：人脸识别、街道交通标志检测<br> 自然语言处理：文本分类、情感分析、自动聊天、文本检测</p> 
<h4><a id="113__16"></a>1.1.3 人工智能阶段课程安排</h4> 
<p><img src="https://images2.imgbox.com/68/e8/yY8ti2Ra_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="12__19"></a>1.2 什么是机器学习</h3> 
<p>定义：从数据中自动分析获得模型，并利用模型对未知数据进行预测<br> 数据、 模型、预测<br> <img src="https://images2.imgbox.com/83/8e/kuF8aPNs_o.png" alt="在这里插入图片描述"><br> 从历史数据当中获得规律？这些历史数据是怎么的格式？</p> 
<h4><a id="123__24"></a>1.2.3 数据集构成</h4> 
<p>结构：特征值 + 目标值<br> <img src="https://images2.imgbox.com/71/ad/vGbWTVhV_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/66/0e/oNNp5L4y_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="13__29"></a>1.3 机器学习算法分类</h3> 
<h4><a id="131__30"></a>1.3.1 分类</h4> 
<p><img src="https://images2.imgbox.com/ad/ab/CO1eRaEd_o.png" alt="在这里插入图片描述"></p> 
<p>有目标值是<mark>监督学习</mark></p> 
<ul><li>目标值：<strong>类别</strong> （猫狗分类）----- <mark>分类问题</mark><br> k-近邻算法、贝叶斯分类、决策树与随机森林</li><li>目标值：<strong>连续型的数据</strong> （房屋价格预测）-------<mark>回归问题</mark><br> 线性回归、岭回归</li><li>无目标值---------- <mark>无监督学习</mark><br> 聚类 k-means</li></ul> 
<h4><a id="132__41"></a>1.3.2 小练习</h4> 
<p>1、预测明天的气温是多少度？ 回归<br> 2、预测明天是阴、晴还是雨？ 分类<br> 3、人脸年龄预测？ 回归（多少岁）/分类（老的小的）<br> 4、人脸识别？ 分类</p> 
<h4><a id="133__47"></a>1.3.3 机器学习算法分类</h4> 
<p><img src="https://images2.imgbox.com/2f/f4/asFwOGE4_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="14__50"></a>1.4 机器学习开发流程</h3> 
<p><img src="https://images2.imgbox.com/70/7b/qAdQY81a_o.png" alt="在这里插入图片描述"><br> 1）获取数据<br> 2）数据处理（缺失值什么的）<br> 3）特征工程（处理为能使用的数据）<br> 4）机器学习算法训练 - 模型<br> 5）模型评估（模型好不好）<br> 6）应用（不好的话返回2、3）<img src="https://images2.imgbox.com/f6/5c/Ij87RbrD_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="15__60"></a>1.5 学习框架和资料介绍</h3> 
<p>1）算法是核心，数据与计算是基础<br> 2）找准定位<br> <img src="https://images2.imgbox.com/c4/22/bI584tup_o.png" alt="在这里插入图片描述"></p> 
<p>3）怎么做？<br> 1、入门<br> 2、实战类书籍<br> 3、机器学习 -”西瓜书”- 周志华<br> 统计学习方法 - 李航<br> 深度学习 - “花书”<br> 4）1.5.1 机器学习库与框架<br> <img src="https://images2.imgbox.com/5a/bb/lFAdv1fs_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="151__73"></a>1.5.1 机器学习库和框架</h4> 
<p><img src="https://images2.imgbox.com/3a/44/x7GvnOZZ_o.png" alt="在这里插入图片描述"><br> sklearn、tensorflow、caffe、pytorch、theano、Chainer<br> 研究算法底层，研究框架</p> 
<h4><a id="152__78"></a>1.5.2 书籍资料</h4> 
<p><img src="https://images2.imgbox.com/59/f6/LPN4hu5x_o.png" alt="在这里插入图片描述"><br> 提升：<br> <img src="https://images2.imgbox.com/92/e2/1KeF2gwg_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_82"></a>二、特征工程</h2> 
<h3><a id="21__83"></a>2.1 数据集</h3> 
<ul><li>分为训练集和测试集</li><li>会使用sklearn的数据集</li></ul> 
<h4><a id="211__86"></a>2.1.1 可用数据集（针对本次三天学习而言）</h4> 
<p>公司内部 百度<br> 数据接口 花钱<br> 数据集<br> 学习阶段可以用的数据集：<br> 1）<a href="https://scikit-learn.org/stable/datasets/index.html#datasets" rel="nofollow">sklearn</a><br> 数据量小、方便学习<br> 2）<a href="https://www.kaggle.com/datasets" rel="nofollow">kaggle</a><br> 大数据竞赛平台、80万科学家、真实数据、数据量巨大<br> 3）<a href="http://archive.ics.uci.edu/ml/index.php" rel="nofollow">UCI</a><br> 目前600多数据集、领域广、数据量几十万</p> 
<ol><li>Scikit-learn工具介绍<br> <img src="https://images2.imgbox.com/95/6b/jtt01PZc_o.png" alt="在这里插入图片描述"></li><li>安装</li></ol> 
<pre><code class="prism language-python">pip3 install Scikit<span class="token operator">-</span>learn<span class="token operator">==</span><span class="token number">0.19</span><span class="token number">.1</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/12/85/v9UIwZpK_o.png" alt="在这里插入图片描述"></p> 
<ol start="3"><li>Scikit-learn包含的内容<br> <img src="https://images2.imgbox.com/44/ec/lBqIYyHV_o.png" alt="在这里插入图片描述"></li></ol> 
<h4><a id="212_sklearn_110"></a>2.1.2 sklearn数据集</h4> 
<ol><li>scikit-learn数据集API介绍<br> <img src="https://images2.imgbox.com/4a/74/Pp6FZcar_o.png" alt="在这里插入图片描述"></li><li>sklearn小数据集<br> <img src="https://images2.imgbox.com/95/33/FHwHTc7a_o.png" alt="在这里插入图片描述"></li><li>sklearn大数据集<br> <img src="https://images2.imgbox.com/10/9c/3N9Sbh2b_o.png" alt="在这里插入图片描述"></li><li>sklearn数据集的使用<br> <img src="https://images2.imgbox.com/80/8f/m0p5NTNv_o.png" alt="在这里插入图片描述"></li><li>数据集的返回值<img src="https://images2.imgbox.com/d6/1c/nq3A6iuf_o.png" alt="在这里插入图片描述"><br> datasets.base.Bunch（继承自字典）<br> dict[“key”] = values<br> bunch.key = values <img src="https://images2.imgbox.com/38/e0/2dUezEY0_o.png" alt="在这里插入图片描述"> 思考：拿到的数据是否全部都用来训练一个模型？<br> no,测试集</li></ol> 
<h4><a id="213__124"></a>2.1.3 数据集的划分</h4> 
<ul><li>训练数据：用于训练，构建模型</li><li>测试数据：在模型检验时使用，用于评估模型是否有效<br> 测试集 20%~30%<br> <img src="https://images2.imgbox.com/26/5c/B7bnBUvM_o.png" alt="在这里插入图片描述"><br> sklearn.model_selection.train_test_split(arrays, *options)<br> 训练集特征值，测试集特征值，训练集目标值，测试集目标值<br> x_train, x_test, y_train, y_test<br> <img src="https://images2.imgbox.com/9c/19/izHoRa5e_o.png" alt="在这里插入图片描述"></li></ul> 
<h3><a id="22__134"></a>2.2 特征工程介绍</h3> 
<p>影响训练效果的原因：算法、特征工程</p> 
<h4><a id="221_Feature_Engineering_136"></a>2.2.1 为什么需要特征工程(Feature Engineering)</h4> 
<p><img src="https://images2.imgbox.com/f6/d8/kU6zKrkq_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="222__139"></a>2.2.2 什么是特征工程</h4> 
<p><img src="https://images2.imgbox.com/66/c8/WduUnjyy_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="223__141"></a>2.2.3 特征工程的位置与数据处理的比较</h4> 
<p><img src="https://images2.imgbox.com/73/1f/Icy3f5rT_o.png" alt="在这里插入图片描述"><br> sklearn 特征工程<br> pandas 数据清洗、数据处理直接<br> 特征抽取/特征提取（有些数据不能处理，需要转换）<br> 机器学习算法 - 统计方法 - 数学公式<br> 文本类型 -》 数值<br> 类型 -》 数值</p> 
<h3><a id="23__149"></a>2.3 特征提取</h3> 
<p><img src="https://images2.imgbox.com/fc/60/bNt6fJy5_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ee/16/c2SaSLOv_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="231__152"></a>2.3.1 特征提取</h4> 
<p>sklearn.feature_extraction</p> 
<h4><a id="232__154"></a>2.3.2 字典特征提取</h4> 
<p>类别 -&gt; one-hot编码<br> <img src="https://images2.imgbox.com/0a/9f/LrbcfxIn_o.png" alt="在这里插入图片描述"><br> sklearn.feature_extraction.DictVectorizer(sparse=True,…)<br> vector 数学：向量 物理：矢量<br> 矩阵 matrix 二维数组<br> 向量 vector 一维数组<br> 父类：转换器类<br> 返回sparse矩阵<br> sparse稀疏<br> 将非零值 按位置表示出来<br> 节省内存 - 提高加载效率<br> <img src="https://images2.imgbox.com/21/14/st2t0NZ7_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/97/b4/yvZAxiOT_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/91/b0/3pS7Whun_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/8a/06/aaEUCsLZ_o.png" alt="在这里插入图片描述"><br> 转化为：<img src="https://images2.imgbox.com/81/27/roj9uKm1_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="233__172"></a>2.3.3 上述总结</h4> 
<p>对于特征当中存在<mark>类别信息</mark>的我们都会做<mark>one-hot编码</mark>处理</p> 
<p>应用场景：<br> 1）pclass, sex 数据集当中<strong>类别特征比较多</strong><br> 1、将数据集的特征-》字典类型<br> 2、DictVectorizer转换<br> 2）<strong>本身</strong>拿到的数据就是<strong>字典类型</strong></p> 
<h4><a id="233_182"></a>2.3.3文本特征提取</h4> 
<p>作用：对文本数据进行特征值化<br> 单词 作为 特征<br> 句子、短语、单词、字母<br> 特征：特征词<br> <img src="https://images2.imgbox.com/ee/84/38tlIubC_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/19/e5/ycJOAwAu_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/09/8d/qqVGzbma_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/af/ac/5rsFNDDY_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/61/61/3jxcLXqc_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/b5/a3/SZcEa3XM_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/7b/20/pFfRvNh1_o.png" alt="在这里插入图片描述"></p> 
<ul><li>方法1：CountVectorizer<br> 统计每个样本特征词出现的个数<br> stop_words停用的<br> 停用词表</li></ul> 
<p>关键词：在某一个类别的文章中，出现的次数很多，但是在其他类别的文章当中出现很少</p> 
<ul><li>方法2：TfidfVectorizer<br> <img src="https://images2.imgbox.com/b0/2d/ZVarR8dl_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/bb/70/sySef3dH_o.png" alt="在这里插入图片描述"></li></ul> 
<p>TF-IDF - 重要程度<br> 两个词 “经济”，“非常”<br> 1000篇文章-语料库<br> 100篇文章 - “非常”<br> 10篇文章 - “经济”<br> 两篇文章<br> 文章A(100词) : 10次“经济” TF-IDF:0.2<br> tf:10/100 = 0.1<br> idf:lg 1000/10 = 2<br> 文章B(100词) : 10次“非常” TF-IDF:0.1<br> tf:10/100 = 0.1<br> idf: log 10 1000/100 = 1</p> 
<p>TF - 词频（term frequency，tf)<br> IDF - 逆向文档频率<br> <img src="https://images2.imgbox.com/5c/83/RkVqCGUk_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="235__224"></a>2.3.5 总结</h4> 
<p>对字典的类别转换为onehot编码<br> 对文本：1. 统计特征值出现的个数 2. 计算词的重要性程度</p> 
<h3><a id="24__228"></a>2.4 特征预处理</h3> 
<h4><a id="241__230"></a>2.4.1 什么是特征预处理</h4> 
<p><img src="https://images2.imgbox.com/03/28/LCFNSbTg_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/74/a0/PcaBhz4N_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/c9/d5/Je0RN0DR_o.png" alt="在这里插入图片描述"><br> 为什么我们要进行归一化/标准化？<br> 无量纲化</p> 
<h4><a id="242__236"></a>2.4.2 归一化</h4> 
<p><img src="https://images2.imgbox.com/89/3a/Ow5yREdM_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/7e/7b/eXDtuP0b_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ca/d7/G39VSSfj_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ae/3b/OuhBBTey_o.png" alt="在这里插入图片描述"><br> 异常值：最大值、最小值<br> <img src="https://images2.imgbox.com/1e/76/ffjxMUEn_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="243__244"></a>2.4.3 标准化</h4> 
<p><img src="https://images2.imgbox.com/c2/7c/DdqgNYcf_o.png" alt="在这里插入图片描述"><br> (x - mean) / std<br> 均值变化不会太大<br> 标准差：集中程度<br> <img src="https://images2.imgbox.com/ec/6b/ZptOXTVr_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/b1/11/ysTAyPHw_o.png" alt="在这里插入图片描述"></p> 
<p>应用场景：<br> <strong>在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。</strong></p> 
<h3><a id="25__255"></a>2.5 特征降维</h3> 
<h4><a id="251____256"></a>2.5.1 降维 - 降低维度</h4> 
<p>ndarray<br> 维数：嵌套的层数<br> 0维 标量<br> 1维 向量<br> 2维 矩阵<br> 3维<br> n维<br> 二维数组<br> 此处的降维：降低特征的个数（列数）<br> 效果：<br> 特征与特征之间不相关</p> 
<h4><a id="252__268"></a>2.5.2 降维的两种方式</h4> 
<p><img src="https://images2.imgbox.com/f3/ea/TTwR2V46_o.png" alt="在这里插入图片描述"></p> 
<ul><li>特征选择</li><li>主成分分析（理解为一种特征提取的方式）</li></ul> 
<h4><a id="253__273"></a>2.5.3 特征选择</h4> 
<p><img src="https://images2.imgbox.com/fb/69/UbQDCcuJ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/b2/3c/abDbgBNb_o.png" alt="在这里插入图片描述"></p> 
<p>特征选择</p> 
<ul><li>Filter过滤式<br> 方差选择法：低方差特征过滤<br> 相关系数 - 特征与特征之间的相关程度<br> 取值范围：–1≤ r ≤+1<br> 皮尔逊相关系数<br> 0.9942<br> <strong>特征与特征之间相关性很高</strong>：<br> 1）选取其中一个<br> 2）加权求和<br> 3）主成分分析</li><li>Embeded嵌入式<br> 决策树 第二天<br> 正则化 第三天<br> 深度学习 第五天</li></ul> 
<h4><a id="254__294"></a>2.5.4 低方差特征过滤</h4> 
<p><img src="https://images2.imgbox.com/b2/fb/Oz5PgzRA_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/14/2b/YnCiiP9Y_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/b7/dd/WNcTm1zX_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/8e/ec/nGqr8yme_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/f0/63/gcZIRXAv_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="26__304"></a>2.6 主成分分析</h3> 
<h4><a id="261_PCA_305"></a>2.6.1 什么是主成分分析(PCA)</h4> 
<p><img src="https://images2.imgbox.com/fe/43/Cf55eAR0_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/36/bd/c6xekARb_o.png" alt="在这里插入图片描述"></p> 
<p>sklearn.decomposition.PCA(n_components=None)<br> n_components<br> 小数 表示保留百分之多少的信息<br> 整数 减少到多少特征</p> 
<h4><a id="262__314"></a>2.6.2 案例：探究用户对物品类别的喜好细分</h4> 
<p><img src="https://images2.imgbox.com/12/1f/SezcG87v_o.png" alt="在这里插入图片描述"></p> 
<p>用户 物品类别<br> user_id aisle<br> 1）需要将user_id和aisle放在同一个表中 - 合并<br> 2）找到user_id和aisle - 交叉表和透视表<br> 3）特征冗余过多 -&gt; PCA降维</p> 
<h2><a id="_323"></a>三、分类算法</h2> 
<p>目标值：类别</p> 
<p>1、sklearn转换器和预估器<br> 2、KNN算法<br> 3、模型选择与调优<br> 4、朴素贝叶斯算法<br> 5、决策树<br> 6、随机森林</p> 
<h3><a id="31_sklearn_333"></a>3.1 sklearn转换器和估计器</h3> 
<p>转换器<br> 估计器(estimator)</p> 
<h4><a id="311____336"></a>3.1.1 转换器 - 特征工程的父类</h4> 
<p>1 实例化 (实例化的是一个转换器类(Transformer))<br> 2 调用fit_transform(对于文档建立分类词频矩阵，不能同时调用)<br> 标准化：<br> (x - mean) / std<br> fit_transform()<br> fit() 计算 每一列的平均值、标准差<br> transform() (x - mean) / std进行最终的转换</p> 
<h4><a id="312_sklearn_347"></a>3.1.2 估计器(sklearn机器学习算法的实现)</h4> 
<p>估计器工作流程：<br> <img src="https://images2.imgbox.com/d9/5a/0v1Aepxv_o.png" alt="在这里插入图片描述"> 估计器(estimator)<br> 1 实例化一个estimator<br> 2 estimator.fit(x_train, y_train) 计算 —— 调用完毕，模型生成<br> 3 模型评估：<br> 1）直接比对真实值和预测值<br> y_predict = estimator.predict(x_test)<br> y_test == y_predict<br> 2）计算准确率<br> accuracy = estimator.score(x_test, y_test)</p> 
<h3><a id="32_KKNN_358"></a>3.2 K-近邻算法（KNN）</h3> 
<h4><a id="321_K_359"></a>3.2.1 什么是K-近邻算法</h4> 
<p>KNN<strong>核心思想</strong>：<br> 你的“邻居”来推断出你的类别</p> 
<ol><li>K-近邻算法(KNN)原理<br> <img src="https://images2.imgbox.com/6c/f6/V3eG2c1l_o.png" alt="在这里插入图片描述"><br> k = 1<br> 容易受到异常点的影响<br> 如何确定谁是邻居？<br> 计算距离：<br> 距离公式<br> 欧氏距离<br> 曼哈顿距离 绝对值距离<br> 明可夫斯基距离</li><li>电影类型分析<br> k = 1 爱情片<br> k = 2 爱情片<br> ……<br> k = 6 无法确定<br> k = 7 动作片</li></ol> 
<p>如果取的最近的电影数量不一样？会是什么结果？<br> k 值取得过小，容易受到异常点的影响<br> k 值取得过大，样本不均衡的影响<br> 结合前面的约会对象数据，分析K-近邻算法需要做什么样的处理<br> 无量纲化的处理<br> 标准化</p> 
<h4><a id="322_kAPI_385"></a>3.2.2 k-邻近算法API</h4> 
<p><img src="https://images2.imgbox.com/ab/52/OFOe4qSr_o.png" alt="在这里插入图片描述"></p> 
<p>sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm=‘auto’)<br> n_neighbors：k值</p> 
<h4><a id="323_1_392"></a>3.2.3 案例1：鸢尾花种类预测</h4> 
<p><img src="https://images2.imgbox.com/1b/40/2g5OAMjg_o.png" alt="在这里插入图片描述"></p> 
<p>1）获取数据<br> 2）数据集划分<br> 3）特征工程<br> 标准化<br> 4）KNN预估器流程<br> 5）模型评估<br> <img src="https://images2.imgbox.com/77/ed/FjIdIdLk_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="324_K_402"></a>3.2.4 K-近邻总结</h4> 
<p>优点：简单，易于理解，易于实现，无需训练<br> 缺点：<br> 1）必须指定K值，K值选择不当则分类精度不能保证<br> 2）懒惰算法，对测试样本分类时的计算量大，内存开销大<br> 使用场景：<strong>小数据场景</strong>，几千～几万样本，具体场景具体业务去测试</p> 
<h3><a id="33__408"></a>3.3 模型选择与调优</h3> 
<h4><a id="331_cross_validation_409"></a>3.3.1 什么是交叉验证(cross validation)</h4> 
<p><img src="https://images2.imgbox.com/d5/5f/4CjkkMjB_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/79/50/WzUtOjHu_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="332_Grid_Search_414"></a>3.3.2 超参数搜索-网格搜索(Grid Search)</h4> 
<p>k的取值<br> [1, 3, 5, 7, 9, 11]<br> 暴力破解<br> <img src="https://images2.imgbox.com/27/ce/zlHfHD2I_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/e6/b3/ERTZCbah_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="333_K_421"></a>3.3.3 鸢尾花案例增加K值调优</h4> 
<p><img src="https://images2.imgbox.com/a1/aa/9vmFygTW_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="324_facebook_424"></a>3.2.4 案例：预测facebook签到位置</h4> 
<p>流程分析：<br> 1）获取数据<br> 2）数据处理<br> 目的：<br> 特征值 x<br> 目标值 y<br> a.缩小数据范围<br> 2 &lt; x &lt; 2.5<br> 1.0 &lt; y &lt; 1.5<br> b.time -&gt; 年月日时分秒<br> c.过滤签到次数少的地点<br> 数据集划分<br> 3）特征工程：标准化<br> 4）KNN算法预估流程<br> 5）模型选择与调优<br> 6）模型评估</p> 
<h3><a id="34__445"></a>3.4 朴素贝叶斯算法</h3> 
<h4><a id="341__447"></a>3.4.1 什么是朴素贝叶斯分类方法</h4> 
<p>分完之后出现概率值<br> <img src="https://images2.imgbox.com/8e/60/tDdR8NTf_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="342__452"></a>3.4.2 概率基础</h4> 
<p>1 概率(Probability)定义<br> <img src="https://images2.imgbox.com/ff/f1/vJfwaAp3_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="343__456"></a>3.4.3 联合概率、条件概率与相互独立</h4> 
<ul><li>联合概率：包含多个条件，且所有条件同时成立的概率<br> P(程序员, 匀称) P(程序员, 超重|喜欢)<br> P(A, B)</li><li>条件概率：就是事件A在另外一个事件B已经发生条件下的发生概率<br> P(程序员|喜欢) P(程序员, 超重|喜欢)<br> P(A|B)</li><li>相互独立:<br> P(A, B) = P(A)P(B) &lt;=&gt; 事件A与事件B相互独立</li></ul> 
<h4><a id="344__465"></a>3.4.4 贝叶斯公式</h4> 
<p><img src="https://images2.imgbox.com/94/0c/c4CaA83e_o.png" alt="在这里插入图片描述"></p> 
<p><strong>朴素？<br> 假设：特征与特征之间是相互独立</strong></p> 
<p>朴素贝叶斯算法：<br> 朴素 + 贝叶斯<br> 应用场景：<br> 文本分类<br> <img src="https://images2.imgbox.com/06/20/FkFrLpij_o.png" alt="在这里插入图片描述"><br> 单词作为特征</p> 
<p><img src="https://images2.imgbox.com/62/6f/W0WtAKmY_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/84/2d/lSjJVwnQ_o.png" alt="在这里插入图片描述"></p> 
<p>拉普拉斯平滑系数</p> 
<h4><a id="346_20_484"></a>3.4.6 案例：20类新闻分类</h4> 
<p>1）获取数据<br> 2）划分数据集<br> 3）特征工程<br> 文本特征抽取<br> 4）朴素贝叶斯预估器流程<br> 5）模型评估<br> <img src="https://images2.imgbox.com/d2/4b/6JRfZIiM_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="347__493"></a>3.4.7 朴素贝叶斯算法总结</h4> 
<p><img src="https://images2.imgbox.com/7a/e6/I4IwLPVv_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="35__496"></a>3.5 决策树</h3> 
<p><img src="https://images2.imgbox.com/e4/8c/GyBW4wLF_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="351__498"></a>3.5.1 认识决策树</h4> 
<p>如何高效的进行决策？<br> 特征的先后顺序</p> 
<h4><a id="352__501"></a>3.5.2 决策树分类原理详解</h4> 
<p>已知 四个特征值 预测 是否贷款给某个人<br> 先看房子，再工作 -&gt; 是否贷款 只看了两个特征<br> 年龄，信贷情况，工作 看了三个特征<br> <img src="https://images2.imgbox.com/5e/d0/0bHeQ9fM_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/79/42/ID0gtV1l_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/be/7a/33WnArdO_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/a1/b9/bAglNlxg_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/56/3e/m5lVIVHz_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="353_API_511"></a>3.5.3 据册数API</h4> 
<p><img src="https://images2.imgbox.com/7d/ce/MlPOLH6R_o.png" alt="在这里插入图片描述"></p> 
<p>信息论基础<br> 1）信息<br> 香农：消除随机不定性的东西<br> 小明 年龄 “我今年18岁” - 信息<br> 小华 ”小明明年19岁” - 不是信息<br> 2）信息的衡量 - 信息量 - 信息熵<br> bit<br> g(D,A) = H(D) - 条件熵H(D|A)<br> 4 决策树的划分依据之一------信息增益<br> 没有免费的午餐</p> 
<h4><a id="354__525"></a>3.5.4 决策树对鸢尾花分类</h4> 
<p><img src="https://images2.imgbox.com/3d/fc/NxbfQpLy_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="355__529"></a>3.5.5 决策树可视化</h4> 
<p><img src="https://images2.imgbox.com/ab/7d/XJsio1Gr_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/bb/2f/kMzWRvow_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="356__537"></a>3.5.6 决策树总结</h4> 
<p><img src="https://images2.imgbox.com/be/47/CsqRCiNv_o.png" alt="在这里插入图片描述"></p> 
<p>优点：<br> 可视化 - 可解释能力强<br> 缺点：<br> 容易产生过拟合</p> 
<h4><a id="357__544"></a>3.5.7 案例：泰坦尼克号乘客生存预测</h4> 
<p>流程分析：<br> 特征值 目标值<br> 1）获取数据<br> 2）数据处理<br> 缺失值处理<br> 特征值 -&gt; 字典类型<br> 3）准备好特征值 目标值<br> 4）划分数据集<br> 5）特征工程：字典特征抽取<br> 6）决策树预估器流程<br> 7）模型评估</p> 
<h3><a id="36__560"></a>3.6 集成学习方法之随机森林</h3> 
<h4><a id="361__561"></a>3.6.1 什么是集成学习方法</h4> 
<p><img src="https://images2.imgbox.com/94/c0/xtjlN0WZ_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="362__563"></a>3.6.2 什么是随机森林</h4> 
<p>随机<br> 森林：包含多个决策树的分类器<br> <img src="https://images2.imgbox.com/8e/98/XBwnakNK_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="363__568"></a>3.6.3 随机森林原理过程</h4> 
<p><img src="https://images2.imgbox.com/da/29/m7lWW2Gu_o.png" alt="在这里插入图片描述"></p> 
<p>训练集：<br> N个样本<br> 特征值 目标值<br> M个特征<br> 随机<br> 两个随机</p> 
<ul><li>训练集随机 - N个样本中随机有放回的抽样N个<br> bootstrap 随机有放回抽样<br> [1, 2, 3, 4, 5]<br> 新的树的训练集<br> [2, 2, 3, 1, 5]</li><li>特征随机 - 从M个特征中随机抽取m个特征<br> M &gt;&gt; m<br> 降维<br> <img src="https://images2.imgbox.com/52/9c/ihqC3AN5_o.png" alt="在这里插入图片描述"></li></ul> 
<h4><a id="364_API_588"></a>3.6.4 API</h4> 
<p><img src="https://images2.imgbox.com/7a/89/MtHS1qcR_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="366__592"></a>3.6.6 总结</h4> 
<p>能够有效地运行在大数据集上，<br> 处理具有高维特征的输入样本，而且不需要降维<br> <img src="https://images2.imgbox.com/02/69/XPAbI4Yv_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_596"></a>四、回归和聚类</h2> 
<p>线性回归<br> 欠拟合与过拟合<br> 岭回归</p> 
<p>分类算法：逻辑回归</p> 
<p>模型保存与加载</p> 
<p>无监督学习 K-means算法</p> 
<h3><a id="41__609"></a>4.1 线性回归</h3> 
<p>回归问题：<br> 目标值 - 连续型的数据</p> 
<h4><a id="411__612"></a>4.1.1 线性回归的原理</h4> 
<p><img src="https://images2.imgbox.com/51/9d/SXH2ID3V_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="412__615"></a>4.1.2 什么是线性回归</h4> 
<p><img src="https://images2.imgbox.com/2a/56/c3ZapF8Z_o.png" alt="在这里插入图片描述"></p> 
<p>函数关系 ：特征值和目标值<br> 线型模型<br> 线性关系<br> y = w1x1 + w2x2 + w3x3 + …… + wnxn + b<br> = wTx + b<br> 数据挖掘基础<br> y = kx + b<br> y=w1<em>x1+w2</em>x2+b<br> <img src="https://images2.imgbox.com/8a/64/L8o90lu0_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/1a/94/QUaRtpzK_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/a4/07/upVVaVAp_o.png" alt="在这里插入图片描述"></p> 
<p>广义线性模型<br> 也有非线性关系<br> <mark>线性模型</mark><br> <strong>自变量一次</strong>(线性关系)<br> y = w1x1 + w2x2 + w3x3 + …… + wnxn + b<br> <strong>参数一次</strong><br> y = w1x1 + w2x1^2 + w3x1^3 + w4x2^3 + …… + b（w1,w2,w3是一次的）<br> <strong>线性关系&amp;线性模型</strong><br> 线性关系一定是线性模型<br> 线性模型不一定是线性关系</p> 
<h4><a id="412__640"></a>4.1.2 线性回归的损失和优化原理（理解记忆）</h4> 
<p>目标：求模型参数<br> 模型参数能够使得预测准确<br> 真实关系：真实房子价格 = 0.02×中心区域的距离 + 0.04×城市一氧化氮浓度 + (-0.12×自住房平均房价) + 0.254×城镇犯罪率<br> 随意假定：预测房子价格 = 0.25×中心区域的距离 + 0.14×城市一氧化氮浓度 + 0.42×自住房平均房价 + 0.34×城镇犯罪率<br> 损失函数/cost/成本函数/目标函数：<br> <img src="https://images2.imgbox.com/e2/32/5ruTHtBD_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/f6/bb/GwlI5wDq_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/d2/e0/SyS2JXl2_o.png" alt="在这里插入图片描述"></p> 
<ul><li> <p>正规方程<br> 天才 - 直接求解W</p> </li><li> <p>梯度下降</p> </li></ul> 
<p>勤奋努力的普通人<br> 试错、改进<br> <img src="https://images2.imgbox.com/2f/25/vToUdu0o_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/20/c5/dMp6l1Hq_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/56/a3/j6JtMEdd_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/8c/93/1yH5ZQV5_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="413_API_665"></a>4.1.3 线性回归API</h4> 
<p><img src="https://images2.imgbox.com/9d/c5/utN0qW4h_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="414__670"></a>4.1.4 波士顿房价预测</h4> 
<p>流程：<br> 1）获取数据集<br> 2）划分数据集<br> 3）特征工程：<br> 无量纲化 - 标准化<br> 4）预估器流程<br> fit() --&gt; 模型<br> coef_ intercept_<br> 5）模型评估</p> 
<h4><a id="415__680"></a>4.1.5 回归的性能评估</h4> 
<p>均方误差<br> <img src="https://images2.imgbox.com/8d/23/cNgEqwS1_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="416__684"></a>4.1.6 正规方程和梯度下降对比</h4> 
<p><img src="https://images2.imgbox.com/aa/9a/Mn5Gh68D_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/5c/a8/jL7Fe9Sq_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/09/54/QoSXBZLI_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="417__688"></a>4.1.7 总结</h4> 
<p><img src="https://images2.imgbox.com/50/c3/rRm5w5fe_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="42__696"></a>4.2 欠拟合与过拟合</h3> 
<p>训练集上表现得好，测试集上不好 - <strong>过拟合</strong></p> 
<h4><a id="421__698"></a>4.2.1 什么是过拟合与欠拟合</h4> 
<ul><li>欠拟合（学习的少，偷懒）</li></ul> 
<p>学习到数据的特征过少<br> 解决：<br> 增加数据的特征数量</p> 
<ul><li>过拟合（想面面俱到，想太多）<br> 原始特征过多，存在一些嘈杂特征， 模型过于复杂是因为模型尝试去兼顾各个测试数据点<br> 解决：<br> 正则化<br> <img src="https://images2.imgbox.com/18/b7/PNIKW47L_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/95/a3/3wusZiCs_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/bc/c2/8yyc0udF_o.png" alt="在这里插入图片描述"></li></ul> 
<h4><a id="422__712"></a>4.2.2 原因及解决方法</h4> 
<p><img src="https://images2.imgbox.com/8e/ae/x1wGnJg0_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/6e/05/fl9ftW0L_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/03/b3/oHTaIvk1_o.png" alt="在这里插入图片描述"></p> 
<p>L1<br> 损失函数 + λ惩罚项<br> LASSO<br> L2 更常用<br> 损失函数 + λ惩罚项<br> Ridge - 岭回归</p> 
<h3><a id="43__723"></a>4.3 线性回归的改进-岭回归</h3> 
<h4><a id="431_L2_725"></a>4.3.1 带有L2正则化的线性回归-岭回归</h4> 
<p><img src="https://images2.imgbox.com/64/3b/bS3wo4xA_o.png" alt="在这里插入图片描述"></p> 
<ol><li>API<br> <img src="https://images2.imgbox.com/f2/94/YZnoFLmv_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/b0/61/k6QIQluA_o.png" alt="在这里插入图片描述"></li></ol> 
<p>alpha 正则化力度=惩罚项系数<br> <img src="https://images2.imgbox.com/56/d4/AmWMBcID_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="44__736"></a>4.4 分类算法-逻辑回归与二分类</h3> 
<h4><a id="441__737"></a>4.4.1 逻辑回归的应用场景</h4> 
<p>广告点击率 是否会被点击<br> 是否为垃圾邮件<br> 是否患病<br> 是否为金融诈骗<br> 是否为虚假账号<br> 正例 / 反例<br> <img src="https://images2.imgbox.com/25/de/FpnsJwvA_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="442__746"></a>4.4.2 逻辑回归的原理</h4> 
<p><img src="https://images2.imgbox.com/5c/58/X6z1GBN6_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/49/a0/ufNOTnVB_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/28/e4/p5W0yNQA_o.png" alt="在这里插入图片描述"></p> 
<p><strong>线型回归的输出 就是 逻辑回归 的 输入</strong></p> 
<ul><li> <p>激活函数<br> sigmoid函数 [0, 1]<br> 1/(1 + e^(-x))<br> 假设函数/线性模型<br> 1/(1 + e^(-(w1x1 + w2x2 + w3x3 + …… + wnxn + b)))</p> </li><li> <p>损失函数<br> (y_predict - y_true)平方和/总数<br> 逻辑回归的真实值/预测值 是否属于某个类别</p> </li><li> <p>对数似然损失<br> log 2 x<br> <img src="https://images2.imgbox.com/49/92/zZQdqZIt_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/65/61/uOxVIv8a_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/47/14/4Vhb4uE6_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/17/30/70kRSZIl_o.png" alt=" "><br> <img src="https://images2.imgbox.com/65/2a/HvejOr4E_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/25/75/sIqwGzJg_o.png" alt="在这里插入图片描述"></p> </li><li> <p>优化损失<br> <img src="https://images2.imgbox.com/65/a3/rWwArLxa_o.png" alt="在这里插入图片描述">梯度下降<br> <img src="https://images2.imgbox.com/b7/fa/pLikyH6P_o.png" alt=" "></p> </li></ul> 
<h4><a id="444__778"></a>4.4.4 案例：癌症分类预测-良／恶性乳腺癌肿瘤预测</h4> 
<p>恶性 - 正例<br> 流程分析：<br> 1）获取数据<br> 读取的时候加上names<br> 2）数据处理<br> 处理缺失值<br> 3）数据集划分<br> 4）特征工程：<br> 无量纲化处理-标准化<br> 5）逻辑回归预估器<br> 6）模型评估<br> 真的患癌症的，能够被检查出来的概率 - 召回率</p> 
<h4><a id="445__791"></a>4.4.5 分类的评估方法</h4> 
<ol><li>精确率与召回率<br> <img src="https://images2.imgbox.com/7f/70/DA1mrN3H_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/36/ab/1etZF4VZ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/af/55/itVgbBEt_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/47/d9/CCtsC6FJ_o.png" alt="在这里插入图片描述"></li></ol> 
<p>1 混淆矩阵<br> TP = True Possitive<br> FN = False Negative<br> 2 精确率(Precision)与召回率(Recall)<br> 精确率<br> 召回率 查得全不全<br> 工厂 质量检测 <strong>次品 召回率</strong><br> 3 F1-score 模型的稳健型<br> 总共有100个人，如果99个样本癌症，1个样本非癌症 - <strong>样本不均衡</strong><br> 不管怎样我全都预测正例(默认癌症为正例) - <strong>不负责任的模型</strong><br> 准确率：99%<br> 召回率：99/99 = 100%<br> 精确率：99%<br> F1-score: 2*99%/ 199% = 99.497%<br> AUC:0.5<br> TPR = 100%<br> FPR = 1 / 1 = 100%<br> <img src="https://images2.imgbox.com/30/24/izhNV5Ea_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/b4/3d/9eymPEB5_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/73/e4/rSnVYghb_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/6c/87/gTcX0r8D_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/29/2e/wgcXqbLM_o.png" alt="在这里插入图片描述"></p> 
<p>2 ROC曲线与AUC指标<br> 1 知道TPR与FPR<br> TPR = TP / (TP + FN) - 召回率<br> 所有真实类别为1的样本中，预测类别为1的比例<br> FPR = FP / (FP + TN)<br> 所有真实类别为0的样本中，预测类别为1的比例</p> 
<h3><a id="45__830"></a>4.5 模型保存和加载</h3> 
<h4><a id="451_sklearnAPI_832"></a>4.5.1 sklearn模型的保存和加载API</h4> 
<p><img src="https://images2.imgbox.com/a6/b1/gwipcX13_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="46_Kmeans_837"></a>4.6 无监督学习-K-means算法</h3> 
<h4><a id="461__838"></a>4.6.1 什么是无监督学习</h4> 
<p>没有目标值 - 无监督学习<br> <img src="https://images2.imgbox.com/be/b9/Q5wcEIb2_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="462__842"></a>4.6.2 无监督学习包含算法</h4> 
<ul><li>聚类<br> K-means(K均值聚类)</li><li>降维<br> PCA</li></ul> 
<h4><a id="463_Kmeans_847"></a>4.6.3 K-means原理</h4> 
<p><img src="https://images2.imgbox.com/de/24/kd52rnwE_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/0c/77/98t2A9oq_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/9b/4d/yn3av8PD_o.png" alt="在这里插入图片描述"><br> K-超参数<br> 1看需求<br> 2调节参数</p> 
<h4><a id="464_API_857"></a>4.6.4 API</h4> 
<p><img src="https://images2.imgbox.com/af/5f/iKWxXt1m_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="465_kmeansInstacart_Market_860"></a>4.6.5 案例：k-means对Instacart Market用户聚类</h4> 
<p>k = 3<br> 流程分析：<br> 降维之后的数据<br> 1）预估器流程<br> 2）看结果<br> 3）模型评估<br> <img src="https://images2.imgbox.com/22/64/7MuBh32j_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="466_Kmeans_870"></a>4.6.6 Kmeans性能评估指标</h4> 
<p>轮廓系数<br> 如果b_i&gt;&gt;a_i:趋近于1效果越好，<br> b_i&lt;&lt;a_i:趋近于-1，效果不好。<br> 轮廓系数的值是介于 [-1,1] ，<br> 越趋近于1代表内聚度和分离度都相对较优。<br> <img src="https://images2.imgbox.com/0f/89/hnTtAtOd_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ad/6f/WCv3QJgb_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/2a/f1/JOGdNyOB_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/7e/c0/k7vgSyMd_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="467_Kmeans_885"></a>4.6.7 K-means总结</h4> 
<p>应用场景：<br> 没有目标值<br> 分类<br> <img src="https://images2.imgbox.com/92/eb/guymTdtI_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ef6c9859bb9f12c021e1607c82b4c1e8/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Less和Scss</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c8543a3fb02be40edfd8504a3fd51476/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">17、生成长图，并上传至服务器</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>