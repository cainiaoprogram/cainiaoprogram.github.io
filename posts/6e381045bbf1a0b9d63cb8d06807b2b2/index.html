<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>二阶段目标检测总结 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="二阶段目标检测总结" />
<meta property="og:description" content="概述：
最新几年的论文都是在单阶段、Transform上进行发掘提升，基本上2020-2021年二阶段论文全军覆没，这篇博文也是总结2016-2019年的发展，最后一篇CenternetV2比较特殊，不能完全算作传统意义的二阶段网络。
目前什么地方还使用二阶段论文？
比赛场景，经常使用FasterRCNN的变种&#43;其它网络进行联合预测目标比较小的场景(使用较少，一般用anchor-free、增大输入去代替、分割图像检测)辅助一阶段使用，和(2)类似但不同。比如检测远距离人形&#43;人脸，方案一：先检测人，后检测人脸，两个都是OneStage。方案二：使用TwoStage网络，第一阶段检测人形，第二阶段检测人脸。当然只有第二类别是第一类别的从属，且是唯一关系才能进行！ 一. FasterRCNN 以torchvision给出的demo为例子：
第一阶段，和基础SSD等一阶段操作类似，但是这一阶段会立刻输出候选区域 https://github.com/pytorch/vision/blob/183a722169421c83638e68ee2d8fc5bd3415c4b4/torchvision/models/detection/rpn.py#L29
将候选区域从FPN输出的feature上抠出来
使用ROIPooling对齐候选特征的大小
https://github.com/pytorch/vision/blob/183a722169421c83638e68ee2d8fc5bd3415c4b4/torchvision/ops/poolers.py#L83-L277
第二阶段，直接进行FC细化Reg/Cls，当然这里只能是一个候选区域最多一个目标。 class FastRCNNPredictor(nn.Module): &#34;&#34;&#34; Standard classification &#43; bounding box regression layers for Fast R-CNN. Args: in_channels (int): number of input channels num_classes (int): number of output classes (including background) &#34;&#34;&#34; def __init__(self, in_channels, num_classes): super(FastRCNNPredictor, self).__init__() self.cls_score = nn.Linear(in_channels, num_classes) self.bbox_pred = nn.Linear(in_channels, num_classes * 4) def forward(self, x): if x.dim() == 4: assert list(x.shape[2:]) == [1, 1] x = x." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/6e381045bbf1a0b9d63cb8d06807b2b2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-06-28T17:24:00+08:00" />
<meta property="article:modified_time" content="2021-06-28T17:24:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">二阶段目标检测总结</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown"> 
 <p><strong>概述：</strong></p> 
 <blockquote> 
  <p>最新几年的论文都是在<strong>单阶段</strong>、<strong>Transform</strong>上进行发掘提升，基本上2020-2021年二阶段论文全军覆没，这篇博文也是总结2016-2019年的发展，最后一篇CenternetV2比较特殊，不能完全算作传统意义的二阶段网络。</p> 
  <p>目前什么地方还使用二阶段论文？</p> 
  <ol><li>比赛场景，经常使用FasterRCNN的变种+其它网络进行联合预测</li><li>目标比较小的场景(使用较少，一般用anchor-free、增大输入去代替、分割图像检测)</li><li>辅助一阶段使用，和(2)类似但不同。比如检测远距离人形+人脸，方案一：先检测人，后检测人脸，两个都是OneStage。方案二：使用TwoStage网络，第一阶段检测人形，第二阶段检测人脸。当然只有第二类别是第一类别的从属，且是唯一关系才能进行！</li></ol> 
 </blockquote> 
 <h2 id="一-fasterrcnn">一. FasterRCNN</h2> 
 <p>以torchvision给出的demo为例子：</p> 
 <ol><li>第一阶段，和基础SSD等一阶段操作类似，但是这一阶段会立刻输出<strong>候选区域</strong></li></ol> 
 <p><a href="https://github.com/pytorch/vision/blob/183a722169421c83638e68ee2d8fc5bd3415c4b4/torchvision/models/detection/rpn.py#L29" target="_blank" rel="noopener noreferrer">https://github.com/pytorch/vision/blob/183a722169421c83638e68ee2d8fc5bd3415c4b4/torchvision/models/detection/rpn.py#L29</a></p> 
 <ol start="2"><li> <p>将候选区域从FPN输出的feature上抠出来</p> </li><li> <p>使用ROIPooling对齐候选特征的大小</p> </li></ol> 
 <p><a href="https://github.com/pytorch/vision/blob/183a722169421c83638e68ee2d8fc5bd3415c4b4/torchvision/ops/poolers.py#L83-L277" target="_blank" rel="noopener noreferrer">https://github.com/pytorch/vision/blob/183a722169421c83638e68ee2d8fc5bd3415c4b4/torchvision/ops/poolers.py#L83-L277</a></p> 
 <ol start="4"><li>第二阶段，直接进行FC细化Reg/Cls，当然这里只能是一个候选区域最多一个目标。</li></ol> 
 <pre class="has"><code class="language-python">class FastRCNNPredictor(nn.Module):
    """
    Standard classification + bounding box regression layers
    for Fast R-CNN.
    Args:
        in_channels (int): number of input channels
        num_classes (int): number of output classes (including background)
    """

    def __init__(self, in_channels, num_classes):
        super(FastRCNNPredictor, self).__init__()
        self.cls_score = nn.Linear(in_channels, num_classes)
        self.bbox_pred = nn.Linear(in_channels, num_classes * 4)

    def forward(self, x):
        if x.dim() == 4:
            assert list(x.shape[2:]) == [1, 1]
        x = x.flatten(start_dim=1)
        scores = self.cls_score(x)
        bbox_deltas = self.bbox_pred(x)

        return scores, bbox_deltas</code></pre> 
 <div style="text-align: center;"> 
  <img src="https://images2.imgbox.com/4a/46/hndwP6Ws_o.png" alt="image-20210624170757195" style="outline: none;"> 
 </div> 
 <p><strong>注释：</strong></p> 
 <ul><li>最后一步FC是参考anchor的做法，首先想到的是<span class="math inline">\(self.bbox\_pred = nn.Linear(in\_channels,4)\)</span> ，因为已经使用class进行了过滤，没有必要再把regression去使用class再去过滤一遍。当然使用class对回归进行区分，这效果肯定优于单个回归。</li><li>我们再进一步延伸，如果在不同的class之下，再使用一种手段(长宽、面积、anchor等)对其进一步划分，比如：假设候选区域数量不变为 <span class="math inline">\(P\)</span>，类别为两类(人形、人脸)，进一步使用anchor限制(两个anchor，5和20，比例4倍以内使用5，超过4倍使用20)，这样会不会更精细？</li><li>上一步我们限制了候选区域数量，能不能使用输入多个不同组的候选区域，后面连接多个不同的predict？这就是后续改进cascadeRCNN的由来。</li></ul> 
 <h2 id="二-maskrcnn">二. MaskRCNN</h2> 
 <ol><li>第一阶段使用FasterRCNN，RPN网络都相同</li><li>第二阶段输出多一个分支 <span class="math inline">\(K\times m\times m\)</span> , 其中 <span class="math inline">\(K\)</span> 表示种类，<span class="math inline">\(m\)</span> 表示输出分辨率</li></ol> 
 <p><strong>注意：</strong> 最后输出的mask大小是固定的，设置大小得根据实际种类而定。这个mask分支和FCN有点区别，这里使用K个feature，然后直接进行二分类操作，而FCN使用单个feature进行多分类操作，目前检测的分类loss都是进行单独的二分类操作。</p> 
 <div style="text-align: center;"> 
  <img src="https://images2.imgbox.com/a7/7a/ipWZKHlr_o.png" alt="image-20210624175016872" style="outline: none;"> 
 </div> 
 <h2 id="三-cascadercnn">三. CascadeRCNN</h2> 
 <ol><li> <p>第一阶段和FasterRCNN完全一样</p> </li><li> <p>第二阶段使用多个RoiHead层进行级联</p> </li></ol> 
 <p>下图完全显示了CascadeRCNN的由来</p> 
 <ul><li>下图(C)仅仅在前向计算的时候使用级联finetune操作，精度也得到一定提升，但是有两个缺陷。1)使用共享的Head-H1，不仅时间没降低，而且参数效果还不好。所以在端侧单阶段目标检测中，一般不使用共享的头，虽然参数减少了，但是计算量一点未变，所以都使用不共享的头。2)都是固定的，不能进行训练，后面的阶段已经处于过拟合状态。</li><li>下图(d)最大的缺点就是候选区域固定，非常容易过拟合。有一个优点，后面的stage仅仅进行分类而不进行reg，这是cascade未考虑的。但是，分类分支花费的代价很小，基本影响不到大局，所以讨论的人很少。</li><li>下图(b)融合了两者的优点，1)可训练。2)不共享。3)候选区域不同。</li></ul> 
 <div style="text-align: center;"> 
  <img src="https://images2.imgbox.com/87/02/iJDEL8zx_o.png" alt="image-20210624175552697" style="outline: none;"> 
 </div> 
 <h2 id="四-libra-rcnn">四. Libra RCNN</h2> 
 <p>未改变实际的流程结构，文章从<strong>均衡</strong>的角度对各个模块进行改进。</p> 
 <ol><li><span class="math inline">\(feature\)</span> 均衡，使用FPN、PAN、BiFPN进行互相链接</li><li>采样均衡，原始SSD分配使用Max-IOU，采样使用Random-Sample</li><li>loss均衡，原始SSD直接使用独立的分支进行计算</li></ol> 
 <p>IOU-Balance采样，下图是困难负样本(<span class="math inline">\(Iou&lt;0.5\)</span> 称为困难负样本)的Iou分布，其中64%样本<span class="math inline">\(Iou&gt;0.05\)</span>，36%的样本<span class="math inline">\(Iou&lt;0.05\)</span>，注意这里是百分比图，不是个数量图！！！下图中使用随机采样，会导致70%的样本<span class="math inline">\(Iou&lt;0.05\)</span> ，这明显是不符合下图的困难负样本的分布图的。如何将下附图的每个方格采样比例趋于平衡？很明显的想到还是使用方格将IOU进行区域划分(直方图的表示方法)，然后按照一定比例进行采样即可。论文给出一些控制参数，使得表达更为通俗易懂。</p> 
 <div style="text-align: center;"> 
  <img src="https://images2.imgbox.com/37/a4/RPD2r5Pj_o.png" alt="image-20210628102008413" style="outline: none;"> 
 </div> 
 <p>关于feature部分的平衡，这部分有点玄，意义不大。。。</p> 
 <p>关于loss的平衡，主要对smooth-L1进行调整，文章分析了loss的贡献率，<span class="math inline">\(loss&lt;1.0\)</span> 贡献30%，相反贡献70%，既然要平衡，那就要提升 <span class="math inline">\(loss&lt;1.0\)</span> 的梯度(loss占比)，其实和focal-loss一样，都是提升困难样本的比重。</p> 
 <div class="math display">
   \[L_{b}(x)=\left\{\begin{array}{ll} \frac{\alpha}{b}(b|x|+1) \ln (b|x|+1)-\alpha|x| &amp; \text { if }|x|&lt;1 \\ \gamma|x|+C &amp; \text { otherwise } \end{array}\right. \] 
 </div> 
 <div style="text-align: center;"> 
  <img src="https://images2.imgbox.com/c8/ca/Qt9ckpXQ_o.png" alt="image-20210628105740276" style="outline: none;"> 
 </div> 
 <h2 id="五-gridrcnn">五. GridRCNN</h2> 
 <ol><li>第一阶段和FasterRCNN完全一样</li><li>分类分支相同，回归分支使用关键点</li></ol> 
 <p>从当前的角度看，这篇论文较为简单，属于简化版的anchor-free模型，因为是对候选区域进行操作，不需要anchor的匹配，不需要进行nms等操作。和top-bottom的pose估计一模一样！！</p> 
 <p>如果把第一阶段也使用关键点去估计，那这篇二阶段文章就完全是anchor-free+pose的文章了。</p> 
 <div style="text-align: center;"> 
  <img src="https://images2.imgbox.com/05/95/054xFYOg_o.png" alt="image-20210628114513211" style="outline: none;"> 
 </div> 
 <p>需要注意一点，经常出现候选区域把目标截断(二阶段网络都存在的问题)，那关键点就不存在(使用0表示)，但是对于定位就很不准确。最简单的方式是直接扩大候选区域的截图范围，作者尝试这种方案效果不理想。作者采用候选区域不变，仅对候选回归框做扩大。如下图所示，<strong>绿色</strong>是目标框，<strong>白色实线</strong>是候选框，输入<strong>白色实线</strong>候选区域，在<strong>白色虚线</strong>的基础上进行回归。</p> 
 <div style="text-align: center;"> 
  <img src="https://images2.imgbox.com/d7/1e/P0leVZ8l_o.png" alt="image-20210628114708997" style="outline: none;"> 
 </div> 
 <h2 id="六-gridrcnn-v2">六. GridRCNN-V2</h2> 
 <ol><li>第一阶段和FasterRCNN完全一样</li><li>第二阶段和GridRCNN-V1基本相同</li></ol> 
 <p>这篇论文主要是对第一版本的速度进行改进：</p> 
 <ul><li>keypoint branch 从<span class="math inline">\(56\times56\)</span>降低到<span class="math inline">\(28 \times 28\)</span> ，论文说这样做不仅会加快速度，而且还会提高精度。<u>之前在关键点上试验，使用期望定位会有精度损失。</u></li><li>减小候选区域的大小，降低channels</li><li>联合整个batch图像进行计算。之前使用for循环，单次计算单张图像，对于目标数量非常敏感，现在是对batch进行一次计算。</li><li>NMS仅进行一次</li></ul> 
 <div style="text-align: center;"> 
  <img src="https://images2.imgbox.com/aa/1f/TgpwyWOQ_o.png" alt="image-20210628143308421" style="outline: none;"> 
 </div> 
 <h2 id="七-double-head-rcnn">七. Double-Head RCNN</h2> 
 <ol><li>第一阶段和FasterRCNN完全一样</li><li>第二阶段ROI-Head使用多个头进行预测</li></ol> 
 <p>此想法类似多模型W联合预测WBF，使用全连接头+卷积头联合预测，唯一注意的是cls分支使用联合loss，而reg分支使用单独预测。</p> 
 <div class="math display">
   \[s=s^{f c}+s^{c o n v}\left(1-s^{f c}\right)=s^{c o n v}+s^{f c}\left(1-s^{c o n v}\right), \] 
 </div> 
 <p>利用联合预测效果绝对有所提升，但是额外的分支预测占用较多的资源，如何做到Trade-Off是关键。</p> 
 <div style="text-align: center;"> 
  <img src="https://images2.imgbox.com/fc/ee/x4qx23BV_o.png" alt="image-20210628192146045" style="outline: none;"> 
 </div> 
 <h2 id="八-centernetv2">八. CenterNetV2</h2> 
 <p>在刚开始的概述(3)中，表达了一种二阶段网络的特殊应用，这篇文章将此应用表述成一种方案：我们可以使用任何一阶段的网络作为RPN层(包括anchor-based、anchor-free操作)，后面ROI阶段可以使用任何之前的第二阶段网络。这不就是集合前人的大杂烩？<strong>搞不懂为啥没有使用YOLOV4作为第一阶段进行比较</strong>？</p> 
 <div style="text-align: center;"> 
  <img src="https://images2.imgbox.com/e6/e4/HZNDS0A0_o.png" alt="image-20210628171112266" style="outline: none;"> 
 </div> 
 <p>进一步引申：</p> 
 <ul><li>类似人形+人脸的检测组合(概述3)</li><li>关键点检测模型(Top-Bottom)，比如人体关键点估计，可以直接使用二阶段网络。</li></ul> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/17cd1ef0f53a1994a13fdbc6f25be9c4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">SQL Server数据库关于bak类型文件的备份与还原</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/5121d5b57c1dedda59dbc2cb3fd6ded0/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">如何利用计算机隐藏文件,怎么把隐藏的文件夹显示出来 多种方法介绍【图文教程】...</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>