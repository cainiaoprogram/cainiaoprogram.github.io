<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>What is the Softmax Function?详解机器学习中的Softmax函数【小白菜可懂】 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="What is the Softmax Function?详解机器学习中的Softmax函数【小白菜可懂】" />
<meta property="og:description" content="目录
定义
公式
计算
Softmax vs Sigmoid
Softmax vs Sigmoid 计算
Softmax vs Argmax
Softmax vs Argmax 计算
应用
神经网络中的Softmax函数
神经网络中的Softmax计算示例
强化学习中的Softmax函数
强化学习中的Softmax计算示例
历史
参考
定义 softmax函数是把K个实值转换为另外K个实值并使K个实值之和为1的函数。输入数值可以是正数/负数/0或者几种的组合，经过softmax函数转换为[0，1]之间的数值，以便于用来描述概率分布。如果一个输入数值很小或负数，softmax函数则将它转换为小的概率值；如果一个输入数值很大，softmax函数则将它转换为大的概率值，每一个转换后的数值都将保持在[0,1]的范围之内。
Softmax函数有时被称为Softargmax函数，或multi-class logistic regression（多类别逻辑回归函数）。这是因为softmax函数是逻辑回归函数的一般化形式，可用来作多种类分类任务，它的公式与常用的逻辑回归sigmoid函数非常相似。Softmax函数这种分类器只能用在类别相互独立不相关的情况下。
很多多层神经网络在倒数第二层结束时输出的结果不便于规整，很难处理。在此，softmax函数正好派上用场，因为它可以把输出的结果数值归一化到概率的分布上，可以展示给用户或者作为输入传给其他系统。因此经常在神经网络的最后一层加上softmax函数。
公式 softmax函数公式如下：
Mathematical definition of the softmax function
所有的zi值是输入vector的元素，可以是任意实值。下面各项是归一化项来保证输出数值之和等于1，也就构成了一个有效的符合逻辑的概率分布。
【Softmax公式符号解释】
输入vector，由（）组成有的zi值是输入vector的元素，可以是任意实值。例如一个神经网络有一个（-0.62,8.12,2.53）的vector输出，这不是一个有效的符合逻辑的概率分布，这正是softmax的用武之地。标准的指数函数被用于输入vector的每一个元素，它输出一个大于0的正值，如果输入负值，则输出结果小，如果输入大正值，则输出结果大。然而，输出结果不会被限定于（0，1）之间，不符合概率分布的要求。公式的分母项是归一化项，它保证函数输出结果值之和为1，且每个输出结果在（0，1）之间，因此构成一个有效的符合逻辑的概率分布。多类别分类器的类别数量。 计算 假设有一个包含三个实值的数组。这些值可能是机器学习模型（神经网络模型）的输出。我们想把这些值转换为概率分布。
首先计算数组每个元素的指数函数值。这是softmax等式的分子项。
这些值还不像是概率值。注意输入元素，尽管8只比5大一点，2981却比148大很多，这是因为指数函数作用。我们能够归一化它们，通过上述三个指数运算后结果的求和来作为softmax等式的分母。
我们看到归一化项主要受z1的影响。
最后，除以归一化项，我们得到了每个元素的softmax输出数值。注意这不是单个的输出数值，而是softmax函数将一个数组转换为同长度的数组，在此长度为3。
经我们检验，三个输出值都是有效的符合逻辑的概率值，它们处于（0，1）之间，且求和为1。
由于指数运算，第一个元素8，softmax函数放大了它的影响力，同时压缩5和0的输出值，降到很小的概率值。
如果你在机器学习模型中使用softmax函数，在解释它为真实概率值之前要谨慎，因为它有一个产生近似于0或1数值的偏向趋势。如果神经网络有一个[8,5,0]输出值，如示例所示，softmax函数将会赋予95%的概率值给第一个类别，但这在实际神经网络预测中存在不确定性。这将会导致神经网络预测有一个很高的置信度，但是预测的结果却不是真实的结果。
Softmax vs Sigmoid 如上所述，softmax函数与sigmoid函数是相似的。softmax运算的是vector，sigmoid运算的是标量。
实际上，sigmoid函数是softmax函数的一种特殊形式，是一种二分类的分类器。我们看到，如果我们设置输入vector为[x,0]，用softmax公式计算第一个元素的输出数值：
分式上下同除以，可得：
可见，当二分类时，sigmoid函数与softmax函数是相等的。计算vector第二个元素的数值是不必要的，因为只有两个概率值且它们和为1。故而，我们用逻辑回归开发一个两个类别的分类器，我们可以使用sigmoid函数，不用去处理vectors。但是，超过两个的/相互独立不相关的类别时须使用softmax函数。
如果超过两个类别但是它们不是相互独立的类别（多标签分类器，举个🌰：一辆骚红色的/敞篷的轿车，打了red/open-body/car三个标签，包含了颜色/框架结构/功能类别三个属性，三个属性至之间的类别便不是相互独立的，可以交叉相互从属）时，分类器必须分解为多个二元分类器，每种使用独有的sigmoid函数。
Softmax vs Sigmoid 计算 我们设定输入vector为[3,0]，将其代入softmax和sigmoid函数。尽管sigmoid函数输入的是标量，我们可以只选择vector第一个元素代入sigmoid函数。
当vector第二个元素被设定为0时，对于vector第一个元素，sigmoid函数和softmax函数给出了相同的数值。因为sigmoid给出的是概率值且两个概率值之和必须是1，所以计算vector的第二个元素的输出值不是必要的。
Softmax vs Argmax 相较于argmax函数，softmax函数被用于是一种平滑的可微的选择。因此softmax函数有时候被更明确地称之为softargmax函数。和softmax函数一样，argmax函数进行vector运算，转换每一个数值到0，除了最大值之外，最大值被转换为1。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/86e86d8faf4fb5ca1390693a8d7cb8bd/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-12-28T15:40:47+08:00" />
<meta property="article:modified_time" content="2021-12-28T15:40:47+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">What is the Softmax Function?详解机器学习中的Softmax函数【小白菜可懂】</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2 id="%E5%AE%9A%E4%B9%89"><a id="_0"></a></h2> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="%E5%AE%9A%E4%B9%89-toc" style="margin-left:0px;"><a href="#%E5%AE%9A%E4%B9%89" rel="nofollow" title="定义">定义</a></p> 
<p id="%E5%85%AC%E5%BC%8F-toc" style="margin-left:0px;"><a href="#%E5%85%AC%E5%BC%8F" rel="nofollow" title="公式">公式</a></p> 
<p id="%E8%AE%A1%E7%AE%97-toc" style="margin-left:0px;"><a href="#%E8%AE%A1%E7%AE%97" rel="nofollow" title="计算">计算</a></p> 
<p id="Softmax%20vs%20Sigmoid-toc" style="margin-left:0px;"><a href="#Softmax%20vs%20Sigmoid" rel="nofollow" title="Softmax vs Sigmoid">Softmax vs Sigmoid</a></p> 
<p id="Softmax%20vs%20Sigmoid%20%E8%AE%A1%E7%AE%97-toc" style="margin-left:40px;"><a href="#Softmax%20vs%20Sigmoid%20%E8%AE%A1%E7%AE%97" rel="nofollow" title="Softmax vs Sigmoid 计算">Softmax vs Sigmoid 计算</a></p> 
<p id="Softmax%20vs%20Argmax-toc" style="margin-left:0px;"><a href="#Softmax%20vs%20Argmax" rel="nofollow" title="Softmax vs Argmax">Softmax vs Argmax</a></p> 
<p id="Softmax%20vs%20Argmax%20%E8%AE%A1%E7%AE%97-toc" style="margin-left:40px;"><a href="#Softmax%20vs%20Argmax%20%E8%AE%A1%E7%AE%97" rel="nofollow" title="Softmax vs Argmax 计算">Softmax vs Argmax 计算</a></p> 
<p id="%E5%BA%94%E7%94%A8-toc" style="margin-left:0px;"><a href="#%E5%BA%94%E7%94%A8" rel="nofollow" title="应用">应用</a></p> 
<p id="%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84Softmax%E5%87%BD%E6%95%B0-toc" style="margin-left:40px;"><a href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84Softmax%E5%87%BD%E6%95%B0" rel="nofollow" title="神经网络中的Softmax函数">神经网络中的Softmax函数</a></p> 
<p id="%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84Softmax%E8%AE%A1%E7%AE%97%E7%A4%BA%E4%BE%8B-toc" style="margin-left:40px;"><a href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84Softmax%E8%AE%A1%E7%AE%97%E7%A4%BA%E4%BE%8B" rel="nofollow" title="神经网络中的Softmax计算示例">神经网络中的Softmax计算示例</a></p> 
<p id="%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84Softmax%E5%87%BD%E6%95%B0-toc" style="margin-left:40px;"><a href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84Softmax%E5%87%BD%E6%95%B0" rel="nofollow" title="强化学习中的Softmax函数">强化学习中的Softmax函数</a></p> 
<p id="%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84Softmax%E8%AE%A1%E7%AE%97%E7%A4%BA%E4%BE%8B-toc" style="margin-left:40px;"><a href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84Softmax%E8%AE%A1%E7%AE%97%E7%A4%BA%E4%BE%8B" rel="nofollow" title="强化学习中的Softmax计算示例">强化学习中的Softmax计算示例</a></p> 
<p id="%E5%8E%86%E5%8F%B2-toc" style="margin-left:0px;"><a href="#%E5%8E%86%E5%8F%B2" rel="nofollow" title="历史">历史</a></p> 
<p id="%E5%8F%82%E8%80%83-toc" style="margin-left:0px;"><a href="#%E5%8F%82%E8%80%83" rel="nofollow" title="参考">参考</a></p> 
<hr id="hr-toc"> 
<h2>定义</h2> 
<p><span style="color:#fe2c24;">softmax函数是把K个实值转换为另外K个实值并使K个实值之和为1的函数。</span>输入数值可以是正数/负数/0或者几种的组合，经过softmax函数转换为[0，1]之间的数值，以便于用来描述概率分布。如果一个输入数值很小或负数，softmax函数则将它转换为小的概率值；如果一个输入数值很大，softmax函数则将它转换为大的概率值，<span style="color:#fe2c24;">每一个转换后的数值都将保持在[0,1]的范围之内</span>。<br> Softmax函数有时被称为Softargmax函数，或multi-class logistic regression（多类别逻辑回归函数）。这是因为softmax函数是逻辑回归函数的一般化形式，可用来作多种类分类任务，它的公式与常用的逻辑回归sigmoid函数非常相似。Softmax函数这种分类器只能用在类别相互独立不相关的情况下。</p> 
<p>很多多层神经网络在倒数第二层结束时输出的结果不便于规整，很难处理。在此，softmax函数正好派上用场，因为它可以把输出的结果数值归一化到概率的分布上，可以展示给用户或者作为输入传给其他系统。因此经常在神经网络的最后一层加上softmax函数。</p> 
<hr> 
<h2 id="%E5%85%AC%E5%BC%8F"><a id="_6"></a><br><br> 公式</h2> 
<p><strong>softmax函数公式如下：</strong></p> 
<p><em>Mathematical definition of the softmax function</em></p> 
<p><img alt="\sigma \left ( \vec{z} \right )_{i}= \frac{e^{z_{i}}}{\sum_{j=1}^{K}e^{z_{i}}}" class="mathcode" src="https://images2.imgbox.com/f1/79/nUidMscy_o.png"></p> 
<p>所有的zi值是输入vector的元素，可以是任意实值。下面各项是归一化项来保证输出数值之和等于1，也就构成了一个有效的符合逻辑的概率分布。</p> 
<p><strong>【Softmax公式符号解释】</strong></p> 
<table align="center" border="1" cellpadding="1" cellspacing="1" style="width:500px;"><tbody><tr><td><img alt="\vec{z}" class="mathcode" src="https://images2.imgbox.com/51/c5/QIDH32ll_o.png"></td><td>输入vector，由（<img alt="z_{0},z_{1},...z_{K}" class="mathcode" src="https://images2.imgbox.com/b4/9d/Hzp27nlE_o.png">）组成</td></tr><tr><td><img alt="z_{i}" class="mathcode" src="https://images2.imgbox.com/33/b1/RgtlsGog_o.png"></td><td>有的zi值是输入vector的元素，可以是任意实值。例如一个神经网络有一个（-0.62,8.12,2.53）的vector输出，这不是一个有效的符合逻辑的概率分布，这正是softmax的用武之地。</td></tr><tr><td><img alt="e^{z_{i}}" class="mathcode" src="https://images2.imgbox.com/f9/a9/zKEPSrpH_o.png"></td><td>标准的指数函数被用于输入vector的每一个元素，它输出一个大于0的正值，如果输入负值，则输出结果小，如果输入大正值，则输出结果大。然而，输出结果不会被限定于（0，1）之间，不符合概率分布的要求。</td></tr><tr><td><img alt="\sum_{j=1}^{K}e^{z_{i}}" class="mathcode" src="https://images2.imgbox.com/df/01/stLlF0cK_o.png"></td><td>公式的分母项是归一化项，它保证函数输出结果值之和为1，且每个输出结果在（0，1）之间，因此构成一个有效的符合逻辑的概率分布。</td></tr><tr><td><img alt="K" class="mathcode" src="https://images2.imgbox.com/59/7d/fFDBUiYI_o.png"></td><td>多类别分类器的类别数量。</td></tr></tbody></table> 
<hr> 
<h2 id="%E8%AE%A1%E7%AE%97"><a id="_16"></a><br><br> 计算</h2> 
<p>假设有一个包含三个实值的数组。这些值可能是机器学习模型（神经网络模型）的输出。我们想把这些值转换为概率分布。</p> 
<p><img alt="\begin{bmatrix} 8\\ 5\\ 0 \end{bmatrix}" class="mathcode" src="https://images2.imgbox.com/92/76/bdmHtKFX_o.png"><br> 首先计算数组每个元素的指数函数值。这是softmax等式的分子项。</p> 
<p><img alt="\left\{\begin{matrix} e^{z_{1}}=e^{8}=2981.0\\ e^{z_{2}}=e^{5}=148.4\\ e^{z_{3}}=e^{0}=1.0 \end{matrix}\right." class="mathcode" src="https://images2.imgbox.com/3c/22/3jLcIyJO_o.png"></p> 
<p>这些值还不像是概率值。注意输入元素，尽管8只比5大一点，2981却比148大很多，这是因为指数函数作用。我们能够归一化它们，通过上述三个指数运算后结果的求和来作为softmax等式的分母。</p> 
<p><img alt="\sum_{j=1}^{K}e^{z_{i}}=e^{z_{1}}+e^{z_{2}}+e^{z_{3}}=2981.0+148.4+1.0=3130.4" class="mathcode" src="https://images2.imgbox.com/44/1d/8Ytq841L_o.png"></p> 
<p>我们看到归一化项主要受z1的影响。</p> 
<p>最后，除以归一化项，我们得到了每个元素的softmax输出数值。注意这不是单个的输出数值，而是softmax函数将一个数组转换为同长度的数组，在此长度为3。</p> 
<p><img alt="\left\{\begin{matrix} \sigma (\vec{z})1=\frac{2981.0}{3130.4}=0.9523\\ \sigma (\vec{z})2=\frac{148.4}{3130.4}=0.0474\\ \sigma (\vec{z})3=\frac{1.0}{3130.4}=0.0003 \end{matrix}\right." class="mathcode" src="https://images2.imgbox.com/8a/59/stMPidhg_o.png"></p> 
<p>经我们检验，三个输出值都是有效的符合逻辑的概率值，它们处于（0，1）之间，且求和为1。</p> 
<p>由于指数运算，第一个元素8，softmax函数放大了它的影响力，同时压缩5和0的输出值，降到很小的概率值。</p> 
<p>如果你在机器学习模型中使用softmax函数，在解释它为真实概率值之前要谨慎，因为它有一个产生近似于0或1数值的偏向趋势。如果神经网络有一个[8,5,0]输出值，如示例所示，softmax函数将会赋予95%的概率值给第一个类别，但这在实际神经网络预测中存在不确定性。这将会导致神经网络预测有一个很高的置信度，但是预测的结果却不是真实的结果。</p> 
<hr> 
<h2 id="Softmax%20vs%20Sigmoid"><a id="_25"></a><br><br> Softmax vs Sigmoid</h2> 
<p>如上所述，softmax函数与sigmoid函数是相似的。softmax运算的是vector，sigmoid运算的是标量。</p> 
<p><img alt="\begin{matrix} softmax: \sigma\left ( \vec{z} \right )i=\frac{e^{z_{i}}}{\sum_{j=1}^{K}e^{z_{i}}}\\ sigmoid: S(x)=\frac{1}{1+e^{-x}} \end{matrix}" class="mathcode" src="https://images2.imgbox.com/24/89/C6EkiPdd_o.png"><br> 实际上，sigmoid函数是softmax函数的一种特殊形式，是一种二分类的分类器。我们看到，如果我们设置输入vector为[x,0]，用softmax公式计算第一个元素的输出数值：</p> 
<p><img alt="\sigma (\vec{z})_{1}=\frac{e^{z_{1}}}{e^{z_{1}}+e^{z_{2}}}=\frac{e^{x}}{e^{x}+e^{0}}=\frac{e^{x}}{e^{x}+1}" class="mathcode" src="https://images2.imgbox.com/ea/79/ipDXq2PN_o.png"></p> 
<p>分式上下同除以<img alt="e^{x}" class="mathcode" src="https://images2.imgbox.com/82/c5/TYzXWTp3_o.png">，可得：</p> 
<p><img alt="\sigma (\vec{z})_{1}=\frac{1}{1+e^{-x}}" class="mathcode" src="https://images2.imgbox.com/80/d6/2OIykTIx_o.png"></p> 
<p>可见，当二分类时，sigmoid函数与softmax函数是相等的。计算vector第二个元素的数值是不必要的，因为只有两个概率值且它们和为1。故而，我们用逻辑回归开发一个两个类别的分类器，我们可以使用sigmoid函数，不用去处理vectors。但是，超过两个的/相互独立不相关的类别时须使用softmax函数。</p> 
<p>如果超过两个类别但是它们不是相互独立的类别（多标签分类器，举个🌰：一辆骚红色的/敞篷的轿车，打了red/open-body/car三个标签，包含了颜色/框架结构/功能类别三个属性，三个属性至之间的类别便不是相互独立的，可以交叉相互从属）时，分类器必须分解为多个二元分类器，每种使用独有的sigmoid函数。</p> 
<h3 id="Softmax%20vs%20Sigmoid%20%E8%AE%A1%E7%AE%97">Softmax vs Sigmoid 计算</h3> 
<p>我们设定输入vector为[3,0]，将其代入softmax和sigmoid函数。尽管sigmoid函数输入的是标量，我们可以只选择vector第一个元素代入sigmoid函数。</p> 
<p><img alt="\begin{matrix} Softmax\\ \sigma (\vec{z})_{1}=\sigma (3)=\frac{e^{3}}{e^{3}+e^{0}}=0.953\\ \sigma (\vec{z})_{2}=\sigma (0)=\frac{e^{0}}{e^{3}+e^{0}}=0.0474\\ Sigmoid\\ S(x)=S(3)=\frac{1}{1+e^{-3}}=0.953 \end{matrix}" class="mathcode" src="https://images2.imgbox.com/d8/01/QnBwzZYr_o.png"></p> 
<p>当vector第二个元素被设定为0时，对于vector第一个元素，sigmoid函数和softmax函数给出了相同的数值。因为sigmoid给出的是概率值且两个概率值之和必须是1，所以计算vector的第二个元素的输出值不是必要的。</p> 
<h2 id="Softmax%20vs%20Argmax">Softmax vs Argmax</h2> 
<p>相较于argmax函数，softmax函数被用于是一种平滑的可微的选择。因此softmax函数有时候被更明确地称之为softargmax函数。和softmax函数一样，argmax函数进行vector运算，转换每一个数值到0，除了最大值之外，最大值被转换为1。</p> 
<p><span style="color:#999aaa;"><img alt="argmax\left ( \begin{bmatrix} z_{1}\\ z_{2}\\ ...\\ z_{n} \end{bmatrix} \right )=\begin{bmatrix} y_{1}\\ y_{2}\\ ...\\ y_{n} \end{bmatrix}=\begin{bmatrix} 0\\ ...\\ 1\\ ...\\ 0 \end{bmatrix}" class="mathcode" src="https://images2.imgbox.com/1c/07/hY7W8Dhh_o.png"></span><br>如果zi是vector z唯一的最大值，则yi=1。</p> 
<p>通常，训练一个机器学习模型时使用softmax，但是利用模型推断时关闭argmax层的softmax层。</p> 
<p>训练时必须使用softmax，因为softmax是连续可微的，允许我们去优化损失函数。然而，推断时有时候我们需要模型输出单独的预测值而非概率值，这种情况下argmax函数更有用。</p> 
<p>当有多个最大值时，通常argmax返回1/Nmax，这是一种归一化结构，以便于保证输出元素之和为1，这点与softmax一致。另外一种可选择的定义是，所有最大值都返回为1，或者只返回第一个为1。 </p> 
<h3 id="Softmax%20vs%20Argmax%20%E8%AE%A1%E7%AE%97">Softmax vs Argmax 计算</h3> 
<p>翻译家让我们再次设定输入vector为[3,0]，softmax计算同上。最大值是第一个元素，故而argmax第一个元素的返回值为1，其他为0。</p> 
<p><img alt="\begin{matrix} Argmax\\ argmax(\vec{z})=argmax(\begin{bmatrix} 3\\ 0 \end{bmatrix})=\begin{bmatrix} 1\\ 0 \end{bmatrix}\\ argmax(\vec{z})_{1}=1\\ argmax(\vec{z})_{2}=0 \end{matrix}" class="mathcode" src="https://images2.imgbox.com/74/4b/eRLoPmtB_o.png"></p> 
<p>从🌰中可以清楚地看到，softmax像是“软化”了的argmax的估算值：它返回的是[0,1]之间的非整数数值，可作为有效的符合逻辑的概率值。如果我们用机器学习模型来作推断，而不是训练它，那么我们可能希望得到一个整数输出结果，这个结果是系统代替我们经过一个困难的决策后作出的，这类决策任务可以是诊断肿瘤/确认版权/签署文件等。这种情况下argmax输出的数值更容易处理，可以用来建立混淆矩阵和计算分类器的准确度和召回率。</p> 
<hr> 
<h2 id="%E5%BA%94%E7%94%A8">应用</h2> 
<h3 id="%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84Softmax%E5%87%BD%E6%95%B0">神经网络中的Softmax函数</h3> 
<p>softmax函数一种用途是放在神经网络的最后一层。举个🌰，一个识别猫狗的CNN，要求一张图像里要么是😺要么是🐶，不能同时包含猫狗，因此这两个类别是相互独立不相关的。一般的，网络最后的全连接层将会产生诸如[-7.98,2.39]这样的vector数值，尚未归一化，不是有效的符合逻辑的概率值。如果我们在网络最后添加一层softmax层，将会把这些数值转换为概率分布。这就意味着输出结果可以直接呈现给用户，例如95%的概率确定这是一只😺。也意味着输出结果可以导入其他的机器学习模型，而不用归一化操作，因为已经保证了[0,1]区间范围。</p> 
<p>注意到如果网络将图像分类为😺和🐶，被配置为有两个种类的输出结果，则每张图像将被强制分类为其中之一，要么😺要么🐶，即使图像既不是😺也不是🐶。如果我们允许这种可能性，我们必须重新配置神经网络使之包含混杂类别的第三种类别。</p> 
<h3 id="%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84Softmax%E8%AE%A1%E7%AE%97%E7%A4%BA%E4%BE%8B">神经网络中的Softmax计算示例</h3> 
<p>训练伸进网络时softmax是必要的。假设我们有一个识别猫狗的神经网络。我们设定😺为class1，🐶为class2。</p> 
<p>当我们输入一张😺的图像到网络时，网络输出结果为[1,0]。当输入一张🐶的图像时，我们希望得到结果[0,1]。</p> 
<p>神经网络图像处理到最后的全连接层结束。这层输出结果包含猫狗的两个数值，但还不是概率值。通常操作是在神经网络的最后再添加一层softmax层，转换结果数值为概率分布。训练之始，神经网络权重是随机配置的。例如😺图像进入网络到达图像处理最后阶段的分值为[1.2,0.3]。将[1.2, 0.3]送入softmax函数，我们便能够得到最初的概率分布[0.71,0.29]。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/29/a6/lP3D20uK_o.png"></p> 
<p> 显而易见，这不是想要的。这种情况下，完美的网络应该输出[1,0]。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/de/d1/NSiPYqGc_o.jpg"></p> 
<p> 我们可以设定网络的损失函数，来量化网络的输出概率和我们期望的概率的差距。损失函数数值越小，输出vector与正确类别越接近。这种情况下最常用的损失函数是交叉损失熵，此函数值为：</p> 
<p><img alt="L=-\left ( 1log0.71+0log0.29 \right )=-0.71" class="mathcode" src="https://images2.imgbox.com/25/a4/zBaaI79F_o.png"></p> 
<p>因为softmax是连续的可微的函数，对于网络中的每一个权重，训练集中每一张图像，可以计算损失函数的导数。</p> 
<p>这种性质允许我们适配网络权重以减小损失函数，使网络输出数值更接近于我们期望的数值，提升网络的准确度。多次迭代训练之后，更新网络权重。现在当同样的一张😺图像送入网络后，全连接层的输出结果为vector[1.9,0.1]。把它再次送入softmax函数，我们得到输出概率值：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/de/e0/JILa1QGB_o.png"></p> 
<p>显然，这是一个更好的结果，更接近于希望的结果[1,0]。重新计算交叉损失熵：</p> 
<p><img alt="L=-(1log0.86+0log0.14)=-0.86" class="mathcode" src="https://images2.imgbox.com/77/38/r7fcXDjR_o.png"></p> 
<p>我们看到，损失函数减小了，表明神经网络提升了。</p> 
<p>区分损失函数差别是为了查明网络权重如何调整的，这种方法当我们使用argmax函数时变的不可行，因为argmax结果没有区分度。这种具有区分度的性质使得softmax函数在训练神经网络时非常有用。</p> 
<h3 id="%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84Softmax%E5%87%BD%E6%95%B0">强化学习中的Softmax函数</h3> 
<p>在强化学习中，当模型需要决策时softmax函数也经常使用，采取行动获得最高的激励，称之为“利用-exploitation”，或者探索步骤，称之为“探索-exploration”。</p> 
<h3 id="%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84Softmax%E8%AE%A1%E7%AE%97%E7%A4%BA%E4%BE%8B">强化学习中的Softmax计算示例</h3> 
<p>假设我们训练一个打扑克的人机对抗的强化学习模型。我们必须配置一个温度<img alt="\tau" class="mathcode" src="https://images2.imgbox.com/eb/73/sCxLZgQl_o.png">，用来描述系统随机探索行为的相似度。现在系统有两个选择：出牌A或出牌王。目前从它学到的判断，当前条件下，出牌A有80%可能赢。假设没有其他可能的出牌方式，出牌王有20%可能赢。我们配置温度参数<img alt="\tau" class="mathcode" src="https://images2.imgbox.com/82/7e/LH5Fe7EB_o.png">为2。 </p> 
<p>强化学习系统使用softmax函数得到出牌A和出牌王的概率。强化学习中修改的softmax公式如下所示：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/90/d7/2zzavDQ1_o.png"></p> 
<p>强化学习softmax公式符号解释 </p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/8f/78/YsZVfNna_o.png"></p> 
<p>将数值代入等式，可得：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/59/1c/IbcbWL2u_o.png"></p> 
<p>这意味着尽管模型出牌A有80%确信度是正确的策略，但是只有57%可能性出这手牌。这是因为强化学习中我们设定一个值去探索（测试出新的策略）同时去利用（利用既有策略）。如果我们选择提高温度参数，模型会变得更加“冲动”：它更可能执行探索步骤而非总是执行获胜的策略。</p> 
<hr> 
<h2 id="%E5%8E%86%E5%8F%B2">历史</h2> 
<p>提示：已知最早的softmax函数的使用早于机器学习。事实上，softmax函数来自于物理学和统计力学，因波尔兹曼分布和吉布斯分布而知名。公式由奥地利物理学家和哲学家Ludwig Boltzmann于1868年提出。</p> 
<p>波尔兹曼研究热平衡系统中的气体统计热力学。他发现波尔兹曼分布能描述特定状态系统的概率，给出状态的能量和系统的温度。他提出的公式的版本与强化学习中的更相似。实际上，在强化学习领域参数<img alt="\tau" class="mathcode" src="https://images2.imgbox.com/88/78/MWPxWDZh_o.png">被称为温度也是为了致敬波尔兹曼。</p> 
<p>1902年美国物理学家和化学家Josiah Willard Gibbs推广了波尔兹曼分布，他据此因热力学和熵的定义获得基金。这也奠定了光谱学的基础，即是通过观察材料吸收和反射的光线来分析材料。</p> 
<p>1959年在自己著作《Individual Choice Behavior: A Theoretical Analysis》中 Robert Duncan Luce提出应用softmax函数。最后1989年John S. Bridle提出在神经网络前向传播中由softmax取代argmax，因为softmax维持了输入值的等级序列，是捷取最大值“赢家通吃”操作的有区分度的一般化过程。近些年来，随着神经网络广泛应用，softmax因为上述性质变得众所周知。</p> 
<hr> 
<p></p> 
<h2 id="%E5%8F%82%E8%80%83">参考</h2> 
<p><br><span style="color:#999aaa;"><a class="link-info has-card" href="https://deepai.org/machine-learning-glossary-and-terms/softmax-layer" rel="nofollow" title="What is the Softmax Function?https://deepai.org/machine-learning-glossary-and-terms/softmax-layer"><span class="link-card-box"><span class="link-title">What is the Softmax Function?https://deepai.org/machine-learning-glossary-and-terms/softmax-layer</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/96/e1/w16gH2sF_o.png" alt="icon-default.png?t=LA92">https://deepai.org/machine-learning-glossary-and-terms/softmax-layer</span></span></a><br><a class="link-info has-card" href="https://en.wikipedia.org/wiki/Softmax_function" rel="nofollow" title="Softmax functionhttps://en.wikipedia.org/wiki/Softmax_function"><span class="link-card-box"><span class="link-title">Softmax functionhttps://en.wikipedia.org/wiki/Softmax_function</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/21/f3/N7k8FGfg_o.png" alt="icon-default.png?t=LA92">https://en.wikipedia.org/wiki/Softmax_function</span></span></a></span></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e4bb0d3f5ee3c112a610306ab48dac1b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">基于SpringBoot&#43;Vue的音乐网站项目-附源码&#43;报告</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/026ef48e86d7428186f43fa497872708/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">C# 获取当前时间戳（正确版）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>