<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Scrapy - Request 和 Response（请求和响应） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Scrapy - Request 和 Response（请求和响应）" />
<meta property="og:description" content="Requests and Responses：http://doc.scrapy.org/en/latest/topics/request-response.html
Requests and Responses（中文版）：https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/request-response.html
请求 和 响应 通常，Request对象 在 爬虫程序中生成并传递到系统，直到它们到达下载程序，后者执行请求并返回一个 Response对象，该对象 返回到发出请求的爬虫程序。
上面一段话比较拗口，有 web 经验的同学，应该都了解的，不明白看下面的图大概理解下。
爬虫 -&gt; Request:创建Request -&gt; Response:获取下载数据Response -&gt; 爬虫:数据 Request 和 Response 类 都有一些子类，它们添加基类中不需要的功能。这些在下面 的 请求子类 和 响应子类中描述。
Request objects class scrapy.http.Request (url [, callback, method=&#39;GET&#39;, headers, body, cookies, meta, encoding=&#39;utf-8&#39;, priority=0, dont_filter=False, errback, flags] )
一个 Request 对象 表示一个 HTTP请求，它通常是在爬虫生成，并由下载执行，从而生成 Response 对象。
参数：
url (string) – the URL of this requestcallback (callable) – the function that will be called with the response of this request (once its downloaded) as its first parameter." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/cb7a1371206c83fa18b16edd7f12291e/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-02-25T12:12:00+08:00" />
<meta property="article:modified_time" content="2019-02-25T12:12:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Scrapy - Request 和 Response（请求和响应）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p> </p> 
<p>Requests and Responses：<a href="http://doc.scrapy.org/en/latest/topics/request-response.html" rel="nofollow">http://doc.scrapy.org/en/latest/topics/request-response.html</a></p> 
<p>Requests and Responses（中文版）：<a href="https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/request-response.html" rel="nofollow">https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/request-response.html</a></p> 
<p> </p> 
<p> </p> 
<p> </p> 
<h2>请求 和 响应</h2> 
<p> </p> 
<p>通常，<strong>Request对象</strong> 在 爬虫程序中生成并传递到系统，直到它们到达下载程序，后者执行请求并返回一个 <strong>Response对象</strong>，该对象 返回到发出请求的爬虫程序。</p> 
<p>上面一段话比较拗口，有 web 经验的同学，应该都了解的，不明白看下面的图大概理解下。</p> 
<ol><li>爬虫  -&gt;  Request:创建</li><li>Request  -&gt;  Response:获取下载数据</li><li>Response  -&gt;  爬虫:数据</li></ol> 
<p><img alt="" class="has" src="https://images2.imgbox.com/35/d9/2JT7TU2i_o.png"></p> 
<p><strong>Request </strong>和 <strong>Response </strong>类 都有一些子类，它们添加基类中不需要的功能。这些在下面 的 请求子类 和 响应子类中描述。</p> 
<p> </p> 
<p> </p> 
<h3>Request objects</h3> 
<p> </p> 
<p><span style="color:#7c79e5;"><strong>class </strong></span><strong>scrapy.http.Request <span style="color:#7c79e5;">(url </span>[<span style="color:#7c79e5;">, callback, method='GET', headers, body, cookies, meta, encoding='utf-8', priority=0, dont_filter=False, errback, flags</span>] <span style="color:#7c79e5;">)</span></strong></p> 
<p>一个 <strong>Request 对象</strong> 表示一个 <strong>HTTP请求</strong>，<span style="color:#e579b6;"><strong>它通常是在爬虫生成，并由下载执行</strong></span>，从而生成 <strong>Response 对象</strong>。</p> 
<p> </p> 
<p>参数：</p> 
<ul><li><strong>url</strong> (<em>string</em>) – the URL of this request</li><li><strong>callback</strong> (<em>callable</em>) – the function that will be called with the response of this request (once its downloaded) as its first parameter. For more information see <a href="http://doc.scrapy.org/en/latest/topics/request-response.html#topics-request-response-ref-request-callback-arguments" rel="nofollow">Passing additional data to callback functions</a> below. If a Request doesn’t specify a callback, the spider’s <a href="http://doc.scrapy.org/en/latest/topics/spiders.html#scrapy.spiders.Spider.parse" rel="nofollow"><code>parse()</code></a> method will be used. Note that if exceptions are raised during processing, errback is called instead.</li><li><strong>method</strong> (<em>string</em>) – the HTTP method of this request. Defaults to <code>'GET'</code>.</li><li><strong>meta</strong> (<em>dict</em>) – the initial values for the <a href="http://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request.meta" rel="nofollow"><code>Request.meta</code></a> attribute. If given, the dict passed in this parameter will be shallow copied.</li><li><strong>body</strong> (<em>str</em><em> or </em><em>unicode</em>) – the request body. If a <code>unicode</code> is passed, then it’s encoded to<code>str</code> using the encoding passed (which defaults to <code>utf-8</code>). If <code>body</code> is not given, an empty string is stored. Regardless of the type of this argument, the final value stored will be a <code>str</code> (never <code>unicode</code> or <code>None</code>).</li><li><strong>headers</strong> (<em>dict</em>) – the headers of this request. The dict values can be strings (for single valued headers) or lists (for multi-valued headers). If <code>None</code> is passed as value, the HTTP header will not be sent at all.</li><li><strong>cookies</strong> (<em>dict</em><em> or </em><em>list</em>) – <p>the request cookies. These can be sent in two forms.</p> 
  <ol><li>Using a dict: <pre class="has"><code class="language-html">request_with_cookies = Request(url="http://www.example.com",
                               cookies={'currency': 'USD', 'country': 'UY'})
</code></pre> </li><li>Using a list of dicts: <pre class="has"><code class="language-html">request_with_cookies = Request(url="http://www.example.com",
                               cookies=[{'name': 'currency',
                                        'value': 'USD',
                                        'domain': 'example.com',
                                        'path': '/currency'}])
</code></pre> </li></ol><p>The latter form allows for customizing the <code>domain</code> and <code>path</code> attributes of the cookie. This is only useful if the cookies are saved for later requests.</p> <p>When some site returns cookies (in a response) those are stored in the cookies for that domain and will be sent again in future requests. That’s the typical behaviour of any regular web browser. However, if, for some reason, you want to avoid merging with existing cookies you can instruct Scrapy to do so by setting the <code>dont_merge_cookies</code> key to True in the <a href="http://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request.meta" rel="nofollow"><code>Request.meta</code></a>.</p> <p>Example of request without merging cookies:</p> <pre class="has"><code class="language-html">request_with_cookies = Request(url="http://www.example.com",
                               cookies={'currency': 'USD', 'country': 'UY'},
                               meta={'dont_merge_cookies': True})
</code></pre> <p>For more info see <a href="http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#cookies-mw" rel="nofollow">CookiesMiddleware</a>.</p> </li><li><strong>encoding</strong> (<em>string</em>) – the encoding of this request (defaults to <code>'utf-8'</code>). This encoding will be used to percent-encode the URL and to convert the body to <code>str</code> (if given as <code>unicode</code>).</li><li><strong>priority</strong> (<em>int</em>) – the priority of this request (defaults to <code>0</code>). The priority is used by the scheduler to define the order used to process requests. Requests with a higher priority value will execute earlier. Negative values are allowed in order to indicate relatively low-priority.</li><li><strong>dont_filter</strong> (<em>boolean</em>) – indicates that this request should not be filtered by the scheduler. This is used when you want to perform an identical request multiple times, to ignore the duplicates filter. Use it with care, or you will get into crawling loops. Default to <code>False</code>.</li><li><strong>errback</strong> (<em>callable</em>) – a function that will be called if any exception was raised while processing the request. This includes pages that failed with 404 HTTP errors and such. It receives a <a href="https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html" rel="nofollow">Twisted Failure</a> instance as first parameter. For more information, see <a href="http://doc.scrapy.org/en/latest/topics/request-response.html#topics-request-response-ref-errbacks" rel="nofollow">Using errbacks to catch exceptions in request processing</a> below.</li><li><strong>flags</strong> (<em>list</em>) – Flags sent to the request, can be used for logging or similar purposes.</li></ul> 
<p> </p> 
<p>对应中文解释：</p> 
<ul><li><code>url（string）</code> - 此请求的网址。请记住，此属性包含转义的网址，因此它可能与构造函数中传递的网址不同。此属性为只读。<span style="color:#f33b45;"><strong>更改请求 的 URL 可以使用 Request 的 <code>replace()</code>。</strong></span></li><li><code>callback（callable）</code> - 将使用此请求的响应（一旦下载）作为其第一个参数调用的函数。有关更多信息，请参阅下面的<a href="https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-response-ref-request-callback-arguments" rel="nofollow">将附加数据传递给回调函数</a>。如果请求没有指定回调，parse()将使用spider的 方法。请注意，如果在处理期间引发异常，则会调用errback。</li><li><code>method（string）</code> - 此请求的HTTP方法。默认为'GET'。注意：必须保证是大写的。例如：<code>"GET"，"POST"，"PUT"</code>等</li><li><code>meta（dict）</code> - 属性的初始值Request.meta。如果给定，在此参数中传递的 dict 将被浅复制。包含此请求的任意元数据的字典。此dict对于新请求为空，通常由不同的Scrapy组件（扩展程序，中间件等）填充。因此，此dict中包含的数据取决于您启用的扩展。有关<a href="https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-meta" rel="nofollow">Scrapy识别</a>的特殊元键列表，请参阅<a href="https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-meta" rel="nofollow">Request.meta特殊键</a>。当使用 <code>copy()</code> 或者 <code>replace()</code>克隆请求时，此 dict 是 <a href="https://docs.python.org/2/library/copy.html" rel="nofollow">浅复制</a> 的 。在爬虫中可以通过 response.meta 属性访问。</li><li><code>body（str或unicode）</code> - 请求体。即 包含请求正文的 str。此属性为只读。更改 <code>body</code> <span style="color:#f33b45;"><strong>可以使用 Request 的 <code>replace()</code></strong></span>。如果unicode传递了a，那么它被编码为 str使用传递的编码（默认为utf-8）。如果 body 没有给出，则存储一个空字符串。不管这个参数的类型，存储的最终值将是一个str（不会是unicode或None）。</li><li><code>headers（dict）</code> - 这个请求的头。dict 值可以是字符串（对于单值标头）或列表（对于多值标头）。如果 None作为值传递，则不会发送HTTP头。</li><li><code>cookie（dict或list）</code> - 请求cookie。这些可以以两种形式发送</li><li><code>encoding（string）</code> - 此请求的编码（默认为'utf-8'）。此编码将用于对URL进行百分比编码，并将正文转换为str（如果给定unicode）。</li><li><code>priority（int）</code> - 此请求的优先级（默认为0）。调度器使用优先级来定义用于处理请求的顺序。具有较高优先级值的请求将较早执行。允许负值以指示相对低优先级。</li><li><code>dont_filter（boolean）</code> - 表示此请求不应由调度程序过滤。当您想要多次执行相同的请求时忽略重复过滤器时使用。小心使用它，或者你会进入爬行循环。默认为False。</li><li> <p><code>errback（callable）</code> - 如果在处理请求时引发任何异常，将调用的函数。这包括失败的404 HTTP错误等页面。它接收一个<a href="https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html" rel="nofollow">Twisted Failure</a>实例作为第一个参数。有关更多信息，请参阅<a href="https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-response-ref-errbacks" rel="nofollow">使用errbacks在请求处理</a>中<a href="https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-response-ref-errbacks" rel="nofollow">捕获异常</a>。</p> </li></ul> 
<p> </p> 
<ul><li> <p><span style="color:#f33b45;"><strong><code>copy（）方法</code></strong></span><br> 返回一个新的请求，它是这个请求的副本。另请参见： <a href="https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-response-ref-request-callback-arguments" rel="nofollow">将附加数据传递到回调函数</a>。</p> </li><li> <p><span style="color:#f33b45;"><strong><code>replace([url, method, headers, body, cookies, meta, encoding, dont_filter, callback, errback])</code></strong></span><br> 返回具有相同成员的Request对象，但通过指定的任何关键字参数赋予新值的成员除外。该属性Request.meta是默认复制（除非新的值在给定的meta参数）。另请参见 <a href="https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-response-ref-request-callback-arguments" rel="nofollow">将附加数据传递给回调函数</a></p> </li></ul> 
<p>repalce() 方法使用：首先看 下 这个方法在源码中的定义（<span style="color:#f33b45;"><strong>scrapy/http/request/__init__.py</strong></span>）：</p> 
<pre class="has"><code class="language-python">"""
This module implements the Request class which is used to represent HTTP
requests in Scrapy.

See documentation in docs/topics/request-response.rst
"""
import six
from w3lib.url import safe_url_string

from scrapy.http.headers import Headers
from scrapy.utils.python import to_bytes
from scrapy.utils.trackref import object_ref
from scrapy.utils.url import escape_ajax
from scrapy.http.common import obsolete_setter


class Request(object_ref):

    def __init__(self, url, callback=None, method='GET', headers=None, body=None,
                 cookies=None, meta=None, encoding='utf-8', priority=0,
                 dont_filter=False, errback=None, flags=None):

        self._encoding = encoding  # this one has to be set first
        self.method = str(method).upper()
        self._set_url(url)
        self._set_body(body)
        assert isinstance(priority, int), "Request priority not an integer: %r" % priority
        self.priority = priority

        if callback is not None and not callable(callback):
            raise TypeError('callback must be a callable, got %s' % type(callback).__name__)
        if errback is not None and not callable(errback):
            raise TypeError('errback must be a callable, got %s' % type(errback).__name__)
        assert callback or not errback, "Cannot use errback without a callback"
        self.callback = callback
        self.errback = errback

        self.cookies = cookies or {}
        self.headers = Headers(headers or {}, encoding=encoding)
        self.dont_filter = dont_filter

        self._meta = dict(meta) if meta else None
        self.flags = [] if flags is None else list(flags)

    @property
    def meta(self):
        if self._meta is None:
            self._meta = {}
        return self._meta

    def _get_url(self):
        return self._url

    def _set_url(self, url):
        if not isinstance(url, six.string_types):
            raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)

        s = safe_url_string(url, self.encoding)
        self._url = escape_ajax(s)

        if ':' not in self._url:
            raise ValueError('Missing scheme in request url: %s' % self._url)

    url = property(_get_url, obsolete_setter(_set_url, 'url'))

    def _get_body(self):
        return self._body

    def _set_body(self, body):
        if body is None:
            self._body = b''
        else:
            self._body = to_bytes(body, self.encoding)

    body = property(_get_body, obsolete_setter(_set_body, 'body'))

    @property
    def encoding(self):
        return self._encoding

    def __str__(self):
        return "&lt;%s %s&gt;" % (self.method, self.url)

    __repr__ = __str__

    def copy(self):
        """Return a copy of this Request"""
        return self.replace()

    def replace(self, *args, **kwargs):
        """Create a new Request with the same attributes except for those
        given new values.
        """
        for x in ['url', 'method', 'headers', 'body', 'cookies', 'meta', 'flags',
                  'encoding', 'priority', 'dont_filter', 'callback', 'errback']:
            kwargs.setdefault(x, getattr(self, x))
        cls = kwargs.pop('cls', self.__class__)
        return cls(*args, **kwargs)

</code></pre> 
<p><span style="color:#f33b45;"><strong>replace() 方法 返回一个 类 的 实例，需要一个变量来保存这个类 实例</strong></span>。所以使用方式如下：</p> 
<pre class="has"><code class="language-python">if __name__ == '__main__':
    from scrapy.http.request import Request
    r = Request(url='https://www.baidu.com')

    r_1 = r.replace(url="https://www.google.com")
    print('r_1.url : {0}'.format(r_1.url))
    print('r_1.method : {0}'.format(r_1.method))

    r_2 = r.replace(method='post')
    print('r_2.url : {0}'.format(r_2.url))
    print('r_2.method : {0}'.format(r_2.method))

    r._set_url('http://www.sina.com')
    print('r.url : {0}'.format(r.url))
    print('r.method : {0}'.format(r.method))</code></pre> 
<p>运行结果截图：</p> 
<p><img alt="" class="has" height="160" src="https://images2.imgbox.com/8b/b4/hOkiwFjQ_o.png" width="356"></p> 
<p> </p> 
<p> </p> 
<h4>将 附加数据 传递 给 回调函数</h4> 
<p> </p> 
<p><span style="color:#f33b45;"><strong>也就是 从 request 中传递数据  到 response</strong></span></p> 
<p>请求的回调是当下载该请求的响应时将被调用的函数。将使用下载的Response对象作为其第一个参数来调用回调函数。</p> 
<p>Example:</p> 
<pre class="has"><code class="language-python">def parse_page1(self, response):
    return scrapy.Request("http://www.example.com/some_page.html",
                          callback=self.parse_page2)

def parse_page2(self, response):
    # this would log http://www.example.com/some_page.html
    self.logger.info("Visited %s", response.url)</code></pre> 
<p>在某些情况下，您可能有兴趣向这些回调函数传递参数，以便稍后在第二个回调中接收参数。您可以使用该Request.meta属性。<br> 以下是使用此机制传递项目以填充来自不同页面的不同字段的示例：</p> 
<pre class="has"><code class="language-python">def parse_page1(self, response):
    item = MyItem()
    item['main_url'] = response.url
    request = scrapy.Request("http://www.example.com/some_page.html",
                             callback=self.parse_page2)
    request.meta['item'] = item
    yield request

def parse_page2(self, response):
    item = response.meta['item']
    item['other_url'] = response.url
    yield item</code></pre> 
<p> </p> 
<p> </p> 
<h4>使用 errbacks 在请求处理中捕获异常</h4> 
<p> </p> 
<p>请求的 errback 是在处理异常时被调用的函数。</p> 
<p>它接收一个<a href="https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html" rel="nofollow">Twisted Failure</a>实例作为第一个参数，并可用于跟踪连接建立超时，DNS错误等。</p> 
<p>这里有一个 <strong>示例爬虫</strong><span style="color:#f33b45;"><strong> 记录所有错误，并捕获一些特定的错误</strong></span>，如果需要：</p> 
<pre class="has"><code class="language-python">import scrapy

from scrapy.spidermiddlewares.httperror import HttpError
from twisted.internet.error import DNSLookupError
from twisted.internet.error import TimeoutError, TCPTimedOutError

class ErrbackSpider(scrapy.Spider):
    name = "errback_example"
    start_urls = [
        "http://www.httpbin.org/",              # HTTP 200 expected
        "http://www.httpbin.org/status/404",    # Not found error
        "http://www.httpbin.org/status/500",    # server issue
        "http://www.httpbin.org:12345/",        # non-responding host, timeout expected
        "http://www.httphttpbinbin.org/",       # DNS error expected
    ]

    def start_requests(self):
        for u in self.start_urls:
            yield scrapy.Request(u, callback=self.parse_httpbin,
                                    errback=self.errback_httpbin,
                                    dont_filter=True)

    def parse_httpbin(self, response):
        self.logger.info('Got successful response from {}'.format(response.url))
        # do something useful here...

    def errback_httpbin(self, failure):
        # log all failures
        self.logger.error(repr(failure))

        # in case you want to do something special for some errors,
        # you may need the failure's type:

        if failure.check(HttpError):
            # these exceptions come from HttpError spider middleware
            # you can get the non-200 response
            response = failure.value.response
            self.logger.error('HttpError on %s', response.url)

        elif failure.check(DNSLookupError):
            # this is the original request
            request = failure.request
            self.logger.error('DNSLookupError on %s', request.url)

        elif failure.check(TimeoutError, TCPTimedOutError):
            request = failure.request
            self.logger.error('TimeoutError on %s', request.url)</code></pre> 
<p> </p> 
<p> </p> 
<h3>Request.meta 特殊键</h3> 
<p> </p> 
<p>该 <code><strong>Request.meta</strong> </code>属性可以包含任何任意数据，但有一些特殊的键由 Scrapy 及其内置扩展识别。</p> 
<p>那些是：</p> 
<pre class="has"><code>dont_redirect
dont_retry
handle_httpstatus_list
handle_httpstatus_all
dont_merge_cookies（参见cookies构造函数的Request参数）
cookiejar
dont_cache
redirect_urls
bindaddress
dont_obey_robotstxt
download_timeout
download_maxsize
download_latency
proxy</code></pre> 
<p> </p> 
<p><strong>bindaddress：</strong>用于执行请求的出站IP地址的IP。</p> 
<p><strong>download_timeout：</strong>下载器在超时前等待的时间量（以秒为单位）。参见：<code>DOWNLOAD_TIMEOUT</code>。</p> 
<p><strong>download_latency：</strong>自请求已启动以来，用于获取响应的时间量，即通过网络发送的HTTP消息。此元键仅在响应已下载时可用。虽然大多数其他元键用于控制Scrapy行为，但这应该是只读的。</p> 
<p> </p> 
<p> </p> 
<h3><span style="color:#f33b45;">Request (请求)</span> 的 <span style="color:#f33b45;"><strong>subclasses(</strong>子类<strong>)</strong></span></h3> 
<p> </p> 
<p>Here is the list of built-in <a href="http://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request" rel="nofollow"><code>Request</code></a> subclasses. You can also subclass it to implement your own custom functionality.</p> 
<p>这是 Scrapy 框架中 <strong>Request </strong>类 的 <strong>内建 subclasses(子类) 列表。</strong>你也可以 通过继承来实现它的一个子类，用来实现啊你自己自定的功能</p> 
<p> </p> 
<p> </p> 
<h4>FormRequest objects</h4> 
<p> </p> 
<p>FormRequest类 扩展了 Request 具有处理HTML表单的功能的基础。它使用lxml.html表单 从 Response对象 的 表单数据 预填充 表单字段。</p> 
<p> </p> 
<p><strong>FormRequest</strong></p> 
<p><span style="color:#7c79e5;"><em>class </em></span><strong><code>scrapy.http.</code><code>FormRequest</code><span style="color:#7c79e5;">(<em>url </em>[, <em>formdata</em>, <em>...</em>])</span></strong></p> 
<p><strong><code>FormRequest</code>类</strong> 增加了新 的构造函数的 参数。其余的参数与 <code>Request</code>类 相同，这里没有记录。</p> 
<ul><li>参数：formdata（元组的dict或iterable） - 是一个包含HTML Form数据的字典（或（key，value）元组的迭代），它将被url编码并分配给请求的主体。</li></ul> 
<p> </p> 
<p><strong>from_response</strong></p> 
<p><code>FormRequest</code>对象 添加下面的方法到 标准的 Request 对象中：</p> 
<p><span style="color:#7c79e5;"><em>classmethod </em></span><strong><code>from_response</code></strong><span style="color:#7c79e5;">(<em>response</em>[, <em>formname=None</em>, <em>formid=None</em>, <em>formnumber=0</em>, <em>formdata=None</em>, <em>formxpath=None</em>, <em>formcss=None</em>, <em>clickdata=None</em>, <em>dont_click=False</em>, <em>...</em>])</span></p> 
<p> </p> 
<p>Returns a new <a href="http://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.FormRequest" rel="nofollow"><code>FormRequest</code></a> object with its form field values pre-populated with those found in the HTML <code>&lt;form&gt;</code> element contained in the given response. For an example see <a href="http://doc.scrapy.org/en/latest/topics/request-response.html#topics-request-response-ref-request-userlogin" rel="nofollow">Using FormRequest.from_response() to simulate a user login</a>.</p> 
<p>返回一个新 <code>FormRequest</code>对象，其中它的 form(表单) 字段值 已预先设置，用在给定的 Response 对象 中 包含的 HTML 中 发现的 HTML &lt;form&gt; 元素 来 填充 字段值（）。即 根据 response找到HTML的&lt;from&gt;元素，以此来填充给定的form字段值，并返回一个新的FormRequest对象。</p> 
<p>有关示例，请参阅 <a href="https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-response-ref-request-userlogin" rel="nofollow">使用FormRequest.from_response（）来模拟用户登录</a>。</p> 
<p>The policy is to automatically simulate a click, by default, on any form control that looks clickable, like a <code>&lt;input type="submit"&gt;</code>. Even though this is quite convenient, and often the desired behaviour, sometimes it can cause problems which could be hard to debug. For example, when working with forms that are filled and/or submitted using javascript, the default <a href="http://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.FormRequest.from_response" rel="nofollow"><code>from_response()</code></a> behaviour may not be the most appropriate. To disable this behaviour you can set the <code>dont_click</code> argument to <code>True</code>. Also, if you want to change the control clicked (instead of disabling it) you can also use the <code>clickdata</code> argument.</p> 
<p> </p> 
<p>参数：</p> 
<ul><li><strong>response</strong> (<a href="http://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Response" rel="nofollow"><code>Response</code></a> object) – the response containing a HTML form which will be used to pre-populate the form fields</li><li><strong>formname</strong> (<em>string</em>) – if given, the form with name attribute set to this value will be used.</li><li><strong>formid</strong> (<em>string</em>) – if given, the form with id attribute set to this value will be used.</li><li><strong>formxpath</strong> (<em>string</em>) – if given, the first form that matches the xpath will be used.</li><li><strong>formcss</strong> (<em>string</em>) – if given, the first form that matches the css selector will be used.</li><li><strong>formnumber</strong> (<em>integer</em>) – the number of form to use, when the response contains multiple forms. The first one (and also the default) is <code>0</code>.</li><li><strong>formdata</strong> (<em>dict</em>) – fields to override in the form data. If a field was already present in the response <code>&lt;form&gt;</code> element, its value is overridden by the one passed in this parameter. If a value passed in this parameter is <code>None</code>, the field will not be included in the request, even if it was present in the response <code>&lt;form&gt;</code> element.</li><li><strong>clickdata</strong> (<em>dict</em>) – attributes to lookup the control clicked. If it’s not given, the form data will be submitted simulating a click on the first clickable element. In addition to html attributes, the control can be identified by its zero-based index relative to other submittable inputs inside the form, via the <code>nr</code>attribute.</li><li><strong>dont_click</strong> (<em>boolean</em>) – If True, the form data will be submitted without clicking in any element.</li></ul> 
<p> </p> 
<p>参数对应中文解释：</p> 
<ul><li>response（Responseobject） - 包含将用于预填充表单字段的HTML表单的响应</li><li>formname（string） - 如果给定，将使用name属性设置为此值的形式。</li><li>formid（string） - 如果给定，将使用id属性设置为此值的形式。</li><li>formxpath（string） - 如果给定，将使用匹配xpath的第一个表单。</li><li>formcss（string） - 如果给定，将使用匹配css选择器的第一个形式。</li><li>formnumber（integer） - 当响应包含多个表单时要使用的表单的数量。第一个（也是默认）是0。</li><li>formdata（dict） - 要在表单数据中覆盖的字段。如果响应&lt;form&gt;元素中已存在字段，则其值将被在此参数中传递的值覆盖。</li><li>clickdata（dict） - 查找控件被点击的属性。如果没有提供，表单数据将被提交，模拟第一个可点击元素的点击。除了html属性，控件可以通过其相对于表单中其他提交表输入的基于零的索引，通过nr属性来标识。</li><li>dont_click（boolean） - 如果为True，表单数据将在不点击任何元素的情况下提交。</li></ul> 
<p> </p> 
<p> </p> 
<h4>Request 用法 示例</h4> 
<p> </p> 
<p><strong>使用 FormRequest 通过 HTTP POST 发送数据</strong></p> 
<p>如果你想在你的爬虫中模拟 HTML 表单POST 并发送几个键值字段，你可以返回一个FormRequest对象（从你的爬虫）像这样：</p> 
<pre class="has"><code class="language-python">return [FormRequest(url="http://www.example.com/post/action",
                    formdata={'name': 'John Doe', 'age': '27'},
                    callback=self.after_post)]</code></pre> 
<p> </p> 
<p><strong>使用 FormRequest.from_response（）来模拟用户登录</strong></p> 
<p>网站通常通过元素（例如会话相关数据或认证令牌（用于登录页面））提供预填充的表单字段。进行剪贴时，您需要自动预填充这些字段，并且只覆盖其中的一些，例如用户名和密码。您可以使用 此作业的方法。这里有一个使用它的爬虫示例：<code>&lt;input type="hidden"&gt; FormRequest.from_response()</code></p> 
<pre class="has"><code class="language-python">import scrapy
 
class LoginSpider(scrapy.Spider):
    name = 'example.com'
    start_urls = ['http://www.example.com/users/login.php']
 
    def parse(self, response):
        return scrapy.FormRequest.from_response(
            response,
            formdata={'username': 'john', 'password': 'secret'},
            callback=self.after_login
        )
 
    def after_login(self, response):
        # check login succeed before going on
        if "authentication failed" in response.body:
            self.logger.error("Login failed")
            return
 
        # continue scraping with authenticated session...</code></pre> 
<p> </p> 
<p> </p> 
<h2>Response 对象</h2> 
<p> </p> 
<p><em>class </em><code>scrapy.http.</code><code>Response</code>(<em>url </em>[, <em>status=200</em>, <em>headers=None</em>, <em>body=b''</em>, <em>flags=None</em>, <em>request=None</em>])</p> 
<p>一个 Response对象 表示 一个 HTTP响应，它通常是由下载器下载，并供给到爬虫进行处理。</p> 
<p> </p> 
<p>参数：</p> 
<ul><li><strong>url</strong> (<em>string</em>) – the URL of this response</li><li><strong>status</strong> (<em>integer</em>) – the HTTP status of the response. Defaults to <code>200</code>.</li><li><strong>headers</strong> (<em>dict</em>) – the headers of this response. The dict values can be strings (for single valued headers) or lists (for multi-valued headers).</li><li><strong>body</strong> (<em>bytes</em>) – the response body. To access the decoded text as str (unicode in Python 2) you can use <code>response.text</code> from an encoding-aware <a href="http://doc.scrapy.org/en/latest/topics/request-response.html#topics-request-response-ref-response-subclasses" rel="nofollow">Response subclass</a>, such as <a href="http://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.TextResponse" rel="nofollow"><code>TextResponse</code></a>.</li><li><strong>flags</strong> (<em>list</em>) – is a list containing the initial values for the <a href="http://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Response.flags" rel="nofollow"><code>Response.flags</code></a> attribute. If given, the list will be shallow copied.</li><li><strong>request</strong> (<a href="http://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request" rel="nofollow"><code>Request</code></a> object) – the initial value of the <a href="http://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Response.request" rel="nofollow"><code>Response.request</code></a> attribute. This represents the <a href="http://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request" rel="nofollow"><code>Request</code></a> that generated this response.</li></ul> 
<p> </p> 
<p>参数 对应中文解释：</p> 
<ul><li>url（string） - 此响应的 URL</li><li>status（integer） - 响应的 HTTP 状态。默认为 200。</li><li>headers（dict） - 这个响应的头。dict 值可以是字符串（对于单值标头）或列表（对于多值标头）。</li><li>body（str） - 响应体。它必须是 str，而不是 unicode，除非你使用一个编码感知 <a href="https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-response-ref-response-subclasses" rel="nofollow">响应子类</a>，如 <code>TextResponse</code>。</li><li>flags（<a href="https://doc.scrapy.org/en/1.3/topics/api.html#scrapy.loader.SpiderLoader.list" rel="nofollow">list</a>） - 是一个包含属性初始值的 <code>Response.flags</code>列表。如果给定，列表将被浅复制。</li><li>request（Requestobject） - 属性的初始值<code>Response.request</code>。这代表Request生成此响应。</li></ul> 
<p> </p> 
<p><strong>url</strong><br> 包含响应的URL的字符串。此属性为只读。To change the URL of a Response use <a href="http://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Response.replace" rel="nofollow"><code>replace()</code></a>.</p> 
<p><strong>status</strong><br> 表示 响应的HTTP状态的整数。示例：200， 404。</p> 
<p><strong>headers</strong><br> 包含响应标题的类字典对象。可以使用get()返回具有指定名称的第一个标头值或getlist()返回具有指定名称的所有标头值来访问值。例如，此调用会为您提供标题中的所有Cookie：</p> 
<pre class="has"><code>response.headers.getlist('Set-Cookie')</code></pre> 
<p><strong>body</strong><br> Response 的 body。<span style="color:#f33b45;"><strong>记住 Response.body 总是一个字节对象</strong></span>。如果你想 unicode 版本使用 TextResponse.text（只在TextResponse 和子类中可用）。此属性为只读。更改响应使用的主体 replace()。</p> 
<p><strong>request</strong><br> 生成 response 的 <a href="http://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request" rel="nofollow"><code>Request</code></a> object。在响应和请求通过所有 <a href="https://doc.scrapy.org/en/1.3/topics/downloader-middleware.html#topics-downloader-middleware" rel="nofollow">下载中间件</a> 后，此属性在 Scrapy <a href="https://www.baidu.com/s?wd=%E5%BC%95%E6%93%8E&amp;tn=24004469_oem_dg&amp;rsv_dl=gh_pl_sl_csd" rel="nofollow">引擎</a> 中分配。特别地，这意味着：</p> 
<ul><li>HTTP 重定向将导致将原始请求（重定向之前的URL）分配给重定向响应（<span style="color:#f33b45;"><strong>重定向后具有最终URL</strong></span>）。</li><li><span style="color:#f33b45;"><strong>Response.request.url</strong></span> <span style="color:#7c79e5;"><strong>并不总是等于</strong></span> <span style="color:#f33b45;"><strong>Response.url</strong></span></li><li>此属性仅在爬虫程序代码和 <a href="https://doc.scrapy.org/en/1.3/topics/spider-middleware.html#topics-spider-middleware" rel="nofollow">Spider Middleware</a>中可用，但不能在Downloader Middleware中使用（尽管您有通过其他方式可用的请求）和处理程序response_downloaded。</li></ul> 
<p><strong>meta</strong><br> 一个快捷方式 对于 Request.meta 的属性 对于 Response.request对象（即 self.request.meta）。与 Response.request属性不同，Response.meta 属性沿重定向和重试传播，因此您将获得Request.meta从您的爬虫发送的原始属性。也可以看看  Request.meta 属性</p> 
<p><strong>flags</strong><br> 包含此响应的标志的列表。标志是用于标记响应的标签。例如：'cached'，'redirected '等等。它们显示在Response（ str 方法）的字符串表示上，它被引擎用于日志记录。</p> 
<p><strong>copy（）：</strong>返回一个新的响应，它是此响应的副本。</p> 
<p>replace（[ url，status，headers，body，request，flags，cls ] ）<br> 返回具有相同成员的Response对象，但通过指定的任何关键字参数赋予新值的成员除外。该属性Response.meta是默认复制。</p> 
<p>urljoin（url ）<br> 通过将响应url与可能的相对URL 组合构造绝对url。</p> 
<p>这是一个包装在<a href="https://docs.python.org/2/library/urlparse.html#urlparse.urljoin" rel="nofollow">urlparse.urljoin</a>，它只是一个别名，使这个调用：</p> 
<pre><code><code>urlparse.urljoin(response.url, url)</code></code></pre> 
<p><strong>follow </strong>(url, callback=None, method='GET', headers=None, body=None, cookies=None, meta=None, encoding='utf-8', priority=0, dont_filter=False, errback=None)</p> 
<p>Return a <a href="http://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request" rel="nofollow"><code>Request</code></a> instance to follow a link <code>url</code>. It accepts the same arguments as <code>Request.__init__</code> method, but <code>url</code> can be a relative URL or a <code>scrapy.link.Link</code> object, not only an absolute URL.</p> 
<p><a href="http://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.TextResponse" rel="nofollow"><code>TextResponse</code></a> provides a <a href="http://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.TextResponse.follow" rel="nofollow"><code>follow()</code></a> method which supports selectors in addition to absolute/relative URLs and Link objects.</p> 
<p> </p> 
<p> </p> 
<h4><span style="color:#f33b45;"><strong>Response（响应）</strong></span> 的 <span style="color:#f33b45;"><strong>subclasses（子类）</strong></span></h4> 
<p> </p> 
<p>这是 Scrapy 框架中 <strong>Response</strong>类 的 <strong>内建 subclasses(子类) 列表。</strong>你也可以 通过继承来实现它的一个子类，用来实现啊你自己自定的功能</p> 
<p> </p> 
<p> </p> 
<p><strong>TextResponse对象</strong></p> 
<p><em>class </em><code>scrapy.http.</code><code>TextResponse</code>(<em>url </em>[, <em>encoding</em>[, <em>...</em>]])</p> 
<p>TextResponse 对象 向基类Response类添加编码能力 ，这意味着仅用于二进制数据，例如图像，声音或任何媒体文件。<br> TextResponse 对象 支持一个新的构造函数参数，除了基础Response对象。其余的功能与Response类相同，这里没有记录。</p> 
<p><strong>参数：</strong> encoding（string） - 是一个字符串，包含用于此响应的编码。如果你创建一个TextResponse具有unicode主体的对象，它将使用这个编码进行编码（记住body属性总是一个字符串）。如果encoding是None（默认值），则将在响应标头和正文中查找编码。</p> 
<p> </p> 
<p>TextResponse除了标准对象之外，对象还支持以下属性Response</p> 
<p> </p> 
<p><strong>text</strong></p> 
<p>响应体，unicode 编码。</p> 
<p>同样 <code>response.body.decode(response.encoding)</code>，但结果是在第一次调用后缓存，因此您可以访问 <code>response.text</code>多次，无需额外的开销。</p> 
<p> </p> 
<p><strong>encoding</strong><br> 包含此响应的编码的字符串。编码通过尝试以下机制按顺序解决：</p> 
<ol><li>在构造函数编码参数中传递的编码</li><li>在Content-Type HTTP头中声明的编码。如果此编码无效（即未知），则会被忽略，并尝试下一个解析机制。</li><li>在响应主体中声明的编码。TextResponse类不提供任何特殊功能。然而， HtmlResponse和XmlResponse类做。</li><li>通过查看响应体来推断的编码。这是更脆弱的方法，但也是最后一个尝试。</li></ol> 
<p> </p> 
<p><strong>selector</strong><br> 一个Selector使用响应为目标实例。选择器在第一次访问时被延迟实例化。TextResponse对象除了标准对象外还支持以下方法Response：</p> 
<p><strong>xpath（查询）</strong></p> 
<p>快捷方式 <code>TextResponse.selector.xpath(query)</code>：</p> 
<pre class="has"><code>response.xpath('//p')</code></pre> 
<p><strong>css(query)</strong></p> 
<p>快捷方式 <code>TextResponse.selector.css(query)</code>:</p> 
<pre class="has"><code>response.css('p')</code></pre> 
<p><strong>body_as_unicode()</strong></p> 
<p>同样text，但可用作方法。保留此方法以实现向后兼容; 请喜欢response.text。</p> 
<p> </p> 
<p><strong>HtmlResponse对象</strong></p> 
<p><code>class scrapy.http.HtmlResponse（url [，... ] ）</code><br> 本HtmlResponse类的子类，TextResponse 这增加了通过查看HTML编码自动发现支持META HTTP-EQUIV属性。见TextResponse.encoding。</p> 
<p> </p> 
<p><strong>XmlResponse对象</strong></p> 
<p><code>class scrapy.http.XmlResponse（url [，... ] ）</code><br> 本XmlResponse类的子类，TextResponse这增加了通过查看XML声明线路编码自动发现支持。见TextResponse.encoding。</p> 
<p> </p> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/3bc4cfb1e978466deb2b555f2e0c7313/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">关于SpringMVC&#43;Spring&#43;Mybatis&#43;JQuery&#43;BootStrap_Table的Maven项目</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/8ed6941c260cf67731cfe32322cc467d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">深度学习：语义分割 FCN与Unet</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>