<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>《Ordinal Depth Supervision for 3D Human Pose Estimation》-CVPR2018 论文阅读 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="《Ordinal Depth Supervision for 3D Human Pose Estimation》-CVPR2018 论文阅读" />
<meta property="og:description" content="简介 《Ordinal Depth Supervision for 3D Human Pose Estimation》是一篇CVPR2018 Oral Presentation的工作，主要解决RGB的3D human pose estimation任务，也是当前的state-of-the-art.
这里对这篇文章做一个简单的总结，如有理解不对的地方，欢迎指正！！
方法 总结一下文章的中心思想，也就是下面的Figure1:
文章最创新的地方在于提出了用Ordinal的思想在辅助3D人体姿态估计任务的训练，2D标注的人体姿态估计数据库很多，比如COCO，MPII，FLIC…，并且具有多样性，也就是In-the-Wild的图片，但是3D人体姿态估计的数据库往往是在室内受限场景下面采集的（比如Human3.6M），场景单一，这样的环境下面训练的CNN模型很容易过拟合，导致对于自然图片无法很好的进行泛化。因此作者采用标注In-the-Wild的图片的Ordinal Depth的方法，使得CNN可以在In-the-Wild的图片上进行训练。
所谓的Ordinal Depth，就是关节点之间的相对深度，不需要知道每一个关节点的绝对物理深度，只需要知道关节点的深度顺序即可，这样我们同样可以采取Ranking Loss的监督进行网络的训练。如下图所示：
可以看到，网络预测的深度并不直接进行监督，而是采用深度值之间的大小关系进行Ranking监督，每一对关节点都可以构成一个loss，最后加和，这样就使得网络的训练成为可能。
那么如何充分利用In-the-Wild图片的2D标注呢？也就是结合2D关节点的GroundTruth同时对网络进行监督，很简单，使网络同时输出2D关节点的像素坐标以及3D的Depth值就可以：
这里作者使用的同时回归2D keypoints以及Ordinal Depth的方法，并不是直接对绝对位置进行回归，而是采用了“Volumetric Prediction”的方式，也就是说最后预测的是一个3维的Volume，其实也就是2D的HeatMap的拓展：
值得注意的是，这个方法也是该作者的团队在CVPR2017年提出的工作：《Coarse-to-Fine Volumetric Prediction for Single-Image 3D Human Pose》。通过预测每一个像素值的3D的Likelihood，最后取Max得到3D的坐标位置：
最后的Loss由2D的关节点L2损失以及Ordinal Depth Ranking Loss构成。这就解决了使用Ordinal Depth进行In-the-Wild图片的3D人体姿态估计问题。
在此之前，都是在介绍怎么使用Ordinal Depth在In-the-Wild图片上进行3D Pose的估计，进一步，如何将3D人体姿态估计数据库中的3D Pose也利用上呢？作者的做法是添加一个Reconstruction模块，输入Ordinal的Depth，输出实际物理距离的Depth：
这样，3D的人体姿态估计数据库就可以也用来训练这个模型了，新增的Reconstruction模块可以很容易的嵌入到目前的网络中去（几个全连接层而已）：
结果 在Human3.6M，MPI-INF-3DHP等几个数据库上面都是state-of-the-art：
更多的实验结果可以去看作者的原文，包括Ablation验证各个component的效果以及最后各个数据库上面报告的结果。
最后给几个数据库的链接：
Human3.6M：http://vision.imar.ro/human3.6m/description.php
COCO关节点：http://cocodataset.org/#keypoints-2017
MPII：http://human-pose.mpi-inf.mpg.de/
LSP：http://sam.johnson.io/research/lsp.html
当然还有这篇文章的项目主页：https://www.seas.upenn.edu/~pavlakos/projects/ordinal/
里面包括了本文的文章、补充材料、代码以及数据。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/61a22eff3dcea3a22223fdb4b94d19ab/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-11-26T09:55:03+08:00" />
<meta property="article:modified_time" content="2018-11-26T09:55:03+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">《Ordinal Depth Supervision for 3D Human Pose Estimation》-CVPR2018 论文阅读</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="_2"></a>简介</h3> 
<p>《<a href="https://arxiv.org/abs/1805.04095" rel="nofollow">Ordinal Depth Supervision for 3D Human Pose Estimation</a>》是一篇CVPR2018 Oral Presentation的工作，主要解决RGB的3D human pose estimation任务，也是当前的state-of-the-art.<br> <em>这里对这篇文章做一个简单的总结，如有理解不对的地方，欢迎指正！！</em></p> 
<h3><a id="_8"></a>方法</h3> 
<p>总结一下文章的中心思想，也就是下面的Figure1:</p> 
<p><img src="https://images2.imgbox.com/8b/a5/vIKo1O0J_o.jpg" alt="fugure1"></p> 
<p>文章最创新的地方在于提出了用Ordinal的思想在辅助3D人体姿态估计任务的训练，2D标注的人体姿态估计数据库很多，比如COCO，MPII，FLIC…，并且具有多样性，也就是In-the-Wild的图片，但是3D人体姿态估计的数据库往往是在室内受限场景下面采集的（比如Human3.6M），场景单一，这样的环境下面训练的CNN模型很容易过拟合，导致对于自然图片无法很好的进行泛化。因此作者采用标注In-the-Wild的图片的Ordinal Depth的方法，使得CNN可以在In-the-Wild的图片上进行训练。</p> 
<p>所谓的Ordinal Depth，就是关节点之间的相对深度，不需要知道每一个关节点的绝对物理深度，只需要知道关节点的深度顺序即可，这样我们同样可以采取Ranking Loss的监督进行网络的训练。如下图所示：<br> <img src="https://images2.imgbox.com/ea/7c/nlTabZ1u_o.png" alt="Fugure2"></p> 
<p>可以看到，网络预测的深度并不直接进行监督，而是采用深度值之间的大小关系进行Ranking监督，每一对关节点都可以构成一个loss，最后加和，这样就使得网络的训练成为可能。</p> 
<p>那么如何充分利用In-the-Wild图片的2D标注呢？也就是结合2D关节点的GroundTruth同时对网络进行监督，很简单，使网络同时输出2D关节点的像素坐标以及3D的Depth值就可以：<br> <img src="https://images2.imgbox.com/9b/58/8rGvofRR_o.png" alt="Figure3"></p> 
<p>这里作者使用的同时回归2D keypoints以及Ordinal Depth的方法，并不是直接对绝对位置进行回归，而是采用了“Volumetric Prediction”的方式，也就是说最后预测的是一个3维的Volume，其实也就是2D的HeatMap的拓展：</p> 
<p><img src="https://images2.imgbox.com/d0/c0/9izkeHU2_o.jpg" alt="Figure4"></p> 
<p>值得注意的是，这个方法也是该作者的团队在CVPR2017年提出的工作：<a href="https://arxiv.org/abs/1611.07828" rel="nofollow">《Coarse-to-Fine Volumetric Prediction for Single-Image 3D Human Pose》</a>。通过预测每一个像素值的3D的Likelihood，最后取Max得到3D的坐标位置：</p> 
<p><img src="https://images2.imgbox.com/44/33/m99XvDfO_o.jpg" alt="Figure5"></p> 
<p>最后的Loss由2D的关节点L2损失以及Ordinal Depth Ranking Loss构成。这就解决了使用Ordinal Depth进行In-the-Wild图片的3D人体姿态估计问题。</p> 
<p>在此之前，都是在介绍怎么使用Ordinal Depth在In-the-Wild图片上进行3D Pose的估计，进一步，如何将3D人体姿态估计数据库中的3D Pose也利用上呢？作者的做法是添加一个Reconstruction模块，输入Ordinal的Depth，输出实际物理距离的Depth：</p> 
<p><img src="https://images2.imgbox.com/f7/3f/C2d2MdBo_o.jpg" alt="Figure6"></p> 
<p>这样，3D的人体姿态估计数据库就可以也用来训练这个模型了，新增的Reconstruction模块可以很容易的嵌入到目前的网络中去（几个全连接层而已）：</p> 
<p><img src="https://images2.imgbox.com/05/0e/dDvCDELk_o.jpg" alt="Figure7"></p> 
<h3><a id="_42"></a>结果</h3> 
<p>在Human3.6M，MPI-INF-3DHP等几个数据库上面都是state-of-the-art：</p> 
<p><img src="https://images2.imgbox.com/b9/7d/vpqZswMe_o.jpg" alt="Figure8"></p> 
<p>更多的实验结果可以去看作者的<a rel="nofollow">原文</a>，包括Ablation验证各个component的效果以及最后各个数据库上面报告的结果。</p> 
<p><strong>最后给几个数据库的链接：</strong><br> Human3.6M：<a href="http://vision.imar.ro/human3.6m/description.php" rel="nofollow">http://vision.imar.ro/human3.6m/description.php</a><br> COCO关节点：<a href="http://cocodataset.org/#keypoints-2017" rel="nofollow">http://cocodataset.org/#keypoints-2017</a><br> MPII：<a href="http://human-pose.mpi-inf.mpg.de/" rel="nofollow">http://human-pose.mpi-inf.mpg.de/</a><br> LSP：<a href="http://sam.johnson.io/research/lsp.html" rel="nofollow">http://sam.johnson.io/research/lsp.html</a></p> 
<p>当然还有这篇文章的项目主页：<a href="https://www.seas.upenn.edu/~pavlakos/projects/ordinal/" rel="nofollow">https://www.seas.upenn.edu/~pavlakos/projects/ordinal/</a><br> 里面包括了本文的文章、补充材料、代码以及数据。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/bad7c1cbfbfcde1bd3e96cac25a6631a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">瓜子二手车如何玩转AI赋能下的新零售（附PPT下载）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/9f6b65d41f5c38f54935f000e854c030/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">两个整数的二进制数有多少的位不同</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>