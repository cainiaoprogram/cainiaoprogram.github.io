<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>用pytorch写个RNN 循环神经网络 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="用pytorch写个RNN 循环神经网络" />
<meta property="og:description" content="持续创作，加速成长！这是我参与「掘金日新计划 · 10 月更文挑战」的第4天，点击查看活动详情
写代码之前先回顾一下RNN的计算公式：
隐藏层计算公式：
\mathbf{H}_t = \phi(\mathbf{X}_t \mathbf{W}_{xh} &#43; \mathbf{H}_{t-1} \mathbf{W}_{hh} &#43; \mathbf{b}_h)Ht​=ϕ(Xt​Wxh​&#43;Ht−1​Whh​&#43;bh​)
输出计算公式：
\mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{hq} &#43; \mathbf{b}_qOt​=Ht​Whq​&#43;bq​
注意：之前我写过这么一篇：手动实现RNN - 掘金 (juejin.cn) 这个没有调用Pytorch的RNN，是自己从零开始写的。本文是调用了人家现成的RNN，两篇文章虽然都是RNN的代码实现，但是二者有本质区别。
import torch from torch import nn from torch.nn import functional as F from d2l import torch as d2l 复制代码 python人必备导包技术，这段代码不用解释吧。
batch_size, num_steps =32,35 train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps) num_hiddens = 256 rnn_layer = nn.RNN(len(vocab), num_hiddens) 复制代码 设置批量大小batch_size和时间步长度num_step，时间步长度就是可以想象成一个样本中RNN要计算的时间步长度是32。
d2l.load_data_time_machine加载数据集。
注意：这里为了方便，加载数据集时候进行数据预处理，使用的是长度为28的语料库词汇表，不是单词表。词汇表是a~z的字母外加 空格 和&lt;unk&gt;。
设置隐藏层大小的256" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/a1e31f8ea37f5f1f72e8bfb9c9c15c7e/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-10-21T17:30:13+08:00" />
<meta property="article:modified_time" content="2022-10-21T17:30:13+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">用pytorch写个RNN 循环神经网络</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p class="img-center"><img alt="" src="https://images2.imgbox.com/8d/ca/7ayPGXCg_o.png"></p> 
<p>持续创作，加速成长！这是我参与「掘金日新计划 · 10 月更文挑战」的第4天，<a href="https://juejin.cn/post/7147654075599978532" rel="nofollow" title="点击查看活动详情">点击查看活动详情</a></p> 
<p>写代码之前先回顾一下RNN的计算公式：</p> 
<p>隐藏层计算公式：</p> 
<p>\mathbf{H}_t = \phi(\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{H}_{t-1} \mathbf{W}_{hh} + \mathbf{b}_h)Ht​=ϕ(Xt​Wxh​+Ht−1​Whh​+bh​)</p> 
<p>输出计算公式：</p> 
<p>\mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{hq} + \mathbf{b}_qOt​=Ht​Whq​+bq​</p> 
<p><strong>注意</strong>：之前我写过这么一篇：<a href="https://juejin.cn/post/7036373347894034462" rel="nofollow" title="手动实现RNN - 掘金 (juejin.cn)">手动实现RNN - 掘金 (juejin.cn)</a> 这个没有调用Pytorch的RNN，是自己从零开始写的。本文是调用了人家现成的RNN，两篇文章虽然都是RNN的代码实现，但是二者有本质区别。</p> 
<hr> 
<pre><code>import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l
复制代码</code></pre> 
<p>python人必备导包技术，这段代码不用解释吧。</p> 
<pre><code>batch_size, num_steps =32,35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
num_hiddens = 256
rnn_layer = nn.RNN(len(vocab), num_hiddens)
复制代码</code></pre> 
<ul><li> <p>设置批量大小<code>batch_size</code>和时间步长度<code>num_step</code>，时间步长度就是可以想象成一个样本中RNN要计算的时间步长度是32。</p> </li><li> <p><code>d2l.load_data_time_machine</code>加载数据集。</p> <p><strong>注意</strong>：这里为了方便，加载数据集时候进行数据预处理，使用的是长度为28的语料库词汇表，不是单词表。词汇表是a~z的字母外加 空格 和&lt;unk&gt;。</p> </li><li> <p>设置隐藏层大小的256</p> </li><li> <p>在这里RNN层直接使用pytorch提供的RNN。</p> </li></ul> 
<pre><code>state = torch.zeros((1, batch_size, num_hiddens))
state.shape
复制代码</code></pre> 
<ul><li> <p>使用<code>torch.zeros</code>用零向量初始化隐状态。隐状态是三维的，形状是隐状态层数、批量大小、隐藏层大小。</p> </li><li> <p>RNN隐状态计算\mathbf{H}_t = \phi(\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{H}_{t-1} \mathbf{W}_{hh} + \mathbf{b}_h)Ht​=ϕ(Xt​Wxh​+Ht−1​Whh​+bh​)，这一步就是要初始化\mathbf{H}_{0}H0​</p> </li><li> <p>使用<code>state.shape</code>看一下\mathbf{H}_{0}H0​的形状。</p> </li><li> <p>测试一下：</p> <pre><code>X = torch.rand(size=(num_steps, batch_size, len(vocab)))
Y, state_new = rnn_layer(X, state)
Y.shape, state_new.shape
复制代码</code></pre> <p>随便搞一个<code>X</code>来试试pytorch自带的rnn层。</p> 
  <ul><li> <p><code>X</code>就是随机初始化的，形状是（时间步长、批量大小、语料库词汇表长度）。</p> </li><li> <p>使用rnn层进行计算返回<code>Y</code>和<code>state_new</code>，注意这里的<code>Y</code>不是我们说的那个RNN的输出，\mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{hq} + \mathbf{b}_qOt​=Ht​Whq​+bq​，←不是这个玩意儿，是隐藏层，是\mathbf{H}H。</p> </li><li> <p>最后一句是输出一下<code>Y</code>和<code>state_new</code>的形状：</p> 
    <blockquote> 
     <p>(torch.Size([35, 32, 256]), torch.Size([1, 32, 256]))</p> 
    </blockquote> <p>这里<code>Y</code>是所有的隐状态，<code>state_new</code>是最后一个隐状态。</p> <p class="img-center"><img alt="" src="https://images2.imgbox.com/4a/47/kDofBqCE_o.png"></p> </li></ul><p><strong>到这里我们可以得出：pytorch自带的RNN层计算的返回值是整个计算过程的隐状态和最后一个隐状态。</strong></p> </li></ul> 
<pre><code>class RNNModel(nn.Module):
    def __init__(self, rnn_layer, vocab_size, **kwargs):
        super(RNNModel, self).__init__(**kwargs)
        self.rnn = rnn_layer
        self.vocab_size = vocab_size
        self.num_hiddens = self.rnn.hidden_size
        
        if not self.rnn.bidirectional:
            self.num_directions = 1
            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)
        else:
            self.num_directions = 2
            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)

    def forward(self, inputs, state):
        X = F.one_hot(inputs.T.long(), self.vocab_size)
        X = X.to(torch.float32)
        Y, state = self.rnn(X, state)
        
        output = self.linear(Y.reshape((-1, Y.shape[-1])))
        return output, state

    def begin_state(self, device, batch_size=1):
        if not isinstance(self.rnn, nn.LSTM):
            return  torch.zeros((self.num_directions * self.rnn.num_layers,
                                 batch_size, self.num_hiddens), 
                                device=device)
        else:
            return (torch.zeros((
                self.num_directions * self.rnn.num_layers,
                batch_size, self.num_hiddens), device=device),
                    torch.zeros((
                        self.num_directions * self.rnn.num_layers,
                        batch_size, self.num_hiddens), device=device))
复制代码</code></pre> 
<p>这个是完整的RNN模型。</p> 
<ul><li> <p><code>__init__</code>中进行基本设置。</p> 
  <ul><li> <p>设定RNN层<code>rnn_layer</code>，设定字典大小<code>vocab_size</code>，设定隐藏层大小<code>num_hiddens</code></p> </li><li> <p>然后这个if-else语句，这个语句是为后面准备的。现在这里还是没有什么作用的。因为之后的双向rnn会用到。</p> <p><code>not self.rnn.bidirectional</code>也就是说当这个RNN不是双向的时候，进入if语句。此时设定它只有一个隐藏层，并且设定它的<code>vocab_size</code>。反之则进入else语句，此时设定它有两个隐藏层（因为是双向RNN一个正向的一个反向的），并且设定它的<code>vocab_size</code>。</p> </li></ul></li><li> <p><code>forward</code>设定前向传播函数。</p> 
  <ul><li>先对<code>X</code>进行处理，使其变为one-hot向量。再将数据类型转换为浮点型。</li><li>注意在这里<code>Y</code>是我们说的隐状态，不是我们常规意义上的输出。</li><li>输出<code>output</code>这里，全连接层首先将<code>Y</code>的形状改为(<code>时间步数</code><em><code>批量大小</code>, <code>隐藏单元数</code>)。再输出<code>output</code>输出形状是 (<code>时间步数</code></em><code>批量大小</code>, <code>词表大小</code>)。</li></ul></li><li> <p><code>begin_state</code>设定初始化的函数。里边也是一个if语句。根据rnn的类型来决定初始化的状态。 <code>isinstance(self.rnn, nn.LSTM)</code>是看我们的<code>self.rnn</code>是不是<code>nn.LSTM</code>，如果他只是一个普通的rnn，那就直接对其进行隐藏状态的初始化就可以了；如果他是LSTM的时候，它的隐形状态需要初始化为张量。</p> </li></ul> 
<pre><code>device = d2l.try_gpu()
net = RNNModel(rnn_layer, vocab_size=len(vocab))
net = net.to(device)
d2l.predict_ch8('time traveller', 10, net, vocab, device)
复制代码</code></pre> 
<p>输出：</p> 
<blockquote> 
 <p>'time travellerxsszsuxsss'</p> 
</blockquote> 
<p>在训练之前，我们先使用我们大号的模型来看一下它的预测结果。可以看出结果非常不好，因为他后边已经开始重复输出xss了。</p> 
<p><em>如果你的输出和我的输出不一样也没有关系。因为它初始化的时候是随机初始化。所以每次运行的结果可能是不同的。</em></p> 
<pre><code>num_epochs, lr = 500, 1
d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, device)
复制代码</code></pre> 
<p>对其进行训练。500个epoch，学习率设定为1。 这张代码的向量函数中还有一个可视化的函数，所以你在跑这张代码的过程中应该可以看到一个下降的过程图表。的那个坐标跑完了之后再运行下边这个预测输出的代码。</p> 
<pre><code>d2l.predict_ch8('time traveller', 10, net, vocab, device)
复制代码</code></pre> 
<p>输出：</p> 
<blockquote> 
 <p>'time travellerit s again'</p> 
</blockquote> 
<p>训练之后再看一下他预测生成的结果。至少比上面那一个靠谱很多。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/967d49295cbd2c57e25abe61d895c60c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">基于ESP8266的四旋翼无人机代码分享，该无人机可以爬墙哦</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/efc937471f14fc009c45c730bf3668b0/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">各linux操作系统查看内核版本命令</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>