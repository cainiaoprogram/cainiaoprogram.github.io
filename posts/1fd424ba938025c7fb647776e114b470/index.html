<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>python爬虫实战---豆瓣电影top250的电影信息抓取 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="python爬虫实战---豆瓣电影top250的电影信息抓取" />
<meta property="og:description" content="本文主要抓取豆瓣电影top250榜单里面的电影数据，提取的数据包括电影名称、电影的链接、电影的星级、电影引言、电影的评论 人数等。
导入包 from bs4 import BeautifulSoup as bs---进行网页解析 import requests---用于网页请求 import time---用于延长时间,防止过于快速抓取数据，封ip import re---正则表达式使用 import csv---数据存储到csv文件 网页分析 通过浏览器右键点击检查，选择netword和里面XHR刷新来看，我们要提取的数据并不是异步加载的，也不是通过其他加密方式加载的，可以直接把网页的源码爬取下来，在进行解析就可以。
网页结构解析 url的结构： https://movie.douban.com/top250?start=0&amp;filter=，里面的参数start就是对应着哪一页，相当于偏移量，且每次在原来的基础上加25，每次修改start的值就可以获取下一页的信息。
网页结构： 每一部电影的信息都在一个class属性为item的div盒子里面，通过BeautifulSoup的find_all(&#39;div&#39;,{&#39;class&#39;:&#39;item&#39;})来找到该页所有相关的盒子，class属性为hd的div盒子,包含电影链接、电影名称，class属性为start的div盒子,包含星级和评论人数，引言在 class属性为quote的p标签里面。
网页下载 # 获取一页数据 def get_one_page(offset): # 设置请求头 headers={ &#39;Referer&#39;:&#39;https://movie.douban.com/top250?start=0&amp;filter=&#39;, &#39;User-Agent&#39;:&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36&#39; } url=&#39;https://movie.douban.com/top250?start=&#39;&#43;str(offset)&#43;&#39;&amp;filter=&#39; try: time.sleep(1) response=requests.get(url,headers=headers) # 判断相应状态，200表示请求成功 if response.status_code == 200: return response.content except Exception as e: print(&#39;出错!&#39;) return None 网页解析 def parse_page(offset): response=get_one_page(offset) if response: # print(response." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/1fd424ba938025c7fb647776e114b470/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-09-01T23:10:44+08:00" />
<meta property="article:modified_time" content="2018-09-01T23:10:44+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">python爬虫实战---豆瓣电影top250的电影信息抓取</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>本文主要抓取豆瓣电影top250榜单里面的电影数据，提取的数据包括电影名称、电影的链接、电影的星级、电影引言、电影的评论 人数等。</p> 
<h3>导入包</h3> 
<h4>from bs4 import BeautifulSoup as bs---进行网页解析</h4> 
<h4>import requests---用于网页请求</h4> 
<h4>import time---用于延长时间,防止过于快速抓取数据，封ip</h4> 
<h4>import re---正则表达式使用</h4> 
<h4>import csv---数据存储到csv文件</h4> 
<h3>网页分析</h3> 
<p>通过浏览器右键点击检查，选择netword和里面XHR刷新来看，我们要提取的数据并不是异步加载的，也不是通过其他加密方式加载的，可以直接把网页的源码爬取下来，在进行解析就可以。</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/72/bb/yPXw8hNe_o.png"></p> 
<h3> 网页结构解析</h3> 
<h4> url的结构：</h4> 
<p><a href="https://movie.douban.com/top250?start=0&amp;filter=" rel="nofollow">https://movie.douban.com/top250?start=0&amp;filter=</a>，<span style="color:#7c79e5;">里面的参数start就是对应着哪一页，相当于偏移量，且每次在原来的基础上加25，每次修改start的值就可以获取下一页的信息。</span></p> 
<h4> 网页结构：</h4> 
<p><strong>每一部电影的信息都在一个</strong><span style="color:#f33b45;"><strong>class属性为item的div盒子</strong></span><strong>里面，通过</strong><span style="color:#f33b45;"><strong>BeautifulSoup的find_all('div',{'class':'item'})</strong></span><strong>来找到该页所有相关的盒子，</strong><span style="color:#f33b45;"><strong>class属性为hd的div盒子,包含电影链接、电影名称</strong></span><strong>，</strong><span style="color:#f33b45;"><strong>class属性为start的div盒子,包含星级和评论人数，引言在 class属性为quote的p标签里面。</strong></span></p> 
<p><span style="color:#f33b45;"><strong><img alt="" class="has" src="https://images2.imgbox.com/15/e3/DCGmsAz5_o.png"></strong></span></p> 
<h3>网页下载 </h3> 
<pre class="has"><code># 获取一页数据
def get_one_page(offset):
	# 设置请求头
	headers={
	'Referer':'https://movie.douban.com/top250?start=0&amp;filter=',
	'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'
	}
	url='https://movie.douban.com/top250?start='+str(offset)+'&amp;filter='
	try:
		time.sleep(1)
		response=requests.get(url,headers=headers)
		# 判断相应状态，200表示请求成功
		if response.status_code == 200:
			return response.content
	except Exception as e:
		print('出错!')
		return None</code></pre> 
<h3> 网页解析</h3> 
<pre class="has"><code>def parse_page(offset):
	response=get_one_page(offset)
	if response:
		# print(response.decode('utf-8'))
		html=bs(response.decode('utf-8'),'lxml')
		items=html.find_all('div',{'class':'item'})
		if items:
			for item in items:
				a=item.find('div',{'class':'hd'}).a
				s=item.find('div',{'class':'star'})
				bd=item.find('div',{'class':'bd'})
				# 链接
				href=a.attrs['href']
				# 名称
				title=a.span.get_text()
				# 引言
				quote=bd.find('p',{'class':'quote'}).span.get_text()
				# 星级
				star=s.find('span',{'class':'rating_num'}).get_text()
				# 评价人数
				conment_people=re.search(r'[0-9].*[^人评价&lt;/span&gt;]',str(s.contents[len(s.contents)-2])).group()
				yield{
				'title':title,
				'href':href,
				'star':star,
				'quote':quote,
				'number':conment_people
				}
		else:
			print('没有找到该节点')</code></pre> 
<h3>数据存储 </h3> 
<p>参数i的作用判断是否为第一次写进文件，作为标记，避免后面的多次写进的时候会出现多行的<span style="color:#f33b45;">名称  链接  星级  引言  评论人数，</span>去遍历生成器的数据，并把每部电影的信息存储到一个csvList列表里面去，再把该列表添加到datalist列表中去，每一个csvfile列表的数据就是csv文件里面的一行数据，最后再把datalist作为参数，传进writerows()方面。</p> 
<pre class="has"><code># 保存数据到csv文件里面
def save_csv_files(contents,i):
	"""
	newline='':去除换行，
	encoding='utf-8':指明编码格式，防止中文乱码
	"""
	with open('douban_top250.csv','a',encoding='utf-8',newline='')as f:
		# 将python中的字典转换为json格式，并对输出中文指定ensure_ascii=False
		csvFile=csv.writer(f)
		if i ==0:
			csvFile.writerow(['名称','链接','星级','引言','评论人数'])
		datalist=[]
		for data in contents:
			csvList=[]
			csvList.append(data['title'])
			csvList.append(data['href'])
			csvList.append(data['star'])
			csvList.append(data['quote'])
			csvList.append(data['number'])
			datalist.append(csvList)
		csvFile.writerows(datalist)</code></pre> 
<h3>总结 </h3> 
<ol><li>BeautifulSoup使用的优点臃肿，在解析网页的时候，可以考虑使用正则表达式去完成。</li><li>csv的文件操作不熟练，有几种方式写进csv文件应该要熟练。</li></ol> 
<h3>源码：</h3> 
<p><a href="" rel="nofollow">链接：https://pan.baidu.com/s/1LxSH2J7HA163mZNrhiWGRQ 密码：kzo1</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8ee42f07727897d0e492eb3d25088f21/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">select into ...from... 提示 Undeclared variable.....错误的解决办法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d01493c97519e96089ecea98666e942e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">大转盘抽奖的网页版和小程序版</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>