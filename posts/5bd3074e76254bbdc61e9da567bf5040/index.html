<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>动手学深度学习-线性回归的简单实现 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="动手学深度学习-线性回归的简单实现" />
<meta property="og:description" content="动手学深度学习-线性回归的简单实现 一、生成数据集二、读取数据集三、定义模型四、初始化模型参数五、定义损失函数六、定义优化算法七、训练 本节介绍如何使用深度学习框架实现线性回归模型。
一、生成数据集 import numpy as np import torch from torch.utils import data from d2l import torch as d2l true_w = torch.tensor([2,-3.4]) true_b = 4.2 # 调用d2l包中生成数据函数，features是X，一个1000行2列的张量 # labels是一个1000行一列的张量 也就是y features,labels = d2l.synthetic_data(true_w,true_b,1000) 二、读取数据集 调用框架中现有的API来读取数据。我们每次只选取部分数据集进行训练。将上面的features和labels作为API的参数进行传递，并通过数据迭代器指定batch_size。此外布尔值is_train表示是否希望数据迭代器对象在每个迭代周期内打乱数据。
def load_array(data_arrays,batch_size,is_train=True): &#34;&#34;&#34;构造一个pytorch数据迭代器&#34;&#34;&#34; dataset = data.TensorDataset(*data_arrays) # TensorDataset对张量进行打包 # dataloader进行数据封装 return data.DataLoader(dataset,batch_size,shuffle=is_train) batch_size = 10 # 返回的是一个迭代器 每次加载batch_size批量的数据 data_iter = load_array((features,labels),batch_size) next(iter(data_iter)) [tensor([[ 0.4959, 0.8714], [ 0.7823, -1.7682], [-0.1917, 0.1726], [-2.0061, -1.2517], [-0.2063, -0." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/5bd3074e76254bbdc61e9da567bf5040/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-04-05T22:38:16+08:00" />
<meta property="article:modified_time" content="2022-04-05T22:38:16+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">动手学深度学习-线性回归的简单实现</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>动手学深度学习-线性回归的简单实现</h4> 
 <ul><li><ul><li><a href="#_5" rel="nofollow">一、生成数据集</a></li><li><a href="#_23" rel="nofollow">二、读取数据集</a></li><li><a href="#_69" rel="nofollow">三、定义模型</a></li><li><a href="#_83" rel="nofollow">四、初始化模型参数</a></li><li><a href="#_94" rel="nofollow">五、定义损失函数</a></li><li><a href="#_99" rel="nofollow">六、定义优化算法</a></li><li><a href="#_108" rel="nofollow">七、训练</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<p>本节介绍如何使用深度学习框架实现线性回归模型。</p> 
<h3><a id="_5"></a>一、生成数据集</h3> 
<pre><code class="prism language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils <span class="token keyword">import</span> data
<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l

true_w <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">3.4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
true_b <span class="token operator">=</span> <span class="token number">4.2</span>

<span class="token comment"># 调用d2l包中生成数据函数，features是X，一个1000行2列的张量</span>
<span class="token comment"># labels是一个1000行一列的张量 也就是y</span>
features<span class="token punctuation">,</span>labels <span class="token operator">=</span> d2l<span class="token punctuation">.</span>synthetic_data<span class="token punctuation">(</span>true_w<span class="token punctuation">,</span>true_b<span class="token punctuation">,</span><span class="token number">1000</span><span class="token punctuation">)</span>

</code></pre> 
<h3><a id="_23"></a>二、读取数据集</h3> 
<p>调用框架中现有的API来读取数据。我们每次只选取部分数据集进行训练。将上面的features和labels作为API的参数进行传递，并通过数据迭代器指定batch_size。此外布尔值is_train表示是否希望数据迭代器对象在每个迭代周期内打乱数据。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">load_array</span><span class="token punctuation">(</span>data_arrays<span class="token punctuation">,</span>batch_size<span class="token punctuation">,</span>is_train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""构造一个pytorch数据迭代器"""</span>
    dataset <span class="token operator">=</span> data<span class="token punctuation">.</span>TensorDataset<span class="token punctuation">(</span><span class="token operator">*</span>data_arrays<span class="token punctuation">)</span>  <span class="token comment"># TensorDataset对张量进行打包</span>
    
    <span class="token comment"># dataloader进行数据封装 </span>
    <span class="token keyword">return</span> data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span>batch_size<span class="token punctuation">,</span>shuffle<span class="token operator">=</span>is_train<span class="token punctuation">)</span>

batch_size <span class="token operator">=</span> <span class="token number">10</span> 

<span class="token comment"># 返回的是一个迭代器  每次加载batch_size批量的数据</span>
data_iter <span class="token operator">=</span> load_array<span class="token punctuation">(</span><span class="token punctuation">(</span>features<span class="token punctuation">,</span>labels<span class="token punctuation">)</span><span class="token punctuation">,</span>batch_size<span class="token punctuation">)</span>

<span class="token builtin">next</span><span class="token punctuation">(</span><span class="token builtin">iter</span><span class="token punctuation">(</span>data_iter<span class="token punctuation">)</span><span class="token punctuation">)</span>


<span class="token punctuation">[</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.4959</span><span class="token punctuation">,</span>  <span class="token number">0.8714</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
         <span class="token punctuation">[</span> <span class="token number">0.7823</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.7682</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
         <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.1917</span><span class="token punctuation">,</span>  <span class="token number">0.1726</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
         <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2.0061</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.2517</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
         <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.2063</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1480</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
         <span class="token punctuation">[</span> <span class="token number">0.8134</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.9006</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
         <span class="token punctuation">[</span> <span class="token number">0.1648</span><span class="token punctuation">,</span>  <span class="token number">0.4840</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
         <span class="token punctuation">[</span> <span class="token number">0.4924</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0704</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
         <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.6901</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.2232</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
         <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.2761</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.3795</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">2.2264</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
         <span class="token punctuation">[</span><span class="token number">11.7656</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
         <span class="token punctuation">[</span> <span class="token number">3.2199</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
         <span class="token punctuation">[</span> <span class="token number">4.4502</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
         <span class="token punctuation">[</span> <span class="token number">4.2977</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
         <span class="token punctuation">[</span> <span class="token number">8.8826</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
         <span class="token punctuation">[</span> <span class="token number">2.8716</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
         <span class="token punctuation">[</span> <span class="token number">5.4167</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
         <span class="token punctuation">[</span> <span class="token number">3.5887</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
         <span class="token punctuation">[</span> <span class="token number">8.3447</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span>

</code></pre> 
<p>关于pytorch的TensorDataset：对张量tensor进行打包。</p> 
<h3><a id="_69"></a>三、定义模型</h3> 
<p>对于标准深度学习模型，我们可以使用框架预先定义好的层，我们只需要关注那些层用来构建模型，而不必关注层的实现细节。我们首先定义一个模型变量net,它是一个Sequential类的实例。Sequential类将多各层串联到一起。当给定输入数据时，Sequential实例将数据传入到第一层，然后将第一层的输出作为第二层的输入。但是线性回归模型只有一层，称之为全连接层。</p> 
<p><strong>在Pytorch中，全连接层在Linear类中定义。将两个参数传递到nn.Linear中。第一个指定输入特征形状，即2,第二个指定输出特征形状，输出特征形状为单个标量，因此为1。</strong></p> 
<pre><code class="prism language-python"><span class="token comment"># 定义模型 全连接层</span>
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token comment"># nn 是神经网络的缩写</span>
net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="_83"></a>四、初始化模型参数</h3> 
<p>在使用net之前，需要初始化模型参数。一般指定每一个权重参数w从均值为0，标准差为0.01的正态分布中随机采样，偏置b = 0</p> 
<p>由于线性回归是单层网络结构，所以我们使用net[0]选择网络中的第一个图层，然后使用weight.data和bias.data方法访问参数。然后再设定参数，那么权重w就使用normal(0,0.01)进行初始化（normal中的两个参数：均值，标准差）。</p> 
<pre><code class="prism language-python">net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0.01</span><span class="token punctuation">)</span>
net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data<span class="token punctuation">.</span>fill_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="_94"></a>五、定义损失函数</h3> 
<p>计算均方误差使用的是MSELoss类，也成为<strong>平方L2范数</strong>。默认情况下，它返回所有样本损失的平均值。</p> 
<h3><a id="_99"></a>六、定义优化算法</h3> 
<p><strong>小批量随机梯度下降算法是一种优化神经网络的标准工具</strong>，指定需要优化的参数，然后设置lr(学习率).</p> 
<pre><code class="prism language-python">trainer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr <span class="token operator">=</span> <span class="token number">0.03</span><span class="token punctuation">)</span>

</code></pre> 
<h3><a id="_108"></a>七、训练</h3> 
<p>在每一个迭代周期中，我们将完整遍历一次数据集（train_data）,不停地从中获取一个小批量的输入和相应的标签。对于每一个小批量，在进行如下步骤：</p> 
<ul><li>铜鼓哦调用net(X)生成预测结果，然后与标签，计算损失函数（前向传播）。</li><li>进行反向传播计算梯度。</li><li>通过调用优化器更新模型参数。梯度下降法</li></ul> 
<pre><code class="prism language-python">num_epochs <span class="token operator">=</span> <span class="token number">3</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> data_iter<span class="token punctuation">:</span>
        l <span class="token operator">=</span> loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">)</span> <span class="token punctuation">,</span>y<span class="token punctuation">)</span>
        trainer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        trainer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
    l <span class="token operator">=</span> loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>features<span class="token punctuation">)</span><span class="token punctuation">,</span> labels<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'epoch </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">, loss </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>l<span class="token punctuation">:</span><span class="token format-spec">f</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>

epoch <span class="token number">1</span><span class="token punctuation">,</span> loss <span class="token number">0.000261</span>
epoch <span class="token number">2</span><span class="token punctuation">,</span> loss <span class="token number">0.000097</span>
epoch <span class="token number">3</span><span class="token punctuation">,</span> loss <span class="token number">0.000097</span>

</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/bb2f6da072af6476f9d2d25013615685/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">接入alipay-sdk</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/448beaab15ef176fdb7152f5fb66e003/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">C#编程时，WinForm中PictureBox.Image加载大尺寸图片，显示实时照片会造成内存溢出，解决方法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>