<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>论文解读：Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentati - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="论文解读：Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentati" />
<meta property="og:description" content="论文解读：Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation 一、动机 Knowledge-intensive任务通常需要借助外部的资源来回答问题，此时需要有一个retrieve模型来完成外部知识的检索；最近大模型（例如ChatGPT）可以解决此类问题，其在大量的文档数据上进行过预训练，并将大量的知识学习在参数里；然而现有的大模型依然存在一个未知的问题，即大模型是否有能力察觉到他们事实知识认知边界；本文则致力于研究检索式增强对于大模型生成能力的影响进行分析。主要关注三个问题：大模型察觉到它的事实知识边界程度、检索增强对大模型生成有什么影响、具有不同特征的辅助文档如何影响LLM。 二、问题定义 开放域问答中，给定一个问题 q q q以及一个文档库 D = { d i } i = 1 m \mathcal{D}=\{d_i\}_{i=1}^m D={di​}i=1m​，目标是给定一个回答 a a a。
先前的方法是通过一个retriever检索文档库，并获得与问题相关的文档，随后使用机器阅读理解的方式回答问题。大模型可以直接利用模型本身的知识来回答问题，免于使用外部知识，即有： a = f L L M ( p , q ) a=f_{LLM}(p, q) a=fLLM​(p,q)，其中 p p p表示prompt。
如果使用检索增强的方式，则为 a = f L L M ( p , q , L ) a=f_{LLM}(p, q, \mathcal{L}) a=fLLM​(p,q,L)，其中 L \mathcal{L} L表示检索得到的support document。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/2755251ce4fbc51b8025c78f7e61c82c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-29T23:18:20+08:00" />
<meta property="article:modified_time" content="2023-07-29T23:18:20+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">论文解读：Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentati</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="Investigating_the_Factual_Knowledge_Boundary_of_Large_Language_Models_with_Retrieval_Augmentation_0"></a>论文解读：Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation</h2> 
<p><img src="https://images2.imgbox.com/a6/35/Vqlu6kEN_o.png" alt="image.png"></p> 
<h3><a id="_2"></a>一、动机</h3> 
<ul><li>Knowledge-intensive任务通常需要借助外部的资源来回答问题，此时需要有一个retrieve模型来完成外部知识的检索；</li><li>最近大模型（例如ChatGPT）可以解决此类问题，其在大量的文档数据上进行过预训练，并将大量的知识学习在参数里；</li><li>然而现有的大模型依然存在一个未知的问题，即大模型是否有能力察觉到他们事实知识认知边界；</li><li>本文则致力于研究检索式增强对于大模型生成能力的影响进行分析。主要关注三个问题：大模型察觉到它的事实知识边界程度、检索增强对大模型生成有什么影响、具有不同特征的辅助文档如何影响LLM。</li></ul> 
<h3><a id="_9"></a>二、问题定义</h3> 
<p>开放域问答中，给定一个问题<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         q 
        
       
      
        q 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span></span></span></span></span>以及一个文档库<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         D 
        
       
         = 
        
       
         { 
        
        
        
          d 
         
        
          i 
         
        
        
        
          } 
         
         
         
           i 
          
         
           = 
          
         
           1 
          
         
        
          m 
         
        
       
      
        \mathcal{D}=\{d_i\}_{i=1}^m 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathcal" style="margin-right: 0.0278em;">D</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.0087em; vertical-align: -0.2587em;"></span><span class="mopen">{<!-- --></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.6644em;"><span class="" style="top: -2.4413em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2587em;"><span class=""></span></span></span></span></span></span></span></span></span></span>，目标是给定一个回答<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         a 
        
       
      
        a 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">a</span></span></span></span></span>。<br> 先前的方法是通过一个retriever检索文档库，并获得与问题相关的文档，随后使用机器阅读理解的方式回答问题。大模型可以直接利用模型本身的知识来回答问题，免于使用外部知识，即有：<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         a 
        
       
         = 
        
        
        
          f 
         
         
         
           L 
          
         
           L 
          
         
           M 
          
         
        
       
         ( 
        
       
         p 
        
       
         , 
        
       
         q 
        
       
         ) 
        
       
      
        a=f_{LLM}(p, q) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em;"><span class="" style="top: -2.55em; margin-left: -0.1076em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">LL</span><span class="mord mathnormal mtight" style="margin-right: 0.109em;">M</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span><span class="mclose">)</span></span></span></span></span>，其中<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         p 
        
       
      
        p 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord mathnormal">p</span></span></span></span></span>表示prompt。<br> 如果使用检索增强的方式，则为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         a 
        
       
         = 
        
        
        
          f 
         
         
         
           L 
          
         
           L 
          
         
           M 
          
         
        
       
         ( 
        
       
         p 
        
       
         , 
        
       
         q 
        
       
         , 
        
       
         L 
        
       
         ) 
        
       
      
        a=f_{LLM}(p, q, \mathcal{L}) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em;"><span class="" style="top: -2.55em; margin-left: -0.1076em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">LL</span><span class="mord mathnormal mtight" style="margin-right: 0.109em;">M</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathcal">L</span><span class="mclose">)</span></span></span></span></span>，其中<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         L 
        
       
      
        \mathcal{L} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathcal">L</span></span></span></span></span>表示检索得到的support document。</p> 
<p>为了能够探测大模型的知识，需要先设计指令来让大模型完成相应的测试任务。</p> 
<h5><a id="Instruction_15"></a>Instruction设计</h5> 
<p>（1）QA模式<br> 提出两种类型：</p> 
<ul><li>Normal Setting：即知让大模型根据参数知识来回答问题。指令如下所示：</li></ul> 
<blockquote> 
 <p>Answer the following question based on your internal knowledge with one or few words. · · ·</p> 
</blockquote> 
<ul><li>检索式：让大模型结合参数内的知识和检索到的外部知识一起进行回答。指令如下所示：</li></ul> 
<blockquote> 
 <p>Given the following information: · · · Answer the following question based on the given information or your internal knowledge with one or few words without the source. · · ·</p> 
</blockquote> 
<p>（2）判别模式<br> 另一种方法则是让大模型去做判定。也存在两种模式：</p> 
<ul><li>Priori judgement：在大模型做出回答之前，先让其判断是否能够回答该问题。指令如下所示：</li></ul> 
<blockquote> 
 <p>Are you sure to accurately answer the following question based on your internal knowledge, if yes, you should give a short answer with one or few words, if no, you should answer ‘Unknown’. · · · ”</p> 
</blockquote> 
<ul><li>Posterior judgement：在大模型回答之后，再让大模型来判断自己生成的结果是否准确。指令如下所示：</li></ul> 
<blockquote> 
 <p>Can you judge if the following answer about the question is correct based on your internal knowl-edge, if yes, you should answer True or False, if no, you should answer ‘Unknown’. · · · ”</p> 
</blockquote> 
<h3><a id="_34"></a>三、实验</h3> 
<h5><a id="_35"></a>数据集</h5> 
<p>Natural Question、TriviaQA、HotpotQA</p> 
<h5><a id="_37"></a>评价指标</h5> 
<p>QA模式：EM、F1<br> 判别模式：</p> 
<ul><li>Give-up：大模型放弃回答的问题的占比，可以估算为大模型回答的置信度；</li><li>Right/G：大模型放弃回答，但实际上能够正确回答的概率；</li><li>Right/NotG：大模型没有放弃回答，且实际上能够正确回答的概率；</li><li>Eval-Right：大模型评估其回答是正确的问题的比例；</li><li>Eval-ACC：大模型对答案的评估（正确或错误）与事实相符的问题的百分比。</li></ul> 
<h5><a id="_47"></a>检索器</h5> 
<p>采用下面三种进行检索：<br> <img src="https://images2.imgbox.com/13/cf/UjTIDh15_o.png" alt="image.png"><br> ChatGPT表示使用ChatGPT插件来检索相关的文档。</p> 
<h5><a id="_51"></a>模型</h5> 
<p>对比text-davinci-003和gpt3.5-turbo</p> 
<h5><a id="_54"></a>实验与结论</h5> 
<p><strong>（1）大模型本身很难知道自己的知识边界，且会过度自信。</strong><br> <img src="https://images2.imgbox.com/12/40/Xh5HYbzJ_o.png" alt="image.png"><br> <strong>（2）LLM无法充分利用自己所拥有的知识，而检索增强可以作为LLM的宝贵知识补充。</strong><br> <img src="https://images2.imgbox.com/19/07/0rhmKk4S_o.png" alt="image.png"><br> 可以知道当用高质量的文档检索器后，大模型的放弃回答的比例下降了很多，说明给定外部知识大模型是可以有效回答问题的。<br> <strong>（3）检索增强提高了大模型感知事实知识边界的能力。</strong><br> 如上表可知给定检索的知识，大模型priori judgement的准确率很高。<br> <strong>（4）更多的文档可以进一步提高检索增强大模型的性能。</strong><br> <img src="https://images2.imgbox.com/7e/79/LEsXNZ74_o.png" alt="image.png"><br> 当检索的文档越多时，EM指标会呈现上升趋势，Given Up率也会下降很多。<br> <strong>（5）检索增强可以改变大模型对不同查询类别的偏好。</strong><br> <img src="https://images2.imgbox.com/f6/93/mI1jlF8D_o.png" alt="image.png"><br> <strong>（6）当提供更高质量的检索文档时，大模型可以表现出很强的质量保证能力和知识边界感知能力。</strong><br> <img src="https://images2.imgbox.com/c1/ea/xLAXIvFP_o.png" alt="image.png"><br> 当检索的文档是准确时（Golden），其不论是EM、F1指标，还是Eval-Acc、Eval-Right都很高，说明高质量的检索文档可以提升模型的知识边界感知能力的。<br> （<strong>7）大模型会依赖于检索到的文档。</strong><br> <strong>（8）大模型对检索到的文档的置信度和依赖程度取决于问题和文档之间的相关性。</strong><br> 上表中的几个检索方式的对应Question与检索文档的相关性排序为：golden doc- uments &gt; retrieved documents &gt; highly-related incorrect documents &gt; weakly-related incorrect documents &gt; random incorrect documents.<br> 可知EM、F1等指标也几乎呈现此顺序。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b5910073635c640ef07b74ffbee1619f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">如何启动Redis，前台启动，后台启动和开机自启</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f6b1a24c769bd5eed2b90676ec53939c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">go-zero学习 — 进阶</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>