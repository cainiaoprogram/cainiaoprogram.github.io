<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>论文及代码详解——Restormer - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="论文及代码详解——Restormer" />
<meta property="og:description" content="文章目录 论文详解Overall pipelineMulti-Dconv Head Transposed AttentionGated-Dconv Feed-Forward Network 代码详解 论文：《Restormer: Efﬁcient Transformer for High-Resolution Image Restoration》
代码：https://github.com/swz30/Restormer
论文详解 本文的目标是开发一个高效的Transformer模型，该模型可以处理高分辨率的图像，用于恢复任务。为了缓解计算瓶颈，我们引入了multi-head SA layer的关键设计和一个比单尺度网络Swin-IR的计算需求更小的multi-scale hierarchical module。
我们首先展示了我们的Restormer architecture的整体结构(见图2)。
然后我们描述了提出的Transformer Block的核心组件:
(a) multi-Dconv head transposed attention (MDTA)
(b)gated-Dconv feed-forward network (GDFN)
最后，我们提供详细的渐进训练方案，以有效地学习图像统计。
Overall pipeline 给定低质量图像 I ∈ R H × W × 3 I∈R^{H×W×3} I∈RH×W×3, Restoremer首先进行卷积，得到底层特征嵌入 F 0 ∈ R H × W × C F_0∈R^{H×W×C} F0​∈RH×W×C; 其中 H×W为空间维数，C为通道数。接下来，这些浅层特征 F 0 F_0 F0​经过一个4级对称encoder-decoder，转化为深层特征 F d ∈ R H × W × 2 C F_d∈R^{H×W×2C} Fd​∈RH×W×2C。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/196f0defe0e8a57c69ecc106a57e7b39/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-08-22T17:32:39+08:00" />
<meta property="article:modified_time" content="2023-08-22T17:32:39+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">论文及代码详解——Restormer</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#_5" rel="nofollow">论文详解</a></li><li><ul><li><a href="#Overall_pipeline_13" rel="nofollow">Overall pipeline</a></li><li><a href="#MultiDconv_Head_Transposed_Attention_26" rel="nofollow">Multi-Dconv Head Transposed Attention</a></li><li><a href="#GatedDconv_FeedForward_Network_47" rel="nofollow">Gated-Dconv Feed-Forward Network</a></li></ul> 
  </li><li><a href="#_55" rel="nofollow">代码详解</a></li></ul> 
</div> 
<p></p> 
<blockquote> 
 <p>论文：<a href="https://arxiv.org/pdf/2111.09881.pdf" rel="nofollow">《Restormer: Efﬁcient Transformer for High-Resolution Image Restoration》</a><br> 代码：<a href="https://github.com/swz30/Restormer">https://github.com/swz30/Restormer</a></p> 
</blockquote> 
<h2><a id="_5"></a>论文详解</h2> 
<p>本文的目标是开发一个高效的Transformer模型，该模型可以处理高分辨率的图像，用于恢复任务。为了缓解计算瓶颈，我们引入了multi-head SA layer的关键设计和一个比单尺度网络Swin-IR的计算需求更小的multi-scale hierarchical module。<br> 我们首先展示了我们的Restormer architecture的整体结构(见图2)。<br> 然后我们描述了提出的Transformer Block的核心组件:<br> (a) <code>multi-Dconv head transposed attention (MDTA)</code><br> (b)<code>gated-Dconv feed-forward network (GDFN)</code><br> 最后，我们提供详细的渐进训练方案，以有效地学习图像统计。<br> <img src="https://images2.imgbox.com/8e/7a/C43HvT8b_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="Overall_pipeline_13"></a>Overall pipeline</h3> 
<p>给定低质量图像<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         I 
        
       
         ∈ 
        
        
        
          R 
         
         
         
           H 
          
         
           × 
          
         
           W 
          
         
           × 
          
         
           3 
          
         
        
       
      
        I∈R^{H×W×3} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7224em; vertical-align: -0.0391em;"></span><span class="mord mathnormal" style="margin-right: 0.0785em;">I</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0077em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0813em;">H</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right: 0.1389em;">W</span><span class="mbin mtight">×</span><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span></span>, Restoremer首先进行卷积，得到底层特征嵌入<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          F 
         
        
          0 
         
        
       
         ∈ 
        
        
        
          R 
         
         
         
           H 
          
         
           × 
          
         
           W 
          
         
           × 
          
         
           C 
          
         
        
       
      
        F_0∈R^{H×W×C} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0077em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0813em;">H</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right: 0.1389em;">W</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right: 0.0715em;">C</span></span></span></span></span></span></span></span></span></span></span></span></span>; 其中 H×W为空间维数，C为通道数。接下来，这些浅层特征<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          F 
         
        
          0 
         
        
       
      
        F_0 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>经过一个4级对称encoder-decoder，转化为深层特征<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          F 
         
        
          d 
         
        
       
         ∈ 
        
        
        
          R 
         
         
         
           H 
          
         
           × 
          
         
           W 
          
         
           × 
          
         
           2 
          
         
           C 
          
         
        
       
      
        F_d∈R^{H×W×2C} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0077em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0813em;">H</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right: 0.1389em;">W</span><span class="mbin mtight">×</span><span class="mord mtight">2</span><span class="mord mathnormal mtight" style="margin-right: 0.0715em;">C</span></span></span></span></span></span></span></span></span></span></span></span></span>。</p> 
<p>encoder-decoder 的每个层都包含多个Transformer Block，其中块的数量从顶部到底部逐渐增加，以保持效率。从高分辨率输入开始，Encoder 分层地减少空间大小，同时扩大信道容量。该Decoder以低分辨率潜在特征<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          F 
         
        
          l 
         
        
       
         ∈ 
        
        
        
          R 
         
         
          
          
            H 
           
          
            8 
           
          
         
           × 
          
          
          
            W 
           
          
            8 
           
          
         
           × 
          
         
           8 
          
         
           C 
          
         
        
       
      
        F_l∈R^ {\frac{H}{8} ×\frac{W}{8} ×8C} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0197em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.9735em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0077em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9735em;"><span class="" style="top: -3.363em; margin-right: 0.05em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8721em;"><span class="" style="top: -2.656em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">8</span></span></span></span><span class="" style="top: -3.2255em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line mtight" style="border-bottom-width: 0.049em;"></span></span><span class="" style="top: -3.384em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0813em;">H</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.344em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8721em;"><span class="" style="top: -2.656em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">8</span></span></span></span><span class="" style="top: -3.2255em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line mtight" style="border-bottom-width: 0.049em;"></span></span><span class="" style="top: -3.384em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.1389em;">W</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.344em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span><span class="mbin mtight">×</span><span class="mord mtight">8</span><span class="mord mathnormal mtight" style="margin-right: 0.0715em;">C</span></span></span></span></span></span></span></span></span></span></span></span></span>为输入，并逐步恢复高分辨率表示。</p> 
<p>对于特征下采样和上采样，我们分别采用了pixel-unshuffle和pixel-shuffle操作。</p> 
<p>为了帮助恢复过程，encoder feature通过skip connections（Unet中提出的操作）连接到decoder freature。连接操作之后是1×1卷积，以在所有levels上减少通道(减半)，除了最上面的levels。</p> 
<p>在level-1，我们让Transformer Block将编码器的低级图像特征与解码器的高级特征聚合在一起。这种方法有利于在恢复后的图像中保持精细的结构和纹理细节。然后，在高空间分辨率的细化阶段进一步丰富深度特征<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          F 
         
        
          d 
         
        
       
      
        F_d 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>。</p> 
<p>这些设计选择产生了质量上的改善，我们将在实验部分(第4节)中看到。最后，对精化的特征进行卷积层处理，生成残差图像<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         R 
        
       
         ∈ 
        
        
        
          R 
         
         
         
           H 
          
         
           × 
          
         
           W 
          
         
           × 
          
         
           3 
          
         
        
       
      
        R∈R^{H×W×3} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7224em; vertical-align: -0.0391em;"></span><span class="mord mathnormal" style="margin-right: 0.0077em;">R</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0077em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0813em;">H</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right: 0.1389em;">W</span><span class="mbin mtight">×</span><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span></span>，在残差图像上加上退化图像，得到恢复后的图像: <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          I 
         
        
          ^ 
         
        
       
         = 
        
       
         I 
        
       
         + 
        
       
         R 
        
       
      
        \hat I= I +R 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.9468em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathnormal" style="margin-right: 0.0785em;">I</span></span><span class="" style="top: -3.2523em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.1389em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.7667em; vertical-align: -0.0833em;"></span><span class="mord mathnormal" style="margin-right: 0.0785em;">I</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.0077em;">R</span></span></span></span></span>。接下来，我们将介绍Transformer模块的模块。</p> 
<h3><a id="MultiDconv_Head_Transposed_Attention_26"></a>Multi-Dconv Head Transposed Attention</h3> 
<p>Transformer的主要计算开销来自于self-attention 层。在传统的SA中，key-query dot - product交互的时间和存储复杂度随输入的空间分辨率(即W×H) 像素图像的<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         O 
        
       
         ( 
        
        
        
          W 
         
        
          2 
         
        
        
        
          H 
         
        
          2 
         
        
       
         ) 
        
       
      
        O(W^2H^2) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0641em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0813em;">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>呈二次增长。</p> 
<p>因此，将SA应用于大多数涉及高分辨率图像的图像恢复任务是不可行的。为了缓解这个问题，我们提出了MDTA，如图2(a)所示，它具有线性复杂度。关键因素是跨通道应用SA，而不是空间维度，即<font color="LightSeaGreen">计算跨通道的cross-covariance，以生成隐式编码全局上下文的注意映射</font> 作为MDTA的另一个重要组成部分，在计算feature covariance生成global attention map之前，我们<font color="LightSeaGreen">引入depth-wise convolutions来强调local context。</font><br> <img src="https://images2.imgbox.com/be/ed/VeHgUjPm_o.png" alt="在这里插入图片描述" width="600"><br> 从层归一化后的张量<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         Y 
        
       
         ∈ 
        
        
        
          R 
         
         
          
          
            H 
           
          
            ^ 
           
          
         
           × 
          
          
          
            W 
           
          
            ^ 
           
          
         
           × 
          
          
          
            C 
           
          
            ^ 
           
          
         
        
       
      
        Y∈R^{\hat H×\hat W×\hat C} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7224em; vertical-align: -0.0391em;"></span><span class="mord mathnormal" style="margin-right: 0.2222em;">Y</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.0257em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0077em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 1.0257em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord accent mtight"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -2.7em;"><span class="pstrut" style="height: 2.7em;"></span><span class="mord mathnormal mtight" style="margin-right: 0.0813em;">H</span></span><span class="" style="top: -2.9523em;"><span class="pstrut" style="height: 2.7em;"></span><span class="accent-body" style="left: -0.1944em;"><span class="mord mtight">^</span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord accent mtight"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -2.7em;"><span class="pstrut" style="height: 2.7em;"></span><span class="mord mathnormal mtight" style="margin-right: 0.1389em;">W</span></span><span class="" style="top: -2.9523em;"><span class="pstrut" style="height: 2.7em;"></span><span class="accent-body" style="left: -0.25em;"><span class="mord mtight">^</span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord accent mtight"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -2.7em;"><span class="pstrut" style="height: 2.7em;"></span><span class="mord mathnormal mtight" style="margin-right: 0.0715em;">C</span></span><span class="" style="top: -2.9523em;"><span class="pstrut" style="height: 2.7em;"></span><span class="accent-body" style="left: -0.1667em;"><span class="mord mtight">^</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>中，我们的MDTA首先生成查询(Q)、键(K)和值(V) projection，丰富了local context。</p> 
<p><font color="LightSeaGreen">它是通过应用1×1卷积来聚合pixel-wise cross-channel context，然后使用3×3 depth-wise convolution 来编码channel-wise spatial context，</font>生成了<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         Q 
        
       
         = 
        
        
        
          W 
         
        
          d 
         
        
          Q 
         
        
        
        
          W 
         
        
          p 
         
        
          Q 
         
        
       
         Y 
        
       
         , 
        
       
         K 
        
       
         = 
        
        
        
          W 
         
        
          d 
         
        
          K 
         
        
        
        
          W 
         
        
          p 
         
        
          K 
         
        
       
         Y 
        
       
          and  
        
       
         V 
        
       
         = 
        
        
        
          W 
         
        
          d 
         
        
          V 
         
        
        
        
          W 
         
        
          p 
         
        
          V 
         
        
       
         Y 
        
       
      
        \mathbf{Q}=W_d^Q W_p^Q \mathbf{Y}, \mathbf{K}=W_d^K W_p^K \mathbf{Y} \text { and } \mathbf{V}=W_d^V W_p^V \mathbf{Y} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8805em; vertical-align: -0.1944em;"></span><span class="mord mathbf">Q</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.3423em; vertical-align: -0.3831em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.9592em;"><span class="" style="top: -2.3987em; margin-left: -0.1389em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span><span class="" style="top: -3.1809em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.3013em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8413em;"><span class="" style="top: -2.453em; margin-left: -0.1389em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.3831em;"><span class=""></span></span></span></span></span></span><span class="mord mathbf" style="margin-right: 0.0288em;">Y</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathbf">K</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.2244em; vertical-align: -0.3831em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8413em;"><span class="" style="top: -2.4169em; margin-left: -0.1389em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0715em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2831em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8413em;"><span class="" style="top: -2.453em; margin-left: -0.1389em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0715em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.3831em;"><span class=""></span></span></span></span></span></span><span class="mord mathbf" style="margin-right: 0.0288em;">Y</span><span class="mord text"><span class="mord"> and </span></span><span class="mord mathbf" style="margin-right: 0.016em;">V</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.2244em; vertical-align: -0.3831em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8413em;"><span class="" style="top: -2.4169em; margin-left: -0.1389em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.2222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2831em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8413em;"><span class="" style="top: -2.453em; margin-left: -0.1389em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.2222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.3831em;"><span class=""></span></span></span></span></span></span><span class="mord mathbf" style="margin-right: 0.0288em;">Y</span></span></span></span></span>。 其中<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          W 
         
        
          p 
         
        
       
         ( 
        
       
         . 
        
       
         ) 
        
       
      
        W_p(.) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0361em; vertical-align: -0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">.</span><span class="mclose">)</span></span></span></span></span> 是 1×1 point-wise convolution，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          W 
         
        
          d 
         
        
       
         ( 
        
       
         . 
        
       
         ) 
        
       
      
        W_d(.) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">.</span><span class="mclose">)</span></span></span></span></span>是3×3 depth-wise convolution。我们在网络中使用bias-free convolutional。</p> 
<p>接下来，我们<font color="LightSeaGreen">对query和key的projections进行reshape，使它们的dot-product interaction生成一个大小为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           R 
          
          
           
           
             C 
            
           
             ^ 
            
           
          
            × 
           
           
           
             C 
            
           
             ^ 
            
           
          
         
        
       
         R^{\hat C×\hat C} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0257em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0077em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 1.0257em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord accent mtight"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -2.7em;"><span class="pstrut" style="height: 2.7em;"></span><span class="mord mathnormal mtight" style="margin-right: 0.0715em;">C</span></span><span class="" style="top: -2.9523em;"><span class="pstrut" style="height: 2.7em;"></span><span class="accent-body" style="left: -0.1667em;"><span class="mord mtight">^</span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord accent mtight"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -2.7em;"><span class="pstrut" style="height: 2.7em;"></span><span class="mord mathnormal mtight" style="margin-right: 0.0715em;">C</span></span><span class="" style="top: -2.9523em;"><span class="pstrut" style="height: 2.7em;"></span><span class="accent-body" style="left: -0.1667em;"><span class="mord mtight">^</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>的Transposed-Attention map (A)</font>，而不是大小为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          R 
         
         
          
          
            H 
           
          
            ^ 
           
          
          
          
            W 
           
          
            ^ 
           
          
         
           × 
          
          
          
            H 
           
          
            ^ 
           
          
          
          
            W 
           
          
            ^ 
           
          
         
        
       
      
        R^{\hat H\hat W×\hat H \hat W} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0257em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0077em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 1.0257em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord accent mtight"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -2.7em;"><span class="pstrut" style="height: 2.7em;"></span><span class="mord mathnormal mtight" style="margin-right: 0.0813em;">H</span></span><span class="" style="top: -2.9523em;"><span class="pstrut" style="height: 2.7em;"></span><span class="accent-body" style="left: -0.1944em;"><span class="mord mtight">^</span></span></span></span></span></span></span><span class="mord accent mtight"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -2.7em;"><span class="pstrut" style="height: 2.7em;"></span><span class="mord mathnormal mtight" style="margin-right: 0.1389em;">W</span></span><span class="" style="top: -2.9523em;"><span class="pstrut" style="height: 2.7em;"></span><span class="accent-body" style="left: -0.25em;"><span class="mord mtight">^</span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord accent mtight"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -2.7em;"><span class="pstrut" style="height: 2.7em;"></span><span class="mord mathnormal mtight" style="margin-right: 0.0813em;">H</span></span><span class="" style="top: -2.9523em;"><span class="pstrut" style="height: 2.7em;"></span><span class="accent-body" style="left: -0.1944em;"><span class="mord mtight">^</span></span></span></span></span></span></span><span class="mord accent mtight"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -2.7em;"><span class="pstrut" style="height: 2.7em;"></span><span class="mord mathnormal mtight" style="margin-right: 0.1389em;">W</span></span><span class="" style="top: -2.9523em;"><span class="pstrut" style="height: 2.7em;"></span><span class="accent-body" style="left: -0.25em;"><span class="mord mtight">^</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>的大型regular attention map。</p> 
<p>总体而言，MDTA流程定义为:</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           X 
          
         
           ^ 
          
         
        
          = 
         
         
         
           W 
          
         
           p 
          
         
        
          A 
         
        
          t 
         
        
          t 
         
        
          e 
         
        
          n 
         
        
          t 
         
        
          i 
         
        
          o 
         
        
          n 
         
        
          ( 
         
         
         
           Q 
          
         
           ^ 
          
         
        
          , 
         
         
         
           K 
          
         
           ^ 
          
         
        
          , 
         
         
         
           V 
          
         
           ^ 
          
         
        
          ) 
         
        
          + 
         
        
          X 
         
         
        
          A 
         
        
          t 
         
        
          t 
         
        
          e 
         
        
          n 
         
        
          t 
         
        
          i 
         
        
          o 
         
        
          n 
         
        
          ( 
         
         
         
           Q 
          
         
           ^ 
          
         
        
          , 
         
         
         
           K 
          
         
           ^ 
          
         
        
          , 
         
         
         
           V 
          
         
           ^ 
          
         
        
          ) 
         
        
          = 
         
         
         
           V 
          
         
           ^ 
          
         
        
          ⋅ 
         
        
          Softmax 
         
        
          ⁡ 
         
        
          ( 
         
         
         
           K 
          
         
           ^ 
          
         
        
          ⋅ 
         
         
         
           Q 
          
         
           ^ 
          
         
        
          / 
         
        
          α 
         
        
          ) 
         
        
       
         \hat{\mathbf{X}}=W_p Attention (\hat{\mathbf{Q}}, \hat{\mathbf{K}}, \hat{\mathbf{V}})+\mathbf{X}\\Attention (\hat{\mathbf{Q}}, \hat{\mathbf{K}}, \hat{\mathbf{V}})=\hat{\mathbf{V}} \cdot \operatorname{Softmax}(\hat{\mathbf{K}} \cdot \hat{\mathbf{Q}} / \alpha) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.9495em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9495em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathbf">X</span></span><span class="" style="top: -3.2551em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.1667em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.2357em; vertical-align: -0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span><span class="mord mathnormal">A</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.9495em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathbf">Q</span></span><span class="" style="top: -3.2551em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.1667em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1944em;"><span class=""></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9495em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathbf">K</span></span><span class="" style="top: -3.2551em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.1944em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9495em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathbf" style="margin-right: 0.016em;">V</span></span><span class="" style="top: -3.2551em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.25em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6861em;"></span><span class="mord mathbf">X</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height: 1.1995em; vertical-align: -0.25em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.9495em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathbf">Q</span></span><span class="" style="top: -3.2551em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.1667em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1944em;"><span class=""></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9495em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathbf">K</span></span><span class="" style="top: -3.2551em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.1944em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9495em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathbf" style="margin-right: 0.016em;">V</span></span><span class="" style="top: -3.2551em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.25em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.9495em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9495em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathbf" style="margin-right: 0.016em;">V</span></span><span class="" style="top: -3.2551em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.25em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1.1995em; vertical-align: -0.25em;"></span><span class="mop"><span class="mord mathrm">Softmax</span></span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9495em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathbf">K</span></span><span class="" style="top: -3.2551em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.1944em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1.1995em; vertical-align: -0.25em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.9495em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathbf">Q</span></span><span class="" style="top: -3.2551em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.1667em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1944em;"><span class=""></span></span></span></span></span><span class="mord">/</span><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span><span class="mclose">)</span></span></span></span></span></span></p> 
<p>其中<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          X 
         
        
          ^ 
         
        
       
      
        \hat X 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.9468em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathnormal" style="margin-right: 0.0785em;">X</span></span><span class="" style="top: -3.2523em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.1667em;"><span class="mord">^</span></span></span></span></span></span></span></span></span></span></span> 和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         X 
        
       
      
        X 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.0785em;">X</span></span></span></span></span> 是输出和输入的feature map, <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          Q 
         
        
          ^ 
         
        
       
         ∈ 
        
        
        
          R 
         
         
          
          
            H 
           
          
            ^ 
           
          
          
          
            W 
           
          
            ^ 
           
          
         
           × 
          
          
          
            C 
           
          
            ^ 
           
          
         
        
       
         ; 
        
        
        
          K 
         
        
          ^ 
         
        
       
         ∈ 
        
        
        
          R 
         
         
          
          
            C 
           
          
            ^ 
           
          
         
           × 
          
          
          
            H 
           
          
            ^ 
           
          
          
          
            W 
           
          
            ^ 
           
          
         
        
       
         ; 
        
       
          and  
        
        
        
          V 
         
        
          ^ 
         
        
       
         ∈ 
        
        
        
          R 
         
         
          
          
            H 
           
          
            ^ 
           
          
          
          
            W 
           
          
            ^ 
           
          
         
           × 
          
          
          
            C 
           
          
            ^ 
           
          
         
        
       
      
        \hat{\mathbf{Q}} \in \mathbb{R}^{\hat{H} \hat{W} \times \hat{C}} ; \hat{\mathbf{K}} \in \mathbb{R}^{\hat{C} \times \hat{H} \hat{W}} ; \text { and } \hat{\mathbf{V}} \in \mathbb{R}^{\hat{H} \hat{W} \times \hat{C}} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.144em; vertical-align: -0.1944em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.9495em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathbf">Q</span></span><span class="" style="top: -3.2551em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.1667em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1944em;"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.2202em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 1.0257em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord accent mtight"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -2.7em;"><span class="pstrut" style="height: 2.7em;"></span><span class="mord mathnormal mtight" style="margin-right: 0.0813em;">H</span></span><span class="" style="top: -2.9523em;"><span class="pstrut" style="height: 2.7em;"></span><span class="accent-body" style="left: -0.1944em;"><span class="mord mtight">^</span></span></span></span></span></span></span><span class="mord accent mtight"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -2.7em;"><span class="pstrut" style="height: 2.7em;"></span><span class="mord mathnormal mtight" style="margin-right: 0.1389em;">W</span></span><span class="" style="top: -2.9523em;"><span class="pstrut" style="height: 2.7em;"></span><span class="accent-body" style="left: -0.25em;"><span class="mord mtight">^</span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord accent mtight"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -2.7em;"><span class="pstrut" style="height: 2.7em;"></span><span class="mord mathnormal mtight" style="margin-right: 0.0715em;">C</span></span><span class="" style="top: -2.9523em;"><span class="pstrut" style="height: 2.7em;"></span><span class="accent-body" style="left: -0.1667em;"><span class="mord mtight">^</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9495em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathbf">K</span></span><span class="" style="top: -3.2551em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.1944em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.2202em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 1.0257em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord accent mtight"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -2.7em;"><span class="pstrut" style="height: 2.7em;"></span><span class="mord mathnormal mtight" style="margin-right: 0.0715em;">C</span></span><span class="" style="top: -2.9523em;"><span class="pstrut" style="height: 2.7em;"></span><span class="accent-body" style="left: -0.1667em;"><span class="mord mtight">^</span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord accent mtight"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -2.7em;"><span class="pstrut" style="height: 2.7em;"></span><span class="mord mathnormal mtight" style="margin-right: 0.0813em;">H</span></span><span class="" style="top: -2.9523em;"><span class="pstrut" style="height: 2.7em;"></span><span class="accent-body" style="left: -0.1944em;"><span class="mord mtight">^</span></span></span></span></span></span></span><span class="mord accent mtight"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -2.7em;"><span class="pstrut" style="height: 2.7em;"></span><span class="mord mathnormal mtight" style="margin-right: 0.1389em;">W</span></span><span class="" style="top: -2.9523em;"><span class="pstrut" style="height: 2.7em;"></span><span class="accent-body" style="left: -0.25em;"><span class="mord mtight">^</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord text"><span class="mord"> and </span></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9495em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathbf" style="margin-right: 0.016em;">V</span></span><span class="" style="top: -3.2551em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.25em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.0257em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 1.0257em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord accent mtight"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -2.7em;"><span class="pstrut" style="height: 2.7em;"></span><span class="mord mathnormal mtight" style="margin-right: 0.0813em;">H</span></span><span class="" style="top: -2.9523em;"><span class="pstrut" style="height: 2.7em;"></span><span class="accent-body" style="left: -0.1944em;"><span class="mord mtight">^</span></span></span></span></span></span></span><span class="mord accent mtight"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -2.7em;"><span class="pstrut" style="height: 2.7em;"></span><span class="mord mathnormal mtight" style="margin-right: 0.1389em;">W</span></span><span class="" style="top: -2.9523em;"><span class="pstrut" style="height: 2.7em;"></span><span class="accent-body" style="left: -0.25em;"><span class="mord mtight">^</span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord accent mtight"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -2.7em;"><span class="pstrut" style="height: 2.7em;"></span><span class="mord mathnormal mtight" style="margin-right: 0.0715em;">C</span></span><span class="" style="top: -2.9523em;"><span class="pstrut" style="height: 2.7em;"></span><span class="accent-body" style="left: -0.1667em;"><span class="mord mtight">^</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> 由原尺寸<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          R 
         
         
          
          
            H 
           
          
            ^ 
           
          
         
           × 
          
          
          
            W 
           
          
            ^ 
           
          
         
           × 
          
          
          
            C 
           
          
            ^ 
           
          
         
        
       
      
        R^{\hat H×\hat W×\hat C} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0257em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0077em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 1.0257em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord accent mtight"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -2.7em;"><span class="pstrut" style="height: 2.7em;"></span><span class="mord mathnormal mtight" style="margin-right: 0.0813em;">H</span></span><span class="" style="top: -2.9523em;"><span class="pstrut" style="height: 2.7em;"></span><span class="accent-body" style="left: -0.1944em;"><span class="mord mtight">^</span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord accent mtight"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -2.7em;"><span class="pstrut" style="height: 2.7em;"></span><span class="mord mathnormal mtight" style="margin-right: 0.1389em;">W</span></span><span class="" style="top: -2.9523em;"><span class="pstrut" style="height: 2.7em;"></span><span class="accent-body" style="left: -0.25em;"><span class="mord mtight">^</span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord accent mtight"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -2.7em;"><span class="pstrut" style="height: 2.7em;"></span><span class="mord mathnormal mtight" style="margin-right: 0.0715em;">C</span></span><span class="" style="top: -2.9523em;"><span class="pstrut" style="height: 2.7em;"></span><span class="accent-body" style="left: -0.1667em;"><span class="mord mtight">^</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>对张量进行reshape 得到矩阵。在这里，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         α 
        
       
      
        \alpha 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span></span></span></span></span> 是一个可学习的标度参数，用于在应用Softmax函数之前控制<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          K 
         
        
          ^ 
         
        
       
      
        \hat K 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.9468em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathnormal" style="margin-right: 0.0715em;">K</span></span><span class="" style="top: -3.2523em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.1944em;"><span class="mord">^</span></span></span></span></span></span></span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          Q 
         
        
          ^ 
         
        
       
      
        \hat Q 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.1412em; vertical-align: -0.1944em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathnormal">Q</span></span><span class="" style="top: -3.2523em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.1667em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1944em;"><span class=""></span></span></span></span></span></span></span></span></span>的点积的大小。</p> 
<p>与传统的多头SA相似，我们将通道的数量划分为“heads”，并同时学习不同的attention map。</p> 
<h3><a id="GatedDconv_FeedForward_Network_47"></a>Gated-Dconv Feed-Forward Network</h3> 
<p>为了变换特征，regular feed-forward network (FN) 分别相同地作用于每个像素位置。它使用两个1×1卷积，一个扩展feature channels (通常 扩展率 γ=4)，另一个减少通道回到原始的输入维数。在隐藏层中应用了non-linearity。</p> 
<p>在这项工作中，我们在FN中提出了两项基本修改，以改进representations learning: <code>(1) gating mechanism (2) depthwise convolutions.</code></p> 
<p>我们的GDFN体系结构如图2(b)所示。<font color="LightSeaGreen">该gating mechanism 是parallel paths of linear transformation layers的element-wise product，其中一个被GELU non-linearity激活。</font><br> <img src="https://images2.imgbox.com/8b/be/qhc0aLeW_o.png" alt="在这里插入图片描述" width="600"><br> 与MDTA一样，我们也<font color="LightSeaGreen">在GDFN中包含depth-wise 来编码来自空间相邻像素位置的信息，这对于学习局部图像结构以便有效恢复非常有用。</font> 上训练的模型在测试时显示出增强的性能，而图像可以具有不同的分辨率(图像恢复的常见情况)。渐进学习策略的行为与课程学习过程类似，即网络从一个较简单的任务开始，逐渐转向学习一个较复杂的任务(需要保持良好的图像结构/纹理)。由于对大补丁的训练需要花费更长的时间，所以随着补丁大小的增加，我们减少了批处理的大小，以便在每个优化步骤中保持与固定补丁训练相同的时间。</p> 
<h2><a id="_55"></a>代码详解</h2> 
<hr> 
<p><strong>to_3d</strong><br> 把4维的张量转换成3维的张量，输入形状<code>(b,c,h,w)</code>, 输出形状<code>(b,h*w,c)</code>。</p> 
<pre><code class="prism language-python"><span class="token comment"># (b,c,h,w)-&gt;(b,h*w,c)</span>
<span class="token keyword">def</span> <span class="token function">to_3d</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> rearrange<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token string">'b c h w -&gt; b (h w) c'</span><span class="token punctuation">)</span>
</code></pre> 
<hr> 
<p><strong>to_4d</strong><br> 把3维的张量转换成4维的张量，输入形状<code>(b,h*w,c)</code>, 输出形状<code>(b,c,h,w)</code>。</p> 
<pre><code class="prism language-python"><span class="token comment"># (b,h*w,c)-&gt;(b,c,h,w)</span>
<span class="token keyword">def</span> <span class="token function">to_4d</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span>h<span class="token punctuation">,</span>w<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> rearrange<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token string">'b (h w) c -&gt; b c h w'</span><span class="token punctuation">,</span>h<span class="token operator">=</span>h<span class="token punctuation">,</span>w<span class="token operator">=</span>w<span class="token punctuation">)</span>
</code></pre> 
<hr> 
<p><strong>BiasFree_LayerNorm</strong><br> 实现了不带偏置的层归一化</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">BiasFree_LayerNorm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> normalized_shape<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>BiasFree_LayerNorm<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>normalized_shape<span class="token punctuation">,</span> numbers<span class="token punctuation">.</span>Integral<span class="token punctuation">)</span><span class="token punctuation">:</span>
            normalized_shape <span class="token operator">=</span> <span class="token punctuation">(</span>normalized_shape<span class="token punctuation">,</span><span class="token punctuation">)</span>
        normalized_shape <span class="token operator">=</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span>normalized_shape<span class="token punctuation">)</span>

        <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>normalized_shape<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span>

        self<span class="token punctuation">.</span>weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>normalized_shape<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>normalized_shape <span class="token operator">=</span> normalized_shape

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    	<span class="token comment"># (b,h*w,c)</span>
        sigma <span class="token operator">=</span> x<span class="token punctuation">.</span>var<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> unbiased<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span> <span class="token comment"># 计算矩阵x沿着最后一个维度的方差</span>
        <span class="token triple-quoted-string string">'''
        var: 计算方差的函数
        -1: 表示最后一个维度
        keepdim=True 表示保留维度
        unbiased = False 表示使用有偏方差的计算方式
        '''</span>
        <span class="token keyword">return</span> x <span class="token operator">/</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>sigma<span class="token operator">+</span><span class="token number">1e-5</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>weight
</code></pre> 
<hr> 
<p><strong>WithBias_LayerNorm</strong><br> 实现了带偏置的层归一化</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">WithBias_LayerNorm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> normalized_shape<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>WithBias_LayerNorm<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>normalized_shape<span class="token punctuation">,</span> numbers<span class="token punctuation">.</span>Integral<span class="token punctuation">)</span><span class="token punctuation">:</span>
            normalized_shape <span class="token operator">=</span> <span class="token punctuation">(</span>normalized_shape<span class="token punctuation">,</span><span class="token punctuation">)</span>
        normalized_shape <span class="token operator">=</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span>normalized_shape<span class="token punctuation">)</span>

        <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>normalized_shape<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span>

        self<span class="token punctuation">.</span>weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>normalized_shape<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bias <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>normalized_shape<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>normalized_shape <span class="token operator">=</span> normalized_shape

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        mu <span class="token operator">=</span> x<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment"># 计算均值</span>
        sigma <span class="token operator">=</span> x<span class="token punctuation">.</span>var<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> unbiased<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> <span class="token punctuation">(</span>x <span class="token operator">-</span> mu<span class="token punctuation">)</span> <span class="token operator">/</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>sigma<span class="token operator">+</span><span class="token number">1e-5</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>weight <span class="token operator">+</span> self<span class="token punctuation">.</span>bias <span class="token comment"># 添加偏置</span>
</code></pre> 
<hr> 
<p><strong>LayerNorm</strong><br> 最终的LayerNorm实现。先把输入的形状从<code>(b,c,h,w)</code>转为<code>(b,h*w,c)</code>；然后再通过上述实现的带偏置的层归一化（WithBias_LayerNorm）或者不带偏置的层归一化（BiasFree_LayerNorm）；最后再把形状变回原来输入的形状<code>(b,c,h,w)</code> 。</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">LayerNorm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># 层归一化</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> LayerNorm_type<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>LayerNorm<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> LayerNorm_type <span class="token operator">==</span><span class="token string">'BiasFree'</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>body <span class="token operator">=</span> BiasFree_LayerNorm<span class="token punctuation">(</span>dim<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>body <span class="token operator">=</span> WithBias_LayerNorm<span class="token punctuation">(</span>dim<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># (b,c,h,w)</span>
        h<span class="token punctuation">,</span> w <span class="token operator">=</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
        <span class="token keyword">return</span> to_4d<span class="token punctuation">(</span>self<span class="token punctuation">.</span>body<span class="token punctuation">(</span>to_3d<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> h<span class="token punctuation">,</span> w<span class="token punctuation">)</span>
        <span class="token comment"># to_3d后：(b,h*w,c)</span>
        <span class="token comment"># body后：(b,h*w,c)</span>
        <span class="token comment"># to_4d后：(b,c,h,w)</span>
</code></pre> 
<hr> 
<p><strong>FeedForward</strong><br> 下面代码主要实现了<code>Gated-Dconv Feed-Forward Network (GDFN)</code>中红框的部分。<br> 但是在代码实现部分，两条支路中的1x1的卷积(point-wise)和3x3的Dconv(depth-wise) 是在原始输入上一起做的，完成后再在通道维度分成两块。<br> <img src="https://images2.imgbox.com/f7/97/xWC1uXVz_o.png" alt="在这里插入图片描述" width="600"></p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">FeedForward</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> ffn_expansion_factor<span class="token punctuation">,</span> bias<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>FeedForward<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        hidden_features <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>dim<span class="token operator">*</span>ffn_expansion_factor<span class="token punctuation">)</span>
        <span class="token comment"># point-wise convolution 1x1的卷积</span>
        self<span class="token punctuation">.</span>project_in <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> hidden_features<span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>
        <span class="token comment"># depth-wise convolution groups=in_channels</span>
        self<span class="token punctuation">.</span>dwconv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>hidden_features<span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span> hidden_features<span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> groups<span class="token operator">=</span>hidden_features<span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>
        <span class="token comment"># 1x1 卷积</span>
        self<span class="token punctuation">.</span>project_out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>hidden_features<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># (b,c,h,w)</span>
        <span class="token comment"># point-wise convolution</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>project_in<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment">#  (b,hidden_features*2,h,w)</span>
        <span class="token comment"># depth-wise convolution</span>
        x1<span class="token punctuation">,</span> x2 <span class="token operator">=</span> self<span class="token punctuation">.</span>dwconv<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>chunk<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment">#  dwconv后：(b,hidden_features*2,h,w)</span>
        <span class="token comment">#  chunk后： x1和x2的大小均为(b,hidden_features,h,w)</span>
        <span class="token comment">#  gelu激活函数  element-wise multiplication</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>gelu<span class="token punctuation">(</span>x1<span class="token punctuation">)</span> <span class="token operator">*</span> x2<span class="token comment"># (b,hidden_features,h,w)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>project_out<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment"># (b,c,h,w)</span>
        <span class="token keyword">return</span> x
</code></pre> 
<hr> 
<p><strong>Attention</strong><br> 下面代码主要实现了<code>Multi-DConv Head Transposed Self-Attention (MDTA)</code>中的红框部分。<br> <img src="https://images2.imgbox.com/ae/4d/KUtv4SnX_o.png" alt="在这里插入图片描述" width="600"><br> 在代码实现上，用于生成k,q,v的三条支路中的1x1的卷积(point-wise)和3x3的Dconv(depth-wise) 是在原始输入上一起做的，完成后再在通道维度分成三块。</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Attention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> bias<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Attention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads
        self<span class="token punctuation">.</span>temperature <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>num_heads<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 初始化是(num_heads,1,1)</span>

        <span class="token comment"># point-wise 1x1的卷积</span>
        self<span class="token punctuation">.</span>qkv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> dim<span class="token operator">*</span><span class="token number">3</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>
        <span class="token comment"># depth-wise groups=in_channels</span>
        self<span class="token punctuation">.</span>qkv_dwconv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>dim<span class="token operator">*</span><span class="token number">3</span><span class="token punctuation">,</span> dim<span class="token operator">*</span><span class="token number">3</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> groups<span class="token operator">=</span>dim<span class="token operator">*</span><span class="token number">3</span><span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>project_out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># x: (b,dim,h,w)</span>
        b<span class="token punctuation">,</span>c<span class="token punctuation">,</span>h<span class="token punctuation">,</span>w <span class="token operator">=</span> x<span class="token punctuation">.</span>shape
        qkv <span class="token operator">=</span> self<span class="token punctuation">.</span>qkv_dwconv<span class="token punctuation">(</span>self<span class="token punctuation">.</span>qkv<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># qkv后：(b,3*dim,h,w)</span>
        <span class="token comment"># qkv_dwconv后: (b,3*dim,h,w)</span>
        q<span class="token punctuation">,</span>k<span class="token punctuation">,</span>v <span class="token operator">=</span> qkv<span class="token punctuation">.</span>chunk<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># chunk后：q、k、v的大学均为(b,dim,h,w)</span>

        <span class="token comment"># (b,dim,h,w)-&gt;(b,num_head,c,h*w)</span>
        q <span class="token operator">=</span> rearrange<span class="token punctuation">(</span>q<span class="token punctuation">,</span> <span class="token string">'b (head c) h w -&gt; b head c (h w)'</span><span class="token punctuation">,</span> head<span class="token operator">=</span>self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span>
        k <span class="token operator">=</span> rearrange<span class="token punctuation">(</span>k<span class="token punctuation">,</span> <span class="token string">'b (head c) h w -&gt; b head c (h w)'</span><span class="token punctuation">,</span> head<span class="token operator">=</span>self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span>
        v <span class="token operator">=</span> rearrange<span class="token punctuation">(</span>v<span class="token punctuation">,</span> <span class="token string">'b (head c) h w -&gt; b head c (h w)'</span><span class="token punctuation">,</span> head<span class="token operator">=</span>self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span>

        <span class="token comment"># 在最后一维进行归一化</span>
        q <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>normalize<span class="token punctuation">(</span>q<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        k <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>normalize<span class="token punctuation">(</span>k<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token comment"># (b,num_head,c,h*w) @ (b,num_head,h*w,c) -&gt; (b,num_head,c,c)</span>
        <span class="token comment"># 然后乘以temperature这个可学习的参数(指的是注意力机制中的sqrt(d),d表示特征的维度)</span>
        attn <span class="token operator">=</span> <span class="token punctuation">(</span>q @ k<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>temperature <span class="token comment"># @ 表示数学中的矩阵乘法</span>
        <span class="token comment"># softmax 函数归一化，得到注意力得分</span>
        attn <span class="token operator">=</span> attn<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment">#  (b,num_head,c,c)</span>
        <span class="token comment"># attn和v做矩阵乘法：(b,num_head,c,c) @ (b,num_head,c,h*w)-&gt;(b,num_head,c,h*w)</span>
        out <span class="token operator">=</span> <span class="token punctuation">(</span>attn @ v<span class="token punctuation">)</span>
        <span class="token comment"># reshape: (b,num_head,c,h*w)-&gt;(b,num_head*c,h,w)</span>
        out <span class="token operator">=</span> rearrange<span class="token punctuation">(</span>out<span class="token punctuation">,</span> <span class="token string">'b head c (h w) -&gt; b (head c) h w'</span><span class="token punctuation">,</span> head<span class="token operator">=</span>self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> h<span class="token operator">=</span>h<span class="token punctuation">,</span> w<span class="token operator">=</span>w<span class="token punctuation">)</span>
        <span class="token comment"># 1x1conv: (b,dim,h,w)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>project_out<span class="token punctuation">(</span>out<span class="token punctuation">)</span> <span class="token comment"># dim=c*num_head</span>
        <span class="token keyword">return</span> out <span class="token comment"># (b,c,h,w)</span>
</code></pre> 
<hr> 
<p><strong>TransformerBlock</strong><br> TransformerBlock就是把刚才实现的GDFN和MDTA分别添加上LN和残差连接后串联起来。</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">TransformerBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> ffn_expansion_factor<span class="token punctuation">,</span> bias<span class="token punctuation">,</span> LayerNorm_type<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>TransformerBlock<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>norm1 <span class="token operator">=</span> LayerNorm<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> LayerNorm_type<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>attn <span class="token operator">=</span> Attention<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm2 <span class="token operator">=</span> LayerNorm<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> LayerNorm_type<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ffn <span class="token operator">=</span> FeedForward<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> ffn_expansion_factor<span class="token punctuation">,</span> bias<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># (b,c,h,w)</span>
        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>attn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># LN-&gt;GDTA-&gt;残差连接</span>
        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>ffn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># LN-&gt;GDFN-&gt;残差连接</span>
        <span class="token keyword">return</span> x <span class="token comment"># (b,c,h,w)</span>
</code></pre> 
<hr> 
<p><strong>OverlapPatchEmbed</strong><br> 通过一个3x3的卷积，把输入特征的通道数变成<code>embed_dim</code></p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">OverlapPatchEmbed</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_c<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> embed_dim<span class="token operator">=</span><span class="token number">48</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>OverlapPatchEmbed<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_c<span class="token punctuation">,</span> embed_dim<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># (b,in_c,h,w)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment"># (b,embed_dim,h,w)</span>
        <span class="token keyword">return</span> x
</code></pre> 
<hr> 
<p><strong>Downsample</strong><br> 下采样操作，输入形状<code>(b,n_feat,h,w)</code>，输出形状<code>(b,n_feat*2,h/2,w/2)</code></p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Downsample</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n_feat<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Downsample<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>body <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>n_feat<span class="token punctuation">,</span> n_feat<span class="token operator">//</span><span class="token number">2</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                  nn<span class="token punctuation">.</span>PixelUnshuffle<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">#x: (b,n_feat,h,w)</span>
        <span class="token comment"># Conv2d后：(b,n_feat/2,h,w)</span>
        <span class="token comment"># PixelUnshuffle: (b,n_feat*2,h/2,w/2)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>body<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre> 
<hr> 
<p><strong>Upsample</strong><br> 上采样操作，输入形状<code>(b,n_feat,h,w)</code>, 输出形状<code>(b,n_feat/2,h*2,w*2)</code>。</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Upsample</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n_feat<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Upsample<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>body <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>n_feat<span class="token punctuation">,</span> n_feat<span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                  nn<span class="token punctuation">.</span>PixelShuffle<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># x: (b,n_feat,h,w)</span>
        <span class="token comment">#Conv2d后：(b,n_feat*2,h,w)</span>
        <span class="token comment">#PixelShuffle后：(b,n_feat/2,h*2,w*2)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>body<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre> 
<hr> 
<p><strong>Restormer</strong><br> 实现最终网络结构的部分。</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Restormer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> 
        inp_channels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> 
        out_channels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> 
        dim <span class="token operator">=</span> <span class="token number">48</span><span class="token punctuation">,</span>
        num_blocks <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
        num_refinement_blocks <span class="token operator">=</span> <span class="token number">4</span><span class="token punctuation">,</span>
        heads <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        ffn_expansion_factor <span class="token operator">=</span> <span class="token number">2.66</span><span class="token punctuation">,</span>
        bias <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
        LayerNorm_type <span class="token operator">=</span> <span class="token string">'WithBias'</span><span class="token punctuation">,</span>   <span class="token comment">## Other option 'BiasFree'</span>
        dual_pixel_task <span class="token operator">=</span> <span class="token boolean">False</span>        <span class="token comment">## True for dual-pixel defocus deblurring only. Also set inp_channels=6</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>

        <span class="token builtin">super</span><span class="token punctuation">(</span>Restormer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>patch_embed <span class="token operator">=</span> OverlapPatchEmbed<span class="token punctuation">(</span>inp_channels<span class="token punctuation">,</span> dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>encoder_level1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">[</span>TransformerBlock<span class="token punctuation">(</span>dim<span class="token operator">=</span>dim<span class="token punctuation">,</span> num_heads<span class="token operator">=</span>heads<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> ffn_expansion_factor<span class="token operator">=</span>ffn_expansion_factor<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">,</span> LayerNorm_type<span class="token operator">=</span>LayerNorm_type<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_blocks<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>down1_2 <span class="token operator">=</span> Downsample<span class="token punctuation">(</span>dim<span class="token punctuation">)</span> <span class="token comment">## From Level 1 to Level 2</span>
        self<span class="token punctuation">.</span>encoder_level2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">[</span>TransformerBlock<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">(</span>dim<span class="token operator">*</span><span class="token number">2</span><span class="token operator">**</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> num_heads<span class="token operator">=</span>heads<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> ffn_expansion_factor<span class="token operator">=</span>ffn_expansion_factor<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">,</span> LayerNorm_type<span class="token operator">=</span>LayerNorm_type<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_blocks<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>down2_3 <span class="token operator">=</span> Downsample<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>dim<span class="token operator">*</span><span class="token number">2</span><span class="token operator">**</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">## From Level 2 to Level 3</span>
        self<span class="token punctuation">.</span>encoder_level3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">[</span>TransformerBlock<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">(</span>dim<span class="token operator">*</span><span class="token number">2</span><span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> num_heads<span class="token operator">=</span>heads<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> ffn_expansion_factor<span class="token operator">=</span>ffn_expansion_factor<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">,</span> LayerNorm_type<span class="token operator">=</span>LayerNorm_type<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_blocks<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>down3_4 <span class="token operator">=</span> Downsample<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>dim<span class="token operator">*</span><span class="token number">2</span><span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">## From Level 3 to Level 4</span>
        self<span class="token punctuation">.</span>latent <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">[</span>TransformerBlock<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">(</span>dim<span class="token operator">*</span><span class="token number">2</span><span class="token operator">**</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> num_heads<span class="token operator">=</span>heads<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> ffn_expansion_factor<span class="token operator">=</span>ffn_expansion_factor<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">,</span> LayerNorm_type<span class="token operator">=</span>LayerNorm_type<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_blocks<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>up4_3 <span class="token operator">=</span> Upsample<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>dim<span class="token operator">*</span><span class="token number">2</span><span class="token operator">**</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">## From Level 4 to Level 3</span>
        self<span class="token punctuation">.</span>reduce_chan_level3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>dim<span class="token operator">*</span><span class="token number">2</span><span class="token operator">**</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>dim<span class="token operator">*</span><span class="token number">2</span><span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>decoder_level3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">[</span>TransformerBlock<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">(</span>dim<span class="token operator">*</span><span class="token number">2</span><span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> num_heads<span class="token operator">=</span>heads<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> ffn_expansion_factor<span class="token operator">=</span>ffn_expansion_factor<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">,</span> LayerNorm_type<span class="token operator">=</span>LayerNorm_type<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_blocks<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>


        self<span class="token punctuation">.</span>up3_2 <span class="token operator">=</span> Upsample<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>dim<span class="token operator">*</span><span class="token number">2</span><span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">## From Level 3 to Level 2</span>
        self<span class="token punctuation">.</span>reduce_chan_level2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>dim<span class="token operator">*</span><span class="token number">2</span><span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>dim<span class="token operator">*</span><span class="token number">2</span><span class="token operator">**</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>decoder_level2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">[</span>TransformerBlock<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">(</span>dim<span class="token operator">*</span><span class="token number">2</span><span class="token operator">**</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> num_heads<span class="token operator">=</span>heads<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> ffn_expansion_factor<span class="token operator">=</span>ffn_expansion_factor<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">,</span> LayerNorm_type<span class="token operator">=</span>LayerNorm_type<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_blocks<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>up2_1 <span class="token operator">=</span> Upsample<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>dim<span class="token operator">*</span><span class="token number">2</span><span class="token operator">**</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment">## From Level 2 to Level 1  (NO 1x1 conv to reduce channels)</span>

        self<span class="token punctuation">.</span>decoder_level1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">[</span>TransformerBlock<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">(</span>dim<span class="token operator">*</span><span class="token number">2</span><span class="token operator">**</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> num_heads<span class="token operator">=</span>heads<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> ffn_expansion_factor<span class="token operator">=</span>ffn_expansion_factor<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">,</span> LayerNorm_type<span class="token operator">=</span>LayerNorm_type<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_blocks<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>refinement <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">[</span>TransformerBlock<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">(</span>dim<span class="token operator">*</span><span class="token number">2</span><span class="token operator">**</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> num_heads<span class="token operator">=</span>heads<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> ffn_expansion_factor<span class="token operator">=</span>ffn_expansion_factor<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">,</span> LayerNorm_type<span class="token operator">=</span>LayerNorm_type<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_refinement_blocks<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        
        <span class="token comment">#### For Dual-Pixel Defocus Deblurring Task ####</span>
        self<span class="token punctuation">.</span>dual_pixel_task <span class="token operator">=</span> dual_pixel_task
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>dual_pixel_task<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>skip_conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>dim<span class="token operator">*</span><span class="token number">2</span><span class="token operator">**</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>
        <span class="token comment">###########################</span>

        self<span class="token punctuation">.</span>output <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>dim<span class="token operator">*</span><span class="token number">2</span><span class="token operator">**</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inp_img<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment">#(b,c,h,w)</span>

        inp_enc_level1 <span class="token operator">=</span> self<span class="token punctuation">.</span>patch_embed<span class="token punctuation">(</span>inp_img<span class="token punctuation">)</span> <span class="token comment"># (b,c,h,w)</span>
        <span class="token comment"># 4个 1-head TransformerBolock</span>
        out_enc_level1 <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder_level1<span class="token punctuation">(</span>inp_enc_level1<span class="token punctuation">)</span> <span class="token comment"># (b,c,h,w)</span>
        
        inp_enc_level2 <span class="token operator">=</span> self<span class="token punctuation">.</span>down1_2<span class="token punctuation">(</span>out_enc_level1<span class="token punctuation">)</span> <span class="token comment"># (b,c*2,h/2,w/2)</span>
        <span class="token comment"># 6个 2-head TransformerBlock</span>
        out_enc_level2 <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder_level2<span class="token punctuation">(</span>inp_enc_level2<span class="token punctuation">)</span> <span class="token comment"># (b,c*2,h/2,w/2)</span>

        inp_enc_level3 <span class="token operator">=</span> self<span class="token punctuation">.</span>down2_3<span class="token punctuation">(</span>out_enc_level2<span class="token punctuation">)</span> <span class="token comment"># (b,c*4,h/4,w/4)</span>
        <span class="token comment"># 6个 4-head TransformerBlock</span>
        out_enc_level3 <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder_level3<span class="token punctuation">(</span>inp_enc_level3<span class="token punctuation">)</span> <span class="token comment"># (b,c*4,h/4,w/4)</span>

        inp_enc_level4 <span class="token operator">=</span> self<span class="token punctuation">.</span>down3_4<span class="token punctuation">(</span>out_enc_level3<span class="token punctuation">)</span> <span class="token comment"># (b,c*8,h/8,w/8)</span>
        <span class="token comment"># 8个 8-head TransformerBlock</span>
        latent <span class="token operator">=</span> self<span class="token punctuation">.</span>latent<span class="token punctuation">(</span>inp_enc_level4<span class="token punctuation">)</span> 
                        
        inp_dec_level3 <span class="token operator">=</span> self<span class="token punctuation">.</span>up4_3<span class="token punctuation">(</span>latent<span class="token punctuation">)</span> <span class="token comment"># (b,c*4,h/4,w/4)</span>
        inp_dec_level3 <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>inp_dec_level3<span class="token punctuation">,</span> out_enc_level3<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># (b,c*8,h/4,w/4)</span>
        inp_dec_level3 <span class="token operator">=</span> self<span class="token punctuation">.</span>reduce_chan_level3<span class="token punctuation">(</span>inp_dec_level3<span class="token punctuation">)</span> <span class="token comment"># (b,c*4,h/4,w/4)</span>
        <span class="token comment"># 6个 4-head TransformerBlock</span>
        out_dec_level3 <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder_level3<span class="token punctuation">(</span>inp_dec_level3<span class="token punctuation">)</span> <span class="token comment"># (b,c*4,h/4,w/4)</span>

        inp_dec_level2 <span class="token operator">=</span> self<span class="token punctuation">.</span>up3_2<span class="token punctuation">(</span>out_dec_level3<span class="token punctuation">)</span> <span class="token comment"># (b,c*2,h/2,w/2)</span>
        inp_dec_level2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>inp_dec_level2<span class="token punctuation">,</span> out_enc_level2<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># (b,c*4,h/2,w/2)</span>
        inp_dec_level2 <span class="token operator">=</span> self<span class="token punctuation">.</span>reduce_chan_level2<span class="token punctuation">(</span>inp_dec_level2<span class="token punctuation">)</span> <span class="token comment"># (b,c*2,h/2,w/2)</span>
        <span class="token comment"># 6个 2-head TransformerBlock</span>
        out_dec_level2 <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder_level2<span class="token punctuation">(</span>inp_dec_level2<span class="token punctuation">)</span> <span class="token comment"># (b,c*2,h/2,w/2)</span>

        inp_dec_level1 <span class="token operator">=</span> self<span class="token punctuation">.</span>up2_1<span class="token punctuation">(</span>out_dec_level2<span class="token punctuation">)</span> <span class="token comment"># (b,c,h,w)</span>
        inp_dec_level1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>inp_dec_level1<span class="token punctuation">,</span> out_enc_level1<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># (b,2*c,h,w)</span>
        <span class="token comment">#4个 1-head TransformerBlock</span>
        out_dec_level1 <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder_level1<span class="token punctuation">(</span>inp_dec_level1<span class="token punctuation">)</span> <span class="token comment"># (b,2*c,h,w)</span>
        <span class="token comment">#4个 1-head Transformer</span>
        out_dec_level1 <span class="token operator">=</span> self<span class="token punctuation">.</span>refinement<span class="token punctuation">(</span>out_dec_level1<span class="token punctuation">)</span> <span class="token comment"># (b,2*c,h,w)</span>

        <span class="token comment">#### For Dual-Pixel Defocus Deblurring Task ####</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>dual_pixel_task<span class="token punctuation">:</span>
            out_dec_level1 <span class="token operator">=</span> out_dec_level1 <span class="token operator">+</span> self<span class="token punctuation">.</span>skip_conv<span class="token punctuation">(</span>inp_enc_level1<span class="token punctuation">)</span>
            out_dec_level1 <span class="token operator">=</span> self<span class="token punctuation">.</span>output<span class="token punctuation">(</span>out_dec_level1<span class="token punctuation">)</span>
        <span class="token comment">###########################</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment"># 残差连接</span>
            out_dec_level1 <span class="token operator">=</span> self<span class="token punctuation">.</span>output<span class="token punctuation">(</span>out_dec_level1<span class="token punctuation">)</span> <span class="token operator">+</span> inp_img <span class="token comment">#(b,c,h,w)</span>


        <span class="token keyword">return</span> out_dec_level1
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/9f65b735d65ba8dd71ec71112dd2d3cf/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">微信小程序使用高德地图实现检索定位附近周边的POI功能示例</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/cf6b643b10b37a053f1a262aa4b9e5f7/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">请忘掉chatgpt 今日分享3个可免费使用的AI创作网站</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>