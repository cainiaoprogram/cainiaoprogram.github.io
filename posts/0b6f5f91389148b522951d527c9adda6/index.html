<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>LLaVA和LLaVA-Plus视觉指令微调及工具使用构建多模态智能体 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="LLaVA和LLaVA-Plus视觉指令微调及工具使用构建多模态智能体" />
<meta property="og:description" content="认识和理解视觉内容，以及基于人类指令对视觉世界进行推理，长久以来一直是一个具有挑战性问题。得益于大规模预训练，OpenAI 的 GPT-4V 展示了在自然语言处理任务和复杂视觉理解中令人印象深刻的能力。
智源社区邀请到了LLaVA的一作柳昊天以及LLaVA-Plus的一作刘世隆，共同分享《LLaVA和LLaVA-Plus视觉指令微调及工具使用构建多模态智能体》欢迎大家观看。
主题一、Visual Instruction Tuning（柳昊天）在这次演讲中，我将介绍 LLaVA，第一个在图像理解和推理方面具有类似 GPT-4V 级别的能力的开源项目。我们证明了这种方法可以以较低成本构建可定制的多模态大模型。首先，我将介绍创建如何利用大语言模型，不需要大量手动注释的情况下，创建多模态指令微调数据集；并且这个方法成本可控，利用现有的预训练的大语言模型和视觉编码器，无需从头开始训练。此外，我将展示 LLaVA-1.5，仅通过对原始 LLaVA 进行简单修改，LLaVA-1.5 在 11 个基准测试中取得了SoTA。LLaVA-1.5 使用全公开数据集，一天内在单个 8-A100 节点上完成训练，并超过了包括Qwen-VL-Chat（使用十亿级数据）在内的方法。最后，我将展示一些 LLaVA 有趣的能力和限制，并概述我们渴望探索的方向。
Recognizing and understanding visual content, as well as reasoning about the visual world based on human instructions, has long been a challenging problem. Recently, OpenAI GPT-4V has showcased impressive capabilities in both NLP tasks and complex visual understanding challenges, thanks to large-scale pretraining and extensive instruction tuning. In this talk, I will introduce LLaVA, the first open-sourced project to demonstrate GPT-4V level capabilities in image understanding and reasoning." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/0b6f5f91389148b522951d527c9adda6/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-20T16:43:56+08:00" />
<meta property="article:modified_time" content="2023-12-20T16:43:56+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">LLaVA和LLaVA-Plus视觉指令微调及工具使用构建多模态智能体</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>认识和理解视觉内容，以及基于人类指令对视觉世界进行推理，长久以来一直是一个具有挑战性问题。得益于大规模预训练，OpenAI 的 GPT-4V 展示了在自然语言处理任务和复杂视觉理解中令人印象深刻的能力。</p> 
<p>智源社区邀请到了LLaVA的一作柳昊天以及LLaVA-Plus的一作刘世隆，共同分享《LLaVA和LLaVA-Plus视觉指令微调及工具使用构建多模态智能体》欢迎大家观看。</p> 
<p>主题一、Visual Instruction Tuning（柳昊天）在这次演讲中，我将介绍 LLaVA，第一个在图像理解和推理方面具有类似 GPT-4V 级别的能力的开源项目。我们证明了这种方法可以以较低成本构建可定制的多模态大模型。首先，我将介绍创建如何利用大语言模型，不需要大量手动注释的情况下，创建多模态指令微调数据集；并且这个方法成本可控，利用现有的预训练的大语言模型和视觉编码器，无需从头开始训练。此外，我将展示 LLaVA-1.5，仅通过对原始 LLaVA 进行简单修改，LLaVA-1.5 在 11 个基准测试中取得了SoTA。LLaVA-1.5 使用全公开数据集，一天内在单个 8-A100 节点上完成训练，并超过了包括Qwen-VL-Chat（使用十亿级数据）在内的方法。最后，我将展示一些 LLaVA 有趣的能力和限制，并概述我们渴望探索的方向。</p> 
<p>Recognizing and understanding visual content, as well as reasoning about the visual world based on human instructions, has long been a challenging problem. Recently, OpenAI GPT-4V has showcased impressive capabilities in both NLP tasks and complex visual understanding challenges, thanks to large-scale pretraining and extensive instruction tuning. In this talk, I will introduce LLaVA, the first open-sourced project to demonstrate GPT-4V level capabilities in image understanding and reasoning. We demonstrate that this approach offers a promising path for building customizable, large multimodal models that follow human intent at an affordable cost. First, I will introduce how we approach this by creating a multimodal instruction-following dataset without the need for extensive manual annotations and by leveraging the existing pretrained LLMs and large vision encoders without the need of training-from-scratch. Additionally, I will present LLaVA-1.5, where it achieves SoTA on 11 benchmarks, with just simple modifications to the original LLaVA. It utilizes all public data, completes training in ~1 day on a single 8-A100 node, and surpasses methods like Qwen-VL-Chat that use billion-scale data. Finally, I will present some intriguing capabilities and limitations of LLaVA and outline a few future directions that we are eager to explore.</p> 
<p>主题二、LLaVA-Plus: Large Language and Vision Assistants that Plug and Learn to Use Skills（刘世隆）</p> 
<p>我们提出了LLaVA-Plus，使用插件（视觉工具）提升多模态大语言模型的视觉能力。我们扩展了多模态大语言模型，使其支持了包括检测、分割、检索、生成、编辑在内的多种视觉能力。</p> 
<p>LLaVA-Plus 维护着一个技能库，其中包含各种视觉和视觉语言预训练模型（工具），并且能够根据用户的多模式输入激活相关工具，以即时组合执行结果来完成许多现实任务。我们通过实验验证了LLaVA-Plus的有效性，在多个基准测试中取得了持续改进的结果，特别是在VisIT-Bench上达到了的新SoTA。</p> 
<p>LLaVA-Plus is a general-purpose multimodal assistant that expands the capabilities of large multimodal models. It maintains a skill repository of pre-trained vision and vision-language models and can activate relevant tools based on users’ inputs to fulfill real-world tasks. LLaVA-Plus is trained on multimodal instruction-following data to acquire the ability to use tools, covering visual understanding, generation, external knowledge retrieval, and compositions. Empirical results show that LLaVA-Plus outperforms LLaVA in existing capabilities and exhibits new ones. It is distinct in that the image query is directly grounded and actively engaged throughout the entire human-AI interaction sessions, significantly improving tool use performance and enabling new scenarios.</p> 
<p>LLaVA: https://arxiv.org/abs/2304.08485<br> LLaVA-1.5: https://arxiv.org/abs/2310.03744<br> LLaVA-Plus: https://arxiv.org/abs/2311.05437</p> 
<p><img src="https://images2.imgbox.com/86/fb/1QkDItGI_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/7d/47/3VkHnnXR_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/92/61/i2j514d8_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/5d/90/Vl04CX2P_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/e7/a6/2nm1g5O9_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/b8/1a/CwdehARK_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/82/0b/EA2bLyuU_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/c8/99/CWCCuy7t_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/26/9d/IN9cKbST_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/9b/9e/fOPKBD4O_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/e2/d0/3lMtlWZT_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/58/c5/ZAGGNSb2_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/a5/d6/nuLCJdQa_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8e323b0647b3a77630d80b443364b145/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">单片机在执行memcpy函数时卡住</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c09a6236e95b12d1e07cab7b01650138/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Machine Learning（study notes）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>