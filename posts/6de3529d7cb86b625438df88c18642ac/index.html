<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>长短时记忆网络(LSTM)(超详细 |附训练代码) - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="长短时记忆网络(LSTM)(超详细 |附训练代码)" />
<meta property="og:description" content="目录
往期回顾
长短时记忆网络是啥
长短时记忆网络的前向计算
长短时记忆网络的训练
LSTM训练算法框架
关于公式和符号的说明
将误差项传递到上一层
​编辑
权重梯度的计算​编辑
长短时记忆网络的实现
激活函数的实现
LSTM初始化
前向计算的实现
反向传播算法的实现
梯度下降算法的实现
梯度检查的实现
GRU
小结
参考资料
往期回顾 在上一篇文章中，我们介绍了循环神经网络以及它的训练算法。我们也介绍了循环神经网络很难训练的原因，这导致了它在实际应用中，很难处理长距离的依赖。在本文中，我们将介绍一种改进之后的循环神经网络：长短时记忆网络(Long Short Term Memory Network, LSTM)，它成功的解决了原始循环神经网络的缺陷，成为当前最流行的RNN，在语音识别、图片描述、自然语言处理等许多领域中成功应用。但不幸的一面是，LSTM的结构很复杂，因此，我们需要花上一些力气，才能把LSTM以及它的训练算法弄明白。在搞清楚LSTM之后，我们再介绍一种LSTM的变体：GRU (Gated Recurrent Unit)。 它的结构比LSTM简单，而效果却和LSTM一样好，因此，它正在逐渐流行起来。最后，我们仍然会动手实现一个LSTM。
长短时记忆网络是啥 我们首先了解一下长短时记忆网络产生的背景。回顾一下零基础入门深度学习(5) - 循环神经网络中推导的，误差项沿时间反向传播的公式：
梯度消失到底意味着什么？在零基础入门深度学习(5) - 循环神经网络中我们已证明，权重数组W最终的梯度是各个时刻的梯度之和，即：
假设某轮训练中，各时刻的梯度以及最终的梯度之和如下图：
我们就可以看到，从上图的t-3时刻开始，梯度已经几乎减少到0了。那么，从这个时刻开始再往之前走，得到的梯度（几乎为零）就不会对最终的梯度值有任何贡献，这就相当于无论t-3时刻之前的网络状态h是什么，在训练中都不会对权重数组W的更新产生影响，也就是网络事实上已经忽略了t-3时刻之前的状态。这就是原始RNN无法处理长距离依赖的原因。
既然找到了问题的原因，那么我们就能解决它。从问题的定位到解决，科学家们大概花了7、8年时间。终于有一天，Hochreiter和Schmidhuber两位科学家发明出长短时记忆网络，一举解决这个问题。
其实，长短时记忆网络的思路比较简单。原始RNN的隐藏层只有一个状态，即h，它对于短期的输入非常敏感。那么，假如我们再增加一个状态，即c，让它来保存长期的状态，那么问题不就解决了么？如下图所示：
新增加的状态c，称为单元状态(cell state)。我们把上图按照时间维度展开：
LSTM的关键，就是怎样控制长期状态c。在这里，LSTM的思路是使用三个控制开关。第一个开关，负责控制继续保存长期状态c；第二个开关，负责控制把即时状态输入到长期状态c；第三个开关，负责控制是否把长期状态c作为当前的LSTM的输出。三个开关的作用如下图所示：
接下来，我们要描述一下，输出h和单元状态c的具体计算方法。
长短时记忆网络的前向计算 前面描述的开关是怎样在算法中实现的呢？这就用到了门（gate）的概念。门实际上就是一层全连接层，它的输入是一个向量，输出是一个0到1之间的实数向量。假设W是门的权重向量，b是偏置项，那么门可以表示为：
门的使用，就是用门的输出向量按元素乘以我们需要控制的那个向量。因为门的输出是0到1之间的实数向量，那么，当门输出为0时，任何向量与之相乘都会得到0向量，这就相当于啥都不能通过；输出为1时，任何向量与之相乘都不会有任何改变，这就相当于啥都可以通过。因为δ（也就是sigmoid函数）的值域是(0,1)，所以门的状态都是半开半闭的。
LSTM用两个门来控制单元状态c的内容，一个是遗忘门（forget gate），它决定了上一时刻的单元状态有多少保留到当前时刻ct；另一个是输入门（input gate），它决定了当前时刻网络的输入xt有多少保存到单元状态ct。LSTM用输出门（output gate）来控制单元状态ct有多少输出到LSTM的当前输出值ht。
下图显示了遗忘门的计算：
接下来看看输入门：
下图是的~ct计算：
这样，我们就把LSTM关于当前的记忆~ct和长期的记忆ct-1组合在一起，形成了新的单元状态ct。由于遗忘门的控制，它可以保存很久很久之前的信息，由于输入门的控制，它又可以避免当前无关紧要的内容进入记忆。下面，我们要看看输出门，它控制了长期记忆对当前输出的影响：
下图表示输出门的计算：
LSTM最终的输出，是由输出门和单元状态共同确定的：
下图表示LSTM最终输出的计算：
式1到式6就是LSTM前向计算的全部公式。至此，我们就把LSTM前向计算讲完了。
长短时记忆网络的训练 熟悉我们这个系列文章的同学都清楚，训练部分往往比前向计算部分复杂多了。LSTM的前向计算都这么复杂，那么，可想而知，它的训练算法一定是非常非常复杂的。现在只有做几次深呼吸，再一头扎进公式海洋吧。
LSTM训练算法框架 LSTM的训练算法仍然是反向传播算法，对于这个算法，我们已经非常熟悉了。主要有下面三个步骤：
前向计算每个神经元的输出值，对于LSTM来说，即ft、it、ct、ot、ht五个向量的值。计算方法已经在上一节中描述过了。反向计算每个神经元的误差项δ值。与循环神经网络一样，LSTM误差项的反向传播也是包括两个方向：一个是沿时间的反向传播，即从当前t时刻开始，计算每个时刻的误差项；一个是将误差项向上一层传播。根据相应的误差项，计算每个权重的梯度。 关于公式和符号的说明 首先，我们对推导中用到的一些公式、符号做一下必要的说明。
接下来的推导中，我们设定gate的激活函数为sigmoid函数，输出的激活函数为tanh函数。他们的导数分别为：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/6de3529d7cb86b625438df88c18642ac/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-08-03T16:15:08+08:00" />
<meta property="article:modified_time" content="2023-08-03T16:15:08+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">长短时记忆网络(LSTM)(超详细 |附训练代码)</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="-toc" style="margin-left:40px;"></p> 
<p id="往期回顾-toc" style="margin-left:40px;"><a href="#%E5%BE%80%E6%9C%9F%E5%9B%9E%E9%A1%BE" rel="nofollow">往期回顾</a></p> 
<p id="长短时记忆网络是啥-toc" style="margin-left:40px;"><a href="#%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C%E6%98%AF%E5%95%A5" rel="nofollow">长短时记忆网络是啥</a></p> 
<p id="长短时记忆网络的前向计算-toc" style="margin-left:40px;"><a href="#%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C%E7%9A%84%E5%89%8D%E5%90%91%E8%AE%A1%E7%AE%97" rel="nofollow">长短时记忆网络的前向计算</a></p> 
<p id="长短时记忆网络的训练-toc" style="margin-left:40px;"><a href="#%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C%E7%9A%84%E8%AE%AD%E7%BB%83" rel="nofollow">长短时记忆网络的训练</a></p> 
<p id="lstm训练算法框架-toc" style="margin-left:80px;"><a href="#lstm%E8%AE%AD%E7%BB%83%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6" rel="nofollow">LSTM训练算法框架</a></p> 
<p id="关于公式和符号的说明-toc" style="margin-left:80px;"><a href="#%E5%85%B3%E4%BA%8E%E5%85%AC%E5%BC%8F%E5%92%8C%E7%AC%A6%E5%8F%B7%E7%9A%84%E8%AF%B4%E6%98%8E" rel="nofollow">关于公式和符号的说明</a></p> 
<p id="%C2%A0%E5%B0%86%E8%AF%AF%E5%B7%AE%E9%A1%B9%E4%BC%A0%E9%80%92%E5%88%B0%E4%B8%8A%E4%B8%80%E5%B1%82-toc" style="margin-left:40px;"><a href="#%C2%A0%E5%B0%86%E8%AF%AF%E5%B7%AE%E9%A1%B9%E4%BC%A0%E9%80%92%E5%88%B0%E4%B8%8A%E4%B8%80%E5%B1%82" rel="nofollow"> 将误差项传递到上一层</a></p> 
<p id="%C2%A0%E2%80%8B%E7%BC%96%E8%BE%91-toc" style="margin-left:40px;"><a href="#%C2%A0%E2%80%8B%E7%BC%96%E8%BE%91" rel="nofollow"> ​编辑</a></p> 
<p id="%C2%A0%E6%9D%83%E9%87%8D%E6%A2%AF%E5%BA%A6%E7%9A%84%E8%AE%A1%E7%AE%97%E2%80%8B%E7%BC%96%E8%BE%91-toc" style="margin-left:40px;"><a href="#%C2%A0%E6%9D%83%E9%87%8D%E6%A2%AF%E5%BA%A6%E7%9A%84%E8%AE%A1%E7%AE%97%E2%80%8B%E7%BC%96%E8%BE%91" rel="nofollow"> 权重梯度的计算​编辑</a></p> 
<p id="长短时记忆网络的实现-toc" style="margin-left:40px;"><a href="#%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AE%9E%E7%8E%B0" rel="nofollow">长短时记忆网络的实现</a></p> 
<p id="激活函数的实现-toc" style="margin-left:80px;"><a href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E5%AE%9E%E7%8E%B0" rel="nofollow">激活函数的实现</a></p> 
<p id="lstm初始化-toc" style="margin-left:80px;"><a href="#lstm%E5%88%9D%E5%A7%8B%E5%8C%96" rel="nofollow">LSTM初始化</a></p> 
<p id="前向计算的实现-toc" style="margin-left:80px;"><a href="#%E5%89%8D%E5%90%91%E8%AE%A1%E7%AE%97%E7%9A%84%E5%AE%9E%E7%8E%B0" rel="nofollow">前向计算的实现</a></p> 
<p id="反向传播算法的实现-toc" style="margin-left:80px;"><a href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E7%9A%84%E5%AE%9E%E7%8E%B0" rel="nofollow">反向传播算法的实现</a></p> 
<p id="梯度下降算法的实现-toc" style="margin-left:80px;"><a href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E7%9A%84%E5%AE%9E%E7%8E%B0" rel="nofollow">梯度下降算法的实现</a></p> 
<p id="梯度检查的实现-toc" style="margin-left:80px;"><a href="#%E6%A2%AF%E5%BA%A6%E6%A3%80%E6%9F%A5%E7%9A%84%E5%AE%9E%E7%8E%B0" rel="nofollow">梯度检查的实现</a></p> 
<p id="gru-toc" style="margin-left:40px;"><a href="#gru" rel="nofollow">GRU</a></p> 
<p id="小结-toc" style="margin-left:40px;"><a href="#%E5%B0%8F%E7%BB%93" rel="nofollow">小结</a></p> 
<p id="参考资料-toc" style="margin-left:40px;"><a href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99" rel="nofollow">参考资料</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h3 id="往期回顾">往期回顾</h3> 
<p>在上一篇文章中，我们介绍了<strong>循环神经网络</strong>以及它的训练算法。我们也介绍了<strong>循环神经网络</strong>很难训练的原因，这导致了它在实际应用中，很难处理长距离的依赖。在本文中，我们将介绍一种改进之后的循环神经网络：<strong>长短时记忆网络(Long Short Term Memory Network, LSTM)</strong>，它成功的解决了原始循环神经网络的缺陷，成为当前最流行的RNN，在语音识别、图片描述、自然语言处理等许多领域中成功应用。但不幸的一面是，<strong>LSTM</strong>的结构很复杂，因此，我们需要花上一些力气，才能把LSTM以及它的训练算法弄明白。在搞清楚<strong>LSTM</strong>之后，我们再介绍一种<strong>LSTM</strong>的变体：<strong>GRU (Gated Recurrent Unit)</strong>。 它的结构比<strong>LSTM</strong>简单，而效果却和<strong>LSTM</strong>一样好，因此，它正在逐渐流行起来。最后，我们仍然会动手实现一个<strong>LSTM</strong>。</p> 
<h3 id="长短时记忆网络是啥">长短时记忆网络是啥</h3> 
<p>我们首先了解一下长短时记忆网络产生的背景。回顾一下<a href="https://zybuluo.com/hanbingtao/note/541458" rel="nofollow" title="零基础入门深度学习(5) - 循环神经网络">零基础入门深度学习(5) - 循环神经网络</a>中推导的，误差项沿时间反向传播的公式：</p> 
<p></p> 
<p><img alt="" height="602" src="https://images2.imgbox.com/8f/81/BsL5R8R2_o.png" width="1171"></p> 
<p> <strong>梯度消失</strong>到底意味着什么？在<a href="https://zybuluo.com/hanbingtao/note/541458" rel="nofollow" title="零基础入门深度学习(5) - 循环神经网络">零基础入门深度学习(5) - 循环神经网络</a>中我们已证明，权重数组W最终的梯度是各个时刻的梯度之和，即：<img alt="" height="178" src="https://images2.imgbox.com/23/8e/Ob6FmkTV_o.png" width="1195"></p> 
<p></p> 
<p>假设某轮训练中，各时刻的梯度以及最终的梯度之和如下图：</p> 
<p></p> 
<p class="img-center"><img alt="" height="201" src="https://images2.imgbox.com/8d/0a/Vpu5cjyf_o.png" width="355"></p> 
<p>我们就可以看到，从上图的t-3时刻开始，梯度已经几乎减少到0了。那么，从这个时刻开始再往之前走，得到的梯度（几乎为零）就不会对最终的梯度值有任何贡献，这就相当于无论t-3时刻之前的网络状态h是什么，在训练中都不会对权重数组W的更新产生影响，也就是网络事实上已经忽略了t-3时刻之前的状态。这就是原始RNN无法处理长距离依赖的原因。</p> 
<p>既然找到了问题的原因，那么我们就能解决它。从问题的定位到解决，科学家们大概花了7、8年时间。终于有一天，Hochreiter和Schmidhuber两位科学家发明出<strong>长短时记忆网络</strong>，一举解决这个问题。</p> 
<p>其实，<strong>长短时记忆网络</strong>的思路比较简单。原始RNN的隐藏层只有一个状态，即h，它对于短期的输入非常敏感。那么，假如我们再增加一个状态，即c，让它来保存长期的状态，那么问题不就解决了么？如下图所示：</p> 
<p></p> 
<p class="img-center"><img alt="" height="192" src="https://images2.imgbox.com/9a/52/AeGWqWpv_o.png" width="243"></p> 
<p>新增加的状态c，称为<strong>单元状态(cell state)</strong>。我们把上图按照时间维度展开：</p> 
<p></p> 
<p class="img-center"><img alt="" height="202" src="https://images2.imgbox.com/f1/01/9Zuu1YMM_o.png" width="448"></p> 
<p><img alt="" height="156" src="https://images2.imgbox.com/e5/f4/pAC07Ien_o.png" width="1176"></p> 
<p></p> 
<p>LSTM的关键，就是怎样控制长期状态c。在这里，LSTM的思路是使用三个控制开关。第一个开关，负责控制继续保存长期状态c；第二个开关，负责控制把即时状态输入到长期状态c；第三个开关，负责控制是否把长期状态c作为当前的LSTM的输出。三个开关的作用如下图所示：</p> 
<p></p> 
<p class="img-center"><img alt="" height="328" src="https://images2.imgbox.com/ec/98/YQUi7AJ4_o.png" width="406"></p> 
<p>接下来，我们要描述一下，输出h和单元状态c的具体计算方法。</p> 
<h3 id="长短时记忆网络的前向计算">长短时记忆网络的前向计算</h3> 
<p></p> 
<p>前面描述的开关是怎样在算法中实现的呢？这就用到了<strong>门（gate）</strong>的概念。门实际上就是一层<strong>全连接层</strong>，它的输入是一个向量，输出是一个0到1之间的实数向量。假设W是门的权重向量，b是偏置项，那么门可以表示为：</p> 
<p><img alt="" height="117" src="https://images2.imgbox.com/91/3d/sPAWqmlj_o.png" width="459"></p> 
<p>门的使用，就是用门的输出向量按元素乘以我们需要控制的那个向量。因为门的输出是0到1之间的实数向量，那么，当门输出为0时，任何向量与之相乘都会得到0向量，这就相当于啥都不能通过；输出为1时，任何向量与之相乘都不会有任何改变，这就相当于啥都可以通过。因为δ（也就是sigmoid函数）的值域是(0,1)，所以门的状态都是半开半闭的。</p> 
<p>LSTM用两个门来控制单元状态c的内容，一个是<strong>遗忘门（forget gate）</strong>，它决定了上一时刻的单元状态有多少保留到当前时刻ct；另一个是<strong>输入门（input gate）</strong>，它决定了当前时刻网络的输入xt有多少保存到单元状态ct。LSTM用<strong>输出门（output gate）</strong>来控制单元状态ct有多少输出到LSTM的当前输出值ht。<img alt="" height="521" src="https://images2.imgbox.com/bb/54/ztAX3IuA_o.png" width="1172"></p> 
<p></p> 
<p>下图显示了遗忘门的计算：</p> 
<p></p> 
<p class="img-center"><img alt="" height="344" src="https://images2.imgbox.com/cb/49/ZjTSblLe_o.png" width="463"></p> 
<p>接下来看看输入门：</p> 
<p> <img alt="" height="378" src="https://images2.imgbox.com/1c/4f/kR1VBioC_o.png" width="1144"></p> 
<p></p> 
<p>下图是的~ct计算：</p> 
<p></p> 
<p class="img-center"><img alt="" height="333" src="https://images2.imgbox.com/24/18/jYvleiSk_o.png" width="461"></p> 
<p> <img alt="" height="239" src="https://images2.imgbox.com/77/b2/xgV2wffA_o.png" width="1152"></p> 
<p></p> 
<p></p> 
<p class="img-center"><img alt="" height="330" src="https://images2.imgbox.com/5b/72/RNO39kdx_o.png" width="462"></p> 
<p>这样，我们就把LSTM关于当前的记忆~ct和长期的记忆ct-1组合在一起，形成了新的单元状态ct。由于遗忘门的控制，它可以保存很久很久之前的信息，由于输入门的控制，它又可以避免当前无关紧要的内容进入记忆。下面，我们要看看输出门，它控制了长期记忆对当前输出的影响：</p> 
<p> <img alt="" height="91" src="https://images2.imgbox.com/45/cd/w0FSdoUI_o.png" width="1198"></p> 
<p></p> 
<p>下图表示输出门的计算：</p> 
<p></p> 
<p class="img-center"><img alt="" height="329" src="https://images2.imgbox.com/74/3e/GQccXEbh_o.png" width="463"></p> 
<p>LSTM最终的输出，是由输出门和单元状态共同确定的：</p> 
<p> <img alt="" height="97" src="https://images2.imgbox.com/73/b5/PMtwbfQO_o.png" width="796"></p> 
<p></p> 
<p>下图表示LSTM最终输出的计算：</p> 
<p></p> 
<p class="img-center"><img alt="" height="334" src="https://images2.imgbox.com/6d/4f/8PILaZUz_o.png" width="464"></p> 
<p><strong>式1</strong>到<strong>式6</strong>就是LSTM前向计算的全部公式。至此，我们就把LSTM前向计算讲完了。</p> 
<h3 id="长短时记忆网络的训练">长短时记忆网络的训练</h3> 
<p>熟悉我们这个系列文章的同学都清楚，训练部分往往比前向计算部分复杂多了。LSTM的前向计算都这么复杂，那么，可想而知，它的训练算法一定是非常非常复杂的。现在只有做几次深呼吸，再一头扎进公式海洋吧。</p> 
<h4 id="lstm训练算法框架">LSTM训练算法框架</h4> 
<p>LSTM的训练算法仍然是反向传播算法，对于这个算法，我们已经非常熟悉了。主要有下面三个步骤：</p> 
<ol><li>前向计算每个神经元的输出值，对于LSTM来说，即ft、it、ct、ot、ht五个向量的值。计算方法已经在上一节中描述过了。</li><li>反向计算每个神经元的<strong>误差项δ</strong>值。与<strong>循环神经网络</strong>一样，LSTM误差项的反向传播也是包括两个方向：一个是沿时间的反向传播，即从当前t时刻开始，计算每个时刻的误差项；一个是将误差项向上一层传播。</li><li>根据相应的误差项，计算每个权重的梯度。</li></ol> 
<h4 id="关于公式和符号的说明">关于公式和符号的说明</h4> 
<p>首先，我们对推导中用到的一些公式、符号做一下必要的说明。</p> 
<p>接下来的推导中，我们设定gate的激活函数为sigmoid函数，输出的激活函数为tanh函数。他们的导数分别为：</p> 
<p><img alt="" height="564" src="https://images2.imgbox.com/6f/5d/SvY2QFEi_o.png" width="1169"></p> 
<p> <img alt="" height="1124" src="https://images2.imgbox.com/c3/db/jhMvo64Q_o.png" width="1175"></p> 
<p> <img alt="" height="1002" src="https://images2.imgbox.com/ae/f7/0BfZkp10_o.png" width="1200"></p> 
<p> <img alt="" height="1158" src="https://images2.imgbox.com/f2/5b/sxWaj5mI_o.png" width="1200"></p> 
<p> <img alt="" height="1200" src="https://images2.imgbox.com/58/9d/IGwEQXjp_o.png" width="1195"></p> 
<p></p> 
<p><img alt="" height="1090" src="https://images2.imgbox.com/3d/15/hirYuaAZ_o.png" width="1200"></p> 
<h3 id="%C2%A0%E5%B0%86%E8%AF%AF%E5%B7%AE%E9%A1%B9%E4%BC%A0%E9%80%92%E5%88%B0%E4%B8%8A%E4%B8%80%E5%B1%82"> 将误差项传递到上一层</h3> 
<h3 id="%C2%A0%E2%80%8B%E7%BC%96%E8%BE%91"> <img alt="" height="790" src="https://images2.imgbox.com/1b/28/cQd0L8c8_o.png" width="1200"></h3> 
<h3 id="%C2%A0%E6%9D%83%E9%87%8D%E6%A2%AF%E5%BA%A6%E7%9A%84%E8%AE%A1%E7%AE%97%E2%80%8B%E7%BC%96%E8%BE%91"> 权重梯度的计算<img alt="" height="1200" src="https://images2.imgbox.com/1c/ea/v7LfWU15_o.png" width="1200"></h3> 
<p> <img alt="" height="1185" src="https://images2.imgbox.com/7b/2d/s3bH6SsT_o.png" width="1125"></p> 
<p> <img alt="" height="722" src="https://images2.imgbox.com/e4/2a/WRG6yxmG_o.png" width="1199"></p> 
<p></p> 
<p>以上就是LSTM的训练算法的全部公式。因为这里面存在很多重复的模式，仔细看看，会发觉并不是太复杂。</p> 
<p>当然，LSTM存在着相当多的变体，读者可以在互联网上找到很多资料。因为大家已经熟悉了基本LSTM的算法，因此理解这些变体比较容易，因此本文就不再赘述了。</p> 
<h3 id="长短时记忆网络的实现">长短时记忆网络的实现</h3> 
<blockquote> 
 <p>完整代码请参考GitHub: <a href="https://github.com/hanbt/learn_dl/blob/master/lstm.py" title="https://github.com/hanbt/learn_dl/blob/master/lstm.py">https://github.com/hanbt/learn_dl/blob/master/lstm.py</a> (python2.7)</p> 
</blockquote> 
<p>在下面的实现中，LSTMLayer的参数包括输入维度、输出维度、隐藏层维度，单元状态维度等于隐藏层维度。gate的激活函数为sigmoid函数，输出的激活函数为tanh。</p> 
<h4 id="激活函数的实现">激活函数的实现</h4> 
<p>我们先实现两个激活函数：sigmoid和tanh。</p> 
<pre><code>class SigmoidActivator(object):
    def forward(self, weighted_input):
        return 1.0 / (1.0 + np.exp(-weighted_input))
    def backward(self, output):
        return output * (1 - output)
class TanhActivator(object):
    def forward(self, weighted_input):
        return 2.0 / (1.0 + np.exp(-2 * weighted_input)) - 1.0
    def backward(self, output):
        return 1 - output * output</code></pre> 
<h4 id="lstm初始化">LSTM初始化</h4> 
<p>和前两篇文章代码架构一样，我们把LSTM的实现放在LstmLayer类中。</p> 
<p><img alt="" height="190" src="https://images2.imgbox.com/01/a8/6sxwaQ8Z_o.png" width="1163"></p> 
<p></p> 
<p>在构造函数的初始化中，只初始化了与forward计算相关的变量，与backward相关的变量没有初始化。这是因为构造LSTM对象的时候，我们还不知道它未来是用于训练（既有forward又有backward）还是推理（只有forward）。</p> 
<pre><code>class LstmLayer(object):
    def __init__(self, input_width, state_width, 
                 learning_rate):
        self.input_width = input_width
        self.state_width = state_width
        self.learning_rate = learning_rate
        # 门的激活函数
        self.gate_activator = SigmoidActivator()
        # 输出的激活函数
        self.output_activator = TanhActivator()
        # 当前时刻初始化为t0
        self.times = 0       
        # 各个时刻的单元状态向量c
        self.c_list = self.init_state_vec()
        # 各个时刻的输出向量h
        self.h_list = self.init_state_vec()
        # 各个时刻的遗忘门f
        self.f_list = self.init_state_vec()
        # 各个时刻的输入门i
        self.i_list = self.init_state_vec()
        # 各个时刻的输出门o
        self.o_list = self.init_state_vec()
        # 各个时刻的即时状态c~
        self.ct_list = self.init_state_vec()
        # 遗忘门权重矩阵Wfh, Wfx, 偏置项bf
        self.Wfh, self.Wfx, self.bf = (
            self.init_weight_mat())
        # 输入门权重矩阵Wfh, Wfx, 偏置项bf
        self.Wih, self.Wix, self.bi = (
            self.init_weight_mat())
        # 输出门权重矩阵Wfh, Wfx, 偏置项bf
        self.Woh, self.Wox, self.bo = (
            self.init_weight_mat())
        # 单元状态权重矩阵Wfh, Wfx, 偏置项bf
        self.Wch, self.Wcx, self.bc = (
            self.init_weight_mat())
    def init_state_vec(self):
        '''
        初始化保存状态的向量
        '''
        state_vec_list = []
        state_vec_list.append(np.zeros(
            (self.state_width, 1)))
        return state_vec_list
    def init_weight_mat(self):
        '''
        初始化权重矩阵
        '''
        Wh = np.random.uniform(-1e-4, 1e-4,
            (self.state_width, self.state_width))
        Wx = np.random.uniform(-1e-4, 1e-4,
            (self.state_width, self.input_width))
        b = np.zeros((self.state_width, 1))
        return Wh, Wx, b</code></pre> 
<h4 id="前向计算的实现">前向计算的实现</h4> 
<p>forward方法实现了LSTM的前向计算：</p> 
<pre><code>    def forward(self, x):
        '''
        根据式1-式6进行前向计算
        '''
        self.times += 1
        # 遗忘门
        fg = self.calc_gate(x, self.Wfx, self.Wfh, 
            self.bf, self.gate_activator)
        self.f_list.append(fg)
        # 输入门
        ig = self.calc_gate(x, self.Wix, self.Wih,
            self.bi, self.gate_activator)
        self.i_list.append(ig)
        # 输出门
        og = self.calc_gate(x, self.Wox, self.Woh,
            self.bo, self.gate_activator)
        self.o_list.append(og)
        # 即时状态
        ct = self.calc_gate(x, self.Wcx, self.Wch,
            self.bc, self.output_activator)
        self.ct_list.append(ct)
        # 单元状态
        c = fg * self.c_list[self.times - 1] + ig * ct
        self.c_list.append(c)
        # 输出
        h = og * self.output_activator.forward(c)
        self.h_list.append(h)
    def calc_gate(self, x, Wx, Wh, b, activator):
        '''
        计算门
        '''
        h = self.h_list[self.times - 1] # 上次的LSTM输出
        net = np.dot(Wh, h) + np.dot(Wx, x) + b
        gate = activator.forward(net)
        return gate</code></pre> 
<p>从上面的代码我们可以看到，门的计算都是相同的算法，而门和的计算仅仅是激活函数不同。因此我们提出了calc_gate方法，这样减少了很多重复代码。</p> 
<h4 id="反向传播算法的实现">反向传播算法的实现</h4> 
<p>backward方法实现了LSTM的反向传播算法。需要注意的是，与backword相关的内部状态变量是在调用backward方法之后才初始化的。这种延迟初始化的一个好处是，如果LSTM只是用来推理，那么就不需要初始化这些变量，节省了很多内存。</p> 
<pre><code>    def backward(self, x, delta_h, activator):
        '''
        实现LSTM训练算法
        '''
        self.calc_delta(delta_h, activator)
        self.calc_gradient(x)</code></pre> 
<p>算法主要分成两个部分，一部分使计算误差项：</p> 
<pre><code>    def calc_delta(self, delta_h, activator):
        # 初始化各个时刻的误差项
        self.delta_h_list = self.init_delta()  # 输出误差项
        self.delta_o_list = self.init_delta()  # 输出门误差项
        self.delta_i_list = self.init_delta()  # 输入门误差项
        self.delta_f_list = self.init_delta()  # 遗忘门误差项
        self.delta_ct_list = self.init_delta() # 即时输出误差项
        # 保存从上一层传递下来的当前时刻的误差项
        self.delta_h_list[-1] = delta_h
        # 迭代计算每个时刻的误差项
        for k in range(self.times, 0, -1):
            self.calc_delta_k(k)
    def init_delta(self):
        '''
        初始化误差项
        '''
        delta_list = []
        for i in range(self.times + 1):
            delta_list.append(np.zeros(
                (self.state_width, 1)))
        return delta_list
    def calc_delta_k(self, k):
        '''
        根据k时刻的delta_h，计算k时刻的delta_f、
        delta_i、delta_o、delta_ct，以及k-1时刻的delta_h
        '''
        # 获得k时刻前向计算的值
        ig = self.i_list[k]
        og = self.o_list[k]
        fg = self.f_list[k]
        ct = self.ct_list[k]
        c = self.c_list[k]
        c_prev = self.c_list[k-1]
        tanh_c = self.output_activator.forward(c)
        delta_k = self.delta_h_list[k]
        # 根据式9计算delta_o
        delta_o = (delta_k * tanh_c * 
            self.gate_activator.backward(og))
        delta_f = (delta_k * og * 
            (1 - tanh_c * tanh_c) * c_prev *
            self.gate_activator.backward(fg))
        delta_i = (delta_k * og * 
            (1 - tanh_c * tanh_c) * ct *
            self.gate_activator.backward(ig))
        delta_ct = (delta_k * og * 
            (1 - tanh_c * tanh_c) * ig *
            self.output_activator.backward(ct))
        delta_h_prev = (
                np.dot(delta_o.transpose(), self.Woh) +
                np.dot(delta_i.transpose(), self.Wih) +
                np.dot(delta_f.transpose(), self.Wfh) +
                np.dot(delta_ct.transpose(), self.Wch)
            ).transpose()
        # 保存全部delta值
        self.delta_h_list[k-1] = delta_h_prev
        self.delta_f_list[k] = delta_f
        self.delta_i_list[k] = delta_i
        self.delta_o_list[k] = delta_o
        self.delta_ct_list[k] = delta_ct</code></pre> 
<p>另一部分是计算梯度：</p> 
<pre><code>    def calc_gradient(self, x):
        # 初始化遗忘门权重梯度矩阵和偏置项
        self.Wfh_grad, self.Wfx_grad, self.bf_grad = (
            self.init_weight_gradient_mat())
        # 初始化输入门权重梯度矩阵和偏置项
        self.Wih_grad, self.Wix_grad, self.bi_grad = (
            self.init_weight_gradient_mat())
        # 初始化输出门权重梯度矩阵和偏置项
        self.Woh_grad, self.Wox_grad, self.bo_grad = (
            self.init_weight_gradient_mat())
        # 初始化单元状态权重梯度矩阵和偏置项
        self.Wch_grad, self.Wcx_grad, self.bc_grad = (
            self.init_weight_gradient_mat())
       # 计算对上一次输出h的权重梯度
        for t in range(self.times, 0, -1):
            # 计算各个时刻的梯度
            (Wfh_grad, bf_grad,
            Wih_grad, bi_grad,
            Woh_grad, bo_grad,
            Wch_grad, bc_grad) = (
                self.calc_gradient_t(t))
            # 实际梯度是各时刻梯度之和
            self.Wfh_grad += Wfh_grad
            self.bf_grad += bf_grad
            self.Wih_grad += Wih_grad
            self.bi_grad += bi_grad
            self.Woh_grad += Woh_grad
            self.bo_grad += bo_grad
            self.Wch_grad += Wch_grad
            self.bc_grad += bc_grad
            print '-----%d-----' % t
            print Wfh_grad
            print self.Wfh_grad
        # 计算对本次输入x的权重梯度
        xt = x.transpose()
        self.Wfx_grad = np.dot(self.delta_f_list[-1], xt)
        self.Wix_grad = np.dot(self.delta_i_list[-1], xt)
        self.Wox_grad = np.dot(self.delta_o_list[-1], xt)
        self.Wcx_grad = np.dot(self.delta_ct_list[-1], xt)
    def init_weight_gradient_mat(self):
        '''
        初始化权重矩阵
        '''
        Wh_grad = np.zeros((self.state_width,
            self.state_width))
        Wx_grad = np.zeros((self.state_width,
            self.input_width))
        b_grad = np.zeros((self.state_width, 1))
        return Wh_grad, Wx_grad, b_grad
    def calc_gradient_t(self, t):
        '''
        计算每个时刻t权重的梯度
        '''
        h_prev = self.h_list[t-1].transpose()
        Wfh_grad = np.dot(self.delta_f_list[t], h_prev)
        bf_grad = self.delta_f_list[t]
        Wih_grad = np.dot(self.delta_i_list[t], h_prev)
        bi_grad = self.delta_f_list[t]
        Woh_grad = np.dot(self.delta_o_list[t], h_prev)
        bo_grad = self.delta_f_list[t]
        Wch_grad = np.dot(self.delta_ct_list[t], h_prev)
        bc_grad = self.delta_ct_list[t]
        return Wfh_grad, bf_grad, Wih_grad, bi_grad, \
               Woh_grad, bo_grad, Wch_grad, bc_grad</code></pre> 
<h4 id="梯度下降算法的实现">梯度下降算法的实现</h4> 
<p>下面是用梯度下降算法来更新权重：</p> 
<pre><code>    def update(self):
        '''
        按照梯度下降，更新权重
        '''
        self.Wfh -= self.learning_rate * self.Whf_grad
        self.Wfx -= self.learning_rate * self.Whx_grad
        self.bf -= self.learning_rate * self.bf_grad
        self.Wih -= self.learning_rate * self.Whi_grad
        self.Wix -= self.learning_rate * self.Whi_grad
        self.bi -= self.learning_rate * self.bi_grad
        self.Woh -= self.learning_rate * self.Wof_grad
        self.Wox -= self.learning_rate * self.Wox_grad
        self.bo -= self.learning_rate * self.bo_grad
        self.Wch -= self.learning_rate * self.Wcf_grad
        self.Wcx -= self.learning_rate * self.Wcx_grad
        self.bc -= self.learning_rate * self.bc_grad</code></pre> 
<h4 id="梯度检查的实现">梯度检查的实现</h4> 
<p>和RecurrentLayer一样，为了支持梯度检查，我们需要支持重置内部状态：</p> 
<pre><code>    def reset_state(self):
        # 当前时刻初始化为t0
        self.times = 0       
        # 各个时刻的单元状态向量c
        self.c_list = self.init_state_vec()
        # 各个时刻的输出向量h
        self.h_list = self.init_state_vec()
        # 各个时刻的遗忘门f
        self.f_list = self.init_state_vec()
        # 各个时刻的输入门i
        self.i_list = self.init_state_vec()
        # 各个时刻的输出门o
        self.o_list = self.init_state_vec()
        # 各个时刻的即时状态c~
        self.ct_list = self.init_state_vec()</code></pre> 
<p>最后，是梯度检查的代码：</p> 
<pre><code>def data_set():
    x = [np.array([[1], [2], [3]]),
         np.array([[2], [3], [4]])]
    d = np.array([[1], [2]])
    return x, d
def gradient_check():
    '''
    梯度检查
    '''
    # 设计一个误差函数，取所有节点输出项之和
    error_function = lambda o: o.sum()
    lstm = LstmLayer(3, 2, 1e-3)
    # 计算forward值
    x, d = data_set()
    lstm.forward(x[0])
    lstm.forward(x[1])
    # 求取sensitivity map
    sensitivity_array = np.ones(lstm.h_list[-1].shape,
                                dtype=np.float64)
    # 计算梯度
    lstm.backward(x[1], sensitivity_array, IdentityActivator())
    # 检查梯度
    epsilon = 10e-4
    for i in range(lstm.Wfh.shape[0]):
        for j in range(lstm.Wfh.shape[1]):
            lstm.Wfh[i,j] += epsilon
            lstm.reset_state()
            lstm.forward(x[0])
            lstm.forward(x[1])
            err1 = error_function(lstm.h_list[-1])
            lstm.Wfh[i,j] -= 2*epsilon
            lstm.reset_state()
            lstm.forward(x[0])
            lstm.forward(x[1])
            err2 = error_function(lstm.h_list[-1])
            expect_grad = (err1 - err2) / (2 * epsilon)
            lstm.Wfh[i,j] += epsilon
            print 'weights(%d,%d): expected - actural %.4e - %.4e' % (
                i, j, expect_grad, lstm.Wfh_grad[i,j])
    return lstm</code></pre> 
<p>我们只对wf做了检查，读者可以自行增加对其他梯度的检查。下面是某次梯度检查的结果：</p> 
<p></p> 
<p></p> 
<p class="img-center"><img alt="" height="86" src="https://images2.imgbox.com/28/6e/qnMDr49b_o.png" width="566"></p> 
<h3 id="gru">GRU</h3> 
<p>前面我们讲了一种普通的LSTM，事实上LSTM存在很多<strong>变体</strong>，许多论文中的LSTM都或多或少的不太一样。在众多的LSTM变体中，<strong>GRU (Gated Recurrent Unit)</strong>也许是最成功的一种。它对LSTM做了很多简化，同时却保持着和LSTM相同的效果。因此，GRU最近变得越来越流行。</p> 
<p>GRU对LSTM做了两个大改动：</p> 
<ol><li>将输入门、遗忘门、输出门变为两个门：更新门（Update Gate）和重置门（Reset Gate）。</li><li>将单元状态与输出合并为一个状态：h。</li></ol> 
<p>GRU的前向计算公式为：<img alt="" height="214" src="https://images2.imgbox.com/ca/0c/73EsTBR1_o.png" width="589"></p> 
<p></p> 
<p>下图是GRU的示意图：</p> 
<p></p> 
<p class="img-center"><img alt="" height="329" src="https://images2.imgbox.com/61/cc/MV5F23og_o.png" width="430"></p> 
<p>GRU的训练算法比LSTM简单一些，留给读者自行推导，本文就不再赘述了。</p> 
<h3 id="小结">小结</h3> 
<p>至此，LSTM——也许是结构最复杂的一类神经网络——就讲完了，相信拿下前几篇文章的读者们搞定这篇文章也不在话下吧！现在我们已经了解<strong>循环神经网络</strong>和它最流行的变体——<strong>LSTM</strong>，它们都可以用来处理序列。但是，有时候仅仅拥有处理序列的能力还不够，还需要处理比序列更为复杂的结构（比如树结构），这时候就需要用到另外一类网络：<strong>递归神经网络(Recursive Neural Network)</strong>，巧合的是，它的缩写也是<strong>RNN</strong>。在下一篇文章中，我们将介绍<strong>递归神经网络</strong>和它的训练算法。现在，漫长的烧脑暂告一段落，休息一下吧:)</p> 
<p></p> 
<p class="img-center"><img alt="" height="420" src="https://images2.imgbox.com/b8/94/Cad1Ygc1_o.png" width="700"></p> 
<h3 id="参考资料">参考资料</h3> 
<ol><li><a href="http://cs224d.stanford.edu/" rel="nofollow" title="CS224d: Deep Learning for Natural Language Processing">CS224d: Deep Learning for Natural Language Processing</a></li><li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="nofollow" title="Understanding LSTM Networks">Understanding LSTM Networks</a></li><li><a href="http://arunmallya.github.io/writeups/nn/lstm/index.html" rel="nofollow" title="LSTM Forward and Backward Pass">LSTM Forward and Backward Pass</a></li></ol> 
<p>转载自:<a href="https://zybuluo.com/hanbingtao/note/581764" rel="nofollow" title="零基础入门深度学习(6) - 长短时记忆网络(LSTM) - 作业部落 Cmd Markdown 编辑阅读器">零基础入门深度学习(6) - 长短时记忆网络(LSTM) - 作业部落 Cmd Markdown 编辑阅读器</a>,若有疑问请移步原文</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f41cb2f51921425805f1807685fee252/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">消息疯狂堆积！RocketMQ出Bug了？</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b2d0672c4711c136e3cebbb52584773d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Avoid mutating a prop directly since the value will be overwritten whenever the parent component re</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>