<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【行为识别】TSN/TRN/TSM/SlowFast/Non-local - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【行为识别】TSN/TRN/TSM/SlowFast/Non-local" />
<meta property="og:description" content="前言 记录视频理解领域的几篇文章吧，由于每篇值得记录的东西不多，所以合在一起。
关于开源框架，有港中文多媒体实验室的MMAction。有设备的就尽量多跑跑模型吧
视频相对于静态图像多了时间维度。静态图像的分类、检测、分割做得相对完善了，视频方面的工作想有创新必须在时间这个维度上钻研。
注意 Action Recognition和Spatio-temporal Action Recognition(又称action localization)的区别。前者只需判断视频的类别，后者要在视频中确定动作从第几帧开始第几帧结束，并在出现的帧上确定包含动作的bounding box。本文介绍的是前者。
预备知识 时空卷积 文章标题：A Closer Look at Spatiotemporal Convolutions for Action Recognition
首先探讨一下几种形式的时空卷积。
f-R2D: 帧上的2D卷积。对每一帧图像做2D卷积，最后将结果融合起来。R2D：视频段上的2D卷积。将帧的维度并入输入通道。假设有l帧，则网络的输入为3lxhxw。3D卷积：输入c×l×h×w（输入通道×帧数×高×宽），卷积核大小 CxKxKxZ(输出通道x宽x高x处理帧数)，假设步长都为1，则输出Cx(l-Z&#43;1)x(h-K&#43;1)x(h-K&#43;1). 以上都暂不考虑batchsize这个维度。3D卷积是2D卷积在时间维度上的拓展。MC卷积: 3D卷积和2D卷积的混合。MC卷积假设对时间处理越早越好，所以在前面的层用3D卷积，在后面的层用2D卷积。rMC卷积：与MC卷积结构相反，其在后面层采用3D卷积。2&#43;1D卷积：把3D卷积分解为连续的空间上的2D卷积和时间上的1D卷积。优点是增加非线性且更易优化(3D卷积难优化). 设原3D卷积核大小 KxKxZ，可拆成KxKx1(空间卷积)和1x1xZ(时间卷积) 模型 TSN 文章标题: Temporal Segment Networks: Towards Good
Practices for Deep Action Recognition
pytorch 实现： https://github.com/yjxiong/tsn-pytorch
Motivation ：1，连续的帧信息往往高度相关，所以对帧作密集采样是不必要的 2，之前的方法都要求输入视频为64~120帧，不能广泛地应用
网络结构
由上图所示，一个输入视频被分为 K 段（segment），一个片段（snippet，几帧图像叠加在一起）从它对应的段中随机采样得到。不同片段的类别得分融合，这是一个视频级的预测。然后对所有模式的预测融合产生最终的预测结果。
文中设置K=3，融合用的是平均函数，分类用的是softmax
TRN 文章标题 Temporal Relational Reasoning in Videos
本文是对TSN最后融合方式做一个改进。TSN每个snippet独立地预测，而TRN在预测前先进行snippet间的特征融合。另外TRN的输入用的是不同帧数的snippet(different scale)。
下图的框架图一目了然，算法实现流程就是先均匀地采样出不同scale的Segment 来对应 2-frame, 3-frame, …, N-frame relation；然后对每个Segment里小片提取 Spatial feature，进行 MLP 的 temporal fusion，送进分类器；最后将不同scale的分类score叠加来作最后预测值。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/17b0258f9cff28b08f06aa08a7a41b8e/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-05-24T14:33:24+08:00" />
<meta property="article:modified_time" content="2020-05-24T14:33:24+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【行为识别】TSN/TRN/TSM/SlowFast/Non-local</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="_0"></a>前言</h3> 
<p>记录视频理解领域的几篇文章吧，由于每篇值得记录的东西不多，所以合在一起。</p> 
<p>关于开源框架，有港中文多媒体实验室的MMAction。有设备的就尽量多跑跑模型吧</p> 
<p>视频相对于静态图像多了时间维度。静态图像的分类、检测、分割做得相对完善了，视频方面的工作想有创新必须在时间这个维度上钻研。</p> 
<p>注意 Action Recognition和Spatio-temporal Action Recognition(又称action localization)的区别。前者只需判断视频的类别，后者要在视频中确定动作从第几帧开始第几帧结束，并在出现的帧上确定包含动作的bounding box。本文介绍的是前者。</p> 
<h3><a id="_8"></a>预备知识</h3> 
<h5><a id="_9"></a>时空卷积</h5> 
<blockquote> 
 <p>文章标题：A Closer Look at Spatiotemporal Convolutions for Action Recognition</p> 
</blockquote> 
<p>首先探讨一下几种形式的时空卷积。</p> 
<ul><li>f-R2D: 帧上的2D卷积。对每一帧图像做2D卷积，最后将结果融合起来。</li><li>R2D：视频段上的2D卷积。将帧的维度并入输入通道。假设有l帧，则网络的输入为3lxhxw。</li><li>3D卷积：输入c×l×h×w（输入通道×帧数×高×宽），卷积核大小 CxKxKxZ(输出通道x宽x高x处理帧数)，假设步长都为1，则输出Cx(l-Z+1)x(h-K+1)x(h-K+1). 以上都暂不考虑batchsize这个维度。3D卷积是2D卷积在时间维度上的拓展。</li><li>MC卷积: 3D卷积和2D卷积的混合。MC卷积假设对时间处理越早越好，所以在前面的层用3D卷积，在后面的层用2D卷积。</li><li>rMC卷积：与MC卷积结构相反，其在后面层采用3D卷积。</li><li>2+1D卷积：把3D卷积分解为连续的空间上的2D卷积和时间上的1D卷积。优点是增加非线性且更易优化(3D卷积难优化). 设原3D卷积核大小 KxKxZ，可拆成KxKx1(空间卷积)和1x1xZ(时间卷积)</li></ul> 
<h3><a id="_19"></a>模型</h3> 
<h5><a id="TSN_21"></a>TSN</h5> 
<blockquote> 
 <p>文章标题: Temporal Segment Networks: Towards Good<br> Practices for Deep Action Recognition<br> pytorch 实现： https://github.com/yjxiong/tsn-pytorch</p> 
</blockquote> 
<p><strong>Motivation</strong> ：1，连续的帧信息往往高度相关，所以对帧作密集采样是不必要的 2，之前的方法都要求输入视频为64~120帧，不能广泛地应用<br> <strong>网络结构</strong><br> <img src="https://images2.imgbox.com/7f/d1/EoVSLVCv_o.png" alt="在这里插入图片描述"><br> 由上图所示，一个输入视频被分为 K 段（segment），一个片段（snippet，几帧图像叠加在一起）从它对应的段中随机采样得到。不同片段的类别得分融合，这是一个视频级的预测。然后对所有模式的预测融合产生最终的预测结果。<br> 文中设置K=3，融合用的是平均函数，分类用的是softmax</p> 
<h5><a id="TRN_33"></a>TRN</h5> 
<blockquote> 
 <p>文章标题 Temporal Relational Reasoning in Videos</p> 
</blockquote> 
<p>本文是对TSN最后融合方式做一个改进。TSN每个snippet独立地预测，而TRN在预测前先进行snippet间的特征融合。另外TRN的输入用的是不同帧数的snippet(different scale)。<br> 下图的框架图一目了然，算法实现流程就是先均匀地采样出不同scale的Segment 来对应 2-frame, 3-frame, …, N-frame relation；然后对每个Segment里小片提取 Spatial feature，进行 MLP 的 temporal fusion，送进分类器；最后将不同scale的分类score叠加来作最后预测值。<br> 图中g是两层MLP。h是一层MLP，其输出维度是类别数。<br> <img src="https://images2.imgbox.com/27/1e/OC3ApWyR_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="TSM_42"></a>TSM</h5> 
<blockquote> 
 <p>文章来源： https://arxiv.org/abs/1811.08383</p> 
</blockquote> 
<p><strong>Motivation</strong>：3D网络的计算量大，而2D网络没有利用时序信息。提出了时间移位(temporal shift)模块，能够用2D网络对时间建模。即将当前帧的特征图部分通道替换为前一帧或后一帧的通道。<br> <img src="https://images2.imgbox.com/0b/7b/NVAu2uEB_o.png" alt="在这里插入图片描述"><br> 图(a)是原始的特征图(省略了batchsize,w,h这三个与讨论无关的维度)，图（b）包括将前一帧和后一帧的通道替换当前帧的通道，适用于离线的方式。视频首尾帧对应位置用零填充。图©仅有前一帧的通道，适用于在线的方式。<br> temporal shift 模块应该作为原来2D网络的补充(即放在残差分支上，如下图(b))，而不能放在主干网络(如下图(a))，否则会破坏当前帧的空间语义。<br> <img src="https://images2.imgbox.com/00/0e/L3CwwYQd_o.png" alt="在这里插入图片描述"><br> 下图显示了in-place和residual两种不同方式，以及其他帧特征的不同占比带来的效果。<br> <img src="https://images2.imgbox.com/63/b8/BxLIofoi_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="SlowFast_52"></a>SlowFast</h5> 
<blockquote> 
 <p>文章来源：https://arxiv.org/pdf/1812.03982.pdf</p> 
</blockquote> 
<p><strong>Motivation</strong>：1，在视频动作识别中，类别语义一般变化得较慢，而动作语义变化得较快 2，人眼有20%的m细胞和80%的p细胞。m细胞在高时间频率下工作，对快速的时间变化有反应，但对空间细节或颜色不敏感。p细胞相反。<br> 所以设计了两路卷积神经网络，一路用来捕获不变或变化较慢的语义信息，称为Slow pathway，一路用来捕获快速变化的语义信息，称为Fast pathway。</p> 
<p><strong>网络结构</strong>：<br> <img src="https://images2.imgbox.com/7f/62/bNYH0w0e_o.png" alt="在这里插入图片描述"><br> Slow分支的帧采样更稀疏，因此会更侧重不变的语义(空间信息)，而Fast 分支的帧采样更密集且通道数更少(限制了表达能力)，因此会更侧重变化(语义)的语义。<br> 如果想要Fast分支更少关注空间信息，可以对Fast分支的输入作以下尝试：将帧宽高分别减半; 将帧灰度化；换成光流; 换成前后帧之差<br> 为了维持时间维度上的高分辨率，Fast分支没有时间维度的下采样操作(池化或带孔卷积)<br> Fast分支有侧向连接到Slow分支。这是一个在目标检测和视频理解很常用的手段。可供选择的方法如下：<br> <img src="https://images2.imgbox.com/66/e7/6AIiMdIv_o.png" alt="在这里插入图片描述"><br> 下面是以resnet50为backbone的SlowFast<br> <img src="https://images2.imgbox.com/80/07/YI834rFY_o.png" alt="在这里插入图片描述"></p> 
<p><strong>Tips</strong></p> 
<ul><li>Slow分支前面的层不宜用时间维度步长大于1的卷积核(称为非退化(non-degenerate)核，即非退化到2D卷积核)，否则准确率会下降。可能原因是浅层网络空间感受野不大，如果目标运动速度快的话相邻帧的同一位置上的语义可能没什么联系。</li><li>为了捕获时间上的联系，Fast分支宜用非退化核</li></ul> 
<h5><a id="Nonlocal_73"></a>Non-local</h5> 
<p><strong>Motivation</strong><br> 之前的方法捕获长范围特征需要累积很多层网络，导致学习效率太低，所有提出一种全局操作模块<br> <strong>网络结构</strong><br> <img src="https://images2.imgbox.com/e5/fb/pRsn1PlD_o.png" alt="在这里插入图片描述"><br> 1，首先对输入的特征图X分别进行<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         1 
        
       
         × 
        
       
         1 
        
       
         × 
        
       
         1 
        
       
      
        1\times 1\times1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span></span></span></span></span>卷积压缩通道数得到<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         θ 
        
       
         , 
        
       
         ϕ 
        
       
         , 
        
       
         g 
        
       
      
        \theta, \phi,g 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.02778em;">θ</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault">ϕ</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">g</span></span></span></span></span>特征<br> 2，reshape <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         θ 
        
       
         , 
        
       
         ϕ 
        
       
      
        \theta, \phi 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.02778em;">θ</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault">ϕ</span></span></span></span></span>，合并上述三个特征除通道数意外的维度，作矩阵相乘，再归一化系数0~1之间。这一步就是计算X的自相关系数，即所有像素对其他像素的关系。<br> 3，将自相关矩阵和g相乘，然后恢复原来的通道数，做残差。</p> 
<p>non-local是Attention机制的应用。其实融合全局信息可以直接用全连接层。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/5d8f7453d37786371561da324cec025e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">windows10升级Android Studio3.2到最新3.6.3版本遇到的冲突问题</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/be8539f67d33d2bb91c99c460f5f97f8/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【问题解决】mysql-[Warning] InnoDB: Table mysql/innodb_table_stats has length mismatch in the column name</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>