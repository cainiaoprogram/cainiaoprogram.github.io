<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ffmpeg的解码过程 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="ffmpeg的解码过程" />
<meta property="og:description" content="1. ffmpeg cpu解码 视频解码，是将压缩后的视频（压缩格式如H264）通过对应解码算法还原为YUV视频流的过程；在计算机看来，首先输入一段01串（压缩的视频），然后进行大量的浮点运算，最后再输出更长的一段01串（还原的非压缩视频）。计算机内部可以进行浮点数计算的部件是CPU，目前市场上涌现了一批GPU和类GPU芯片，如Nvidia、海思芯片甚至Intel自家的核显。利用前者进行解码一般称为“软解码”，后者被称为“硬解码”，如果没有特殊指定，FFMPEG是用CPU进行解码的，即软解。本文将介绍的是软解，也就是FFMPEG最通用的做法。
1.1 ffmpeg 软解API变化 FFPEAG官方参考技术手册：ffmpeg.org/developer.h…
1.2 ffmpeg解码套路 和很多工具一样，FFMPEG解码也是有套路的，以下是雷神的解码过程：
最新版的解码过程如下所示：
★文末名片可以免费领取音视频开发学习资料，内容包括（FFmpeg ，webRTC ，rtmp ，hls ，rtsp ，ffplay ，srs）以及音视频学习路线图等等。
见下方!↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓
1.3 详解解码过程 1.3.1 连接和打开视频流
连接和打开视频流**必然是后续进行解码的关键，该步骤对应的API调用为：
[int avformat_network_init(void)]：官方文档建议加上avformat_network_init()，虽然这个不是必须的。深入阅读该实现过程，说白了，该函数会初始化和启动底层的TLS库，这也就解释了网上很多资料关于如果要打开网络流的话，这个API是必须的的说法了。
int avformat_open_input(AVFormatContext * ps, const char filename, AVInputFormat* fmt, AVDictionary ** options) avformat_open_input()官方说法是“打开并读取视频头信息”，该函数较为复杂，笔者还没有完全吃透他的每一行源码，大致了解其功能为AVFormatContext内存分配，如果是视频文件，会探测其封装格式并将视频源装入内部buffer中；如果是网络流视频，则会创建socket等工作连接视频获取其内容，装入内部buffer中。最后读取视频头信息。
上面步骤结束后，就可以调用APIav_dump_format()打印文件的基本信息了，如文件时长、比特率、fps、编码格式等，信息大概如下：
Input #0, avi, from &#39;${input_video_file_name}&#39;: Metadata: encoder : Lavf57.83.100 Duration: 00:10:00.00, start: 0.000000, bitrate: 4196 kb/s Stream #0:0: Video: h264 (High) (H264 / 0x34363248), yuvj420p(pc, bt709, progressive), 1920x1080, 4194 kb/s, 12 fps, 12 tbr, 12 tbn, 24 tbc" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/44e1d71a01dbd7e8b9a1526b01f88649/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-08T20:44:24+08:00" />
<meta property="article:modified_time" content="2023-04-08T20:44:24+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ffmpeg的解码过程</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3>1. ffmpeg cpu解码</h3> 
<p>视频解码，是将压缩后的视频（压缩格式如H264）通过对应解码算法还原为YUV视频流的过程；在计算机看来，首先输入一段01串（压缩的视频），然后进行大量的<a href="https://link.juejin.cn?target=https%3A%2F%2Fwww.zhihu.com%2Fsearch%3Fq%3D%E6%B5%AE%E7%82%B9%E8%BF%90%E7%AE%97%26search_source%3DEntity%26hybrid_search_source%3DEntity%26hybrid_search_extra%3D%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A64739970%7D" rel="nofollow" title="浮点运算">浮点运算</a>，最后再输出更长的一段01串（还原的非压缩视频）。计算机内部可以进行浮点数计算的部件是CPU，目前市场上涌现了一批GPU和类GPU芯片，如Nvidia、<a href="https://link.juejin.cn?target=https%3A%2F%2Fwww.zhihu.com%2Fsearch%3Fq%3D%E6%B5%B7%E6%80%9D%E8%8A%AF%E7%89%87%26search_source%3DEntity%26hybrid_search_source%3DEntity%26hybrid_search_extra%3D%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A64739970%7D" rel="nofollow" title="海思芯片">海思芯片</a>甚至Intel自家的核显。利用前者进行解码一般称为“<strong>软解码</strong>”，后者被称为“<strong>硬解码</strong>”，如果没有特殊指定，FFMPEG是用CPU进行解码的，即<strong>软解</strong>。本文将介绍的是<strong>软解</strong>，也就是FFMPEG最通用的做法。</p> 
<h4>1.1 ffmpeg 软解API变化</h4> 
<p>FFPEAG官方参考技术手册：<a href="https://link.juejin.cn?target=https%3A%2F%2Fffmpeg.org%2Fdeveloper.html" rel="nofollow" title="ffmpeg.org/developer.h…">ffmpeg.org/developer.h…</a></p> 
<h4>1.2 ffmpeg解码套路</h4> 
<p>和很多工具一样，FFMPEG解码也是有套路的，以下是雷神的解码过程：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/48/ec/LfJncpbi_o.png"></p> 
<p>最新版的解码过程如下所示：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/5b/05/mUkTH8TH_o.png"></p> 
<p><span style="color:#fe2c24;"><strong>★文末名片可以免费领取音视频开发学习资料，内容包括（FFmpeg ，webRTC ，rtmp ，hls ，rtsp ，ffplay ，srs）以及音视频学习路线图等等。</strong></span></p> 
<p><span style="color:#fe2c24;"><strong>见下方!↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓</strong></span></p> 
<h4>1.3 详解解码过程</h4> 
<p>1.3.1 <strong>连接和打开视频流</strong></p> 
<ul><li> <p>连接和打开视频流**必然是后续进行解码的关键，该步骤对应的API调用为：</p> </li></ul> 
<p>[int avformat_network_init(void)]：官方文档建议加上<code>avformat_network_init()</code>，虽然这个不是必须的。深入阅读该实现过程，说白了，该函数会初始化和启动底层的<strong><a href="https://link.juejin.cn?target=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%3A%2F%2Fzh.wikipedia.org%2Fwiki%2F%E5%82%B3%E8%BC%B8%E5%B1%A4%E5%AE%89%E5%85%A8%E6%80%A7%E5%8D%94%E5%AE%9A" rel="nofollow" title="TLS库">TLS库</a></strong>，这也就解释了网上很多资料关于<strong>如果要打开网络流的话，这个API是必须的</strong>的说法了。</p> 
<ul><li> <p>int avformat_open_input(AVFormatContext <em>* ps, const char</em> filename, AVInputFormat* fmt, AVDictionary ** options) avformat_open_input()官方说法是“打开并读取视频头信息”，该函数较为复杂，笔者还没有完全吃透他的每一行源码，大致了解其功能为AVFormatContext内存分配，<strong>如果是视频文件，会探测其封装格式并将视频源装入内部buffer中；如果是网络流视频，则会创建socket等工作连接视频获取其内容，装入内部buffer中。最后读取视频头信息</strong>。</p> </li><li> <p>上面步骤结束后，就可以调用API<code>av_dump_format()</code>打印文件的基本信息了，如文件时长、<a href="https://link.juejin.cn?target=https%3A%2F%2Fwww.zhihu.com%2Fsearch%3Fq%3D%E6%AF%94%E7%89%B9%E7%8E%87%26search_source%3DEntity%26hybrid_search_source%3DEntity%26hybrid_search_extra%3D%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A64739970%7D" rel="nofollow" title="比特率">比特率</a>、fps、编码格式等，信息大概如下：</p> <p>Input #0, avi, from '${input_video_file_name}': Metadata: encoder : Lavf57.83.100 Duration: 00:10:00.00, start: 0.000000, bitrate: 4196 kb/s Stream #0:0: Video: h264 (High) (H264 / 0x34363248), yuvj420p(pc, bt709, progressive), 1920x1080, 4194 kb/s, 12 fps, 12 tbr, 12 tbn, 24 tbc</p> <p>另外，函数<code>av_register_all()</code>在FFMPEG4.0及以上版本中被弃用了，见<a href="https://link.juejin.cn?target=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%3A%2F%2Fgithub.com%2Fleandromoreira%2Fffmpeg-libav-tutorial%2Fissues%2F29" rel="nofollow" title="av_register_all() has been deprecated in ffmpeg 4.0">av_register_all() has been deprecated in ffmpeg 4.0</a>。**</p> </li></ul> 
<p>1.3.2 定位视频流数据</p> 
<p>无论是离线的还是在线的视频文件，相对正确的称呼应该是“多媒体”文件。要知道，这些文件一般不止有一路<a href="https://link.juejin.cn?target=https%3A%2F%2Fwww.zhihu.com%2Fsearch%3Fq%3D%E8%A7%86%E9%A2%91%E6%B5%81%E6%95%B0%E6%8D%AE%26search_source%3DEntity%26hybrid_search_source%3DEntity%26hybrid_search_extra%3D%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A64739970%7D" rel="nofollow" title="视频流数据">视频流数据</a>，可能同时包括多路音频数据、视频数据甚至字幕数据等。因此我们在做解码之前，需要首先找到我们需要的视频流数据。</p> 
<ul><li> <p>int avformat_find_stream_info(AVFormatContext** ic, AVDictionary ** options) avformat_find_stream_info()进一步解析该视频文件信息，主要是指AVFormatContext结构体的AVStream。 从雷神的FFmpeg源代码简单分析：avformat_find_stream_info()文章可以了解到，该函数内部已经做了一套完整的解码流程，获取了多媒体流的信息。 请注意，一个视频文件中可能会同时包括视频文件和音频文件等多个媒体流，这也就解释了为什么后续还要遍历AVFormatContext的streams成员（类型是AVStream）做对应的解码。</p> </li></ul> 
<p>1.3.3 准备解码器codec</p> 
<p>codec是FFMPEG的灵魂，顾名思义，解码必须由解码器完成。**准备解码器的步骤包括：寻找合适的解码器 -&gt; 拷贝解码器（optiona）-&gt; 打开解码器。</p> 
<ul><li> <p>寻找合适的解码器 - <code>AVCodec* avcodec_find_decoder(enum AVCodecID id)</code> <code>avcodec_find_decoder</code>是从codec库内返回与<code>id</code>匹配到的解码器。另外还有一个与其对应的寻找解码器的API-<code>AVCodec* avcodec_find_decoder_by_name(const char* name)</code>，这个函数是从codec库内返回名字为<code>name</code>的解码器，一般在硬解码时，会通过应解码器名字指定应解码器。（硬解码的流程会更复杂些，往往还需要打开相关硬件的底层库驱动等，本文不会涉及）。</p> </li><li> <p>拷贝解码器 - <code>AVCodecContext* avcodec_alloc_context3(const AVCodec* codec)</code>和<code>int avcodec_parameters_to_context(const AVCodec* codec, const AVCodecParameters* par)</code> <code>avcodec_alloc_context3()</code>创建了<code>AVCodecContext</code>，而<code>avcodec_parameters_to_context()</code>才真正执行了内容拷贝。</p> </li><li> <p>打开解码器 - [avcodec_open2(AVCodecContext* avctx, const AVCodec* codec, AVDictionary ** options) ，该函数主要服务于解码器，包括为其分配相关变量内存、检查解码器状态等。</p> </li></ul> 
<p>1.3.4 解码</p> 
<p>解码的核心是<strong>重复进行取包、拆包解帧的工作</strong>，这里说的包是FFMPEG非常重要的数据结构之一：<code>AVPacket</code>，帧是其中同样重要的数据结构：<code>AVFrame</code>。</p> 
<ul><li> <p><strong>AVPacket</strong></p> <p>该数据结构的介绍和分析网上资料很多，推荐阅读<a href="https://link.juejin.cn?target=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%3A%2F%2Fwww.jianshu.com%2Fp%2Fbb6d3905907e" rel="nofollow" title="FFMPEG">FFMPEG</a><a href="https://link.juejin.cn?target=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%3A%2F%2Fwww.jianshu.com%2Fp%2Fbb6d3905907e" rel="nofollow" title="结构体：AVPacket解析">结构体：AVPacket解析</a>，简言之，该结构保存了解码，或者说解压缩之前的多媒体数据，包括流数据本身和附加信息。AVPacket是由函数<a href="https://link.juejin.cn?target=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%3A%2F%2Fffmpeg.org%2Fdoxygen%2F2.8%2Fgroup__lavf__decoding.html%23ga4fdb3084415a82e3810de6ee60e46a61" rel="nofollow" title="int av_read_frame(AVFormatContext* s, AVPacket* pkt)">int av_read_frame(AVFormatContext* s, AVPacket* pkt)</a>获取得到的，该函数的具体实现在新版本中做了改良，确保每次取出的一定是完整的帧数据。</p> </li><li> <p><strong>AVFrame</strong></p> <p>该数据结构的介绍和分析网上资料也不少，推荐阅读<a href="https://link.juejin.cn?target=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%3A%2F%2Fwww.jianshu.com%2Fp%2F18fa498eb19e" rel="nofollow" title="FFMPEG">FFMPEG</a><a href="https://link.juejin.cn?target=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%3A%2F%2Fwww.jianshu.com%2Fp%2F18fa498eb19e" rel="nofollow" title="结构体分析：AVFrame">结构体分析：AVFrame</a>，简言之，该结构保存了解码后，即解压缩后的帧本身的数据和附加信息。AVFrame在新版本中由函数[int avcodec_send_packet(AVCodecContext* avctx, AVPacket* pkt)]和int avcodec_receive_frame(AVCodecContext* avctx, AVFrame* frame)产生，前者真正地执行了解码操作，后者则是从缓存或者解码器内存中取出解压出来地帧数据。</p> <p>老版本中用的是<code>avcodec_decode_video2()</code>，目前已经被弃用。 此外需要注意的是，一般而言，一次<code>avcodec_send_packet()</code>对应一次<code>avcodec_receive_frame()</code>，但是也会有一次对应多次的情况。这个关系参考一个<code>AVPacket</code>对应一个或多个<code>AVFrame</code>。</p> </li></ul> 
<h4>1.4 解码其他注意事项</h4> 
<p>1.4.1 帧转码</p> 
<p>软解得到的帧格式是YUV格式的，具体格式可以存放在<code>AVFrame</code>的<code>format（类型为int）</code>成员中，打印出数值后，再到<code>AVPixelFormat</code>中查找具体是哪个格式。一般而言，大多是实际使用场景中，最常用的是RGB格式，因此接下来就以RGB举例说明如何做帧转码。注意，其他格式的做法也是一样的。核心是调用<code>int sws_scale(struct SwsContext* c, ...)</code>，该函数接受的参数有一大堆，具体参数和对应的含义建议查询官网，该函数主要做了尺寸缩放（scale）和转码（transcode）工作。第一个参数struct [SwsContext] c，需要调用struct SwsContext* sws_getContext(..., enum AVPixelFormat dstFormat, ...)创建，该函数也是一堆参数，请自行官网查询，其中参数enum <a href="https://link.juejin.cn?target=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%3A%2F%2Fffmpeg.org%2Fdoxygen%2F3.0%2Fpixfmt_8h.html%23a9a8e335cf3be472042bc9f0cf80cd4c5" rel="nofollow" title="AVPixelFormat">AVPixelFormat</a> <em>dstFormat</em>，指定了目标格式，随后调用<code>sws_scale()</code>后得到的目标帧就是<em>dstFormat</em>格式的。因此，如果你的目标格式是RGB，只需要指定<em>dstFormat</em>为需要的RGB类型即可，FFMPEG中的RGB系列的有<code>AV_PIX_FMT_RGB24</code>、<code>AV_PIX_FMT_ARGB</code>等。</p> 
<p>1.4.2 帧输出</p> 
<p>除了考虑输出帧的格式，另一个实际的问题是：**解出来的帧放在哪儿，怎么放？放在哪儿的问题看个人需求，有些可能直接dump到磁盘，保存成本地视频文件或者一帧一帧的图片；在有些应用场景，解码可能只是系统最前端模块，此时可能需要存放到共享内存或者系统内存。随之而来的是怎么放的问题，前者如保存成视频，可以通过<code>fopen()</code>创建视频文件，接着再解码的循环内部调用<code>fwrite()</code>将帧数据保存到文件，最后用<code>fclose()</code>关闭即可；后者一定涉及到需要把<code>AVFrame</code>的帧数据转化成<code>uint8_t*</code>/<code>unsigned char*</code>的操作，可以调用API函数<code>int av_image_copy_to_buffer()</code>达到这个目的。</p> 
<p>1.4.3 刷新缓冲区</p> 
<p>在实际做解码工作时一定要注意<strong>刷新缓冲区</strong>！！！如果不这么做的话，最后解码出来的帧数目和实际视频帧数是对不齐的，会发现总是少了一些尾帧。原因就是FFMPEG内部有一个buffer，需要再把buffer的帧刷出来。其实做法也很简单，在解码的最后，将packet的<code>data</code>和<code>size</code>成员分别赋值为<code>nullptr</code>和<code>0</code>，这个时候缓冲区所有的帧数据都会被放进一个packet中，因此最后再进行一次解码就可以拿出所有的帧数据了。</p> 
<p>1.4.4 帧释放</p> 
<p>FFMPEG非常重要的一点，有些申请的变量一定要在结束前显示释放。具体哪些API的调用需要显示释放，在官方文档上都有详细的说明。这里补充本样例代码的变量释放部分：了解了以上几点，整个解码流程是真正搭建起来了。最后提<code>AVDictionary</code>，一个名称为<code>可选项</code>，但是实际上非常有用的结构。</p> 
<p>AVDictionary options<code>参数，尽管这个参数可以被置为</code>nullptr，但实际上这个参数的用处还是挺大的，比如设置FFMPEG缓存区大小、探测码流格式的时间、最大延时、超时时间、以及支持的协议的白名单等。</p> 
<h3>2. ffpeag 硬解</h3> 
<p>视频硬解码和软解码有什么区别？本质上没什么区别，都是用芯片执行编解码计算。</p> 
<p>软硬的称呼容易引起歧义，实质上：用CPU通用<a href="https://link.juejin.cn?target=https%3A%2F%2Fwww.zhihu.com%2Fsearch%3Fq%3D%E8%AE%A1%E7%AE%97%E5%8D%95%E5%85%83%26search_source%3DEntity%26hybrid_search_source%3DEntity%26hybrid_search_extra%3D%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A168240163%7D" rel="nofollow" title="计算单元">计算单元</a>（无论是Intel还是AMD）就是软解；用专用芯片模组（GPU、QSV等）就是硬解。</p> 
<p>因此区别也就出来了：底层接口不同、<a href="https://link.juejin.cn?target=https%3A%2F%2Fwww.zhihu.com%2Fsearch%3Fq%3D%E6%8C%87%E4%BB%A4%E9%9B%86%26search_source%3DEntity%26hybrid_search_source%3DEntity%26hybrid_search_extra%3D%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A168240163%7D" rel="nofollow" title="指令集">指令集</a>不同、硬件驱动不同。由此引申出来的问题也就显而易见了：</p> 
<ol><li> <p>首先，因为CPU是通用计算单元，所以接口通用，移植性好；而专用<a href="https://link.juejin.cn?target=https%3A%2F%2Fwww.zhihu.com%2Fsearch%3Fq%3D%E8%8A%AF%E7%89%87%E6%A8%A1%E7%BB%84%26search_source%3DEntity%26hybrid_search_source%3DEntity%26hybrid_search_extra%3D%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A168240163%7D" rel="nofollow" title="芯片模组">芯片模组</a>之间无法移植互用；</p> </li><li> <p>其次，因为CPU接口通用，因此编解码内部很多细节方便开发人员修改；而专用芯片模组，接口和驱动都是不同厂商提供的，很多是非开源，因此比较难控制内部细节。</p> </li><li> <p>最后，目前用CPU做编解码的效果，在实际测试下来会比专用芯片模组的效果好些。不过这个问题可以通过优化算法和芯片解决，这就是厂商的事儿了，我们控制不了。</p> </li></ol> 
<p>至于实际生活生产中，<strong>到底选择硬解码还是软解码？</strong></p> 
<p>要视不同情况而定。比如：</p> 
<ol><li> <p>CPU富余、需要精准控制解码流程、有解码算法的优化、通用性要求高，直接使用<a href="https://link.juejin.cn?target=https%3A%2F%2Fwww.zhihu.com%2Fsearch%3Fq%3D%E8%BD%AF%E8%A7%A3%26search_source%3DEntity%26hybrid_search_source%3DEntity%26hybrid_search_extra%3D%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A168240163%7D" rel="nofollow" title="软解">软解</a>（也就是CPU解码）；</p> </li><li> <p>有其他编解码芯片/模组、CPU不够用，就不得不需要转向硬解码（也就是专用芯片解码）。</p> </li></ol> 
<h4>2.1 支持的硬解格式</h4> 
<p>首先来看下FFMPEG原生支持哪些硬解码类型，在<code>AVHWDeviceType</code>（libavutil/hwcontext.h）中列举出所有原生支持的硬解码类型：</p> 
<pre>enum AVHWDeviceType {
    AV_HWDEVICE_TYPE_NONE,
    AV_HWDEVICE_TYPE_VDPAU,
    AV_HWDEVICE_TYPE_CUDA,
    AV_HWDEVICE_TYPE_VAAPI,
    AV_HWDEVICE_TYPE_DXVA2,
    AV_HWDEVICE_TYPE_QSV,
    AV_HWDEVICE_TYPE_VIDEOTOOLBOX,
    AV_HWDEVICE_TYPE_D3D11VA,
    AV_HWDEVICE_TYPE_DRM,
    AV_HWDEVICE_TYPE_OPENCL,
    AV_HWDEVICE_TYPE_MEDIACODEC,
};</pre> 
<p>上面的<code>AV_HWDEVICE_TYPE_CUDA</code>就是笔者目前正在做的<a href="https://link.juejin.cn?target=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%3A%2F%2Fzh.wikipedia.org%2Fwiki%2FCUDA" rel="nofollow" title="CUDA">CUDA</a>是NVIDIA的硬件加速库，<code>AV_HWDEVICE_TYPE_QSV</code>则是以前做的<a href="https://link.juejin.cn?target=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%3A%2F%2Fen.wikipedia.org%2Fwiki%2FIntel_Quick_Sync_Video" rel="nofollow" title="QSV">QSV</a>是Intel提供的一套集显上的硬件加速方案。</p> 
<p>那么，<strong>究竟要怎么知道系统当前的FFMPEG究竟支持哪些硬件库</strong>？</p> 
<p>可以通过命令行查看：<code>ffmpeg -hwaccel</code>。在<code>hardware acceleration methods:</code>下面可以看到当前FFMPEG集成的硬解码库。</p> 
<p>然后，<strong>如果发现自己需要的硬件库不在当前FFMPEG中怎么办？</strong></p> 
<p>答案是：很可能需要自己重新编译源码。</p> 
<h4>2.2 硬解API</h4> 
<p><strong>硬解</strong>步骤和<strong>软解</strong>步骤类似，笔者绘制了一幅FFMPEG硬件解码流程图：图中橙色部分是硬解码中有而软解码没有的部分。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/f2/ca/vvNLuR7t_o.png"></p> 
<p>2.2.1 寻找硬解codec</p> 
<ul><li> <p><code>AVCodec* avcodec_find_decoder_by_name(const char *name)</code> 通过名字来寻找对应的<code>AVCodec</code>。每一个<a href="https://link.juejin.cn?target=https%3A%2F%2Fwww.zhihu.com%2Fsearch%3Fq%3D%E8%A7%A3%E7%A0%81%E5%99%A8%26search_source%3DEntity%26hybrid_search_source%3DEntity%26hybrid_search_extra%3D%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A168240163%7D" rel="nofollow" title="解码器">解码器</a>的名字一定是全局唯一的，在<code>AVCodec</code>头文件中有相应的描述：</p> </li></ul> 
<blockquote> 
 <p>Name of the codec implementation. The name is <strong>globally unique</strong> among encoders and among decoders (but an encoder and a decoder can share the same name). This is the primary way to find a codec from the user perspective.</p> 
</blockquote> 
<p>其实在FFMPEG内部每一个解码器codec都是一个结构体，维护了该解码器自己的信息、具体执行的函数等信息。比如Intel的QSV解码器（在<a href="https://link.juejin.cn?target=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%3A%2F%2Fgithub.com%2FFFmpeg%2FFFmpeg%2Fblob%2F33623307417c8d1d38768eae61024a5bb879bd86%2Flibavcodec%2Fqsvdec_h2645.c" rel="nofollow" title="libavcodec/qsvdec_h2645.c">libavcodec/qsvdec_h2645.c</a>）是：</p> 
<pre>AVCodec ff_h264_qsv_decoder = {
    .name = "h264_qsv",
    .long_name = NULL_IF_CONFIG_SMALL("H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10 (Intel Quick Sync Video acceleration)"),
    .priv_data_size = sizeof(QSVH2645Context),
    .type = AVMEDIA_TYPE_VIDEO,
    .id = AV_CODEC_ID_H264,
    .init = qsv_decode_init,
    .decode = qsv_decode_frame,
    .flush = qsv_decode_flush,
    .close = qsv_decode_close,
    .capabilities = AV_CODEC_CAP_DELAY | AV_CODEC_CAP_DR1 | AV_CODEC_CAP_AVOID_PROBING | AV_CODEC_CAP_HYBRID,
    .priv_class = &amp;class,
    .pix_fmts = (const enum AVPixelFormat[]){ AV_PIX_FMT_NV12,
 AV_PIX_FMT_P010,
 AV_PIX_FMT_QSV,
 AV_PIX_FMT_NONE },
    .hw_configs = ff_qsv_hw_configs,
    .bsfs = "h264_mp4toannexb",
    .wrapper_name = "qsv",
};
复制代码</pre> 
<p>可以看到这个codec支持的<code>codec id</code>是<code>AV_CODEC_ID_H264</code>，支持的目标像素格式有<code>{ AV_PIX_FMT_NV12,AV_PIX_FMT_P010,AV_PIX_FMT_QSV,AV_PIX_FMT_NONE }</code>。 是的，<strong>硬件解码器不同于通用解码器，只能支持有限的目标像素格式。</strong> 再来看看CUDA解码器（在<a href="https://link.juejin.cn?target=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%3A%2F%2Fgithub.com%2FFFmpeg%2FFFmpeg%2Fblob%2Ff821ae8591add999ce717d932a80673fc29b00f6%2Flibavcodec%2Fcuviddec.c" rel="nofollow" title="libavcodec/cuviddec.c">libavcodec/cuviddec.c</a>），同样的，他也只能支持有限的目标像素格式：</p> 
<pre>AVCodec ff_##x##_cuvid_decoder = { \
        .name = #x "_cuvid", \
        .long_name = NULL_IF_CONFIG_SMALL("Nvidia CUVID " #X " decoder"), \
        .type = AVMEDIA_TYPE_VIDEO, \
        .id = AV_CODEC_ID_##X, \
        .priv_data_size = sizeof(CuvidContext), \
        .priv_class = &amp;x##_cuvid_class, \
        .init = cuvid_decode_init, \
        .close = cuvid_decode_end, \
        .decode = cuvid_decode_frame, \
        .receive_frame = cuvid_output_frame, \
        .flush = cuvid_flush, \
        .bsfs = bsf_name, \
        .capabilities = AV_CODEC_CAP_DELAY | AV_CODEC_CAP_AVOID_PROBING | AV_CODEC_CAP_HARDWARE, \
        .pix_fmts = (const enum AVPixelFormat[]){ AV_PIX_FMT_CUDA, \
 AV_PIX_FMT_NV12, \
 AV_PIX_FMT_P010, \
 AV_PIX_FMT_P016, \
 AV_PIX_FMT_NONE }, \
        .hw_configs = cuvid_hw_configs, \
        .wrapper_name = "cuvid", \
    };
复制代码</pre> 
<p>2.2.2 <strong>寻找硬解目标像素</strong></p> 
<p>硬解码codec支持的目标像素是有限的、且各自不一定相同。因此找到了硬解码codec之后，就得准备设置它的目标像素（pixel format）。</p> 
<ul><li> <p><code>enum AVHWDeviceType av_hwdevice_find_type_by_name(const char *name)</code> 这个函数是通过名称去寻找对应的<code>AVHWDeviceType</code>，这是一个枚举类型的变量（定义在libavutil/hwcontext.h头文件中）：</p> </li></ul> 
<pre>enum AVHWDeviceType {
 AV_HWDEVICE_TYPE_NONE,
 AV_HWDEVICE_TYPE_VDPAU,
 AV_HWDEVICE_TYPE_CUDA,
 AV_HWDEVICE_TYPE_VAAPI,
 AV_HWDEVICE_TYPE_DXVA2,
 AV_HWDEVICE_TYPE_QSV,
 AV_HWDEVICE_TYPE_VIDEOTOOLBOX,
 AV_HWDEVICE_TYPE_D3D11VA,
 AV_HWDEVICE_TYPE_DRM,
 AV_HWDEVICE_TYPE_OPENCL,
 AV_HWDEVICE_TYPE_MEDIACODEC,
 AV_HWDEVICE_TYPE_VULKAN,
};</pre> 
<p>这个类型和名称的关系表就简单多了，在FFMPEG代码中是用hw_type_names关系表来维护的（定义在libavutil/hwcontext.c文件中）：</p> 
<pre>static const char *const hw_type_names[] = {
  [AV_HWDEVICE_TYPE_CUDA]   = "cuda",
  [AV_HWDEVICE_TYPE_DRM]    = "drm",
  [AV_HWDEVICE_TYPE_DXVA2]  = "dxva2",
  [AV_HWDEVICE_TYPE_D3D11VA] = "d3d11va",
  [AV_HWDEVICE_TYPE_OPENCL] = "opencl",
  [AV_HWDEVICE_TYPE_QSV]    = "qsv",
  [AV_HWDEVICE_TYPE_VAAPI]  = "vaapi",
  [AV_HWDEVICE_TYPE_VDPAU]  = "vdpau",
  [AV_HWDEVICE_TYPE_VIDEOTOOLBOX] = "videotoolbox",
  [AV_HWDEVICE_TYPE_MEDIACODEC] = "mediacodec",
  [AV_HWDEVICE_TYPE_VULKAN] = "vulkan",
};</pre> 
<p><code>const AVCodecHWConfig * avcodec_get_hw_config (const AVCodec *codec, int index)</code> 紧接着，调用这个函数去获取到该解码器codec的硬件属性，比如可以支持的目标像素格式等。而这个信息就存储在<code>AVCodecHWConfig</code>中：</p> 
<pre>typedef struct AVCodecHWConfig {
 /**
 * A hardware pixel format which the codec can use. ！！！硬解码codec支持的像素格式！！！
 */
 enum AVPixelFormat pix_fmt;
 /**
 * Bit set of AV_CODEC_HW_CONFIG_METHOD_* flags, describing the possible
 * setup methods which can be used with this configuration.
 */
 int methods;
 /**
 * The device type associated with the configuration.
 *
 * Must be set for AV_CODEC_HW_CONFIG_METHOD_HW_DEVICE_CTX and
 * AV_CODEC_HW_CONFIG_METHOD_HW_FRAMES_CTX, otherwise unused.
 */
 enum AVHWDeviceType device_type;
} AVCodecHWConfig;</pre> 
<ul><li> <p>enum AVPixelFormat (*get_format)(struct AVCodecContext *s, const enum AVPixelFormat * fmt); 这是一个回调函数，它的作用就是告诉解码器codec自己的目标像素格式是什么。在上一步骤获取到了硬解码器codec可以支持的目标格式之后，就通过这个回调函数告知给codec，具体做法:</p> <pre>enum AVPixelFormat get_hw_format(struct AVCodecContext *s, const enum AVPixelFormat *fmt) {
 for (const enum AVPixelFormat *p = fmt; *p != -1; p++) {
 if (*p == hw_pix_fmt) return *p;
    }
 return AV_PIX_FMT_NONE;
}</pre> </li></ul> 
<p>我们也可以通过阅读<code>AVCodec</code>结构内对于这个回调函数的定义，可以知道：</p> 
<ol><li> 
  <ol><li> <p>fmt是这个解码器codec支持的像素格式，且按照质量优劣进行排序；</p> </li><li> <p>如果没有特别的需要，这个步骤是可以省略的。内部默认会使用“native”的格式。</p> </li></ol></li></ol> 
<pre>* callback to negotiate the pixelFormat
 * @param fmt is the list of formats which are supported by the codec, it is terminated by -1 as 0 is a valid format, the formats are ordered by quality. The first is always the native one.
 * @note The callback may be called again immediately if initialization for the selected (hardware-accelerated) pixel format failed.
 * @warning Behavior is undefined if the callback returns a value not in the fmt list of formats.
 * @return the chosen format
 * - encoding: unused
 * - decoding: Set by user, if not set the native format will be chosen.
复制代码</pre> 
<p>2.2.3 <strong>准备和打开硬解码</strong></p> 
<ul><li> <p><code>int av_hwdevice_ctx_create(AVBufferRef **pdevice_ref, enum AVHWDeviceType type, const char *device, AVDictionary *opts, int flags)</code> 这个函数的作用是，创建硬件设备相关的上下文信息<code>AVHWDeviceContext</code>，包括分配内存资源、对硬件设备进行初始化。 准备好硬件设备上下文<code>AVHWDeviceContext</code>后，需要把这个信息绑定到<code>AVCodecContext</code>，就可以像<strong>软解</strong>一样的流程执行解码操作了。</p> </li></ul> 
<p>2.2.4 <strong>取回数据</strong></p> 
<p>按照一般<strong>软解</strong>的流程，在调用<code>avcodec_receive_frame()</code>之后，得到的数据其实还在硬件模组/芯片上，也就是说，如果是用CUDA解码，数据是在显存上（或者说是在显卡encoder/decoder的buffer上）的。对于很多应用而言，解码之后往往还要进行后续操作，比如保存成一幅幅图片之类的，那么就需要把数据取回。</p> 
<ul><li> <p><code>int av_hwframe_transfer_data(AVFrame *dst, const AVFrame *src, int flags)</code> 这个函数是负责在cpu内存和硬件内存（原文是hw surface）之间做数据交换的。也就是说，它不但可以把数据从硬件surface上搬回系统内存，反向操作也支持；甚至可以直接在硬件内存之间做数据交互。</p> </li></ul> 
<p>原文链接：<a href="https://juejin.cn/post/7041094699079188517" rel="nofollow" title="ffmpeg的解码过程 - 掘金">ffmpeg的解码过程 - 掘金</a></p> 
<p><span style="color:#fe2c24;"><strong>★文末名片可以免费领取音视频开发学习资料，内容包括（FFmpeg ，webRTC ，rtmp ，hls ，rtsp ，ffplay ，srs）以及音视频学习路线图等等。</strong></span></p> 
<p><span style="color:#fe2c24;"><strong>见下方!↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓</strong></span></p> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/bfcd2cf4b4b291c10778b1860bd96531/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">降低gitlab的资源占用（直接干）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e45eca40b4bee4a97a71e620ae14fdb9/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">修改运行中的docker容器的端口映射（简单、高效）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>