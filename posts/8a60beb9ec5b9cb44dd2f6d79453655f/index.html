<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【CUDA编程--编程模型简介&amp;算子开发流程】 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【CUDA编程--编程模型简介&amp;算子开发流程】" />
<meta property="og:description" content="官方文档：https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html
什么是CUDA CUDA全称（Compute Unified Device Architecture）统一计算架构，是NVIDIA推出的并行计算平台深度学习加速：对于神经网络，无论是离线训练还是在线推理，都有巨量的矩阵、归一化、softmax等运算，且其中有非常多的并行计算，非常适合用GPU来进行运算加速
一般来说，应用程序混合有并行部分和顺序部分，因此系统设计时混合使用 GPU 和 CPU，以最大限度地提高整体性能。具有高度并行性的应用程序可以利用 GPU 的大规模并行特性来实现比 CPU 更高的性能 CUDA编程模型 多核CPU和众核GPU的出现意味着主流处理器芯片现在都是并行系统 kernel 核 不同于C语言中函数的调用，CUDA的内核函数调用时需要指定总的线程数量，以及相应的线程布局（grid和block维度配置） // C函数 function_name (argument list); // CUDA kernel call kernel_name&lt;&lt;&lt;4, 8&gt;&gt;&gt;(argument list); // 这里执行有grid中有4个block, 以及每个block中有8个线程运行 限定符 因为数据在全局内存中是线性存储的，所以可以通过blockIdx.x和threadIdx.x来标识grid中的线程，建立线程和数据之间的映射关系
核函数限定符的意义如下
限定符执行调用备注globalDevice执行Host调用/Device调用必须有一个void的返回类型deviceDevice执行Device调用–hostHost执行Host调用– 举例 实现的功能是两个长度为的tensor相加，每个block有1024个线程，一共有n/1024
个block
cudademo.cu #include &lt;iostream&gt; #include &lt;cuda_runtime.h&gt; // 代码的核心诉求(Cuda上运行)： // 输入a: 0,1,2,3,4..... // 输入b: 0,2,4,6,8..... // 输出c: 0,3,6,9,12..... __global__ void my_add_kernel(float* c, const float* a, const float* b, int n) { // 定义核函数 add for (int i = blockIdx." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/8a60beb9ec5b9cb44dd2f6d79453655f/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-11-13T20:23:54+08:00" />
<meta property="article:modified_time" content="2023-11-13T20:23:54+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【CUDA编程--编程模型简介&amp;算子开发流程】</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><img src="https://images2.imgbox.com/35/38/9ERdB2oz_o.png" alt="在这里插入图片描述"></p> 
<p>官方文档：https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html</p> 
<h2><a id="CUDA_4"></a>什么是CUDA</h2> 
<ul><li>CUDA全称（Compute Unified Device Architecture）统一计算架构，是NVIDIA推出的并行计算平台</li><li>深度学习加速：对于神经网络，无论是离线训练还是在线推理，都有巨量的矩阵、归一化、softmax等运算，且其中有非常多的并行计算，非常适合用GPU来进行运算加速<br> <img src="https://images2.imgbox.com/18/9a/SIZM76dp_o.png" alt="在这里插入图片描述"></li><li>一般来说，应用程序混合有并行部分和顺序部分，因此系统设计时混合使用 GPU 和 CPU，以最大限度地提高整体性能。具有高度并行性的应用程序可以利用 GPU 的大规模并行特性来实现比 CPU 更高的性能</li></ul> 
<h2><a id="CUDA_10"></a>CUDA编程模型</h2> 
<ul><li>多核CPU和众核GPU的出现意味着主流处理器芯片现在都是并行系统</li></ul> 
<h3><a id="kernel__13"></a>kernel 核</h3> 
<ul><li>不同于C语言中函数的调用，CUDA的内核函数调用时需要指定总的线程数量，以及相应的线程布局（grid和block维度配置）</li></ul> 
<pre><code class="prism language-c"><span class="token comment">// C函数</span>
<span class="token function">function_name</span> <span class="token punctuation">(</span>argument list<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment">// CUDA kernel call</span>
kernel_name<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>argument list<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token comment">// 这里执行有grid中有4个block, 以及每个block中有8个线程运行</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/18/4d/HeksFetn_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="_23"></a>限定符</h4> 
<p>因为数据在全局内存中是线性存储的，所以可以通过blockIdx.x和threadIdx.x来标识grid中的线程，建立线程和数据之间的映射关系<br> 核函数限定符的意义如下</p> 
<table><thead><tr><th>限定符</th><th>执行</th><th>调用</th><th>备注</th></tr></thead><tbody><tr><td><strong>global</strong></td><td>Device执行</td><td>Host调用/Device调用</td><td>必须有一个void的返回类型</td></tr><tr><td><strong>device</strong></td><td>Device执行</td><td>Device调用</td><td>–</td></tr><tr><td><strong>host</strong></td><td>Host执行</td><td>Host调用</td><td>–</td></tr></tbody></table> 
<h4><a id="_32"></a>举例</h4> 
<ul><li>实现的功能是两个长度为的tensor相加，每个block有1024个线程，一共有n/1024<br> 个block<br> cudademo.cu</li></ul> 
<pre><code class="prism language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;iostream&gt;</span>  </span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;cuda_runtime.h&gt;</span></span>

<span class="token comment">// 代码的核心诉求(Cuda上运行)：</span>
<span class="token comment">// 输入a: 0,1,2,3,4.....</span>
<span class="token comment">// 输入b: 0,2,4,6,8.....</span>
<span class="token comment">// 输出c: 0,3,6,9,12.....</span>

__global__ <span class="token keyword">void</span> <span class="token function">my_add_kernel</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token operator">*</span> c<span class="token punctuation">,</span>  
                            <span class="token keyword">const</span> <span class="token keyword">float</span><span class="token operator">*</span> a<span class="token punctuation">,</span>  
                            <span class="token keyword">const</span> <span class="token keyword">float</span><span class="token operator">*</span> b<span class="token punctuation">,</span>  
                            <span class="token keyword">int</span> n<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>  
    <span class="token comment">// 定义核函数 add  </span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>  
            i <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> i <span class="token operator">+=</span> gridDim<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>  
        c<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+</span> b<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>  
    <span class="token punctuation">}</span>  
<span class="token punctuation">}</span>

<span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token keyword">int</span> n <span class="token operator">=</span> <span class="token number">1000</span><span class="token punctuation">;</span> <span class="token comment">// 元素数量</span>

    <span class="token comment">// 分配主机内存用于输入和输出数组</span>
    <span class="token keyword">float</span><span class="token operator">*</span> host_a <span class="token operator">=</span> new <span class="token keyword">float</span><span class="token punctuation">[</span>n<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token keyword">float</span><span class="token operator">*</span> host_b <span class="token operator">=</span> new <span class="token keyword">float</span><span class="token punctuation">[</span>n<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token keyword">float</span><span class="token operator">*</span> host_c <span class="token operator">=</span> new <span class="token keyword">float</span><span class="token punctuation">[</span>n<span class="token punctuation">]</span><span class="token punctuation">;</span>

    <span class="token comment">// 在主机上填充输入数组 a 和 b</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        host_a<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> i<span class="token punctuation">;</span>
        host_b<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> i <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>

    <span class="token comment">// 在设备上分配内存用于输入和输出数组</span>
    <span class="token keyword">float</span><span class="token operator">*</span> device_a<span class="token punctuation">;</span>
    <span class="token keyword">float</span><span class="token operator">*</span> device_b<span class="token punctuation">;</span>
    <span class="token keyword">float</span><span class="token operator">*</span> device_c<span class="token punctuation">;</span>
    <span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>device_a<span class="token punctuation">,</span> n <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>device_b<span class="token punctuation">,</span> n <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>device_c<span class="token punctuation">,</span> n <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token comment">// 将输入数组从主机内存复制到设备内存</span>
    <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>device_a<span class="token punctuation">,</span> host_a<span class="token punctuation">,</span> n <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cudaMemcpyHostToDevice<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>device_b<span class="token punctuation">,</span> host_b<span class="token punctuation">,</span> n <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cudaMemcpyHostToDevice<span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token comment">// 定义 CUDA 核函数的执行配置</span>
    <span class="token keyword">int</span> block_size <span class="token operator">=</span> <span class="token number">256</span><span class="token punctuation">;</span>
    <span class="token keyword">int</span> grid_size <span class="token operator">=</span> <span class="token punctuation">(</span>n <span class="token operator">+</span> block_size <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> block_size<span class="token punctuation">;</span>

    <span class="token comment">// 调用 CUDA 核函数</span>
    my_add_kernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid_size<span class="token punctuation">,</span> block_size<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>device_c<span class="token punctuation">,</span> device_a<span class="token punctuation">,</span> device_b<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token comment">// 将输出数组从设备内存复制到主机内存</span>
    <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>host_c<span class="token punctuation">,</span> device_c<span class="token punctuation">,</span> n <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cudaMemcpyDeviceToHost<span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token comment">// 打印输出数组</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> host_c<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">&lt;&lt;</span> <span class="token string">" "</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> std<span class="token operator">::</span>endl<span class="token punctuation">;</span>

    <span class="token comment">// 释放主机和设备内存</span>
    delete<span class="token punctuation">[</span><span class="token punctuation">]</span> host_a<span class="token punctuation">;</span>
    delete<span class="token punctuation">[</span><span class="token punctuation">]</span> host_b<span class="token punctuation">;</span>
    delete<span class="token punctuation">[</span><span class="token punctuation">]</span> host_c<span class="token punctuation">;</span>
    <span class="token function">cudaFree</span><span class="token punctuation">(</span>device_a<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaFree</span><span class="token punctuation">(</span>device_b<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaFree</span><span class="token punctuation">(</span>device_c<span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<ul><li>直接编译执行</li></ul> 
<pre><code class="prism language-c"><span class="token punctuation">[</span><span class="token punctuation">]</span># nvcc cudademo<span class="token punctuation">.</span>cu <span class="token operator">-</span>o cudademo
<span class="token punctuation">[</span><span class="token punctuation">]</span># cudademo
<span class="token number">0</span> <span class="token number">3</span> <span class="token number">6</span> <span class="token number">9</span> <span class="token number">12</span> <span class="token number">15</span> <span class="token number">18</span> <span class="token number">21</span> <span class="token number">24</span> <span class="token number">27</span> <span class="token number">30</span> <span class="token number">33</span> <span class="token number">36</span> <span class="token number">39</span> <span class="token number">42</span> <span class="token number">45</span> <span class="token number">48</span> <span class="token number">51</span> <span class="token number">54</span> <span class="token number">57</span> <span class="token number">60</span> <span class="token number">63</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
</code></pre> 
<h5><a id="cuda_115"></a>如何准备安装cuda运行环境</h5> 
<ul><li>确认驱动版本等信息</li></ul> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span><span class="token punctuation">]</span>:~$ nvidia-smi
Mon Nov <span class="token number">13</span> <span class="token number">11</span>:22:17 <span class="token number">2023</span>       
+-----------------------------------------------------------------------------+
<span class="token operator">|</span> NVIDIA-SMI <span class="token number">515.65</span>.01    Driver Version: <span class="token number">515.65</span>.01    CUDA Version: <span class="token number">11.7</span>     <span class="token operator">|</span>
<span class="token operator">|</span>-------------------------------+----------------------+----------------------+
<span class="token operator">|</span> GPU  Name        Persistence-M<span class="token operator">|</span> Bus-Id        Disp.A <span class="token operator">|</span> Volatile Uncorr. ECC <span class="token operator">|</span>
<span class="token operator">|</span> Fan  Temp  Perf  Pwr:Usage/Cap<span class="token operator">|</span>         Memory-Usage <span class="token operator">|</span> GPU-Util  Compute M. <span class="token operator">|</span>
<span class="token operator">|</span>                               <span class="token operator">|</span>                      <span class="token operator">|</span>               MIG M. <span class="token operator">|</span>
<span class="token operator">|</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span><span class="token operator">+=</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span><span class="token operator">+=</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span><span class="token operator">|</span>
<span class="token operator">|</span>   <span class="token number">0</span>  NVIDIA GeForce <span class="token punctuation">..</span>.  Off  <span class="token operator">|</span> 00000000:35:00.0 Off <span class="token operator">|</span>                  N/A <span class="token operator">|</span>
<span class="token operator">|</span> <span class="token number">30</span>%   23C    P8    15W / 350W <span class="token operator">|</span>    807MiB / 24576MiB <span class="token operator">|</span>      <span class="token number">0</span>%      Default <span class="token operator">|</span>
<span class="token operator">|</span>                               <span class="token operator">|</span>                      <span class="token operator">|</span>                  N/A <span class="token operator">|</span>
+-------------------------------+----------------------+----------------------+
<span class="token operator">|</span>   <span class="token number">1</span>  NVIDIA GeForce <span class="token punctuation">..</span>.  Off  <span class="token operator">|</span> 00000000:36:00.0 Off <span class="token operator">|</span>                  N/A <span class="token operator">|</span>
<span class="token operator">|</span> <span class="token number">30</span>%   26C    P8    13W / 350W <span class="token operator">|</span>      2MiB / 24576MiB <span class="token operator">|</span>      <span class="token number">0</span>%      Default <span class="token operator">|</span>
<span class="token operator">|</span>                               <span class="token operator">|</span>                      <span class="token operator">|</span>                  N/A <span class="token operator">|</span>
+-------------------------------+----------------------+----------------------+
<span class="token operator">|</span>   <span class="token number">2</span>  NVIDIA GeForce <span class="token punctuation">..</span>.  Off  <span class="token operator">|</span> 00000000:39:00.0 Off <span class="token operator">|</span>                  N/A <span class="token operator">|</span>
<span class="token operator">|</span> <span class="token number">75</span>%   67C    P2   251W / 350W <span class="token operator">|</span>   9306MiB / 24576MiB <span class="token operator">|</span>     <span class="token number">85</span>%      Default <span class="token operator">|</span>
<span class="token operator">|</span>                               <span class="token operator">|</span>                      <span class="token operator">|</span>                  N/A <span class="token operator">|</span>
+-------------------------------+----------------------+----------------------+
<span class="token operator">|</span>   <span class="token number">3</span>  NVIDIA GeForce <span class="token punctuation">..</span>.  Off  <span class="token operator">|</span> 00000000:3D:00.0 Off <span class="token operator">|</span>                  N/A <span class="token operator">|</span>
<span class="token operator">|</span> <span class="token number">30</span>%   25C    P8    13W / 350W <span class="token operator">|</span>      2MiB / 24576MiB <span class="token operator">|</span>      <span class="token number">0</span>%      Default <span class="token operator">|</span>
<span class="token operator">|</span>                               <span class="token operator">|</span>                      <span class="token operator">|</span>                  N/A <span class="token operator">|</span>
+-------------------------------+----------------------+----------------------+
</code></pre> 
<ul><li>也就需要我们使用cuda11.7版本</li><li>cuda与pytorch 软件版本对应关系以及安装：https://pytorch.org/get-started/previous-versions/</li><li>cuda官方镜像网站：https://hub.docker.com/r/nvidia/cuda/tags</li><li>当前根据版本对应关系，直接可以使用 <code>docker pull nvidia/cuda:11.7.1-cudnn8-devel-ubuntu20.04</code> 该版本</li></ul> 
<pre><code class="prism language-yaml"><span class="token key atrule">version</span><span class="token punctuation">:</span> <span class="token string">'3'</span>
<span class="token key atrule">services</span><span class="token punctuation">:</span>
  <span class="token key atrule">my_container</span><span class="token punctuation">:</span>
    <span class="token key atrule">image</span><span class="token punctuation">:</span> harbor.uat.enflame.cc/library/enflame.cn/nvidia/cuda<span class="token punctuation">:</span>11.7.1<span class="token punctuation">-</span>cudnn8<span class="token punctuation">-</span>devel<span class="token punctuation">-</span>ubuntu20.04
    <span class="token key atrule">runtime</span><span class="token punctuation">:</span> nvidia  <span class="token comment"># 指定使用NVIDIA GPU运行时</span>
    <span class="token key atrule">devices</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> /dev/nvidia0  <span class="token comment"># 将主机的NVIDIA GPU设备映射到容器</span>
      <span class="token punctuation">-</span> /dev/nvidia1
      <span class="token punctuation">-</span> /dev/nvidia2
      <span class="token punctuation">-</span> /dev/nvidia3
      <span class="token punctuation">-</span> /dev/nvidia4
      <span class="token punctuation">-</span> /dev/nvidia5
      <span class="token punctuation">-</span> /dev/nvidia6
      <span class="token punctuation">-</span> /dev/nvidia7
    <span class="token key atrule">network_mode</span><span class="token punctuation">:</span> host
    <span class="token key atrule">command</span><span class="token punctuation">:</span> sleep 10000000000000000
    <span class="token key atrule">shm_size</span><span class="token punctuation">:</span> <span class="token string">'8gb'</span>
    <span class="token key atrule">privileged</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
</code></pre> 
<h3><a id="_169"></a>数据处理方式</h3> 
<p>佛林分类法Flynn’s Taxonomy，根据指令和数据进入CPU的方式对计算机架构进行分类，分为以下四类<br> - 单指令单数据 (SISD)：传统的单核处理数据方式<br> - 单指令多数据(SIMD)：单核执行一条指令完成多数据处理（游戏中向量、矩阵）<br> - 多指令单数据 (MISD)：多核执行不同的指令处理单个数据（少见）<br> - 多指令多数据 (MIMD)：多核执行不同的指令处理多个数据<br> <img src="https://images2.imgbox.com/8c/43/8E8PJU4O_o.png" alt="在这里插入图片描述"><br> 为了提高并行的计算能力，架构上实现下面这些性能提升：</p> 
<ul><li>降低延迟（latency）：指操作从开始到结束所需要的时间，一般用微秒计算，延迟越低越好</li><li>增加带宽（bandwidth）：单位时间内处理的数据量，一般用MB/s或者GB/s表示</li><li>增加吞吐（throughput）：单位时间内成功处理的运算数量</li></ul> 
<h3><a id="_181"></a>内存划分</h3> 
<ul><li> <p>分布式内存的多节点系统</p> 
  <ul><li>集群，各个机器之前通过网络进行数据交互</li><li>传统的比如redis集群这种通信等</li></ul> </li><li> <p>共享内存的多处理器系统</p> 
  <ul><li>包括单片多核，多片多核，主要是针对同设备多核进行数据通信，GPU是众核架构，表述为Single Instruction, Multiple Thread (SIMT)，不同于SIMD，SIMT是真正的启动了多个线程，执行相同的指令，去完成数据的并行运算</li><li>3090显卡拥有10496个CUDA核心,相比上一代2080Ti显卡的4352个CUDA核心数量增加了一倍</li></ul> </li></ul> 
<h3><a id="_190"></a>编程结构</h3> 
<ul><li> <p>CUDA编程让你可以在CPU-GPU的异构计算系统上高效执行应用程序，语法只是在C语言的基础上做了简单的扩展，CUDA C++ 通过允许程序员定义称为内核的 C++ 函数来扩展 C++，这些函数在调用时由 N 个不同的CUDA 线程并行执行 N 次，而不是像常规 C++ 函数那样只执行一次，在开始编程前，我们首先得理清Host和Device的概念</p> 
  <ul><li>Host：CPU及其内存</li><li>Device：GPU及其内存</li></ul> </li><li> <p>运行在GPU设备上的代码我们称为kernel</p> </li><li> <p>典型的CUDA程序处理流程</p> 
  <ul><li>分配内存，数据初始化<br> 将数据从Host拷贝到Device</li><li>调用kernels处理数据，然后存在GPU内存（Device）</li><li>将数据从Device拷贝到Host</li><li>内存释放<br> <img src="https://images2.imgbox.com/27/a2/p2bWZCL7_o.png" alt="在这里插入图片描述"></li></ul> </li></ul> 
<h3><a id="_204"></a>内存管理</h3> 
<table><thead><tr><th>标准C函数</th><th>CUDA 函数</th><th>CUDA函数说明</th></tr></thead><tbody><tr><td>malloc</td><td>cudaMalloc</td><td>GPU内存分配</td></tr><tr><td>memcpy</td><td>cudaMemcpy</td><td>用于Host和Device之间数据传输</td></tr><tr><td>memset</td><td>cudaMemset</td><td>设定数据填充到GPU内存中</td></tr><tr><td>free</td><td>cudaFree</td><td>释放GPU内存</td></tr></tbody></table> 
<p><img src="https://images2.imgbox.com/23/2c/UhR0z8Yn_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/cb/36/gBBWk1gg_o.png" alt="在这里插入图片描述"></p> 
<ul><li>CUDA 线程在执行期间可以访问多个内存空间中的数据</li><li>每个线程都有私有本地内存</li><li>每个线程块都有对该块的所有线程可见的共享内存，并且与该块具有相同的生命周期</li><li>线程块簇中的线程块可以对彼此的共享内存执行读、写和原子操作。所有线程都可以访问相同的全局内存。</li><li>还有两个可供所有线程访问的附加只读内存空间：常量内存空间和纹理内存空间</li></ul> 
<h4><a id="GridBlock_219"></a>Grid&amp;&amp;Block</h4> 
<p><img src="https://images2.imgbox.com/b1/c9/9JOuElBh_o.png" alt="在这里插入图片描述"></p> 
<ul><li>一个Kernel所launch的所有线程称为grid，他们共享相同的全局内存空间（global memory space）</li><li>一个grid由多个block（线程块）组成，block内部的线程可以通过以下两点进行协作（不同block间的线程不能协作） 
  <ul><li>block本地同步（synchronization）</li><li>block本地共享内存（sharedmemory）</li></ul> </li><li>一个线程通过blockIdx（grid内的index）和threadIdx（block内的index）这两个坐标变量（三维类型unit3）来唯一标识（线程运行的时候这两个变量会被CUDA赋上相应的坐标值，可以直接使用）</li><li>grid和block的维度信息通过gridDim和blockDim（dim3）来表示 
  <ul><li>gridDim：表示一个grid里面有多少个blocks</li><li>blockDim：表示一个block里面有多少个threads</li></ul> </li></ul> 
<h5><a id="_230"></a>线程块结构</h5> 
<p>为了方便起见，threadIdx是一个3分量向量，因此可以使用一维、二维或三维线程索引 来标识线程，形成一维、二维或三维线程块，称为线程块。这提供了一种自然的方式来调用域中元素（例如向量、矩阵或体积）的计算<br> <img src="https://images2.imgbox.com/bd/1f/m3JrAZfT_o.png" alt="在这里插入图片描述"><br> 每个块的线程数量是有限的，因为块中的所有线程都应驻留在同一个流式多处理器核心上，并且必须共享该核心的有限内存资源。在当前的 GPU 上，一个线程块最多可以包含 1024 个线程</p> 
<p>然而，一个内核可以由多个形状相同的线程块来执行，因此线程总数等于每个块的线程数乘以块数<br> 块被组织成一维、二维或三维线程块网格，如图4所示。网格中线程块的数量通常由正在处理的数据的大小决定，该数据通常超过系统中处理器的数量</p> 
<pre><code class="prism language-c"><span class="token comment">// Kernel definition</span>
__global__ <span class="token keyword">void</span> <span class="token function">MatAdd</span><span class="token punctuation">(</span><span class="token keyword">float</span> A<span class="token punctuation">[</span>N<span class="token punctuation">]</span><span class="token punctuation">[</span>N<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">float</span> B<span class="token punctuation">[</span>N<span class="token punctuation">]</span><span class="token punctuation">[</span>N<span class="token punctuation">]</span><span class="token punctuation">,</span>
                       <span class="token keyword">float</span> C<span class="token punctuation">[</span>N<span class="token punctuation">]</span><span class="token punctuation">[</span>N<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token punctuation">{<!-- --></span>
    <span class="token keyword">int</span> i <span class="token operator">=</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
    <span class="token keyword">int</span> j <span class="token operator">=</span> threadIdx<span class="token punctuation">.</span>y<span class="token punctuation">;</span>
    C<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> A<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">+</span> B<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>

<span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">{<!-- --></span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    <span class="token comment">// Kernel invocation with one block of N * N * 1 threads</span>
    <span class="token keyword">int</span> numBlocks <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span>
    dim3 <span class="token function">threadsPerBlock</span><span class="token punctuation">(</span>N<span class="token punctuation">,</span> N<span class="token punctuation">)</span><span class="token punctuation">;</span>
    MatAdd<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>numBlocks<span class="token punctuation">,</span> threadsPerBlock<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>A<span class="token punctuation">,</span> B<span class="token punctuation">,</span> C<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token punctuation">}</span>
</code></pre> 
<h4><a id="_259"></a>线程块簇</h4> 
<p><img src="https://images2.imgbox.com/b3/fb/VwD29chP_o.png" alt="在这里插入图片描述"></p> 
<ul><li>随着 NVIDIA计算能力 9.0的推出，CUDA 编程模型引入了一个可选的层次结构级别，称为由线程块组成的线程块集群。与如何保证线程块中的线程在流式多处理器上共同调度类似，集群中的线程块也保证在 GPU 中的 GPU 处理集群 (GPC) 上共同调度</li></ul> 
<h5><a id="_264"></a>小结</h5> 
<ul><li>如何要使用cuda进行并行计算，使用cuda函数进行数据等操作</li><li>cuda的线程结构将cuda编程结构分为块与线程，都是可以由一维、二维或三维唯一索引来标识,如代码</li></ul> 
<pre><code>{
    int i = threadIdx.x;
    int j = threadIdx.y;
    C[i][j] = A[i][j] + B[i][j];
}
</code></pre> 
<h3><a id="GPU_275"></a>GPU架构</h3> 
<blockquote> 
 <p>阅读官方文档：https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html， 一句话：太多了，不如实操，还是简单总结下</p> 
</blockquote> 
<p>GPU架构就是由可扩展的流式多处理器（Streaming Multiprocessors简称SM）阵列所构建，整个硬件的并行就是不断复制这种架构实现的。通常每个GPU都有多个SM，每个SM都支持上百个线程的并行，所以GPU可以支持上千个线程的并行</p> 
<h4><a id="SM_279"></a>SM中的核心部件</h4> 
<ul><li>CUDA Cores：核心，是最小的执行单元</li><li>Shared Memory/L1 Cache：共享内存和L1缓存，他们共用64KB空间，根据Bl</li><li>Register File：寄存器，根据线程划分</li><li>Load/Store Units：16个数据读写单元，支持16个线程一起从Cache/DRAM存取数据</li><li>Special Function Units：4个特殊函数处理单元，用于sin/cos这类指令计算</li><li>Warp Scheduler：Warp调度器，所谓Warp就是32个线程组成的线程束，是最小的调度单元</li></ul> 
<h4><a id="GPU_287"></a>GPU内存</h4> 
<p><img src="https://images2.imgbox.com/a1/a7/hbKzMVRv_o.png" alt="在这里插入图片描述"></p> 
<ul><li>寄存器：GPU上访问最快的存储空间，是SM中的稀缺资源，对于每个线程是私有的，Fermi架构中每个线程最多63个，Kepler结构扩展到255个。如果变量太多寄存器不够，会发生寄存器溢出，此时本地内存会存储多出来的变量，这种情况对性能影响较大。</li><li>本地内存：本质上是和全局内存放在同一块存储区域中（compute capability 2.0以上的设备，会放在SM的一级缓存，或者设备的二级缓存上）具有高延迟、低带宽，编译器可能会将以下变量存放于本地内存： 
  <ul><li>编译时期无法确定索引引用的本地数组</li><li>可能会消耗大量寄存器的较大本地数组/结构体</li><li>任何不满足核函数寄存器限定条件的变量</li></ul> </li><li>共享内存：因为是片上内存，所以相比全局内存和本地内存，具有较高的带宽和较低的延迟 
  <ul><li>SM中的一级缓存，和共享内存共享一个64k的片上内存，L1不可编程，共享内存可以</li><li>切勿过度使用共享内存，导致部分线程块无法被SM启动，影响Warp调度</li><li>可以使用__syncthreads()来实现Block内线程的同步</li></ul> </li><li>常量内存：驻留在设备内存中，每个SM都有专用的常量内存缓存 
  <ul><li>常量内存在核函数外，全局范围内声明，对于所有设备，只可以声明64k的常量内存</li><li>核函数无法修改，Host端使用cudaMemcpyToSymbol接口初始化</li></ul> </li><li>纹理内存：驻留在设备内存中，在每个SM的只读缓存中缓存，对于2D数据的访问性能较好</li><li>全局内存：GPU上最大的内存空间，延迟最高，使用最常见的内存，访问是对齐访问，也就是一次要读取指定大小（32，64，128）整数倍字节的内存，所以当线程束执行内存加载/存储时，需要满足的传输数量通常取决与以下两个因素：</li><li>跨线程的内存地址分布</li><li>内存事务的对齐方式。</li></ul> 
<table><thead><tr><th>修饰符</th><th>变量名称</th><th>存储器</th><th>作用域</th><th>生命周期</th></tr></thead><tbody><tr><td></td><td>float var</td><td>寄存器</td><td>线程</td><td>线程</td></tr><tr><td></td><td>float var[100]</td><td>本地</td><td>线程</td><td>线程</td></tr><tr><td><strong>share</strong></td><td>float var*</td><td>共享</td><td>块</td><td>块</td></tr><tr><td><strong>device</strong></td><td>float var*</td><td>全局</td><td>全局</td><td>应用程序</td></tr><tr><td><strong>constant</strong></td><td>float var*</td><td>常量</td><td>全局</td><td>应用程序</td></tr></tbody></table> 
<table><thead><tr><th>存储器</th><th>缓存</th><th>存取</th><th>范围</th><th>生命周期</th></tr></thead><tbody><tr><td>寄存器</td><td></td><td>R/W</td><td>一个线程</td><td>线程</td></tr><tr><td>本地</td><td>1.0以上有</td><td>R/W</td><td>一个线程</td><td>线程</td></tr><tr><td>共享</td><td></td><td>R/W</td><td>块内所有线程</td><td>块</td></tr><tr><td>全局</td><td>1.0以上有</td><td>R/W</td><td>所有线程+主机</td><td>主机配置</td></tr><tr><td>常量</td><td></td><td>R</td><td>所有线程+主机</td><td>主机配置</td></tr><tr><td>纹理</td><td></td><td>R</td><td>所有线程+主机</td><td>主机配置</td></tr></tbody></table> 
<ul><li>GPU缓存<br> 与CPU缓存类似，GPU缓存不可编程，其行为出厂是时已经设定好了。GPU上有4种缓存： 
  <ul><li>一级缓存：每个SM都有一个一级缓存，与共享内存公用空间</li><li>二级缓存：所有SM公用一个二级缓存</li><li>只读常量缓存：每个SM有</li><li>只读纹理缓存：每个SM有</li></ul> </li></ul> 
<h2><a id="_334"></a>案例</h2> 
<blockquote> 
 <p>不说概念了，直接肝</p> 
</blockquote> 
<h3><a id="GPU_337"></a>获取GPU信息</h3> 
<pre><code class="prism language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span><span class="token string">&lt;iostream&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span><span class="token string">&lt;cuda.h&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span><span class="token string">&lt;cuda_runtime.h&gt;</span></span>
<span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token keyword">int</span> dev <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
    cudaDeviceProp devProp<span class="token punctuation">;</span>
    <span class="token function">cudaGetDeviceProperties</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>devProp<span class="token punctuation">,</span> dev<span class="token punctuation">)</span><span class="token punctuation">;</span>
    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"GPU Device Name"</span> <span class="token operator">&lt;&lt;</span> dev <span class="token operator">&lt;&lt;</span> <span class="token string">": "</span> <span class="token operator">&lt;&lt;</span> devProp<span class="token punctuation">.</span>name <span class="token operator">&lt;&lt;</span> std<span class="token operator">::</span>endl<span class="token punctuation">;</span>
    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"SM Count: "</span> <span class="token operator">&lt;&lt;</span> devProp<span class="token punctuation">.</span>multiProcessorCount <span class="token operator">&lt;&lt;</span> std<span class="token operator">::</span>endl<span class="token punctuation">;</span>
    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"Shared Memory Size per Thread Block: "</span> <span class="token operator">&lt;&lt;</span> devProp<span class="token punctuation">.</span>sharedMemPerBlock <span class="token operator">/</span> <span class="token number">1024.0</span> <span class="token operator">&lt;&lt;</span> <span class="token string">" KB"</span> <span class="token operator">&lt;&lt;</span> std<span class="token operator">::</span>endl<span class="token punctuation">;</span>
    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"Threads per Thread Block: "</span> <span class="token operator">&lt;&lt;</span> devProp<span class="token punctuation">.</span>maxThreadsPerBlock <span class="token operator">&lt;&lt;</span> std<span class="token operator">::</span>endl<span class="token punctuation">;</span>
    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"Threads per SM: "</span> <span class="token operator">&lt;&lt;</span> devProp<span class="token punctuation">.</span>maxThreadsPerMultiProcessor <span class="token operator">&lt;&lt;</span> std<span class="token operator">::</span>endl<span class="token punctuation">;</span>
    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"Warps per SM: "</span> <span class="token operator">&lt;&lt;</span> devProp<span class="token punctuation">.</span>maxThreadsPerMultiProcessor <span class="token operator">/</span> <span class="token number">32</span> <span class="token operator">&lt;&lt;</span> std<span class="token operator">::</span>endl<span class="token punctuation">;</span>
    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token comment"># nvcc checkDeviceInfor.cu -o checkDeviceInfor</span>
GPU Device Name0: NVIDIA GeForce RTX <span class="token number">3090</span>
SM Count: <span class="token number">82</span>
Shared Memory Size per Thread Block: <span class="token number">48</span> KB
Threads per Thread Block: <span class="token number">1024</span>
Threads per SM: <span class="token number">1536</span>
Warps per SM: <span class="token number">48</span>
</code></pre> 
<h3><a id="CUDA_365"></a>实现CUDA算子</h3> 
<p>下面的案例你将学习到：</p> 
<ul><li>最简单的CUDA算子的写法。</li><li>最简洁的PyTorch和TensorFlow封装CUDA算子的方法。</li><li>几种编译CUDA算子的方法。</li><li>python调用CUDA算子的几种方式。</li><li>python中统计CUDA算子运行时间的正确方法。</li><li>PyTorch和TensorFlow自定义算子梯度的方法</li></ul> 
<h4><a id="_374"></a>代码结构</h4> 
<pre><code class="prism language-bash">├── include
│   └── add2.h <span class="token comment"># cuda算子的头文件</span>
├── kernel
│   ├── add2_kernel.cu <span class="token comment"># cuda算子的具体实现</span>
│   └── add2.cpp <span class="token comment"># cuda算子的cpp torch封装</span>
├── CMakeLists.txt
├── LICENSE
├── README.md
├── setup.py
├── time.py <span class="token comment"># 比较cuda算子和torch实现的时间差异</span>
└── train.py <span class="token comment"># 使用cuda算子来训练模型</span>
</code></pre> 
<p>代码结构还是很清晰的。include文件夹用来放cuda算子的头文件（.h文件），里面是cuda算子的定义。kernel文件夹放cuda算子的具体实现（.cu文件）和cpp torch的接口封装（.cpp文件）。</p> 
<p>最后是python端调用，我实现了两个功能。一是比较运行时间，上一篇教程详细讲过了；二是训练一个PyTorch模型</p> 
<h5><a id="_392"></a>头文件</h5> 
<pre><code class="prism language-c"><span class="token keyword">void</span> <span class="token function">launch_add2</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span>c<span class="token punctuation">,</span>
                 <span class="token keyword">const</span> <span class="token keyword">float</span> <span class="token operator">*</span>a<span class="token punctuation">,</span>
                 <span class="token keyword">const</span> <span class="token keyword">float</span> <span class="token operator">*</span>b<span class="token punctuation">,</span>
                 <span class="token keyword">int</span> n<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<h5><a id="_400"></a>算子核函数</h5> 
<ul><li>kernel/add2_kernel.cu</li></ul> 
<pre><code class="prism language-c">__global__ <span class="token keyword">void</span> <span class="token function">add2_kernel</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token operator">*</span> c<span class="token punctuation">,</span>
                            <span class="token keyword">const</span> <span class="token keyword">float</span><span class="token operator">*</span> a<span class="token punctuation">,</span>
                            <span class="token keyword">const</span> <span class="token keyword">float</span><span class="token operator">*</span> b<span class="token punctuation">,</span>
                            <span class="token keyword">int</span> n<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span> \
            i <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> i <span class="token operator">+=</span> gridDim<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        c<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+</span> b<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>

<span class="token keyword">void</span> <span class="token function">launch_add2</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token operator">*</span> c<span class="token punctuation">,</span>
                 <span class="token keyword">const</span> <span class="token keyword">float</span><span class="token operator">*</span> a<span class="token punctuation">,</span>
                 <span class="token keyword">const</span> <span class="token keyword">float</span><span class="token operator">*</span> b<span class="token punctuation">,</span>
                 <span class="token keyword">int</span> n<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    dim3 <span class="token function">grid</span><span class="token punctuation">(</span><span class="token punctuation">(</span>n <span class="token operator">+</span> <span class="token number">1023</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">1024</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    dim3 <span class="token function">block</span><span class="token punctuation">(</span><span class="token number">1024</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    add2_kernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>c<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<ul><li>kernel/add2.cpp</li></ul> 
<pre><code class="prism language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;torch/extension.h&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">"add2.h"</span></span>

<span class="token keyword">void</span> <span class="token function">torch_launch_add2</span><span class="token punctuation">(</span>torch<span class="token operator">::</span>Tensor <span class="token operator">&amp;</span>c<span class="token punctuation">,</span>
                       <span class="token keyword">const</span> torch<span class="token operator">::</span>Tensor <span class="token operator">&amp;</span>a<span class="token punctuation">,</span>
                       <span class="token keyword">const</span> torch<span class="token operator">::</span>Tensor <span class="token operator">&amp;</span>b<span class="token punctuation">,</span>
                       <span class="token keyword">int</span> n<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token function">launch_add2</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span><span class="token punctuation">)</span>c<span class="token punctuation">.</span><span class="token function">data_ptr</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                <span class="token punctuation">(</span><span class="token keyword">const</span> <span class="token keyword">float</span> <span class="token operator">*</span><span class="token punctuation">)</span>a<span class="token punctuation">.</span><span class="token function">data_ptr</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                <span class="token punctuation">(</span><span class="token keyword">const</span> <span class="token keyword">float</span> <span class="token operator">*</span><span class="token punctuation">)</span>b<span class="token punctuation">.</span><span class="token function">data_ptr</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                n<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>

<span class="token function">PYBIND11_MODULE</span><span class="token punctuation">(</span>TORCH_EXTENSION_NAME<span class="token punctuation">,</span> m<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    m<span class="token punctuation">.</span><span class="token function">def</span><span class="token punctuation">(</span><span class="token string">"torch_launch_add2"</span><span class="token punctuation">,</span>
          <span class="token operator">&amp;</span>torch_launch_add2<span class="token punctuation">,</span>
          <span class="token string">"add2 kernel warpper"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span> <span class="token comment">// cpp端用的是pybind11进行封装,适用python</span>

<span class="token function">TORCH_LIBRARY</span><span class="token punctuation">(</span>add2<span class="token punctuation">,</span> m<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    m<span class="token punctuation">.</span><span class="token function">def</span><span class="token punctuation">(</span><span class="token string">"torch_launch_add2"</span><span class="token punctuation">,</span> torch_launch_add2<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span> <span class="token comment">// cpp端用的是TORCH_LIBRARY进行封装,适用于</span>

</code></pre> 
<h5><a id="_449"></a>编译</h5> 
<h6><a id="JIT_450"></a>JIT</h6> 
<p>JIT就是just-in-time，也就是即时编译，或者说动态编译，就是说在python代码运行的时候再去编译cpp和cuda文件。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>cpp_extension <span class="token keyword">import</span> load
cuda_module <span class="token operator">=</span> load<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">"add2"</span><span class="token punctuation">,</span>
                   extra_include_paths<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"include"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                   sources<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"kernel/add2.cpp"</span><span class="token punctuation">,</span> <span class="token string">"kernel/add2_kernel.cu"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                   verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
cuda_module<span class="token punctuation">.</span>torch_launch_add2<span class="token punctuation">(</span>c<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> n<span class="token punctuation">)</span>
</code></pre> 
<p>需要注意的就是两个参数，extra_include_paths表示包含的头文件目录，sources表示需要编译的代码，一般就是.cpp和.cu文件<br> 运行成功可以看到</p> 
<pre><code class="prism language-bach">[1/2] nvcc -c add2_kernel.cu -o add2_kernel.cuda.o
[2/3] c++ -c add2.cpp -o add2.o
[3/3] c++ add2.o add2_kernel.cuda.o -shared -o add2.so
</code></pre> 
<h6><a id="Setuptools_469"></a>Setuptools</h6> 
<p>编译的方式是通过Setuptools，也就是编写setup.py，具体代码如下</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> setuptools <span class="token keyword">import</span> setup
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>cpp_extension <span class="token keyword">import</span> BuildExtension<span class="token punctuation">,</span> CUDAExtension

setup<span class="token punctuation">(</span>
    name<span class="token operator">=</span><span class="token string">"add2"</span><span class="token punctuation">,</span>
    include_dirs<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"include"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    ext_modules<span class="token operator">=</span><span class="token punctuation">[</span>
        CUDAExtension<span class="token punctuation">(</span>
            <span class="token string">"add2"</span><span class="token punctuation">,</span>
            <span class="token punctuation">[</span><span class="token string">"kernel/add2.cpp"</span><span class="token punctuation">,</span> <span class="token string">"kernel/add2_kernel.cu"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
    <span class="token punctuation">]</span><span class="token punctuation">,</span>
    cmdclass<span class="token operator">=</span><span class="token punctuation">{<!-- --></span>
        <span class="token string">"build_ext"</span><span class="token punctuation">:</span> BuildExtension
    <span class="token punctuation">}</span>
<span class="token punctuation">)</span>
</code></pre> 
<p>执行<code>python3 setup.py install</code><br> 这样就能生成动态链接库，同时将add2添加为python的模块了，可以直接import add2来调用</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> add2
add2<span class="token punctuation">.</span>torch_launch_add2<span class="token punctuation">(</span>c<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> n<span class="token punctuation">)</span>
</code></pre> 
<h6><a id="cmake_497"></a>cmake</h6> 
<p>最后就是cmake编译的方式了，要编写一个CMakeLists.txt文件，代码如下</p> 
<pre><code>cmake_minimum_required(VERSION 3.1 FATAL_ERROR)
# 修改为你自己的nvcc路径，或者删掉这行，如果能运行的话。
set(CMAKE_CUDA_COMPILER "/usr/local/cuda/bin/nvcc")
project(add2 LANGUAGES CXX CUDA)

find_package(Torch REQUIRED)
find_package(CUDA REQUIRED)
find_library(TORCH_PYTHON_LIBRARY torch_python PATHS "${TORCH_INSTALL_PREFIX}/lib")

# 修改为你自己的python路径，或者删掉这行，如果能运行的话。
include_directories(/usr/include/python3.7)
include_directories(include)

set(SRCS kernel/add2.cpp kernel/add2_kernel.cu)
add_library(add2 SHARED ${SRCS})

target_link_libraries(add2 "${TORCH_LIBRARIES}" "${TORCH_PYTHON_LIBRARY}")
</code></pre> 
<p>编译</p> 
<pre><code class="prism language-bash"><span class="token function">mkdir</span> build
<span class="token builtin class-name">cd</span> build
cmake <span class="token parameter variable">-DCMAKE_PREFIX_PATH</span><span class="token operator">=</span><span class="token string">"<span class="token variable"><span class="token variable">$(</span>python3 <span class="token parameter variable">-c</span> <span class="token string">'import torch.utils; print(torch.utils.cmake_prefix_path)'</span><span class="token variable">)</span></span>"</span> <span class="token punctuation">..</span>/
<span class="token function">make</span>
</code></pre> 
<p>最后会在build目录下生成一个libadd2.so，通过如下方式在python端调用：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
torch<span class="token punctuation">.</span>ops<span class="token punctuation">.</span>load_library<span class="token punctuation">(</span><span class="token string">"build/libadd2.so"</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>ops<span class="token punctuation">.</span>add2<span class="token punctuation">.</span>torch_launch_add2<span class="token punctuation">(</span>c<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> n<span class="token punctuation">)</span>
</code></pre> 
<h6><a id="_532"></a>将上诉代码汇总</h6> 
<ul><li>timer.py</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">import</span> time
<span class="token keyword">import</span> argparse
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> torch

<span class="token comment"># c = a + b (shape: [n])</span>
n <span class="token operator">=</span> <span class="token number">1024</span> <span class="token operator">*</span> <span class="token number">1024</span>
a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>n<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">"cuda:0"</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>n<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">"cuda:0"</span><span class="token punctuation">)</span>
cuda_c <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>n<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">"cuda:0"</span><span class="token punctuation">)</span>

ntest <span class="token operator">=</span> <span class="token number">10</span>

<span class="token keyword">def</span> <span class="token function">show_time</span><span class="token punctuation">(</span>func<span class="token punctuation">)</span><span class="token punctuation">:</span>
    times <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    res <span class="token operator">=</span> <span class="token boolean">None</span>
    <span class="token comment"># GPU warm up</span>
    <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        res <span class="token operator">=</span> func<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>ntest<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># sync the threads to get accurate cuda running time</span>
        torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>synchronize<span class="token punctuation">(</span>device<span class="token operator">=</span><span class="token string">"cuda:0"</span><span class="token punctuation">)</span>
        start_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>
        func<span class="token punctuation">(</span><span class="token punctuation">)</span>
        torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>synchronize<span class="token punctuation">(</span>device<span class="token operator">=</span><span class="token string">"cuda:0"</span><span class="token punctuation">)</span>
        end_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>
        times<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span>end_time<span class="token operator">-</span>start_time<span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">1e6</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> times<span class="token punctuation">,</span> res

<span class="token keyword">def</span> <span class="token function">run_cuda</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> args<span class="token punctuation">.</span>compiler <span class="token operator">==</span> <span class="token string">'jit'</span><span class="token punctuation">:</span>
        cuda_module<span class="token punctuation">.</span>torch_launch_add2<span class="token punctuation">(</span>cuda_c<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> n<span class="token punctuation">)</span>
    <span class="token keyword">elif</span> args<span class="token punctuation">.</span>compiler <span class="token operator">==</span> <span class="token string">'setup'</span><span class="token punctuation">:</span>
        add2<span class="token punctuation">.</span>torch_launch_add2<span class="token punctuation">(</span>cuda_c<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> n<span class="token punctuation">)</span>
    <span class="token keyword">elif</span> args<span class="token punctuation">.</span>compiler <span class="token operator">==</span> <span class="token string">'cmake'</span><span class="token punctuation">:</span>
        torch<span class="token punctuation">.</span>ops<span class="token punctuation">.</span>add2<span class="token punctuation">.</span>torch_launch_add2<span class="token punctuation">(</span>cuda_c<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> n<span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token keyword">raise</span> Exception<span class="token punctuation">(</span><span class="token string">"Type of cuda compiler must be one of jit/setup/cmake."</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> cuda_c

<span class="token keyword">def</span> <span class="token function">run_torch</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    c <span class="token operator">=</span> a <span class="token operator">+</span> b
    <span class="token keyword">return</span> c<span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>
    parser <span class="token operator">=</span> argparse<span class="token punctuation">.</span>ArgumentParser<span class="token punctuation">(</span><span class="token punctuation">)</span>
    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--compiler'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">str</span><span class="token punctuation">,</span> choices<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'jit'</span><span class="token punctuation">,</span> <span class="token string">'setup'</span><span class="token punctuation">,</span> <span class="token string">'cmake'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token string">'jit'</span><span class="token punctuation">)</span>
    args <span class="token operator">=</span> parser<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">if</span> args<span class="token punctuation">.</span>compiler <span class="token operator">==</span> <span class="token string">'jit'</span><span class="token punctuation">:</span>
        <span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>cpp_extension <span class="token keyword">import</span> load
        cuda_module <span class="token operator">=</span> load<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">"add2"</span><span class="token punctuation">,</span>
                           extra_include_paths<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"include"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                           sources<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"pytorch/add2_ops.cpp"</span><span class="token punctuation">,</span> <span class="token string">"kernel/add2_kernel.cu"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                           verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token keyword">elif</span> args<span class="token punctuation">.</span>compiler <span class="token operator">==</span> <span class="token string">'setup'</span><span class="token punctuation">:</span>
        <span class="token keyword">import</span> add2
    <span class="token keyword">elif</span> args<span class="token punctuation">.</span>compiler <span class="token operator">==</span> <span class="token string">'cmake'</span><span class="token punctuation">:</span>
        torch<span class="token punctuation">.</span>ops<span class="token punctuation">.</span>load_library<span class="token punctuation">(</span><span class="token string">"build/libadd2.so"</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token keyword">raise</span> Exception<span class="token punctuation">(</span><span class="token string">"Type of cuda compiler must be one of jit/setup/cmake."</span><span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Running cuda..."</span><span class="token punctuation">)</span>
    cuda_time<span class="token punctuation">,</span> cuda_res <span class="token operator">=</span> show_time<span class="token punctuation">(</span>run_cuda<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Cuda time:  {:.3f}us"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>cuda_time<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Running torch..."</span><span class="token punctuation">)</span>
    torch_time<span class="token punctuation">,</span> torch_res <span class="token operator">=</span> show_time<span class="token punctuation">(</span>run_torch<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Torch time:  {:.3f}us"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>torch_time<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    torch<span class="token punctuation">.</span>allclose<span class="token punctuation">(</span>cuda_res<span class="token punctuation">,</span> torch_res<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Kernel test passed."</span><span class="token punctuation">)</span>
</code></pre> 
<ul><li>train.py</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">import</span> argparse
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Function

<span class="token keyword">class</span> <span class="token class-name">AddModelFunction</span><span class="token punctuation">(</span>Function<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token decorator annotation punctuation">@staticmethod</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>
        c <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token operator">=</span><span class="token string">"cuda:0"</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> args<span class="token punctuation">.</span>compiler <span class="token operator">==</span> <span class="token string">'jit'</span><span class="token punctuation">:</span>
            cuda_module<span class="token punctuation">.</span>torch_launch_add2<span class="token punctuation">(</span>c<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> n<span class="token punctuation">)</span>
        <span class="token keyword">elif</span> args<span class="token punctuation">.</span>compiler <span class="token operator">==</span> <span class="token string">'setup'</span><span class="token punctuation">:</span>
            add2<span class="token punctuation">.</span>torch_launch_add2<span class="token punctuation">(</span>c<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> n<span class="token punctuation">)</span>
        <span class="token keyword">elif</span> args<span class="token punctuation">.</span>compiler <span class="token operator">==</span> <span class="token string">'cmake'</span><span class="token punctuation">:</span>
            torch<span class="token punctuation">.</span>ops<span class="token punctuation">.</span>add2<span class="token punctuation">.</span>torch_launch_add2<span class="token punctuation">(</span>c<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> n<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> Exception<span class="token punctuation">(</span><span class="token string">"Type of cuda compiler must be one of jit/setup/cmake."</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> c

    <span class="token decorator annotation punctuation">@staticmethod</span>
    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> grad_output<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token punctuation">(</span>grad_output<span class="token punctuation">,</span> grad_output<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">AddModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>AddModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>n <span class="token operator">=</span> n
        self<span class="token punctuation">.</span>a <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a<span class="token punctuation">.</span>data<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>mean<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b<span class="token punctuation">.</span>data<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>mean<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        a2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>square<span class="token punctuation">(</span>self<span class="token punctuation">.</span>a<span class="token punctuation">)</span>
        b2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>square<span class="token punctuation">(</span>self<span class="token punctuation">.</span>b<span class="token punctuation">)</span>
        c <span class="token operator">=</span> AddModelFunction<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>a2<span class="token punctuation">,</span> b2<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n<span class="token punctuation">)</span>
        <span class="token keyword">return</span> c

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>
    parser <span class="token operator">=</span> argparse<span class="token punctuation">.</span>ArgumentParser<span class="token punctuation">(</span><span class="token punctuation">)</span>
    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--compiler'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">str</span><span class="token punctuation">,</span> choices<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'jit'</span><span class="token punctuation">,</span> <span class="token string">'setup'</span><span class="token punctuation">,</span> <span class="token string">'cmake'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token string">'jit'</span><span class="token punctuation">)</span>
    args <span class="token operator">=</span> parser<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">if</span> args<span class="token punctuation">.</span>compiler <span class="token operator">==</span> <span class="token string">'jit'</span><span class="token punctuation">:</span>
        <span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>cpp_extension <span class="token keyword">import</span> load
        cuda_module <span class="token operator">=</span> load<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">"add2"</span><span class="token punctuation">,</span>
                           extra_include_paths<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"include"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                           sources<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"pytorch/add2_ops.cpp"</span><span class="token punctuation">,</span> <span class="token string">"kernel/add2_kernel.cu"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                           verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token keyword">elif</span> args<span class="token punctuation">.</span>compiler <span class="token operator">==</span> <span class="token string">'setup'</span><span class="token punctuation">:</span>
        <span class="token keyword">import</span> add2
    <span class="token keyword">elif</span> args<span class="token punctuation">.</span>compiler <span class="token operator">==</span> <span class="token string">'cmake'</span><span class="token punctuation">:</span>
        torch<span class="token punctuation">.</span>ops<span class="token punctuation">.</span>load_library<span class="token punctuation">(</span><span class="token string">"build/libadd2.so"</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token keyword">raise</span> Exception<span class="token punctuation">(</span><span class="token string">"Type of cuda compiler must be one of jit/setup/cmake."</span><span class="token punctuation">)</span>

    n <span class="token operator">=</span> <span class="token number">1024</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Initializing model..."</span><span class="token punctuation">)</span>
    model <span class="token operator">=</span> AddModel<span class="token punctuation">(</span>n<span class="token punctuation">)</span>
    model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token operator">=</span><span class="token string">"cuda:0"</span><span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Initializing optimizer..."</span><span class="token punctuation">)</span>
    opt <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Begin training..."</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">500</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        opt<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        output <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token punctuation">)</span>
        loss <span class="token operator">=</span> output<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> epoch <span class="token operator">%</span> <span class="token number">25</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"epoch {:&gt;3d}: loss = {:&gt;8.3f}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>epoch<span class="token punctuation">,</span> loss<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/04474934296bb91b3546ea5f4026f96c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">virtualbox安装时发生严重错误</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/58851d967ebb90133755bfdf92866a31/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">零基础独立开发QT上位机项目指北</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>