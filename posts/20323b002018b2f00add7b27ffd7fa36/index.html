<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【目标检测】yolo系列-yolo v2(yolo9000)(2016年) - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【目标检测】yolo系列-yolo v2(yolo9000)(2016年)" />
<meta property="og:description" content="目录
YOLOv2创新点
☆Batch Normalization
High Resolution Classifier (高分辨率图像微调主干网络)
☆使用 anchor box 先验框
☆New Network：Darknet-19
Dimension Clusters（聚类提取先验框的尺度信息）
Direct location prediction（约束预测边框的位置）
Fine-Grained Features （细粒度特性）
Multi-Scale Training（多尺度训练）
YOLOv2 存在的问题
YOLOv2创新点 YOLOv1 虽然检测速度快，但在定位方面不够准确，并且召回率较低。为了提升定位准确度，改善召回率，YOLOv2 在 YOLOv1 的基础上提出了几种改进策略
YOLOv2-1
☆Batch Normalization YOLOv2 中在每个卷积层后加 Batch Normalization (BN) 层，去掉 dropout。BN 层可以起到一定的正则化效果，能提升模型收敛速度，防止模型过拟合。YOLOv2 通过使用 BN 层使得 mAP 提高了 2%。
High Resolution Classifier (高分辨率图像微调主干网络) 目前的大部分检测模型都会使用主流分类网络（如 vgg、resnet）在 ImageNet 上的预训练模型作为特征提取器，而这些分类网络大部分都是以小于 256x256 的图片作为输入进行训练的，低分辨率会影响模型检测能力。YOLOv2 将输入图片的分辨率提升至 448x448，为了使网络适应新的分辨率，YOLOv2 先在 ImageNet 上以 448x448 的分辨率对网络进行 10 个 epoch 的微调，让网络适应高分辨率的输入。通过使用高分辨率的输入，YOLOv2 的 mAP 提升了约 4%。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/20323b002018b2f00add7b27ffd7fa36/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-08-08T16:30:03+08:00" />
<meta property="article:modified_time" content="2021-08-08T16:30:03+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【目标检测】yolo系列-yolo v2(yolo9000)(2016年)</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E5%88%9B%E6%96%B0%E7%82%B9-toc" style="margin-left:0px;"><a href="#%E5%88%9B%E6%96%B0%E7%82%B9" rel="nofollow">YOLOv2创新点</a></p> 
<p id="Batch%20Normalization-toc" style="margin-left:80px;"><a href="#Batch%20Normalization" rel="nofollow">☆Batch Normalization</a></p> 
<p id="High%20Resolution%20Classifier%C2%A0%EF%BC%88%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%EF%BC%89-toc" style="margin-left:80px;"><a href="#High%20Resolution%20Classifier%C2%A0%EF%BC%88%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%EF%BC%89" rel="nofollow">High Resolution Classifier (高分辨率图像微调主干网络)</a></p> 
<p id="Convolutional%20With%20Anchor%20Boxes%C2%A0%E4%BD%BF%E7%94%A8%20anchor%20box%20%E8%BF%9B%E8%A1%8C%E5%8D%B7%E7%A7%AF-toc" style="margin-left:80px;"><a href="#Convolutional%20With%20Anchor%20Boxes%C2%A0%E4%BD%BF%E7%94%A8%20anchor%20box%20%E8%BF%9B%E8%A1%8C%E5%8D%B7%E7%A7%AF" rel="nofollow">☆使用 anchor box 先验框</a></p> 
<p id="New%20Network%EF%BC%9ADarknet-19-toc" style="margin-left:80px;"><a href="#New%20Network%EF%BC%9ADarknet-19" rel="nofollow">☆New Network：Darknet-19</a></p> 
<p id="Dimension%20Clusters%C2%A0%C2%A0%E7%BB%B4%E5%BA%A6%E9%9B%86%E7%BE%A4-toc" style="margin-left:80px;"><a href="#Dimension%20Clusters%C2%A0%C2%A0%E7%BB%B4%E5%BA%A6%E9%9B%86%E7%BE%A4" rel="nofollow">Dimension Clusters（聚类提取先验框的尺度信息）</a></p> 
<p id="Direct%20location%20prediction-toc" style="margin-left:80px;"><a href="#Direct%20location%20prediction" rel="nofollow">Direct location prediction（约束预测边框的位置）</a></p> 
<p id="Fine-Grained%20Features%C2%A0%EF%BC%88%E7%BB%86%E7%B2%92%E5%BA%A6%E7%89%B9%E6%80%A7%EF%BC%89-toc" style="margin-left:80px;"><a href="#Fine-Grained%20Features%C2%A0%EF%BC%88%E7%BB%86%E7%B2%92%E5%BA%A6%E7%89%B9%E6%80%A7%EF%BC%89" rel="nofollow">Fine-Grained Features （细粒度特性）</a></p> 
<p id="Multi-Scale%20Training-toc" style="margin-left:80px;"><a href="#Multi-Scale%20Training" rel="nofollow">Multi-Scale Training（多尺度训练）</a></p> 
<p id="YOLOv2%20%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98-toc" style="margin-left:80px;"><a href="#YOLOv2%20%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98" rel="nofollow">YOLOv2 存在的问题</a></p> 
<hr id="hr-toc"> 
<h2 id="%E5%88%9B%E6%96%B0%E7%82%B9"><strong>YOLOv2创新点</strong></h2> 
<p>YOLOv1 虽然检测速度快，但在<strong>定位方面不够准确，并且召回率较低</strong>。为了提升定位准确度，改善召回率，YOLOv2 在 YOLOv1 的基础上提出了几种改进策略</p> 
<p><img alt="" class="has" height="235" src="https://images2.imgbox.com/33/53/O4v27Ow8_o.png" width="491"></p> 
<p>YOLOv2-1</p> 
<ul><li> <h4 id="Batch%20Normalization"><strong>☆Batch Normalization</strong></h4> </li></ul> 
<p>        YOLOv2 中在每个卷积层后加 Batch Normalization (BN) 层，去掉 dropout。BN 层可以起到一定的正则化效果，能<strong>提升模型收敛速度，防止模型过拟合</strong>。<em>YOLOv2 通过使用 BN 层使得 mAP 提高了 2%</em>。</p> 
<ul><li> <h4 id="High%20Resolution%20Classifier%C2%A0%EF%BC%88%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%EF%BC%89"><strong>High Resolution Classifier</strong> (高分辨率图像微调主干网络)</h4> </li></ul> 
<p>        目前的大部分检测模型都会使用主流分类网络（如 vgg、resnet）在 ImageNet 上的预训练模型作为特征提取器，而这些分类网络大部分都是以小于 256x256 的图片作为输入进行训练的，低分辨率会影响模型检测能力。YOLOv2 将输入图片的分辨率提升至 <strong>448x448</strong>，为了使网络适应新的分辨率，YOLOv2 先在 ImageNet 上以 448x448 的分辨率对网络进行 10 个 epoch 的微调，让网络适应高分辨率的输入。通过使用高分辨率的输入，YOLOv2 的 mAP 提升了约 4%。</p> 
<ul><li> <h4 id="Convolutional%20With%20Anchor%20Boxes%C2%A0%E4%BD%BF%E7%94%A8%20anchor%20box%20%E8%BF%9B%E8%A1%8C%E5%8D%B7%E7%A7%AF"><strong>☆</strong>使用 anchor box 先验框</h4> </li></ul> 
<p>        YOLOV1包含有全连接层，从而能直接预测Bounding Boxes的坐标值。 Faster R-CNN的方法只用卷积层与Region Proposal Network来预测Anchor Box的偏移值与置信度，而不是直接预测坐标值。作者发现通过预测偏移量而不是坐标值能够简化问题，让神经网络学习起来更容易。</p> 
<p>        YOLOv2 去掉了 YOLOv1 中的全连接层，<strong>使用</strong><strong>Anchor Boxes 预测边界框</strong>，同时为了得到更高分辨率的特征图，YOLOv2 还去掉了一个池化层。YOLOv2通过缩减网络，使用416x416的输入，模型下采样的总步长为 32，<strong>最后得到 13x13 的特征图</strong>，然后对 13x13 的特征图的<strong>每个 cell 预测 5 个 anchor boxes</strong>，对每个 anchor box 预测边界框的位置信息、置信度和一套分类概率值。使用 anchor boxes 之后，<strong>YOLOv2 可以预测 13x13x5=845 个边界框</strong>，模型的召回率由原来的 81% 提升到 88%，mAP 由原来的 69.5% 降低到 69.2%；召回率提升了 7%，准确率下降了 0.3%。</p> 
<ul><li> <h4 id="New%20Network%EF%BC%9ADarknet-19"><strong>☆New Network：Darknet-19</strong></h4> </li></ul> 
<p>        YOLOv2 采用 Darknet-19，其网络结构如下图所示，包括19个卷积层和5个max pooling 层，主要采用3x3卷积和 1x1 卷积，<strong>这里 1x1 卷积可以压缩特征图通道数以降低模型计算量和参数</strong>，每个卷积层后使用<strong>BN层以加快模型收敛同时防止过拟合</strong>。最终采用global avg pool做预测。</p> 
<p>        Darknet-19只需要55.8亿次操作就可以处理一张图像，但在ImageNet上却可以达到72.9%的top-1精度和91.2%的top-5精度。优点是计算量小精度高。YOLOv2采用Darknet-19作为基网络，减少了模型的计算量。</p> 
<p><img alt="" class="has" height="424" src="https://images2.imgbox.com/1d/3f/XFV5KU94_o.png" width="321"></p> 
<ul><li> <h4 id="Dimension%20Clusters%C2%A0%C2%A0%E7%BB%B4%E5%BA%A6%E9%9B%86%E7%BE%A4"><strong>Dimension Clusters（聚类提取先验框的尺度信息）</strong></h4> </li></ul> 
<p>        在 Faster R-CNN 和 SSD 中，先验框都是手动设定的，带有一定的主观性。YOLOv2 <strong>采用 k-means 聚类算法对训练集中的边界框做了聚类分析</strong>，寻找尽可能匹配样本的边框尺寸。选用 boxes 之间的 IOU 值作为聚类指标。综合考虑模型复杂度和召回率，最终选择 5 个聚类中心，得到 5 个先验框，发现其中扁长的框较少，而瘦高的框更多，更符合行人特征。通过对比实验，发现用聚类分析得到的先验框比手动选择的先验框有更高的平均 IOU 值，这使得模型更容易训练学习。</p> 
<p><img alt="" class="has" height="270" src="https://images2.imgbox.com/d6/cb/l93M8BiD_o.png" width="455"></p> 
<ul><li> <h4 id="Direct%20location%20prediction"><strong>Direct location prediction</strong>（约束预测边框的位置）</h4> </li></ul> 
<p>        YOLOv2 对边界框的位置预测进行约束，<strong>将预测边框的中心约束在特定gird网格内</strong>，使模型更容易稳定训练，这种方式使得模型的mAP值提升了约 5%。       </p> 
<p>         <img alt="" height="484" src="https://images2.imgbox.com/b5/93/j8sdAsM5_o.png" width="633"></p> 
<p><img alt="" height="300" src="https://images2.imgbox.com/64/17/9ymg9hEk_o.png" width="402"></p> 
<p></p> 
<ul><li> <h4 id="Fine-Grained%20Features%C2%A0%EF%BC%88%E7%BB%86%E7%B2%92%E5%BA%A6%E7%89%B9%E6%80%A7%EF%BC%89"><strong>Fine-Grained Features</strong> （细粒度特性）</h4> </li></ul> 
<p>        目标检测面临的一个问题是图像中对象会有大有小，输入图像经过多层网络提取特征，最后输出的特征图中（比如YOLO2中输入416*416经过卷积网络下采样最后输出是13*13），较小的对象可能特征已经不明显甚至被忽略掉了。为了更好的检测出一些比较小的对象，最后输出的特征图需要保留一些更细节的信息。</p> 
<p>        YOLO2引入一种称为passthrough层的方法在特征图中保留一些细节信息。具体来说，就是在最后一个pooling之前，特征图的大小是26*26*512，<strong>将特征图尺寸1拆4(26*26拆分成4个13*13)，直接传递(passthrough)到pooling+conv后的特征图，然后两者叠加到一起作为输出的特征图</strong>。使用 Fine-Grained Features，YOLOv2 的性能提升了 1%.</p> 
<p><img alt="" height="300" src="https://images2.imgbox.com/21/fb/qrFqPugv_o.png" width="500"></p> 
<ul><li> <h4 id="Multi-Scale%20Training"><strong>Multi-Scale Training</strong><strong>（多尺度训练）</strong></h4> </li></ul> 
<p>        YOLOv2 使用的 Darknet-19 网络结构中只有卷积层和池化层，所以其对输入图片的大小没有限制。YOLOv2 采用多尺度输入的方式训练，在训练过程中每隔 10 个 batches , 重新随机选择输入图片的尺寸，由于Darknet-19下采样总步长为32，输入图片的尺寸一般选择32的倍数 {320,352,…,608}。</p> 
<p>        采用 Multi-Scale Training, 可以适应不同大小的图片输入。当采用低分辨率的图片输入时，mAP值略有下降但速度更快；当采用高分辨率的图片输入时，能得到较高mAP值，但速度有所下降。</p> 
<p><img alt="" height="230" src="https://images2.imgbox.com/ff/87/GHZAG7RO_o.png" width="328"></p> 
<p>YOLOv2 借鉴了很多其它目标检测方法的一些技巧，如 <strong>Faster R-CNN 的 anchor boxes</strong>, <strong>SSD 中的多尺度检测</strong>。除此之外，YOLOv2 在网络设计上做了很多 tricks, 使它能在保证速度的同时提高检测准确率，<strong>Multi-Scale Training </strong>更使得同一个模型适应不同大小的输入，从而可以在速度和精度上进行自由权衡 。</p> 
<h4 id="YOLOv2%20%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98"><strong>YOLOv2 存在的问题</strong></h4> 
<p>YOLO v2 对 YOLO v1 的缺陷进行优化，大幅度高了检测的性能，但仍存在一定的问题，<strong>如无法解决重叠问题的分类等</strong>。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a571eb4e4f5a60dce029fcf460e69db2/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">One solution to error: “Failed to compile generated C file xsim.dir/ ... /xsim_1.c“</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/68db674a1b52c92d08e66af1fa244ff7/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">如何运行一个Java文件？</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>