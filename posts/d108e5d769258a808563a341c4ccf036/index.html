<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>学习笔记①：使用python进行文本分类 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="学习笔记①：使用python进行文本分类" />
<meta property="og:description" content="-- coding:utf-8 -- author = ‘zengyun’ import numpy as np
def 回调函数 词表到向量的转换函数 def load_data_set(): # 创建试验样本。 posting_list = [[‘my’, ‘dog’, ‘has’, ‘flea’, ‘problems’, ‘help’, ‘please’], [‘maybe’, ‘not’, ‘take’, ‘him’, ‘to’, ‘dog’, ‘park’, ‘stupid’], [‘my’, ‘dalmation’, ‘is’, ‘so’, ‘cute’, ‘i’, ‘love’, ‘him’], [‘stop’, ‘posting’, ‘stupid’, ‘worthless’, ‘garbage’], [‘mr’, ‘licks’, ‘ate’, ‘my’, ‘steak’, ‘how’, ‘to’, ‘stop’, ‘him’], [‘quit’, ‘buying’, ‘worthless’, ‘dog’, ‘food’, ‘stupid’]] class_vec = [0, 1, 0, 1, 0, 1] # 1 代表侮辱性文字，0 代表正常言论。 # class_vec 返回第二个变量，是一个类别标签的集合。有两类 1 和 0 。 # 这些文本的类别由人工标注，这些标注信息用于训练程序以便自动检测侮辱性留言。 return posting_list, class_vec" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/d108e5d769258a808563a341c4ccf036/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2017-03-20T20:30:42+08:00" />
<meta property="article:modified_time" content="2017-03-20T20:30:42+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">学习笔记①：使用python进行文本分类</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2 id="codingutf-8">-<em>- coding:utf-8 -</em>-</h2> 
<h2 id="author-zengyun"><strong>author</strong> = ‘zengyun’</h2> 
<p>import numpy as np</p> 
<h2 id="def-回调函数">def 回调函数</h2> 
<h2 id="词表到向量的转换函数">词表到向量的转换函数</h2> 
<p>def load_data_set(): # 创建试验样本。 <br> posting_list = [[‘my’, ‘dog’, ‘has’, ‘flea’, ‘problems’, ‘help’, ‘please’], <br> [‘maybe’, ‘not’, ‘take’, ‘him’, ‘to’, ‘dog’, ‘park’, ‘stupid’], <br> [‘my’, ‘dalmation’, ‘is’, ‘so’, ‘cute’, ‘i’, ‘love’, ‘him’], <br> [‘stop’, ‘posting’, ‘stupid’, ‘worthless’, ‘garbage’], <br> [‘mr’, ‘licks’, ‘ate’, ‘my’, ‘steak’, ‘how’, ‘to’, ‘stop’, ‘him’], <br> [‘quit’, ‘buying’, ‘worthless’, ‘dog’, ‘food’, ‘stupid’]] <br> class_vec = [0, 1, 0, 1, 0, 1] # 1 代表侮辱性文字，0 代表正常言论。 <br> # class_vec 返回第二个变量，是一个类别标签的集合。有两类 1 和 0 。 <br> # 这些文本的类别由人工标注，这些标注信息用于训练程序以便自动检测侮辱性留言。 <br> return posting_list, class_vec</p> 
<h2 id="将词条列表传递给-set-构造函数set-就会返回一个不重复的列表">将词条列表传递给 set 构造函数，set 就会返回一个不重复的列表。</h2> 
<p>def create_vocab_list(data_set): # 创建一个包含在所有文档中出现的不同重复词的列表，为此使用set集合。 <br> vocab_set = set([]) # 创建一个空集合。 <br> for document in data_set: <br> vocab_set = vocab_set | set(document) # 创建两个集合的并集。 <br> return list(vocab_set)</p> 
<h2 id="函数的输入参数是词汇表-vocablist-和某个文档-inputset输出的是文档向量向量的每个元素为-1-或者-0">函数的输入参数是词汇表 vocab_list 和某个文档 input_set，输出的是文档向量，向量的每个元素为 1 或者 0，</h2> 
<h2 id="分别表示词汇表中的单词在输入文档中是否出现">分别表示词汇表中的单词在输入文档中是否出现。</h2> 
<h2 id="词集模型setofword将每个词的出现与否作为1个特征">词集模型（set_of_word）:将每个词的出现与否作为1个特征。</h2> 
<p>def set_of_words_2vec(vocab_list, input_set): <br> return_vec = [0] * len(vocab_list) # 创建一个其中所包含元素都为0的向量。 <br> for word in input_set: <br> if word in vocab_list: <br> # 　遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为 1 . <br> return_vec[vocab_list.index(word)] = 1 <br> else: <br> print(“the word: %s is not in my vocabulary!” % word) <br> return return_vec</p> 
<h2 id="词袋模型bagofword如果每个词在文档中出现不止一次意味着包含该词是否出现在文档中所不能表达的某种信息">词袋模型（bag_of_word）:如果每个词在文档中出现不止一次，意味着包含该词是否出现在文档中所不能表达的某种信息。</h2> 
<p>def bag_of_words_2vec(vocab_list, input_bag): <br> return_vec = [0] * len(vocab_list) <br> for word in input_bag: <br> if word in vocab_list: <br> return_vec[vocab_list.index(word)] += 1 <br> # 　遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值加 1 . <br> return return_vec</p> 
<h2 id="朴素贝叶斯分类器训练函数">朴素贝叶斯分类器训练函数</h2> 
<p>def train_nb0(train_matrix, train_category): <br> # 输入参数为 文档矩阵 train_matrix， 以及由每篇文档类别标签所构成的向量 train_category。 <br> num_train_docs = len(train_matrix) # 首先计算 侮文档矩阵 train_matrix 的总词数。 <br> # 对多于两类的分类问题，则需要对代码进行修改。 <br> num_words = len(train_matrix[0]) <br> pa_busive = sum(train_category) / float(num_train_docs) <br> # 初始化概率（初始化程序中的分子变量和分母变量）。 <br> # 1.如果其中一个概率值为 0，那么最后的乘积也为 0，为了降低这种影响，可以将所有词的出现次数初始化为 1，并将分母初始化为 2. <br> p0_num = np.ones(num_words) <br> p1_num = np.ones(num_words) <br> p0_denom = 2.0 <br> p1_denom = 2.0 <br> for i in range(num_train_docs): <br> # 分母变量是一个元素个数等于词汇表大小的 numpy 数组 <br> if train_category[i] == 1: <br> p1_num += train_matrix[i] <br> p1_denom += sum(train_matrix[i]) <br> else: <br> p0_num += train_matrix[i] <br> p0_denom += sum(train_matrix[i]) <br> # 在 for 循环中 ，要遍历训练集 train_matrix 中的所有文档。一旦某个词语（侮辱性或正常词语）在某一文档中出现， <br> # 则该词对应的个数（p1_num）和（p0_num）就加 1 ，而且在所有的文档中，该文档的总词数也相应加 1 。对于两个类别都要进行同样的计算处理。 <br> # 2.下溢出 由于太多很小的数相乘造成的。代数中，有 ln(a+b)=ln(a)+ln(b)，通过求对数可以避免下溢出或者浮点数舍入导致的错误。 <br> # 同时，采用自然对数进行处理不会有任何损失。 <br> p1_vect = np.log(p1_num / p1_denom) <br> p0_vect = np.log(p0_num / p0_denom) <br> # 对每个元素除以该类别中的总词数。 <br> return p0_vect, p1_vect, pa_busive</p> 
<h2 id="朴素贝叶斯分类函数">朴素贝叶斯分类函数</h2> 
<h2 id="4个输入参数要分类的向量-vec2classify-使用函数-trainnb0-计算得到的3个概率">4个输入参数：要分类的向量 vec_2_classify, 使用函数 train_nb0() 计算得到的3个概率。</h2> 
<p>def classify_nb(vec_2_classify, p0_vec, p1_vec, p_class): <br> p1 = sum(vec_2_classify * p1_vec) + np.log(p_class) <br> p0 = sum(vec_2_classify * p0_vec) + np.log(1.0 - p_class) <br> if p1 &gt; p0: <br> return 1 <br> else: <br> return 0</p> 
<p>def testing_nb(): # 便利函数 <br> list_posts, list_classes = load_data_set() <br> my_vocab_list = create_vocab_list(list_posts) <br> train_mat = [] <br> for post_in_doc in list_posts: <br> train_mat.append(set_of_words_2vec(my_vocab_list, post_in_doc)) <br> p0_v, p1_v, p_ab = train_nb0(np.array(train_mat), np.array(list_classes)) <br> test_entry = [‘love’, ‘my’, ‘dalmation’] <br> this_doc = np.array(set_of_words_2vec(my_vocab_list, test_entry)) <br> print(test_entry, “classified as:”, classify_nb(this_doc, p0_v, p1_v, p_ab)) <br> test_entry = [‘stupid’, ‘garbage’] <br> this_doc = np.array(set_of_words_2vec(my_vocab_list, test_entry)) <br> print(test_entry, “classified as:”, classify_nb(this_doc, p0_v, p1_v, p_ab))</p> 
<h2 id="使用朴素贝叶斯进行交叉验证">使用朴素贝叶斯进行交叉验证</h2> 
<h2 id="接受一个大字符串并将其解析为字符串列表该函数去掉少于两个字符的字符串并将所有字符串转换为小写">接受一个大字符串并将其解析为字符串列表。该函数去掉少于两个字符的字符串，并将所有字符串转换为小写。</h2> 
<p>def text_parse(big_string): <br> import re <br> list_of_tokens = re.split(r’\W*’, big_string) <br> return [tok.lower() for tok in list_of_tokens if len(tok) &gt; 2]</p> 
<h2 id="对贝叶斯垃圾邮件分类器进行自动化处理">对贝叶斯垃圾邮件分类器进行自动化处理</h2> 
<p>def spam_test(): <br> doc_list = [] <br> class_list = [] <br> full_text = [] <br> for i in range(1, 26): <br> # 导入文件夹spam与ham下的文本文件，并将他们解析为词列表 <br> file_name = ‘email/spam/’ + str(i) + ‘.txt’ <br> with open(file_name, ‘rb’) as fd: # r 表示只读, b 表示二进制 <br> word_list = text_parse(fd.read().decode(‘utf-8’, ‘ignore’)) <br> # 使用 decode() 和 encode() 来进行解码和编码，现在用 utf-8 解码。 <br> doc_list.append(word_list) <br> full_text.extend(word_list) <br> class_list.append(1)</p> 
<pre><code>    file_name = 'email/ham/' + str(i) + '.txt'
    with open(file_name, 'rb') as fd:
        word_list = text_parse(fd.read().decode('utf-8', 'ignore'))
    doc_list.append(word_list)
    full_text.extend(word_list)
    class_list.append(0)
vocab_list = create_vocab_list(doc_list)
# 接下来构建一个测试集与一个训练集，两个集合中的邮件都是随机选出来的。本例中共有50封电子邮件，其中10封电子邮件被选择为测试集。
# training_set 是一个整数列表，其中的值是 0 到 49。
training_set = list(range(50))
test_set = []
# 接下来随机选择其中的10个文件。
for i in range(10):
    rand_index = int(np.random.uniform(0, len(training_set)))
    test_set.append(training_set[rand_index])
    del (training_set[rand_index])
# 选择出的数字所对应的文档被添加到测试集，同时也将其从训练集中剔除。
# 这种随机选择数据的一部分作为训练集，而剩余部分作为测试集的过程称为留存交叉验证。
# 假定现在只完成了一次迭代，那么为了更精确地估计分类器的错误率，就应该进行多次迭代后求出平均错误率。
train_mat = []
train_classes = []
for doc_index in training_set:
    train_mat.append(set_of_words_2vec(vocab_list, doc_list[doc_index]))
    train_classes.append(class_list[doc_index])
p0_v, p1_v, ps_pam = train_nb0(np.array(train_mat), np.array(train_classes))
error_count = 0
# 接下来的for循环遍历训练集的所有文档，对每封邮件基于词汇表使用 set_of_words_2vec() 函数来构建词向量。
# 这些词在 train_nb0() 函数中用于计算分类所需的概率。然后遍历测试集，对其中每封电子邮件进行分类。
# 如果邮件分类错误，则错误数加 1，最后给出总的错误百分比。
for doc_index in test_set:
    word_vector = set_of_words_2vec(vocab_list, doc_list[doc_index])
    if classify_nb(np.array(word_vector), p0_v, p1_v, ps_pam) != class_list[doc_index]:
        error_count += 1
print("the error rate is:", float(error_count) / len(test_set))
</code></pre> 
<pre class="prettyprint"><code class=" hljs http">

<span class="vala">
<span class="hljs-preprocessor">###新建一个Python文件</span>
</span></code></pre> 
<p>import bayes</p> 
<p>list_posts, list_classes = bayes.load_data_set() <br> my_vocab_list = bayes.create_vocab_list(list_postes) <br> print(my_vocab_list)</p> 
<p>print(bayes.spam_test()) <br> print(bayes.spam_test())</p> 
<p>“`</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8a4cda3f228f9b350a0a53639b9e1a06/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">EL表达式和JSTL表达式</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/157258ef94e5fa513e8957c8c30fb3eb/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">数据库并发操作会带来哪些问题及原因</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>