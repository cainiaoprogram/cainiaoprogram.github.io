<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>基于改进YOLOv5的红外图像行人目标检测方法 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="基于改进YOLOv5的红外图像行人目标检测方法" />
<meta property="og:description" content="背景技术
行人检测要求准确判断输入图像或视频是否包含行人，并给出行人在图像中的空间坐标信息，广泛应用于智能监控、地区侦查、人体行为理解、自动驾驶等领域。与传统的基于可见光的行人检测任务相比，红外成像抗干扰能力强、受光线和恶劣天气影响小，具备全天时工作的能力。但是红外图像对比度较低、纹理特征弱、干扰大，行人目标在强噪声和相似背景的影响下，成为弱小目标；传统的基于人工提取特征检测的方式利用选取的区域人工设计提取特征，对于提取的特征进行学习并用分类器进行分类，此类方法需通过滑动窗口实现，时间复杂度较高，且鲁棒性不强，很难保障红外行人检测的精度和实时性。深度学习的出现，使得红外行人目标检测有了突破性进展，特别是基于回归的单阶段目标检测YOLO模型，增强了红外行人目标检测的鲁棒性并提高了检测准确率和检测速度。
河南大学在其拥有的发明专利技术申请“基于改进YOLOv3的红外图像弱小目标检测方法”(专利申请号：2020109263845，公开号：CN112101434A)中公开了一种基于改进YOLOv3的红外图像弱小目标检测方法，用于红外行人目标检测。该发明在网络的每个残差块中引入了通道和空间注意机制，通过叠加残差块中产生的注意感知特征，将图像中的关键信息提取出来；用边界框的重叠程度和两个边界框的中心距离即CIoU(CompleteIntersection over Union)作为边界框的损失函数，使预测框与真实框之间更加地切合；用GIoU(Generalized Intersection over Union)和两边界框中心点距离作为非极大值抑制NMS(Non-Maximum Suppression)算法中筛选候选框的阈值并使用高斯模型抑制周围边界框；还通过深度可分离卷积代替标准的卷积运算，并采用Hard-swish作为更深层次网络的激活函数。这种方法在红外行人小目标检测精度上取得了一定的效果，但是距离实际应用还有一定的差距。
史健婷、张贵强在论文“改进的YOLOv3红外图像行人检测算法”(黑龙江科技大学学报2020年30卷4期第442页到447页)中提出了一种基于YOLOv3的红外行人监控算法，可提高低像素及小目标检测环境中行人检测的准确率。该方法根据实际检测情况在YOLOv3的基础上进行优化，借鉴了DenseNet密集连接的改进方案，优化主干网络，加强了模块与模块间的特征传递，对网络检测部分二次优化浅层和深层融合后的特征信息，增强了特征信息的传递能力。这种方法利用网络结构的特征优化主干网络，通过二次优化浅层和深沉融合后的特征信息完善信息的丰富度，对灰度图及小目标的检测有一定的提升，但整体准确率改进有限。
发明内容
本发明的目的在于提供一种基于改进YOLOv5的红外图像行人目标检测方法，以解决背景技术中现有的红外图像检测方法存在的准确率不足等问题。
为了实现上述目的，本发明采用以下技术方案：
一种基于改进YOLOv5的红外图像行人目标检测方法，包括以下步骤：
步骤1：构建基于改进YOLOv5的红外图像行人目标检测深度学习模型，具体的：
步骤1.1：压缩输入通道，将YOLOv5目标检测模型输入端的3通道输入压缩为单通道输入；
步骤1.2：拓展YOLOv5目标检测模型骨干网络CSPDarknet中浅层跨阶段局部网络模块的厚度，提取红外图像行人目标在浅层的细节特征；
步骤1.3：引入注意力机制，所述的注意力机制用于骨干网络特征提取过程中加强红外图像行人目标特征的提取；
步骤1.4：在YOLOv5目标检测模型的PANet特征融合网络的上采样模块中，增加一个上采样层，所述的上采样层是在8倍、16倍和32倍上采样层的基础上，增加的4倍上采样层；
步骤1.5：在YOLOv5目标检测模型的PANet特征融合网络中，增加一个Concatenate融合层，将步骤1.4中所述的增加的4倍上采样层与骨干网络特征提取过程中得到的相同尺寸的特征图通过增加的Concatenate融合层进行特征融合，产生的4倍上采样的特征图；
步骤1.6：增加小目标检测层，将步骤1.5中所述的4倍上采样的特征图用于小目标的检测，基于改进YOLOv5的红外图像行人目标检测深度学习模型增加为4种尺度的预测层，用于Head部分的多尺度检测；
步骤1.7：根据步骤1.6增加的小目标检测层，增加一组小目标尺寸的锚点框，采用K-means自适应算法获取符合红外图像行人小目标尺度特点的锚点框；
步骤2：建立红外图像行人检测数据集，具体的：
步骤2.1：建立红外图像行人检测训练集、验证集和测试集，随机选取60％的红外图像行人检测数据作为训练集，20％的红外图像行人检测数据作为验证集，20％的红外图像行人检测数据作为测试集；
步骤2.2：对建立的红外图像行人检测训练集、验证集和测试集进行预处理，采用Mosaic算法实现红外行人数据的数据增强；
步骤3：训练步骤1中构建的基于改进YOLOv5的红外图像行人目标检测深度学习模型，具体的：
步骤3.1：设置训练参数，利用随机优化算法Adam进行训练，训练批次的大小设置为Batch＝64，动量Momentum＝0.9，学习率初始设定为Ir＝0.001，训练迭代次数Epoch＝300；
步骤3.2：将步骤2.2预处理后的红外图像行人检测训练集和验证集送入步骤1中构建的基于改进YOLOv5的红外图像行人目标检测深度学习模型；
步骤3.3：自适应缩放图片尺寸，根据网络设定的输入图片尺寸大小实现自适应红外图像行人检测训练集和验证集图片的缩放；
步骤3.4：训练基于改进YOLOv5的红外图像行人目标检测深度学习模型，根据训练集和验证集交叉验证的平均精度变化和损失变化趋势，调整学习率和迭代次数，直至精度变化和损失变化逐步趋于稳定的状态，确定最终的学习率和迭代次数；
步骤3.5：根据步骤3.4确定的学习率和迭代次数，完成基于改进YOLOv5的红外图像行人目标检测深度学习模型的训练，得到收敛良好的基于改进YOLOv5的红外图像行人目标检测深度学习模型；
步骤4：将步骤2.2预处理后的红外图像行人检测测试集送入步骤3.5中训练好的基于改进YOLOv5的红外图像行人目标检测深度学习模型，测试基于改进YOLOv5的红外图像行人目标检测深度学习模型的性能；
步骤5：评估模型，具体的：
根据步骤4的测试结果，从平均检测精度和检测速度上对步骤3训练的基于改进YOLOv5的红外图像行人目标检测深度学习模型进行评估；
步骤6：判断基于改进YOLOv5的红外图像行人目标检测深度学习模型评估结果的检测精度和速度是否满足实际应用需求，若基于改进YOLOv5的红外图像行人目标检测深度学习模型满足实际应用需求，则执行步骤8，否则，执行步骤7；
步骤7：修正步骤1构建的基于改进YOLOv5的红外图像行人目标检测深度学习模型的宽度和深度，并跳转至步骤3.4重新训练；
步骤8：将步骤6中满足实际应用需求的基于改进YOLOv5的红外图像行人目标检测深度学习模型用于红外图像或红外视频中行人目标的检测。
步骤1.3中所述的注意力机制选用改进的SK注意力机制，分为Split、Fuse、Scale三部分，具体的，步骤1.3具体包括以下步骤：
步骤1.3.1：Split操作，具体的：
将输入的特征图利用3×3和5×5两个卷积核进行卷积，分别得到输出特征向量U1和U2，U1和U2相加之后得到特征向量U；
步骤1.3.2：根据步骤1.3.1得到的特征向量U，进行Fuse操作，具体的：
用全局平均池化Fgp将矩阵压缩到1×1×C，用一个通道描述符表示每个通道的信息，建立通道间的依赖关系，如公式(1)所示，再通过两个全连接层Ffc操作使得通道间的关系是灵活的并且非线性的，输出经过sigmoid激活函数得到权重值，如公式(2)所示：
Ffc(Fgp，ω)＝σ(B(Fgp，ω))； (2)
其中，C表示通道数channel，W和H分别为图像的宽和高，i和j分别为图像的第i行和第j列，ω为相应的权重，σ为sigmoid激活函数，B为批归一化操作；
步骤1.3.3：Scale操作：
将步骤1.3.1中得到的特征向量U1和U2及步骤1.3.2中得到的权重值进行加权操作，得到SK注意力机制最终输出的加权特征图Fscale(U，Ffc)，其中，U表示特征向量，Ffc表示步骤1.3.2中得到的权重值，Fscale(U，Ffc)加权计算如公式(3)所示：
Fscale(U，Ffc)＝V1&#43;V2＝U1·Ffc&#43;U2·Ffc； (3)
其中，V1和V2分别为特征向量U1和U2的加权后的特征向量。
步骤2中，所述的建立的红外图像行人检测数据集选用公共数据集KAIST，并将目标类别设置people和person两个行人类别。
步骤3.2中，所述的将步骤2.2预处理后的红外图像行人检测训练集和验证集送入步骤1中构建的基于改进YOLOv5的红外图像行人目标检测深度学习模型时，自动删除KAIST数据集中不包含行人目标的红外图像。
与现有技术相比，本发明的有益效果为：
第一，由于本发明改进了骨干网络结构CSPDarknet中浅层CSP模块的厚度，使得特征提取网络更加关注浅层特征的提取，能更加彻底地提取浅层细节特征和深层高级语义特征；
第二，由于本发明在CSP模块中引入改进的SK注意力机制模块，从通道维度对行人目标进行加权融合，迫使网络关注红外行人图像中不同尺度的目标，以更好的学习鲁棒性和可辨别性特征；
第三，针对红外摄像距离远、目标小的问题，本发明改进了网络检测头结构，增加多尺度目标检测层，采用4级空间金字塔池化(Spatial Pyramid Pooling)增大感受野，提高红外行人小目标检测精度；
第四，本发明无论从检测精度、检测速度还是网络参数、权重文件等与YOLO v3红外行人图像目标检测相比都有一个质的提高，平均精度提高了约30.6％，mAP50达到了98.3％，mAP50：75也达到了95.6％；另外，检测时间减少约62％，网络参数减少约89％，权重文件减少94％以上，计算量减少84％以上。
附图说明
为了更清楚地说明本发明具体实施方式或现有技术中的技术方案，下面将对具体实施方式或现有技术描述中所需要使用的附图作简单地介绍，显而易见地，下面描述中的附图是本发明的一些实施方式，对于本领域普通技术人员来讲，在不付出创造性劳动的前提下，还可以根据这些附图获得其他的附图。
图1为本发明的方法流程图；
图2是本发明的检测网络结构示意图；
图3是本发明的检测结果的示例图。
具体实施方式
下面将结合附图对本发明的技术方案进行清楚、完整地描述，显然，所描述的实施例是本发明一部分实施例，而不是全部的实施例。基于本发明中的实施例，本领域普通技术人员在没有做出创造性劳动前提下所获得的所有其他实施例，都属于本发明保护的范围。
如图1所示：本发明所述的一种基于改进YOLOv5的红外图像行人目标检测方法，包括以下步骤：
步骤1：构建基于改进YOLOv5的红外图像行人目标检测深度学习模型；" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/1d4a2d4609d27e7e34b3e17ba6c5462c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-08-24T18:51:18+08:00" />
<meta property="article:modified_time" content="2022-08-24T18:51:18+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">基于改进YOLOv5的红外图像行人目标检测方法</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p></p> 
<p>背景技术</p> 
<p>行人检测要求准确判断输入图像或视频是否包含行人，并给出行人在图像中的空间坐标信息，广泛应用于智能监控、地区侦查、人体行为理解、自动驾驶等领域。与传统的基于可见光的行人检测任务相比，红外成像抗干扰能力强、受光线和恶劣天气影响小，具备全天时工作的能力。但是红外图像对比度较低、纹理特征弱、干扰大，行人目标在强噪声和相似背景的影响下，成为弱小目标；传统的基于人工提取特征检测的方式利用选取的区域人工设计提取特征，对于提取的特征进行学习并用分类器进行分类，此类方法需通过滑动窗口实现，时间复杂度较高，且鲁棒性不强，很难保障红外行人检测的精度和实时性。深度学习的出现，使得红外行人目标检测有了突破性进展，特别是基于回归的单阶段目标检测YOLO模型，增强了红外行人目标检测的鲁棒性并提高了检测准确率和检测速度。</p> 
<p>河南大学在其拥有的发明专利技术申请“基于改进YOLOv3的红外图像弱小目标检测方法”(专利申请号：2020109263845，公开号：CN112101434A)中公开了一种基于改进YOLOv3的红外图像弱小目标检测方法，用于红外行人目标检测。该发明在网络的每个残差块中引入了通道和空间注意机制，通过叠加残差块中产生的注意感知特征，将图像中的关键信息提取出来；用边界框的重叠程度和两个边界框的中心距离即CIoU(CompleteIntersection over Union)作为边界框的损失函数，使预测框与真实框之间更加地切合；用GIoU(Generalized Intersection over Union)和两边界框中心点距离作为非极大值抑制NMS(Non-Maximum Suppression)算法中筛选候选框的阈值并使用高斯模型抑制周围边界框；还通过深度可分离卷积代替标准的卷积运算，并采用Hard-swish作为更深层次网络的激活函数。这种方法在红外行人小目标检测精度上取得了一定的效果，但是距离实际应用还有一定的差距。</p> 
<p>史健婷、张贵强在论文“改进的YOLOv3红外图像行人检测算法”(黑龙江科技大学学报2020年30卷4期第442页到447页)中提出了一种基于YOLOv3的红外行人监控算法，可提高低像素及小目标检测环境中行人检测的准确率。该方法根据实际检测情况在YOLOv3的基础上进行优化，借鉴了DenseNet密集连接的改进方案，优化主干网络，加强了模块与模块间的特征传递，对网络检测部分二次优化浅层和深层融合后的特征信息，增强了特征信息的传递能力。这种方法利用网络结构的特征优化主干网络，通过二次优化浅层和深沉融合后的特征信息完善信息的丰富度，对灰度图及小目标的检测有一定的提升，但整体准确率改进有限。</p> 
<p>发明内容</p> 
<p>本发明的目的在于提供一种基于改进YOLOv5的红外图像行人目标检测方法，以解决背景技术中现有的红外图像检测方法存在的准确率不足等问题。</p> 
<p>为了实现上述目的，本发明采用以下技术方案：</p> 
<p>一种基于改进YOLOv5的红外图像行人目标检测方法，包括以下步骤：</p> 
<p>步骤1：构建基于改进YOLOv5的红外图像行人目标检测深度学习模型，具体的：</p> 
<p>步骤1.1：压缩输入通道，将YOLOv5目标检测模型输入端的3通道输入压缩为单通道输入；</p> 
<p>步骤1.2：拓展YOLOv5目标检测模型骨干网络CSPDarknet中浅层跨阶段局部网络模块的厚度，提取红外图像行人目标在浅层的细节特征；</p> 
<p>步骤1.3：引入注意力机制，所述的注意力机制用于骨干网络特征提取过程中加强红外图像行人目标特征的提取；</p> 
<p>步骤1.4：在YOLOv5目标检测模型的PANet特征融合网络的上采样模块中，增加一个上采样层，所述的上采样层是在8倍、16倍和32倍上采样层的基础上，增加的4倍上采样层；</p> 
<p>步骤1.5：在YOLOv5目标检测模型的PANet特征融合网络中，增加一个Concatenate融合层，将步骤1.4中所述的增加的4倍上采样层与骨干网络特征提取过程中得到的相同尺寸的特征图通过增加的Concatenate融合层进行特征融合，产生的4倍上采样的特征图；</p> 
<p>步骤1.6：增加小目标检测层，将步骤1.5中所述的4倍上采样的特征图用于小目标的检测，基于改进YOLOv5的红外图像行人目标检测深度学习模型增加为4种尺度的预测层，用于Head部分的多尺度检测；</p> 
<p>步骤1.7：根据步骤1.6增加的小目标检测层，增加一组小目标尺寸的锚点框，采用K-means自适应算法获取符合红外图像行人小目标尺度特点的锚点框；</p> 
<p>步骤2：建立红外图像行人检测数据集，具体的：</p> 
<p>步骤2.1：建立红外图像行人检测训练集、验证集和测试集，随机选取60％的红外图像行人检测数据作为训练集，20％的红外图像行人检测数据作为验证集，20％的红外图像行人检测数据作为测试集；</p> 
<p>步骤2.2：对建立的红外图像行人检测训练集、验证集和测试集进行预处理，采用Mosaic算法实现红外行人数据的数据增强；</p> 
<p>步骤3：训练步骤1中构建的基于改进YOLOv5的红外图像行人目标检测深度学习模型，具体的：</p> 
<p>步骤3.1：设置训练参数，利用随机优化算法Adam进行训练，训练批次的大小设置为Batch＝64，动量Momentum＝0.9，学习率初始设定为Ir＝0.001，训练迭代次数Epoch＝300；</p> 
<p>步骤3.2：将步骤2.2预处理后的红外图像行人检测训练集和验证集送入步骤1中构建的基于改进YOLOv5的红外图像行人目标检测深度学习模型；</p> 
<p>步骤3.3：自适应缩放图片尺寸，根据网络设定的输入图片尺寸大小实现自适应红外图像行人检测训练集和验证集图片的缩放；</p> 
<p>步骤3.4：训练基于改进YOLOv5的红外图像行人目标检测深度学习模型，根据训练集和验证集交叉验证的平均精度变化和损失变化趋势，调整学习率和迭代次数，直至精度变化和损失变化逐步趋于稳定的状态，确定最终的学习率和迭代次数；</p> 
<p>步骤3.5：根据步骤3.4确定的学习率和迭代次数，完成基于改进YOLOv5的红外图像行人目标检测深度学习模型的训练，得到收敛良好的基于改进YOLOv5的红外图像行人目标检测深度学习模型；</p> 
<p>步骤4：将步骤2.2预处理后的红外图像行人检测测试集送入步骤3.5中训练好的基于改进YOLOv5的红外图像行人目标检测深度学习模型，测试基于改进YOLOv5的红外图像行人目标检测深度学习模型的性能；</p> 
<p>步骤5：评估模型，具体的：</p> 
<p>根据步骤4的测试结果，从平均检测精度和检测速度上对步骤3训练的基于改进YOLOv5的红外图像行人目标检测深度学习模型进行评估；</p> 
<p>步骤6：判断基于改进YOLOv5的红外图像行人目标检测深度学习模型评估结果的检测精度和速度是否满足实际应用需求，若基于改进YOLOv5的红外图像行人目标检测深度学习模型满足实际应用需求，则执行步骤8，否则，执行步骤7；</p> 
<p>步骤7：修正步骤1构建的基于改进YOLOv5的红外图像行人目标检测深度学习模型的宽度和深度，并跳转至步骤3.4重新训练；</p> 
<p>步骤8：将步骤6中满足实际应用需求的基于改进YOLOv5的红外图像行人目标检测深度学习模型用于红外图像或红外视频中行人目标的检测。</p> 
<p>步骤1.3中所述的注意力机制选用改进的SK注意力机制，分为Split、Fuse、Scale三部分，具体的，步骤1.3具体包括以下步骤：</p> 
<p>步骤1.3.1：Split操作，具体的：</p> 
<p>将输入的特征图利用3×3和5×5两个卷积核进行卷积，分别得到输出特征向量U1和U2，U1和U2相加之后得到特征向量U；</p> 
<p>步骤1.3.2：根据步骤1.3.1得到的特征向量U，进行Fuse操作，具体的：</p> 
<p>用全局平均池化Fgp将矩阵压缩到1×1×C，用一个通道描述符表示每个通道的信息，建立通道间的依赖关系，如公式(1)所示，再通过两个全连接层Ffc操作使得通道间的关系是灵活的并且非线性的，输出经过sigmoid激活函数得到权重值，如公式(2)所示：</p> 
<p>Ffc(Fgp，ω)＝σ(B(Fgp，ω))； (2)</p> 
<p>其中，C表示通道数channel，W和H分别为图像的宽和高，i和j分别为图像的第i行和第j列，ω为相应的权重，σ为sigmoid激活函数，B为批归一化操作；</p> 
<p>步骤1.3.3：Scale操作：</p> 
<p>将步骤1.3.1中得到的特征向量U1和U2及步骤1.3.2中得到的权重值进行加权操作，得到SK注意力机制最终输出的加权特征图Fscale(U，Ffc)，其中，U表示特征向量，Ffc表示步骤1.3.2中得到的权重值，Fscale(U，Ffc)加权计算如公式(3)所示：</p> 
<p>Fscale(U，Ffc)＝V1+V2＝U1·Ffc+U2·Ffc； (3)</p> 
<p>其中，V1和V2分别为特征向量U1和U2的加权后的特征向量。</p> 
<p>步骤2中，所述的建立的红外图像行人检测数据集选用公共数据集KAIST，并将目标类别设置people和person两个行人类别。</p> 
<p>步骤3.2中，所述的将步骤2.2预处理后的红外图像行人检测训练集和验证集送入步骤1中构建的基于改进YOLOv5的红外图像行人目标检测深度学习模型时，自动删除KAIST数据集中不包含行人目标的红外图像。</p> 
<p>与现有技术相比，本发明的有益效果为：</p> 
<p>第一，由于本发明改进了骨干网络结构CSPDarknet中浅层CSP模块的厚度，使得特征提取网络更加关注浅层特征的提取，能更加彻底地提取浅层细节特征和深层高级语义特征；</p> 
<p>第二，由于本发明在CSP模块中引入改进的SK注意力机制模块，从通道维度对行人目标进行加权融合，迫使网络关注红外行人图像中不同尺度的目标，以更好的学习鲁棒性和可辨别性特征；</p> 
<p>第三，针对红外摄像距离远、目标小的问题，本发明改进了网络检测头结构，增加多尺度目标检测层，采用4级空间金字塔池化(Spatial Pyramid Pooling)增大感受野，提高红外行人小目标检测精度；</p> 
<p>第四，本发明无论从检测精度、检测速度还是网络参数、权重文件等与YOLO v3红外行人图像目标检测相比都有一个质的提高，平均精度提高了约30.6％，mAP50达到了98.3％，mAP50：75也达到了95.6％；另外，检测时间减少约62％，网络参数减少约89％，权重文件减少94％以上，计算量减少84％以上。</p> 
<p>附图说明</p> 
<p>为了更清楚地说明本发明具体实施方式或现有技术中的技术方案，下面将对具体实施方式或现有技术描述中所需要使用的附图作简单地介绍，显而易见地，下面描述中的附图是本发明的一些实施方式，对于本领域普通技术人员来讲，在不付出创造性劳动的前提下，还可以根据这些附图获得其他的附图。</p> 
<p>图1为本发明的方法流程图；</p> 
<p>图2是本发明的检测网络结构示意图；</p> 
<p>图3是本发明的检测结果的示例图。</p> 
<p>具体实施方式</p> 
<p>下面将结合附图对本发明的技术方案进行清楚、完整地描述，显然，所描述的实施例是本发明一部分实施例，而不是全部的实施例。基于本发明中的实施例，本领域普通技术人员在没有做出创造性劳动前提下所获得的所有其他实施例，都属于本发明保护的范围。</p> 
<p>如图1所示：本发明所述的一种基于改进YOLOv5的红外图像行人目标检测方法，包括以下步骤：</p> 
<p>步骤1：构建基于改进YOLOv5的红外图像行人目标检测深度学习模型；</p> 
<p>首先，YOLOv5目标检测模型使用CSPDarknet作为提取特征的骨干网络，CSPDarknet解决了其他大型卷积神经网络框架Backbone中网络优化的梯度信息重复问题，将梯度的变化从头到尾地集成到特征图中，减少了模型的参数量和FLOPS数值，既保证了推理速度和准确率，又减小了模型尺寸；其次，YOLOv5目标检测模型的网络结构非常简洁，在灵活性与速度上远强于YOLOv3，在模型的快速部署上具有极强优势；最后，针对红外图像行人目标距离远、能量弱、分辨率低、干扰大导致的行人目标检测识别率低、虚警率高等问题，选择基于YOLOv5目标检测模型进行改进，基于改进YOLOv5的红外图像行人目标检测深度学习模型包括：骨干网络设计中拓展跨阶段局部网络(CSP)模块，并在CSP的残差块中引入注意力机制；特征融合网络的设计中增加一个上采样特征层和Concatenate融合层；检测层Head部分采用4个尺度的特征图作为目标预测层，基于改进YOLOv5的红外行人检测深度网络模型如图2所示，具体的：</p> 
<p>步骤1.1：压缩输入通道，将YOLOv5目标检测模型输入端的3通道输入压缩为适合红外图像的单通道输入，进而将3通道输入Focus切片操作后得到的12通道压缩单通道输入得到的4通道，减少输入参数，加速网络计算；</p> 
<p>步骤1.2：拓展YOLOv5目标检测模型骨干网络CSPDarknet中浅层跨阶段局部网络模块(CSP)的厚度，最大限度地提取浅层跨阶段局部网络模块中有利于红外图像弱小目标检测的丰富细节特征，充分利用卷积神经网络在浅层的高分辨率行人特征；拓展后的骨干网络特征提取层如表1所示，对于512×512×1的输入图像，在经过Focus切片操作后，将Conv(卷积模块)和CSP两个模块堆叠三次，即将浅层拓展为与深层同样的CSP模块反馈迭代次数，逐级得到不同尺寸大小的特征图，实现对浅层信息的细粒度特征和深层高级语义信息的充分提取，具体骨干网络结构如图2特征提取部分所示；</p> 
<p>表1拓展后的骨干网络特征提取层</p> 
<p>步骤1.3：引入注意力机制，所述的注意力机制用于骨干网络特征提取过程中加强红外图像行人目标特征的提取，具体的：</p> 
<p>所述的注意力机制选用改进的SK注意力机制，用于在骨干网络特征提取过程中加强红外行人特征的提取，由于SK注意力机制是可直接嵌入网络的轻量级模块，且通过获取不同的感受野信息自适应的调整结构，泛化能力强，将有益于红外图像行人的检测，而且仅需极少的计算负担就能实现系统性的改善；SK注意力机制分为Split、Fuse、Scale三部分，具体的，步骤1.3具体包括以下步骤：</p> 
<p>步骤1.3.1：Split操作，具体的：</p> 
<p>将输入的特征图利用3×3和5×5两个卷积核进行卷积，分别得到输出特征向量U1和U2，U1和U2相加之后得到特征向量U；</p> 
<p>步骤1.3.2：根据步骤1.3.1得到的特征向量U，进行Fuse操作，具体的：</p> 
<p>用全局平均池化Fgp将矩阵压缩到1×1×C，用一个通道描述符表示每个通道的信息，建立通道间的依赖关系，如公式(1)所示，再通过两个全连接层Ffc操作使得通道间的关系是灵活的并且非线性的，这里采用两个全连接层，增加更多的非线性，拟合通道间的复杂相关性，并尽可能减少参数量和计算量，输出经过sigmoid激活函数得到权重值，如公式(2)所示：</p> 
<p>Ffc(Fgp，ω)＝σ(B(Fgp，ω))； (2)</p> 
<p>其中，C表示通道数channel，W和H分别为图像的宽和高，i和j分别为图像的第i行和第j列，ω为相应的权重，σ为sigmoid激活函数，B为批归一化操作；</p> 
<p>步骤1.3.3：Scale操作：</p> 
<p>Scale为一个加权操作，将步骤1.3.1中得到的特征向量U1和U2及步骤1.3.2中得到的权重值进行加权操作，乘回原来的矩阵中，得到加权特征向量V1和V2，并最终得到SK注意力机制最终输出的加权特征图Fscale(U，Ffc)，其中，U表示特征向量，Ffc表示步骤1.3.2中得到的权重值，实现了对于不同channel-wise特征提取，并加强有用的弱小行人目标信息，Fscale(U，Ffc)加权计算如公式(3)所示：</p> 
<p>Fscale(U，Ffc)＝V1+V2＝U1·Ffc+U2·Ffc； (3)</p> 
<p>其中，V1和V2分别为特征向量U1和U2的加权后的特征向量；</p> 
<p>步骤1.4：在YOLOv5目标检测模型的PANet特征融合网络的上采样模块中，增加一个上采样层，所述的上采样层是在8倍、16倍和32倍上采样层的基础上，增加的4倍上采样层；</p> 
<p>步骤1.5：在YOLOv5目标检测模型的PANet特征融合网络中，增加一个Concatenate融合层，将步骤1.4中所述的增加的4倍上采样层与骨干网络特征提取过程中得到的相同尺寸的特征图通过增加的Concatenate融合层进行特征融合，并采用4级空间金字塔池化(Spatial Pyramid Pooling)增大感受野，将4级不同尺寸的特征图利用SPP实现多尺度特征融合，实现Neck部分的多尺度特征融合，具体的Neck结构如图2网络融合部分所示；</p> 
<p>步骤1.6：增加小目标检测层，将步骤1.5中所述的4倍上采样的特征图用于小目标的检测，基于改进YOLOv5的红外图像行人目标检测深度学习模型增加为4种尺度的预测层，分别为4倍、8倍、16倍、32倍上采样特征层，对于512×512的输入红外图像，增加检测层后得到的四个特征尺度分别为：128×128尺度特征层，64×64尺度特征层，32×32尺度特征层和16×16尺度特征层，用于实现Head部分的多尺度检测，具体的Head结构如图2检测结果所示；</p> 
<p>步骤1.7：根据步骤1.6增加的小目标检测层，增加一组小目标尺寸的锚点框(anchors)尺寸，采用K-means自适应算法获取符合红外图像行人小目标尺度特点的锚点框；根据步骤1.6所述的增加的128×128尺度特征层，划分的小尺度网格的需要增加对应小尺度的anchors，因此，anchors增加为对应4个检测尺度的12组；</p> 
<p>步骤2：建立红外图像行人检测数据集，具体的：</p> 
<p>步骤2.1：建立红外图像行人检测训练集、验证集和测试集，所述的建立的红外图像行人检测数据集选用公共数据集KAIST，共95328张图片，包含了校园、街道和乡下的各种常规交通场景，根据6：2：2的比例划分KAIST数据集，即随机选取60％的红外图像行人检测数据作为训练集，20％的红外图像行人检测数据作为验证集，20％的红外图像行人检测数据作为测试集；并根据行人检测目标需求将目标类别设置people和person两个行人类别，其中，红外图像中比较好区分的行人目标被标注为person，不太好分辨的多个行人被标注为people；</p> 
<p>步骤2.2：对建立的红外图像行人检测训练集、验证集和测试集进行预处理，采用Mosaic算法实现红外行人数据的数据增强，Mosaic算法将4张图片，通过随机缩放、随机裁减、随机排布的方式进行拼接，丰富了检测物体的背景和小目标，并且在计算批归一化的时候一次会计算四张图片的数据，使得mini-batch大小不需要很大就可以达到比较好的效果，同时多样的目标样本使训练得到的模型具有更强的泛化能力；</p> 
<p>步骤3：训练步骤1中构建的基于改进YOLOv5的红外图像行人目标检测深度学习模型，具体的：</p> 
<p>步骤3.1：设置训练参数，利用随机优化算法Adam进行训练，训练批次的大小设置为Batch＝64，动量Momentum＝0.9，学习率初始设定为Ir＝0.001，训练迭代次数Epoch＝300；</p> 
<p>步骤3.2：将步骤2.2预处理后的红外图像行人检测训练集和验证集送入步骤1中构建的基于改进YOLOv5的红外图像行人目标检测深度学习模型，在此过程中，即在基于改进YOLOv5的红外图像行人目标检测深度学习模型读入训练集和验证集时，根据读入的标签数据确定红外图像中是否包含目标，自动删除训练集和验证集中不包含行人目标的红外图像，以免干扰模型的训练；</p> 
<p>步骤3.3：自适应缩放图片尺寸，根据网络设定的输入图片尺寸大小实现自适应红外图像行人检测训练集和验证集图片的缩放；</p> 
<p>步骤3.4：训练基于改进YOLOv5的红外图像行人目标检测深度学习模型，检测精度可以反映模型的检测准确率，损失在训练过程中起着重要作用，它可以反映真实值与预测值之间的关系。损失越小，预测值越接近真实值，并且模型的性能越好；因此，根据训练集和验证集交叉验证的平均精度(mAP)变化和损失(loss)变化趋势，调整学习率和迭代次数，直至精度变化和损失变化逐步趋于稳定的状态，确定最终的学习率和迭代次数；</p> 
<p>步骤3.5：根据步骤3.4确定的学习率和迭代次数，完成基于改进YOLOv5的红外图像行人目标检测深度学习模型的训练，得到收敛良好的基于改进YOLOv5的红外图像行人目标检测深度学习模型；</p> 
<p>步骤4：将步骤2.2预处理后的红外图像行人检测测试集送入步骤3.5中训练好的基于改进YOLOv5的红外图像行人目标检测深度学习模型，测试基于改进YOLOv5的红外图像行人目标检测深度学习模型的性能；</p> 
<p>步骤5：评估模型，具体的：</p> 
<p>根据步骤4的测试结果，从平均检测精度和检测速度上对步骤3训练的基于改进YOLOv5的红外图像行人目标检测深度学习模型进行评估，根据实际应用还可从计算复杂度、参数量、权重文件大小等方面对模型进行评估；</p> 
<p>步骤6：判断基于改进YOLOv5的红外图像行人目标检测深度学习模型评估结果的检测精度和速度是否满足实际应用需求，若基于改进YOLOv5的红外图像行人目标检测深度学习模型满足实际应用需求，则执行步骤8，否则，执行步骤7；具体的，根据模型最终的实验结果，平均精度(mAP50)达到98.3％，mAP50:75也达到了95.6％，检测速度为14ms(检测一张图片需要的时间)；另外，相比于YOLOv3检测模型，基于改进YOLOv5的红外行人检测模型在检测时间上减少约62％，网络参数减少约89％，权重文件减少94％以上，计算量减少84％以上，满足实际应用中高检测精度和实时检测的需求；</p> 
<p>步骤7：修正步骤1构建的基于改进YOLOv5的红外图像行人目标检测深度学习模型的宽度和深度，并跳转至步骤3.4重新训练；</p> 
<p>步骤8：将步骤6中满足实际应用需求的基于改进YOLOv5的红外图像行人目标检测深度学习模型用于红外图像或红外视频中行人目标的检测；检测结果示例如图3所示，通过增强浅层特征，融合多特征，改进多尺度检测等改进YOLOv5目标检测模型，实现对于远近距离不同的行人目标的高准确率和实时检测。</p> 
<p>与现有技术相比，本发明的有益效果为：</p> 
<p>第一，由于本发明改进了骨干网络结构CSPDarknet中浅层CSP模块的厚度，使得特征提取网络更加关注浅层特征的提取，能更加彻底地提取浅层细节特征和深层高级语义特征；</p> 
<p>第二，由于本发明在CSP模块中引入改进的SK注意力机制模块，从通道维度对行人目标进行加权融合，迫使网络关注红外行人图像中不同尺度的目标，以更好的学习鲁棒性和可辨别性特征；</p> 
<p>第三，针对红外摄像距离远、目标小的问题，本发明改进了网络检测头结构，增加多尺度目标检测层，采用4级空间金字塔池化(Spatial Pyramid Pooling)增大感受野，提高红外行人小目标检测精度；</p> 
<p>第四，本发明无论从检测精度、检测速度还是网络参数、权重文件等与YOLO v3红外行人图像目标检测相比都有一个质的提高，平均精度提高了约30.6％，mAP50达到了98.3％，mAP50:75也达到了95.6％；另外，检测时间减少约62％，网络参数减少约89％，权重文件减少94％以上，计算量减少84％以上。</p> 
<p>最后应说明的是：以上各实施例仅用以说明本发明的技术方案，而非对其限制；尽管参照前述各实施例对本发明进行了详细的说明，本领域的普通技术人员应当理解：其依然可以对前述各实施例所记载的技术方案进行修改，或者对其中部分或者全部技术特征进行等同替换；而这些修改或者替换，并不使相应技术方案的本质脱离本发明各实施例技术方案的范围。</p> 
<p></p> 
<p></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/025d41cf4967a166b30074d413c6028c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">计算机网络学习笔记（一）---物理层，数据链路层以及网络层</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/554ec384ff88504074b07c31c90b835b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">使用Java的PrinterJob实现调用打印机打印</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>