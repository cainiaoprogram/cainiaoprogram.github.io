<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>知识蒸馏（Knowledge distillation）必读论文合集 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="知识蒸馏（Knowledge distillation）必读论文合集" />
<meta property="og:description" content="1.早期论文 Model Compression, KDD 2006
Do Deep Nets Really Need to be Deep?, NIPS 2014
Distilling the Knowledge in a Neural Network, NIPS-workshop 2014
2.特征蒸馏(Feature Distillation) FitNets: Hints for Thin Deep Nets, ICLR 2015
Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer, ICLR 2017
https://github.com/szagoruyko/attention-transfer
Learning Deep Representations with Probabilistic Knowledge Transfer, ECCV 2018
https://github.com/passalis/probabilistic_kt
Knowledge Distillation via Instance Relationship Graph, CVPR 2019" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/61f040ce01180520037122942f206d13/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-19T16:29:48+08:00" />
<meta property="article:modified_time" content="2023-04-19T16:29:48+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">知识蒸馏（Knowledge distillation）必读论文合集</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h4>1.早期论文</h4> 
<p><br> Model Compression, KDD 2006<br> Do Deep Nets Really Need to be Deep?, NIPS 2014<br> Distilling the Knowledge in a Neural Network, NIPS-workshop 2014</p> 
<h4>2.特征蒸馏(Feature Distillation)</h4> 
<p><br> FitNets: Hints for Thin Deep Nets, ICLR 2015<br> Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer, ICLR 2017<br><span style="color:#fe2c24;">https://github.com/szagoruyko/attention-transfer</span><br> Learning Deep Representations with Probabilistic Knowledge Transfer, ECCV 2018<br><span style="color:#fe2c24;">https://github.com/passalis/probabilistic_kt</span><br> Knowledge Distillation via Instance Relationship Graph, CVPR 2019<br><span style="color:#fe2c24;">https://github.com/yufanLIU/IRG</span><br> Relational Knowledge Distillation, CVPR 2019<br><span style="color:#fe2c24;">https://github.com/lenscloth/RKD</span><br> Similarity-Preserving Knowledge Distillation, CVPR 2019<br> Variational Information Distillation for Knowledge Transfer, CVPR 2019<br> Contrastive Representation Distillation, ICLR 2020<br><span style="color:#fe2c24;">https://github.com/HobbitLong/RepDistiller</span><br> Heterogeneous Knowledge Distillation using Information Flow Modeling, CVPR 2020<br><span style="color:#fe2c24;">https://github.com/passalis/pkth</span><br> Matching Guided Distillation, ECCV 2020<br><span style="color:#fe2c24;">https://github.com/KaiyuYue/mgd</span><br> Cross-Layer Distillation with Semantic Calibration, AAAI 2021<br><span style="color:#fe2c24;">https://github.com/DefangChen/SemCKD</span><br> Distilling Holistic Knowledge with Graph Neural Networks, ICCV 2021<br><span style="color:#fe2c24;">https://github.com/wyc-ruiker/HKD</span><br> Knowledge Distillation with the Reused Teacher Classifier, CVPR 2022<br><span style="color:#fe2c24;">https://github.com/DefangChen/SimKD</span></p> 
<h4>3.在线知识蒸馏(Online Knowledge Distillation)</h4> 
<p><br> Deep Mutual Learning, CVPR 2018<br><span style="color:#fe2c24;">https://github.com/huanghoujing/AlignedReID-Re-Production-Pytorch</span><br> Large scale distributed neural network training through online distillation, ICLR 2018<br> Collaborative Learning for Deep Neural Networks, NIPS 2018<br> Knowledge Distillation by On-the-Fly Native Ensemble, NIPS 2018<br><span style="color:#fe2c24;">https://github.com/Lan1991Xu/ONE_NeurIPS2018</span><br> Online Knowledge Distillation with Diverse Peers, AAAI 2020<br><span style="color:#fe2c24;">https://github.com/DefangChen/OKDDip-AAAI2020</span><br> Online Knowledge Distillation via Collaborative Learning, CVPR 2020</p> 
<h4>4.多教师知识蒸馏(Multi-Teacher Knowledge Distillation)</h4> 
<p><br> Distilling knowledge from ensembles of neural networks for speech recognition, INTERSPEECH 2016<br> Efficient Knowledge Distillation from an Ensemble of Teachers, INTERSPEECH 2017<br> Learning from Multiple Teacher Networks, KDD 2017<br> Multi-teacher Knowledge Distillation for Compressed Video Action Recognition on Deep Neural Networks, ICASSP 2019<br> Agree to Disagree: Adaptive Ensemble Knowledge Distillation in Gradient Space, NIPS 2020<br><span style="color:#fe2c24;">https://github.com/AnTuo1998/AE-KD</span><br> Adaptive Knowledge Distillation Based on Entropy, ICASSP 2020<br> Reinforced Multi-Teacher Selection for Knowledge Distillation, AAAI 2021<br> Adaptive Distillation: Aggregating Knowledge from Multiple Paths for Efficient Distillation, BMVC 2021<br><span style="color:#fe2c24;">https://github.com/wyze-AI/AdaptiveDistillation</span><br> Confidence-Aware Multi-Teacher Knowledge Distillation, ICASSP 2022<br><span style="color:#fe2c24;">https://github.com/Rorozhl/CA-MKD</span></p> 
<h4>5.扩散蒸馏(Diffusion Distillation)</h4> 
<p><br> Progressive Distillation for Fast Sampling of Diffusion Models, ICLR 2022<br><span style="color:#fe2c24;">https://github.com/google-research/google-research/tree/master/diffusion_distillation</span><br> Accelerating Diffusion Sampling with Classifier-based Feature Distillation, ICME 2023<br><span style="color:#fe2c24;">https://github.com/zju-SWJ/RCFD</span></p> 
<h4>6.无数据知识蒸馏(Data-Free Knowledge Distillation)</h4> 
<p><br> Data-Free Knowledge Distillation for Deep Neural Networks, NIPS-workshop 2017<br><span style="color:#fe2c24;">https://github.com/iRapha/replayed_distillation</span><br> DAFL: Data-Free Learning of Student Networks, ICCV 2019<br><span style="color:#fe2c24;">https://github.com/huawei-noah/Efficient-Computing/tree/master/Data-Efficient-Model-Compression</span><br> Zero-Shot Knowledge Distillation in Deep Networks, ICML 2019<br><span style="color:#fe2c24;">https://github.com/vcl-iisc/ZSKD</span><br> Zero-shot Knowledge Transfer via Adversarial Belief Matching, NIPS 2019<br><span style="color:#fe2c24;">https://github.com/polo5/ZeroShotKnowledgeTransfer</span><br> Knowledge Extraction with No Observable Data, NIPS 2019<br><span style="color:#fe2c24;">https://github.com/snudatalab/KegNet</span><br> Dream Distillation: A Data-Independent Model Compression Framework, ICML-workshop 2019<br> DeGAN : Data-Enriching GAN for Retrieving Representative Samples from a Trained Classifier, AAAI 2020<br><span style="color:#fe2c24;">https://github.com/vcl-iisc/DeGAN</span><br> Dreaming to Distill: Data-free Knowledge Transfer via DeepInversion, CVPR 2020<br><span style="color:#fe2c24;">https://github.com/NVlabs/DeepInversion</span><br> The Knowledge Within: Methods for Data-Free Model Compression, CVPR 2020<br> Data-Free Adversarial Distillation, ICASSP 2020<br> Data-Free Knowledge Distillation with Soft Targeted Transfer Set Synthesis, AAAI 2021<br> Learning Student Networks in the Wild, CVPR 2021<br><span style="color:#fe2c24;">https://github.com/huawei-noah/Data-Efficient-Model-Compression</span><br> Contrastive Model Inversion for Data-Free Knowledge Distillation, IJCAI 2021<br><span style="color:#fe2c24;">https://github.com/zju-vipa/DataFree</span></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/65a0f5ebdfe716f53f17a8feb9eaaa84/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Ubuntu20.04&#43;Windows10双系统迁移新硬盘并解决引导损坏全流程总结</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/577072d86b597b046bfd7e51a7df0765/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">vue3新特性 v-bind</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>