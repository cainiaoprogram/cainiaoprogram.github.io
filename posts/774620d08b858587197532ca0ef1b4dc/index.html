<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>机器学习：降维算法-核主成分分析KPCA算法原理推导 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="机器学习：降维算法-核主成分分析KPCA算法原理推导" />
<meta property="og:description" content="我的小程序： 待办计划：给自己立个小目标吧！ 说真的，刚开始接触机器学习，一看到带“核”字的算法就头疼（-…-），没高人指引，总觉得理解起来费劲，也不确定理解的对不对。可能是因为这个“核”有点抽象，没有具体形式（形式不确定），操作很风骚。当然到现在也不敢说自己有多理解，只能扯扯目前所理解到的核主成分分析KPCA。
PCA是简单的线性降维的方法，KPCA则是其非线性的扩展，可以挖掘到数据集中的非线性信息。
KPCA的主要过程是：
首先将原始数据非线性映射到高维空间；再把第1步中的数据从高维空间投影降维到需要的维数d&#39; 可以看出，KPCA只是比PCA多了一步映射到高维空间的操作，降维的操作是一样的。所以，KPCA最终投影到的超平面也应满足PCA中的最近重构性或最大可分性。于是KPCA最终的投影向量也应满足PCA中的等式（参见：机器学习：降维算法-主成分分析PCA算法两种角度的推导）：
.
即：
.
其中是样本点映射到的高维空间中的像。进一步得：
.
其中是的第j个分量。
假设是由原始空间的样本通过映射产生，即.若映射已知，那么容易求到和投影向量，问题就解决了。但通常情况下，是不知道的具体形式的（后面会发现，我们不用知道的具体形式一样能求解！）。不管怎样，先表达出这种映射关系：
将写成：
................................................................(1)
将写成：
..................................................................................(2)
将(2)式代入(1)式得：
................................(3)
记：
,
.
将和代入(3)式得：
................................................................................(4)
关键的地方到了，接下来引入核函数：
.
核函数本质上也就是个函数，跟y=kx没啥区别，特殊之处在于，它的计算结果表达了变量在高维空间的内积值。就是说，本来是在高维空间（甚至无限维空间）计算的内积，可以在原始较低维的空间通过核函数计算得到。
有常见的几种核函数，到底选用哪种核函数，可能需要去尝试，看哪种核函数产生的效果好（比如在KPCA中，就是看哪种核函数带来的降维效果更好）。
之所以引入核函数，可以认为有两方面原因：
映射关系未知；映射到的高维空间维数可能非常高（甚至无限维），在高维空间计算开销太大，十分困难。 核函数对应的核矩阵为：
.
(4)式两边左乘得：
，
把核矩阵K代入上式，可得：
：
两边去掉K，得：
.
（这里还是没搞清楚，为什么能直接去掉K，理论上K是可逆矩阵时才能直接消去。但核函数确定的核矩阵只要求是半正定的，半正定矩阵又不一定是可逆的。这里去掉K的依据是什么？望博友指点。）
可以看到，通过核函数的这一波操作，原始空间到高维空间的映射变得无形了，最终得到的式子并没有看到映射的影子，这就是“核”操作的风骚之处。我们不知道映射的具体形式，我们也不用知道它的形式。
显然，上式是特征分解问题，取K的最大的d&#39;个特征值对应的特征向量即可。
对于新样本x，其投影后第维坐标为：
.
可以看到，KPCA需要对所有样本求和，计算开销还是挺大的。
待办计划：给自己立个小目标吧！ 参考资料：周志华《机器学习》
参考博文：
核主成分分析(Kernel Principal Component Analysis, KPCA)的公式推导过程 机器学习：核函数和核矩阵简介
相关博文：机器学习：降维算法-主成分分析PCA算法两种角度的推导" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/774620d08b858587197532ca0ef1b4dc/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-09-03T14:09:52+08:00" />
<meta property="article:modified_time" content="2022-09-03T14:09:52+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">机器学习：降维算法-核主成分分析KPCA算法原理推导</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h4 style="text-align:center;"><span style="color:#fe2c24;"><strong>我的小程序：</strong></span></h4> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" height="200" src="https://images2.imgbox.com/29/1e/v5UA3g4D_o.jpg" width="200"> 
  <figcaption> 
   <strong>待办计划：给自己立个小目标吧！</strong> 
  </figcaption> 
 </figure> 
</div> 
<p>说真的，刚开始接触机器学习，一看到带“核”字的算法就头疼（-…-），没高人指引，总觉得理解起来费劲，也不确定理解的对不对。可能是因为这个“核”有点抽象，没有具体形式（形式不确定），操作很风骚。当然到现在也不敢说自己有多理解，只能扯扯目前所理解到的核主成分分析KPCA。</p> 
<p>PCA是简单的线性降维的方法，KPCA则是其非线性的扩展，可以<strong>挖掘到数据集中的非线性信息</strong>。</p> 
<p>KPCA的主要过程是：</p> 
<ol><li>首先将原始数据非线性映射到高维空间；</li><li>再把第1步中的数据从高维空间投影降维到需要的维数d'</li></ol> 
<p>可以看出，KPCA只是比PCA多了一步映射到高维空间的操作，降维的操作是一样的。所以，KPCA最终投影到的超平面也应满足PCA中的最近重构性或最大可分性。于是KPCA最终的投影向量<img alt="w_{j}" class="mathcode" src="https://images2.imgbox.com/47/13/IJJ8netu_o.gif">也应满足PCA中的等式（参见：<a href="https://blog.csdn.net/weixin_35732969/article/details/81556904" title="机器学习：降维算法-主成分分析PCA算法两种角度的推导">机器学习：降维算法-主成分分析PCA算法两种角度的推导</a>）：</p> 
<p><img alt="ZZ^{T}w_{j} = \lambda_{j}w_{j}" class="mathcode" src="https://images2.imgbox.com/bd/8b/fhkzjNbT_o.gif">.</p> 
<p>即：</p> 
<p><img alt="(\sum_{i=1}^{m}z_{i}z_{i}^{T})w_{j} = \lambda_{j}w_{j}" class="mathcode" src="https://images2.imgbox.com/62/2a/mOY9wOZc_o.gif">.</p> 
<p>其中<img alt="z_{i}" class="mathcode" src="https://images2.imgbox.com/13/c1/TYF5xv2s_o.gif">是样本点<img alt="x_{i}" class="mathcode" src="https://images2.imgbox.com/d6/40/kjl5WmFq_o.gif">映射到的高维空间中的像。进一步得：</p> 
<p><img alt="\begin{align*} w_{j} &amp;= \frac{1}{\lambda_{j}}(\sum_{i=1}^{m}z_{i}z_{i}^{T})w_{j}\\ &amp;= \sum_{i=1}^{m}z_{i}\frac{z_{i}^{T}w_{j}}{\lambda_{j}}\\ &amp;= \sum_{i=1}^{m}z_{i}\alpha _{i}^{j} \end{align*}" class="mathcode" src="https://images2.imgbox.com/fc/69/94I2CyLY_o.gif">.</p> 
<p>其中<img alt="\alpha _{i}^{j} = \frac{z_{i}^{T}w_{j}}{\lambda_{j}}" class="mathcode" src="https://images2.imgbox.com/b4/07/hWYZNU61_o.gif">是<img alt="\alpha _{i}" class="mathcode" src="https://images2.imgbox.com/0a/b6/y0sw3mjw_o.gif">的第j个分量。</p> 
<p>假设<img alt="z_{i}" class="mathcode" src="https://images2.imgbox.com/77/37/APhAOMCQ_o.gif">是由原始空间的样本<img alt="x_{i}" class="mathcode" src="https://images2.imgbox.com/50/8b/7273H4WN_o.gif">通过映射<img alt="\phi" class="mathcode" src="https://images2.imgbox.com/e6/8b/PbXlPTVq_o.gif">产生，即<img alt="z_{i} = \phi (x_{i}),i=1,2,...,m" class="mathcode" src="https://images2.imgbox.com/1a/30/aLwQRTIG_o.gif">.若映射已知，那么容易求到<img alt="z_{i}" class="mathcode" src="https://images2.imgbox.com/bb/52/njdnS5x5_o.gif">和投影向量<img alt="w_{j}" class="mathcode" src="https://images2.imgbox.com/14/55/T4GbGk45_o.gif">，问题就解决了。但通常情况下，是不知道<img alt="\phi" class="mathcode" src="https://images2.imgbox.com/d7/ed/XGBifKqr_o.gif">的具体形式的（后面会发现，我们不用知道<img alt="\phi" class="mathcode" src="https://images2.imgbox.com/35/12/JiZvtof1_o.gif">的具体形式一样能求解！）。不管怎样，先表达出<img alt="\phi" class="mathcode" src="https://images2.imgbox.com/f5/c7/IkdnVEra_o.gif">这种映射关系：</p> 
<p>将<img alt="(\sum_{i=1}^{m}z_{i}z_{i}^{T})w_{j} = \lambda_{j}w_{j}" class="mathcode" src="https://images2.imgbox.com/fb/dc/etd1ejAx_o.gif">写成：</p> 
<p><img alt="(\sum_{i=1}^{m}\phi (x_{i})\phi (x_{i})^{T})w_{j} = \lambda_{j}w_{j}" class="mathcode" src="https://images2.imgbox.com/aa/7b/dKZ9HPLW_o.gif">................................................................(1)</p> 
<p>将<img alt="w_{j} = \sum_{i=1}^{m}z_{i}\alpha _{i}^{j}" class="mathcode" src="https://images2.imgbox.com/bd/54/e2hzGhbR_o.gif">写成：</p> 
<p><img alt="w_{j} = \sum_{i=1}^{m}\phi (x_{i})\alpha _{i}^{j}" class="mathcode" src="https://images2.imgbox.com/a2/19/gd7scVGD_o.gif">..................................................................................(2)</p> 
<p>将(2)式代入(1)式得：</p> 
<p><img alt="(\sum_{i=1}^{m}\phi (x_{i})\phi (x_{i})^{T})\sum_{i=1}^{m}\phi (x_{i})\alpha _{i}^{j} = \lambda_{j}\sum_{i=1}^{m}\phi (x_{i})\alpha _{i}^{j}" class="mathcode" src="https://images2.imgbox.com/98/42/Q3eoiVch_o.gif">................................(3)</p> 
<p>记：</p> 
<p><img alt="\Phi = \{\phi (x_{1}),\phi (x_{2}),...,\phi (x_{m})\}" class="mathcode" src="https://images2.imgbox.com/af/85/iJtKcu01_o.gif">,</p> 
<p><img alt="\alpha ^{j} = (\alpha _{1}^{j},\alpha _{2}^{j},...,\alpha _{m}^{j})^{T}" class="mathcode" src="https://images2.imgbox.com/85/dc/zNZRCNVe_o.gif">.</p> 
<p>将<img alt="\Phi" class="mathcode" src="https://images2.imgbox.com/7b/7e/RTT97T0l_o.gif">和<img alt="\alpha ^{j}" class="mathcode" src="https://images2.imgbox.com/09/2d/3fkULOGO_o.gif">代入(3)式得：</p> 
<p><img alt="\Phi \Phi ^{T}\Phi \alpha ^{j} = \lambda_{j}\Phi\alpha ^{j}" class="mathcode" src="https://images2.imgbox.com/d4/44/PrSiUnVc_o.gif">................................................................................(4)</p> 
<p>关键的地方到了，接下来引入<a href="https://blog.csdn.net/weixin_35732969/article/details/81603520" title="核函数">核函数</a>：</p> 
<p><img alt="\kappa (x_{i},x_{j}) = \phi (x_{i})^{T}\phi (x_{j})" class="mathcode" src="https://images2.imgbox.com/da/25/DE5wguBm_o.gif">.</p> 
<p>核函数本质上也就是个函数，跟y=kx没啥区别，特殊之处在于，它的计算结果表达了变量<img alt="x_{i},x_{j}" class="mathcode" src="https://images2.imgbox.com/50/13/FM7n5dK7_o.gif">在高维空间的内积<img alt="\phi (x_{i})^{T}\phi (x_{i})" class="mathcode" src="https://images2.imgbox.com/ac/43/kP3ZdmNx_o.gif">值。就是说，本来是在高维空间（甚至无限维空间）计算的内积<img alt="\phi (x_{i})^{T}\phi (x_{i})" class="mathcode" src="https://images2.imgbox.com/26/32/HKvL3Z6H_o.gif">，可以在原始较低维的空间通过核函数<img alt="\kappa (*,*)" class="mathcode" src="https://images2.imgbox.com/d6/ad/Tugwbf21_o.gif">计算得到。</p> 
<p>有常见的几种核函数，到底选用哪种核函数，可能需要去尝试，看哪种核函数产生的效果好（比如在KPCA中，就是看哪种核函数带来的降维效果更好）。</p> 
<p>之所以引入核函数，可以认为有两方面原因：</p> 
<ol><li>映射关系<img alt="\phi" class="mathcode" src="https://images2.imgbox.com/45/da/Rrl3ZWBM_o.gif">未知；</li><li>映射到的高维空间维数可能非常高（甚至无限维），在高维空间计算<img alt="\phi (x_{i})^{T}\phi (x_{i})" class="mathcode" src="https://images2.imgbox.com/75/7d/PyCA0Mac_o.gif">开销太大，十分困难。</li></ol> 
<p>核函数对应的<a href="https://blog.csdn.net/weixin_35732969/article/details/81603520" title="核矩阵">核矩阵</a>为：</p> 
<p><img alt="K = \begin{bmatrix} \kappa (x_{1},x_{1})&amp; . &amp; . &amp; \kappa (x_{1},x_{m})\\ . &amp; . &amp; . &amp; .\\ . &amp; . &amp; . &amp; .\\ \kappa (x_{m},x_{1}) &amp; . &amp; . &amp; \kappa (x_{m},x_{m}) \end{bmatrix} = \Phi ^{T}\Phi" class="mathcode" src="https://images2.imgbox.com/76/00/VtBEIwv1_o.gif">.</p> 
<p>(4)式两边左乘<img alt="\Phi ^{T}" class="mathcode" src="https://images2.imgbox.com/96/82/ye0DC7nw_o.gif">得：</p> 
<p><img alt="\Phi ^{T}\Phi \Phi ^{T}\Phi \alpha ^{j} = \lambda_{j}\Phi ^{T}\Phi\alpha ^{j}" class="mathcode" src="https://images2.imgbox.com/24/e7/sx0u1V9S_o.gif">，</p> 
<p>把核矩阵K代入上式，可得：</p> 
<p><img alt="K^{2}\alpha ^{j} = \lambda_{j}K\alpha ^{j}" class="mathcode" src="https://images2.imgbox.com/ee/bb/77ayWldx_o.gif">：</p> 
<p>两边去掉K，得：</p> 
<p><img alt="K\alpha ^{j} = \lambda_{j}\alpha ^{j}" class="mathcode" src="https://images2.imgbox.com/ee/26/0OrZDZ8j_o.gif">.</p> 
<p>（这里还是没搞清楚，为什么能直接去掉K，理论上K是可逆矩阵时才能直接消去。但核函数确定的核矩阵只要求是半正定的，半正定矩阵又不一定是可逆的。这里<strong>去掉K的依据是什么</strong>？望博友指点。）</p> 
<p>可以看到，通过核函数的这一波操作，原始空间到高维空间的映射变得无形了，最终得到的式子并没有看到映射<img alt="\phi" class="mathcode" src="https://images2.imgbox.com/bc/73/xEt3ew7H_o.gif">的影子，这就是“核”操作的风骚之处。我们不知道映射<img alt="\phi" class="mathcode" src="https://images2.imgbox.com/e2/1d/MMfEi9Sv_o.gif">的具体形式，我们也不用知道它的形式。</p> 
<p>显然，上式是特征分解问题，取K的最大的d'个特征值对应的特征向量即可。</p> 
<p>对于新样本x，其投影后第<img alt="j(j = 1,2,...,d')" class="mathcode" src="https://images2.imgbox.com/24/c4/Wv7E2Vnj_o.gif">维坐标为：</p> 
<p><img alt="\begin{align*} x'_{j} &amp;=w_{j}^{T}\phi (x) \\ &amp;=\sum_{i=1}^{m}\alpha _{i}^{j}\phi (x_{i})^{T} \phi (x)\\ &amp;= \sum_{i=1}^{m}\alpha _{i}^{j}\kappa (x_{i},x) \end{align*}" class="mathcode" src="https://images2.imgbox.com/09/9d/X3tque3r_o.gif">.</p> 
<p>可以看到，KPCA需要对所有样本求和，计算开销还是挺大的。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" height="200" src="https://images2.imgbox.com/5f/11/NO3toKZF_o.jpg" width="200"> 
  <figcaption> 
   <strong>待办计划：给自己立个小目标吧！</strong> 
  </figcaption> 
 </figure> 
</div> 
<p> </p> 
<p>参考资料：周志华《机器学习》</p> 
<p>参考博文：</p> 
<p><a href="https://www.cnblogs.com/wt869054461/p/6686037.html" rel="nofollow" title="核主成分分析(Kernel Principal Component Analysis, KPCA)的公式推导过程 ">核主成分分析(Kernel Principal Component Analysis, KPCA)的公式推导过程 </a></p> 
<p><a href="https://blog.csdn.net/weixin_35732969/article/details/81603520" title="机器学习：核函数和核矩阵简介">机器学习：核函数和核矩阵简介</a></p> 
<p>相关博文：<a href="https://blog.csdn.net/weixin_35732969/article/details/81556904" title="机器学习：降维算法-主成分分析PCA算法两种角度的推导">机器学习：降维算法-主成分分析PCA算法两种角度的推导</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/93919d0c737079d47022265b58139dd7/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">最详细移动硬盘安装linux过程，装在移动硬盘上的linux系统不能在另一台电脑启动的解决办法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b6c3c1123f746c4d95eaea1f462128be/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">机器学习：聚类-闵科夫斯基距离和无序属性的VDM距离计算</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>