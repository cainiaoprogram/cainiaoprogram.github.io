<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>深层网络梯度消失-爆炸原因 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="深层网络梯度消失-爆炸原因" />
<meta property="og:description" content="声明：文章仅作知识整理、分享，如有侵权请联系作者删除博文，谢谢！
网上有很多关于梯度消失-爆炸这方面的文章，相似的也比较多，最近对不同文章进行整理，修改部分文章公式错误，形成整理。
1、概念 目前优化神经网络的方法都是基于BP，即根据损失函数计算的误差通过梯度反向传播的方式，指导深度网络权值的更新优化。其中将误差从末层往前传递的过程需要链式法则（Chain Rule）的帮助，因此反向传播算法可以说是梯度下降在链式法则中的应用。
而链式法则是一个连乘的形式，所以当层数越深的时候，梯度将以指数形式传播。梯度消失问题和梯度爆炸问题一般随着网络层数的增加会变得越来越明显。在根据损失函数计算的误差通过梯度反向传播的方式对深度网络权值进行更新时，得到的梯度值接近0或特别大，也就是梯度消失或爆炸。梯度消失或梯度爆炸在本质原理上其实是一样的。
1.1、梯度消失 经常出现，产生的原因有：一是在深层网络中，二是采用了不合适的损失函数，比如sigmoid。当梯度消失发生时，接近于输出层的隐藏层由于其梯度相对正常，所以权值更新时也就相对正常，但是当越靠近输入层时，由于梯度消失现象，会导致靠近输入层的隐藏层权值更新缓慢或者更新停滞。这就导致在训练时，只等价于后面几层的浅层网络的学习。
梯度消失的影响：
1）浅层基本不学习，后面几层一直在学习，失去深度的意义。
2）无法收敛，相当于浅层网络。
1.2、梯度爆炸 根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都大于1的话，在经过足够多层传播之后，误差对输入层的偏导会趋于无穷大。这种情况又会导致靠近输入层的隐含层神经元调整变动极大。梯度爆炸一般出现在深层网络和权值初始化值太大的情况下。另外，初始学习率太小或太大也会出现梯度消失或爆炸。
梯度爆炸的影响：
1）模型不稳定，导致更新过程中的损失出现显著变化；
2）训练过程中，在极端情况下，权重的值变得非常大，以至于溢出，导致模型损失变成 NaN等等。
2、产生梯度消失和梯度爆炸的原因 梯度消失的根源—–深度神经求导网络和反向传播。目前深度学习方法中，深度神经网络的发展造就了我们可以构建更深层的网络完成更复杂的任务，深层网络比如深度卷积网络，LSTM等等，而且最终结果表明，在处理复杂任务上，深度网络比浅层的网络具有更好的效果。但是，目前优化神经网络的方法都是基于反向传播的思想，即根据损失函数计算的误差通过梯度反向传播的方式，指导深度网络权值的更新优化。下面将从这3个角度分析一下产生这两种现象的根本原因：
比较简单的深层网络如下：
2.1、深层网络 如图所示的含有3个隐藏层的神经网络，梯度消失问题发生时，接近于输出层的hidden layer 3等的权值更新相对正常，但前面的hidden layer 1的权值更新会变得很慢，导致前面的层权值几乎不变，仍接近于初始化的权值，这就导致hidden layer 1相当于只是一个映射层，对所有的输入做了一个同一映射，这是此深层网络的学习就等价于只有后几层的浅层网络的学习了。
图中是一个四层的全连接网络，假设每一层网络激活后的输出为fi(x),其中i为第i层, x代表第i层的输入，也就是第i−1层的输出，f是激活函数，那么，得出：
简单记为：
BP算法基于梯度下降策略，以目标的负梯度方向对参数进行调整，参数的更新为w←w&#43;Δw，给定学习率α，得出：
如果要激活函数的导数、网络初值(w,b)连续相乘表现为w的更新第一隐藏层量。避免网络不work的权值信息过程就是调整这部分连乘结果，根据链式求导法则使其保持在1附近。学习率决定的网络学习的快慢，过大或过小也会直接影响网络的参数更新梯度信息过程。
，很容易看出来：
，即第一层的输入。
所以说af4/af3就是对激活函数进行求导，如果此部分大于1，那么层数增多的时候，最终的求出的梯度更新将以指数形式增加，即发生梯度爆炸，如果此部分小于1，那么随着层数增多，求出的梯度更新信息将会以指数形式衰减，即发生了梯度消失。
如果说从数学上看不够直观的话，下面几个图可以很直观的说明深层网络的梯度问题：
图中的曲线表示权值更新的速度，对于下图两个隐层的网络来说，已经可以发现隐藏层2的权值更新速度要比隐藏层1更新的速度慢。
那么对于四个隐层的网络来说，就更明显了，第四隐藏层比第一隐藏层的更新速度慢了两个数量级：
总结：从深层网络角度来讲，不同的层学习的速度差异很大，表现为网络中靠近输出的层学习的情况很好，靠近输入的层学习的很慢，有时甚至训练了很久，前几层的权值和刚开始随机初始化的值差不多。因此，梯度消失、爆炸，其根本原因在于反向传播训练法则，属于先天不足。
2.2、激活函数 以下图的反向传播为例（假设每一层只有一个神经元且对于每一层：
偏置b可以推导出：
而sigmoid的导数为：
同理，使用tanh作为损失函数，它的导数图如下，可以看出，tanh比sigmoid要好一些，但是它的倒数仍然是小于1的。tanh数学表达为：
如果接近输出层的激活函数求导后梯度值大于1，那么层数增多的时候，最终求出的梯度很容易指数级增长，就会产生梯度爆炸；相反，如果小于1，那么经过链式法则的连乘形式，也会很容易衰减至0，就会产生梯度消失。
2.3、初始化权值太大 如上图所示，当：
，也就是w比较大的情况。根据链式相乘(反向传播)可得，则前面的网络层比后面的网络层梯度变化更快，很容易发生梯度爆炸的问题。
3、总结 深层网络出现梯度消失或爆炸，主要是由于链式求导发现传播引起。参数的更新为w←w&#43;Δw，给定学习率α，得出：
激活函数的导数、网络初值(w,b)连续相乘表现为w的更新量。避免网络不work的过程就是调整这部分连乘结果，使其保持在1附近。学习率决定的网络学习的快慢，过大或过小也会直接影响网络的参数更新过程。
参考文章：
1、https://zhuanlan.zhihu.com/p/25631496
2、https://zhuanlan.zhihu.com/p/72589432" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/abcb5f3983a99bf0b8b8c8a6e3bd15d9/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-08-17T20:57:41+08:00" />
<meta property="article:modified_time" content="2020-08-17T20:57:41+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深层网络梯度消失-爆炸原因</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><span style="color:#ffbb66;">声明：文章仅作知识整理、分享，如有侵权请联系作者删除博文，谢谢！</span></p> 
<p style="text-indent:33px;">网上有很多关于梯度消失-爆炸这方面的文章，相似的也比较多，最近对不同文章进行整理，修改部分文章公式错误，形成整理。</p> 
<h2>1、概念</h2> 
<p style="text-indent:33px;">目前优化神经网络的方法都是基于BP，即根据损失函数计算的误差通过梯度反向传播的方式，指导深度网络权值的更新优化。其中将误差从末层往前传递的过程需要链式法则（Chain Rule）的帮助，因此反向传播算法可以说是梯度下降在链式法则中的应用。</p> 
<p style="text-indent:33px;">而链式法则是一个连乘的形式，所以当层数越深的时候，梯度将以指数形式传播。梯度消失问题和梯度爆炸问题一般随着网络层数的增加会变得越来越明显。在根据损失函数计算的误差通过梯度反向传播的方式对深度网络权值进行更新时，得到的梯度值接近0或特别大，也就是梯度消失或爆炸。梯度消失或梯度爆炸在本质原理上其实是一样的。</p> 
<h3>1.1、梯度消失</h3> 
<p style="text-indent:33px;">经常出现，产生的原因有：一是在深层网络中，二是采用了不合适的损失函数，比如sigmoid。当梯度消失发生时，接近于输出层的隐藏层由于其梯度相对正常，所以权值更新时也就相对正常，但是当越靠近输入层时，由于梯度消失现象，会导致靠近输入层的隐藏层权值更新缓慢或者更新停滞。这就导致在训练时，只等价于后面几层的浅层网络的学习。</p> 
<p style="text-indent:0;"><strong>梯度消失的影响：</strong></p> 
<p style="text-indent:33px;">1）浅层基本不学习，后面几层一直在学习，失去深度的意义。</p> 
<p style="text-indent:33px;">2）无法收敛，相当于浅层网络。</p> 
<h3>1.2、梯度爆炸</h3> 
<p style="text-indent:33px;">根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都大于1的话，在经过足够多层传播之后，误差对输入层的偏导会趋于无穷大。这种情况又会导致靠近输入层的隐含层神经元调整变动极大。梯度爆炸一般出现在深层网络和权值初始化值太大的情况下。另外，初始学习率太小或太大也会出现梯度消失或爆炸。</p> 
<p style="text-indent:0;"><strong>梯度爆炸的影响：</strong></p> 
<p style="text-indent:33px;">1）模型不稳定，导致更新过程中的损失出现显著变化；</p> 
<p style="text-indent:33px;">2）训练过程中，在极端情况下，权重的值变得非常大，以至于溢出，导致模型损失变成 NaN等等。</p> 
<h2>2、产生梯度消失和梯度爆炸的原因</h2> 
<p style="text-indent:33px;">梯度消失的根源—–深度神经求导网络和反向传播。目前深度学习方法中，深度神经网络的发展造就了我们可以构建更深层的网络完成更复杂的任务，深层网络比如深度卷积网络，LSTM等等，而且最终结果表明，在处理复杂任务上，深度网络比浅层的网络具有更好的效果。但是，目前优化神经网络的方法都是基于反向传播的思想，即根据损失函数计算的误差通过梯度反向传播的方式，指导深度网络权值的更新优化。下面将从这3个角度分析一下产生这两种现象的根本原因：</p> 
<p style="text-indent:33px;">比较简单的深层网络如下：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/6e/75/PvivMruy_o.jpg"></p> 
<h3>2.1、深层网络</h3> 
<p style="text-indent:33px;">如图所示的含有3个隐藏层的神经网络，<span style="color:#f33b45;">梯度消失问题发生时，接近于输出层的hidden layer 3等的权值更新相对正常，但前面的hidden layer 1的权值更新会变得很慢，</span>导致前面的层权值几乎不变，仍接近于初始化的权值，这就导致hidden layer 1相当于只是一个映射层，对所有的输入做了一个同一映射，这是此深层网络的学习就等价于只有后几层的浅层网络的学习了。</p> 
<p style="text-indent:33px;"> 图中是一个四层的全连接网络，假设每一层网络激活后的输出为fi(x),其中i为第i层, x代表第i层的输入，也就是第i−1层的输出，f是激活函数，那么，得出：</p> 
<p style="text-align:center;"><img alt="" height="30" src="https://images2.imgbox.com/18/3d/JDPgWA5F_o.png" width="260"></p> 
<p style="text-indent:33px;">简单记为：</p> 
<p style="text-align:center;"><img alt="" height="36" src="https://images2.imgbox.com/37/83/7CL4Fhzw_o.png" width="260"></p> 
<p style="text-indent:33px;">BP算法基于梯度下降策略，以目标的负梯度方向对参数进行调整，参数的更新为w←w+Δw，给定学习率α，得出：</p> 
<p style="text-align:center;"><img alt="" height="29" src="https://images2.imgbox.com/c8/d8/ahyolJOh_o.png" width="113"></p> 
<p style="text-indent:33px;">如果要激活函数的导数、网络初值(w,b)连续相乘表现为w的更新第一隐藏层量。避免网络不work的权值信息过程就是调整这部分连乘结果，根据链式求导法则使其保持在1附近。学习率决定的网络学习的快慢，过大或过小也会直接影响网络的参数更新梯度信息过程。</p> 
<p style="text-align:center;"><img alt="" height="55" src="https://images2.imgbox.com/c4/9b/5rEB4B4T_o.png" width="395"></p> 
<p style="text-indent:33px;"> ，很容易看出来：</p> 
<p style="text-align:center;"><img alt="" height="59" src="https://images2.imgbox.com/64/ac/mP3LUq4e_o.png" width="113"></p> 
<p style="text-indent:33px;">，即第一层的输入。</p> 
<p style="text-indent:33px;"> 所以说af4/af3就是对激活函数进行求导，如果此部分大于1，那么层数增多的时候，最终的求出的梯度更新将以指数形式增加，即发生梯度爆炸，如果此部分小于1，那么随着层数增多，求出的梯度更新信息将会以指数形式衰减，即发生了梯度消失。</p> 
<p style="text-indent:33px;">如果说从数学上看不够直观的话，下面几个图可以很直观的说明深层网络的梯度问题：</p> 
<p style="text-indent:33px;">图中的曲线表示权值更新的速度，对于下图两个隐层的网络来说，已经可以发现隐藏层2的权值更新速度要比隐藏层1更新的速度慢。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/07/37/P05wlL9N_o.jpg"></p> 
<p style="text-indent:33px;">那么对于四个隐层的网络来说，就更明显了，第四隐藏层比第一隐藏层的更新速度慢了两个数量级：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/a3/c7/pXAsdGBA_o.jpg"></p> 
<p style="text-indent:33px;">总结：从深层网络角度来讲，不同的层学习的速度差异很大，表现为网络中靠近输出的层学习的情况很好，靠近输入的层学习的很慢，有时甚至训练了很久，前几层的权值和刚开始随机初始化的值差不多。因此，梯度消失、爆炸，其根本原因在于反向传播训练法则，属于先天不足。</p> 
<h3>2.2、激活函数</h3> 
<p style="text-indent:33px;">以下图的反向传播为例（假设每一层只有一个神经元且对于每一层：</p> 
<p style="text-align:center;"><img alt="" height="34" src="https://images2.imgbox.com/3f/07/xHMhhZwc_o.png" width="338"></p> 
<p style="text-indent:33px;">偏置b可以推导出：</p> 
<p style="text-align:center;"><img alt="" height="86" src="https://images2.imgbox.com/dc/3b/HwinwttX_o.png" width="438"></p> 
<p style="text-indent:33px;">而sigmoid的导数为：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/fd/ee/Q78g0Klr_o.png"></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/c9/38/XyA4cq73_o.png"></p> 
<p style="text-indent:33px;"> 同理，使用tanh作为损失函数，它的导数图如下，可以看出，tanh比sigmoid要好一些，但是它的倒数仍然是小于1的。tanh数学表达为：</p> 
<p style="text-align:center;"><img alt="" height="40" src="https://images2.imgbox.com/b0/ab/zgFZrq2w_o.png" width="157"></p> 
<p style="text-align:center;"><img alt="" height="225" src="https://images2.imgbox.com/97/88/nzKvgJqH_o.jpg" width="511"></p> 
<p style="text-indent:33px;">如果接近输出层的激活函数求导后梯度值大于1，那么层数增多的时候，最终求出的梯度很容易指数级增长，就会产生梯度爆炸；相反，如果小于1，那么经过链式法则的连乘形式，也会很容易衰减至0，就会产生梯度消失。</p> 
<h3>2.3、初始化权值太大</h3> 
<p style="text-align:center;"><img alt="" height="108" src="https://images2.imgbox.com/4e/9c/Zhkz1DZ1_o.png" width="434"></p> 
<p style="text-indent:33px;">如上图所示，当：</p> 
<p style="text-align:center;"><img alt="" height="26" src="https://images2.imgbox.com/ec/22/hRnlraMn_o.png" width="118"></p> 
<p style="text-indent:33px;"> </p> 
<p style="text-indent:33px;">，也就是w比较大的情况。根据链式相乘(反向传播)可得，则前面的网络层比后面的网络层梯度变化更快，很容易发生梯度爆炸的问题。</p> 
<h2>3、总结</h2> 
<p style="text-indent:33px;">深层网络出现梯度消失或爆炸，主要是由于链式求导发现传播引起。参数的更新为w←w+Δw，给定学习率α，得出：</p> 
<p style="text-align:center;"><img alt="" height="29" src="https://images2.imgbox.com/56/00/68DRrvwN_o.png" width="113"></p> 
<p style="text-indent:33px;">激活函数的导数、网络初值(w,b)连续相乘表现为w的更新量。避免网络不work的过程就是调整这部分连乘结果，使其保持在1附近。学习率决定的网络学习的快慢，过大或过小也会直接影响网络的参数更新过程。</p> 
<p style="text-indent:0;"><strong>参考文章：</strong></p> 
<p style="text-indent:33px;">1、<a href="https://zhuanlan.zhihu.com/p/25631496" rel="nofollow">https://zhuanlan.zhihu.com/p/25631496</a></p> 
<p style="text-indent:33px;">2、<a href="https://zhuanlan.zhihu.com/p/72589432" rel="nofollow">https://zhuanlan.zhihu.com/p/72589432</a></p> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/516a077f046088508390ce93bf13039c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">解决Windows 10输入法无法安装报0x800F0954的问题</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/3bb18c4144beb9bc76496d6ea6383f6d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">看完让你彻底理解 WebSocket 原理</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>