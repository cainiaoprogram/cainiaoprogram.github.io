<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【GPU高性能编程 CUDA实战】学习笔记 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【GPU高性能编程 CUDA实战】学习笔记" />
<meta property="og:description" content="CUDA By Example - an Introduction to General-Purpose GPU Programming 第1章 为什么需要CUDA第2章 入门第3章 CUDA C第4章 CUDA C并行编程第5章 线程协作第6章 常量内存与事件第7章 纹理内存第8章 图形互操作性第9章 原子性第10章 流第11章 多GPU系统上的CUDA C第12章 后记附录 高级原子操作 第1章 为什么需要CUDA 在2000年早期，GPU的主要目标都是通过可编程计算单元为屏幕上的每个像素计算出一个颜色值，这些计算单元也称为像素着色器（Pixel Shader）。
第2章 入门 开发环境：
支持CUDA的图形处理器NVIDIA设备驱动程序CUDA开发工具箱标准C编译器 第3章 CUDA C 了解为**主机（Host）编写的代码与为设备（Device）**编写的代码之间的区别。
如何从主机上运行设备代码
了解如何在支持CUDA的设备上使用设备内存
一个空的函数kernel()，并且带有修饰符_global_。
对这个空函数的调用，并且带有修饰符&lt;&lt;&lt;1,1&gt;&gt;&gt;。
可以像调用C函数那样将参数传递给核函数
当设备执行任何有用的操作时，都需要分配内存，例如将计算值返回给主机。
设备信息，被课程全在NVIDIA 750Ti下测试。
--- General Information for device 0 --- Name: NVIDIA GeForce GTX 750 Ti Compute capability: 5.0 Clock rate: 1084500 Device copy overlap: Enabled Kernel execution timeout : Enabled --- Memory Information for device 0 --- Total global mem: 2097414144 Total constant Mem: 65536 Max mem pitch: 2147483647 Texture Alignment: 512 --- MP Information for device 0 --- Multiprocessor count: 5 Shared mem per mp: 49152 Registers per mp: 65536 Threads in warp: 32 Max threads per block: 1024 Max thread dimensions: (1024, 1024, 64) Max grid dimensions: (2147483647, 65535, 65535) 第4章 CUDA C并行编程 &lt;&lt;&lt;[线程块数量], [每个线程块中线程数量]&gt;&gt;&gt;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/b42913730a01d25a7f8ae3d8ed7915ec/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-12-14T12:11:04+08:00" />
<meta property="article:modified_time" content="2022-12-14T12:11:04+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【GPU高性能编程 CUDA实战】学习笔记</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>CUDA By Example - an Introduction to General-Purpose GPU Programming</h4> 
 <ul><li><a href="#1_CUDA_3" rel="nofollow">第1章 为什么需要CUDA</a></li><li><a href="#2__9" rel="nofollow">第2章 入门</a></li><li><a href="#3_CUDA_C_20" rel="nofollow">第3章 CUDA C</a></li><li><a href="#4_CUDA_C_59" rel="nofollow">第4章 CUDA C并行编程</a></li><li><a href="#5__73" rel="nofollow">第5章 线程协作</a></li><li><a href="#6__186" rel="nofollow">第6章 常量内存与事件</a></li><li><a href="#7__241" rel="nofollow">第7章 纹理内存</a></li><li><a href="#8__350" rel="nofollow">第8章 图形互操作性</a></li><li><a href="#9__360" rel="nofollow">第9章 原子性</a></li><li><a href="#10__384" rel="nofollow">第10章 流</a></li><li><a href="#11_GPUCUDA_C_520" rel="nofollow">第11章 多GPU系统上的CUDA C</a></li><li><a href="#12__571" rel="nofollow">第12章 后记</a></li><li><a href="#__622" rel="nofollow">附录 高级原子操作</a></li></ul> 
</div> 
<p></p> 
<h2><a id="1_CUDA_3"></a>第1章 为什么需要CUDA</h2> 
<p>在2000年早期，GPU的主要目标都是通过可编程计算单元为屏幕上的每个像素计算出一个颜色值，这些计算单元也称为<strong>像素着色器（Pixel Shader）</strong>。</p> 
<h2><a id="2__9"></a>第2章 入门</h2> 
<p>开发环境：</p> 
<ul><li>支持CUDA的图形处理器</li><li>NVIDIA设备驱动程序</li><li>CUDA开发工具箱</li><li>标准C编译器</li></ul> 
<h2><a id="3_CUDA_C_20"></a>第3章 CUDA C</h2> 
<ul><li> <p>了解为**主机（Host）<strong>编写的代码与为</strong>设备（Device）**编写的代码之间的区别。</p> </li><li> <p>如何从主机上运行设备代码</p> </li><li> <p>了解如何在支持CUDA的设备上使用设备内存</p> </li><li> <p>一个空的函数kernel()，并且带有修饰符_<em>global</em>_。</p> </li><li> <p>对这个空函数的调用，并且带有修饰符&lt;&lt;&lt;1,1&gt;&gt;&gt;。</p> </li><li> <p>可以像调用C函数那样将参数传递给核函数</p> </li><li> <p>当设备执行任何有用的操作时，都需要分配内存，例如将计算值返回给主机。</p> </li></ul> 
<p>设备信息，被课程全在NVIDIA 750Ti下测试。</p> 
<pre><code>   --- General Information for device 0 ---
Name:  NVIDIA GeForce GTX 750 Ti
Compute capability:  5.0
Clock rate:  1084500
Device copy overlap:  Enabled
Kernel execution timeout :  Enabled
   --- Memory Information for device 0 ---
Total global mem:  2097414144
Total constant Mem:  65536
Max mem pitch:  2147483647
Texture Alignment:  512
   --- MP Information for device 0 ---
Multiprocessor count:  5
Shared mem per mp:  49152
Registers per mp:  65536
Threads in warp:  32
Max threads per block:  1024
Max thread dimensions:  (1024, 1024, 64)
Max grid dimensions:  (2147483647, 65535, 65535)
</code></pre> 
<h2><a id="4_CUDA_C_59"></a>第4章 CUDA C并行编程</h2> 
<p>&lt;&lt;&lt;[线程块数量], [每个线程块中线程数量]&gt;&gt;&gt;</p> 
<p><strong>blockIdx</strong>，一个内置变量，描述线程块的编号，即上面三尖括号中的第一个参数</p> 
<p><strong>gridDim</strong>，也是内置变量</p> 
<p><strong>__global__</strong>，启动，</p> 
<p><strong>__device__</strong>，在设备（GPU）上运行的代码，只能从其他__device__或者__global__函数调用它们</p> 
<h2><a id="5__73"></a>第5章 线程协作</h2> 
<ul><li>了解CUDA C中的线程</li><li>了解不同线程之间的通信机制</li><li>了解并行执行线程的同步机制</li></ul> 
<p>&lt;&lt;&lt;[线程块数量], [每个线程块中线程数量]&gt;&gt;&gt;</p> 
<p><strong>CUDA程序中如何计算线程号：</strong></p> 
<p><code>[线程块数量] dim3 dimGrid([], [])</code></p> 
<p><code>[每个线程块中线程数量] dim3 dimBlock([], [])</code></p> 
<p><strong>线程块用<code>blockIdx</code>标识，并且是列优先的；线程块中的线程用<code>threadIdx</code>标识，也是列优先的。</strong></p> 
<p><strong>线程块的维度用<code>gridDim</code>标识，单个线程块内线程的维度用<code>blockDim</code>标识。</strong></p> 
<ol><li> <p>使用N个线程块，每个线程块只有一个线程，即</p> <pre><code class="prism language-cpp">dim3 <span class="token function">dimGrid</span><span class="token punctuation">(</span>N<span class="token punctuation">)</span><span class="token punctuation">;</span>
dim3 <span class="token function">dimBlock</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
threadID <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>	<span class="token comment">// 找到线程块，就是找到了线程的编号</span>
</code></pre> </li><li> <p>使用<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           M 
          
         
           × 
          
         
           N 
          
         
        
          M\times N 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.76666em; vertical-align: -0.08333em;"></span><span class="mord mathdefault" style="margin-right: 0.10903em;">M</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.10903em;">N</span></span></span></span></span>个线程块，每个线程块一个线程，显然是<strong>列优先</strong></p> <pre><code class="prism language-cpp">dim3 <span class="token function">dimGrid</span><span class="token punctuation">(</span>M<span class="token punctuation">,</span> N<span class="token punctuation">)</span><span class="token punctuation">;</span>
dim3 <span class="token function">dimBlock</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
threadID <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>y <span class="token operator">*</span> gridDim<span class="token punctuation">.</span>x <span class="token operator">+</span> blockIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>	<span class="token comment">// gridDim.x = M</span>
</code></pre> </li><li> <p>使用一个线程块，该线程块具有N个线程</p> <pre><code class="prism language-cpp">dim3 <span class="token function">dimGrid</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
dim3 <span class="token function">dimBlock</span><span class="token punctuation">(</span>N<span class="token punctuation">)</span><span class="token punctuation">;</span>
threadID <span class="token operator">=</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
</code></pre> </li><li> <p>使用M个线程块，每个线程块内有N个线程，</p> <pre><code class="prism language-cpp">dim3 <span class="token function">dimGrid</span><span class="token punctuation">(</span>M<span class="token punctuation">)</span><span class="token punctuation">;</span>
dim3 <span class="token function">dimBlock</span><span class="token punctuation">(</span>N<span class="token punctuation">)</span><span class="token punctuation">;</span>
threadID <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>	<span class="token comment">// blockDim.x = N</span>
</code></pre> </li><li> <p>使用<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           M 
          
         
           × 
          
         
           N 
          
         
        
          M \times N 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.76666em; vertical-align: -0.08333em;"></span><span class="mord mathdefault" style="margin-right: 0.10903em;">M</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.10903em;">N</span></span></span></span></span>的二维线程块，每个线程块中有<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           P 
          
         
           × 
          
         
           Q 
          
         
        
          P\times Q 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.76666em; vertical-align: -0.08333em;"></span><span class="mord mathdefault" style="margin-right: 0.13889em;">P</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.87777em; vertical-align: -0.19444em;"></span><span class="mord mathdefault">Q</span></span></span></span></span>个线程，索引有两个维度</p> <pre><code class="prism language-cpp">dim3 <span class="token function">dimGrid</span><span class="token punctuation">(</span>M<span class="token punctuation">,</span> N<span class="token punctuation">)</span><span class="token punctuation">;</span>
dim3 <span class="token function">dimBlock</span><span class="token punctuation">(</span>P<span class="token punctuation">,</span> Q<span class="token punctuation">)</span><span class="token punctuation">;</span>
blockID <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>y <span class="token operator">*</span> gridDim<span class="token punctuation">.</span>x <span class="token operator">+</span> blockIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>	<span class="token comment">// gridDim.x = M, 找到属于哪个线程块</span>
threadID <span class="token operator">=</span> threadIdx<span class="token punctuation">.</span>y <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>		<span class="token comment">// blockDim.x = P,找到在当前线程块中的编号</span>
idx <span class="token operator">=</span> blockID <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>y <span class="token operator">+</span> threadID<span class="token punctuation">;</span>
</code></pre> </li></ol> 
<p>总结，对于一个三维网格，和一个三维的线程块</p> 
<p>&lt;&lt;&lt;[dim3 grid], [dim3 block]&gt;&gt;&gt;</p> 
<p>总的线程数量<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         N 
        
       
      
        \rm N 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathrm">N</span></span></span></span></span></span>为：<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          N 
         
        
          = 
         
        
          g 
         
        
          r 
         
        
          i 
         
        
          d 
         
        
          D 
         
        
          i 
         
        
          m 
         
        
          . 
         
        
          x 
         
        
          × 
         
        
          g 
         
        
          r 
         
        
          i 
         
        
          d 
         
        
          D 
         
        
          i 
         
        
          m 
         
        
          . 
         
        
          y 
         
        
          × 
         
        
          g 
         
        
          r 
         
        
          i 
         
        
          d 
         
        
          D 
         
        
          i 
         
        
          m 
         
        
          . 
         
        
          z 
         
        
          × 
         
        
          b 
         
        
          l 
         
        
          o 
         
        
          c 
         
        
          k 
         
        
          D 
         
        
          i 
         
        
          m 
         
        
          . 
         
        
          x 
         
        
          × 
         
        
          b 
         
        
          l 
         
        
          o 
         
        
          c 
         
        
          k 
         
        
          D 
         
        
          i 
         
        
          m 
         
        
          . 
         
        
          y 
         
        
          × 
         
        
          b 
         
        
          l 
         
        
          o 
         
        
          c 
         
        
          k 
         
        
          D 
         
        
          i 
         
        
          m 
         
        
          . 
         
        
          z 
         
        
       
         \rm N = gridDim.x \times gridDim.y \times gridDim.z \times blockDim.x \times blockDim.y \times blockDim.z 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord"><span class="mord mathrm">N</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mord mathrm" style="margin-right: 0.01389em;">g</span><span class="mord mathrm">r</span><span class="mord mathrm">i</span><span class="mord mathrm">d</span><span class="mord mathrm">D</span><span class="mord mathrm">i</span><span class="mord mathrm">m</span><span class="mord mathrm">.</span><span class="mord mathrm">x</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord mathrm" style="margin-right: 0.01389em;">g</span><span class="mord mathrm">r</span><span class="mord mathrm">i</span><span class="mord mathrm">d</span><span class="mord mathrm">D</span><span class="mord mathrm">i</span><span class="mord mathrm">m</span><span class="mord mathrm">.</span><span class="mord mathrm" style="margin-right: 0.01389em;">y</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord mathrm" style="margin-right: 0.01389em;">g</span><span class="mord mathrm">r</span><span class="mord mathrm">i</span><span class="mord mathrm">d</span><span class="mord mathrm">D</span><span class="mord mathrm">i</span><span class="mord mathrm">m</span><span class="mord mathrm">.</span><span class="mord mathrm">z</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord mathrm">b</span><span class="mord mathrm">l</span><span class="mord mathrm">o</span><span class="mord mathrm">c</span><span class="mord mathrm">k</span><span class="mord mathrm">D</span><span class="mord mathrm">i</span><span class="mord mathrm">m</span><span class="mord mathrm">.</span><span class="mord mathrm">x</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord mathrm">b</span><span class="mord mathrm">l</span><span class="mord mathrm">o</span><span class="mord mathrm">c</span><span class="mord mathrm">k</span><span class="mord mathrm">D</span><span class="mord mathrm">i</span><span class="mord mathrm">m</span><span class="mord mathrm">.</span><span class="mord mathrm" style="margin-right: 0.01389em;">y</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord mathrm">b</span><span class="mord mathrm">l</span><span class="mord mathrm">o</span><span class="mord mathrm">c</span><span class="mord mathrm">k</span><span class="mord mathrm">D</span><span class="mord mathrm">i</span><span class="mord mathrm">m</span><span class="mord mathrm">.</span><span class="mord mathrm">z</span></span></span></span></span></span></span><br> 线程标号按照下面的顺序求：</p> 
<ol><li> <p>先找到当前线程位于那一个线程块中<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
        
         
          
          
            b 
           
          
            l 
           
          
            o 
           
          
            c 
           
          
            k 
           
          
            I 
           
          
            D 
           
          
            = 
           
          
            b 
           
          
            l 
           
          
            o 
           
          
            c 
           
          
            k 
           
          
            I 
           
          
            d 
           
          
            x 
           
          
            . 
           
          
            x 
           
          
            + 
           
          
            b 
           
          
            l 
           
          
            o 
           
          
            c 
           
          
            k 
           
          
            I 
           
          
            d 
           
          
            x 
           
          
            . 
           
          
            y 
           
          
            × 
           
          
            g 
           
          
            r 
           
          
            i 
           
          
            d 
           
          
            D 
           
          
            i 
           
          
            m 
           
          
            . 
           
          
            x 
           
          
            + 
           
          
            b 
           
          
            l 
           
          
            o 
           
          
            c 
           
          
            k 
           
          
            I 
           
          
            d 
           
          
            x 
           
          
            . 
           
          
            z 
           
          
            × 
           
          
            g 
           
          
            r 
           
          
            i 
           
          
            d 
           
          
            D 
           
          
            i 
           
          
            m 
           
          
            . 
           
          
            x 
           
          
            × 
           
          
            g 
           
          
            r 
           
          
            i 
           
          
            d 
           
          
            D 
           
          
            i 
           
          
            m 
           
          
            . 
           
          
            y 
           
          
         
           \rm blockID = blockIdx.x + blockIdx.y \times gridDim.x + blockIdx.z \times gridDim.x \times gridDim.y 
          
         
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord"><span class="mord mathrm">b</span><span class="mord mathrm">l</span><span class="mord mathrm">o</span><span class="mord mathrm">c</span><span class="mord mathrm">k</span><span class="mord mathrm">I</span><span class="mord mathrm">D</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mord mathrm">b</span><span class="mord mathrm">l</span><span class="mord mathrm">o</span><span class="mord mathrm">c</span><span class="mord mathrm">k</span><span class="mord mathrm">I</span><span class="mord mathrm">d</span><span class="mord mathrm">x</span><span class="mord mathrm">.</span><span class="mord mathrm">x</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord mathrm">b</span><span class="mord mathrm">l</span><span class="mord mathrm">o</span><span class="mord mathrm">c</span><span class="mord mathrm">k</span><span class="mord mathrm">I</span><span class="mord mathrm">d</span><span class="mord mathrm">x</span><span class="mord mathrm">.</span><span class="mord mathrm" style="margin-right: 0.01389em;">y</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord mathrm" style="margin-right: 0.01389em;">g</span><span class="mord mathrm">r</span><span class="mord mathrm">i</span><span class="mord mathrm">d</span><span class="mord mathrm">D</span><span class="mord mathrm">i</span><span class="mord mathrm">m</span><span class="mord mathrm">.</span><span class="mord mathrm">x</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord mathrm">b</span><span class="mord mathrm">l</span><span class="mord mathrm">o</span><span class="mord mathrm">c</span><span class="mord mathrm">k</span><span class="mord mathrm">I</span><span class="mord mathrm">d</span><span class="mord mathrm">x</span><span class="mord mathrm">.</span><span class="mord mathrm">z</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord mathrm" style="margin-right: 0.01389em;">g</span><span class="mord mathrm">r</span><span class="mord mathrm">i</span><span class="mord mathrm">d</span><span class="mord mathrm">D</span><span class="mord mathrm">i</span><span class="mord mathrm">m</span><span class="mord mathrm">.</span><span class="mord mathrm">x</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord mathrm" style="margin-right: 0.01389em;">g</span><span class="mord mathrm">r</span><span class="mord mathrm">i</span><span class="mord mathrm">d</span><span class="mord mathrm">D</span><span class="mord mathrm">i</span><span class="mord mathrm">m</span><span class="mord mathrm">.</span><span class="mord mathrm" style="margin-right: 0.01389em;">y</span></span></span></span></span></span></span></p> </li><li> <p>找到当前线程位于当前线程块中的位置<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
        
         
          
          
            t 
           
          
            h 
           
          
            r 
           
          
            e 
           
          
            a 
           
          
            d 
           
          
            I 
           
          
            D 
           
          
            = 
           
          
            t 
           
          
            h 
           
          
            r 
           
          
            e 
           
          
            a 
           
          
            d 
           
          
            I 
           
          
            d 
           
          
            x 
           
          
            . 
           
          
            x 
           
          
            + 
           
          
            t 
           
          
            h 
           
          
            r 
           
          
            e 
           
          
            a 
           
          
            d 
           
          
            I 
           
          
            d 
           
          
            x 
           
          
            . 
           
          
            y 
           
          
            × 
           
          
            b 
           
          
            l 
           
          
            o 
           
          
            c 
           
          
            k 
           
          
            D 
           
          
            i 
           
          
            m 
           
          
            . 
           
          
            x 
           
          
            + 
           
          
            t 
           
          
            h 
           
          
            r 
           
          
            e 
           
          
            a 
           
          
            d 
           
          
            I 
           
          
            d 
           
          
            x 
           
          
            . 
           
          
            z 
           
          
            × 
           
          
            b 
           
          
            l 
           
          
            o 
           
          
            c 
           
          
            k 
           
          
            D 
           
          
            i 
           
          
            m 
           
          
            . 
           
          
            x 
           
          
            × 
           
          
            b 
           
          
            l 
           
          
            o 
           
          
            c 
           
          
            k 
           
          
            D 
           
          
            i 
           
          
            m 
           
          
            . 
           
          
            y 
           
          
         
           \rm threadID = threadIdx.x + threadIdx.y \times blockDim.x + threadIdx.z \times blockDim.x \times blockDim.y 
          
         
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord"><span class="mord mathrm">t</span><span class="mord mathrm">h</span><span class="mord mathrm">r</span><span class="mord mathrm">e</span><span class="mord mathrm">a</span><span class="mord mathrm">d</span><span class="mord mathrm">I</span><span class="mord mathrm">D</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mord mathrm">t</span><span class="mord mathrm">h</span><span class="mord mathrm">r</span><span class="mord mathrm">e</span><span class="mord mathrm">a</span><span class="mord mathrm">d</span><span class="mord mathrm">I</span><span class="mord mathrm">d</span><span class="mord mathrm">x</span><span class="mord mathrm">.</span><span class="mord mathrm">x</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord mathrm">t</span><span class="mord mathrm">h</span><span class="mord mathrm">r</span><span class="mord mathrm">e</span><span class="mord mathrm">a</span><span class="mord mathrm">d</span><span class="mord mathrm">I</span><span class="mord mathrm">d</span><span class="mord mathrm">x</span><span class="mord mathrm">.</span><span class="mord mathrm" style="margin-right: 0.01389em;">y</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord mathrm">b</span><span class="mord mathrm">l</span><span class="mord mathrm">o</span><span class="mord mathrm">c</span><span class="mord mathrm">k</span><span class="mord mathrm">D</span><span class="mord mathrm">i</span><span class="mord mathrm">m</span><span class="mord mathrm">.</span><span class="mord mathrm">x</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord mathrm">t</span><span class="mord mathrm">h</span><span class="mord mathrm">r</span><span class="mord mathrm">e</span><span class="mord mathrm">a</span><span class="mord mathrm">d</span><span class="mord mathrm">I</span><span class="mord mathrm">d</span><span class="mord mathrm">x</span><span class="mord mathrm">.</span><span class="mord mathrm">z</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord mathrm">b</span><span class="mord mathrm">l</span><span class="mord mathrm">o</span><span class="mord mathrm">c</span><span class="mord mathrm">k</span><span class="mord mathrm">D</span><span class="mord mathrm">i</span><span class="mord mathrm">m</span><span class="mord mathrm">.</span><span class="mord mathrm">x</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord mathrm">b</span><span class="mord mathrm">l</span><span class="mord mathrm">o</span><span class="mord mathrm">c</span><span class="mord mathrm">k</span><span class="mord mathrm">D</span><span class="mord mathrm">i</span><span class="mord mathrm">m</span><span class="mord mathrm">.</span><span class="mord mathrm" style="margin-right: 0.01389em;">y</span></span></span></span></span></span></span></p> </li><li> <p>计算一个线程块中一共有多少线程<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
        
         
          
          
            M 
           
          
            = 
           
          
            b 
           
          
            l 
           
          
            o 
           
          
            c 
           
          
            k 
           
          
            D 
           
          
            i 
           
          
            m 
           
          
            . 
           
          
            x 
           
          
            × 
           
          
            b 
           
          
            l 
           
          
            o 
           
          
            c 
           
          
            k 
           
          
            D 
           
          
            i 
           
          
            m 
           
          
            . 
           
          
            y 
           
          
            × 
           
          
            b 
           
          
            l 
           
          
            o 
           
          
            c 
           
          
            k 
           
          
            D 
           
          
            i 
           
          
            m 
           
          
            . 
           
          
            z 
           
          
         
           \rm M = blockDim.x \times blockDim.y \times blockDim.z 
          
         
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord"><span class="mord mathrm">M</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mord mathrm">b</span><span class="mord mathrm">l</span><span class="mord mathrm">o</span><span class="mord mathrm">c</span><span class="mord mathrm">k</span><span class="mord mathrm">D</span><span class="mord mathrm">i</span><span class="mord mathrm">m</span><span class="mord mathrm">.</span><span class="mord mathrm">x</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord mathrm">b</span><span class="mord mathrm">l</span><span class="mord mathrm">o</span><span class="mord mathrm">c</span><span class="mord mathrm">k</span><span class="mord mathrm">D</span><span class="mord mathrm">i</span><span class="mord mathrm">m</span><span class="mord mathrm">.</span><span class="mord mathrm" style="margin-right: 0.01389em;">y</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord mathrm">b</span><span class="mord mathrm">l</span><span class="mord mathrm">o</span><span class="mord mathrm">c</span><span class="mord mathrm">k</span><span class="mord mathrm">D</span><span class="mord mathrm">i</span><span class="mord mathrm">m</span><span class="mord mathrm">.</span><span class="mord mathrm">z</span></span></span></span></span></span></span></p> </li><li> <p>求得当前的线程序列号idx<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
        
         
          
          
            i 
           
          
            d 
           
          
            x 
           
          
            = 
           
          
            t 
           
          
            h 
           
          
            r 
           
          
            e 
           
          
            a 
           
          
            d 
           
          
            I 
           
          
            D 
           
          
            + 
           
          
            M 
           
          
            × 
           
          
            b 
           
          
            l 
           
          
            o 
           
          
            c 
           
          
            k 
           
          
            I 
           
          
            D 
           
          
         
           \rm idx = threadID + M \times blockID 
          
         
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.77777em; vertical-align: -0.08333em;"></span><span class="mord"><span class="mord mathrm">i</span><span class="mord mathrm">d</span><span class="mord mathrm">x</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mord mathrm">t</span><span class="mord mathrm">h</span><span class="mord mathrm">r</span><span class="mord mathrm">e</span><span class="mord mathrm">a</span><span class="mord mathrm">d</span><span class="mord mathrm">I</span><span class="mord mathrm">D</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord mathrm">M</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord mathrm">b</span><span class="mord mathrm">l</span><span class="mord mathrm">o</span><span class="mord mathrm">c</span><span class="mord mathrm">k</span><span class="mord mathrm">I</span><span class="mord mathrm">D</span></span></span></span></span></span></span></p> </li></ol> 
<p><strong><code>gridDim, blockDim, blockIdx, threadIdx</code>都是内置常量，一旦kernel启动，他们就是确定的了</strong></p> 
<p><strong>__shared__</strong> 声明一个驻留在共享内存中的变量，<strong>注意</strong>，是线程块中的线程共享，不同线程块不能共享。</p> 
<pre><code class="prism language-cpp"><span class="token comment">// 声明一个驻留在共享内存中的变量，注意，是线程块中的线程共享，不同线程块不能共享。</span>
__shared__ <span class="token keyword">float</span> cache<span class="token punctuation">[</span>threadPerBlock<span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token comment">// 对线程块中的线程进行同步</span>
<span class="token function">__syncthreads</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<p><strong>__syncthreads()</strong>; 也和MPI的**MPI_Barrier()**一样，需要组内所有的线程（进程）都执行到这条指令，才会继续运行，所以要谨慎的将其加入条件语句中，最好是不加。</p> 
<p>对一个输入数组执行某种计算，然后产生一个更小的结果数组，这种过程也称为<strong>归约（Reduction）</strong>。</p> 
<h2><a id="6__186"></a>第6章 常量内存与事件</h2> 
<p><strong>常量内存（Constant Memory）</strong></p> 
<p>通过事件来测量CUDA应用程序的性能。通过这些测量放，你可以定量地分析对应用程序某个修改是否会带来性能提升（或者性能下降）</p> 
<ul><li>了解如何在CUDA C中使用常量内存</li><li>了解常量内存的性能特性</li><li>学习如何使用CUDA事件来测量应用程序的性能。</li></ul> 
<p><strong>常量内存（Constant Memory） __constant__</strong></p> 
<ul><li> <p>申请的是device内存，<strong><code>cudaMemcpyToSymbol</code><strong>拷贝就是从host拷贝到</strong>global memory</strong>。</p> </li><li> <p>申请的是constant内存，<strong><code>cudaMemcpyToSymbol</code><strong>拷贝就是从host拷贝到</strong>constant memory</strong>。</p> </li></ul> 
<p>与从全局内存中读取数据相比，从常量内存中读取相同的数据可以节约内存带宽，主要有两个方面：</p> 
<ul><li>对常量内存的单词读操作可以广播到其他了“邻近（Nearby）”线程，将节约15次读取操作。</li><li>常量内存的数据将缓存起来，因此对相同地址的连续读操作将不会产生额外的内存通信量。</li></ul> 
<p><strong>线程束warp</strong>，Warp可以看成是一组线程通过交织而形成的一个整体。在CUDA架构中，Warp是指一个包含32个线程的集合，这个线程集合被“编织在一起”并且以“步调一致（Lockstep）“的形式执行。在程序的每一行，Warp中的每个线程都将在不同的数据上执行相同的指令。</p> 
<p>当处理常量内存时，NVIDIA硬件把单次内存读取操作广播到每个<strong>半线程束（Half-Warp）</strong>。在半线程束中包含了16个线程，即线程束中线程数量的一半。如果在半线程束中的每个线程都从常量内存的相同地址上读取数据，那么GPU只会产生一次读取请求并在随后将数据广播到每个线程。如果从常量内存中读取大量的数据，那么这种方法产出的内存流量只是使用全局内存时的<strong>1/16（大约6%）</strong>。</p> 
<p>由于这块内存的内容不会改变，因此硬件将主动将这个常量数据缓存在GPU上。因此只有第一次读取会产生内存流量，后面都会命中缓存。这将进一步减少额外的内存流量。</p> 
<p><strong>负面影响</strong>：当半线程束同时读相同地址时，这个功能可以极大提升性能，但是当所有16个线程分别读取不同的地址时，他实际上会<strong>降低性能</strong></p> 
<p><strong>使用事件来测量性能</strong></p> 
<p>为了测量GPU在某个任务上花费的时间，我们将使用CUDA的事件API。</p> 
<p>CUDA中的事件本质上是一个<strong>GPU时间戳</strong>。获得时间戳的<strong>两个步骤</strong>：首先创建一个事件，然后记录一个事件。</p> 
<pre><code class="prism language-cpp">cudaEvent_t start<span class="token punctuation">,</span> stop<span class="token punctuation">;</span>
<span class="token function">cudaEventCreate</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>start<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">cudaEventCreate</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>stop<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">cudaEventRecord</span><span class="token punctuation">(</span>start<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>		<span class="token comment">// 第二个参数在讨论Stream（流）时再解释</span>
<span class="token comment">// 在GPU上执行一些工作</span>
<span class="token function">cudaEventRecord</span><span class="token punctuation">(</span>stop<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">cudaEventSynchronize</span><span class="token punctuation">(</span>stop<span class="token punctuation">)</span><span class="token punctuation">;</span>	<span class="token comment">// 运行时，阻塞之后的语句，知道GPU执行stop事件。当返回时，我们知道stop事件之前的所有GPU工作都已完成，可以读stop中保存的值了。</span>
<span class="token keyword">float</span> elapsedTime<span class="token punctuation">;</span>	<span class="token comment">// 单位 ms</span>
<span class="token function">cudaEventElapsedTime</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>elapsedTime<span class="token punctuation">,</span> start<span class="token punctuation">,</span> stop<span class="token punctuation">)</span><span class="token punctuation">;</span>	
<span class="token function">cudaEventDestroy</span><span class="token punctuation">(</span>start<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">cudaEventDestroy</span><span class="token punctuation">(</span>stop<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<p>值得注意的是，由于CUDA事件是直接在GPU上实现的，因此他们不适用于对同时包含设备代码和主机代码的混合代码的计时。也就是说，如果试图通过CUDA事件对核函数和设备内存复制之外的代码进行计时，将得到不可靠的结果。</p> 
<h2><a id="7__241"></a>第7章 纹理内存</h2> 
<p><strong>纹理内存（Texture Memory）</strong>，与常量内存一样，是另一种类型的制度内存，在特定的访问模式中，纹理内存同样能够提升性能并减少内存流量。</p> 
<ul><li>了解纹理内存的性能特性</li><li>了解如何在CUDA C中使用一维、二维纹理内存</li></ul> 
<p>纹理缓存是专门为那些在内存访问模式中存在大量空间局部性（Spatial Locality）的图形应用程序而设计的。</p> 
<p>首先，将输入的数据声明为<code>texture</code>类型的引用。</p> 
<pre><code class="prism language-cpp">texture<span class="token operator">&lt;</span><span class="token keyword">float</span><span class="token operator">&gt;</span> texConstSrc<span class="token punctuation">;</span>
texture<span class="token operator">&lt;</span><span class="token keyword">float</span><span class="token operator">&gt;</span> texIn<span class="token punctuation">;</span>
texture<span class="token operator">&lt;</span><span class="token keyword">float</span><span class="token operator">&gt;</span> texOut<span class="token punctuation">;</span>
</code></pre> 
<p>在为这三个缓冲区分配了GPU内存后，需要通过<code>cudaBindTexture()</code>将这些变量绑定到内存缓冲区，这等于告诉CUDA运行时两件事：</p> 
<ul><li>我们希望将指定的缓冲区作为纹理来使用</li><li>我们希望将纹理引用作为纹理的名字</li></ul> 
<pre><code class="prism language-cpp"><span class="token function">HANDLE_ERROR</span><span class="token punctuation">(</span> <span class="token function">cudaBindTexture</span><span class="token punctuation">(</span> <span class="token constant">NULL</span><span class="token punctuation">,</span> texConstSrc<span class="token punctuation">,</span>
                               data<span class="token punctuation">.</span>dev_constSrc<span class="token punctuation">,</span>
                               imageSize <span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<p><code>tex1Dfetch()</code>，两个参数，第一参数表明缓存位置，第二参数表明取缓存中的哪个元素。这个虽然看上去像是一个函数，但它其实是<strong>编译器内置函数(Intrinsic)</strong>。由于纹理引用必须声明为文件作用域内的全局变量，因此不再将输入缓冲区和输出缓存区作为参数传递给<code>blend_kernel()</code>，<strong>因为编译器需要在编译时知道<code>tex1Dfetch()</code>应该对哪些纹理采样。</strong></p> 
<pre><code class="prism language-cpp"><span class="token comment">// this kernel takes in a 2-d array of floats</span>
<span class="token comment">// it updates the value-of-interest by a scaled value based</span>
<span class="token comment">// on itself and its nearest neighbors</span>
__global__ <span class="token keyword">void</span> <span class="token function">blend_kernel</span><span class="token punctuation">(</span> <span class="token keyword">float</span> <span class="token operator">*</span>dst<span class="token punctuation">,</span>
                              <span class="token keyword">bool</span> dstOut <span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token comment">// map from threadIdx/BlockIdx to pixel position</span>
    <span class="token keyword">int</span> x <span class="token operator">=</span> threadIdx<span class="token punctuation">.</span>x <span class="token operator">+</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
    <span class="token keyword">int</span> y <span class="token operator">=</span> threadIdx<span class="token punctuation">.</span>y <span class="token operator">+</span> blockIdx<span class="token punctuation">.</span>y <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>y<span class="token punctuation">;</span>
    <span class="token keyword">int</span> offset <span class="token operator">=</span> x <span class="token operator">+</span> y <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">*</span> gridDim<span class="token punctuation">.</span>x<span class="token punctuation">;</span>

    <span class="token keyword">int</span> left <span class="token operator">=</span> offset <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span>
    <span class="token keyword">int</span> right <span class="token operator">=</span> offset <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>x <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span>   left<span class="token operator">++</span><span class="token punctuation">;</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>x <span class="token operator">==</span> DIM<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> right<span class="token operator">--</span><span class="token punctuation">;</span> 

    <span class="token keyword">int</span> top <span class="token operator">=</span> offset <span class="token operator">-</span> DIM<span class="token punctuation">;</span>
    <span class="token keyword">int</span> bottom <span class="token operator">=</span> offset <span class="token operator">+</span> DIM<span class="token punctuation">;</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>y <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span>   top <span class="token operator">+=</span> DIM<span class="token punctuation">;</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>y <span class="token operator">==</span> DIM<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> bottom <span class="token operator">-=</span> DIM<span class="token punctuation">;</span>

    <span class="token keyword">float</span>   t<span class="token punctuation">,</span> l<span class="token punctuation">,</span> c<span class="token punctuation">,</span> r<span class="token punctuation">,</span> b<span class="token punctuation">;</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>dstOut<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        t <span class="token operator">=</span> <span class="token function">tex1Dfetch</span><span class="token punctuation">(</span>texIn<span class="token punctuation">,</span>top<span class="token punctuation">)</span><span class="token punctuation">;</span>
        l <span class="token operator">=</span> <span class="token function">tex1Dfetch</span><span class="token punctuation">(</span>texIn<span class="token punctuation">,</span>left<span class="token punctuation">)</span><span class="token punctuation">;</span>
        c <span class="token operator">=</span> <span class="token function">tex1Dfetch</span><span class="token punctuation">(</span>texIn<span class="token punctuation">,</span>offset<span class="token punctuation">)</span><span class="token punctuation">;</span>
        r <span class="token operator">=</span> <span class="token function">tex1Dfetch</span><span class="token punctuation">(</span>texIn<span class="token punctuation">,</span>right<span class="token punctuation">)</span><span class="token punctuation">;</span>
        b <span class="token operator">=</span> <span class="token function">tex1Dfetch</span><span class="token punctuation">(</span>texIn<span class="token punctuation">,</span>bottom<span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{<!-- --></span>
        t <span class="token operator">=</span> <span class="token function">tex1Dfetch</span><span class="token punctuation">(</span>texOut<span class="token punctuation">,</span>top<span class="token punctuation">)</span><span class="token punctuation">;</span>
        l <span class="token operator">=</span> <span class="token function">tex1Dfetch</span><span class="token punctuation">(</span>texOut<span class="token punctuation">,</span>left<span class="token punctuation">)</span><span class="token punctuation">;</span>
        c <span class="token operator">=</span> <span class="token function">tex1Dfetch</span><span class="token punctuation">(</span>texOut<span class="token punctuation">,</span>offset<span class="token punctuation">)</span><span class="token punctuation">;</span>
        r <span class="token operator">=</span> <span class="token function">tex1Dfetch</span><span class="token punctuation">(</span>texOut<span class="token punctuation">,</span>right<span class="token punctuation">)</span><span class="token punctuation">;</span>
        b <span class="token operator">=</span> <span class="token function">tex1Dfetch</span><span class="token punctuation">(</span>texOut<span class="token punctuation">,</span>bottom<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    dst<span class="token punctuation">[</span>offset<span class="token punctuation">]</span> <span class="token operator">=</span> c <span class="token operator">+</span> SPEED <span class="token operator">*</span> <span class="token punctuation">(</span>t <span class="token operator">+</span> b <span class="token operator">+</span> r <span class="token operator">+</span> l <span class="token operator">-</span> <span class="token number">4</span> <span class="token operator">*</span> c<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<p><strong>清理工作</strong>，不仅要释放全局缓冲区，还需清除与纹理的绑定</p> 
<pre><code class="prism language-cpp"><span class="token comment">// clean up memory allocated on the GPU</span>
<span class="token keyword">void</span> <span class="token function">anim_exit</span><span class="token punctuation">(</span> DataBlock <span class="token operator">*</span>d <span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token function">cudaUnbindTexture</span><span class="token punctuation">(</span> texIn <span class="token punctuation">)</span><span class="token punctuation">;</span>	 <span class="token comment">// 取消纹理的绑定</span>
    <span class="token function">cudaUnbindTexture</span><span class="token punctuation">(</span> texOut <span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaUnbindTexture</span><span class="token punctuation">(</span> texConstSrc <span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">HANDLE_ERROR</span><span class="token punctuation">(</span> <span class="token function">cudaFree</span><span class="token punctuation">(</span> d<span class="token operator">-&gt;</span>dev_inSrc <span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">HANDLE_ERROR</span><span class="token punctuation">(</span> <span class="token function">cudaFree</span><span class="token punctuation">(</span> d<span class="token operator">-&gt;</span>dev_outSrc <span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">HANDLE_ERROR</span><span class="token punctuation">(</span> <span class="token function">cudaFree</span><span class="token punctuation">(</span> d<span class="token operator">-&gt;</span>dev_constSrc <span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token function">HANDLE_ERROR</span><span class="token punctuation">(</span> <span class="token function">cudaEventDestroy</span><span class="token punctuation">(</span> d<span class="token operator">-&gt;</span>start <span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">HANDLE_ERROR</span><span class="token punctuation">(</span> <span class="token function">cudaEventDestroy</span><span class="token punctuation">(</span> d<span class="token operator">-&gt;</span>stop <span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<p><strong>二维纹理</strong></p> 
<p><code>tex2D()</code>，可以直接通过x，y坐标来访问纹理，而且不用担心溢出的问题，如果x小于0，那么返回0处的值，大于宽度，返回宽度处的值，y同理。</p> 
<p>绑定二维纹理时，CUDA运行时要求提供一个<code>cudaCreateChannelDesc</code>。通道格式描述符（Channel Format Description）的声明。</p> 
<pre><code class="prism language-cpp">cudaChannelFormatDesc desc <span class="token operator">=</span> <span class="token generic-function"><span class="token function">cudaCreateChannelDesc</span><span class="token generic class-name"><span class="token operator">&lt;</span><span class="token keyword">float</span><span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">HANDLE_ERROR</span><span class="token punctuation">(</span> <span class="token function">cudaBindTexture2D</span><span class="token punctuation">(</span> <span class="token constant">NULL</span><span class="token punctuation">,</span> texConstSrc<span class="token punctuation">,</span>
                                data<span class="token punctuation">.</span>dev_constSrc<span class="token punctuation">,</span>
                                desc<span class="token punctuation">,</span> DIM<span class="token punctuation">,</span> DIM<span class="token punctuation">,</span>	<span class="token comment">// 纹理维数</span>
                                <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span> <span class="token operator">*</span> DIM <span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<p>一维二维同样的函数取消绑定。</p> 
<h2><a id="8__350"></a>第8章 图形互操作性</h2> 
<p>能否在同一应用程序中GPU既执行渲染运算，又执行通用计算？如果要渲染的图像依赖通用计算的结果，那么该如何处理？或者，如果想要在已经渲染的帧上执行某种图像处理或者同级，又该如何实现？</p> 
<p>假设已经具备一些其他的技术背景知识，因为在示例代码中包含了大量的OpenGL和GLUT（OpenGL Utility Toolkit）代码，但是没有对其解释。</p> 
<p><strong>这章暂时搁置，里面用的图形库没用过，不懂，目前重点不在这。</strong></p> 
<h2><a id="9__360"></a>第9章 原子性</h2> 
<ul><li>了解不同NVIDIA GPU的计算功能集</li><li>了解原子操作以及为什么需要他们</li><li>了解如何在CUDA C核函数中执行带有原子操作的运算</li></ul> 
<ol><li>使用全局内存原子操作的直方图核函数</li></ol> 
<p><code>cudaMemset()</code>和<code>memset()</code>的行为基本相同。</p> 
<p>原子操作<code>atomicAdd( addr, y);</code>，包括读取addr处的值，将y加到这个值，然后结果保存回addr</p> 
<pre><code class="prism language-cpp"><span class="token function">atomicAdd</span><span class="token punctuation">(</span> <span class="token operator">&amp;</span><span class="token punctuation">(</span>histo<span class="token punctuation">[</span>buffer<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<p>由于在核函数中只包含了非常少的计算工作，因此很可能是全局内存上的原子操作导致了性能的降低。当数千个线程尝试访问少量的内存位置时，将发生大量的竞争。为了确保递增操作的原子性，对相同内存位置的操作都将被硬件串行化。这可能导致保存未完成操作的队列非常长，因此会抵消通过并行运行线程而获得的性能提升。</p> 
<ol start="2"><li>使用共享内存原子操作和全局内存原子操作的直方图核函数</li></ol> 
<p>也就是先共享内存算好一部分的，最后给加到全局的上面。</p> 
<h2><a id="10__384"></a>第10章 流</h2> 
<p>**任务并行性（Task Parallelism）**是指并行执行两个或多个不同的任务，而并不是在大量数据上执行同一个任务。</p> 
<ul><li>了解如何分配**页锁定（Page-Locked）**类型的主机内存</li><li>了解CUDA流的概念</li><li>了解如何使用CUDA流来加速应用程序</li></ul> 
<p><code>cudaHostAlloc()</code>分配页锁定的主机内存，页锁定内存也称为**固定内存（Pinned Memory）**或者不可分页内存，<strong>操作系统将不会对这块内存分页交换到磁盘上，从而确保了该内存始终驻留在物理内存中</strong>。因此，操作系统能够安全地使某个应用程序访问该内存的物理地址，因为这块内存将不会被破坏或者重新定位。</p> 
<p>由于GPU知道物理地址，那么可用DMA来复制数据。</p> 
<pre><code class="prism language-cpp"><span class="token function">cudaHostAlloc</span><span class="token punctuation">(</span> <span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>a<span class="token punctuation">,</span> size <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span> <span class="token operator">*</span>a <span class="token punctuation">)</span><span class="token punctuation">,</span> cudaHostAllocDefault <span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">cudaFreeHost</span><span class="token punctuation">(</span> a <span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment">// up 从主机到设备，	down 从设备到主机</span>
<span class="token comment">// 使用页锁定内存有四五倍的加速</span>
Time <span class="token keyword">using</span> cudaMalloc<span class="token operator">:</span>  <span class="token number">10072.8</span> ms
        MB<span class="token operator">/</span>s during copy up<span class="token operator">:</span>  <span class="token number">2541.5</span>
Time <span class="token keyword">using</span> cudaMalloc<span class="token operator">:</span>  <span class="token number">14715.3</span> ms
        MB<span class="token operator">/</span>s during copy down<span class="token operator">:</span>  <span class="token number">1739.7</span>
Time <span class="token keyword">using</span> cudaHostAlloc<span class="token operator">:</span>  <span class="token number">2323.9</span> ms
        MB<span class="token operator">/</span>s during copy up<span class="token operator">:</span>  <span class="token number">11016.1</span>
Time <span class="token keyword">using</span> cudaHostAlloc<span class="token operator">:</span>  <span class="token number">2041.3</span> ms
        MB<span class="token operator">/</span>s during copy down<span class="token operator">:</span>  <span class="token number">12541.0</span>
</code></pre> 
<p><code>cudaEventRecord(cudaEvent_t, stream)</code>第二个参数用于指定插入事件的<strong>流（Stream）</strong>。</p> 
<p>CUDA流在加速应用程序方面起着重要作用。CUDA流表示一个GPU操作队列，并且该队列中的操作将以指定的顺序执行。我们可以在流中添加一些操作，例如核函数启动、内存复制，以及事件的启动和结束等。将这些操作添加到流的顺讯也就是他们执行的顺序。你可以将每个流视为GPU上的一个任务，并且这些任务可以并行执行。</p> 
<p>支持**设备重叠（Device Overlap）**功能的GPU能够在执行一个CUDA C核函数的同时，还能在设备与主机之间执行复制操作。</p> 
<pre><code class="prism language-cpp"><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{<!-- --></span>
	cudaStream_t    stream<span class="token punctuation">;</span>
	<span class="token comment">// initialize the stream</span>
	<span class="token function">cudaStreamCreate</span><span class="token punctuation">(</span> <span class="token operator">&amp;</span>stream <span class="token punctuation">)</span><span class="token punctuation">;</span>
    
	<span class="token comment">/****** 初始化工作 ************/</span>
    
    <span class="token comment">// now loop over full data, in bite-sized chunks</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> i<span class="token operator">&lt;</span>FULL_DATA_SIZE<span class="token punctuation">;</span> i<span class="token operator">+=</span> N<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token comment">// copy the locked memory to the device, async</span>
        <span class="token comment">// 用于在GPU与主机之间复制数据, 没用之前的cudaMemcpy(),cudaMemcpy()同步的，返回时，复制操作便已经完成，并且在输出缓存区中包含了刚复制进去的内容</span>
        <span class="token function">HANDLE_ERROR</span><span class="token punctuation">(</span> <span class="token function">cudaMemcpyAsync</span><span class="token punctuation">(</span> dev_a<span class="token punctuation">,</span> host_a<span class="token operator">+</span>i<span class="token punctuation">,</span>
                                       N <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                       cudaMemcpyHostToDevice<span class="token punctuation">,</span>
                                       stream <span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token function">HANDLE_ERROR</span><span class="token punctuation">(</span> <span class="token function">cudaMemcpyAsync</span><span class="token punctuation">(</span> dev_b<span class="token punctuation">,</span> host_b<span class="token operator">+</span>i<span class="token punctuation">,</span>
                                       N <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                       cudaMemcpyHostToDevice<span class="token punctuation">,</span>
                                       stream <span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token punctuation">;</span>

        kernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>N<span class="token operator">/</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span>stream<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span> dev_a<span class="token punctuation">,</span> dev_b<span class="token punctuation">,</span> dev_c <span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// copy the data from device to locked memory</span>
        <span class="token function">HANDLE_ERROR</span><span class="token punctuation">(</span> <span class="token function">cudaMemcpyAsync</span><span class="token punctuation">(</span> host_c<span class="token operator">+</span>i<span class="token punctuation">,</span> dev_c<span class="token punctuation">,</span>
                                       N <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                       cudaMemcpyDeviceToHost<span class="token punctuation">,</span>
                                       stream <span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p><code>cudaMemcoyAsync()</code>用于在GPU与主机之间复制数据。没用之前的<code>cudaMemcpy()</code>，<code>cudaMemcpy()</code>同步的，返回时，复制操作便已经完成，并且在输出缓存区中包含了刚复制进去的内容。<strong><code>cudaMemcoyAsync()</code>是异步的</strong>，调用时，只是放置一个请求，表示在流中执行一次内存复制操作，这个流是通过参数stream来指定的。当函数返回时，我们无法确保复制操作是否已经启动，更无法保证它是否已经结束。**我们能够得到的保证是：复制操作肯定会在下一个被放入流中的操作之前执行。**任何传递给<code>cudaMemcoyAsync()</code>的主机内存指针都必须已经通过<code>cudaHostAlloc()</code>分配，也就是说，你只能以异步方式对页锁定内存进行复制操作（注解：这里翻译应该不大准确，从异步操作来看，这块内存不应该被置换的，那么这句话应该这么说：<strong>异步方式的复制只能对页锁定内存操作</strong>）。</p> 
<p>核函数的尖括号，现在我们知道他能有四个参数了。</p> 
<pre><code>kernel&lt;&lt;&lt;[gridDim3], [blockDim3], [stream number], [cudaStream_t]&gt;&gt;&gt;();
</code></pre> 
<p>从上面的流来看，很像MPI中的非阻塞通信。所以我猜测，stream这个参数中，肯定包括标识流中的操作是否完成的标志。</p> 
<p>果然有用于同步的函数。<strong><code>cudaStreamSynchronize([stream])</code></strong></p> 
<pre><code class="prism language-cpp"><span class="token comment">// copy result chunk from locked to full buffer</span>
<span class="token function">cudaStreamSynchronize</span><span class="token punctuation">(</span> stream <span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<p>当要确保相互独立的流能够真正地并行执行时，我们自己要起到一定的作用。<strong>记住，硬件在处理内存复制和核函数执行时分别采用了不同的引擎</strong>，因此我们需要知道，将操作放入流中队列中的顺序将影响着CUDA驱动程序调度这些操作以及执行的方式。</p> 
<p><strong>高效地使用多个CUDA流</strong></p> 
<p>如果同时调度某个流的所有操作，那么很容易在无意中阻塞另一个流的复制操作或者核函数的执行。要解决这个问题，在将操作放入流的队列时应该采用<strong>宽度优先</strong>方式，而非<strong>深度优先</strong>方式。</p> 
<pre><code class="prism language-cpp">    <span class="token comment">// now loop over full data, in bite-sized chunks</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> i<span class="token operator">&lt;</span>FULL_DATA_SIZE<span class="token punctuation">;</span> i<span class="token operator">+=</span> N<span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token comment">// enqueue copies of a in stream0 and stream1</span>
        <span class="token comment">// 将操作交替插入两个不同的流</span>
        <span class="token function">HANDLE_ERROR</span><span class="token punctuation">(</span> <span class="token function">cudaMemcpyAsync</span><span class="token punctuation">(</span> dev_a0<span class="token punctuation">,</span> host_a<span class="token operator">+</span>i<span class="token punctuation">,</span>
                                       N <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                       cudaMemcpyHostToDevice<span class="token punctuation">,</span>
                                       stream0 <span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token function">HANDLE_ERROR</span><span class="token punctuation">(</span> <span class="token function">cudaMemcpyAsync</span><span class="token punctuation">(</span> dev_a1<span class="token punctuation">,</span> host_a<span class="token operator">+</span>i<span class="token operator">+</span>N<span class="token punctuation">,</span>
                                       N <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                       cudaMemcpyHostToDevice<span class="token punctuation">,</span>
                                       stream1 <span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// enqueue copies of b in stream0 and stream1</span>
        <span class="token function">HANDLE_ERROR</span><span class="token punctuation">(</span> <span class="token function">cudaMemcpyAsync</span><span class="token punctuation">(</span> dev_b0<span class="token punctuation">,</span> host_b<span class="token operator">+</span>i<span class="token punctuation">,</span>
                                       N <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                       cudaMemcpyHostToDevice<span class="token punctuation">,</span>
                                       stream0 <span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token function">HANDLE_ERROR</span><span class="token punctuation">(</span> <span class="token function">cudaMemcpyAsync</span><span class="token punctuation">(</span> dev_b1<span class="token punctuation">,</span> host_b<span class="token operator">+</span>i<span class="token operator">+</span>N<span class="token punctuation">,</span>
                                       N <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                       cudaMemcpyHostToDevice<span class="token punctuation">,</span>
                                       stream1 <span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// enqueue kernels in stream0 and stream1   </span>
        kernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>N<span class="token operator">/</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span>stream0<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span> dev_a0<span class="token punctuation">,</span> dev_b0<span class="token punctuation">,</span> dev_c0 <span class="token punctuation">)</span><span class="token punctuation">;</span>
        kernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>N<span class="token operator">/</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span>stream1<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span> dev_a1<span class="token punctuation">,</span> dev_b1<span class="token punctuation">,</span> dev_c1 <span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// enqueue copies of c from device to locked memory</span>
        <span class="token function">HANDLE_ERROR</span><span class="token punctuation">(</span> <span class="token function">cudaMemcpyAsync</span><span class="token punctuation">(</span> host_c<span class="token operator">+</span>i<span class="token punctuation">,</span> dev_c0<span class="token punctuation">,</span>
                                       N <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                       cudaMemcpyDeviceToHost<span class="token punctuation">,</span>
                                       stream0 <span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token function">HANDLE_ERROR</span><span class="token punctuation">(</span> <span class="token function">cudaMemcpyAsync</span><span class="token punctuation">(</span> host_c<span class="token operator">+</span>i<span class="token operator">+</span>N<span class="token punctuation">,</span> dev_c1<span class="token punctuation">,</span>
                                       N <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                       cudaMemcpyDeviceToHost<span class="token punctuation">,</span>
                                       stream1 <span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
</code></pre> 
<h2><a id="11_GPUCUDA_C_520"></a>第11章 多GPU系统上的CUDA C</h2> 
<ul><li>了解如何分配和使用<strong>零拷贝内存（Zero-Copy Memory）</strong></li><li>了解如何在同一个应用程序中使用多个GPU</li><li>了解如何分配和使用<strong>可移动的固定内存（Portable Pinned Memory）</strong></li></ul> 
<p><code>cudaHostAlloc()</code>的<code>cudaHostAllocDefault</code>参数来获得默认的固定内存。本章会介绍除此之外的其他参数值。<code>cudaHostAllocMapped</code>分配的主机内存也是固定的。<strong>可以在CUDA C核函数中直接访问这种类型的主机内存</strong>。由于这种内存不需要复制到GPU，因此也称为<strong>零拷贝内存</strong>。</p> 
<p>矢量点积运算，<code>cudaMalloc()</code>和<code>cudaHostAlloc(, cudaHostAllocMapped)</code>使用，运算时间的对比。</p> 
<pre><code>// allocate the memory on the CPU
cudaHostAlloc( (void**)&amp;a,size*sizeof(float),cudaHostAllocWriteCombined | cudaHostAllocMapped );
cudaHostAlloc( (void**)&amp;b,size*sizeof(float),cudaHostAllocWriteCombined | cudaHostAllocMapped );
cudaHostAlloc( (void**)&amp;partial_c,blocksPerGrid*sizeof(float),cudaHostAllocMapped );
                              
Value calculated:  27621697910970467221504.000000
Time using cudaMalloc:  147.3 ms
Value calculated:  27621697910970467221504.000000
Time using cudaHostAlloc:  24.8 ms
</code></pre> 
<p><code>cudaHostAllocWriteCombined</code> 表示运行时应该将内存分配为**“合并式写入（Write-Combined）”**内存。这个标志不会改变应用程序的功能，但却可以显著地提升GPU读取内存时的性能。然而，当CPU也要读取这块内存时，会显得低效，因此使用之前，必须首先考虑应用程序可能的访问模式。</p> 
<p><code>cudaHostAlloc()</code>返回CPU上的指针，需要<code>cudaHostGetDevicePointer()</code>来获得这块内存在GPU上的有效指针。</p> 
<pre><code class="prism language-cpp"><span class="token comment">// find out the GPU pointers</span>
<span class="token function">cudaHostGetDevicePointer</span><span class="token punctuation">(</span> <span class="token operator">&amp;</span>dev_a<span class="token punctuation">,</span> a<span class="token punctuation">,</span> <span class="token number">0</span> <span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">cudaHostGetDevicePointer</span><span class="token punctuation">(</span> <span class="token operator">&amp;</span>dev_b<span class="token punctuation">,</span> b<span class="token punctuation">,</span> <span class="token number">0</span> <span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">cudaHostGetDevicePointer</span><span class="token punctuation">(</span> <span class="token operator">&amp;</span>dev_partial_c<span class="token punctuation">,</span> partial_c<span class="token punctuation">,</span> <span class="token number">0</span> <span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<p><code>cudaThreadSynchronize()</code>将CPU与GPU同步。</p> 
<p>当输入内存和输出内存都只使用一次时，那么在独立GPU上使用零拷贝内存将带来性能提升。由于GPU在设计时考虑了隐藏内存访问带来的延迟，因此这种机制在某种程度上将减轻PCIe总线上读取和写入等操作的延迟，从而会带来可观的性能提升。但由于GPU不会缓存零拷贝内存的内容，如果多次读取内存，那么最终会得不偿失，还不如一开始就将数据复制到GPU。</p> 
<p>集成GPU在物理上是与CPU共享内存的，那么将缓冲区声明为零拷贝内存的作用就是避免不必要的数据复制。</p> 
<p>零拷贝内存也是一种固定内存（页锁定），每个固定内存都是占用系统的可用物理内存，并且不会被交换到磁盘中，这会降低系统的性能。</p> 
<p>固定内存只对于分配它的线程是页锁定的，对于其他线程而言，就是一块普通内存对待，所以不能用<code>cudaMemcpyAsync()</code>，且不能放入流中，而使用<code>cudaMemcpy()</code>这种速率大约为最高传输速率的50%。</p> 
<p><code>cudaHostAlloc(, cudaHostAllocPortable)</code>固定内存分配为可移动的，<code>cudaFreeHost()</code></p> 
<h2><a id="12__571"></a>第12章 后记</h2> 
<ul><li> <p><strong>CUDA 工具箱</strong></p> 
  <ul><li>CUFFT</li><li>CUBLAS，线性代数库，其中包含了著名的基本线性代数子程序(Basic LInear Algebra Subprograms, BLAS)，列优先</li></ul> </li><li> <p>NVIDIA GPU Computing SDK</p> <p>包含许多GPU计算示例程序</p> 
  <ul><li>CUDA基本主题</li><li>CUDA高级主题</li><li>CUDA系统集成</li><li>数据并行算法</li><li>图形互操作</li><li>纹理</li><li>性能策略</li><li>线性代数</li><li>图形/视频处理</li><li>计算金融</li><li>数据压缩</li><li>物理模拟</li></ul> </li><li> <p>NVIDIA性能原语（NVIDIA Performance Primitives，NPP）。</p> </li><li> <p>调试CUDA C</p> 
  <ul><li><strong>CUDA-GDB</strong>，除了调试器功能，还提供了CUDA内存检查器（CUDA Memory Checker）</li><li><strong>NVIDIA Parallel Nsight</strong>，集成在VS中的GPU/CPU调试器</li></ul> </li><li> <p><strong>CUDA Visual Profiler</strong></p> <p>可视化分析工具来运行核函数，根据profiling调优。写出真正的高性能计算程序</p> </li><li> <p>参考资料</p> 
  <ul><li>《Programming Massively Parallel Processors: a Hands-On Approach》，2012年出的第二版，国内有翻译的了，目前最新是2022年的第四版，目前国内还没有翻译版本，第三版也没有翻译版本。伊利诺伊大学的课程可一起看，我看过这本第二版的翻译版，中间更多的讲思想与原理。</li></ul> </li><li> <p>CUDA U</p> <p>大学课程，最好的之一是伊利诺伊大学的课程</p> </li><li> <p>NVIDIA 论坛</p> </li><li> <p>代码资源</p> 
  <ul><li><strong>CUDA 数据并行原语库（CUDA Data Parallel Primitives Library，CUDPP）</strong>，这些原语为许多数据并行算法提供了重要基础，包括排序、流压缩、构建数据结构以及其他并行算法等等，如果你正在编写某个算法，CUDPP可能已经提供了这个算法或这个算法的大部分功能</li><li>语言封装器</li></ul> </li></ul> 
<h2><a id="__622"></a>附录 高级原子操作</h2> 
<p><strong>原子锁</strong></p> 
<pre><code class="prism language-cpp"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">ifndef</span> <span class="token expression">__LOCK_H__</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">__LOCK_H__</span></span>

<span class="token keyword">struct</span> <span class="token class-name">Lock</span> <span class="token punctuation">{<!-- --></span>
    <span class="token keyword">int</span> <span class="token operator">*</span>mutex<span class="token punctuation">;</span>
    <span class="token function">Lock</span><span class="token punctuation">(</span> <span class="token keyword">void</span> <span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token function">HANDLE_ERROR</span><span class="token punctuation">(</span> <span class="token function">cudaMalloc</span><span class="token punctuation">(</span> <span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span> <span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token function">HANDLE_ERROR</span><span class="token punctuation">(</span> <span class="token function">cudaMemset</span><span class="token punctuation">(</span> mutex<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span> <span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    <span class="token operator">~</span><span class="token function">Lock</span><span class="token punctuation">(</span> <span class="token keyword">void</span> <span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token function">cudaFree</span><span class="token punctuation">(</span> mutex <span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    __device__ <span class="token keyword">void</span> <span class="token function">lock</span><span class="token punctuation">(</span> <span class="token keyword">void</span> <span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token keyword">while</span><span class="token punctuation">(</span> <span class="token function">atomicCAS</span><span class="token punctuation">(</span> mutex<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span> <span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token number">0</span> <span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">__threadfence</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    __device__ <span class="token keyword">void</span> <span class="token function">unlock</span><span class="token punctuation">(</span> <span class="token keyword">void</span> <span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token function">__threadfence</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token function">atomicExch</span><span class="token punctuation">(</span> mutex<span class="token punctuation">,</span> <span class="token number">0</span> <span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span><span class="token punctuation">;</span>

<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">endif</span></span>
</code></pre> 
<p>原子锁和CPP中学到的差不多，可以迁移过来使用。</p> 
<p><strong>实现散列表</strong></p> 
<p>多线程散列表，要考虑多个线程同时将节点加入散列表中的同一个位置。</p> 
<p>GPU中散列表的构造还是使用**<code>Lock</code>**</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/cc58c960c9b38e042c37e8f07b5942ea/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">深度学习之心得——dropout</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/9ae370152109f29a849aa9ecf7f7acfc/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">使用devtools安装monocle3</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>