<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>AI常见部署方式 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="AI常见部署方式" />
<meta property="og:description" content="文章目录 1.AI部署简介2.具体部署 1.AI部署简介 没人告诉你的大规模部署AI高效流程！ 对大部分 TensorFlow 模型来说，部署流程是相同的： 1. 将图像固化为 Protobuf 二进制文件 2. 调整推断代码，使它可以处理固化的图 3. 容器化应用程序 4. 在最上面加上 API 层 本文提出了一个大规模部署 AI 的高效工作流程： 固化图并将推断封装在 API 下 重复使用会话和图，缓存输入和输出 用 Docker 容器化应用程序（包括 API 层） 将大规模应用程序与 Kubernetes 一起部署在你选择的云上 将训练从推断中分离出来 建立任务队列，将较小的任务确立为优先级 使用这些技术，你就可以在成本最小、速度和效率最大的情况下大规模部署 AI。 2.具体部署 使用ONNX部署深度学习和传统机器学习模型自动部署深度神经网络模型TensorFlow（Keras）到生产环境中深度学习Tensorflow生产环境部署（上·环境准备篇） 客户端 ----&gt; web服务(flask或者tornado) --grpc或者rest--&gt; tensorflow serving 深度学习Tensorflow生产环境部署（下·模型部署篇）Tensorflow 2.x模型-部署与实践如何部署tensorflow训练的模型将tensorflow模型部署到服务器上Tensorflow如何进行工业部署？ 确定好输入和输出节点，把模型导出成SavedModel格式， 然后用TF-Serving启动服务， 调用方发http请求或者grpc请求就可以拿到预测结果 使用 Docker 部署 TensorFlow 环境
已经训练好的tensorflow模型如何部署到web上？
可以做成一个http服务,提供一个web接口,模型只在服务器上就可以了. 使用时候客户端通过http请求上传要识别的图片等数据到服务端接口,服务端计算后返回给客户端. 部署PyTorch模型到终端如何将pytorch模型通过docker部署到服务器 在服务端训练出特定的算法模型——再将这个模型部署到服务端或者终端（以后大多数场景下是部署到终端）——需要服务端AI框架到终端AI推理框架的转换工具。 Tensorflowserving:
模型部署 TensorFlowServingtensorflow2.0基础（10）——使用tensorflow-serving部署模型Tensorflow-serving部署模型到服务器tensorflow serving部署keras或tf2.0模型docker部署tensorflowserving以及模型替换Tensorflow-serving&#43;Docker安装&#43;模型部署用Docker容器自带的tensorflowserving部署模型对外服务（成功率100%）TensorFlow Serving &#43; Docker &#43;Tornado机器学习模型生产级快速部署 常用的做法如使用flask、Django、tornado等web框架创建一个服务器app，这个app在启动后就会一直挂在后台，然后等待用户使用客户端POST一个请求上来（例如上传了一张图片的url），app检测到有请求，就会下载这个url的图片，接着调用你的模型，得到推理结果后以json的格式把结果返回给用户。 这个做法对于简单部署来说代码量不多，对于不熟悉web框架的朋友来说随便套用一个模板就能写出来，但是也会有一些明显的缺点： 1." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/e5389c5e0e9ade81de6b284e634eef22/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-08T15:25:31+08:00" />
<meta property="article:modified_time" content="2022-07-08T15:25:31+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">AI常见部署方式</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><ul><li><a href="#1AI_2" rel="nofollow">1.AI部署简介</a></li><li><a href="#2_27" rel="nofollow">2.具体部署</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h3><a id="1AI_2"></a>1.AI部署简介</h3> 
<ol><li><a href="https://www.sohu.com/a/287055173_129720" rel="nofollow">没人告诉你的大规模部署AI高效流程！</a></li></ol> 
<pre><code class="prism language-python">对大部分 TensorFlow 模型来说，部署流程是相同的：

 <span class="token number">1.</span> 将图像固化为 Protobuf 二进制文件
 <span class="token number">2.</span> 调整推断代码，使它可以处理固化的图
 <span class="token number">3.</span> 容器化应用程序
 <span class="token number">4.</span> 在最上面加上 API 层
</code></pre> 
<pre><code class="prism language-python">本文提出了一个大规模部署 AI 的高效工作流程：

固化图并将推断封装在 API 下
重复使用会话和图，缓存输入和输出
用 Docker 容器化应用程序（包括 API 层）
将大规模应用程序与 Kubernetes 一起部署在你选择的云上
将训练从推断中分离出来
建立任务队列，将较小的任务确立为优先级
使用这些技术，你就可以在成本最小、速度和效率最大的情况下大规模部署 AI。
</code></pre> 
<h3><a id="2_27"></a>2.具体部署</h3> 
<ol><li><a href="https://www.jianshu.com/p/c1e0efe6482f" rel="nofollow">使用ONNX部署深度学习和传统机器学习模型</a></li><li><a href="https://www.jianshu.com/p/0a9ea854910d" rel="nofollow">自动部署深度神经网络模型TensorFlow（Keras）到生产环境中</a></li><li><a href="https://blog.csdn.net/u010159842/article/details/104060429">深度学习Tensorflow生产环境部署（上·环境准备篇）</a></li></ol> 
<pre><code class="prism language-python">客户端 <span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">&gt;</span> web服务<span class="token punctuation">(</span>flask或者tornado<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">-</span>grpc或者rest<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">&gt;</span> tensorflow serving
</code></pre> 
<ol start="4"><li><a href="https://blog.csdn.net/xingoo_/article/details/86164566">深度学习Tensorflow生产环境部署（下·模型部署篇）</a></li><li><a href="https://blog.csdn.net/qihoo_tech/article/details/108067640">Tensorflow 2.x模型-部署与实践</a></li><li><a href="https://www.jianshu.com/p/703febf017b3" rel="nofollow">如何部署tensorflow训练的模型</a></li><li><a href="https://blog.csdn.net/qq_17190121/article/details/99696768">将tensorflow模型部署到服务器上</a></li><li><a href="https://www.zhihu.com/question/271624806/answer/1386811629" rel="nofollow">Tensorflow如何进行工业部署？</a></li></ol> 
<pre><code class="prism language-python">确定好输入和输出节点，把模型导出成SavedModel格式，
然后用TF<span class="token operator">-</span>Serving启动服务，
调用方发http请求或者grpc请求就可以拿到预测结果
</code></pre> 
<ol start="9"><li> <p><a href="https://tf.wiki/zh_hans/appendix/docker.html" rel="nofollow">使用 Docker 部署 TensorFlow 环境<br> </a><br> <img src="https://images2.imgbox.com/a1/3b/sFneBX7S_o.png" alt="在这里插入图片描述"></p> </li><li> <p><a href="https://www.oschina.net/question/4675385_2319031" rel="nofollow">已经训练好的tensorflow模型如何部署到web上？</a></p> </li></ol> 
<pre><code class="prism language-python">可以做成一个http服务<span class="token punctuation">,</span>提供一个web接口<span class="token punctuation">,</span>模型只在服务器上就可以了<span class="token punctuation">.</span>
使用时候客户端通过http请求上传要识别的图片等数据到服务端接口<span class="token punctuation">,</span>服务端计算后返回给客户端<span class="token punctuation">.</span>
</code></pre> 
<ol start="11"><li><a href="https://zhuanlan.zhihu.com/p/54665674" rel="nofollow">部署PyTorch模型到终端</a></li><li><a href="https://zhuanlan.zhihu.com/p/159191983" rel="nofollow">如何将pytorch模型通过docker部署到服务器</a></li></ol> 
<pre><code class="prism language-python">在服务端训练出特定的算法模型——再将这个模型部署到服务端或者终端（以后大多数场景下是部署到终端）——需要服务端AI框架到终端AI推理框架的转换工具。
</code></pre> 
<p><strong>Tensorflowserving</strong>:</p> 
<ol><li><a href="https://blog.csdn.net/weixin_30344131/article/details/98825190">模型部署 TensorFlowServing</a></li><li><a href="https://blog.csdn.net/qq_35793394/article/details/107813230">tensorflow2.0基础（10）——使用tensorflow-serving部署模型</a></li><li><a href="https://blog.csdn.net/u012940753/article/details/105653686?utm_medium=distribute.pc_relevant.none-task-blog-title-10&amp;spm=1001.2101.3001.4242">Tensorflow-serving部署模型到服务器</a></li><li><a href="https://blog.csdn.net/data_scientist/article/details/102613618">tensorflow serving部署keras或tf2.0模型</a></li><li><a href="https://www.cnblogs.com/aidenzdly/p/10455917.html" rel="nofollow">docker部署tensorflowserving以及模型替换</a></li><li><a href="https://www.jianshu.com/p/bd67b40e6b85" rel="nofollow">Tensorflow-serving+Docker安装+模型部署</a></li><li><a href="https://www.cnblogs.com/zhwl/p/12737264.html" rel="nofollow">用Docker容器自带的tensorflowserving部署模型对外服务（成功率100%）</a></li><li><a href="https://zhuanlan.zhihu.com/p/52096200" rel="nofollow">TensorFlow Serving + Docker +Tornado机器学习模型生产级快速部署</a></li></ol> 
<pre><code class="prism language-python">常用的做法如使用flask、Django、tornado等web框架创建一个服务器app，这个app在启动后就会一直挂在后台，然后等待用户使用客户端POST一个请求上来（例如上传了一张图片的url），app检测到有请求，就会下载这个url的图片，接着调用你的模型，得到推理结果后以json的格式把结果返回给用户。
这个做法对于简单部署来说代码量不多，对于不熟悉web框架的朋友来说随便套用一个模板就能写出来，但是也会有一些明显的缺点：
<span class="token number">1.</span> 需要在服务器上重新安装项目所需的所有依赖。
<span class="token number">2.</span> 当接收到并发请求的时候，服务器可能要后台启动多个进程进行推理，造成资源紧缺。
<span class="token number">3.</span> 不同的模型需要启动不同的服务。
而为了解决第一个问题，Docker是最好的方案。
</code></pre> 
<p><img src="https://images2.imgbox.com/a1/69/RRlRT9Gt_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/dd8bfe39fbcce9ce2992cefeb4e324af/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Latex左对齐</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7857625d22940f98783dc9aa29650113/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">抖音取关-autojs</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>