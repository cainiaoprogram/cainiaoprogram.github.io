<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【深度学习】 基于Keras的Attention机制代码实现及剖析——LSTM&#43;Attention - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【深度学习】 基于Keras的Attention机制代码实现及剖析——LSTM&#43;Attention" />
<meta property="og:description" content="说明 这是接前面【深度学习】基于Keras的Attention机制代码实现及剖析——Dense&#43;Attention的后续。
参考的代码来源1：Attention mechanism Implementation for Keras.网上大部分代码都源于此，直接使用时注意Keras版本，若版本不对应，在merge处会报错，解决办法为：导入Multiply层并将merge改为Multiply()。
参考的代码来源2：Attention Model（注意力模型）思想初探，这篇也是运行了一下来源1，做对照。在实验之前需要一些预备知识，如RNN、LSTM的基本结构，和Attention的大致原理，快速获得这方面知识可看RNN&amp;Attention机制&amp;LSTM 入门了解。 如果你对本系列感兴趣，可接以下传送门：
【深度学习】 基于Keras的Attention机制代码实现及剖析——Dense&#43;Attention
【深度学习】 基于Keras的Attention机制代码实现及剖析——LSTM&#43;Attention
【深度学习】 Attention机制的几种实现方法总结(基于Keras框架)
目录 说明实验目的实验设计数据集生成模型搭建Attention层封装LSTM之前使用AttentionLSTM之后使用Attention 结果展示注意权重共享&#43;LSTM之前使用注意力注意权重共享&#43;LSTM之后使用注意力注意权重不共享&#43;LSTM之前使用注意力注意权重不共享&#43;LSTM之后使用注意力结果总结完整代码(1个文件) 实验目的 现实生活中有很多序列问题，对一个序列而言，其每个元素的“重要性”显然是不同的，即权重不同，这样一来就有使用Attention机制的空间，本次实验将在LSTM基础上实现Attention机制的运用。检验Attention是否真的捕捉到了关键特征，即被Attention分配的关键特征的权重是否更高。 实验设计 问题设计：同Dense&#43;Attention一样，我们也设计成二分类问题，给定特征和标签进行训练。Attention聚焦测试：将特征的某一列与标签值设置成相同，这样就人为的造了一列关键特征，可视化Attention给每个特征分配的权重，观察关键特征的权重是否更高。Attention位置测试：在模型不同地方加上Attention会有不同的含义，那么是否每个地方Attention都能捕捉到关键信息呢？我们将变换Attention层的位置，分别放在整个分类模型的输入层(LSTM之前)和输出层(LSTM之后)进行比较。 数据集生成 数据集要为LSTM的输入做准备，而LSTM里面一个重要的参数就是time_steps，指的就是序列长度，而input_dim则指得是序列每一个单元的维度。
def get_data_recurrent(n, time_steps, input_dim, attention_column=10): &#34;&#34;&#34; Data generation. x is purely random except that it&#39;s first value equals the target y. In practice, the network should learn that the target = x[attention_column]. Therefore, most of its attention should be focused on the value addressed by attention_column." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/4d09d67b80111d3a8c546747ff0ef130/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-09-21T08:18:31+08:00" />
<meta property="article:modified_time" content="2022-09-21T08:18:31+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【深度学习】 基于Keras的Attention机制代码实现及剖析——LSTM&#43;Attention</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night-eighties">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_0"></a>说明</h2> 
<ul><li>这是接前面<a href="https://blog.csdn.net/qq_34862636/article/details/103457982">【深度学习】基于Keras的Attention机制代码实现及剖析——Dense+Attention</a>的后续。<br>   参考的<strong>代码来源1</strong>：<a href="https://github.com/philipperemy/keras-attention-mechanism">Attention mechanism Implementation for Keras.</a>网上大部分代码都源于此，直接使用时注意Keras版本，若版本不对应，在merge处会报错，解决办法为：导入Multiply层并将merge改为Multiply()。<br>   参考的<strong>代码来源2</strong>：<a href="https://www.cnblogs.com/LittleHann/p/9722779.html#_lab2_1_1" rel="nofollow">Attention Model（注意力模型）思想初探</a>，这篇也是运行了一下来源1，做对照。</li><li>在实验之前需要一些预备知识，如RNN、LSTM的基本结构，和Attention的大致原理，快速获得这方面知识可看<a href="https://segmentfault.com/a/1190000014574524" rel="nofollow">RNN&amp;Attention机制&amp;LSTM 入门了解</a>。</li></ul> 
<p>  如果你对本系列感兴趣，可接以下传送门：<br>   <a href="https://blog.csdn.net/qq_34862636/article/details/103457982">【深度学习】 基于Keras的Attention机制代码实现及剖析——Dense+Attention</a><br>   <a href="https://blog.csdn.net/qq_34862636/article/details/103472650">【深度学习】 基于Keras的Attention机制代码实现及剖析——LSTM+Attention</a><br> <a href="https://blog.csdn.net/qq_34862636/article/details/106642625">  【深度学习】 Attention机制的几种实现方法总结(基于Keras框架)</a></p> 
<p></p> 
<div class="toc"> 
 <h4>目录</h4> 
 <ul><li><a href="#_0" rel="nofollow">说明</a></li><li><a href="#_14" rel="nofollow">实验目的</a></li><li><a href="#_17" rel="nofollow">实验设计</a></li><li><a href="#_22" rel="nofollow">数据集生成</a></li><li><a href="#_43" rel="nofollow">模型搭建</a></li><li><ul><li><a href="#Attention_44" rel="nofollow">Attention层封装</a></li><li><a href="#LSTMAttention_74" rel="nofollow">LSTM之前使用Attention</a></li><li><a href="#LSTMAttention_90" rel="nofollow">LSTM之后使用Attention</a></li></ul> 
  </li><li><a href="#_108" rel="nofollow">结果展示</a></li><li><ul><li><a href="#LSTM_109" rel="nofollow">注意权重共享+LSTM之前使用注意力</a></li><li><a href="#LSTM_112" rel="nofollow">注意权重共享+LSTM之后使用注意力</a></li><li><a href="#LSTM_115" rel="nofollow">注意权重不共享+LSTM之前使用注意力</a></li><li><a href="#LSTM_118" rel="nofollow">注意权重不共享+LSTM之后使用注意力</a></li><li><a href="#_120" rel="nofollow">结果总结</a></li><li><a href="#1_123" rel="nofollow">完整代码(1个文件)</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="_14"></a>实验目的</h2> 
<ul><li>现实生活中有很多序列问题，对一个序列而言，其每个元素的“重要性”显然是不同的，即权重不同，这样一来就有使用Attention机制的空间，本次实验将在LSTM基础上实现Attention机制的运用。</li><li>检验Attention是否真的捕捉到了关键特征，即被Attention分配的关键特征的权重是否更高。</li></ul> 
<h2><a id="_17"></a>实验设计</h2> 
<ul><li>问题设计：同Dense+Attention一样，我们也设计成二分类问题，给定特征和标签进行训练。</li><li>Attention聚焦测试：将特征的某一列与标签值设置成相同，这样就人为的造了一列关键特征，可视化Attention给每个特征分配的权重，观察关键特征的权重是否更高。</li><li>Attention位置测试：在模型不同地方加上Attention会有不同的含义，那么是否每个地方Attention都能捕捉到关键信息呢？我们将变换Attention层的位置，分别放在整个分类模型的输入层(LSTM之前)和输出层(LSTM之后)进行比较。</li></ul> 
<h2><a id="_22"></a>数据集生成</h2> 
<p>  数据集要为LSTM的输入做准备，而LSTM里面一个重要的参数就是time_steps，指的就是序列长度，而input_dim则指得是序列每一个单元的维度。</p> 
<pre><code class="prism language-py"><span class="token keyword">def</span> <span class="token function">get_data_recurrent</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> time_steps<span class="token punctuation">,</span> input_dim<span class="token punctuation">,</span> attention_column<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Data generation. x is purely random except that it's first value equals the target y.
    In practice, the network should learn that the target = x[attention_column].
    Therefore, most of its attention should be focused on the value addressed by attention_column.
    :param n: the number of samples to retrieve.
    :param time_steps: the number of time steps of your series.
    :param input_dim: the number of dimensions of each element in the series.
    :param attention_column: the column linked to the target. Everything else is purely random.
    :return: x: model inputs, y: model targets
    """</span>
    x <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>standard_normal<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> time_steps<span class="token punctuation">,</span> input_dim<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">#标准正态分布随机特征值</span>
    y <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span>low<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> high<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">#二分类，随机标签值</span>
    x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> attention_column<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>tile<span class="token punctuation">(</span>y<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> input_dim<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">#将第attention_column个column的值置为标签值</span>
    <span class="token keyword">return</span> x<span class="token punctuation">,</span> y
</code></pre> 
<p>  我们设置input_dim = 2，尝试输出前三个x和y来看看，因为函数参数attention_column=10，所以第10个column的特征和标签值相同。<br> <img src="https://images2.imgbox.com/97/26/DwOfAfBw_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_43"></a>模型搭建</h2> 
<h3><a id="Attention_44"></a>Attention层封装</h3> 
<p>  上一章我们谈到Attention的实现可直接由一个激活函数为softmax的Dense层实现，Dense层的输出乘以Dense的输入即完成了Attention权重的分配。在这里的实现看上去比较复杂，但本质上仍是那两步操作，只是为了将问题更为泛化，把维度进行了扩展。</p> 
<pre><code class="prism language-py"><span class="token keyword">def</span> <span class="token function">attention_3d_block</span><span class="token punctuation">(</span>inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># inputs.shape = (batch_size, time_steps, input_dim)</span>
    input_dim <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>inputs<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    a <span class="token operator">=</span> Permute<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
    a <span class="token operator">=</span> Reshape<span class="token punctuation">(</span><span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> TIME_STEPS<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span> <span class="token comment"># this line is not useful. It's just to know which dimension is what.</span>
    a <span class="token operator">=</span> Dense<span class="token punctuation">(</span>TIME_STEPS<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>
    <span class="token keyword">if</span> SINGLE_ATTENTION_VECTOR<span class="token punctuation">:</span>
        a <span class="token operator">=</span> Lambda<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> K<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>x<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'dim_reduction'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>
        a <span class="token operator">=</span> RepeatVector<span class="token punctuation">(</span>input_dim<span class="token punctuation">)</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>
    a_probs <span class="token operator">=</span> Permute<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'attention_vec'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>
    output_attention_mul <span class="token operator">=</span> Multiply<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">[</span>inputs<span class="token punctuation">,</span> a_probs<span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> output_attention_mul
</code></pre> 
<p>  这里涉及到多个Keras的层，我们一个一个来看看它的功能。</p> 
<ul><li><a href="https://blog.csdn.net/chenglinben/article/details/95864488">Permute层</a>：索引从1开始，根据给定的模式(dim)置换输入的维度。(2,1)即置换输入的第1和第2个维度，可以理解成转置。</li><li><a href="https://blog.csdn.net/xykimred/article/details/89013432">Reshape层</a>：将输出调整为特定形状，INPUT_DIM = 2，TIME_STEPS = 20，就将其调整为了2行，20列。</li><li><a href="https://blog.csdn.net/VeritasCN/article/details/89884036">Lambda层</a>：本函数用以对上一层的输出施以任何Theano/TensorFlow表达式。这里的“表达式”指得就是K.mean，其原型为keras.backend.mean(x, axis=None, keepdims=False)，指张量在某一指定轴的均值。</li><li><a href="https://blog.csdn.net/chuxuezhew/article/details/97890995">RepeatVector层</a>：作用为将输入重复n次。</li></ul> 
<p>  接下来，我们分析下这样设计有什么作用，重点看下SINGLE_ATTENTION_VECTOR分别为True和False时的异同。<br>   先看第一个Permute层，由前面数据集的前三个输出我们知道，输入网络的数据的shape是(time_steps, input_dim)，这是方便输入到LSTM层里的输入格式。无论注意力层放在LSTM的前面还是后面，最终输入到注意力层的数据shape仍为(time_steps, input_dim)，对于注意力结构里的Dense层而言，(input_dim, time_steps)才是符合的，因此要进行维度变换。<br>   再看第一个Reshape层，可以发现其作用为将数据转化为(input_dim, time_steps)。这个操作不是在第一个Permute层就已经完成了吗？没错，实际上这一步操作物理上是无效的，因为格式已经变换好了，但这样做有一个好处，就是可以清楚的知道此时的数据格式，shape的每一个值分别代表什么含义。<br>   接下来是一个Dense层，这个Dense层的激活函数是softmax，显然就是注意力结构里的Dense层，用于计算每个特征的权重。<br>   马上就到SINGLE_ATTENTION_VECTOR值的判断了，现在出现了一个问题，我们的特征在一个时间结点上的维度是多维的(input_dim维)，即有可能是多个特征随时间变换一起发生了变换，那对应的，我们的注意力算出来也是多维的。此时，我们会想：是<strong>多维特征共享一个注意力权重，还是每一维特征单独有一个注意力权重呢？</strong> 这就是SINGLE_ATTENTION_VECTOR值的判断的由来了。SINGLE_ATTENTION_VECTOR=True，则共享一个注意力权重，如果=False则每维特征会单独有一个权重，换而言之，注意力权重也变成多维的了。<br>   下面对当SINGLE_ATTENTION_VECTOR=True时，代码进行分析。Lambda层将原本多维的注意力权重取平均，RepeatVector层再按特征维度复制粘贴，那么每一维特征的权重都是一样的了，也就是所说的共享一个注意力。<br>   接下来就是第二个Permute层，到这步就已经是算好的注意力权重了，我们知道Attention的第二个结构就是乘法，而这个乘法要对应元素相乘，因此要再次对维度进行变换。<br>   最后一个Multiply层，权重乘以输入，注意力层就此完工。</p> 
<h3><a id="LSTMAttention_74"></a>LSTM之前使用Attention</h3> 
<p>  如题，在LSTM之前使用Attention与上一篇文章Dense+Attention的结构类似，放一张图上来应该会更清晰。<br> 在输入层(LSTM之前)加Attention的结构图：<br> <img src="https://images2.imgbox.com/3b/68/g693oQYf_o.png" alt="在这里插入图片描述"><br>   由于封装好了Attention，所以结构看起来清晰明了，只需注意此时LSTM参数里return_sequences=False，也就是N对1结构，才符合我们的问题。</p> 
<pre><code class="prism language-py"><span class="token keyword">def</span> <span class="token function">model_attention_applied_before_lstm</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    K<span class="token punctuation">.</span>clear_session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#清除之前的模型，省得压满内存</span>
    inputs <span class="token operator">=</span> Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>TIME_STEPS<span class="token punctuation">,</span> INPUT_DIM<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    attention_mul <span class="token operator">=</span> attention_3d_block<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
    lstm_units <span class="token operator">=</span> <span class="token number">32</span>
    attention_mul <span class="token operator">=</span> LSTM<span class="token punctuation">(</span>lstm_units<span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">(</span>attention_mul<span class="token punctuation">)</span>
    output <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>attention_mul<span class="token punctuation">)</span>
    model <span class="token operator">=</span> Model<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span><span class="token punctuation">[</span>inputs<span class="token punctuation">]</span><span class="token punctuation">,</span> output<span class="token operator">=</span>output<span class="token punctuation">)</span>
    <span class="token keyword">return</span> model
</code></pre> 
<h3><a id="LSTMAttention_90"></a>LSTM之后使用Attention</h3> 
<p>  注意此时LSTM的结构就不是N对1而是N对N了，因为要用Attention，所以输入到Attention里的特征要是多个才有意义。<br> 在输出层(LSTM之后)加Attention的结构图：<br> <img src="https://images2.imgbox.com/45/b3/YVVWmzXk_o.png" alt="在这里插入图片描述"><br>   再看代码,此时除了各层位置发生变换以外，return_sequences也置为了True，输出也是序列，N对N结构。此外还多加了一个<a href="https://blog.csdn.net/qq_34840129/article/details/86566109">Flatten层</a>，中文叫扁平层，作用是将多维的数据平铺成1维，和输出层做连接。</p> 
<pre><code class="prism language-py"><span class="token keyword">def</span> <span class="token function">model_attention_applied_after_lstm</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    K<span class="token punctuation">.</span>clear_session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#清除之前的模型，省得压满内存</span>
    inputs <span class="token operator">=</span> Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>TIME_STEPS<span class="token punctuation">,</span> INPUT_DIM<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    lstm_units <span class="token operator">=</span> <span class="token number">32</span>
    lstm_out <span class="token operator">=</span> LSTM<span class="token punctuation">(</span>lstm_units<span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
    attention_mul <span class="token operator">=</span> attention_3d_block<span class="token punctuation">(</span>lstm_out<span class="token punctuation">)</span>
    attention_mul <span class="token operator">=</span> Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>attention_mul<span class="token punctuation">)</span>
    output <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>attention_mul<span class="token punctuation">)</span>
    model <span class="token operator">=</span> Model<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span><span class="token punctuation">[</span>inputs<span class="token punctuation">]</span><span class="token punctuation">,</span> output<span class="token operator">=</span>output<span class="token punctuation">)</span>
    <span class="token keyword">return</span> model
</code></pre> 
<h2><a id="_108"></a>结果展示</h2> 
<h3><a id="LSTM_109"></a>注意权重共享+LSTM之前使用注意力</h3> 
<p><img src="https://images2.imgbox.com/f3/17/rk8VmQBc_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/af/35/blkm0JiI_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="LSTM_112"></a>注意权重共享+LSTM之后使用注意力</h3> 
<p><img src="https://images2.imgbox.com/ee/a5/e70TDhnm_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/f3/60/UicrtZSS_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="LSTM_115"></a>注意权重不共享+LSTM之前使用注意力</h3> 
<p><img src="https://images2.imgbox.com/e8/f3/tSYlmGzS_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/86/1b/jw9bjJ6x_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="LSTM_118"></a>注意权重不共享+LSTM之后使用注意力</h3> 
<p><img src="https://images2.imgbox.com/9e/58/0oQK3StF_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/a8/ce/7Ow8LFLB_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_120"></a>结果总结</h3> 
<p>  四种情况的模型在验证集上分类准确率都达到了100%,同时人工指定的“关键特征”也被准确的捕捉到了，都是最高。值得注意的是在LSTM之后再用注意力时，会导致有一部分注意力被其他特征分散了，这是因为LSTM之后，特征更为抽象了，更难解释了。<br>   至于注意力层权重共不共享，个人觉得还得具体到问题上来，理论上权重不共享，注意力的刻画就更丰富，但同时参数也变多了，模型速度肯定会受影响，怎样取舍看各自问题。</p> 
<h3><a id="1_123"></a>完整代码(1个文件)</h3> 
<pre><code class="prism language-py"><span class="token keyword">import</span> keras<span class="token punctuation">.</span>backend <span class="token keyword">as</span> K
<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Multiply
<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>core <span class="token keyword">import</span> <span class="token operator">*</span>
<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>recurrent <span class="token keyword">import</span> LSTM
<span class="token keyword">from</span> keras<span class="token punctuation">.</span>models <span class="token keyword">import</span> <span class="token operator">*</span>
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token keyword">def</span> <span class="token function">get_data_recurrent</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> time_steps<span class="token punctuation">,</span> input_dim<span class="token punctuation">,</span> attention_column<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Data generation. x is purely random except that it's first value equals the target y.
    In practice, the network should learn that the target = x[attention_column].
    Therefore, most of its attention should be focused on the value addressed by attention_column.
    :param n: the number of samples to retrieve.
    :param time_steps: the number of time steps of your series.
    :param input_dim: the number of dimensions of each element in the series.
    :param attention_column: the column linked to the target. Everything else is purely random.
    :return: x: model inputs, y: model targets
    """</span>
    x <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>standard_normal<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> time_steps<span class="token punctuation">,</span> input_dim<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">#标准正态分布随机特征值</span>
    y <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span>low<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> high<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">#二分类，随机标签值</span>
    x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> attention_column<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>tile<span class="token punctuation">(</span>y<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> input_dim<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">#将第attention_column个column的值置为标签值</span>
    <span class="token keyword">return</span> x<span class="token punctuation">,</span> y

<span class="token keyword">def</span> <span class="token function">get_activations</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> print_shape_only<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> layer_name<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># Documentation is available online on Github at the address below.</span>
    <span class="token comment"># From: https://github.com/philipperemy/keras-visualize-activations</span>
<span class="token comment">#    print('----- activations -----')</span>
    activations <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    inp <span class="token operator">=</span> model<span class="token punctuation">.</span><span class="token builtin">input</span>
    <span class="token keyword">if</span> layer_name <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        outputs <span class="token operator">=</span> <span class="token punctuation">[</span>layer<span class="token punctuation">.</span>output <span class="token keyword">for</span> layer <span class="token keyword">in</span> model<span class="token punctuation">.</span>layers<span class="token punctuation">]</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        outputs <span class="token operator">=</span> <span class="token punctuation">[</span>layer<span class="token punctuation">.</span>output <span class="token keyword">for</span> layer <span class="token keyword">in</span> model<span class="token punctuation">.</span>layers <span class="token keyword">if</span> layer<span class="token punctuation">.</span>name <span class="token operator">==</span> layer_name<span class="token punctuation">]</span>  <span class="token comment"># all layer outputs</span>
    funcs <span class="token operator">=</span> <span class="token punctuation">[</span>K<span class="token punctuation">.</span>function<span class="token punctuation">(</span><span class="token punctuation">[</span>inp<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span>K<span class="token punctuation">.</span>learning_phase<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>out<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> out <span class="token keyword">in</span> outputs<span class="token punctuation">]</span>  <span class="token comment"># evaluation functions</span>
    layer_outputs <span class="token operator">=</span> <span class="token punctuation">[</span>func<span class="token punctuation">(</span><span class="token punctuation">[</span>inputs<span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">for</span> func <span class="token keyword">in</span> funcs<span class="token punctuation">]</span>
    <span class="token keyword">for</span> layer_activations <span class="token keyword">in</span> layer_outputs<span class="token punctuation">:</span>
        activations<span class="token punctuation">.</span>append<span class="token punctuation">(</span>layer_activations<span class="token punctuation">)</span>
<span class="token comment">#        if print_shape_only:</span>
<span class="token comment">#            print(layer_activations.shape)</span>
<span class="token comment">#        else:</span>
<span class="token comment">#            print(layer_activations)</span>
    <span class="token keyword">return</span> activations

<span class="token keyword">def</span> <span class="token function">attention_3d_block</span><span class="token punctuation">(</span>inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># inputs.shape = (batch_size, time_steps, input_dim)</span>
    input_dim <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>inputs<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    a <span class="token operator">=</span> Permute<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
    a <span class="token operator">=</span> Reshape<span class="token punctuation">(</span><span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> TIME_STEPS<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span> <span class="token comment"># this line is not useful. It's just to know which dimension is what.</span>
    a <span class="token operator">=</span> Dense<span class="token punctuation">(</span>TIME_STEPS<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>
    <span class="token keyword">if</span> SINGLE_ATTENTION_VECTOR<span class="token punctuation">:</span>
        a <span class="token operator">=</span> Lambda<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> K<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>x<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'dim_reduction'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>
        a <span class="token operator">=</span> RepeatVector<span class="token punctuation">(</span>input_dim<span class="token punctuation">)</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>
    a_probs <span class="token operator">=</span> Permute<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'attention_vec'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>
    output_attention_mul <span class="token operator">=</span> Multiply<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">[</span>inputs<span class="token punctuation">,</span> a_probs<span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> output_attention_mul


<span class="token keyword">def</span> <span class="token function">model_attention_applied_after_lstm</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    K<span class="token punctuation">.</span>clear_session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#清除之前的模型，省得压满内存</span>
    inputs <span class="token operator">=</span> Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>TIME_STEPS<span class="token punctuation">,</span> INPUT_DIM<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    lstm_units <span class="token operator">=</span> <span class="token number">32</span>
    lstm_out <span class="token operator">=</span> LSTM<span class="token punctuation">(</span>lstm_units<span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
    attention_mul <span class="token operator">=</span> attention_3d_block<span class="token punctuation">(</span>lstm_out<span class="token punctuation">)</span>
    attention_mul <span class="token operator">=</span> Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>attention_mul<span class="token punctuation">)</span>
    output <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>attention_mul<span class="token punctuation">)</span>
    model <span class="token operator">=</span> Model<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span><span class="token punctuation">[</span>inputs<span class="token punctuation">]</span><span class="token punctuation">,</span> output<span class="token operator">=</span>output<span class="token punctuation">)</span>
    <span class="token keyword">return</span> model


<span class="token keyword">def</span> <span class="token function">model_attention_applied_before_lstm</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    K<span class="token punctuation">.</span>clear_session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#清除之前的模型，省得压满内存</span>
    inputs <span class="token operator">=</span> Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>TIME_STEPS<span class="token punctuation">,</span> INPUT_DIM<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    attention_mul <span class="token operator">=</span> attention_3d_block<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
    lstm_units <span class="token operator">=</span> <span class="token number">32</span>
    attention_mul <span class="token operator">=</span> LSTM<span class="token punctuation">(</span>lstm_units<span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">(</span>attention_mul<span class="token punctuation">)</span>
    output <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>attention_mul<span class="token punctuation">)</span>
    model <span class="token operator">=</span> Model<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span><span class="token punctuation">[</span>inputs<span class="token punctuation">]</span><span class="token punctuation">,</span> output<span class="token operator">=</span>output<span class="token punctuation">)</span>
    <span class="token keyword">return</span> model

SINGLE_ATTENTION_VECTOR <span class="token operator">=</span> <span class="token boolean">False</span>
APPLY_ATTENTION_BEFORE_LSTM <span class="token operator">=</span> <span class="token boolean">True</span>
INPUT_DIM <span class="token operator">=</span> <span class="token number">2</span>
TIME_STEPS <span class="token operator">=</span> <span class="token number">20</span>

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    
    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">1337</span><span class="token punctuation">)</span>  <span class="token comment"># for reproducibility</span>

    <span class="token comment"># if True, the attention vector is shared across the input_dimensions where the attention is applied.</span>

    
    N <span class="token operator">=</span> <span class="token number">300000</span>
    <span class="token comment"># N = 300 -&gt; too few = no training</span>
    inputs_1<span class="token punctuation">,</span> outputs <span class="token operator">=</span> get_data_recurrent<span class="token punctuation">(</span>N<span class="token punctuation">,</span> TIME_STEPS<span class="token punctuation">,</span> INPUT_DIM<span class="token punctuation">)</span>
<span class="token comment">#    for i in range(0,3):</span>
<span class="token comment">#        print(inputs_1[i])</span>
<span class="token comment">#        print(outputs[i])</span>
    <span class="token keyword">if</span> APPLY_ATTENTION_BEFORE_LSTM<span class="token punctuation">:</span>
        m <span class="token operator">=</span> model_attention_applied_before_lstm<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        m <span class="token operator">=</span> model_attention_applied_after_lstm<span class="token punctuation">(</span><span class="token punctuation">)</span>

    m<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer<span class="token operator">=</span><span class="token string">'adam'</span><span class="token punctuation">,</span> loss<span class="token operator">=</span><span class="token string">'binary_crossentropy'</span><span class="token punctuation">,</span> metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    m<span class="token punctuation">.</span>summary<span class="token punctuation">(</span><span class="token punctuation">)</span>

    m<span class="token punctuation">.</span>fit<span class="token punctuation">(</span><span class="token punctuation">[</span>inputs_1<span class="token punctuation">]</span><span class="token punctuation">,</span> outputs<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> validation_split<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>

    attention_vectors <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">300</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        testing_inputs_1<span class="token punctuation">,</span> testing_outputs <span class="token operator">=</span> get_data_recurrent<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> TIME_STEPS<span class="token punctuation">,</span> INPUT_DIM<span class="token punctuation">)</span>
        attention_vector <span class="token operator">=</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>get_activations<span class="token punctuation">(</span>m<span class="token punctuation">,</span>
                                                   testing_inputs_1<span class="token punctuation">,</span>
                                                   print_shape_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                                   layer_name<span class="token operator">=</span><span class="token string">'attention_vec'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment">#        print('attention =', attention_vector)</span>
        <span class="token keyword">assert</span> <span class="token punctuation">(</span>np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>attention_vector<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1.0</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">1e-5</span>
        attention_vectors<span class="token punctuation">.</span>append<span class="token punctuation">(</span>attention_vector<span class="token punctuation">)</span>

    attention_vector_final <span class="token operator">=</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>attention_vectors<span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
    <span class="token comment"># plot part.</span>


    pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>attention_vector_final<span class="token punctuation">,</span> columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'attention (%)'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>kind<span class="token operator">=</span><span class="token string">'bar'</span><span class="token punctuation">,</span>
                                                                         title<span class="token operator">=</span><span class="token string">'Attention Mechanism as '</span>
                                                                               <span class="token string">'a function of input'</span>
                                                                               <span class="token string">' dimensions.'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/21db3c4812243a5d6af8f3eb5de90a59/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">NAS折腾：zeroTier简明教程（组成p2p的虚拟局域网）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e6f1181b159989a9dd7357e78244d400/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【深度学习】 基于Keras的Attention机制代码实现及剖析——Dense&#43;Attention</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>