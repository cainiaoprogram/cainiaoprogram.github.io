<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>各种归一化层（BatchNorm、LayerNorm、InstanceNorm、GroupNorm、Weight Standardization）及其Pytorch实现 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="各种归一化层（BatchNorm、LayerNorm、InstanceNorm、GroupNorm、Weight Standardization）及其Pytorch实现" />
<meta property="og:description" content="BN，LN，IN，GN，WS 从学术化上解释差异：
BatchNorm：batch方向做归一化，算NHW的均值，对小batchsize效果不好；BN主要缺点是对batchsize的大小比较敏感，由于每次计算均值和方差是在一个batch上，所以如果batchsize太小，则计算的均值、方差不足以代表整个数据分布；
LayerNorm：channel方向做归一化，算CHW的均值，主要对RNN作用明显；
InstanceNorm：一个channel内做归一化，算H*W的均值，用在风格化迁移；因为在图像风格化中，生成结果主要依赖于某个图像实例，所以对整个batch归一化不适合图像风格化中，因而对HW做归一化。可以加速模型收敛，并且保持每个图像实例之间的独立。
GroupNorm：将channel方向分group，然后每个group内做归一化，算(C//G)HW的均值；这样与batchsize无关，不受其约束。
SwitchableNorm：将BN、LN、IN结合，赋予权重，让网络自己去学习归一化层应该使用什么方法。
Weight Standardization：权重标准化，2019年约翰霍普金斯大学研究人员提出。
1、BatchNorm torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
torch.nn.BatchNorm3d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
BN图示 参数：
num_features： 来自期望输入的特征数，该期望输入的大小为’batch_size × num_features [× width]’
eps： 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。
momentum： 动态均值和动态方差所使用的动量。默认为0.1。
affine： 布尔值，当设为true，给该层添加可学习的仿射变换参数。
track_running_stats：布尔值，当设为true，记录训练过程中的均值和方差；
实现公式：
大部分深度网络通常都会使用 BN 层去加速训练和帮助模型更好收敛。虽然 BN 层非常实用，但从研究者的角度看，依然有一些非常显眼的缺点。比如（1）我们非常缺乏对于 BN 层成功原因的理解；（2）BN 层仅在 batch size 足够大时才有明显的效果，因此不能用在微批次的训练中。虽然现在已经有专门针对微批次训练设计的归一化方法（GN），但图 1 所示，它很难在大批次训练时媲美 BN 的效果。
2、GroupNorm FAIR 团队的吴育昕和何恺明提出了组归一化（Group Normalization，简称 GN）的方法，GN 将信号通道分成一个个组别，并在每个组别内计算归一化的均值和方差，以进行归一化处理。GN 的计算与批量大小无关，而且在批次大小大幅变化时，精度依然稳定。通常来说，在使用 Batch Normalization（以下将简称 BN）时，采用小批次很难训练一个网络，而对于不使用批次的优化方法来说，效果很难媲美采用大批次BN时的训练结果。当使用 Group Normalization（以下将简称 GN），且 batch size 大小为 1 时，仅需要多写两行代码加入权重标准化方法，就能比肩甚至超越大批次BN时的训练效果。 torch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/c54965a24235528883526d13355f8daa/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-06-05T15:53:30+08:00" />
<meta property="article:modified_time" content="2020-06-05T15:53:30+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">各种归一化层（BatchNorm、LayerNorm、InstanceNorm、GroupNorm、Weight Standardization）及其Pytorch实现</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <blockquote> 
 <p><strong>BN，LN，IN，GN，WS 从学术化上解释差异：</strong><br><strong>BatchNorm：</strong>batch方向做归一化，算NHW的均值，对小batchsize效果不好；BN主要缺点是对batchsize的大小比较敏感，由于每次计算均值和方差是在一个batch上，所以如果batchsize太小，则计算的均值、方差不足以代表整个数据分布；<br><strong>LayerNorm：</strong>channel方向做归一化，算CHW的均值，主要对RNN作用明显；<br><strong>InstanceNorm：</strong>一个channel内做归一化，算H*W的均值，用在风格化迁移；因为在图像风格化中，生成结果主要依赖于某个图像实例，所以对整个batch归一化不适合图像风格化中，因而对HW做归一化。可以加速模型收敛，并且保持每个图像实例之间的独立。<br><strong>GroupNorm：</strong>将channel方向分group，然后每个group内做归一化，算(C//G)HW的均值；这样与batchsize无关，不受其约束。<br><strong>SwitchableNorm：</strong>将BN、LN、IN结合，赋予权重，让网络自己去学习归一化层应该使用什么方法。</p> 
 <p><strong>Weight Standardization：</strong>权重标准化，2019年约翰霍普金斯大学研究人员提出。</p> 
</blockquote> 
<p style="text-align:center;"><img alt="" height="333" src="https://images2.imgbox.com/82/80/gVuVPTQP_o.png" width="1200"></p> 
<p> </p> 
<p style="text-align:center;"><img alt="" height="316" src="https://images2.imgbox.com/f5/98/BPmXJuEe_o.png" width="291"></p> 
<h4><strong>1、BatchNorm</strong></h4> 
<p><span style="color:#86ca5e;"><strong>torch.nn.BatchNorm1d(<em>num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True</em>)<br> torch.nn.BatchNorm2d(<em>num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True</em>)<br> torch.nn.BatchNorm3d(<em>num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True</em>)</strong></span></p> 
<div style="text-align:center;"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/7a/ee/IxumCRWq_o.png"> 
  <figcaption>
    BN图示 
  </figcaption> 
 </figure> 
</div> 
<blockquote> 
 <p>参数：</p> 
 <p>num_features： 来自期望输入的特征数，该期望输入的大小为’<strong>batch_size</strong> × <strong>num_features </strong>[× <strong>width</strong>]’<br> eps： 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。<br> momentum： 动态均值和动态方差所使用的动量。默认为0.1。<br> affine： 布尔值，当设为true，给该层添加可学习的仿射变换参数。<br> track_running_stats：布尔值，当设为true，记录训练过程中的均值和方差；<br>  </p> 
</blockquote> 
<p> <strong>实现公式：</strong><br><img alt="在这里插入图片描述" src="https://images2.imgbox.com/a3/a8/CktUXNC3_o.png"></p> 
<p style="text-indent:33px;">大部分深度网络通常都会使用 BN 层去加速训练和帮助模型更好收敛。<strong>虽然 BN 层非常实用，但从研究者的角度看，依然有一些非常显眼的缺点。</strong>比如（1）我们非常缺乏对于 BN 层成功原因的理解；（2）BN 层仅在 batch size 足够大时才有明显的效果，因此不能用在微批次的训练中。虽然现在已经有专门针对微批次训练设计的归一化方法（GN），但图 1 所示，它很难在大批次训练时媲美 BN 的效果。</p> 
<h4>2、GroupNorm</h4> 
<p style="text-indent:33px;">FAIR 团队的吴育昕和何恺明提出了组归一化（Group Normalization，简称 GN）的方法，GN 将信号通道分成一个个组别，并在每个组别内计算归一化的<strong>均值</strong>和<strong>方差</strong>，以进行归一化处理。GN 的计算与批量大小无关，而且在批次大小大幅变化时，精度依然稳定。通常来说，在使用 Batch Normalization（以下将简称 BN）时，采用小批次很难训练一个网络，而对于不使用批次的优化方法来说，效果很难媲美采用大批次BN时的训练结果。当使用 Group Normalization（以下将简称 GN），且 batch size 大小为 1 时，仅需要多写两行代码加入权重标准化方法，就能比肩甚至超越大批次BN时的训练效果。 </p> 
<p><strong><span style="color:#86ca5e;">torch.nn.GroupNorm(<em>num_groups, num_channels, eps=1e-05, affine=True</em>)</span></strong></p> 
<blockquote> 
 <p>参数：</p> 
 <p>num_groups：需要划分为的groups<br> num_features： 来自期望输入的特征数，该期望输入的大小为’batch_size x num_features [x width]’<br> eps： 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。<br> momentum： 动态均值和动态方差所使用的动量。默认为0.1。<br> affine： 布尔值，当设为true，给该层添加可学习的仿射变换参数。</p> 
</blockquote> 
<p> 实现公式：</p> 
<p><img alt="" src="https://images2.imgbox.com/24/23/uObMb5M2_o.png"></p> 
<p><strong>Tensorflow的代码如下：</strong> </p> 
<p><img alt="" height="316" src="https://images2.imgbox.com/95/63/lAhWNL0W_o.png" width="591"></p> 
<h4>3、InstanceNorm</h4> 
<p><span style="color:#86ca5e;"><strong>torch.nn.InstanceNorm1d(<em>num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False</em>)<br> torch.nn.InstanceNorm2d(<em>num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False</em>)<br> torch.nn.InstanceNorm3d(<em>num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False</em>)</strong></span></p> 
<blockquote> 
 <p>参数：</p> 
 <p>num_features： 来自期望输入的特征数，该期望输入的大小为’batch_size x num_features [x width]’<br> eps： 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。<br> momentum： 动态均值和动态方差所使用的动量。默认为0.1。<br> affine： 布尔值，当设为true，给该层添加可学习的仿射变换参数。<br> track_running_stats：布尔值，当设为true，记录训练过程中的均值和方差；</p> 
</blockquote> 
<p>实现公式：</p> 
<p><img alt="" src="https://images2.imgbox.com/f2/01/ioxaBABC_o.png"></p> 
<h4>4、LayerNorm</h4> 
<p><span style="color:#86ca5e;"><strong>torch.nn.LayerNorm(<em>normalized_shape, eps=1e-05, elementwise_affine=True</em>)</strong></span></p> 
<div style="text-align:center;"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/e8/50/JmFhefry_o.png"> 
  <figcaption>
    LN图示 
  </figcaption> 
 </figure> 
</div> 
<blockquote> 
 <p>参数：</p> 
 <p>normalized_shape： 输入尺寸<br> [∗×normalized_shape[0]×normalized_shape[1]×…×normalized_shape[−1]]<br> eps： 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。<br> elementwise_affine： 布尔值，当设为true，给该层添加可学习的仿射变换参数。</p> 
</blockquote> 
<p>实现公式：</p> 
<p><img alt="" src="https://images2.imgbox.com/a0/d1/FxMjkSgz_o.png"></p> 
<h4><br> 5、LocalResponseNorm</h4> 
<p><span style="color:#86ca5e;"><strong>torch.nn.LocalResponseNorm(<em>size, alpha=0.0001, beta=0.75, k=1.0</em>)</strong></span></p> 
<blockquote> 
 <p>参数：</p> 
 <p>size：用于归一化的邻居通道数<br> alpha：乘积因子，Default: 0.0001<br> beta ：指数，Default: 0.75<br> k：附加因子，Default: 1</p> 
</blockquote> 
<p>实现公式：</p> 
<p><img alt="" src="https://images2.imgbox.com/40/fd/JKA5YYAN_o.png"></p> 
<h4><strong>6、Weight Standardization</strong></h4> 
<blockquote> 
 <p><strong>论文《</strong><a href="https://arxiv.org/pdf/1903.10520v2.pdf" rel="nofollow">Micro-Batch Training with Batch-ChannelNormalization and Weight Standardization</a><strong>》<em> </em></strong><em>JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 20151</em></p> 
 <p><strong>代码</strong> <a href="https://github.com/joe-siyuan-qiao/WeightStandardization">Github/joe-siyuan-qiao / WeightStandardization</a><a href="https://github.com/joe-siyuan-qiao/WeightStandardization"> </a></p> 
</blockquote> 
<p style="text-align:center;"><img alt="" height="186" src="https://images2.imgbox.com/1c/d0/OnJO6sth_o.png" width="206"></p> 
<p style="text-indent:33px;">下图是在<strong>网络前馈</strong>（<span style="color:#3399ea;">青色</span>）和<strong>反向传播</strong>（<span style="color:#f33b45;">红色</span>）时，进行权重梯度标准化的计算表达式：</p> 
<p style="text-align:center;"><img alt="" height="162" src="https://images2.imgbox.com/92/ba/tsJcVQQv_o.png" width="414"></p> 
<p style="text-indent:33px;">以卷积神经网络中的卷积核为例，Pytorch中传统的卷积模块为：</p> 
<p style="text-indent:33px;"><img alt="" height="283" src="https://images2.imgbox.com/52/60/2qXqAyXK_o.png" width="781"></p> 
<p style="text-indent:33px;">         <img alt="" height="168" src="https://images2.imgbox.com/54/3e/FitoMkMD_o.png" width="634"> </p> 
<p style="text-indent:33px;">引入<strong>WS</strong>后的实现为：</p> 
<pre><code class="language-python"># Pytorch
class Conv2d(nn.Conv2d):
    '''
    shape:
    input: (Batch_size, in_channels, H_in, W_in)
    output: ((Batch_size, out_channels, H_out, W_out))
    '''
    def __init__(self, in_channels, out_channels, kernel_size, stride=1,
                 padding=0, dilation=1, groups=1, bias=True):
        super(Conv2d, self).__init__(in_channels, out_channels, kernel_size, stride,
                 padding, dilation, groups, bias)

    def forward(self, x):
        weight = self.weight   #self.weight 的shape为(out_channels, in_channels, kernel_size_w, kernel_size_h)
        weight_mean = weight.mean(dim=1, keepdim=True).mean(dim=2,
                                  keepdim=True).mean(dim=3, keepdim=True)
        weight = weight - weight_mean
        std = weight.view(weight.size(0), -1).std(dim=1).view(-1, 1, 1, 1) + 1e-5
        weight = weight / std.expand_as(weight)
        return F.conv2d(x, weight, self.bias, self.stride,
                        self.padding, self.dilation, self.groups)</code></pre> 
<p style="text-indent:33px;"> </p> 
<h4>附录：</h4> 
<h4>总结及代码测试</h4> 
<blockquote> 
 <p>来源：《<a href="https://mathpretty.com/11223.html" rel="nofollow">BatchNorm, LayerNorm, InstanceNorm和GroupNorm总结</a>》</p> 
</blockquote> 
<p style="text-indent:33px;">下图是对<strong>BatchNorm</strong>, <strong>LayerNorm</strong>, <strong>InstanceNorm</strong>和<strong>GroupNorm</strong>四种Normalization方式的一个汇总(我个人感觉这个图看起来方便一些).</p> 
<ul><li>图中每一个正方体块表示一个数据<span style="color:#e579b6;">(<strong>比如说这里一个正方体就是一个图像</strong>)</span></li><li>每一个正方体中的<strong>C</strong>, <strong>H</strong>, <strong>W</strong>分别表示<strong>channel</strong>(通道个数), <strong>height</strong>(图像的高), <strong>weight</strong>(图像的宽)</li><li>下图介绍了4中Norm的方式, 如Layer Norm中<span style="color:#f33b45;"><strong>NHWC</strong></span><span style="color:#3399ea;"><strong>-----&gt;</strong></span><span style="color:#f33b45;"><strong>N111</strong></span>表示是将<strong>后面的三个进行标准化</strong>, 不与<strong>batchsize</strong>有关.</li><li>我们可以看到, 后面的<strong>LayerNorm, InstanceNorm和GroupNorm</strong>这三种方式都<strong>是和Batchsize是没有关系的。</strong></li></ul> 
<p style="text-align:center;"><img alt="" height="864" src="https://images2.imgbox.com/ea/46/41D0dEAZ_o.png" width="526"></p> 
<p style="text-indent:33px;">下面我们使用一个(2, 2, 4)的数据来举一个例子, 我们可以将其看成<strong>有2个图像组成的单通道的图像，</strong></p> 
<p><strong>（1）生成测试使用数据</strong></p> 
<p style="text-indent:33px;">我们首先生成测试使用的数据, 数据的大小为<strong>(2, 2, 4)；</strong></p> 
<pre><code class="language-python">x_test = np.array([[[1,2,-1,1],[3,4,-2,2]],
                   [[1,2,-1,1],[3,4,-2,2]]])
x_test = torch.from_numpy(x_test).float()
x_test
"""
tensor([[[ 1.,  2., -1.,  1.],
         [ 3.,  4., -2.,  2.]],
        [[ 1.,  2., -1.,  1.],
         [ 3.,  4., -2.,  2.]]])
"""</code></pre> 
<p><strong>（2）测试LayerNorm与GroupNorm</strong></p> 
<p style="text-indent:33px;">关于这里的计算的细节, 会在后面的计算细节描述部分进行叙述. 这里就看一下如何使用Pytorch来进行计算, 和最终计算得到的结果。</p> 
<p style="text-indent:33px;"><strong>LayerNorm</strong>就是对(2, <strong><span style="color:#f33b45;">2</span>, <span style="color:#f33b45;">4</span></strong>), 后面这一部分进行整个的标准化。可以理解为对整个图像进行标准化。</p> 
<pre><code class="language-python">m = nn.LayerNorm(normalized_shape = [2,4])
output = m(x_test)
output
"""
tensor([[[-0.1348,  0.4045, -1.2136, -0.1348],
         [ 0.9439,  1.4832, -1.7529,  0.4045]],
        [[-0.1348,  0.4045, -1.2136, -0.1348],
         [ 0.9439,  1.4832, -1.7529,  0.4045]]], grad_fn=&lt;AddcmulBackward&gt;)
"""</code></pre> 
<p style="text-indent:33px;">当<strong>GroupNorm中group</strong>的数量是1的时候, 是与上面的LayerNorm是等价的.</p> 
<pre><code class="language-python"># Separate 2 channels into 1 groups (equivalent with LayerNorm)
m = nn.GroupNorm(num_groups=1, num_channels=2, affine=False)
output = m(x_test)
output
"""
tensor([[[-0.1348,  0.4045, -1.2136, -0.1348],
         [ 0.9439,  1.4832, -1.7529,  0.4045]],
        [[-0.1348,  0.4045, -1.2136, -0.1348],
         [ 0.9439,  1.4832, -1.7529,  0.4045]]])
"""</code></pre> 
<p> </p> 
<p><strong>（3）测试InstanceNorm和GroupNorm</strong></p> 
<p style="text-indent:33px;"><strong>InstanceNorm</strong>就是对(2, 2,<span style="color:#f33b45;"> <strong>4</strong></span>), 标红的这一部分进行Norm。</p> 
<pre><code class="language-python">m = nn.InstanceNorm1d(num_features=2)
output = m(x_test)
output
"""
tensor([[[ 0.2294,  1.1471, -1.6059,  0.2294],
         [ 0.5488,  0.9879, -1.6465,  0.1098]],
        [[ 0.2294,  1.1471, -1.6059,  0.2294],
         [ 0.5488,  0.9879, -1.6465,  0.1098]]])
"""</code></pre> 
<p style="text-indent:33px;">上面这种InstanceNorm等价于当<strong>GroupNorm</strong>时<strong>num_groups的数量等于num_channel</strong>的数量。</p> 
<pre><code class="language-python"># Separate 2 channels into 2 groups (equivalent with InstanceNorm)
m = nn.GroupNorm(num_groups=2, num_channels=2, affine=False)
output = m(x_test)
output
"""
tensor([[[ 0.2294,  1.1471, -1.6059,  0.2294],
         [ 0.5488,  0.9879, -1.6465,  0.1098]],
        [[ 0.2294,  1.1471, -1.6059,  0.2294],
         [ 0.5488,  0.9879, -1.6465,  0.1098]]])
"""</code></pre> 
<p><strong>（4）计算细节描述</strong></p> 
<p style="text-indent:33px;">我们看一下在上面的LayerNorm和InstanceNorm中的结果是如何计算出来的. <strong>我们只看第一行第一列的数据1进行标准化的过程</strong>. 下面是详细的计算的过程(这里的计算结果与上面直接计算的结果是相同的)。</p> 
<p style="text-align:center;"><img alt="" height="461" src="https://images2.imgbox.com/cc/5c/IF4AKEG1_o.png" width="540"></p> 
<p> </p> 
<p> </p> 
<p><strong>（5）每一种方式适合的场景</strong></p> 
<ul><li><strong>batchNorm</strong>是在batch上，对小batchsize效果不好；</li><li><strong>layerNorm</strong>在通道方向上，主要对RNN作用明显；</li><li><strong>instanceNorm</strong>在图像像素上，用在风格化迁移；</li><li><strong>GroupNorm</strong>将channel分组，然后再做归一化, 在batchsize&lt;16的时候, 可以使用这种归一化。</li></ul> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8ee39b15fdeb295e903bac091f6d5852/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【Uni-App】出现Cannot read property ’apply’ of undefined错误</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c5398c757d69a718e8a597be4af306ad/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">ROS中对point_cloud旋转平移</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>