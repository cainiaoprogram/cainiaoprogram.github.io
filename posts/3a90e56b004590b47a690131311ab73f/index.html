<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>RTMO~ - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="RTMO~" />
<meta property="og:description" content="RTMO 是基于 MMPose 的单阶段实时人体姿态估计模型，它结合了坐标回归与 YOLOX 检测框架，在多人场景下实现了领先的速度与精度。RTMO 在 CrowdPose 数据集上首次达到了 80&#43; 的 AP，是多人拥挤场景当前效果最好的实时模型。
实时高精度的单阶段人体姿态估计算法 RTMO 正式开源！RTMO 结合了坐标回归策略与 YOLOX 检测框架，克服了现有的单阶段人体姿态估计模型精度与速度难以兼得的难题。RTMO 具有两个突出的优势：
在密集的多人场景中，速度和精度均领先。相比传统的二阶段模型如 RTMPose，RTMO 的速度受画面中的人数的影响可以忽略不计。同时它在多个多人密集场景的数据集上获得了最高精度，如在 CrowdPose 数据集上取得83.8 AP，较以往最佳模型提升 5.3 AP。
模型简单易部署。作为端到端的单阶段模型，RTMO 可以一条命令完成推理和部署，无需额外的人体检测器网络，大大简化了使用流程。
项目主页：https://github.com/open-mmlab/mmpose/tree/main/projects/rtmo
论文链接：https://arxiv.org/abs/2312.07526
效果展示
让我们一起先看看 RTMO 在多人场景中的检测效果（latency 为 NVIDIA 3090 TensorRT-FP16 测试结果）
图1 推理效果图1
图2 推理效果图2
实时姿态估计模型的性能对比
现有的实时姿态估计模型主要包括二阶段 top-down 架构的轻量级模型（例如 RTMPose）以及基于 YOLO 架构的单阶段模型（例如 YOLO-Pose, YOLOv8-Pose）。我们在 CPU 和 GPU 上分别比较了它们的速度和精度。
图3 CPU 上实时姿态估计模型的速度-精度对比图
图4 GPU 上实时姿态估计模型的速度-精度对比图
相较其他实时单阶段模型，RTMO 在性能上全面领先，达到了和二阶段模型 RTMPose 差不多的精度。而相比二阶段模型，RTMO 在画面中人数较多时有速度优势。在相同精度要求下，当画面中人数超过 4(cpu)/2(gpu) 时，RTMO 的推理速度就已经超越了 RTMPose。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/3a90e56b004590b47a690131311ab73f/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-05T22:22:49+08:00" />
<meta property="article:modified_time" content="2024-01-05T22:22:49+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">RTMO~</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>RTMO 是基于 MMPose 的单阶段实时人体姿态估计模型，它结合了坐标回归与 YOLOX 检测框架，在多人场景下实现了领先的速度与精度。RTMO 在 CrowdPose 数据集上首次达到了 80+ 的 AP，是多人拥挤场景当前效果最好的实时模型。</p> 
<p>实时高精度的单阶段人体姿态估计算法 RTMO 正式开源！RTMO 结合了坐标回归策略与 YOLOX 检测框架，克服了现有的单阶段人体姿态估计模型精度与速度难以兼得的难题。RTMO 具有两个突出的优势：</p> 
<ul><li> <p>在密集的多人场景中，速度和精度均领先。相比传统的二阶段模型如 RTMPose，RTMO 的速度受画面中的人数的影响可以忽略不计。同时它在多个多人密集场景的数据集上获得了最高精度，如在 CrowdPose 数据集上取得83.8 AP，较以往最佳模型提升 5.3 AP。</p> </li><li> <p>模型简单易部署。作为端到端的单阶段模型，RTMO 可以一条命令完成推理和部署，无需额外的人体检测器网络，大大简化了使用流程。</p> </li></ul> 
<p>项目主页：https://github.com/open-mmlab/mmpose/tree/main/projects/rtmo</p> 
<p>论文链接：https://arxiv.org/abs/2312.07526</p> 
<p><strong>效果展示</strong></p> 
<p>让我们一起先看看 RTMO 在多人场景中的检测效果（latency 为 NVIDIA 3090 TensorRT-FP16 测试结果）</p> 
<p><img alt="" height="149" src="https://images2.imgbox.com/b5/e6/HEayUvOm_o.gif" width="264"></p> 
<p>图1 推理效果图1</p> 
<p><img alt="" height="149" src="https://images2.imgbox.com/aa/89/VhC2i6tD_o.gif" width="264"></p> 
<p>图2 推理效果图2</p> 
<p><strong>实时姿态估计模型的性能对比</strong></p> 
<p>现有的实时姿态估计模型主要包括二阶段 top-down 架构的轻量级模型（例如 RTMPose）以及基于 YOLO 架构的单阶段模型（例如 YOLO-Pose, YOLOv8-Pose）。我们在 CPU 和 GPU 上分别比较了它们的速度和精度。</p> 
<p><img alt="" height="1065" src="https://images2.imgbox.com/39/25/tOlKkq0i_o.png" width="1080"></p> 
<p>图3 CPU 上实时姿态估计模型的速度-精度对比图</p> 
<p><img alt="" height="1065" src="https://images2.imgbox.com/d6/14/advGdYhw_o.png" width="1080"></p> 
<p>图4 GPU 上实时姿态估计模型的速度-精度对比图</p> 
<p>相较其他实时单阶段模型，RTMO 在性能上全面领先，达到了和二阶段模型 RTMPose 差不多的精度。而相比二阶段模型，RTMO 在画面中人数较多时有速度优势。在相同精度要求下，当画面中人数超过 4(cpu)/2(gpu) 时，RTMO 的推理速度就已经超越了 RTMPose。</p> 
<p><strong>多人场景下的领先性能</strong></p> 
<p>多人加拥挤场景一直是姿态估计领域的难题。RTMO 在训练过程中使用了 MixUp 数据增强，模拟了拥挤的场景；在正负样本分配的过程中考虑了姿态估计的准确度，减少了关键点错位造成的误差；再加上 MMPose 多数据集训练的加持。因此，RTMO 在多人拥挤场景下依旧有很高的精度。在针对这类场景的 CrowdPose 和 OCHuman 两个数据集上，RTMO 都取得了当前最高的精度。</p> 
<p><img alt="" height="522" src="https://images2.imgbox.com/6d/c8/xnpTpCng_o.png" width="1080"></p> 
<p>图5 CrowdPose 测试集 SOTA 模型精度</p> 
<p>在 CrowdPose 上，RTMO-l 取得了 83.8 的 AP，相比之前的最佳模型有 5.3 AP 的提升。谁能想到这是一个能跑 100+ FPS 的轻量级模型呢？</p> 
<p><img alt="" height="604" src="https://images2.imgbox.com/b4/60/ERELDhBK_o.png" width="1080"></p> 
<p>图6 OCHuman 数据集 SOTA 模型精度</p> 
<p>在 OCHuman 上，RTMO 同样取得了当前最高的精度（不使用标注检测框的情况下）。</p> 
<p><strong>便捷的推理接口</strong></p> 
<p>得益于 MMPose 框架便捷的推理接口，只需要一行代码，我们就可以调用 RTMO 模型进行推理。</p> 
<p><img alt="" height="137" src="https://images2.imgbox.com/4a/0a/MGupRN7H_o.png" width="857"></p> 
<p>图7 一行代码用 RTMO 完成多人姿态估计</p> 
<p>在 OpenXLab 应用平台上，RTMPose 应用也已集成 RTMO。无需本地安装，在浏览器中就能在线使用 RTMO 处理图像、视频。</p> 
<p>RTMPose 应用链接：https://openxlab.org.cn/apps/detail/mmpose/RTMPose</p> 
<p><img alt="" height="638" src="https://images2.imgbox.com/b0/d4/6SnuCPxG_o.png" width="1010"></p> 
<p>图8 在线试用 RTMO</p> 
<p><strong>多平台部署支持</strong></p> 
<p>对于需要在业务侧部署模型的小伙伴，RTMO 也带来了福音。</p> 
<p>不同于二阶段姿态估计模型，RTMO 无需使用人体检测器预先获取人体检测框，因此它的部署相比 RTMPose 要更加简单。RTMO 项目主页介绍了如何使用 MMDeploy 部署 RTMO 模型，无论是用 ONNX 还是 TensorRT 后端都可以一键搞定。      whaosoft <a href="https://so.csdn.net/so/search?q=aiot&amp;spm=1001.2101.3001.7020" title="aiot">aiot</a> <a href="http://143ai.com/" rel="nofollow" title="http://143ai.com">http://143ai.com</a></p> 
<p><img alt="" height="832" src="https://images2.imgbox.com/ee/f7/RSIfaYdQ_o.png" width="868"></p> 
<p>图9 RTMO 部署教程</p> 
<p>此外，我们还提供了开箱即用的轻量推理库 rtmlib （https://github.com/Tau-J/rtmlib）</p> 
<ul><li> <p>无需安装 mmcv，mmengine，mmpose 等一系列训练库，有 opencv 就能推理</p> </li><li> <p>超级友好简洁的推理和可视化接口</p> </li><li> <p>自动下载和缓存 onnx 模型</p> </li><li> <p>支持 RTMPose 全系列官方及衍生模型：RTMPose，DWPose，RTMO，RTMW etc.</p> </li></ul> 
<p>代码样例：</p> 
<pre><code class="language-python">import cv2

from rtmlib import Body, draw_skeleton

device = 'cpu'  # cpu, cuda
backend = 'onnxruntime'  # opencv, onnxruntime, openvino
img = cv2.imread('./demo.jpg')

openpose_skeleton = False  # True for openpose-style, False for mmpose-style

body = Body(
    pose='rtmo',
    to_openpose=openpose_skeleton,
    mode='balanced',  # balanced, performance, lightweight
    backend=backend,
    device=device)

keypoints, scores = body(img)

# visualize

# if you want to use black background instead of original image,
# img_show = np.zeros(img_show.shape, dtype=np.uint8)

img_show = draw_skeleton(img_show, keypoints, scores, kpt_thr=0.5)

cv2.imshow('img', img_show)
cv2.waitKey()</code></pre> 
<p><strong>One More Thing</strong></p> 
<p>RTMPose 家族的全身姿态估计模型 RTMW 也进行了全面更新，基于 DWPose 蒸馏方案（同样已集成在 MMPose 中）进行了加强，相较于 alpha 版本中 RTMW-x 取得 70.2 mAP 精度，蒸馏得到的 RTMW-l 用 60% 的计算量取得了 70.1 mAP。</p> 
<p><img alt="" height="159" src="https://images2.imgbox.com/9d/1c/tl5NBPLV_o.gif" width="141"></p> 
<p>图10 DWPose (上) 与 RTMW (下) 效果对比动图</p> 
<p><img alt="" height="610" src="https://images2.imgbox.com/a4/03/4M4Pa19L_o.png" width="1080"></p> 
<p><img alt="" height="610" src="https://images2.imgbox.com/ab/d2/8NoDQbY1_o.png" width="1080"></p> 
<p>图11  DWPose (上) 与 RTMW (下) 效果对比静图</p> 
<p><strong>总结</strong></p> 
<p>RTMO 是基于 MMPose 的单阶段实时人体姿态估计模型，它结合了坐标回归与 YOLOX 检测框架，在多人场景下实现了领先的速度与精度。RTMO 在 CrowdPose 数据集上首次达到了 80+ 的 AP，是多人拥挤场景当前效果最好的实时模型。作为单阶段模型，RTMO 推理部署简单方便。我们还提供了 Python 推理接口、多平台部署方案，开发者可以快速上手使用。诚挚欢迎对人体姿态估计感兴趣的小伙伴来试用 RTMO，提出宝贵意见！</p> 
<p>项目主页：https://github.com/open-mmlab/mmpose/tree/main/projects/rtmo</p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/45eed431ff7d3908a3849b8c7dfe1da4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Python识别图片中的文字</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b2e585601070b9fee74ec0074f3a4c87/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【书生·浦语大模型实战营01】《书生·浦语大模型全链路开源体系》</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>