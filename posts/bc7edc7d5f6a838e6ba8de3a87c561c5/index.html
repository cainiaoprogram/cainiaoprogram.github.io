<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>机器学习--朴素贝叶斯分类器 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="机器学习--朴素贝叶斯分类器" />
<meta property="og:description" content="第1关：条件概率 什么是条件概率
概率指的是某一事件A发生的可能性，表示为P(A)。而条件概率指的是某一事件A已经发生了条件下，另一事件B发生的可能性，表示为P(B|A)，举个例子：
今天有25%的可能性下雨，即P(下雨)=0.25; 今天75%的可能性是晴天，即P(晴天)=0.75; 如果下雨，我有75%的可能性穿外套，即P(穿外套|下雨)=0.75; 如果下雨，我有25%的可能性穿T恤，即P(穿T恤|下雨)=0.25;
从上述例子可以看出，条件概率描述的是|右边的事件已经发生之后，左边的事件发生的可能性，而不是两个事件同时发生的可能性！
1、P(AB)表示的是事件A与事件B同时发生的概率，P(A|B)表示的是事件B已经发生的条件下，事件A发生的概率。
A、对 B、错 2、从1,2,...,15中小明和小红两人各任取一个数字，现已知小明取到的数字是5的倍数，请问小明取到的数大于小红取到的数的概率是多少？
A、7/14 B、8/14 C、9/14 D、10/14 答案：1、A 2、C 第2关：贝叶斯公式 全概率公式：
当为了达到某种目的，但是达到目的有很多种方式，如果想知道通过所有方式能够达到目的的概率是多少的话，就需要用到全概率公式（上面的例子就是这种情况！）。全概率公式的定义如下：
若事件B1​,B2​,...,Bn​两两互不相容，并且其概率和为1。那么对于任意一个事件C都满足：
P(C)=P(B1​)P(C∣B1​)&#43;...&#43;P(Bn​)P(C∣Bn​)=i=1∑n​P(Bi​)P(C∣Bi​)
贝叶斯公式：
当已知引发事件发生的各种原因的概率，想要算该事件发生的概率时，我们可以用全概率公式。但如果现在反过来，已知事件已经发生了，但想要计算引发该事件的各种原因的概率时，我们就需要用到贝叶斯公式了。
贝叶斯公式定义如下，其中A表示已经发生的事件，Bi为导致事件A发生的第i个原因：
P(Bi​∣A)=∑i=1n​P(A∣Bi​)P(Bi​)P(A∣Bi​)P(Bi​)​
贝叶斯公式看起来比较复杂，其实非常简单，分子部分是乘法定理，分母部分是全概率公式（分母等于P(A)）。
如果我们对贝叶斯公式进行一个简单的数学变换（两边同时乘以分母，再两边同时除以P(Bi)）。就能够得到如下公式：
P(A∣Bi​)=P(Bi​)P(Bi​∣A)P(A)​
这个公式是朴素贝叶斯分类算法的核心数学公式。
1、对以往数据分析结果表明，当机器调整得良好时，产品的合格率为98%，而当机器发生某种故障时，产品的合格率为55%。每天早上机器开动时，机器调整得良好的概率为95%。计算已知某日早上第一件产品是合格时，机器调整得良好的概率是多少？
A、0.94 B、0.95 C、0.96 D、0.97 2、一批产品共8件，其中正品6件，次品2件。现不放回地从中取产品两次，每次一件，求第二次取得正品的概率。
A、1/4 B、1/2 C、3/4 D、1 答案：1、D 2、C 第3关：朴素贝叶斯分类算法流程
import numpy as np class NaiveBayesClassifier(object): def __init__(self): &#39;&#39;&#39; self.label_prob表示每种类别在数据中出现的概率 例如，{0:0.333, 1:0.667}表示数据中类别0出现的概率为0.333，类别1的概率为0.667 &#39;&#39;&#39; self.label_prob = {} &#39;&#39;&#39; self.condition_prob表示每种类别确定的条件下各个特征出现的概率 例如训练数据集中的特征为 [[2, 1, 1], [1, 2, 2], [2, 2, 2], [2, 1, 2], [1, 2, 3]] 标签为[1, 0, 1, 0, 1] 那么当标签为0时第0列的值为1的概率为0." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/bc7edc7d5f6a838e6ba8de3a87c561c5/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-07T22:52:45+08:00" />
<meta property="article:modified_time" content="2023-06-07T22:52:45+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">机器学习--朴素贝叶斯分类器</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h4>第1关：条件概率</h4> 
<p>什么是条件概率</p> 
<p>概率指的是某一事件<code>A</code>发生的可能性，表示为<code>P(A)</code>。而条件概率指的是某一事件<code>A</code>已经发生了条件下，另一事件<code>B</code>发生的可能性，表示为<code>P(B|A)</code>，举个例子：</p> 
<p>今天有<code>25%</code>的可能性下雨，即<code>P(下雨)=0.25</code>; 今天<code>75%</code>的可能性是晴天，即<code>P(晴天)=0.75</code>; 如果下雨，我有<code>75%</code>的可能性穿外套，即<code>P(穿外套|下雨)=0.75</code>; 如果下雨，我有<code>25%</code>的可能性穿T恤，即<code>P(穿T恤|下雨)=0.25</code>;</p> 
<p>从上述例子可以看出，条件概率描述的是<code>|</code>右边的事件已经发生之后，左边的事件发生的可能性，<strong>而不是两个事件同时发生的可能性！</strong></p> 
<p></p> 
<ul><li> <p>1、<code>P(AB)</code>表示的是事件<code>A</code>与事件<code>B</code>同时发生的概率，<code>P(A|B)</code>表示的是事件<code>B</code>已经发生的条件下，事件<code>A</code>发生的概率。</p> A、对   B、错</li><li> <p>2、从<code>1,2,...,15</code>中小明和小红两人各任取一个数字，现已知小明取到的数字是<code>5</code>的倍数，请问小明取到的数大于小红取到的数的概率是多少？</p> A、7/14   B、8/14  C、9/14   D、10/14</li></ul> 
<p>答案：1、A        2、C </p> 
<h4>第2关：贝叶斯公式</h4> 
<p><strong>全概率公式：</strong></p> 
<p>当为了达到某种目的，但是达到目的有很多种方式，如果想知道通过所有方式能够达到目的的概率是多少的话，就需要用到<strong>全概率公式</strong>（<strong>上面的例子就是这种情况！</strong>）。全概率公式的定义如下：</p> 
<p>若事件B1​,B2​,...,Bn​两两互不相容，并且其概率和为<code>1</code>。那么对于任意一个事件<code>C</code>都满足：</p> 
<p>P(C)=P(B1​)P(C∣B1​)+...+P(Bn​)P(C∣Bn​)=i=1∑n​P(Bi​)P(C∣Bi​)</p> 
<p><strong>贝叶斯公式：</strong></p> 
<p>当已知引发事件发生的各种原因的概率，想要算该事件发生的概率时，我们可以用<strong>全概率公式</strong>。但如果现在反过来，已知事件已经发生了，但想要计算引发该事件的各种原因的概率时，我们就需要用到<strong>贝叶斯公式</strong>了。</p> 
<p>贝叶斯公式定义如下，其中<code>A</code>表示已经发生的事件，<code>Bi</code>为导致事件<code>A</code>发生的第<code>i</code>个原因：</p> 
<p>P(Bi​∣A)=∑i=1n​P(A∣Bi​)P(Bi​)P(A∣Bi​)P(Bi​)​</p> 
<p>贝叶斯公式看起来比较复杂，其实非常简单，分子部分是<strong>乘法定理</strong>，分母部分是<strong>全概率公式</strong>（分母等于<code>P(A)</code>）。</p> 
<p>如果我们对贝叶斯公式进行一个简单的数学变换（两边同时乘以分母，再两边同时除以<code>P(Bi)</code>）。就能够得到如下公式：</p> 
<p>P(A∣Bi​)=P(Bi​)P(Bi​∣A)P(A)​</p> 
<p>这个公式是朴素贝叶斯分类算法的核心数学公式。</p> 
<ul><li> <p>1、对以往数据分析结果表明，当机器调整得良好时，产品的合格率为<code>98%</code>，而当机器发生某种故障时，产品的合格率为<code>55%</code>。每天早上机器开动时，机器调整得良好的概率为<code>95%</code>。计算已知某日早上第一件产品是合格时，机器调整得良好的概率是多少？</p> A、0.94    B、0.95    C、0.96    D、0.97</li><li> <p>2、一批产品共<code>8</code>件，其中正品<code>6</code>件，次品<code>2</code>件。现不放回地从中取产品两次，每次一件，求第二次取得正品的概率。</p> A、1/4    B、1/2    C、3/4    D、1</li></ul> 
<p>答案：1、D   2、C        </p> 
<p></p> 
<p><strong>第3关：朴素贝叶斯分类算法流程</strong></p> 
<pre><code class="hljs">import numpy as np
class NaiveBayesClassifier(object):
    def __init__(self):
        '''
        self.label_prob表示每种类别在数据中出现的概率
        例如，{0:0.333, 1:0.667}表示数据中类别0出现的概率为0.333，类别1的概率为0.667
        '''
        self.label_prob = {}
        '''
        self.condition_prob表示每种类别确定的条件下各个特征出现的概率
        例如训练数据集中的特征为 [[2, 1, 1],
                              [1, 2, 2],
                              [2, 2, 2],
                              [2, 1, 2],
                              [1, 2, 3]]
        标签为[1, 0, 1, 0, 1]
        那么当标签为0时第0列的值为1的概率为0.5，值为2的概率为0.5;
        当标签为0时第1列的值为1的概率为0.5，值为2的概率为0.5;
        当标签为0时第2列的值为1的概率为0，值为2的概率为1，值为3的概率为0;
        当标签为1时第0列的值为1的概率为0.333，值为2的概率为0.666;
        当标签为1时第1列的值为1的概率为0.333，值为2的概率为0.666;
        当标签为1时第2列的值为1的概率为0.333，值为2的概率为0.333,值为3的概率为0.333;
        因此self.label_prob的值如下：     
        {
            0:{
                0:{
                    1:0.5
                    2:0.5
                }
                1:{
                    1:0.5
                    2:0.5
                }
                2:{
                    1:0
                    2:1
                    3:0
                }
            }
            1:
            {
                0:{
                    1:0.333
                    2:0.666
                }
                1:{
                    1:0.333
                    2:0.666
                }
                2:{
                    1:0.333
                    2:0.333
                    3:0.333
                }
            }
        }
        '''
        self.condition_prob = {}
    def fit(self, feature, label):
        '''
        对模型进行训练，需要将各种概率分别保存在self.label_prob和self.condition_prob中
        :param feature: 训练数据集所有特征组成的ndarray
        :param label:训练数据集中所有标签组成的ndarray
        :return: 无返回
        '''

        #********* Begin *********#
        row_num=len(feature)
        col_num=len(feature[0])
        for c in label:
            if c in self.label_prob:
                self.label_prob[c]+=1
            else:
                self.label_prob[c]=1;
        for key in  self.label_prob.keys():
            self.label_prob[key]/=row_num
            self.condition_prob[key]={}
            for i in range(col_num):
                self.condition_prob[key][i]={}
                for k in np.unique(feature[:,i],axis=0):
                    self.condition_prob[key][i][k]=0
        for i in range(len(feature)):
            for j in range(len(feature[i])):
                if feature[i][j] in self.condition_prob[label[i]]:
                    self.condition_prob[label[i]][j][feature[i][j]]+=1
                else:
                    self.condition_prob[label[i]][j][feature[i][j]]=1
        for label_key in self.condition_prob.keys():
            for k in self.condition_prob[label_key].keys():
                total=0
                for v in self.condition_prob[label_key][k].values():
                    total +=v
                for kk in self.condition_prob[label_key][k].keys():
                    self.condition_prob[label_key][k][kk] /=total

        #********* End *********#

    def predict(self, feature):
        '''
        对数据进行预测，返回预测结果
        :param feature:测试数据集所有特征组成的ndarray
        :return:
        '''
        # ********* Begin *********#
        result =[]
        for i,f in enumerate(feature):
            prob=np.zeros(len(self.label_prob.keys()))
            ii=0
            for label,label_prob in self.label_prob.items():
                prob[ii]=label_prob
                for  j  in range(len(feature[0])):
                    prob[ii] *= self.condition_prob[label][j][f[j]]
                ii+=1
            result.append(list(self.label_prob.keys())[np.argmax(prob)])
        return np.array(result)
        #********* End *********#
    
</code></pre> 
<p><strong>第4关：拉普拉斯平滑 </strong></p> 
<pre><code class="hljs">import numpy as np

class NaiveBayesClassifier(object):
    def __init__(self):
        '''
        self.label_prob表示每种类别在数据中出现的概率
        例如，{0:0.333, 1:0.667}表示数据中类别0出现的概率为0.333，类别1的概率为0.667
        '''
        self.label_prob = {}
        '''
        self.condition_prob表示每种类别确定的条件下各个特征出现的概率
        例如训练数据集中的特征为 [[2, 1, 1],
                              [1, 2, 2],
                              [2, 2, 2],
                              [2, 1, 2],
                              [1, 2, 3]]
        标签为[1, 0, 1, 0, 1]
        那么当标签为0时第0列的值为1的概率为0.5，值为2的概率为0.5;
        当标签为0时第1列的值为1的概率为0.5，值为2的概率为0.5;
        当标签为0时第2列的值为1的概率为0，值为2的概率为1，值为3的概率为0;
        当标签为1时第0列的值为1的概率为0.333，值为2的概率为0.666;
        当标签为1时第1列的值为1的概率为0.333，值为2的概率为0.666;
        当标签为1时第2列的值为1的概率为0.333，值为2的概率为0.333,值为3的概率为0.333;
        因此self.label_prob的值如下：     
        {
            0:{
                0:{
                    1:0.5
                    2:0.5
                }
                1:{
                    1:0.5
                    2:0.5
                }
                2:{
                    1:0
                    2:1
                    3:0
                }
            }
            1:
            {
                0:{
                    1:0.333
                    2:0.666
                }
                1:{
                    1:0.333
                    2:0.666
                }
                2:{
                    1:0.333
                    2:0.333
                    3:0.333
                }
            }
        }
        '''
        self.condition_prob = {}

    def fit(self, feature, label):
        '''
        对模型进行训练，需要将各种概率分别保存在self.label_prob和self.condition_prob中
        :param feature: 训练数据集所有特征组成的ndarray
        :param label:训练数据集中所有标签组成的ndarray
        :return: 无返回
        '''

        #********* Begin *********#
        row_num=len(feature)
        col_num=len(feature[0])
        unique_label_count=len(set(label))
        for c in label:
            if c in self.label_prob:
                self.label_prob[c]+=1
            else:
                self.label_prob[c]=1
        for key in self.label_prob.keys():
            self.label_prob[key]+=1
            self.label_prob[key]/=(unique_label_count+row_num)
            self.condition_prob[key]={}
            for i in range(col_num):
                self.condition_prob[key][i]={}
                for k in np.unique(feature[:,i],axis=0):
                    self.condition_prob[key][i][k]=1
        for i in range(len(feature)):
            for j in range(len(feature[i])):
                if feature[i][j] in self.condition_prob[label[i]]:
                    self.condition_prob[label[i]][j][feature[i][j]]+=1
        
        for label_key in self.condition_prob.keys():
            for k in self.condition_prob[label_key].keys():
                total =len(self.condition_prob[label_key].keys())
                for v in self.condition_prob[label_key][k].values():
                    total+=v
                for kk in self.condition_prob[label_key][k].keys():
                    self.condition_prob[label_key][k][kk]/=total
                #********* End *********#


    def predict(self, feature):
        '''
        对数据进行预测，返回预测结果
        :param feature:测试数据集所有特征组成的ndarray
        :return:
        '''

        result = []
        # 对每条测试数据都进行预测
        for i, f in enumerate(feature):
            # 可能的类别的概率
            prob = np.zeros(len(self.label_prob.keys()))
            ii = 0
            for label, label_prob in self.label_prob.items():
                # 计算概率
                prob[ii] = label_prob
                for j in range(len(feature[0])):
                    prob[ii] *= self.condition_prob[label][j][f[j]]
                ii += 1
            # 取概率最大的类别作为结果
            result.append(list(self.label_prob.keys())[np.argmax(prob)])
        return np.array(result)
        
</code></pre> 
<p><strong>第5关：sklearn中的朴素贝叶斯分类器 </strong></p> 
<pre><code class="hljs">from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfTransformer

def news_predict(train_sample, train_label, test_sample):
    '''
    训练模型并进行预测，返回预测结果
    :param train_sample:原始训练集中的新闻文本，类型为ndarray
    :param train_label:训练集中新闻文本对应的主题标签，类型为ndarray
    :param test_sample:原始测试集中的新闻文本，类型为ndarray
    :return 预测结果，类型为ndarray
    '''
    #********* Begin *********#
    vec=CountVectorizer()
    train_sample=vec.fit_transform(train_sample)
    test_sample=vec.transform(test_sample)
    
    tfidf=TfidfTransformer()

    train_sample =tfidf.fit_transform(train_sample)
    test_sample=tfidf.transform(test_sample)
    mnb=MultinomialNB(alpha=0.01)
    mnb.fit(train_sample,train_label)
    predict=mnb.predict(test_sample)
    return predict
    #********* End *********#

</code></pre> 
<p> </p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/38c1ec6c48a67572ef7055bb1a714e93/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">别人用开源项目赚钱，我却吃上牢饭？</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/3247672ff9b5f6f1cab5b46d3bc419be/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">VC&#43;&#43;中线程局部存储（TLS）的使用</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>