<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Elasticsearch：与多个 PDF 聊天 | LangChain Python 应用教程（免费 LLMs 和嵌入） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Elasticsearch：与多个 PDF 聊天 | LangChain Python 应用教程（免费 LLMs 和嵌入）" />
<meta property="og:description" content="在本博客中，你将学习创建一个 LangChain 应用程序，以使用 ChatGPT API 和 Huggingface 语言模型与多个 PDF 文件聊天。
如上所示，我们在最最左边摄入 PDF 文件，并它们连成一起，并分为不同的 chunks。我们可以通过使用 huggingface 来对 chunks 进行处理并形成 embeddings。我们把 embeddings 写入到 Elasticsearch 向量数据库中，并保存。在搜索的时候，我们通过 LangChain 来进行向量化，并使用 Elasticsearch 进行向量搜索。在最后，我们通过大模型的使用，针对提出的问题来进行提问。我们最终的界面如下：
如上所示，它可以针对我们的问题进行回答。进一步阅读 使用 LangChain 和 Elasticsearch 对私人数据进行人工智能搜索 使用 LangChain 和 Elasticsearch 的隐私优先 AI 搜索
所有的源码可以在地址 GitHub - liu-xiao-guo/ask-multiple-pdfs: A Langchain app that allows you to chat with multiple PDFs 进行下载。
安装 如果你还没有安装好自己的 Elasticsearch 及 Kibana 的话，那么请参考如下的链接：
如何在 Linux，MacOS 及 Windows 上进行安装 Elasticsearch
Kibana：如何在 Linux，MacOS 及 Windows 上安装 Elastic 栈中的 Kibana" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/7c901deb342ac5085f57620e9cc4d6b5/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-09-25T17:05:34+08:00" />
<meta property="article:modified_time" content="2023-09-25T17:05:34+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Elasticsearch：与多个 PDF 聊天 | LangChain Python 应用教程（免费 LLMs 和嵌入）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>在本博客中，你将学习创建一个 LangChain 应用程序，以使用 ChatGPT API 和 Huggingface 语言模型与多个 PDF 文件聊天。</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/d2/b1/5463kXad_o.jpg" width="1200"></p> 
<p>如上所示，我们在最最左边摄入 PDF 文件，并它们连成一起，并分为不同的 chunks。我们可以通过使用 huggingface 来对 chunks 进行处理并形成 embeddings。我们把 embeddings 写入到 Elasticsearch 向量数据库中，并保存。在搜索的时候，我们通过 LangChain 来进行向量化，并使用 Elasticsearch 进行向量搜索。在最后，我们通过大模型的使用，针对提出的问题来进行提问。我们最终的界面如下：</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/c2/8a/dckzTkH6_o.png" width="1200"></p> 
<p>如上所示，它可以针对我们的问题进行回答。进一步阅读 </p> 
<ul><li><a class="link-info" href="https://elasticstack.blog.csdn.net/article/details/132979480" rel="nofollow" title="使用 LangChain 和 Elasticsearch 对私人数据进行人工智能搜索">使用 LangChain 和 Elasticsearch 对私人数据进行人工智能搜索</a></li><li> <p id="articleContentId"><a class="link-info" href="https://elasticstack.blog.csdn.net/article/details/130940333" rel="nofollow" title="使用 LangChain 和 Elasticsearch 的隐私优先 AI 搜索">使用 LangChain 和 Elasticsearch 的隐私优先 AI 搜索</a></p> </li></ul> 
<p>所有的源码可以在地址 <a href="https://github.com/liu-xiao-guo/ask-multiple-pdfs" title="GitHub - liu-xiao-guo/ask-multiple-pdfs: A Langchain app that allows you to chat with multiple PDFs">GitHub - liu-xiao-guo/ask-multiple-pdfs: A Langchain app that allows you to chat with multiple PDFs</a> 进行下载。</p> 
<p></p> 
<h2>安装</h2> 
<p>如果你还没有安装好自己的 Elasticsearch 及 Kibana 的话，那么请参考如下的链接：</p> 
<ul><li> <p id="articleContentId"><a href="https://elasticstack.blog.csdn.net/article/details/99413578" rel="nofollow" title="如何在 Linux，MacOS 及 Windows 上进行安装 Elasticsearch">如何在 Linux，MacOS 及 Windows 上进行安装 Elasticsearch</a></p> </li><li> <p id="articleContentId"><a href="https://elasticstack.blog.csdn.net/article/details/99433732" rel="nofollow" title="Kibana：如何在 Linux，MacOS 及 Windows 上安装 Elastic 栈中的 Kibana">Kibana：如何在 Linux，MacOS 及 Windows 上安装 Elastic 栈中的 Kibana</a></p> </li></ul> 
<p>在安装的时候，我们选择 Elastic Stack 9.x 的安装指南来进行安装。在默认的情况下，Elasticsearch 集群的访问具有 HTTPS 的安全访问。</p> 
<p>在安装时，我们可以在 Elasticsearch 的如下地址找到相应的证书文件 http_ca.crt:</p> 
<pre><code class="hljs">$ pwd
/Users/liuxg/elastic/elasticsearch-8.10.0/config/certs
$ ls
http.p12      http_ca.crt   transport.p12</code></pre> 
<p>我们需要把该证书拷贝到项目文件的根目录下：</p> 
<pre><code class="hljs">$ tree -L 3
.
├── app.py
├── docs
│   └── PDF-LangChain.jpg
├── htmlTemplates.py
├── http_ca.crt
├── lib_embeddings.py
├── lib_indexer.py
├── lib_llm.py
├── lib_vectordb.py
├── myapp.py
├── pdf_files
│   ├── sample1.pdf
│   └── sample2.pdf
├── readme.md
├── requirements.txt
└── simple.cfg</code></pre> 
<p>如上所示，我们把 http_ca.crt 拷贝到应用的根目录下。我们在 pdf_files 里放了两个用于测试的 PDF 文件。你可以使用自己的 PDF 文件来进行测试。我们在 simple.cfg 做如下的配置：</p> 
<pre><code class="hljs">ES_SERVER: "localhost" 
ES_PASSWORD: "vXDWYtL*my3vnKY9zCfL"
ES_FINGERPRINT: "e2c1512f617f432ddf242075d3af5177b28f6497fecaaa0eea11429369bb7b00"</code></pre> 
<p>在上面，我们需要配置 ES_SERVER。这个是 Elasticsearch 集群的地址。这里的 ES_PASSWORD 是 Elasticsearch 的超级用户 elastic 的密码。我们可以在 Elasticsearch 第一次启动的画面中找到这个 ES_FINGERPRINT：</p> 
<p><img alt="" height="750" src="https://images2.imgbox.com/95/ed/BeZ6syiW_o.png" width="1200"></p> 
<p>你还可以在 Kibana 的配置文件 confgi/kibana.yml 文件中获得 fingerprint 的配置：</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/11/bc/4ukPNWNF_o.png" width="1200"></p> 
<p>在项目的目录中，我们还可以看到一个叫做 .env-example 的文件。我们可以使用如下的命令把它重新命名为 .env:</p> 
<pre><code class="hljs">mv .env.example .env</code></pre> 
<p>在 .env 中，我们输入 huggingface.co 网站得到的 token：</p> 
<pre><code class="hljs">$ cat .env
OPENAI_API_KEY=your_openai_key
HUGGINGFACEHUB_API_TOKEN=your_huggingface_key</code></pre> 
<p>在本例中，我们将使用 huggingface 来进行测试。如果你需要使用到 OpenAI，那么你需要配置它的 key。有关 huggingface 的开发者 key，你可以在<a class="link-info" href="https://huggingface.co/settings/tokens" rel="nofollow" title="地址">地址</a>获得。</p> 
<p></p> 
<h2>运行项目</h2> 
<p>在运行项目之前，你需要做一下安装的动作：</p> 
<pre><code class="hljs">python3 -m venv env
source env/bin/activate
python3 -m pip install --upgrade pip
pip install -r requirements.txt</code></pre> 
<p></p> 
<h3>创建界面</h3> 
<p>本应用的界面，我们采用是 streamlit 来创建的。它的创建也是非常地简单。我们可以在 myapp.py 中看到如下的代码：</p> 
<p><strong>myapp.py</strong></p> 
<pre><code class="hljs">import streamlit as st
from dotenv import load_dotenv
from PyPDF2 import PdfReader
from htmlTemplates import css, bot_template, user_template

def get_pdf_texts(pdf_docs):
    text = ""
    for pdf in pdf_docs:
        pdf_reader = PdfReader(pdf)
        for page in pdf_reader.pages:
            text += page.extract_text()
    return text

def main():
    load_dotenv()
    st.set_page_config(page_title="Chat with multiple PDFs", page_icon=":books:")
    st.write(css, unsafe_allow_html=True)
    st.header("Chat with multiple PDFs :books:")
    user_question = st.text_input("Ask a question about your documents")
    if user_question:
        pass
    
    st.write(user_template.replace("{<!-- -->{MSG}}", "Hello, human").replace("{<!-- -->{MSG1}}", " "), unsafe_allow_html=True)
    st.write(bot_template.replace("{<!-- -->{MSG}}", "Hello, robot").replace("{<!-- -->{MSG1}}", " "), unsafe_allow_html=True)

    # Add a side bar
    with st.sidebar:
        st.subheader("Your documents")
        pdf_docs = st.file_uploader(
            "Upload your PDFs here and press on click on Process", accept_multiple_files=True)
        print(pdf_docs)
        if st.button("Process"):
            with st.spinner("Processing"):
                # Get pdf text from
                raw_text = get_pdf_texts(pdf_docs)
                st.write(raw_text)

    
if __name__ == "__main__":
    main()</code></pre> 
<p>在上面的代码中，我创建了一个 sidebar 用来选择需要的 PDF 文件。我们可以点击 Process 按钮来显示已经提取的 PDF 文本。我们可以使用如下的命令来运行应用：</p> 
<pre><code class="hljs">(venv) $ streamlit run myapp.py</code></pre> 
<pre><code class="hljs">venv) $ streamlit run myapp.py

  You can now view your Streamlit app in your browser.

  Local URL: http://localhost:8502
  Network URL: http://198.18.1.13:8502</code></pre> 
<p>运行完上面的命令后，我们可以在浏览器中打开应用：</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/84/cc/F6buGiLe_o.png" width="1200"></p> 
<p>我们点击 Browse files，并选中 PDF 文件：</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/de/ae/W7cBRW5E_o.png" width="1200"></p> 
<p>点击上面的 Process，我们可以看到：</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/cc/e9/e61jATQe_o.png" width="1200"></p> 
<p>在上面，我们为了显示的方便，我使用 st.write 直接把结果写到浏览器的页面里。我们接下来需要针对这个长的文字进行切分为一个一个的 chunks。我们需要按照模型的需要，不能超过模型允许的最大值。</p> 
<p>上面我简单地叙述了 UI 的构造。最终完整的 myapp.py 的设计如下：</p> 
<p><strong>myapp.py</strong></p> 
<pre><code class="hljs">import streamlit as st
from dotenv import load_dotenv
from PyPDF2 import PdfReader
from langchain.text_splitter import CharacterTextSplitter
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from htmlTemplates import css, bot_template, user_template

import lib_indexer
import lib_llm
import lib_embeddings
import lib_vectordb

index_name = "pdf_docs"

def get_pdf_text(pdf):
    text = ""
    pdf_reader = PdfReader(pdf)
    for page in pdf_reader.pages:
        text += page.extract_text()
    return text


def get_pdf_texts(pdf_docs):
    text = ""
    for pdf in pdf_docs:
        pdf_reader = PdfReader(pdf)
        for page in pdf_reader.pages:
            text += page.extract_text()
    return text

def get_text_chunks(text):
    text_splitter = CharacterTextSplitter(
        separator="\n", 
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    chunks = text_splitter.split_text(text)
    # chunks = text_splitter.split_documents(text)
    return chunks

def get_text_chunks1(text):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=384, chunk_overlap=0)
    chunks = text_splitter.split_text(text)
    return chunks
        
def handle_userinput(db, llm_chain_informed, user_question):
    similar_docs = db.similarity_search(user_question)
    print(f'The most relevant passage: \n\t{similar_docs[0].page_content}')

    ## 4. Ask Local LLM context informed prompt
    # print("&gt;&gt; 4. Asking The Book ... and its response is: ")
    informed_context= similar_docs[0].page_content
    response = llm_chain_informed.run(context=informed_context,question=user_question)

    st.write(user_template.replace("{<!-- -->{MSG}}", user_question).replace("{<!-- -->{MSG1}}", " "), unsafe_allow_html=True)
    st.write(bot_template.replace("{<!-- -->{MSG}}", response).replace("{<!-- -->{MSG1}}", similar_docs[0].page_content),unsafe_allow_html=True)
            
def main():
    
    # # Huggingface embedding setup
    hf = lib_embeddings.setup_embeddings()

    # # # ## Elasticsearch as a vector db
    db, url = lib_vectordb.setup_vectordb(hf, index_name)

    # # # ## set up the conversational LLM
    llm_chain_informed= lib_llm.make_the_llm()

    load_dotenv()
    st.set_page_config(page_title="Chat with multiple PDFs", page_icon=":books:")
    st.write(css, unsafe_allow_html=True)
    st.header("Chat with multiple PDFs :books:")
    user_question = st.text_input("Ask a question about your documents")
    if user_question:
        handle_userinput(db, llm_chain_informed, user_question)
            
    st.write(user_template.replace("{<!-- -->{MSG}}", "Hello, human").replace("{<!-- -->{MSG1}}", " "), unsafe_allow_html=True)
    st.write(bot_template.replace("{<!-- -->{MSG}}", "Hello, robot").replace("{<!-- -->{MSG1}}", " "), unsafe_allow_html=True)
    
    # Add a side bar
    with st.sidebar:
        st.subheader("Your documents")
        pdf_docs = st.file_uploader(
            "Upload your PDFs here and press on click on Process", accept_multiple_files=True)
        print(pdf_docs)
        if st.button("Process"):
            with st.spinner("Processing"):
                # Get pdf text from
                # raw_text = get_pdf_text(pdf_docs[0])
                raw_text = get_pdf_texts(pdf_docs)
                # st.write(raw_text)
                print(raw_text)
                
                # Get the text chunks
                text_chunks = get_text_chunks(raw_text)
                # st.write(text_chunks)
                
                # Create vector store
                lib_indexer.loadPdfChunks(text_chunks, url, hf, db, index_name)


if __name__ == "__main__":
    main()</code></pre> 
<p></p> 
<h3>创建嵌入模型</h3> 
<p><strong>lib_embedding.py</strong></p> 
<pre><code>## for embeddings
from langchain.embeddings import HuggingFaceEmbeddings

def setup_embeddings():
    # Huggingface embedding setup
    print("&gt;&gt; Prep. Huggingface embedding setup")
    model_name = "sentence-transformers/all-mpnet-base-v2"
    return HuggingFaceEmbeddings(model_name=model_name)</code></pre> 
<p></p> 
<h3> 创建向量存储</h3> 
<p><strong>lib_vectordb.py</strong></p> 
<pre><code>import os
from config import Config

## for vector store
from langchain.vectorstores import ElasticVectorSearch

def setup_vectordb(hf,index_name):
    # Elasticsearch URL setup
    print("&gt;&gt; Prep. Elasticsearch config setup")
    
    with open('simple.cfg') as f:
        cfg = Config(f)
    
    endpoint = cfg['ES_SERVER']
    username = "elastic"
    password = cfg['ES_PASSWORD']
    
    ssl_verify = {
        "verify_certs": True,
        "basic_auth": (username, password),
        "ca_certs": "./http_ca.crt",
    }

    url = f"https://{username}:{password}@{endpoint}:9200"

    return ElasticVectorSearch( embedding = hf, 
                                elasticsearch_url = url, 
                                index_name = index_name, 
                                ssl_verify = ssl_verify), url</code></pre> 
<p></p> 
<h3> 创建使用带有上下文和问题变量的提示模板的离线 LLM</h3> 
<p><strong>lib_llm.py</strong></p> 
<pre><code>## for conversation LLM
from langchain import PromptTemplate, HuggingFaceHub, LLMChain
from langchain.llms import HuggingFacePipeline
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM


def make_the_llm():
    # Get Offline flan-t5-large ready to go, in CPU mode
    print("&gt;&gt; Prep. Get Offline flan-t5-large ready to go, in CPU mode")
    model_id = 'google/flan-t5-large'# go for a smaller model if you dont have the VRAM
    tokenizer = AutoTokenizer.from_pretrained(model_id) 
    model = AutoModelForSeq2SeqLM.from_pretrained(model_id) #load_in_8bit=True, device_map='auto'
    pipe = pipeline(
        "text2text-generation",
        model=model, 
        tokenizer=tokenizer, 
        max_length=100
    )
    local_llm = HuggingFacePipeline(pipeline=pipe)
    # template_informed = """
    # I know the following: {context}
    # Question: {question}
    # Answer: """

    template_informed = """
    I know: {context}
    when asked: {question}
    my response is: """

    prompt_informed = PromptTemplate(template=template_informed, input_variables=["context", "question"])

    return LLMChain(prompt=prompt_informed, llm=local_llm)</code></pre> 
<p></p> 
<h3>写入以向量表示的 PDF 文件</h3> 
<p>以下是我的分块和向量存储代码。 它需要在 Elasticsearch 中准备好组成的 Elasticsearch url、huggingface 嵌入模型、向量数据库和目标索引名称</p> 
<p><strong>lib_indexer.py</strong></p> 
<pre><code class="hljs">
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import TextLoader

## for vector store
from langchain.vectorstores import ElasticVectorSearch
from elasticsearch import Elasticsearch
from config import Config

with open('simple.cfg') as f:
    cfg = Config(f)

fingerprint = cfg['ES_FINGERPRINT']
endpoint = cfg['ES_SERVER']
username = "elastic"
password = cfg['ES_PASSWORD']
ssl_verify = {
    "verify_certs": True,
    "basic_auth": (username, password),
    "ca_certs": "./http_ca.crt"
}

url = f"https://{username}:{password}@{endpoint}:9200"

def parse_book(filepath):
    loader = TextLoader(filepath)
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=384, chunk_overlap=0)
    docs = text_splitter.split_documents(documents)
    return docs

def parse_triplets(filepath):
    docs = parse_book(filepath)
    result = []
    for i in range(len(docs) - 2):
        concat_str = docs[i].page_content + " " + docs[i+1].page_content + " " + docs[i+2].page_content
        result.append(concat_str)
    return result
    #db.from_texts(docs, embedding=hf, elasticsearch_url=url, index_name=index_name)

## load book utility
## params
##  filepath: where to get the book txt ... should be utf-8
##  url: the full Elasticsearch url with username password and port embedded
##  hf: hugging face transformer for sentences
##  db: the VectorStore Langcahin object ready to go with embedding thing already set up
##  index_name: name of index to use in ES
##
##  will check if the index_name exists already in ES url before attempting split and load
def loadBookTriplets(filepath, url, hf, db, index_name):
    with open('simple.cfg') as f:
        cfg = Config(f)
    
    fingerprint = cfg['ES_FINGERPRINT']
    es = Elasticsearch( [ url ], 
                    basic_auth = ("elastic", cfg['ES_PASSWORD']), 
                    ssl_assert_fingerprint = fingerprint, 
                    http_compress = True  )
   
    ## Parse the book if necessary
    if not es.indices.exists(index=index_name):
        print(f'\tThe index: {index_name} does not exist')
        print("&gt;&gt; 1. Chunk up the Source document")
        
        results = parse_triplets(filepath)

        print("&gt;&gt; 2. Index the chunks into Elasticsearch")
        
        elastic_vector_search= ElasticVectorSearch.from_documents( docs,
                                embedding = hf, 
                                elasticsearch_url = url, 
                                index_name = index_name, 
                                ssl_verify = ssl_verify)
    else:
        print("\tLooks like the pdfs are already loaded, let's move on")

def loadBookBig(filepath, url, hf, db, index_name):
    es = Elasticsearch( [ url ], 
                       basic_auth = ("elastic", cfg['ES_PASSWORD']), 
                       ssl_assert_fingerprint = fingerprint, 
                       http_compress = True  )
    
    ## Parse the book if necessary
    if not es.indices.exists(index=index_name):
        print(f'\tThe index: {index_name} does not exist')
        print("&gt;&gt; 1. Chunk up the Source document")
        
        docs = parse_book(filepath)
        
        # print(docs)

        print("&gt;&gt; 2. Index the chunks into Elasticsearch")
        
        elastic_vector_search= ElasticVectorSearch.from_documents( docs,
                                embedding = hf, 
                                elasticsearch_url = url, 
                                index_name = index_name, 
                                ssl_verify = ssl_verify)   
    else:
        print("\tLooks like the pdfs are already loaded, let's move on")

def loadPdfChunks(chunks, url, hf, db, index_name):    
    es = Elasticsearch( [ url ], 
                       basic_auth = ("elastic", cfg['ES_PASSWORD']), 
                       ssl_assert_fingerprint = fingerprint, 
                       http_compress = True  )
    
    ## Parse the book if necessary
    if not es.indices.exists(index=index_name):
        print(f'\tThe index: {index_name} does not exist')        
        print("&gt;&gt; 2. Index the chunks into Elasticsearch")
        
        print("url: ", url)
        print("index_name", index_name)
        
        elastic_vector_search = db.from_texts( chunks,
                                embedding = hf, 
                                elasticsearch_url = url, 
                                index_name = index_name, 
                                ssl_verify = ssl_verify)   
    else:
        print("\tLooks like the pdfs are already loaded, let's move on")
</code></pre> 
<p></p> 
<h3>提问</h3> 
<p>我们使用 streamlit 的 input 来进行提问：</p> 
<pre><code class="hljs">    user_question = st.text_input("Ask a question about your documents")
    if user_question:
        handle_userinput(db, llm_chain_informed, user_question)</code></pre> 
<p>当我们打入 ENTER 键后，上面的代码调用 handle_userinput(db, llm_chain_informed, user_question)：</p> 
<pre><code class="hljs">def handle_userinput(db, llm_chain_informed, user_question):
    similar_docs = db.similarity_search(user_question)
    print(f'The most relevant passage: \n\t{similar_docs[0].page_content}')

    ## 4. Ask Local LLM context informed prompt
    # print("&gt;&gt; 4. Asking The Book ... and its response is: ")
    informed_context= similar_docs[0].page_content
    response = llm_chain_informed.run(context=informed_context,question=user_question)

    st.write(user_template.replace("{<!-- -->{MSG}}", user_question).replace("{<!-- -->{MSG1}}", " "), unsafe_allow_html=True)
    st.write(bot_template.replace("{<!-- -->{MSG}}", response).replace("{<!-- -->{MSG1}}", similar_docs[0].page_content),unsafe_allow_html=True)</code></pre> 
<p>首先它使用 db 进行相似性搜索，然后我们再使用大模型来得到我们想要的答案。</p> 
<p></p> 
<h2>运行结果</h2> 
<p>我们使用命令来运行代码：</p> 
<pre><code class="hljs">streamlit run myapp.py</code></pre> 
<p>我们在浏览器中选择在 pdf_files 中的两个 PDF 文件：</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/6d/fe/AT02z60P_o.png" width="1200"></p> 
<p>在上面，我们输入想要的问题：</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/2a/cb/138Alcmd_o.png" width="1200"></p> 
<p>上面的问题是：</p> 
<pre><code class="hljs">what do I make all the same and put a cup next to him on the desk?</code></pre> 
<p>再进行提问：</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/32/7e/43LtSpYk_o.png" width="1200"></p> 
<p>上面的问题是：</p> 
<pre><code class="hljs">when should you come? I will send a car to meet you from the half past four arrival at Harrogate Station.</code></pre> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/8b/e2/zYbBISOj_o.png" width="1200"></p> 
<p>上面的问题是：</p> 
<pre><code class="hljs">what will I send to meet you from the half past four arrival at Harrogate Station?</code></pre> 
<p>你进行多次尝试其它的问题。Happy journery :)</p> 
<p>有关 ChatGPT 的使用也是基本相同的。你需要使用 ChatGPT 的模型及其相应的 key 即可。在这里就不赘述了。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a0621f8f81003eda85a654979eaffdf4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">CUDA 编程 基础与实践（樊哲勇） 摘录</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6df491c90720f363ac6da5148c0a6303/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Vue中前端导出word文件</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>