<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>明明加了唯一索引，为什么还是产生重复数据？ - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="明明加了唯一索引，为什么还是产生重复数据？" />
<meta property="og:description" content="前言 前段时间我踩过一个坑：在mysql8的一张innodb引擎的表中，加了唯一索引，但最后发现数据竟然还是重复了。
到底怎么回事呢？
本文通过一次踩坑经历，聊聊唯一索引，一些有意思的知识点。
1.还原问题现场 前段时间，为了防止商品组产生重复的数据，我专门加了一张防重表。
如果大家对防重表，比较感兴趣，可以看看我的另一篇文章 《高并发下如何防重？》，里面有详细的介绍。
问题就出在商品组的防重表上。
具体表结构如下：
CREATE TABLE `product_group_unique` ( `id` bigint NOT NULL, `category_id` bigint NOT NULL, `unit_id` bigint NOT NULL, `model_hash` varchar(255) COLLATE utf8mb4_bin DEFAULT NULL, `in_date` datetime NOT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin; 为了保证数据的唯一性，我给那种商品组防重表，建了唯一索引：
alter table product_group_unique add unique index ux_category_unit_model(category_id,unit_id,model_hash); 根据分类编号、单位编号和商品组属性的hash值，可以唯一确定一个商品组。
给商品组防重表创建了唯一索引之后，第二天查看数据，发现该表中竟然产生了重复的数据：
表中第二条数据和第三条数据重复了。
这是为什么呢？
2.唯一索引字段包含null 如果你仔细查看表中的数据，会发现其中一个比较特殊地方：商品组属性的hash值（model_hash字段）可能为null，即商品组允许不配置任何属性。
在product_group_unique表中插入了一条model_hash字段等于100的重复数据：
执行结果：
从上图中看出，mysql的唯一性约束生效了，重复数据被拦截了。
接下来，我们再插入两条model_hash为null的数据，其中第三条数据跟第二条数据中category_id、unit_id和model_hash字段值都一样。
从图中看出，竟然执行成功了。
换句话说，如果唯一索引的字段中，出现了null值，则唯一性约束不会生效。
最终插入的数据情况是这样的：
当model_hash字段不为空时，不会产生重复的数据。
当model_hash字段为空时，会生成重复的数据。
我们需要特别注意：创建唯一索引的字段，都不能允许为null，否则mysql的唯一性约束可能会失效。
3.逻辑删除表加唯一索引 我们都知道唯一索引非常简单好用，但有时候，在表中它并不好加。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/f3369e526a083b3c15a493db4c862cba/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-03T10:16:17+08:00" />
<meta property="article:modified_time" content="2024-01-03T10:16:17+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">明明加了唯一索引，为什么还是产生重复数据？</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3>前言</h3> 
<p>前段时间我踩过一个坑：在<code>mysql8</code>的一张<code>innodb</code>引擎的<code>表</code>中，加了<code>唯一索引</code>，但最后发现<code>数据</code>竟然还是<code>重复</code>了。</p> 
<p>到底怎么回事呢？</p> 
<p>本文通过一次踩坑经历，聊聊唯一索引，一些有意思的知识点。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="739" src="https://images2.imgbox.com/81/eb/MzWhJSah_o.png" width="1080"></p> 
<h3>1.还原问题现场</h3> 
<p>前段时间，为了防止商品组产生重复的数据，我专门加了一张<code>防重表</code>。</p> 
<p>如果大家对防重表，比较感兴趣，可以看看我的另一篇文章 《<a href="https://mp.weixin.qq.com/s?__biz=MzkwNjMwMTgzMQ==&amp;mid=2247495570&amp;idx=1&amp;sn=eed3102c7dffc4ddbc59844dd9b865a5&amp;chksm=c0e8377af79fbe6c29aefa3ae3aab48c6459b673005e2f97ae402172f6e5cdf8573aea5e7663&amp;token=758132007&amp;lang=zh_CN&amp;scene=21#wechat_redirect" rel="nofollow" title="高并发下如何防重？">高并发下如何防重？</a>》，里面有详细的介绍。</p> 
<p>问题就出在商品组的防重表上。</p> 
<p>具体表结构如下：</p> 
<pre><code>CREATE TABLE `product_group_unique` (
  `id` bigint NOT NULL,
  `category_id` bigint NOT NULL,
  `unit_id` bigint NOT NULL,
  `model_hash` varchar(255) COLLATE utf8mb4_bin DEFAULT NULL,
  `in_date` datetime NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin;
</code></pre> 
<p>为了保证数据的<code>唯一性</code>，我给那种商品组防重表，建了唯一索引：</p> 
<pre><code>alter table product_group_unique add unique index 
ux_category_unit_model(category_id,unit_id,model_hash);
</code></pre> 
<p>根据分类编号、单位编号和商品组属性的hash值，可以唯一确定一个商品组。</p> 
<p>给商品组防重表创建了<code>唯一索引</code>之后，第二天查看数据，发现该表中竟然产生了重复的数据：</p> 
<p class="img-center"><img alt="图片" height="154" src="https://images2.imgbox.com/6c/a8/VTDtOCmK_o.png" width="864"></p> 
<p>表中第二条数据和第三条数据重复了。</p> 
<p>这是为什么呢？</p> 
<h3>2.唯一索引字段包含null</h3> 
<p>如果你仔细查看表中的数据，会发现其中一个比较特殊地方：商品组属性的hash值（model_hash字段）可能为<code>null</code>，即商品组允许不配置任何属性。</p> 
<p>在product_group_unique表中插入了一条model_hash字段等于100的重复数据：</p> 
<p class="img-center"><img alt="图片" height="122" src="https://images2.imgbox.com/69/d2/ZpYkhny8_o.png" width="884"></p> 
<p>执行结果：</p> 
<p class="img-center"><img alt="图片" height="250" src="https://images2.imgbox.com/e1/9d/ynBZqL8m_o.png" width="832"></p> 
<p>从上图中看出，mysql的唯一性约束生效了，重复数据被拦截了。</p> 
<p>接下来，我们再插入两条model_hash为null的数据，其中第三条数据跟第二条数据中category_id、unit_id和model_hash字段值都一样。</p> 
<p class="img-center"><img alt="图片" height="154" src="https://images2.imgbox.com/2c/bb/NQTcyh2q_o.png" width="864"></p> 
<p>从图中看出，竟然执行成功了。</p> 
<p>换句话说，如果唯一索引的字段中，出现了null值，则唯一性约束不会生效。</p> 
<p>最终插入的数据情况是这样的：</p> 
<ol><li> <p>当model_hash字段不为空时，不会产生重复的数据。</p> </li><li> <p>当model_hash字段为空时，会生成重复的数据。</p> </li></ol> 
<blockquote> 
 <p>我们需要特别注意：创建唯一索引的字段，都不能允许为null，否则mysql的唯一性约束可能会失效。</p> 
</blockquote> 
<h3>3.逻辑删除表加唯一索引</h3> 
<p>我们都知道唯一索引非常简单好用，但有时候，在表中它并不好加。</p> 
<p>不信，我们一起往下看。</p> 
<p>通常情况下，要删除表的某条记录的话，如果用<code>delete</code>语句操作的话。</p> 
<p>例如：</p> 
<pre><code>delete from product where id=123;
</code></pre> 
<p>这种delete操作是<code>物理删除</code>，即该记录被删除之后，后续通过sql语句基本查不出来。（不过通过其他技术手段可以找回，那是后话了）</p> 
<p>还有另外一种是<code>逻辑删除</code>，主要是通过<code>update</code>语句操作的。</p> 
<p>例如：</p> 
<pre><code>update product set delete_status=1,edit_time=now(3) 
where id=123;
</code></pre> 
<p>逻辑删除需要在表中额外增加一个删除状态字段，用于记录数据是否被删除。在所有的业务查询的地方，都需要过滤掉已经删除的数据。</p> 
<p>通过这种方式删除数据之后，数据任然还在表中，只是从逻辑上过滤了删除状态的数据而已。</p> 
<p>其实对于这种逻辑删除的表，是没法加唯一索引的。</p> 
<p>为什么呢？</p> 
<p>假设之前给商品表中的<code>name</code>和<code>model</code>加了唯一索引，如果用户把某条记录删除了，delete_status设置成1了。后来，该用户发现不对，又重新添加了一模一样的商品。</p> 
<p>由于唯一索引的存在，该用户第二次添加商品会失败，即使该商品已经被删除了，也没法再添加了。</p> 
<p>这个问题显然有点严重。</p> 
<p>有人可能会说：把<code>name</code>、<code>model</code>和<code>delete_status</code>三个字段同时做成<code>唯一索引</code>不就行了？</p> 
<p>答：这样做确实可以解决用户逻辑删除了某个商品，后来又重新添加相同的商品时，添加不了的问题。但如果第二次添加的商品，又被删除了。该用户第三次添加相同的商品，不也出现问题了？</p> 
<p>由此可见，如果表中有逻辑删除功能，是不方便创建唯一索引的。</p> 
<p>但如果真的想给包含逻辑删除的表，增加唯一索引，该怎么办呢？</p> 
<h4>3.1 删除状态+1</h4> 
<p>通过前面知道，如果表中有逻辑删除功能，是不方便创建唯一索引的。</p> 
<p>其根本原因是，记录被删除之后，delete_status会被设置成1，默认是0。相同的记录第二次删除的时候，delete_status被设置成1，但由于创建了唯一索引（把name、model和delete_status三个字段同时做成唯一索引），数据库中已存在delete_status为1的记录，所以这次会操作失败。</p> 
<p>我们为啥不换一种思考：不要纠结于delete_status为1，表示删除，当delete_status为1、2、3等等，只要大于1都表示删除。</p> 
<p>这样的话，每次删除都获取那条相同记录的最大删除状态，然后加1。</p> 
<p>这样数据操作过程变成：</p> 
<ol><li> <p>添加记录a，delete_status=0。</p> </li><li> <p>删除记录a，delete_status=1。</p> </li><li> <p>添加记录a，delete_status=0。</p> </li><li> <p>删除记录a，delete_status=2。</p> </li><li> <p>添加记录a，delete_status=0。</p> </li><li> <p>删除记录a，delete_status=3。</p> </li></ol> 
<p>由于记录a，每次删除时，delete_status都不一样，所以可以保证唯一性。</p> 
<p>该方案的优点是：不用调整字段，非常简单和直接。</p> 
<p>缺点是：可能需要修改sql逻辑，特别是有些查询sql语句，有些使用delete_status=1判断删除状态的，需要改成delete_status&gt;=1。</p> 
<h4>2.1 增加redis缓存</h4> 
<p>对于简单的count(*)，比如：统计浏览总次数或者浏览总人数，我们可以直接将接口使用redis缓存起来，没必要实时统计。</p> 
<p>当用户打开指定页面时，在缓存中每次都设置成count = count+1即可。</p> 
<p>用户第一次访问页面时，redis中的count值设置成1。用户以后每访问一次页面，都让count加1，最后重新设置到redis中。</p> 
<p class="img-center"><img alt="图片" height="572" src="https://images2.imgbox.com/8e/46/XjanHkQB_o.png" width="626"></p> 
<p>这样在需要展示数量的地方，从redis中查出count值返回即可。</p> 
<p>该场景无需从数据埋点表中使用count(*)实时统计数据，性能将会得到极大的提升。</p> 
<p>不过在高并发的情况下，可能会存在缓存和数据库的数据不一致的问题。</p> 
<p>但对于统计浏览总次数或者浏览总人数这种业务场景，对数据的准确性要求并不高，容忍数据不一致的情况存在。</p> 
<h4>2.2 加二级缓存</h4> 
<p>对于有些业务场景，新增数据很少，大部分是统计数量操作，而且查询条件很多。这时候使用传统的count(*)实时统计数据，性能肯定不会好。</p> 
<p>假如在页面中可以通过id、name、状态、时间、来源等，一个或多个条件，统计品牌数量。</p> 
<p>这种情况下用户的组合条件比较多，增加联合索引也没用，用户可以选择其中一个或者多个查询条件，有时候联合索引也会失效，只能尽量满足用户使用频率最高的条件增加索引。</p> 
<p>也就是有些组合条件可以走索引，有些组合条件没法走索引，这些没法走索引的场景，该如何优化呢？</p> 
<p>答：使用<code>二级缓存</code>。</p> 
<p>二级缓存其实就是内存缓存。</p> 
<p>我们可以使用<code>caffine</code>或者<code>guava</code>实现二级缓存的功能。</p> 
<p>目前<code>SpringBoot</code>已经集成了caffine，使用起来非常方便。</p> 
<p>只需在需要增加二级缓存的查询方法中，使用<code>@Cacheable</code>注解即可。</p> 
<pre><code> @Cacheable(value = "brand", , keyGenerator = "cacheKeyGenerator")
   public BrandModel getBrand(Condition condition) {
       return getBrandByCondition(condition);
   }
</code></pre> 
<p>然后自定义cacheKeyGenerator，用于指定缓存的key。</p> 
<pre><code>public class CacheKeyGenerator implements KeyGenerator {
    @Override
    public Object generate(Object target, Method method, Object... params) {
        return target.getClass().getSimpleName() + UNDERLINE
                + method.getName() + ","
                + StringUtils.arrayToDelimitedString(params, ",");
    }
}
</code></pre> 
<p>这个key是由各个条件组合而成。</p> 
<p>这样通过某个条件组合查询出品牌的数据之后，会把结果缓存到内存中，设置过期时间为5分钟。</p> 
<p>后面用户在5分钟内，使用相同的条件，重新查询数据时，可以直接从二级缓存中查出数据，直接返回了。</p> 
<p>这样能够极大的提示count(*)的查询效率。</p> 
<p>但是如果使用二级缓存，可能存在不同的服务器上，数据不一样的情况。我们需要根据实际业务场景来选择，没法适用于所有业务场景。</p> 
<h4>2.3 多线程执行</h4> 
<p>不知道你有没有做过这样的需求：统计有效订单有多少，无效订单有多少。</p> 
<p>这种情况一般需要写两条sql，统计有效订单的sql如下：</p> 
<pre><code>select count(*) from order where status=1;
</code></pre> 
<p>统计无效订单的sql如下：</p> 
<pre><code>select count(*) from order where status=0;
</code></pre> 
<p>但如果在一个接口中，同步执行这两条sql效率会非常低。</p> 
<p>这时候，可以改成成一条sql：</p> 
<pre><code>select count(*),status from order
group by status;
</code></pre> 
<p>使用<code>group by</code>关键字分组统计相同status的数量，只会产生两条记录，一条记录是有效订单数量，另外一条记录是无效订单数量。</p> 
<p>但有个问题：status字段只有1和0两个值，重复度很高，区分度非常低，不能走索引，会全表扫描，效率也不高。</p> 
<p>还有其他的解决方案不？</p> 
<p>答：使用多线程处理。</p> 
<p>我们可以使用<code>CompleteFuture</code>使用两个<code>线程</code>异步调用统计有效订单的sql和统计无效订单的sql，最后汇总数据，这样能够提升查询接口的性能。</p> 
<p>最近我建了新的技术交流群，打算将它打造成高质量的活跃群，欢迎小伙伴们加入。</p> 
<p><strong>我以往的技术群里技术氛围非常不错，大佬很多。</strong></p> 
<p></p> 
<p class="img-center"><img alt="image.png" height="1200" src="https://images2.imgbox.com/4f/8e/kfqZiOxB_o.png" width="828"></p> 
<p>加微信：su_san_java，备注：加群，即可加入该群。</p> 
<h4>3.2 增加时间戳字段</h4> 
<p>导致逻辑删除表，不好加唯一索引最根本的地方在逻辑删除那里。</p> 
<p>我们为什么不加个字段，专门处理逻辑删除的功能呢？</p> 
<p>答：可以增加<code>时间戳</code>字段。</p> 
<p>把name、model、delete_status和timeStamp，四个字段同时做成唯一索引</p> 
<p>在添加数据时，timeStamp字段写入默认值<code>1</code>。</p> 
<p>然后一旦有逻辑删除操作，则自动往该字段写入时间戳。</p> 
<p>这样即使是同一条记录，逻辑删除多次，每次生成的时间戳也不一样，也能保证数据的唯一性。</p> 
<p>时间戳一般精确到<code>秒</code>。</p> 
<p>除非在那种极限并发的场景下，对同一条记录，两次不同的逻辑删除操作，产生了相同的时间戳。</p> 
<p>这时可以将时间戳精确到<code>毫秒</code>。</p> 
<p>该方案的优点是：可以在不改变已有代码逻辑的基础上，通过增加新字段实现了数据的唯一性。</p> 
<p>缺点是：在极限的情况下，可能还是会产生重复数据。</p> 
<h4>3.3 增加id字段</h4> 
<p>其实，增加时间戳字段基本可以解决问题。但在在极限的情况下，可能还是会产生重复数据。</p> 
<p>有没有办法解决这个问题呢？</p> 
<p>答：增加<code>主键</code>字段：delete_id。</p> 
<p>该方案的思路跟增加时间戳字段一致，即在添加数据时给delete_id设置默认值1，然后在逻辑删除时，给delete_id赋值成当前记录的主键id。</p> 
<p>把name、model、delete_status和delete_id，四个字段同时做成唯一索引。</p> 
<p>这可能是最优方案，无需修改已有删除逻辑，也能保证数据的唯一性。</p> 
<h3>4. 重复历史数据如何加唯一索引？</h3> 
<p>前面聊过如果表中有逻辑删除功能，不太好加唯一索引，但通过文中介绍的三种方案，可以顺利的加上唯一索引。</p> 
<p>但来自灵魂的一问：如果某张表中，已存在<code>历史重复数据</code>，该如何加索引呢？</p> 
<p>最简单的做法是，增加一张<code>防重表</code>，然后把数据初始化进去。</p> 
<p>可以写一条类似这样的sql：</p> 
<pre><code>insert into product_unqiue(id,name,category_id,unit_id,model) 
select max(id), select name,category_id,unit_id,model from product
group by name,category_id,unit_id,model;
</code></pre> 
<p>这样做可以是可以，但今天的主题是直接在原表中加唯一索引，不用防重表。</p> 
<p>那么，这个唯一索引该怎么加呢？</p> 
<p>其实可以借鉴上一节中，增加<code>id</code>字段的思路。</p> 
<p>增加一个delete_id字段。</p> 
<p>不过在给product表创建唯一索引之前，先要做数据处理。</p> 
<p>获取相同记录的最大id：</p> 
<pre><code>select max(id), select name,category_id,unit_id,model from product
group by name,category_id,unit_id,model;
</code></pre> 
<p>然后将delete_id字段设置成1。</p> 
<p>然后将其他的相同记录的delete_id字段，设置成当前的主键。</p> 
<p>这样就能区分历史的重复数据了。</p> 
<p>当所有的delete_id字段都设置了值之后，就能给name、model、delete_status和delete_id，四个字段加唯一索引了。</p> 
<p>完美。</p> 
<h4>2.1 增加redis缓存</h4> 
<p>对于简单的count(*)，比如：统计浏览总次数或者浏览总人数，我们可以直接将接口使用redis缓存起来，没必要实时统计。</p> 
<p>当用户打开指定页面时，在缓存中每次都设置成count = count+1即可。</p> 
<p>用户第一次访问页面时，redis中的count值设置成1。用户以后每访问一次页面，都让count加1，最后重新设置到redis中。</p> 
<p class="img-center"><img alt="图片" height="572" src="https://images2.imgbox.com/0e/3d/7ILo5i3V_o.png" width="626"></p> 
<p>这样在需要展示数量的地方，从redis中查出count值返回即可。</p> 
<p>该场景无需从数据埋点表中使用count(*)实时统计数据，性能将会得到极大的提升。</p> 
<p>不过在高并发的情况下，可能会存在缓存和数据库的数据不一致的问题。</p> 
<p>但对于统计浏览总次数或者浏览总人数这种业务场景，对数据的准确性要求并不高，容忍数据不一致的情况存在。</p> 
<h4>2.2 加二级缓存</h4> 
<p>对于有些业务场景，新增数据很少，大部分是统计数量操作，而且查询条件很多。这时候使用传统的count(*)实时统计数据，性能肯定不会好。</p> 
<p>假如在页面中可以通过id、name、状态、时间、来源等，一个或多个条件，统计品牌数量。</p> 
<p>这种情况下用户的组合条件比较多，增加联合索引也没用，用户可以选择其中一个或者多个查询条件，有时候联合索引也会失效，只能尽量满足用户使用频率最高的条件增加索引。</p> 
<p>也就是有些组合条件可以走索引，有些组合条件没法走索引，这些没法走索引的场景，该如何优化呢？</p> 
<p>答：使用<code>二级缓存</code>。</p> 
<p>二级缓存其实就是内存缓存。</p> 
<p>我们可以使用<code>caffine</code>或者<code>guava</code>实现二级缓存的功能。</p> 
<p>目前<code>SpringBoot</code>已经集成了caffine，使用起来非常方便。</p> 
<p>只需在需要增加二级缓存的查询方法中，使用<code>@Cacheable</code>注解即可。</p> 
<pre><code> @Cacheable(value = "brand", , keyGenerator = "cacheKeyGenerator")
   public BrandModel getBrand(Condition condition) {
       return getBrandByCondition(condition);
   }
</code></pre> 
<p>然后自定义cacheKeyGenerator，用于指定缓存的key。</p> 
<pre><code>public class CacheKeyGenerator implements KeyGenerator {
    @Override
    public Object generate(Object target, Method method, Object... params) {
        return target.getClass().getSimpleName() + UNDERLINE
                + method.getName() + ","
                + StringUtils.arrayToDelimitedString(params, ",");
    }
}
</code></pre> 
<p>这个key是由各个条件组合而成。</p> 
<p>这样通过某个条件组合查询出品牌的数据之后，会把结果缓存到内存中，设置过期时间为5分钟。</p> 
<p>后面用户在5分钟内，使用相同的条件，重新查询数据时，可以直接从二级缓存中查出数据，直接返回了。</p> 
<p>这样能够极大的提示count(*)的查询效率。</p> 
<p>但是如果使用二级缓存，可能存在不同的服务器上，数据不一样的情况。我们需要根据实际业务场景来选择，没法适用于所有业务场景。</p> 
<h4>2.3 多线程执行</h4> 
<p>不知道你有没有做过这样的需求：统计有效订单有多少，无效订单有多少。</p> 
<p>这种情况一般需要写两条sql，统计有效订单的sql如下：</p> 
<pre><code>select count(*) from order where status=1;
</code></pre> 
<p>统计无效订单的sql如下：</p> 
<pre><code>select count(*) from order where status=0;
</code></pre> 
<p>但如果在一个接口中，同步执行这两条sql效率会非常低。</p> 
<p>这时候，可以改成成一条sql：</p> 
<pre><code>select count(*),status from order
group by status;
</code></pre> 
<p>使用<code>group by</code>关键字分组统计相同status的数量，只会产生两条记录，一条记录是有效订单数量，另外一条记录是无效订单数量。</p> 
<p>但有个问题：status字段只有1和0两个值，重复度很高，区分度非常低，不能走索引，会全表扫描，效率也不高。</p> 
<p>还有其他的解决方案不？</p> 
<p>答：使用多线程处理。</p> 
<p>我们可以使用<code>CompleteFuture</code>使用两个<code>线程</code>异步调用统计有效订单的sql和统计无效订单的sql，最后汇总数据，这样能够提升查询接口的性能。</p> 
<p>最近我建了新的技术交流群，打算将它打造成高质量的活跃群，欢迎小伙伴们加入。</p> 
<p><strong>我以往的技术群里技术氛围非常不错，大佬很多。</strong></p> 
<p></p> 
<p class="img-center"><img alt="image.png" height="1200" src="https://images2.imgbox.com/2d/ac/CTKWn4z8_o.png" width="828"></p> 
<p>加微信：su_san_java，备注：加群，即可加入该群。</p> 
<h3>5.给大字段加唯一索引</h3> 
<p>接下来，我们聊一个有趣的话题：如何给大字段增加唯一索引。</p> 
<p>有时候，我们需要给几个字段同时加一个唯一索引，比如给name、model、delete_status和delete_id等。</p> 
<p>但如果model字段很大，这样就会导致该唯一索引，可能会占用较多存储空间。</p> 
<p>我们都知道唯一索引，也会走索引。</p> 
<p>如果在索引的各个节点中存大数据，检索效率会非常低。</p> 
<p>由此，有必要对唯一索引长度做限制。</p> 
<p>目前mysql innodb存储引擎中索引允许的最大长度是3072 bytes，其中unqiue key最大长度是1000 bytes。</p> 
<p>如果字段太大了，超过了1000 bytes，显然是没法加唯一索引的。</p> 
<p>此时，有没有解决办法呢？</p> 
<h4>5.1 增加hash字段</h4> 
<p>我们可以增加一个hash字段，取大字段的hash值，生成一个较短的新值。该值可以通过一些hash算法生成，固定长度16位或者32位等。</p> 
<p>我们只需要给name、hash、delete_status和delete_id字段，增加唯一索引。</p> 
<p>这样就能避免唯一索引太长的问题。</p> 
<p>但它也会带来一个新问题：</p> 
<p>一般hash算法会产生hash冲突，即两个不同的值，通过hash算法生成值相同。</p> 
<p>当然如果还有其他字段可以区分，比如：name，并且业务上允许这种重复的数据，不写入数据库，该方案也是可行的。</p> 
<h4>5.2 不加唯一索引</h4> 
<p>如果实在不好加唯一索引，就不加唯一索引，通过其他技术手段保证唯一性。</p> 
<p>如果新增数据的入口比较少，比如只有job，或者数据导入，可以单线程顺序执行，这样就能保证表中的数据不重复。</p> 
<p>如果新增数据的入口比较多，最终都发mq消息，在mq消费者中单线程处理。</p> 
<h4>5.3 redis分布式锁</h4> 
<p>由于字段太大了，在mysql中不好加唯一索引，为什么不用<code>redis分布式锁</code>呢？</p> 
<p>但如果直接加给name、model、delete_status和delete_id字段，加<code>redis分布式锁</code>，显然没啥意义，效率也不会高。</p> 
<p>我们可以结合5.1章节，用name、model、delete_status和delete_id字段，生成一个hash值，然后给这个新值加锁。</p> 
<p>即使遇到hash冲突也没关系，在并发的情况下，毕竟是小概率事件。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="1018" src="https://images2.imgbox.com/29/50/QDym9Zol_o.png" width="482"></p> 
<h3>6.批量插入数据</h3> 
<p>有些小伙们，可能认为，既然有redis分布式锁了，就可以不用唯一索引了。</p> 
<p>那是你没遇到，批量插入数据的场景。</p> 
<p>假如通过查询操作之后，发现有一个集合：list的数据，需要批量插入数据库。</p> 
<p>如果使用redis分布式锁，需要这样操作：</p> 
<pre><code>for(Product product: list) {
   try {
        String hash = hash(product);
        rLock.lock(hash);
        //查询数据
        //插入数据
    } catch (InterruptedException e) {
       log.error(e);
    } finally {
        rLock.unlock();
    }
}
</code></pre> 
<p>需要在一个循环中，给每条数据都加锁。</p> 
<p>这样性能肯定不会好。</p> 
<p>当然有些小伙伴持反对意见，说使用redis的<code>pipeline</code>批量操作不就可以了？</p> 
<p>也就是一次性给500条，或者1000条数据上锁，最后使用完一次性释放这些锁？</p> 
<p>想想都有点不靠谱，这个锁得有多大呀。</p> 
<p>极容易造成锁超时，比如业务代码都没有执行完，锁的过期时间就已经到了。</p> 
<p>针对这种批量操作，如果此时使用mysql的唯一索引，直接批量insert即可，一条sql语句就能搞定。</p> 
<p>数据库会自动判断，如果存在重复的数据，会报错。如果不存在重复数据，才允许插入数据。</p> 
<p></p> 
<h4>最后说一句(求关注，别白嫖我)</h4> 
<p>如果这篇文章对您有所帮助，或者有所启发的话，帮忙扫描下发二维码关注一下，您的支持是我坚持写作最大的动力。</p> 
<p>求一键三连：点赞、转发、在看。</p> 
<p>关注公众号：【苏三说技术】，在公众号中回复：面试、代码神器、开发手册、时间管理有超赞的粉丝福利，另外回复：加群，可以跟很多BAT大厂的前辈交流和学习。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f68cce2442b6172651167f6b7d357afa/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Java中100==100为true，而1000==1000为false？</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/deec6a537f3a6131cea3febe39fceed2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">QT上位机开发（数据库sqlite编程）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>