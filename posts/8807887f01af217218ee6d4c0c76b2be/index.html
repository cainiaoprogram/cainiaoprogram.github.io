<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>TensorRT 系列 （0）C&#43;&#43; API 构建编译网络 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="TensorRT 系列 （0）C&#43;&#43; API 构建编译网络" />
<meta property="og:description" content="TensorRT的核心在于对模型算子的优化（合并算子、利用GPU特性选择特定核函数等多种策略），通过tensorRT，能够在Nvidia系列GPU上获得最好的性能，因此tensorRT的模型需要在目标GPU上实际运行的方式选择最优算法和配置，也因此tensorRT生成的模型只能在特定条件下运行（依赖于编译的trt版本、cuda版本、编译时的GPU型号）。TensorRT提供的C&#43;&#43;、Python接口用于直接构建网络结构，本次主要介绍C&#43;&#43;接口实现网络的构建与模型的编译，当然TensorRT也可以实现由其它框架的模型直接进行转换，如下是TensorRT工作流：
如下是通过C&#43;&#43; API要实现的网络结构：
示例代码如下：
// tensorRT include #include &lt;NvInfer.h&gt; #include &lt;NvInferRuntime.h&gt; // cuda include #include &lt;cuda_runtime.h&gt; // system include #include &lt;stdio.h&gt; class TRTLogger : public nvinfer1::ILogger{ public: virtual void log(Severity severity, nvinfer1::AsciiChar const* msg) noexcept override{ if(severity &lt;= Severity::kVERBOSE){ printf(&#34;%d: %s\n&#34;, severity, msg); } } }; nvinfer1::Weights make_weights(float* ptr, int n){ nvinfer1::Weights w; w.count = n; w.type = nvinfer1::DataType::kFLOAT; w.values = ptr; return w; } int main(){ // 本代码主要实现一个最简单的神经网络 TRTLogger logger; // logger是必要的，用来捕捉warning和info等 // ----------------------------- 1." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/8807887f01af217218ee6d4c0c76b2be/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-09-16T11:47:27+08:00" />
<meta property="article:modified_time" content="2022-09-16T11:47:27+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">TensorRT 系列 （0）C&#43;&#43; API 构建编译网络</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>TensorRT的核心在于对模型算子的优化（合并算子、利用GPU特性选择特定核函数等多种策略），通过tensorRT，能够在Nvidia系列GPU上获得最好的性能，因此tensorRT的模型需要在目标GPU上实际运行的方式选择最优算法和配置，也因此tensorRT生成的模型只能在特定条件下运行（依赖于编译的trt版本、cuda版本、编译时的GPU型号）。TensorRT提供的C++、Python接口用于直接构建网络结构，本次主要介绍C++接口实现网络的构建与模型的编译，当然TensorRT也可以实现由其它框架的模型直接进行转换，如下是TensorRT工作流：</p> 
<p><img alt="" height="489" src="https://images2.imgbox.com/db/05/rzOsI41d_o.png" width="1158"></p> 
<p></p> 
<p>如下是通过C++ API要实现的网络结构：<img alt="" height="649" src="https://images2.imgbox.com/f0/94/1XaU5o21_o.png" width="926"></p> 
<p> 示例代码如下：</p> 
<pre><code class="language-cpp">
// tensorRT include
#include &lt;NvInfer.h&gt;
#include &lt;NvInferRuntime.h&gt;

// cuda include
#include &lt;cuda_runtime.h&gt;

// system include
#include &lt;stdio.h&gt;

class TRTLogger : public nvinfer1::ILogger{
public:
    virtual void log(Severity severity, nvinfer1::AsciiChar const* msg) noexcept override{
        if(severity &lt;= Severity::kVERBOSE){
            printf("%d: %s\n", severity, msg);
        }
    }
};

nvinfer1::Weights make_weights(float* ptr, int n){
    nvinfer1::Weights w;
    w.count = n;
    w.type = nvinfer1::DataType::kFLOAT;
    w.values = ptr;
    return w;
}

int main(){
    // 本代码主要实现一个最简单的神经网络
     
    TRTLogger logger; // logger是必要的，用来捕捉warning和info等

    // ----------------------------- 1. 定义 builder, config 和network -----------------------------
    // 这是基本需要的组件
    //形象的理解是你需要一个builder去build这个网络
    nvinfer1::IBuilder* builder = nvinfer1::createInferBuilder(logger);

    // 创建一个构建配置，指定TensorRT应该如何优化模型，tensorRT生成的模型只能在特定配置下运行
    nvinfer1::IBuilderConfig* config = builder-&gt;createBuilderConfig();

    // 创建网络定义，其中createNetworkV2(1)表示采用显性batch size，新版tensorRT(&gt;=7.0)时，不建议采用0非显性batch size
    // 因此贯穿以后，请都采用createNetworkV2(1)而非createNetworkV2(0)或者createNetwork
    nvinfer1::INetworkDefinition* network = builder-&gt;createNetworkV2(1);

    // 构建一个模型
    /*
        Network definition:

        image
          |
        linear (fully connected)  input = 3, output = 2, bias = True     w=[[1.0, 2.0, 0.5], [0.1, 0.2, 0.5]], b=[0.3, 0.8]
          |
        sigmoid
          |
        prob
    */

    // ----------------------------- 2. 输入，模型结构和输出的基本信息 -----------------------------
    const int num_input = 3;   // in_channel
    const int num_output = 2;  // out_channel
    float layer1_weight_values[] = {1.0, 2.0, 0.5, 0.1, 0.2, 0.5}; // 前3个给w1的rgb，后3个给w2的rgb 
    float layer1_bias_values[]   = {0.3, 0.8};

    //输入指定数据的名称、数据类型和完整维度，将输入层添加到网络
    nvinfer1::ITensor* input = network-&gt;addInput("image", nvinfer1::DataType::kFLOAT, nvinfer1::Dims4(1, num_input, 1, 1));
    nvinfer1::Weights layer1_weight = make_weights(layer1_weight_values, 6);
    nvinfer1::Weights layer1_bias   = make_weights(layer1_bias_values, 2);

    //添加全连接层
    auto layer1 = network-&gt;addFullyConnected(*input, num_output, layer1_weight, layer1_bias);      // 注意对input进行了解引用
    
    //添加激活层 
    auto prob = network-&gt;addActivation(*layer1-&gt;getOutput(0), nvinfer1::ActivationType::kSIGMOID); // 注意更严谨的写法是*(layer1-&gt;getOutput(0)) 即对getOutput返回的指针进行解引用
    
    // 将我们需要的prob标记为输出
    network-&gt;markOutput(*prob-&gt;getOutput(0));

    printf("Workspace Size = %.2f MB\n", (1 &lt;&lt; 28) / 1024.0f / 1024.0f); // 256Mib
    config-&gt;setMaxWorkspaceSize(1 &lt;&lt; 28);
    builder-&gt;setMaxBatchSize(1); // 推理时 batchSize = 1 

    // ----------------------------- 3. 生成engine模型文件 -----------------------------
    //TensorRT 7.1.0版本已弃用buildCudaEngine方法，统一使用buildEngineWithConfig方法
    nvinfer1::ICudaEngine* engine = builder-&gt;buildEngineWithConfig(*network, *config);
    if(engine == nullptr){
        printf("Build engine failed.\n");
        return -1;
    }

    // ----------------------------- 4. 序列化模型文件并存储 -----------------------------
    // 将模型序列化，并储存为文件
    nvinfer1::IHostMemory* model_data = engine-&gt;serialize();
    FILE* f = fopen("engine.trtmodel", "wb");
    fwrite(model_data-&gt;data(), 1, model_data-&gt;size(), f);
    fclose(f);

    // 卸载顺序按照构建顺序倒序
    model_data-&gt;destroy();
    engine-&gt;destroy();
    network-&gt;destroy();
    config-&gt;destroy();
    builder-&gt;destroy();
    printf("Done.\n");
    return 0;
}</code></pre> 
<p></p> 
<p>Makefile :</p> 
<pre><code class="language-cpp">cc        := g++
name      := pro
workdir   := workspace
srcdir    := src
objdir    := objs
stdcpp    := c++11
cuda_home := /home/liuhongyuan/miniconda3/envs/trtpy/lib/python3.8/site-packages/trtpy/trt8cuda112cudnn8
syslib    := /home/liuhongyuan/miniconda3/envs/trtpy/lib/python3.8/site-packages/trtpy/lib
cpp_pkg   := /home/liuhongyuan/miniconda3/envs/trtpy/lib/python3.8/site-packages/trtpy/cpp-packages
cuda_arch := 
nvcc      := $(cuda_home)/bin/nvcc -ccbin=$(cc)

# 定义cpp的路径查找和依赖项mk文件
cpp_srcs := $(shell find $(srcdir) -name "*.cpp")
cpp_objs := $(cpp_srcs:.cpp=.cpp.o)
cpp_objs := $(cpp_objs:$(srcdir)/%=$(objdir)/%)
cpp_mk   := $(cpp_objs:.cpp.o=.cpp.mk)

# 定义cu文件的路径查找和依赖项mk文件
cu_srcs := $(shell find $(srcdir) -name "*.cu")
cu_objs := $(cu_srcs:.cu=.cu.o)
cu_objs := $(cu_objs:$(srcdir)/%=$(objdir)/%)
cu_mk   := $(cu_objs:.cu.o=.cu.mk)

# 定义opencv和cuda需要用到的库文件
link_cuda      := cudart cudnn
link_trtpro    := 
link_tensorRT  := nvinfer
link_opencv    := 
link_sys       := stdc++ dl
link_librarys  := $(link_cuda) $(link_tensorRT) $(link_sys) $(link_opencv)

# 定义头文件路径，请注意斜杠后边不能有空格
# 只需要写路径，不需要写-I
include_paths := src              \
    $(cuda_home)/include/cuda     \
	$(cuda_home)/include/tensorRT \
	$(cpp_pkg)/opencv4.2/include

# 定义库文件路径，只需要写路径，不需要写-L
library_paths := $(cuda_home)/lib64 $(syslib) $(cpp_pkg)/opencv4.2/lib

# 把library path给拼接为一个字符串，例如a b c =&gt; a:b:c
# 然后使得LD_LIBRARY_PATH=a:b:c
empty := 
library_path_export := $(subst $(empty) $(empty),:,$(library_paths))

# 把库路径和头文件路径拼接起来成一个，批量自动加-I、-L、-l
run_paths     := $(foreach item,$(library_paths),-Wl,-rpath=$(item))
include_paths := $(foreach item,$(include_paths),-I$(item))
library_paths := $(foreach item,$(library_paths),-L$(item))
link_librarys := $(foreach item,$(link_librarys),-l$(item))

# 如果是其他显卡，请修改-gencode=arch=compute_75,code=sm_75为对应显卡的能力
# 显卡对应的号码参考这里：https://developer.nvidia.com/zh-cn/cuda-gpus#compute
# 如果是 jetson nano，提示找不到-m64指令，请删掉 -m64选项。不影响结果
cpp_compile_flags := -std=$(stdcpp) -w -g -O0 -m64 -fPIC -fopenmp -pthread
cu_compile_flags  := -std=$(stdcpp) -w -g -O0 -m64 $(cuda_arch) -Xcompiler "$(cpp_compile_flags)"
link_flags        := -pthread -fopenmp -Wl,-rpath='$$ORIGIN'

cpp_compile_flags += $(include_paths)
cu_compile_flags  += $(include_paths)
link_flags        += $(library_paths) $(link_librarys) $(run_paths)

# 如果头文件修改了，这里的指令可以让他自动编译依赖的cpp或者cu文件
ifneq ($(MAKECMDGOALS), clean)
-include $(cpp_mk) $(cu_mk)
endif

$(name)   : $(workdir)/$(name)

all       : $(name)
run       : $(name)
	@cd $(workdir) &amp;&amp; ./$(name) $(run_args)

$(workdir)/$(name) : $(cpp_objs) $(cu_objs)
	@echo Link $@
	@mkdir -p $(dir $@)
	@$(cc) $^ -o $@ $(link_flags)

$(objdir)/%.cpp.o : $(srcdir)/%.cpp
	@echo Compile CXX $&lt;
	@mkdir -p $(dir $@)
	@$(cc) -c $&lt; -o $@ $(cpp_compile_flags)

$(objdir)/%.cu.o : $(srcdir)/%.cu
	@echo Compile CUDA $&lt;
	@mkdir -p $(dir $@)
	@$(nvcc) -c $&lt; -o $@ $(cu_compile_flags)

# 编译cpp依赖项，生成mk文件
$(objdir)/%.cpp.mk : $(srcdir)/%.cpp
	@echo Compile depends C++ $&lt;
	@mkdir -p $(dir $@)
	@$(cc) -M $&lt; -MF $@ -MT $(@:.cpp.mk=.cpp.o) $(cpp_compile_flags)
    
# 编译cu文件的依赖项，生成cumk文件
$(objdir)/%.cu.mk : $(srcdir)/%.cu
	@echo Compile depends CUDA $&lt;
	@mkdir -p $(dir $@)
	@$(nvcc) -M $&lt; -MF $@ -MT $(@:.cu.mk=.cu.o) $(cu_compile_flags)

# 定义清理指令
clean :
	@rm -rf $(objdir) $(workdir)/$(name) $(workdir)/*.trtmodel

# 防止符号被当做文件
.PHONY : clean run $(name)

# 导出依赖库路径，使得能够运行起来
export LD_LIBRARY_PATH:=$(library_path_export)</code></pre> 
<p>执行make run可编译运行代码。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/465c453000395e3c8360dd6d6940044c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">IPC和RPC</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/75f2a72a063268e54908a9b11c7d3ff9/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">python数据分析预处理z-score标准化</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>