<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>NLP预训练方法：从BERT到ALBERT详解 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="NLP预训练方法：从BERT到ALBERT详解" />
<meta property="og:description" content="BERT基于所有层中的左、右语境进行联合调整，来预训练深层双向表征。只需要增加一个输出层，就可以对预训练的BERT表征进行微调，就能够为更多的任务创建当前的最优模型。使用的是Transformer，相对于rnn而言更加高效、能捕捉更长距离的依赖。
1. 预训练优点 假设已有A训练集，先用A对网络进行预训练，在A任务上学会网络参数，然后保存以备后用，当来一个新的任务B，采取相同的网络结构，网络参数初始化的时候可以加载A学习好的参数，其他的高层参数随机初始化，之后用B任务的训练数据来训练网络，当加载的参数保持不变时，称为&#34;frozen&#34;，当加载的参数随着B任务的训练进行不断的改变，称为“fine-tuning”，即更好地把参数进行调整使得更适合当前的B任务
优点：当任务B的训练数据较少时，很难很好的训练网络，但是获得了A训练的参数，会比仅仅使用B训练的参数更优
2. BERT模型 BERT：全称是Bidirectional Encoder Representation from Transformers，即双向Transformer的Encoder，BERT的模型架构基于多层双向转换解码，因为decoder是不能获要预测的信息的，模型的主要创新点都在pre-traing方法上，即用了Masked LM和Next Sentence Prediction两种方法分别捕捉词语和句子级别的representation
其中“双向”表示模型在处理某一个词时，它能同时利用前面的词和后面的词两部分信息，这种“双向”的来源在于BERT与传统语言模型不同，它不是在给你大牛股所有前面词的条件下预测最可能的当前词，而是随机遮掩一些词，并利用所有没被遮掩的词进行预测
如图其中 BERT 和 ELMo 都使用双向信息，OpenAI GPT 使用单向信息。
3. ALBERT 通常情况下，增加预训练模型大小会带来效果的提升；然而，当模型大小达到一定的程度之后，就很难再进行了，因为受到了GPU内存和训练时间的限制。为了减小模型参数和模型训练时间，ALBERT提出了两种解决方法。ALBERT也是采用和Bert一样的Transformer的Encoder[1]结构，激活函数也是GLUE[2]。相比于Bert，ALBERT主要改进之处在于以下几点：Embedding因式分解(Factorized embedding parameterization)、层间参数共享(Cross-layer parameter sharing)、句子间关联损失(Inter-sentence coherence loss)。
3.1 Embedding因式分解 我们用H表示隐藏层大小，E表示embedding维度，V表示词汇表的大小。
Bert base模型的Encoder输出大小(H)和embedding维度(E)都是768，然而，ALBERT认为词级别的embedding是没有上下文依赖的表述；而隐藏层的输出值，不仅包括了词本身的意思，还包含了上下文信息。理论上来说隐藏层包含的信息更多一些，因此应该让H &gt;&gt; E。所以ALBERT的embedding维度是小于encoder输出维度的。在NLP任务中，通常词典都很大，embedding矩阵的大小是E×V，如果和Bert一样让H = E，那么embedding矩阵的参数量会很大，并且在反向传播过程中，更新的内容也比较稀疏。
结合上述所说的两点，ALBERT采用了一种因式分解的方法来降低参数量。首先把one-hot向量映射到一个低纬度的空间，大小为E，然后再映射到一个高纬度的空间(H)；从而把参数量从O(V×H)降低到了O(V×E &#43; E×H)，当E &lt;&lt; H时，参数量就减少得很明显了。
3.2 层间参数共享 Transformer中共享参数的方案有多种，比如，只共享全连接层，只共享Attention[1]层。ALBERT结合了上述两种方案，对全连接层和Attention都进行参数共享，也就是共享encoder所有的参数。同量级下的Transformer采用该共享方案后实际效果有所下降，但是参数量减少了很多，训练速度也提升了很多。
ALBERT减小了模型参数，提升了训练速度，且每一层的输出的embedding相比于BERT来说震荡幅度更小一些。可见参数共享其实是有稳定网络参数的作用的。
3.3 构建自学习任务-句子连贯性预测 在预训练任务上提高：改造NSP任务，强化网络学习句子的连续性；
NSP任务实际上是一个二分类任务，即预测两句话是采样于同一个文档中的两个连续的句子（正样本），还是采样于两个不同的文档中的句子（负样本）。NSP任务实际上包含两个子任务，即topic预测和关系一致性预测，而topic预测其实很简单；NSP在很多实践中被证明没有太好效果；
因此，ALBERT选择去除topic预测的影响，只保留关系一致性预测，于是提出了一个新的任务 Sentence-Order Prediction (SOP)，SOP的正样本和NSP的获取方式一样，负样本把正样本的两句话顺序反转；
3.4 去掉dropout dropout在防止过拟合上有显著效果，但是实际上MLM很难过拟合，去掉dropout，由于可以腾出很多临时变量占用的内存而使得内存上有所提升；
from transformers import AlbertConfig, TFAlbertMainLayer config = AlbertConfig(vocab_size=0,embedding_size=item_dim, hidden_size=hidden_size,num_hidden_layers=num_layers,num_hidden_groups=1,num_attention_heads=num_heads,intermediate_size=intermediate_size,inner_group_num=1,max_position_embeddings=self." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/d957d0be6fac7141269010f32c5a58bf/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-08-25T17:34:36+08:00" />
<meta property="article:modified_time" content="2021-08-25T17:34:36+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">NLP预训练方法：从BERT到ALBERT详解</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>BERT基于所有层中的左、右语境进行联合调整，来预训练深层双向表征。只需要增加一个输出层，就可以对预训练的BERT表征进行微调，就能够为更多的任务创建当前的最优模型。使用的是Transformer，相对于rnn而言更加高效、能捕捉更长距离的依赖。</p> 
<h3><a id="1__2"></a>1. 预训练优点</h3> 
<p>假设已有A训练集，先用A对网络进行预训练，在A任务上学会网络参数，然后保存以备后用，当来一个新的任务B，采取相同的网络结构，网络参数初始化的时候可以加载A学习好的参数，其他的高层参数随机初始化，之后用B任务的训练数据来训练网络，当加载的参数保持不变时，称为"frozen"，当加载的参数随着B任务的训练进行不断的改变，称为“fine-tuning”，即更好地把参数进行调整使得更适合当前的B任务</p> 
<p>优点：当任务B的训练数据较少时，很难很好的训练网络，但是获得了A训练的参数，会比仅仅使用B训练的参数更优</p> 
<h3><a id="2_BERT_7"></a>2. BERT模型</h3> 
<p>BERT：全称是Bidirectional Encoder Representation from Transformers，即双向Transformer的Encoder，BERT的模型架构基于多层双向转换解码，因为decoder是不能获要预测的信息的，模型的主要创新点都在pre-traing方法上，即用了Masked LM和Next Sentence Prediction两种方法分别捕捉词语和句子级别的representation</p> 
<p>其中“双向”表示模型在处理某一个词时，它能同时利用前面的词和后面的词两部分信息，这种“双向”的来源在于BERT与传统语言模型不同，它不是在给你大牛股所有前面词的条件下预测最可能的当前词，而是随机遮掩一些词，并利用所有没被遮掩的词进行预测</p> 
<p><img src="https://images2.imgbox.com/f6/0d/1iOQJZ8j_o.png" alt="在这里插入图片描述"><br> 如图其中 BERT 和 ELMo 都使用双向信息，OpenAI GPT 使用单向信息。</p> 
<h3><a id="3_ALBERT_15"></a>3. ALBERT</h3> 
<p>通常情况下，增加预训练模型大小会带来效果的提升；然而，当模型大小达到一定的程度之后，就很难再进行了，因为受到了GPU内存和训练时间的限制。为了减小模型参数和模型训练时间，ALBERT提出了两种解决方法。ALBERT也是采用和Bert一样的Transformer的Encoder[1]结构，激活函数也是GLUE[2]。相比于Bert，ALBERT主要改进之处在于以下几点：Embedding因式分解(Factorized embedding parameterization)、层间参数共享(Cross-layer parameter sharing)、句子间关联损失(Inter-sentence coherence loss)。</p> 
<h4><a id="31_Embedding_17"></a>3.1 Embedding因式分解</h4> 
<p>我们用H表示隐藏层大小，E表示embedding维度，V表示词汇表的大小。<br> Bert base模型的Encoder输出大小(H)和embedding维度(E)都是768，然而，ALBERT认为词级别的embedding是没有上下文依赖的表述；而隐藏层的输出值，不仅包括了词本身的意思，还包含了上下文信息。理论上来说隐藏层包含的信息更多一些，因此应该让H &gt;&gt; E。所以ALBERT的embedding维度是小于encoder输出维度的。在NLP任务中，通常词典都很大，embedding矩阵的大小是E×V，如果和Bert一样让H = E，那么embedding矩阵的参数量会很大，并且在反向传播过程中，更新的内容也比较稀疏。<br> <img src="https://images2.imgbox.com/e2/aa/GLUUQU8B_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/3e/4e/kGH2i6ht_o.png" alt="在这里插入图片描述"></p> 
<p>结合上述所说的两点，ALBERT采用了一种因式分解的方法来降低参数量。首先把one-hot向量映射到一个低纬度的空间，大小为E，然后再映射到一个高纬度的空间(H)；从而把参数量从O(V×H)降低到了O(V×E + E×H)，当E &lt;&lt; H时，参数量就减少得很明显了。</p> 
<h4><a id="32__24"></a>3.2 层间参数共享</h4> 
<p>Transformer中共享参数的方案有多种，比如，只共享全连接层，只共享Attention[1]层。ALBERT结合了上述两种方案，对全连接层和Attention都进行参数共享，也就是共享encoder所有的参数。同量级下的Transformer采用该共享方案后实际效果有所下降，但是参数量减少了很多，训练速度也提升了很多。<br> ALBERT减小了模型参数，提升了训练速度，且每一层的输出的embedding相比于BERT来说震荡幅度更小一些。可见参数共享其实是有稳定网络参数的作用的。</p> 
<h4><a id="33__27"></a>3.3 构建自学习任务-句子连贯性预测</h4> 
<p>在预训练任务上提高：改造NSP任务，强化网络学习句子的连续性；<br> NSP任务实际上是一个二分类任务，即预测两句话是采样于同一个文档中的两个连续的句子（正样本），还是采样于两个不同的文档中的句子（负样本）。NSP任务实际上包含两个子任务，即topic预测和关系一致性预测，而topic预测其实很简单；NSP在很多实践中被证明没有太好效果；<br> 因此，ALBERT选择去除topic预测的影响，只保留关系一致性预测，于是提出了一个新的任务 Sentence-Order Prediction (SOP)，SOP的正样本和NSP的获取方式一样，负样本把正样本的两句话顺序反转；</p> 
<h4><a id="34_dropout_31"></a>3.4 去掉dropout</h4> 
<p>dropout在防止过拟合上有显著效果，但是实际上MLM很难过拟合，去掉dropout，由于可以腾出很多临时变量占用的内存而使得内存上有所提升；</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AlbertConfig<span class="token punctuation">,</span> TFAlbertMainLayer
config <span class="token operator">=</span> AlbertConfig<span class="token punctuation">(</span>vocab_size<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>embedding_size<span class="token operator">=</span>item_dim<span class="token punctuation">,</span> hidden_size<span class="token operator">=</span>hidden_size<span class="token punctuation">,</span>num_hidden_layers<span class="token operator">=</span>num_layers<span class="token punctuation">,</span>num_hidden_groups<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>num_attention_heads<span class="token operator">=</span>num_heads<span class="token punctuation">,</span>intermediate_size<span class="token operator">=</span>intermediate_size<span class="token punctuation">,</span>inner_group_num<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>max_position_embeddings<span class="token operator">=</span>self<span class="token punctuation">.</span>max_len<span class="token punctuation">)</span>
bert <span class="token operator">=</span> TFAlbertMainLayer<span class="token punctuation">(</span>config<span class="token operator">=</span>config<span class="token punctuation">)</span>
seq_emb <span class="token operator">=</span> bert<span class="token punctuation">(</span>seq_emb<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

</code></pre> 
<p>参考文献：<br> https://zhuanlan.zhihu.com/p/108114453<br> https://blog.csdn.net/yangfengling1023/article/details/84025313</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/60c4fb4013f489675230fccb89b3ee1f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">反弹shell与正向shell的区别</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/435f3078aef91f56f35b8d4397c93a3d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【深度学习】目前几种热门的数据增强方法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>