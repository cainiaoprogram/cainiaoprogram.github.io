<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>学习笔记(二):逻辑斯蒂回归算法（Logistic Regression） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="学习笔记(二):逻辑斯蒂回归算法（Logistic Regression）" />
<meta property="og:description" content="一、算法简介
代码下载:https://download.csdn.net/download/xdg2008/16744956?spm=1001.2014.3001.5503
1.1 定义
逻辑斯蒂回归(Logistic Regression) 虽然名字中有回归，但模型最初是为了解决二分类问题,后来可以解决多分类问题。
线性回归模型帮助我们用最简单的线性方程实现了对数据的拟合，但只实现了回归而无法进行分类。因此LR就是在线性回归的基础上，构造的一种分类模型。
对线性模型进行分类如二分类任务，简单的是通过阶跃函数(unit-step function)，即将线性模型的输出值套上一个函数进行分割，大于z的判定为0，小于z的判定为1。
但这样的分段函数数学性质不好，既不连续也不可微。因此有人提出了对数几率函数，见上图右，简称Sigmoid函数。
该函数具有很好的数学性质，既可以用于预测类别，并且任意阶可微，因此可用于求解最优解。将函数带进去，可得LR模型为
其实，LR 模型就是在拟合 z = w^T x &#43;b 这条直线，使得这条直线尽可能地将原始数据中的两个类别正确的划分开
1.2 损失函数表达式
回归问题的损失函数一般为平均误差平方损失 MSE，LR解决二分类问题中，损失函数为如下形式
L=∑ylogy&#43;(1-y)log(1-y)
1.3梯度下降
得到了逻辑回归的表达式，下一步跟线性回归类似，构建似然函数，然后最大似然估计，最终推导出θ的迭代更新表达式。这个思路不清楚的请参考文章《线性回归、梯度下降》，只不过这里用的不是梯度下降，而是梯度上升，因为这里是最大化似然函数不是最小化似然函数
L=∑ylogy&#43;(1-y)log(1-y)
转换后的似然函数对w求偏导，在这里我们以只有一个训练样本的情况为例：
这个求偏导过程第一步是对θ偏导的转化，依据偏导公式：y=lnx y&#39;=1/x。
第二步是根据g(z)求导的特性g&#39;(z) = g(z)(1 - g(z)) 。
第三步就是普通的变换。
这样我们就得到了梯度上升每次迭代的更新方向，那么θ的迭代表达式为：
逻辑回归误差对w和b的偏导数∂1,∂2 有以下公式:
∂1_w=X*(H(X)-Y)
其中:H(X)=sigmoid(Feature*Weight),Y=Label,X=Feature.T
二、案列实现
2.1 案列背景 杜紫藤女士, 一直在某婚恋网站上寻找自己中意的他, 网站给她推荐了很多男士, 但是她并不是每个都喜欢, 网站统计了这些男士的情况，
这些男士可以划分为如下几类:
(1)不喜欢的人(didntLike)
(2)魅力一般的人(smallDoses)
(3)极具魅力的人(largeDoses)
2.2 特征数字
杜紫藤女士收集了一些样本数据，她把样本数据存放在datingTestSet.txt文本文件中,每个样本数据占据一行，总共有1000行。
这些样本数据主要有以下3个特征:
（1)每年获得的飞行常客里程数
(2)玩视频游戏所消耗的时间百分比
(3)每周消费的冰淇淋公升数
3.3算法及代码:
import numpy as np
def filetomatrix(filename):" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/4e2ed095ac8c8f382dce8019f5a490b6/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-01-21T22:29:36+08:00" />
<meta property="article:modified_time" content="2021-01-21T22:29:36+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">学习笔记(二):逻辑斯蒂回归算法（Logistic Regression）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>一、算法简介</p> 
<p>代码下载:<a href="https://download.csdn.net/download/xdg2008/16744956?spm=1001.2014.3001.5503">https://download.csdn.net/download/xdg2008/16744956?spm=1001.2014.3001.5503</a></p> 
<p>1.1 定义</p> 
<p>逻辑斯蒂回归(Logistic Regression) 虽然名字中有回归，但模型最初是为了解决二分类问题,后来可以解决多分类问题。</p> 
<p>线性回归模型帮助我们用最简单的线性方程实现了对数据的拟合，但只实现了回归而无法进行分类。因此LR就是在线性回归的基础上，构造的一种分类模型。</p> 
<p>对线性模型进行分类如二分类任务，简单的是通过阶跃函数(unit-step function)，即将线性模型的输出值套上一个函数进行分割，大于z的判定为0，小于z的判定为1。</p> 
<p><img alt="" src="https://images2.imgbox.com/04/be/Cjukl2uY_o.png"></p> 
<p><img alt="" height="86" src="https://images2.imgbox.com/cd/d8/EbDM9UXa_o.png" width="180"></p> 
<p>但这样的分段函数数学性质不好，既不连续也不可微。因此有人提出了对数几率函数，见上图右，简称Sigmoid函数。</p> 
<p><img alt="" height="57" src="https://images2.imgbox.com/43/83/5VSrrKZw_o.png" width="99"></p> 
<p>该函数具有很好的数学性质，既可以用于预测类别，并且任意阶可微，因此可用于求解最优解。将函数带进去，可得LR模型为</p> 
<p><img alt="" height="47" src="https://images2.imgbox.com/a7/fe/6YQanxBs_o.png" width="125"></p> 
<p>其实，LR 模型就是在拟合 z = w^T x +b 这条直线，使得这条直线尽可能地将原始数据中的两个类别正确的划分开</p> 
<p>1.2 损失函数表达式</p> 
<p>    回归问题的损失函数一般为平均误差平方损失 MSE，LR解决二分类问题中，损失函数为如下形式</p> 
<p>    L=∑ylogy+(1-y)log(1-y)</p> 
<p>   <img alt="" src="https://images2.imgbox.com/a2/51/7tv1l4Ah_o.png"></p> 
<p>1.3梯度下降</p> 
<p>     得到了逻辑回归的表达式，下一步跟线性回归类似，构建似然函数，然后最大似然估计，最终推导出θ的迭代更新表达式。这个思路不清楚的请参考文章《<a href="http://www.cnblogs.com/BYRans/p/4700202.html" rel="nofollow">线性回归、梯度下降</a>》，只不过这里用的不是梯度下降，而是梯度上升，因为这里是最大化似然函数不是最小化似然函数</p> 
<p>     L=∑ylogy+(1-y)log(1-y)</p> 
<p>    转换后的似然函数对w求偏导，在这里我们以只有一个训练样本的情况为例：</p> 
<p style="text-align:center;"><img alt="" height="152" src="https://images2.imgbox.com/59/35/zFNl5XuA_o.png" width="554"></p> 
<p>    这个求偏导过程第一步是对θ偏导的转化，依据偏导公式：y=lnx y'=1/x。</p> 
<p>    第二步是根据g(z)求导的特性g'(z) = g(z)(1 - g(z)) 。</p> 
<p>    第三步就是普通的变换。</p> 
<p>    这样我们就得到了梯度上升每次迭代的更新方向，那么θ的迭代表达式为：</p> 
<p>     逻辑回归误差对w和b的偏导数∂1,∂2 有以下公式:<img alt="" src="https://images2.imgbox.com/37/2b/5zi15SGu_o.png"></p> 
<p>      ∂1_w=X*(H(X)-Y)</p> 
<p>    其中:H(X)=sigmoid(Feature*Weight),Y=Label,X=Feature.T</p> 
<p>二、案列实现</p> 
<p>    2.1 案列背景 </p> 
<p>       杜紫藤女士, 一直在某婚恋网站上寻找自己中意的他, 网站给她推荐了很多男士, 但是她并不是每个都喜欢, 网站统计了这些男士的情况，</p> 
<p>        这些男士可以划分为如下几类:</p> 
<p>        (1)不喜欢的人(didntLike)</p> 
<p>        (2)魅力一般的人(smallDoses)</p> 
<p>        (3)极具魅力的人(largeDoses)</p> 
<p>    2.2 特征数字</p> 
<p>        杜紫藤女士收集了一些样本数据，她把样本数据存放在datingTestSet.txt文本文件中,每个样本数据占据一行，总共有1000行。</p> 
<p>        这些样本数据主要有以下3个特征:</p> 
<p>       （1)每年获得的飞行常客里程数</p> 
<p>         (2)玩视频游戏所消耗的时间百分比</p> 
<p>         (3)每周消费的冰淇淋公升数</p> 
<p>   3.3算法及代码:</p> 
<p>        import numpy as np</p> 
<p>       def filetomatrix(filename):<br>             fr = open(filename)<br>             arrayOLines = fr.readlines()             #读取文件的所有行<br>            numberOfLines = len(arrayOLines)        #获取文件行数<br>            returnMat =np.zeros((numberOfLines,3))  #返回Numpy矩阵,解析完成的数据为numberOfLines行，3列零数据矩阵classLabelVector = []  #返回的分类标签向量 <br>            classLabelVector = []                   #返回的分类标签向量 <br>            index = 0<br>           for line in arrayOLines:    <br>               line = line.strip()                         #使用line.strip()函数截取掉所有的空白符，s.strip(rm),当rm空时，默认删除空白符（包括'\n','\r','\t',' ')<br>               listFromLine = line.split('\t')             #将line根据'\t'分隔符进行切片<br>              returnMat[index,:] = listFromLine[0:3]      #选取前三列元素，将它们存储到特征矩阵中<br>              classLabelVector.append(listFromLine[-1])   #将最后一列数据加入classLabelVector中，作为情感分类标签<br>              index += 1<br>          return returnMat,classLabelVector</p> 
<p>      rmat1,label1=filetomatrix('D:\Train_test\datingTestSet.txt')</p> 
<p>       lab=[]<br>        l=[] </p> 
<p>      for i,item in enumerate(label1):<br>             if item =='largeDoses':<br>                 lab.append(0)<br>            elif item =='smallDoses':<br>                 lab.append(1)<br>           elif item =='didntLike':<br>                 lab.append(2)</p> 
<p>     one-hot编码<br>     label11=np.array(lab).astype(int)<br>     label12=np.eye(3)[label11]</p> 
<p>     def sigmoid(z):<br>           return 1 / (1 + np.exp(-z))</p> 
<p>def gradentdecent(train,target):<br>     #feature = train[:,0:1]<br>     feature = train<br>     ones = np.ones((len(feature),1))<br>     Feature = np.hstack((feature ,ones))<br>     Label = target<br>     weight = np.ones((Feature.shape[1],target.shape[1]))<br>     changeweight  = np.zeros((Feature.shape[1],target.shape[1]))<br>     msehistory = []<br>     learningrate = 1<br>     for i in range(10000):<br>         y = np.dot(Feature,weight)<br>         error=sigmoid(y)-Label<br>         mse = np.sum(np.power(error,2))<br>         msehistory.append(mse)<br>         if len(msehistory)&gt;=2:<br>             if(msehistory[-1]&gt;msehistory[-2]):<br>                 learningrate = learningrate/2<br>             else:<br>                 learningrate = learningrate * 1.1<br>         change = np.dot(Feature.T,error)<br>         changeweight = changeweight + change**2       <br>         weight = weight - learningrate* change/np.sqrt(np.float32(changeweight))<br>         if(np.sum(np.square(change))&lt;0.01):<br>             break<br>     return weight</p> 
<p>def datingClassTest(train,target,lab):<br>     Ratio = 0.10                                                                  #使用Ratio比例来分割预测数据和测试数据<br>     m = train.shape[0]<br>     numTestVecs = int(m*Ratio)                 #计算测试向<br>     weight=gradentdecent(train[numTestVecs:m,:],target[numTestVecs:m])<br>     errorCount = 0.0<br>     for i in range(numTestVecs):               #0到numTestVecs为测试数据集,numTestVecs到为训练数据集<br>         #classifierResult = knn(feature[numTestVecs:m,:],datingLabels[numTestVecs:m],feature[i,:],8)<br>         value = train[i,:].reshape(1,-1)<br>         ones = np.ones((len(value),1))<br>         vas = np.hstack((value ,ones))<br>         np.set_printoptions(suppress=True)<br>         result=np.exp(np.dot(vas,weight))/np.sum(np.exp(np.dot(vas,weight)))<br>         expect = np.argmax(result)<br>         if expect==0:<br>             classifierResult="largeDose"<br>         elif expect==1:<br>             classifierResult="smallDose"<br>         elif expect==2:<br>             classifierResult="didntLike"<br>         <br>         if lab[i]==0:<br>             datingLabels="largeDose"<br>         elif lab[i]==1:<br>             datingLabels="smallDose"<br>         elif lab[i]==2:<br>             datingLabels="didntLike"<br>         print("预测分类类别:%s,真实列表类别:%s" % (classifierResult,datingLabels))<br>         if (classifierResult != datingLabels) : errorCount +=1.0<br>            print ("错误率:%f " % (errorCount/float(numTestVecs)))</p> 
<p>     datingClassTest(rmat1,label12,lab)</p> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/2c6d28f2f1930cad27c0e2fc402d11e2/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">TextView跑马灯效果不生效的原因分析及解决方案</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/5f1622050a1c436f7b97583edc1f874e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">lotus version 1.4.1 AddPiece 优化</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>