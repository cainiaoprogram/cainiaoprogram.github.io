<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【论文精读】Arxiv 2023 - Segment Anything - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【论文精读】Arxiv 2023 - Segment Anything" />
<meta property="og:description" content="【论文精读】Arxiv 2023 - 分割一切 【论文原文】：Segment Anything
【作者信息】：Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll’ar, Piotr and Girshick, Ross
论文：https://arxiv.org/pdf/2304.02643.pdf 代码：https://github.com/facebookresearch/segment-anything 博主关键词：语义分割，少样本，零样本，提示学习
推荐论文：无
摘要 我们介绍了Segment Anything (SA)项目:一个用于图像分割的新任务、模型和数据集。在数据收集循环中使用我们的高效模型，我们构建了迄今为止(到目前为止)最大的分割数据集，在1100万张授权的图像上拥有超过10亿个掩码。该模型被设计和训练为可提示的，因此它可以将零样本迁移到新的图像分布和任务。我们评估了它在许多任务上的能力，发现它的零样本表现令人印象深刻——经常与之前的完全监督结果媲美，甚至更好。我们在https://segment-anything.com上发布了分割任何模型(SAM)和对应的10亿掩模和1100万图像数据集(SA-1B)，以促进对计算机视觉基础模型的研究。
1、简介 在网络规模的数据集上预训练的大型语言模型正在以强大的零样本和少样本泛化彻底改变NLP。这些“基础模型”[8]可以泛化到训练过程中看不到的任务和数据分布。这种功能通常通过提示工程实现，在提示工程中，使用手工制作的文本提示语言模型为手头的任务生成有效的文本响应。当用网络上丰富的文本语料进行扩展和训练时，这些模型的零样本和少样本性能与微调模型(在某些情况下甚至匹配)相比惊人地好[10,21]。经验趋势表明，这种行为随着模型规模、数据集大小和总训练计算而改善[56,10,21,51]。
基础模型也在计算机视觉中进行了探索，尽管程度较轻。也许最突出的说明是将成对的文本和图像从网络上对齐。例如，CLIP[82]和ALIGN[55]使用对比性学习来训练文本和图像编码器，使两种模式对齐。一旦训练完成，工程化的文本提示就能实现对新的视觉概念和数据分布的零样本泛化。这样的编码器还能与其他模块有效地组合，以实现下游任务，如图像生成（如DALL-E[83]）。虽然在视觉和语言编码器方面已经取得了很多进展，但计算机视觉包括了超出这一范围的广泛问题，而且对于其中的许多问题，并不存在丰富的训练数据。
在这项工作中，我们的目标是建立一个基础的图像分割模型。也就是说，我们试图开发一个可提示的模型，并在一个广泛的数据集上使用一个能实现强大泛化的任务对其进行预训练。有了这个模型，我们的目标是利用提示工程解决新数据分布上的一系列下游分割问题。
这个计划的成功取决于三个部分：任务、模型和数据。为了发展它们，我们解决了以下关于图像分割的问题：
什么任务可以实现零样本泛化？什么是相应的模型结构？什么数据可以为这个任务和模型提供动力？ 这些问题错综复杂，需要综合解决。我们首先定义了一个可提示的分割任务，该任务足够通用，可以提供强大的预训练目标，并实现广泛的下游应用。此任务需要一个支持灵活提示的模型，并且可以在提示时实时输出分割掩码，以便进行交互使用。为了训练我们的模型，我们需要一个多样化的、大规模的数据源。不幸的是，没有用于分割的网络规模的数据源；为了解决这个问题，我们构建了一个“数据引擎”，即我们在使用高效的模型来帮助数据收集和使用新收集的数据来改进模型之间进行迭代。接下来，我们介绍每个互连的组件，然后是我们创建的数据集和证明我们方法有效性的实验。
任务。在NLP和最近的计算机视觉中，基础模型是一个很有前途的发展，使用“提示”技术情况下，它可以对新数据集和任务执行零样本和少样本学习。受这项工作的启发，我们提出了可提示的分割任务，其中的目标是在给定任何分割提示的情况下返回有效的分割掩码（见图第1a段）。提示只是指定在图像中分割什么，例如，提示可以包括标识对象的空间或文本信息。有效输出掩码的要求意味着，即使提示不明确，并且可能涉及多个目标（例如，衬衫上的一个点可能指示衬衫或穿着衬衫的人），输出也应该是这些目标中至少一个的合理掩码。我们使用可提示的分割任务作为预训练目标，并通过提示工程解决一般的下游分割任务。
模型。可提示的分割任务和现实世界使用的目标对模型体系结构施加了约束。特别是，该模型必须支持灵活的提示，需要实时计算掩码以允许交互式使用，并且必须具有模糊性。令人惊讶的是，我们发现一个简单的设计满足了所有三个约束：一个强大的图像编码器计算图像嵌入，一个提示编码器嵌入提示，然后将这两个信息源组合在一个预测分割掩码的轻量级掩码解码器中。我们将此模型称为Segment Anything Model或SAM（见图1b）。通过将SAM分离为图像编码器和快速提示编码器/掩码解码器，可以在不同提示下重复使用相同的图像嵌入（并分摊其成本）。在给定图像嵌入的情况下，在web浏览器中提示编码器和掩码解码器从提示预测掩码的时间大约为50ms。我们专注于点、框和掩码提示，并通过自由形式的文本提示显示初始结果。为了让SAM意识到歧义，我们将其设计为预测单个提示的多个掩码，使SAM能够自然地处理歧义，例如衬衫和人的例子。
数据引擎。为了实现对新数据分布的强泛化，我们发现有必要在一组庞大而多样的掩码上训练SAM，超越现有的任何分割数据集。虽然基础模型的一种典型方法是在线获取数据[82]，但掩码并不自然丰富，因此我们需要一种替代策略。我们的解决方案是建立一个“数据引擎”，即我们与模型在环数据集标注共同开发我们的模型（见图1c）。我们的数据引擎有三个阶段：辅助手动(assisted-manual)、半自动(semi-automatic)和全自动(fully automatic)。在第一阶段，SAM帮助标注者标注掩码，类似于经典的交互式分段设置。在第二阶段，SAM可以通过提示可能的对象位置来自动生成对象子集的掩码，标注者专注于标注其余对象，有助于增加掩码的多样性。在最后阶段，我们用前景点的规则网格提示SAM，平均每张图像产生约100个高质量掩码。
数据集。我们最终的数据集，SA-1B，包括来自1100万张许可和隐私保护图像的10亿多个掩码（见图2）。SA-1B是使用我们的数据引擎的最后阶段完全自动收集的，比任何现有的分割数据集[66, 44, 117, 60]有400倍的掩码，而且正如我们广泛验证的那样，这些掩码具有高质量和多样性。除了用于训练SAM的鲁棒性和通用性，我们希望SA-1B成为旨在建立新基础模型的研究的宝贵资源。
负责任的AI。我们研究并报告了使用SA-1B和SAM时潜在的公平性问题和偏见。SA-1B中的图像跨越了地理上和经济上不同的国家，我们发现SAM在不同人群中的表现相似。我们希望这将使我们的工作在现实世界的使用案例中更加公平。我们在附录中提供了模型和数据集卡。
实验。我们对SAM进行了广泛的评估。首先，使用一套新的23个分割数据集，我们发现SAM从单个前景点生成高质量的掩码，通常仅略低于手动标注的真实情况。其次，我们使用提示工程在零样本传输协议下的各种下游任务上发现了一致的强定量和定性结果，包括边缘检测、对象建议生成、实例分割和文本到掩码预测的初步探索。这些结果表明，SAM可以与即时工程一起开箱即用，以解决涉及SAM训练数据之外的对象和图像分布的各种任务。尽管如此，正如我们在第8节中所讨论的那样，仍有改进的空间。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/a7ed204044dea8e0a9537a53eaec3c72/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-12T11:50:42+08:00" />
<meta property="article:modified_time" content="2023-04-12T11:50:42+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【论文精读】Arxiv 2023 - Segment Anything</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="Arxiv_2023___0"></a>【论文精读】Arxiv 2023 - 分割一切</h2> 
<p>【论文原文】：Segment Anything</p> 
<p>【作者信息】：Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll’ar, Piotr and Girshick, Ross</p> 
<pre><code>论文：https://arxiv.org/pdf/2304.02643.pdf
代码：https://github.com/facebookresearch/segment-anything
</code></pre> 
<p><strong>博主关键词</strong>：语义分割，少样本，零样本，提示学习</p> 
<p><strong>推荐论文</strong>：无</p> 
<p><img src="https://images2.imgbox.com/1e/01/QJPZcvYN_o.png" alt=" "></p> 
<h3><a id="_18"></a>摘要</h3> 
<p>我们介绍了Segment Anything (SA)项目:一个用于图像分割的新任务、模型和数据集。在数据收集循环中使用我们的高效模型，我们构建了迄今为止(到目前为止)最大的分割数据集，在1100万张授权的图像上拥有超过10亿个掩码。该<strong>模型被设计和训练为可提示的，因此它可以将零样本迁移到新的图像分布和任务</strong>。我们评估了它在许多任务上的能力，发现它的零样本表现令人印象深刻——经常与之前的完全监督结果媲美，甚至更好。我们在https://segment-anything.com上发布了分割任何模型(SAM)和对应的10亿掩模和1100万图像数据集(SA-1B)，以促进对计算机视觉基础模型的研究。</p> 
<h3><a id="1_22"></a>1、简介</h3> 
<p>在网络规模的数据集上预训练的大型语言模型正在以强大的零样本和少样本泛化彻底改变NLP。这些“基础模型”[8]可以泛化到训练过程中看不到的任务和数据分布。这种功能通常通过提示工程实现，在提示工程中，使用手工制作的文本提示语言模型为手头的任务生成有效的文本响应。当用网络上丰富的文本语料进行扩展和训练时，这些模型的零样本和少样本性能与微调模型(在某些情况下甚至匹配)相比惊人地好[10,21]。经验趋势表明，这种行为<strong>随着模型规模、数据集大小和总训练计算而改善</strong>[56,10,21,51]。</p> 
<p><img src="https://images2.imgbox.com/3b/f4/pcM3CzB5_o.png" alt=" "></p> 
<p><strong>基础模型也在计算机视觉中进行了探索，尽管程度较轻</strong>。也许最突出的说明是将成对的文本和图像从网络上对齐。例如，CLIP[82]和ALIGN[55]使用对比性学习来训练文本和图像编码器，使两种模式对齐。一旦训练完成，工程化的文本提示就能实现对新的视觉概念和数据分布的零样本泛化。这样的编码器还能与其他模块有效地组合，以实现下游任务，如图像生成（如DALL-E[83]）。虽然在视觉和语言编码器方面已经取得了很多进展，但<strong>计算机视觉包括了超出这一范围的广泛问题</strong>，而且对于其中的许多问题，并不存在丰富的训练数据。</p> 
<p>在这项工作中，我们的<strong>目标是建立一个基础的图像分割模型</strong>。也就是说，我们试图<strong>开发一个可提示的模型，并在一个广泛的数据集上使用一个能实现强大泛化的任务对其进行预训练</strong>。有了这个模型，我们的<strong>目标是利用提示工程解决新数据分布上的一系列下游分割问题</strong>。</p> 
<p>这个计划的成功取决于三个部分：<strong>任务</strong>、<strong>模型</strong>和<strong>数据</strong>。为了发展它们，我们解决了以下关于图像分割的问题：</p> 
<ol><li>什么任务可以实现零样本泛化？</li><li>什么是相应的模型结构？</li><li>什么数据可以为这个任务和模型提供动力？</li></ol> 
<p>这些问题错综复杂，需要综合解决。我们首先定义了一个可提示的分割任务，该<strong>任务足够通用，可以提供强大的预训练目标，并实现广泛的下游应用</strong>。此<strong>任务需要一个支持灵活提示的模型，并且可以在提示时实时输出分割掩码，以便进行交互使用</strong>。为了训练我们的模型，我们<strong>需要一个多样化的、大规模的数据源</strong>。不幸的是，没有用于分割的网络规模的数据源；为了解决这个问题，我们构建了一个“数据引擎”，即我们在使用高效的模型来帮助数据收集和使用新收集的数据来改进模型之间进行迭代。接下来，我们介绍每个互连的组件，然后是我们创建的数据集和证明我们方法有效性的实验。</p> 
<p><strong>任务</strong>。在NLP和最近的计算机视觉中，基础模型是一个很有前途的发展，使用“提示”技术情况下，它可以对新数据集和任务执行零样本和少样本学习。受这项工作的启发，我们提出了可提示的分割任务，其中的目标是在给定任何分割提示的情况下返回有效的分割掩码（见图第1a段）。提示只是指定在图像中分割什么，例如，提示可以包括标识对象的空间或文本信息。<strong>有效输出掩码的要求意味着，即使提示不明确，并且可能涉及多个目标（例如，衬衫上的一个点可能指示衬衫或穿着衬衫的人），输出也应该是这些目标中至少一个的合理掩码</strong>。我们使用可提示的分割任务作为预训练目标，并通过提示工程解决一般的下游分割任务。</p> 
<p><strong>模型</strong>。可提示的分割任务和现实世界使用的目标对模型体系结构施加了约束。特别是，该模型必须支持灵活的提示，需要<strong>实时</strong>计算掩码以允许交互式使用，并且必须具有<strong>模糊性</strong>。令人惊讶的是，我们发现一个简单的设计满足了所有三个约束：<strong>一个强大的图像编码器计算图像嵌入</strong>，<strong>一个提示编码器嵌入提示</strong>，然后将这两个信息源组合在<strong>一个预测分割掩码的轻量级掩码解码器</strong>中。我们将此模型称为Segment Anything Model或SAM（见图1b）。通过将SAM分离为图像编码器和快速提示编码器/掩码解码器，可以在不同提示下重复使用相同的图像嵌入（并分摊其成本）。在给定图像嵌入的情况下，在web浏览器中提示编码器和掩码解码器从提示预测掩码的时间大约为50ms。我们专注于<strong>点、框和掩码提示</strong>，并通过自由形式的文本提示显示初始结果。为了让SAM意识到歧义，我们将其设计为预测单个提示的多个掩码，使SAM能够自然地处理歧义，例如衬衫和人的例子。</p> 
<p><strong>数据引擎</strong>。为了实现对新数据分布的强泛化，我们发现有必要在一组庞大而多样的掩码上训练SAM，超越现有的任何分割数据集。虽然基础模型的一种典型方法是在线获取数据[82]，但掩码并不自然丰富，因此我们需要一种替代策略。我们的解决方案是建立一个“数据引擎”，即我们与模型在环数据集标注共同开发我们的模型（见图1c）。我们的数据引擎有三个阶段：<strong>辅助手动</strong>(assisted-manual)、<strong>半自动</strong>(semi-automatic)和<strong>全自动</strong>(fully automatic)。在第一阶段，<strong>SAM帮助标注者标注掩码，类似于经典的交互式分段设置</strong>。在第二阶段，<strong>SAM可以通过提示可能的对象位置来自动生成对象子集的掩码，标注者专注于标注其余对象，有助于增加掩码的多样性</strong>。在最后阶段，我们用前景点的规则网格提示SAM，平均每张图像产生约100个高质量掩码。</p> 
<p><strong>数据集</strong>。我们最终的数据集，SA-1B，包括来自1100万张许可和隐私保护图像的10亿多个掩码（见图2）。SA-1B是使用我们的数据引擎的最后阶段完全自动收集的，比任何现有的分割数据集[66, 44, 117, 60]有400倍的掩码，而且正如我们广泛验证的那样，这些掩码具有高质量和多样性。除了用于训练SAM的鲁棒性和通用性，我们希望SA-1B成为旨在建立新基础模型的研究的宝贵资源。</p> 
<p><strong>负责任的AI</strong>。我们研究并报告了使用SA-1B和SAM时潜在的公平性问题和偏见。SA-1B中的图像跨越了地理上和经济上不同的国家，我们发现SAM在不同人群中的表现相似。我们希望这将使我们的工作在现实世界的使用案例中更加公平。我们在附录中提供了模型和数据集卡。</p> 
<p><strong>实验</strong>。我们对SAM进行了广泛的评估。首先，使用一套新的23个分割数据集，我们发现SAM从单个前景点生成高质量的掩码，通常仅略低于手动标注的真实情况。其次，我们使用提示工程在零样本传输协议下的各种下游任务上发现了一致的强定量和定性结果，包括边缘检测、对象建议生成、实例分割和文本到掩码预测的初步探索。这些结果表明，SAM可以与即时工程一起开箱即用，以解决涉及SAM训练数据之外的对象和图像分布的各种任务。尽管如此，正如我们在第8节中所讨论的那样，仍有改进的空间。</p> 
<h3><a id="2_53"></a>2、分割一切任务</h3> 
<p>我们从NLP中获得了灵感，在NLP中，<strong>next token prediction</strong>任务用于基础模型预训练，并通过即时工程解决不同的下游任务[10]。为了建立分割的基础模型，我们的目标是定义一个具有类似功能的任务。</p> 
<p><strong>任务</strong>。我们首先将提示的概念从NLP转换为分割，其中<strong>提示可以是一组前景/背景点、粗略框或掩码、自由格式文本，或者通常是指示在图像中分割什么的任何信息</strong>。那么，可提示的分割任务是在给定任何提示的情况下返回<strong>有效的分割掩码</strong>。“有效”掩码的要求只是意味着，即使提示不明确，并且可能涉及多个对象（例如，衬衫与人的例子，见图3），输出也应该是其中至少一个对象的合理掩码。这一要求类似于期望语言模型对不明确的提示输出一致的响应。我们选择此任务是因为它会产生一种自然的预训练算法和一种通过提示将零样本转移到下游分割任务的通用方法。</p> 
<p><img src="https://images2.imgbox.com/e4/6b/wEp9OdKC_o.png" alt=" "></p> 
<p><strong>预训练</strong>。可提示分割任务提出了一种自然的预训练算法，该算法模拟每个训练样本的提示序列（例如，点、框、掩码），并将模型的掩码预测与ground-truth进行比较。我们将这种方法从交互式分割中进行了调整[109,70]，尽管与交互式分割不同，<strong>交互式分割的目的是在足够的用户输入后最终预测有效的掩码，但我们的目的是在任何提示的情况下始终预测出有效掩码，即使提示不明确</strong>。这确保了预先训练的模型在涉及歧义的用例中是有效的，包括我们的数据引擎所要求的自动标注。我们注意到，在这项任务中表现出色是具有挑战性的，需要专门的建模和训练损失选择，我们在第三部分中对此进行了讨论。</p> 
<p><strong>零样本迁移</strong>。直观地说，我们的<strong>预训练任务赋予了模型在推理时对任何提示做出适当响应的能力，因此下游任务可以通过设计适当的提示来解决</strong>。例如，如果有一个猫的边界框检测器，则可以通过向我们的模型提供检测器的框输出作为提示来解决猫实例分割。一般来说，一系列实用的分割任务可以作为提示。除了自动数据集标记外，我们还在第七部分中的实验中探索了五个不同的示例任务。</p> 
<p><strong>相关任务</strong>。分割是一个广泛的领域：有交互式分割[57109]、边缘检测[3]、超级像素化[85]、对象建议生成[2]、前景分割[94]、语义分割[90]、实例分割[66]、全景分割[59]等。</p> 
<p>我们的可提示分割任务的目标是通过即时工程生成一个功能广泛的模型，该模型可以适应许多（尽管不是全部）现有和新的分割任务。这种能力是任务泛化的一种形式[26]。请注意，这与之前关于多任务分割系统的工作不同。在多任务系统中，单个模型执行一组固定的任务，例如联合语义、实例和全景分割[114,19,54]，但训练和测试任务是相同的。我们工作中的一个重要区别是，为<strong>可提示分割训练的模型可以在推理时通过充当更大系统中的组件来执行新的不同任务</strong>，例如，为了执行实例分割，将可提示分割模型与现有的目标检测器相结合。</p> 
<p><strong>讨论</strong>。提示和组合是功能强大的工具，使单个模型能够以可扩展的方式使用，有可能完成模型设计时未知的任务。这种方法类似于其他基础模型的使用方式，例如CLIP[82]是DALL·e[83]图像生成系统的文本图像对齐组件。我们预计，与专门为固定任务集训练的系统相比，以快速工程等技术为动力的可组合系统设计将实现更广泛的应用程序。从合成的角度比较可提示分割和交互式分割也很有趣：虽然交互式分割模型是在考虑人类用户的情况下设计的，但为可提示分割训练的模型也可以组成一个更大的算法系统，正如我们将要演示的那样。</p> 
<h3><a id="3_72"></a>3、分割一切模型</h3> 
<p>接下来我们将描述用于可提示分割的分段任意模型（SAM）。SAM有三个组件，如图4所示：图像编码器、灵活提示编码器和快速掩码解码器。我们建立在Transformer视觉模型[14,33,20,62]的基础上，对实时性能进行了特定的权衡。我们在这里对这些组件进行了高层描述，详细信息见附录A。</p> 
<p><img src="https://images2.imgbox.com/fc/cc/SXy5l9Dl_o.png" alt=" "></p> 
<p><strong>图像编码器</strong>。受可扩展性和强大的预训练方法的启发，我们使用了MAE[47]预训练Vision Transformer（ViT）[33]，ViT至少适用于处理高分辨率输入[62]。图像编码器每个图像运行一次，并且可以在提示模型之前应用。</p> 
<p><strong>提示编码器</strong>。我们考虑两组提示：稀疏（点、框、文本）和密集（掩码）。我们通过位置编码[95]来表示点和框，这些位置编码与每个提示类型的学习嵌入相加，并使用CLIP[82]的现成文本编码器来表示自由格式文本。密集提示（即掩码）使用卷积嵌入，并与图像嵌入逐元素求和。</p> 
<p><strong>掩码解码器</strong>。掩码解码器有效地将图像嵌入、提示嵌入和输出token映射到掩码。该设计受到[14，20]的启发，采用了完善的Transformer解码器块[103]，然后是动态掩码预测头。我们修改的解码器块在两个方向上使用提示自注意力和交叉注意力（提示到图像嵌入，反之亦然）来更新所有嵌入。在运行两个块之后，我们对图像嵌入进行上采样，MLP将输出token映射到动态线性分类器，然后动态线性分类器计算每个图像位置的掩码前景概率。</p> 
<p><strong>解决歧义</strong>。对于一个输出，如果给出不明确的提示，模型将平均多个有效掩码。为了解决这个问题，我们修改了模型，以预测单个提示的多个输出掩码（见图3）。我们发现，3个掩码输出足以解决大多数常见情况（嵌套掩码通常最多有三个深度：整体、部分和子部分）。在训练过程中，我们只在掩码上反向探测最小的损失[15，45，64]。为了对掩码进行排序，该模型预测每个掩码的置信度得分（即估计的IoU）。</p> 
<p><strong>效率</strong>。整体模型设计在很大程度上是出于效率的考虑。给定预先计算的图像嵌入，提示编码器和掩码解码器在网络浏览器中运行，在CPU上运行，时间约为50ms。这种运行时性能使我们的模型能够无缝、实时地进行交互式提示。</p> 
<p><strong>损失和训练</strong>。我们使用[14]中使用的focal loss[65]和dice loss[73]的线性组合来监督掩模预测。我们使用几何提示的混合来训练可提示的分割任务（文本提示见章节7.5）。在[92，37]之后，我们通过在每个掩码的11轮中随机采样提示来模拟交互式设置，使SAM能够无缝集成到我们的数据引擎中。</p> 
<h3><a id="4_91"></a>4、分割一切数据引擎</h3> 
<p>由于互联网上的分割掩码并不丰富，我们建立了一个数据引擎来收集我们的1.1亿掩码数据集SA-1B。数据引擎有三个阶段：（1）<strong>模型辅助的手动标注阶段</strong>，（2）混合了自动预测掩码和模型辅助标注的<strong>半自动阶段</strong>，以及（3）<strong>全自动阶段</strong>，在该阶段中，我们的模型在没有标注者输入的情况下生成掩码。我们将详细介绍下一步。</p> 
<p><strong>辅助手动阶段</strong>。在第一阶段，类似于经典的交互式分割，一组专业标注人员通过使用SAM提供的基于浏览器的交互式分割工具点击前景/背景对象点来标记掩码。遮罩可以使用像素精度的“画笔”和“橡皮擦”工具进行细化。我们的模型辅助标注直接在浏览器内实时运行（使用预先计算的图像嵌入），从而实现真正的交互式体验。我们没有对标记对象施加语义约束，标注者可以自由地标记“东西”和“事物”[1]。我们建议标注者标记他们可以命名或描述的对象，但没有收集这些名称或描述。标注者被要求按照显著的顺序标记对象，一旦遇到某些图片需要标注掩码超过30秒，标注者将被鼓励去标注下一张图片。</p> 
<p>在这个阶段开始时，SAM是使用公共分割数据集进行训练的。在充分的数据标注之后，仅使用新标注的掩码对SAM进行再训练。随着更多掩码的收集，图像编码器从ViT-B扩展到ViT-H，其他架构细节也在发展；我们总共对模型进行了6次训练。随着模型的改进，每个掩码的平均标注时间从34秒减少到14秒。我们注意到，14秒比COCO[66]的掩码标注快6.5倍，仅比使用极值点的边界框标记慢2倍[76，71]。随着SAM的改进，每张图像的平均掩码数量从20个增加到44个。总的来说，我们在这个阶段从120k张图像中收集了430万个掩码。</p> 
<p><strong>半自动阶段</strong>。在这个阶段，我们的目标是增加掩码的多样性，以提高我们的模型分割任何一切的能力。为了将标注者集中在不太突出的对象上，我们首先自动检测到置信度高的掩码。然后，我们向标注者展示了预先填充了这些掩码的图像，并要求他们标注任何其他未标注的对象。为了检测置信度高的掩码，我们使用通用的“对象”类别在所有第一阶段掩码上训练了一个<strong>边界框检测器</strong>[84]。在此阶段，我们在180k张图像中额外收集了590万个掩码（总共1020万个掩码）。与第一阶段一样，我们定期根据新收集的数据对模型进行再训练（5次）。每个掩码的平均标注时间回到了34秒（不包括自动掩码），因为这些对象更难标记。每张图像的平均掩码数量从44个增加到72个（包括自动掩码）。</p> 
<p><strong>全自动阶段</strong>。在最后阶段，标注是完全自动的。这是可行的，因为我们的模型有两个主要的增强。首先，在这个阶段开始时，我们<strong>收集了足够的掩码</strong>，以大大改进模型，包括前一阶段的各种掩码。其次，到了这个阶段，我们已经开发了<strong>模糊感知模型</strong>，它使我们能够预测有效的掩码，即使在模糊的情况下也是如此。具体来说，我们用32×32的规则网格提示模型，并为每个点预测一组可能对应于有效对象的掩码。对于模糊感知模型，如果一个点位于部分或子部分上，我们的模型将返回子部分、部分和整个对象。我们模型的IoU预测模块用于选择置信度高的掩码；此外，我们只识别和选择了稳定的掩码（如果在<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         0.5 
        
       
         − 
        
       
         δ 
        
       
      
        0.5−δ 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;"></span><span class="mord">0.5</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal" style="margin-right: 0.0379em;">δ</span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         0.5 
        
       
         + 
        
       
         δ 
        
       
      
        0.5+δ 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;"></span><span class="mord">0.5</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal" style="margin-right: 0.0379em;">δ</span></span></span></span></span>处对概率图进行阈值处理会导致类似的掩码，则我们认为掩码是稳定的）。最后，在选择了置信度高的和稳定的掩码后，我们应用非最大值抑制（NMS）来过滤重复。为了进一步提高较小掩码的质量，我们还处理了多个重叠的放大图像裁剪。有关此阶段的更多详细信息，请参见附录B。我们将全自动掩模生成应用于数据集中的所有1100万张图像，总共生成了11亿个高质量掩码。接下来，我们将描述并分析生成的数据集SA-1B。</p> 
<h3><a id="5_103"></a>5、分割一切数据集</h3> 
<p>我们的数据集SA-1B由1100万多样、高分辨率、许可和隐私保护的图像和使用我们的数据引擎收集的11亿高质量分割掩码组成。我们将SA-1B与现有数据集进行比较，并分析掩模质量和属性。我们正在发布SA-1B，以帮助未来计算机视觉基础模型的开发。我们注意到，SA-1B将在某些研究用途的有利许可协议下发布，并为研究人员提供保护。</p> 
<p><strong>图像</strong>。我们从一家直接与摄影师合作的供应商那里获得了一组1100万张新图像的许可。这些图像具有高分辨率（平均3300×4950像素），由此产生的数据大小可能会带来可访问性和存储方面的挑战。因此，我们正在发布最短边设置为1500像素的下采样图像。即使在下采样之后，我们的图像的分辨率也明显高于许多现有的视觉数据集（例如，COCO[66]图像的分辨率约为480×640像素）。请注意，目前大多数模型的输入分辨率要低得多。在公布的图像中，人脸和车牌被模糊了。</p> 
<p><strong>掩码</strong>。我们的数据引擎产生了11亿个掩码，其中99.1%是完全自动生成的。因此，自动掩码的质量至关重要。我们将其直接与专业标注进行比较，并查看各种掩码属性与显著分割数据集进行比较。正如下面的分析和第七部分中的实验所证实的那样，我们的主要结论是，我们的自动掩码质量高，对训练模型有效。受这些发现的启发，SA-1B仅包括自动生成的掩码。</p> 
<p><strong>掩码质量</strong>。为了估计掩模质量，我们随机采样了500张图像（约5万个掩码），并要求我们的专业标注人员提高这些图像中所有掩码的质量。标注人员使用我们的模型和像素精确的“画笔”和“橡皮擦”编辑工具来完成这项工作。这一过程产生了一对自动预测和专业校正的掩码。我们计算了每对之间的IoU，发现94%目标对的IoU大于90%（97%的对的IoU大于75%）。为了进行比较，先前的工作估计标注者之间的一致性为85-91%IoU[44，60]。我们在第七部分中的实验通过人类评级证实，相对于各种数据集，掩码质量很高，并且在自动掩码上训练我们的模型几乎与使用数据引擎产生的所有掩码一样好。</p> 
<p><img src="https://images2.imgbox.com/5a/10/RIVlAdKR_o.png" alt=" "></p> 
<p><strong>掩码属性</strong>。在图5中，与现有最大的分割数据集相比，我们绘制了SA-1B中<strong>对象中心的空间分布</strong>。所有数据集中都存在常见的摄影师偏见。我们观察到，与分布最相似的两个数据集LVIS v1[44]和ADE20K[117]相比，SA-1B具有更大的图像角覆盖范围，而COCO[66]和Open Images V5[60]具有更显著的中心偏差。在图6（图例）中，我们按大小比较了这些数据集。SA-1B比第二大的Open images多了11倍的图像和400倍的掩码。平均而言，它每张图像的掩码比Open Images多36倍。在这方面最接近的数据集ADE20K，每张图像的掩码仍然减少了3.5倍。图6（左）绘制了掩码的周边图像分布。接下来，我们看看图6（中间）中的图像相对掩码大小（掩码面积除以图像面积的平方根）。正如预期的那样，由于我们的数据集每个图像有更多的掩码，因此它也倾向于包括更大比例的中小型相对大小掩码。最后，为了分析形状复杂性，我们观察图中的掩码凹度（1减去掩码面积除以掩码凸包的面积）。第6（右）段。由于形状复杂度与掩码大小相关，我们通过首先从掩码大小执行分层采样来控制数据集的掩码大小分布。我们观察到，我们的掩码的凹陷分布与其他数据集的凹陷分布大致相似。</p> 
<p><img src="https://images2.imgbox.com/25/76/SbUpLWWe_o.png" alt=" "></p> 
<h3><a id="6Responsible_AI_121"></a>6、分割一切的Responsible AI分析</h3> 
<p>接下来，我们通过调查使用SA-1B和SAM时潜在的公平问题和偏见，对我们的工作进行负责任的人工智能（RAI）分析。我们重点关注SA-1B的地理和收入分配，以及SAM在受保护的人的属性中的公平性。我们还在附录F中提供了数据集、数据标注和模型卡。</p> 
<p><strong>地域和收入代表性</strong>。我们推断国家图像是使用标准方法拍摄的（见附录C）。在图7中，我们可视化了SA-1B（左）和图像最多的50个国家（右）中的每个国家的图像计数。我们注意到，排名前三的国家来自世界不同地区。接下来，在表1中，我们比较了SA-1B、COCO[66]和Open Images[60]的地理和收入表示。SA-1B在欧洲、亚洲和大洋洲以及中等收入国家的图像比例要高得多。所有数据集都低估了非洲和低收入国家的代表性。我们注意到，在SA-1B中，包括非洲在内的所有地区都至少有2800万个掩码，比以前任何数据集的掩码总数都多10倍。最后，我们观察到，每张图像（未显示）的平均掩码数量在区域和收入之间相当一致（每张图像94-108个）。<br> <img src="https://images2.imgbox.com/73/96/B8tVEfYr_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/cf/ea/YqspinDx_o.png" alt=" "></p> 
<p><strong>细分人群的公平性</strong>。我们通过测量各组之间SAM的表现差异，调查了感知性别表现、感知年龄组和感知肤色的潜在公平问题。我们使用更具包容性的人群标注（MIAP）[87]数据集来进行性别表示和年龄，并使用专有的肤色数据集（见附录C）。我们的评估使用模拟交互式分割，随机采样1点到3点（见附录D）。表2（左上角）显示了感知性别表现的结果。我们注意到，女性在检测和分割数据集中的代表性不足[115]，但观察到SAM在各组中的表现相似。我们重复表2（左下）中对感知年龄的分析，注意到那些被感知为越来越年轻的人在大规模数据集中的代表性不足[110]。SAM在那些被认为年龄较大的人身上表现最好（尽管置信区间很大）。最后，我们重复表2（右）中对感知肤色的分析，注意到在大规模数据集中，表观肤色较浅的人被证明代表性过高，而肤色较深的人代表性不足[110]。由于MIAP不包含感知肤色标注，我们使用了一个专有数据集，该数据集包含感知Fitzpatrick皮肤类型的标注[36]，其范围从1（最浅肤色）到6（最深肤色）。虽然平均数有所不同，但我们没有发现各组之间的显著差异。我们相信我们的发现源于任务的性质，并承认当SAM被用作更大系统的组件时可能会出现偏差。最后，在附录C中，我们将分析扩展到服装分割，在那里我们发现了感知性别表现的偏见。</p> 
<p><img src="https://images2.imgbox.com/56/e4/TBYhUGcR_o.png" alt=" "></p> 
<h3><a id="7_138"></a>7、实验</h3> 
<p>在本节中，我们将介绍SAM的零样本迁移实验，即分割一切模型。我们考虑了五个任务，其中四个任务与用于训练SAM的提示分割任务显著不同。这些实验在数据集和训练过程中没有看到的任务上评估SAM（我们使用的“零样本迁移”遵循CLIP中的用法[82]）。数据集可以包括新颖的图像分布，例如水下或以自我为中心的图像（例如。图8），据我们所知，并未出现在SA-1B中。</p> 
<p><img src="https://images2.imgbox.com/62/b7/ENyiZa2A_o.png" alt=" "></p> 
<p>我们的实验从测试可提示分割的核心目标开始：从任何提示生成有效的掩码。我们强调单一前景点提示的挑战性场景，因为它比其他更具体的提示更有可能是模糊的。接下来，我们介绍了一系列实验，这些实验横跨低、中、高级别的图像理解，并大致平行于该领域的历史发展。具体而言，我们提示SAM（1）执行边缘检测，（2）分割一切，即对象建议生成，（3）分割检测到的对象，即实例分割，以及（4）作为概念证明，从自由格式文本中分割对象。这四项任务与SAM接受训练并通过即时工程实施的可提示分段任务有很大不同。我们的实验以消融研究结束。</p> 
<p><strong>实施细节</strong>。除非另有规定：（1）SAM使用MAE[47]预训练的ViT-H[33]图像编码器，（2）SAM在SA-1B上训练，注意该数据集仅包括数据引擎最后阶段自动生成的掩码。有关所有其他模型和训练细节，如超参数，请参阅附录§A。</p> 
<h4><a id="71__149"></a>7.1 零样本单点的有效掩码评估</h4> 
<p><strong>任务</strong>。我们评估从单个前景点分割对象。由于一个点可以引用多个对象，因此此任务不适定。大多数数据集中的ground-truth掩码并没有枚举所有可能的掩码，这可能会使自动度量变得不可靠。因此，我们用一项人类研究来补充标准的mIoU度量（即预测掩码和ground-truth掩码之间的所有IoU的平均值），在该研究中，标注者对掩码质量的评分从1（无意义）到10（像素完美）。有关更多详细信息，请参见附录D.1、附录E和附录G。</p> 
<p>默认情况下，我们根据交互式分割中的标准评估协议[92]，从ground-truth掩码的“中心”（掩码内部距离变换的最大值）采样点。由于SAM能够预测多个掩码，因此默认情况下，我们只评估模型中置信度最高的掩码。基线都是单掩码方法。我们主要与RITM[92]进行比较，这是一种强大的交互式分割器，与其他强大的基线相比，它在我们的基准上表现最好[67，18]。</p> 
<p><strong>数据集</strong>。我们使用了一套新编译的23个数据集，这些数据集具有不同的图像分布。图8列出了数据集，并显示了每个数据集的样本（更多细节请参见附录表7）。我们使用所有23个数据集进行mIoU评估。对于人类研究，我们使用图9b中列出的子集（由于此类研究的资源需求）。该子集包括SAM根据自动度量优于和低于RITM的两个数据集。</p> 
<p><img src="https://images2.imgbox.com/ff/11/EzXE9sB6_o.png" alt=" "></p> 
<p><strong>结果</strong>。首先，我们使用mIoU对23个数据集的全套数据集进行自动评估。我们比较了图9a中每个数据集的结果与RITM。SAM在23个数据集中的16个数据集上产生了更高的结果，高达约47 IoU。我们还提出了一个“oracle”结果，<strong>其中通过将SAM的3个掩码与ground-truth进行比较来选择最相关的掩码，而不是选择置信度最高的掩码</strong>。这揭示了歧义对自动评估的影响。特别是，使用oracle来执行模糊性解决，SAM在所有数据集上都优于RITM。</p> 
<p>图9c显示了额外的基线，SimpleClick[67]和FocalClick[18]，它们获得的单点性能低于RITM和SAM。随着点数从1增加到9，我们观察到方法之间的差距减小。随着任务变得更容易，这是意料之中的事；此外，SAM并没有针对非常高的IoU状态进行优化。最后，在图9d中，我们将默认的中心点采样替换为随机点采样。我们观察到SAM和基线之间的差距越来越大，并且SAM能够在任何一种采样方法下获得可比较的结果。</p> 
<h4><a id="72__164"></a>7.2 零样本边缘检测</h4> 
<p><strong>方法</strong>。我们使用BSDS500[72，3]在边缘检测的经典低级别任务上评估SAM。我们使用了一个简化版本的自动掩码生成管道。具体来说，我们用前景点的16×16规则网格提示SAM，得到768个预测掩码（每个点3个）。NMS移除冗余掩码。然后，使用无阈值掩码概率图的Sobel滤波和标准轻量级后处理（包括边缘NMS）来计算边缘图（详见附录D.2）。</p> 
<p><strong>结果</strong>。我们在图10中可视化了具有代表性的边缘图（更多信息请参见图15）。定性地说，我们观察到，即使SAM没有经过边缘检测训练，它也能产生合理的边缘图。与ground-truth相比，SAM预测了更多的边缘，包括BSDS500中未标注的合理边缘。这种偏差在表3中得到了定量反映：50%精度（R50）的召回率很高，但以精度为代价。SAM自然落后于学习BSDS500偏差的最先进方法，即要抑制哪些边缘。然而，与HED[108]（也接受过BSDS500训练）等开创性深度学习方法相比，SAM表现良好，并且明显优于以前的零样本迁移方法，尽管该方法已经过时。</p> 
<p><img src="https://images2.imgbox.com/fa/a6/jCorZldz_o.png" alt=" "></p> 
<p><img src="https://images2.imgbox.com/df/d0/Nc9RcnwW_o.png" alt=" "></p> 
<h4><a id="73__176"></a>7.3 零样本对象建议</h4> 
<p><strong>方法</strong>。接下来，我们在对象建议生成的中级任务[2, 102]上评估SAM。这项任务在目标检测研究中发挥了重要作用，是开创性系统中的中间步骤（例如，[102，41，84]）。为了生成对象建议，我们运行了一个稍微修改过的自动掩码生成管道版本，并将掩码作为建议输出（有关详细信息，请参见附录D.3）。</p> 
<p>我们在LVIS v1上计算标准平均召回率（AR）度量[44]。我们专注于LVIS，因为它的大量类别是一个具有挑战性的测试。我们将其与作为ViTDet[62]检测器（具有级联掩码R-CNN[48，11]的ViT-H）实现的强基线进行比较。我们注意到，这个“基线”对应于game AR中显示的“Detector Masquerading as Proposal generator”（DMP）方法[16]，这使其成为一个真正要求苛刻的比较。</p> 
<p><strong>结果</strong>。在表4中，我们毫不奇怪地看到，使用来自ViTDet-H的检测作为对象建议（即game AR的DMP方法[16]）总体上表现最好。然而，SAM在几个指标上做得非常好。值得注意的是，它在中型和大型对象以及稀有和常见对象上的性能优于ViTDet-H。事实上，SAM只在小对象和频繁对象上表现不佳，其中ViTDet-H可以很容易地学习LVIS特定的标注偏差，因为它是在LVIS上训练的，而不是SAM。我们还将其与消除歧义的SAM版本（“single-out.”）进行了比较，后者在所有AR指标上的表现都比SAM差得多。</p> 
<p><img src="https://images2.imgbox.com/88/2f/E3PcK0BF_o.png" alt=" "></p> 
<h4><a id="74__187"></a>7.4 零样本实例分割</h4> 
<p><strong>方法</strong>。转到更高层次的视觉任务，我们使用SAM作为实例分割器的分割模块。实现很简单：我们运行一个目标检测器（之前使用的ViTDet），并用它的输出框提示SAM。这说明了在更大的系统中组成SAM。</p> 
<p><strong>结果</strong>。我们比较了表5中SAM和ViTDet对COCO和LVIS预测的掩码。从掩码AP度量来看，我们在两个数据集上都观察到了差距，其中SAM相当接近，尽管肯定落后于ViTDet。通过可视化输出，我们观察到SAM掩模通常在质量上优于ViTDet的掩模，具有更清晰的边界（见附录D.4和图16）。为了调查这一观察结果，我们进行了一项额外的人工评估，要求标注者在之前使用的1到10质量等级上对ViTDet掩码和SAM掩码进行评分。在图11中，我们观察到SAM在人工评估中始终优于ViTDet。</p> 
<p><img src="https://images2.imgbox.com/ff/70/IBxwj31Y_o.png" alt=" "></p> 
<p><img src="https://images2.imgbox.com/ba/55/BpmX8txB_o.png" alt=" "></p> 
<p>我们假设，在COCO上，掩码AP间隙较大，ground-truth质量相对较低（正如人类研究所证实的那样），ViTDet了解COCO掩码的具体偏差。SAM是一种零样本方法，无法利用这些（通常不需要的）偏差。LVIS数据集具有更高质量的ground-truth，但仍然存在特定的特性（例如，掩码不包含空洞，它们是构造的简单多边形）和modal与amodal掩码的偏差。同样，SAM没有接受过学习这些偏置的训练，而ViTDet可以利用这些偏见。</p> 
<h4><a id="75__201"></a>7.5 零样本文本到掩码</h4> 
<p><strong>方法</strong>。最后，我们考虑一个更高层次的任务：从自由形式的文本中分割目标。这个实验证明了SAM处理文本提示的能力。虽然我们在之前的所有实验中都使用了完全相同的SAM，但对于这一次，SAM的训练过程被修改为具有文本意识，但不需要新的文本标注。具体地，对于每个面积大于10000的手动收集的掩模，我们提取CLIP图像嵌入。然后，在训练过程中，我们用提取的CLIP图像嵌入作为SAM的第一次交互来提示SAM。这里的关键观察是，因为CLIP的图像嵌入被训练为与文本嵌入对齐，所以我们可以使用图像嵌入进行训练，但使用文本嵌入进行推理。也就是说，在推理时，我们通过CLIP的文本编码器运行文本，然后将生成的文本嵌入作为SAM的提示（详见§D.5）。</p> 
<p><strong>结果</strong>。我们在图12中显示了定性结果。AM可以根据简单的文本提示（如“a wheel”）以及短语（如“beaver tooth grille”）对目标进行分割。当SAM无法仅从文本提示中选择正确的对象时，额外的一点通常会修复预测，类似于[31]。</p> 
<h4><a id="76__207"></a>7.6 消融实验</h4> 
<p>我们使用单中心点提示协议对23个数据集套件进行了多次消融。回想一下，单个点可能是模糊的，而这种模糊性可能不会在ground-truth中表示，因为每个点只包含一个掩码。由于SAM在零样本迁移设置中操作，SAM的top-ranked掩码与数据标注指南产生的掩码之间可能存在系统偏差。因此，我们还报告了关于ground-truth的最佳掩码（“oracle”）。</p> 
<p>图13（左）绘制了根据数据引擎阶段的累积数据进行训练时SAM的性能。我们观察到，每个阶段都会增加mIoU。在所有三个阶段的训练中，自动掩码的数量远远超过手动和半自动掩码。为了解决这个问题，我们发现在训练过程中对手动和半自动掩码进行10倍的过采样可以获得最佳效果。这种设置使训练变得复杂。因此，我们测试了第四种设置，它只使用自动生成的掩码。有了这些数据，SAM的性能仅略低于使用所有数据（约0.5 mIoU）。因此，默认情况下，我们只使用自动生成的掩码来简化训练设置。</p> 
<p>在图13（中间）中，我们观察了数据量的影响。完整的SA-1B包含1100万图像，我们将其均匀地分为100万和10万进行消融。在10万张图像中，我们观察到在所有设置下mIoU都有很大的下降。然而，对于100万张图像，约占完整数据集的10%，我们观察到的结果与使用完整数据集相当。这个数据体系仍然包括大约1亿个掩码，对于许多用例来说可能是一个实用的设置。</p> 
<p>最后，图13（右）显示了ViT-B、ViT-L和ViT-H图像编码器的结果。与ViT-B相比，ViT-H显著提高，但与ViT-L相比仅略有提高。此时，进一步的图像编码器缩放似乎没有取得成效。</p> 
<p><img src="https://images2.imgbox.com/d0/4f/IYyzq4iW_o.png" alt=" "></p> 
<h3><a id="8_220"></a>8、讨论</h3> 
<p><strong>基础模型</strong>。自机器学习的早期以来，预训练的模型已经适应了下游任务[99]。近年来，随着对规模的日益重视，这种范式变得越来越重要，这类模型最近被（重新）称为“基础模型”：即“<strong>在大规模的广泛数据上训练并适应广泛下游任务的模型</strong>”[8]。我们的工作与这一定义有很好的相关性，尽管我们注意到图像分割的基础模型本质上是有限的，因为它代表了计算机视觉的一个重要部分的子集。我们还将我们的方法的一个方面与[8]进行了对比，后者强调了自我监督学习在基础模型中的作用。<strong>虽然我们的模型是用自监督技术（MAE[47]）初始化的，但其绝大多数能力来自大规模的监督训练</strong>。在数据引擎可以扩展可用标注的情况下，如我们的情况，监督训练提供了一个有效的解决方案。</p> 
<p><strong>语义合成性</strong>。经过预训练的模型可以提供新的能力，甚至超出训练时的想象。一个突出的例子是CLIP[82]如何在更大的系统中用作组件，如DALL·E[83]。我们的目标是通过SAM使这种合成变得简单。我们的目标通过要求SAM预测各种分割提示的有效掩码来实现这一点。其效果是在SAM和其他组件之间创建一个可靠的接口。例如，MCC[106]可以很容易地使用SAM来分割感兴趣的目标，并实现对看不见的目标的强泛化，以便从单个RGB-D图像进行3D重建。在另一个例子中，SAM可以通过可穿戴设备检测到的注视点来提示，从而启用新的应用程序。由于SAM能够推广到以自我为中心的图像等新领域，因此此类系统无需额外训练即可工作。</p> 
<p><strong>限制</strong>。虽然SAM总体表现良好，但并不完美。它<strong>可能会错过精细的结构，有时会产生小的断开的组件的幻觉，并且不会像“放大”的计算密集型方法那样清晰地产生边界</strong>，例如[18]。通常，当提供许多点时，我们预计专门设计的交互式分割方法会优于SAM，例如[67]。与这些方法不同，<strong>SAM是为通用性和使用广度而设计的，而不是高IoU交互式分割</strong>。此外，<strong>SAM可以实时处理提示，但当使用重型图像编码器时，SAM的总体性能不是实时的</strong>。我们<strong>对文本到掩码(text-to-mask)任务的尝试是探索性的，并不完全稳健，尽管我们相信可以通过更多的努力来改进</strong>。虽然SAM可以执行许多任务，但<strong>尚不清楚如何设计实现语义和全景分割的简单提示</strong>。最后，还有一些<strong>特定领域</strong>的工具，如[7]，我们预计它们在各自的领域中会优于SAM。</p> 
<p><strong>总结</strong>。Segment Anything项目试图将图像分割提升到基础模型区域。我们的主要贡献是一项新任务（可提示分割）、模型（SAM）和数据集（SA-1B），使这一飞跃成为可能。SAM是否达到了基础模型的地位，还有待于它在社区中的使用方式，但无论我们对这项工作的前景如何，超过10亿个掩码的发布和我们可推广的细分模型都将有助于为未来铺平道路。</p> 
<h3><a id="___230"></a>【论文速递 | 精选】</h3> 
<div align="center"> 
 <img src="https://images2.imgbox.com/d9/15/NUY6tkKE_o.png"> 
</div> 
<center>
  论坛地址：https://bbs.csdn.net/forums/paper 
</center> 
<p><a href="https://blog.csdn.net/qq_45041871/article/details/129295622?spm=1001.2014.3001.5502">最近工作</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/7bfc0a5ca1d4558181b7d533f9a1b2b4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【遗传算法整数交叉】</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d665753ad113387f4f052728a1033bfa/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">X-former系列（Transformer大家族）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>