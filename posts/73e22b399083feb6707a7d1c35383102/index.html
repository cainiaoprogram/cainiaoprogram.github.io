<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>蒸馏_2022 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="蒸馏_2022" />
<meta property="og:description" content=" Focal and Global Knowledge Distillation for Detectors Abstract 在目标检测当中，老师的特征和学生的特征在不同的区域有很大的变化，尤其是在前景和背景中。因此，如果我们平等地蒸馏，特征图之间的差异会恶化结果。（PS：目标检测中的蒸馏会比对教师模型和原模型的特征图）
翻译：前景不好学，背景很好学，如果这两部分内容进行一样地教授，效果不大好。
因此，我们提出了聚焦且全局蒸馏。聚焦蒸馏分割前景和背景，强迫学生关注教师的关键像素及通道；全局蒸馏则重建不同像素之间的关系，并将这一知识传递给学生。
微信链接
Method 通常而言，教师和学生之间的蒸馏过程如下所示：
f是调整通道，用于让教师特征和学生特征的通道保持一致。
这样一种方法平等地对待所有部分并且缺乏不同像素之间全局关系的蒸馏。
Focal Distillation 首先设置一个区分前景和背景的mask：
其中r表示GT boxes。
不同大小的目标会有不一样的损失，为了平衡这一问题，本文采用了一个规模化的mask：
Hr、Wr表示GT框的大小，
SENet和CBAM给出结论：关注重要的像素和通道有助于基于CNN的模型得到更好的结果。这里计算了像素或者通道层面的绝对平均值：
之后得到注意力图：
最终，我们的特征图损失如下：
其中的注意力mask采用的是教师模型的。
除此之外，还对学生得到的注意力图和教师的注意力图进行损失计算，其中l是L1loss：
Global Distillation 采用GcBlock来捕捉全局关系信息，并以此进行蒸馏：
对每一像素的一种重新赋值。
Decoupled Knowledge Distillation 知乎链接
logit蒸馏 和 特征蒸馏
前者计算量小且语义程度高，后者反之。理论来说高维语义特征的效果应该更加好，但事实却不是如此。
实验表明：只有应用NCKD才能获得与经典KD相当甚至更好的结果。
上述实验揭示了logit蒸馏效果不好的原因：经典的KD损失是高度耦合的形式。NCKD损失带有的权重与teacher在目标预测上的置信度是负相关的，即大的预测分数会带来小的权重。值得一提的是，训练样本的置信度普遍较高。
另外，如果训练数据越难，TCKD带给学生的提升就越大。
TCKD适合教难的，NCKD适合教简单的。但是一遇到简单的样本，在经典的KD损失中，NCKD的能力会被抑制。
Co-advise: Cross Inductive Bias Distillation 决定学生能力提升的不是教师的能力而是教师给予的归纳偏置；不同归纳偏置的给予可以带来更好的能力提升；token与教师的对齐有助于学习。 " />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/73e22b399083feb6707a7d1c35383102/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-05T21:58:42+08:00" />
<meta property="article:modified_time" content="2023-04-05T21:58:42+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">蒸馏_2022</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="Focal_and_Global_Knowledge_Distillation_for_Detectors_0"></a>Focal and Global Knowledge Distillation for Detectors</h2> 
<h3><a id="Abstract_1"></a>Abstract</h3> 
<p>在目标检测当中，老师的特征和学生的特征在不同的区域有很大的变化，尤其是在前景和背景中。因此，如果我们平等地蒸馏，特征图之间的差异会恶化结果。（PS：目标检测中的蒸馏会比对教师模型和原模型的特征图）</p> 
<p>翻译：前景不好学，背景很好学，如果这两部分内容进行一样地教授，效果不大好。</p> 
<p>因此，我们提出了聚焦且全局蒸馏。聚焦蒸馏分割前景和背景，强迫学生关注教师的关键像素及通道；全局蒸馏则重建不同像素之间的关系，并将这一知识传递给学生。</p> 
<p><a href="https://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247592616&amp;idx=2&amp;sn=45ad9478adba3938224a1dda7ebaedbd&amp;chksm=96f118fca18691ea7a87aee3794bae3d6f5e39674860f414327856d5247f2a28ac1a2cb03785&amp;scene=27" rel="nofollow">微信链接</a></p> 
<h3><a id="Method_10"></a>Method</h3> 
<p><img src="https://images2.imgbox.com/30/84/xUthLJcF_o.png" alt="请添加图片描述"></p> 
<p>通常而言，教师和学生之间的蒸馏过程如下所示：<br> <img src="https://images2.imgbox.com/a6/6e/izcPiC3k_o.png" alt="请添加图片描述"></p> 
<p>f是调整通道，用于让教师特征和学生特征的通道保持一致。</p> 
<p>这样一种方法平等地对待所有部分并且缺乏不同像素之间全局关系的蒸馏。</p> 
<h4><a id="Focal_Distillation_20"></a>Focal Distillation</h4> 
<p>首先设置一个区分前景和背景的mask：</p> 
<p><img src="https://images2.imgbox.com/df/b1/RKamFUXK_o.png" alt="请添加图片描述"></p> 
<p>其中r表示GT boxes。</p> 
<p>不同大小的目标会有不一样的损失，为了平衡这一问题，本文采用了一个规模化的mask：</p> 
<p><img src="https://images2.imgbox.com/6c/ce/9WmsC1GA_o.png" alt="在这里插入图片描述"></p> 
<p>Hr、Wr表示GT框的大小，</p> 
<p>SENet和CBAM给出结论：关注重要的像素和通道有助于基于CNN的模型得到更好的结果。这里计算了像素或者通道层面的绝对平均值：</p> 
<p><img src="https://images2.imgbox.com/fb/71/xrgJSTSr_o.png" alt="请添加图片描述"></p> 
<p>之后得到注意力图：</p> 
<p><img src="https://images2.imgbox.com/10/7c/qzi169RE_o.png" alt="请添加图片描述"></p> 
<p>最终，我们的特征图损失如下：</p> 
<p><img src="https://images2.imgbox.com/a2/c5/Nq6iubF7_o.png" alt="请添加图片描述"><br> 其中的注意力mask采用的是教师模型的。</p> 
<p>除此之外，还对学生得到的注意力图和教师的注意力图进行损失计算，其中l是L1loss：</p> 
<p><img src="https://images2.imgbox.com/fa/a4/bbS68cvb_o.png" alt="请添加图片描述"></p> 
<h4><a id="Global_Distillation_50"></a>Global Distillation</h4> 
<p>采用GcBlock来捕捉全局关系信息，并以此进行蒸馏：</p> 
<p><img src="https://images2.imgbox.com/1c/0a/8E79GBcL_o.png" alt="请添加图片描述"><br> 对每一像素的一种重新赋值。</p> 
<h2><a id="Decoupled_Knowledge_Distillation_56"></a>Decoupled Knowledge Distillation</h2> 
<p><a href="https://zhuanlan.zhihu.com/p/533890707" rel="nofollow">知乎链接</a></p> 
<p>logit蒸馏 和 特征蒸馏</p> 
<p>前者计算量小且语义程度高，后者反之。理论来说高维语义特征的效果应该更加好，但事实却不是如此。</p> 
<p>实验表明：只有应用NCKD才能获得与经典KD相当甚至更好的结果。</p> 
<p>上述实验揭示了logit蒸馏效果不好的原因：经典的KD损失是高度耦合的形式。NCKD损失带有的权重与teacher在目标预测上的置信度是负相关的，即大的预测分数会带来小的权重。值得一提的是，训练样本的置信度普遍较高。</p> 
<p>另外，如果训练数据越难，TCKD带给学生的提升就越大。</p> 
<p>TCKD适合教难的，NCKD适合教简单的。但是一遇到简单的样本，在经典的KD损失中，NCKD的能力会被抑制。</p> 
<h2><a id="Coadvise_Cross_Inductive_Bias_Distillation_70"></a>Co-advise: Cross Inductive Bias Distillation</h2> 
<ol><li>决定学生能力提升的不是教师的能力而是教师给予的归纳偏置；</li><li>不同归纳偏置的给予可以带来更好的能力提升；</li><li>token与教师的对齐有助于学习。</li></ol>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/93d08bdaf482f8d630013d34d2dc60ae/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【Redis学习】Redis安装配置</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f0ece2c5727cff52283ae14383ca45a1/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">蓝桥杯python部分题目和答案分享（个人做法）通俗易懂 [十题]</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>