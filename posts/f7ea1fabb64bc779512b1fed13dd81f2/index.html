<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>PyTorch学习笔记(八) ---- torch.nn 到底是什么？ - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="PyTorch学习笔记(八) ---- torch.nn 到底是什么？" />
<meta property="og:description" content="转载请注明作者和出处： http://blog.csdn.net/john_bh/
文章目录 1. MNIST 数据设置2. 从零开始的神经网络(无 torch.nn）3. 重构3.1 使用 torch.nn.functional3.2. 使用 nn.Module 进行重构3.3. 使用 nn.Linear 重构3.4. 使用优化重构3.5. 使用数据集进行重构3.6. 使用 DataLoader 进行重构 4. 添加基本功能4.1. 添加验证4.2. 创建 fit(）和 get_data(） 5. 切换到 CNN6. nn.Sequential7. 包装 DataLoader8. 使用 GPU9. 总结 PyTorch 提供设计优雅的模块和类 torch.nn ， torch.optim ， Dataset 和 DataLoader 来帮助您创建和训练神经网络。 为了充分利用它们的功能并针对您的问题对其进行自定义，需要真正地了解他们的工作。 为了建立这种理解，将首先在 MNIST 数据集上训练基本神经网络，而无需使用这些模型的任何功能； 最初只会使用最基本的 PyTorch 张量功能。 然后，将一次从 torch.nn， torch.optim， Dataset或 DataLoader中逐个添加一个功能，确切地显示每个功能，以及如何使代码更简洁或更灵活。 1. MNIST 数据设置 将使用经典的 MNIST 数据集，该数据集由手绘数字的黑白图像组成(介于 0 到 9 之间）。使用 pathlib 处理路径(Python 3 标准库的一部分），并使用请求下载数据集。 只会在使用模块时才导入它们，因此可以确切地看到正在使用模块的每个细节。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/f7ea1fabb64bc779512b1fed13dd81f2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-08-29T16:15:23+08:00" />
<meta property="article:modified_time" content="2020-08-29T16:15:23+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">PyTorch学习笔记(八) ---- torch.nn 到底是什么？</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><strong>转载请注明作者和出处： <a href="http://blog.csdn.net/john_bh/">http://blog.csdn.net/john_bh/</a></strong></p> 
<p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><ul><li><a href="#1_MNIST__5" rel="nofollow">1. MNIST 数据设置</a></li><li><a href="#2__torchnn_70" rel="nofollow">2. 从零开始的神经网络(无 torch.nn）</a></li><li><a href="#3__191" rel="nofollow">3. 重构</a></li><li><ul><li><a href="#31__torchnnfunctional_192" rel="nofollow">3.1 使用 torch.nn.functional</a></li><li><a href="#32__nnModule__217" rel="nofollow">3.2. 使用 nn.Module 进行重构</a></li><li><a href="#33__nnLinear__293" rel="nofollow">3.3. 使用 nn.Linear 重构</a></li><li><a href="#34__326" rel="nofollow">3.4. 使用优化重构</a></li><li><a href="#35__376" rel="nofollow">3.5. 使用数据集进行重构</a></li><li><a href="#36__DataLoader__421" rel="nofollow">3.6. 使用 DataLoader 进行重构</a></li></ul> 
   </li><li><a href="#4___462" rel="nofollow">4. 添加基本功能</a></li><li><ul><li><a href="#41__464" rel="nofollow">4.1. 添加验证</a></li><li><a href="#42__fit_get_data_506" rel="nofollow">4.2. 创建 fit(）和 get_data(）</a></li></ul> 
   </li><li><a href="#5__CNN_562" rel="nofollow">5. 切换到 CNN</a></li><li><a href="#6_nnSequential_597" rel="nofollow">6. nn.Sequential</a></li><li><a href="#7__DataLoader_637" rel="nofollow">7. 包装 DataLoader</a></li><li><a href="#8__GPU_688" rel="nofollow">8. 使用 GPU</a></li><li><a href="#9__726" rel="nofollow">9. 总结</a></li></ul> 
 </li></ul> 
</div> 
<br> PyTorch 提供设计优雅的模块和类 
<code>torch.nn</code> ， 
<code>torch.optim</code> ， 
<code>Dataset</code> 和 
<code>DataLoader</code> 来帮助您创建和训练神经网络。 为了充分利用它们的功能并针对您的问题对其进行自定义，需要真正地了解他们的工作。 为了建立这种理解，将首先在 MNIST 数据集上训练基本神经网络，而无需使用这些模型的任何功能； 最初只会使用最基本的 PyTorch 张量功能。 然后，将一次从 
<code>torch.nn</code>， 
<code>torch.optim</code>， 
<code>Dataset</code>或 
<code>DataLoader</code>中逐个添加一个功能，确切地显示每个功能，以及如何使代码更简洁或更灵活。 
<p></p> 
<h3><a id="1_MNIST__5"></a>1. MNIST 数据设置</h3> 
<p>将使用经典的 MNIST 数据集，该数据集由手绘数字的黑白图像组成(介于 0 到 9 之间）。使用 pathlib 处理路径(Python 3 标准库的一部分），并使用请求下载数据集。 只会在使用模块时才导入它们，因此可以确切地看到正在使用模块的每个细节。</p> 
<pre><code class="prism language-bash">from pathlib <span class="token function">import</span> Path
<span class="token function">import</span> requests

DATA_PATH <span class="token operator">=</span> Path<span class="token punctuation">(</span><span class="token string">"data"</span><span class="token punctuation">)</span>
PATH <span class="token operator">=</span> DATA_PATH / <span class="token string">"mnist"</span>

PATH.mkdir<span class="token punctuation">(</span>parents<span class="token operator">=</span>True, exist_ok<span class="token operator">=</span>True<span class="token punctuation">)</span>

URL <span class="token operator">=</span> <span class="token string">"http://deeplearning.net/data/mnist/"</span>
FILENAME <span class="token operator">=</span> <span class="token string">"mnist.pkl.gz"</span>

<span class="token keyword">if</span> not <span class="token punctuation">(</span>PATH / FILENAME<span class="token punctuation">)</span>.exists<span class="token punctuation">(</span><span class="token punctuation">)</span>:
        content <span class="token operator">=</span> requests.get<span class="token punctuation">(</span>URL + FILENAME<span class="token punctuation">)</span>.content
        <span class="token punctuation">(</span>PATH / FILENAME<span class="token punctuation">)</span>.open<span class="token punctuation">(</span><span class="token string">"wb"</span><span class="token punctuation">)</span>.write<span class="token punctuation">(</span>content<span class="token punctuation">)</span>
</code></pre> 
<p>该数据集为 numpy 数组格式，并已使用 pickle(一种用于序列化数据的 python 特定格式）存储。</p> 
<pre><code class="prism language-bash"><span class="token function">import</span> pickle
<span class="token function">import</span> <span class="token function">gzip</span>

with gzip.open<span class="token punctuation">((</span>PATH / FILENAME<span class="token punctuation">)</span>.as_posix<span class="token punctuation">(</span><span class="token punctuation">)</span>, <span class="token string">"rb"</span><span class="token punctuation">)</span> as f:
        <span class="token punctuation">((</span>x_train, y_train<span class="token punctuation">)</span>, <span class="token punctuation">(</span>x_valid, y_valid<span class="token punctuation">)</span>, _<span class="token punctuation">)</span> <span class="token operator">=</span> pickle.load<span class="token punctuation">(</span>f, encoding<span class="token operator">=</span><span class="token string">"latin-1"</span><span class="token punctuation">)</span>
</code></pre> 
<p>每个图像为 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         28 
        
       
         × 
        
       
         28 
        
       
      
        28 \times 28 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">2</span><span class="mord">8</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">2</span><span class="mord">8</span></span></span></span></span>，并存储被拍平长度为 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         784 
        
       
         ( 
        
       
         = 
        
       
         28 
        
       
         × 
        
       
         28 
        
       
         ） 
        
       
      
        784(= 28 \times 28） 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord">7</span><span class="mord">8</span><span class="mord">4</span><span class="mopen">(</span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">2</span><span class="mord">8</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">2</span><span class="mord">8</span><span class="mord cjk_fallback">）</span></span></span></span></span>的向量。 来看一个； 需要先将其重塑为 2d。</p> 
<pre><code class="prism language-bash">from matplotlib <span class="token function">import</span> pyplot
<span class="token function">import</span> numpy as np

pyplot.imshow<span class="token punctuation">(</span>x_train<span class="token punctuation">[</span>0<span class="token punctuation">]</span>.reshape<span class="token variable"><span class="token punctuation">((</span><span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">))</span></span>, cmap<span class="token operator">=</span><span class="token string">"gray"</span><span class="token punctuation">)</span>
print<span class="token punctuation">(</span>x_train.shape<span class="token punctuation">)</span>
</code></pre> 
<p>输出：</p> 
<blockquote> 
 <p>torch.Size([50000, 784])<br> <img src="https://images2.imgbox.com/a7/40/6JOZtc3I_o.png" alt="在这里插入图片描述"></p> 
</blockquote> 
<p>PyTorch 使用torch.tensor而不是 numpy 数组，因此需要转换数据。</p> 
<pre><code class="prism language-bash"><span class="token function">import</span> torch

x_train, y_train, x_valid, y_valid <span class="token operator">=</span> map<span class="token punctuation">(</span>
    torch.tensor, <span class="token punctuation">(</span>x_train, y_train, x_valid, y_valid<span class="token punctuation">)</span>
<span class="token punctuation">)</span>
n, c <span class="token operator">=</span> x_train.shape
x_train, x_train.shape, y_train.min<span class="token punctuation">(</span><span class="token punctuation">)</span>, y_train.max<span class="token punctuation">(</span><span class="token punctuation">)</span>
print<span class="token punctuation">(</span>x_train, y_train<span class="token punctuation">)</span>
print<span class="token punctuation">(</span>x_train.shape<span class="token punctuation">)</span>
print<span class="token punctuation">(</span>y_train.min<span class="token punctuation">(</span><span class="token punctuation">)</span>, y_train.max<span class="token punctuation">(</span><span class="token punctuation">))</span>
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-bash">tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span>0., 0., 0.,  <span class="token punctuation">..</span>., 0., 0., 0.<span class="token punctuation">]</span>,
       	<span class="token punctuation">[</span>0., 0., 0.,  <span class="token punctuation">..</span>., 0., 0., 0.<span class="token punctuation">]</span>,
        <span class="token punctuation">[</span>0., 0., 0.,  <span class="token punctuation">..</span>., 0., 0., 0.<span class="token punctuation">]</span>,
        <span class="token punctuation">..</span>.,
        <span class="token punctuation">[</span>0., 0., 0.,  <span class="token punctuation">..</span>., 0., 0., 0.<span class="token punctuation">]</span>,
        <span class="token punctuation">[</span>0., 0., 0.,  <span class="token punctuation">..</span>., 0., 0., 0.<span class="token punctuation">]</span>,
        <span class="token punctuation">[</span>0., 0., 0.,  <span class="token punctuation">..</span>., 0., 0., 0.<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>5, 0, 4,  <span class="token punctuation">..</span>., 8, 4, 8<span class="token punctuation">]</span><span class="token punctuation">)</span>
torch.Size<span class="token punctuation">(</span><span class="token punctuation">[</span>50000, 784<span class="token punctuation">]</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span>0<span class="token punctuation">)</span> tensor<span class="token punctuation">(</span>9<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="2__torchnn_70"></a>2. 从零开始的神经网络(无 torch.nn）</h3> 
<p>PyTorch 提供了创建随机或零填充张量的方法，这里将使用它们为简单的线性模型创建权重和偏差。这些只是常规张量，还有一个特殊的附加值：告诉python 它们需要的梯度。这使PyTorch 记录了在张量上完成的所有操作，因此它可以在反向传播时<code>自动地计算梯度</code>。</p> 
<p>对于权重，在初始化之后设置<code>requires_grad</code> ，因为不希望该步骤包含在梯度中。 (请注意，<code>PyTorch 中的尾随_表示该操作是就地执行的</code>。）</p> 
<ol><li> <p>Xavier 初始化<br> 在这里用 Xavier 初始化(通过乘以 1 / sqrt(n））来初始化权重。</p> <pre><code class="prism language-bash"><span class="token function">import</span> math

weights <span class="token operator">=</span> torch.randn<span class="token punctuation">(</span>784, 10<span class="token punctuation">)</span> / math.sqrt<span class="token punctuation">(</span>784<span class="token punctuation">)</span>
weights.requires_grad_<span class="token punctuation">(</span><span class="token punctuation">)</span>
bias <span class="token operator">=</span> torch.zeros<span class="token punctuation">(</span>10, requires_grad<span class="token operator">=</span>True<span class="token punctuation">)</span>
</code></pre> </li><li> <p>线性模型和激活函数<br> 由于 PyTorch 具有自动计算梯度的功能，可以将任何标准的 Python 函数(或可调用对象）用作模型！ 因此，编写一个简单的矩阵乘法和广播加法来创建一个简单的线性模型。 还需要激活函数，因此将编写并使用 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           l 
          
         
           o 
          
         
           g 
          
         
           _ 
          
         
           s 
          
         
           o 
          
         
           f 
          
         
           t 
          
         
           m 
          
         
           a 
          
         
           x 
          
         
        
          log\_softmax 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.00444em; vertical-align: -0.31em;"></span><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right: 0.03588em;">g</span><span class="mord" style="margin-right: 0.02778em;">_</span><span class="mord mathdefault">s</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right: 0.10764em;">f</span><span class="mord mathdefault">t</span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span></span></span></span></span> 。</p> 
  <blockquote> 
   <p>请记住：尽管 PyTorch 提供了许多预先编写的损失函数，激活函数等，但是您可以使用纯 Python 轻松编写自己的函数。 PyTorch甚至会自动为您的函数创建快速 GPU 或矢量化的 CPU 代码。</p> 
  </blockquote> <pre><code class="prism language-bash">def log_softmax<span class="token punctuation">(</span>x<span class="token punctuation">)</span>:
    <span class="token keyword">return</span> x - x.exp<span class="token punctuation">(</span><span class="token punctuation">)</span>.sum<span class="token punctuation">(</span>-1<span class="token punctuation">)</span>.log<span class="token punctuation">(</span><span class="token punctuation">)</span>.unsqueeze<span class="token punctuation">(</span>-1<span class="token punctuation">)</span>

def model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>:
    <span class="token keyword">return</span> log_softmax<span class="token punctuation">(</span>xb @ weights + bias<span class="token punctuation">)</span>
</code></pre> <p>在上面，@代表点积运算。 将对一批数据(在这种情况下为 64 张图像）调用函数。 这是一个前向传播。</p> 
  <blockquote> 
   <p>请注意，由于我们从随机权重开始，因此在这一阶段，我们的预测不会比随机预测更好。</p> 
  </blockquote> <pre><code class="prism language-bash">bs <span class="token operator">=</span> 64  <span class="token comment"># batch size</span>

xb <span class="token operator">=</span> x_train<span class="token punctuation">[</span>0:bs<span class="token punctuation">]</span>  <span class="token comment"># a mini-batch from x</span>
preds <span class="token operator">=</span> model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>  <span class="token comment"># predictions</span>
preds<span class="token punctuation">[</span>0<span class="token punctuation">]</span>, preds.shape
print<span class="token punctuation">(</span>preds<span class="token punctuation">[</span>0<span class="token punctuation">]</span>, preds.shape<span class="token punctuation">)</span>
</code></pre> <p>输出结果：</p> <pre><code class="prism language-bash">tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>-2.8144, -2.9565, -2.1889, -2.8592, -1.6188, -1.8370, -2.1526, -2.5953,
        -2.4119, -2.5154<span class="token punctuation">]</span>, grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>SelectBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span> torch.Size<span class="token punctuation">(</span><span class="token punctuation">[</span>64, 10<span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> <p>可以看出，preds张量不仅包含张量值，还包含梯度函数。 稍后将使用它进行反向传播。</p> </li><li> <p>损失函数<br> 实现负对数似然作为损失函数(同样，只能使用标准 Python）：</p> <pre><code class="prism language-bash">def nll<span class="token punctuation">(</span>input, target<span class="token punctuation">)</span>:
    <span class="token keyword">return</span> -input<span class="token punctuation">[</span>range<span class="token punctuation">(</span>target.shape<span class="token punctuation">[</span>0<span class="token punctuation">]</span><span class="token punctuation">)</span>, target<span class="token punctuation">]</span>.mean<span class="token punctuation">(</span><span class="token punctuation">)</span>

loss_func <span class="token operator">=</span> nll
</code></pre> <p>用随机模型来检查损失，以便我们以后看向后传播后是否可以改善。</p> <pre><code class="prism language-bash">yb <span class="token operator">=</span> y_train<span class="token punctuation">[</span>0:bs<span class="token punctuation">]</span>
print<span class="token punctuation">(</span>loss_func<span class="token punctuation">(</span>preds, yb<span class="token punctuation">))</span>
</code></pre> <p>输出结果：</p> <pre><code class="prism language-bash">tensor<span class="token punctuation">(</span>2.4812, grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NegBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>
</code></pre> </li><li> <p>计算准确性<br> 还实现一个函数来计算模型的准确性。 对于每个预测，如果具有最大值的索引与目标值匹配，则该预测是正确的。</p> <pre><code class="prism language-bash">def accuracy<span class="token punctuation">(</span>out, yb<span class="token punctuation">)</span>:
    preds <span class="token operator">=</span> torch.argmax<span class="token punctuation">(</span>out, dim<span class="token operator">=</span>1<span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span>preds <span class="token operator">==</span> yb<span class="token punctuation">)</span>.float<span class="token punctuation">(</span><span class="token punctuation">)</span>.mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> <p>检查一下随机模型的准确性，以便我们可以看出随着损失的增加，准确性是否有所提高。</p> <pre><code class="prism language-bash">print<span class="token punctuation">(</span>accuracy<span class="token punctuation">(</span>preds, yb<span class="token punctuation">))</span>
</code></pre> <p>输出结果：</p> <pre><code class="prism language-bash">tensor<span class="token punctuation">(</span>0.0781<span class="token punctuation">)</span>
</code></pre> </li><li> <p>训练<br> 运行一个训练循环。 对于每次迭代：选择一个小批量数据(大小为bs)；使用模型进行预测；计算损失；loss.backward()更新模型的梯度，在这种情况下为weights和bias。</p> <p>现在，使用这些梯度来更新权重和偏差。 在torch.no_grad()上下文管理器中执行此操作，因为我们不希望在下一步的梯度计算中记录这些操作。</p> <p>然后，将梯度设置为零，以便为下一个循环做好准备。 否则，我们的<code>梯度会记录所有已发生操作的运行记录(即loss.backward() 将梯度添加到已存储的内容中，而不是替换它们）</code>。</p> <p>可以使用标准的 python 调试器逐步浏览 PyTorch 代码，从而可以在每一步检查各种变量值。 取消注释以下set_trace()即可尝试。</p> <pre><code class="prism language-bash">from IPython.core.debugger <span class="token function">import</span> set_trace

lr <span class="token operator">=</span> 0.5  <span class="token comment"># learning rate</span>
epochs <span class="token operator">=</span> 2  <span class="token comment"># how many epochs to train for</span>

<span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>epochs<span class="token punctuation">)</span>:
    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">((</span>n - 1<span class="token punctuation">)</span> // bs + 1<span class="token punctuation">)</span>:
        <span class="token comment">#         set_trace()</span>
        start_i <span class="token operator">=</span> i * bs
        end_i <span class="token operator">=</span> start_i + bs
        xb <span class="token operator">=</span> x_train<span class="token punctuation">[</span>start_i:end_i<span class="token punctuation">]</span>
        yb <span class="token operator">=</span> y_train<span class="token punctuation">[</span>start_i:end_i<span class="token punctuation">]</span>
        pred <span class="token operator">=</span> model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> loss_func<span class="token punctuation">(</span>pred, yb<span class="token punctuation">)</span>

        loss.backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        with torch.no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>:
            weights -<span class="token operator">=</span> weights.grad * lr
            bias -<span class="token operator">=</span> bias.grad * lr
            weights.grad.zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>
            bias.grad.zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> <p>这样完全从头开始创建并训练了一个最小的神经网络(在这种情况下，是逻辑回归，因为没有隐藏的层）！</p> <p>检查损失和准确性，并将其与我们之前获得的进行比较。 希望损失会减少，准确性会增加，而且确实如此。</p> <pre><code class="prism language-bash">print<span class="token punctuation">(</span>loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>, yb<span class="token punctuation">)</span>, accuracy<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>, yb<span class="token punctuation">))</span>	
</code></pre> <p>输出结果：</p> <pre><code class="prism language-bash">tensor<span class="token punctuation">(</span>0.0572, grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NegBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span> tensor<span class="token punctuation">(</span>1.<span class="token punctuation">)</span>
</code></pre> </li></ol> 
<h3><a id="3__191"></a>3. 重构</h3> 
<h4><a id="31__torchnnfunctional_192"></a>3.1 使用 torch.nn.functional</h4> 
<p>现在，将重构代码，使其与以前相同，只是将开始利用 PyTorch 的nn类使其更加简洁和灵活。 从这里开始的每一步，都应该使代码中的一个或多个：更短，更易理解和/或更灵活。</p> 
<p>第一步也是最简单的步骤，就是用<code>torch.nn.functional</code>(通常按照惯例将其导入到名称空间F中）替换我们的手写激活和损失函数，从而缩短代码长度。 该模块包含<code>torch.nn</code>库中的所有函数(而该库的其他部分包含类）。 除了广泛的损失和激活函数外，您还会在这里找到一些合适的函数来创建神经网络，例如池化函数。 (还有一些用于进行卷积，线性图层等的函数，但是正如我们将看到的那样，通常可以使用库的其他部分来更好地处理这些函数。）</p> 
<p>如果您使用的是负对数似然损失和 log softmax 激活，那么 Pytorch 会提供将两者结合的单个函数<code>F.cross_entropy</code>。 因此，我们甚至可以从模型中删除激活函数。</p> 
<pre><code class="prism language-bash"><span class="token function">import</span> torch.nn.functional as F

loss_func <span class="token operator">=</span> F.cross_entropy

def model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>:
    <span class="token keyword">return</span> xb @ weights + bias
</code></pre> 
<blockquote> 
 <p>请注意，不再在model函数中调用log_softmax。 确认损失和准确性与以前相同：</p> 
</blockquote> 
<pre><code class="prism language-bash">print<span class="token punctuation">(</span>loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>, yb<span class="token punctuation">)</span>, accuracy<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>, yb<span class="token punctuation">))</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code class="prism language-bash">tensor<span class="token punctuation">(</span>0.0572, grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span> tensor<span class="token punctuation">(</span>1.<span class="token punctuation">)</span>
</code></pre> 
<h4><a id="32__nnModule__217"></a>3.2. 使用 nn.Module 进行重构</h4> 
<p>接下来，使用<code>nn.Module和nn.Parameter</code>进行<code>更清晰，更简洁</code>的训练循环。 将<code>nn.Module</code>子类化(它本身是一个类并且能够跟踪状态）。 在这种情况下，我们要创建一个类，该类包含前进步骤的权重，偏差和方法。 <code>nn.Module</code>具有许多我们将要使用的属性和方法(例如.parameters()和.zero_grad()）。</p> 
<blockquote> 
 <p>nn.Module(大写 M）是 PyTorch 的特定概念，也是我们将经常使用的一个类。 nn.Module不要与(小写m）模块的 Python 概念混淆，该模块是可以导入的 Python 代码文件。</p> 
</blockquote> 
<pre><code class="prism language-bash">from torch <span class="token function">import</span> nn

class Mnist_Logistic<span class="token punctuation">(</span>nn.Module<span class="token punctuation">)</span>:
    def __init__<span class="token punctuation">(</span>self<span class="token punctuation">)</span>:
        super<span class="token punctuation">(</span><span class="token punctuation">)</span>.__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self.weights <span class="token operator">=</span> nn.Parameter<span class="token punctuation">(</span>torch.randn<span class="token punctuation">(</span>784, 10<span class="token punctuation">)</span> / math.sqrt<span class="token punctuation">(</span>784<span class="token punctuation">))</span>
        self.bias <span class="token operator">=</span> nn.Parameter<span class="token punctuation">(</span>torch.zeros<span class="token punctuation">(</span>10<span class="token punctuation">))</span>

    def forward<span class="token punctuation">(</span>self, xb<span class="token punctuation">)</span>:
        <span class="token keyword">return</span> xb @ self.weights + self.bias
</code></pre> 
<p>由于<code>现在使用的是对象而不是仅使用函数，因此我们首先必须实例化模型</code>：</p> 
<pre><code class="prism language-bash">model <span class="token operator">=</span> Mnist_Logistic<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>现在可以像以前一样计算损失。 <code>请注意，nn.Module对象的使用就像它们是函数一样(即，它们是可调用的），但是在后台 Pytorch 会自动调用我们的forward方法。</code></p> 
<pre><code class="prism language-bash">print<span class="token punctuation">(</span>loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>, yb<span class="token punctuation">))</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code class="prism language-bash">tensor<span class="token punctuation">(</span>2.3484, grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>
</code></pre> 
<p>以前，在训练循环中，必须按名称更新每个参数的值，并手动将每个参数的 grads 分别归零，如下所示：</p> 
<pre><code class="prism language-bash">with torch.no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>:
    weights -<span class="token operator">=</span> weights.grad * lr
    bias -<span class="token operator">=</span> bias.grad * lr
    weights.grad.zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>
    bias.grad.zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>现在我们可以利用 <code>model.parameters(）</code>和 <code>model.zero_grad(）</code>(它们都由 PyTorch 为nn.Module定义）来使这些步骤更简洁，并且更不会出现忘记某些参数的错误，特别是在 我们有一个更复杂的模型：</p> 
<pre><code class="prism language-bash">with torch.no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>:
    <span class="token keyword">for</span> p <span class="token keyword">in</span> model.parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>: p -<span class="token operator">=</span> p.grad * lr
    model.zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>我们将把小的训练循环包装在fit函数中，以便稍后再运行。</p> 
<pre><code class="prism language-bash">def fit<span class="token punctuation">(</span><span class="token punctuation">)</span>:
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>epochs<span class="token punctuation">)</span>:
        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">((</span>n - 1<span class="token punctuation">)</span> // bs + 1<span class="token punctuation">)</span>:
            start_i <span class="token operator">=</span> i * bs
            end_i <span class="token operator">=</span> start_i + bs
            xb <span class="token operator">=</span> x_train<span class="token punctuation">[</span>start_i:end_i<span class="token punctuation">]</span>
            yb <span class="token operator">=</span> y_train<span class="token punctuation">[</span>start_i:end_i<span class="token punctuation">]</span>
            pred <span class="token operator">=</span> model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>
            loss <span class="token operator">=</span> loss_func<span class="token punctuation">(</span>pred, yb<span class="token punctuation">)</span>

            loss.backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            with torch.no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>:
                <span class="token keyword">for</span> p <span class="token keyword">in</span> model.parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>:
                    p -<span class="token operator">=</span> p.grad * lr
                model.zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>

fit<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>仔细检查一下我们的损失是否下降了：</p> 
<pre><code class="prism language-bash">print<span class="token punctuation">(</span>loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>, yb<span class="token punctuation">))</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code class="prism language-bash">tensor<span class="token punctuation">(</span>0.0838, grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="33__nnLinear__293"></a>3.3. 使用 nn.Linear 重构</h4> 
<p>继续重构我们的代码。 代替手动定义和初始化self.weights和self.bias并计算xb @ self.weights + self.bias，我们将对线性层使用 Pytorch 类 nn.Linear ，这将为我们完成所有工作。 Pytorch 具有许多类型的预定义层，可以大大简化我们的代码，并且通常也可以使其速度更快。</p> 
<pre><code class="prism language-bash">class Mnist_Logistic<span class="token punctuation">(</span>nn.Module<span class="token punctuation">)</span>:
    def __init__<span class="token punctuation">(</span>self<span class="token punctuation">)</span>:
        super<span class="token punctuation">(</span><span class="token punctuation">)</span>.__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self.lin <span class="token operator">=</span> nn.Linear<span class="token punctuation">(</span>784, 10<span class="token punctuation">)</span>

    def forward<span class="token punctuation">(</span>self, xb<span class="token punctuation">)</span>:
        <span class="token keyword">return</span> self.lin<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>
</code></pre> 
<p>用与以前相同的方式实例化模型并计算损失：</p> 
<pre><code class="prism language-bash">model <span class="token operator">=</span> Mnist_Logistic<span class="token punctuation">(</span><span class="token punctuation">)</span>
print<span class="token punctuation">(</span>loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>, yb<span class="token punctuation">))</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code class="prism language-bash">tensor<span class="token punctuation">(</span>2.3402, grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>
</code></pre> 
<p>仍然可以使用与以前相同的fit方法。</p> 
<pre><code class="prism language-bash">fit<span class="token punctuation">(</span><span class="token punctuation">)</span>

print<span class="token punctuation">(</span>loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>, yb<span class="token punctuation">))</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code class="prism language-bash">tensor<span class="token punctuation">(</span>0.0819, grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="34__326"></a>3.4. 使用优化重构</h4> 
<p>Pytorch 还提供了一个包含各种优化算法的软件包torch.optim。 我们可以使用优化器中的step方法采取向前的步骤，而不是手动更新每个参数。</p> 
<p>这就是我们将要替换之前手动编码的优化步骤：</p> 
<pre><code class="prism language-bash">with torch.no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>:
    <span class="token keyword">for</span> p <span class="token keyword">in</span> model.parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>: p -<span class="token operator">=</span> p.grad * lr
    model.zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>我们只需使用下面的代替：</p> 
<pre><code class="prism language-bash">opt.step<span class="token punctuation">(</span><span class="token punctuation">)</span>
opt.zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<blockquote> 
 <p>(optim.zero_grad()将梯度重置为 0，我们需要在计算下一个小批量的梯度之前调用它。）</p> 
</blockquote> 
<pre><code class="prism language-bash">from torch <span class="token function">import</span> optim
</code></pre> 
<p>定义一个小函数来创建模型和优化器，以便将来再次使用。</p> 
<pre><code class="prism language-bash">def get_model<span class="token punctuation">(</span><span class="token punctuation">)</span>:
    model <span class="token operator">=</span> Mnist_Logistic<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> model, optim.SGD<span class="token punctuation">(</span>model.parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>, lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>

model, opt <span class="token operator">=</span> get_model<span class="token punctuation">(</span><span class="token punctuation">)</span>
print<span class="token punctuation">(</span>loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>, yb<span class="token punctuation">))</span>

<span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>epochs<span class="token punctuation">)</span>:
    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token variable"><span class="token punctuation">((</span>n <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span><span class="token operator">/</span> bs <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">:</span>
        start_i <span class="token operator">=</span> i <span class="token operator">*</span> bs
        end_i <span class="token operator">=</span> start_i <span class="token operator">+</span> bs
        xb <span class="token operator">=</span> x_train[start_i<span class="token operator">:</span>end_i]
        yb <span class="token operator">=</span> y_train[start_i<span class="token operator">:</span>end_i]
        pred <span class="token operator">=</span> model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> loss_func<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> yb<span class="token punctuation">)</span>

        loss.backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt.step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt.zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>

print<span class="token punctuation">(</span>loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span><span class="token punctuation">,</span> yb<span class="token punctuation">))</span></span>
</code></pre> 
<p>输出结果：</p> 
<pre><code class="prism language-bash">tensor<span class="token punctuation">(</span>2.2361, grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span>0.0818, grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="35__376"></a>3.5. 使用数据集进行重构</h4> 
<p>PyTorch 有一个抽象的 <code>Dataset</code> 类。 数据集可以是具有<code>__len__</code>函数(由 Python 的标准len函数调用）和具有<code>__getitem__</code>函数作为对其进行索引的一种方法。 本教程演示了一个不错的示例，该示例创建一个自定义<code>FacialLandmarkDataset</code>类作为<code>Dataset</code>的子类。</p> 
<p>PyTorch 的 TensorDataset 是一个数据集包装张量。 通过定义索引的长度和方式，这也为我们提供了沿张量的一维进行迭代，索引和切片的方法。 这将使我们在训练的同一行中更容易访问自变量和因变量。</p> 
<pre><code class="prism language-bash">from torch.utils.data <span class="token function">import</span> TensorDataset

</code></pre> 
<blockquote> 
 <p>x_train和y_train都可以合并为一个TensorDataset，这将更易于迭代和切片。</p> 
</blockquote> 
<pre><code class="prism language-bash">train_ds <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>x_train, y_train<span class="token punctuation">)</span>
</code></pre> 
<p>以前，我们不得不分别遍历 x 和 y 值的迷你批处理：</p> 
<pre><code class="prism language-bash">xb <span class="token operator">=</span> x_train<span class="token punctuation">[</span>start_i:end_i<span class="token punctuation">]</span>
yb <span class="token operator">=</span> y_train<span class="token punctuation">[</span>start_i:end_i<span class="token punctuation">]</span>
</code></pre> 
<p>现在，我们可以将两个步骤一起执行：</p> 
<pre><code class="prism language-bash">xb,yb <span class="token operator">=</span> train_ds<span class="token punctuation">[</span>i*bs <span class="token keyword">:</span> i*bs+bs<span class="token punctuation">]</span>
</code></pre> 
<pre><code class="prism language-bash">model, opt <span class="token operator">=</span> get_model<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>epochs<span class="token punctuation">)</span>:
    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token variable"><span class="token punctuation">((</span>n <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span><span class="token operator">/</span> bs <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">:</span>
        xb<span class="token punctuation">,</span> yb <span class="token operator">=</span> train_ds[i <span class="token operator">*</span> bs<span class="token operator">:</span> i <span class="token operator">*</span> bs <span class="token operator">+</span> bs]
        pred <span class="token operator">=</span> model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> loss_func<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> yb<span class="token punctuation">)</span>

        loss.backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt.step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt.zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>

print<span class="token punctuation">(</span>loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span><span class="token punctuation">,</span> yb<span class="token punctuation">))</span></span>
</code></pre> 
<p>输出结果：</p> 
<pre><code class="prism language-bash">tensor<span class="token punctuation">(</span>0.0805, grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="36__DataLoader__421"></a>3.6. 使用 DataLoader 进行重构</h4> 
<p>PyTorch 的<code>DataLoader</code> 负责批次管理。可以从任何Dataset创建一个DataLoader。DataLoader使迭代变得更加容易，不必使用<code>train_ds[i*bs : i*bs+bs]</code> ，DataLoader 会自动为我们提供每个小批量。</p> 
<pre><code class="prism language-bash">from torch.utils.data <span class="token function">import</span> DataLoader

train_ds <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>x_train, y_train<span class="token punctuation">)</span>
train_dl <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>train_ds, batch_size<span class="token operator">=</span>bs<span class="token punctuation">)</span>
</code></pre> 
<p>以前，我们的循环遍历批处理(xb，yb），如下所示：</p> 
<pre><code class="prism language-bash"><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">((</span>n-1<span class="token punctuation">)</span>//bs + 1<span class="token punctuation">)</span>:
    xb,yb <span class="token operator">=</span> train_ds<span class="token punctuation">[</span>i*bs <span class="token keyword">:</span> i*bs+bs<span class="token punctuation">]</span>
    pred <span class="token operator">=</span> model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>
</code></pre> 
<p>现在，我们的循环更加简洁了，因为(xb，yb）是从数据加载器自动加载的：</p> 
<pre><code class="prism language-bash"><span class="token keyword">for</span> xb,yb <span class="token keyword">in</span> train_dl:
    pred <span class="token operator">=</span> model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-bash">model, opt <span class="token operator">=</span> get_model<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>epochs<span class="token punctuation">)</span>:
    <span class="token keyword">for</span> xb, yb <span class="token keyword">in</span> train_dl:
        pred <span class="token operator">=</span> model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> loss_func<span class="token punctuation">(</span>pred, yb<span class="token punctuation">)</span>

        loss.backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt.step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt.zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>

print<span class="token punctuation">(</span>loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>, yb<span class="token punctuation">))</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code class="prism language-bash">tensor<span class="token punctuation">(</span>0.0804, grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="4___462"></a>4. 添加基本功能</h3> 
<p>得益于 Pytorch 的<code>nn.Module</code>，<code>nn.Parameter</code>，<code>Dataset</code>和<code>DataLoader</code>，我们的训练循环现在变得更小，更容易理解。 现在，让我们尝试添加在实践中创建有效模型所需的基本功能。</p> 
<h4><a id="41__464"></a>4.1. 添加验证</h4> 
<p>在第 3 节中，只是试图建立一个合理的训练循环以用于我们的训练数据。 实际上，应该具有验证集，以便识别是否过度拟合。</p> 
<p>打乱训练数据顺序对于防止批次与过度拟合之间的相关性很重要。 另一方面，无论我们是否打乱验证集，验证损失都是相同的。 由于打乱顺序需要花费更多时间，因此打乱验证集数据顺序没有任何意义。</p> 
<p>我们将验证集的批次大小设为训练集的两倍。 这是因为<code>验证集不需要反向传播，因此占用的内存更少(不需要存储渐变）</code>。 利用这一优势来使用更大的批量，并更快地计算损失。</p> 
<pre><code class="prism language-bash">train_ds <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>x_train, y_train<span class="token punctuation">)</span>
train_dl <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>train_ds, batch_size<span class="token operator">=</span>bs, shuffle<span class="token operator">=</span>True<span class="token punctuation">)</span>

valid_ds <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>x_valid, y_valid<span class="token punctuation">)</span>
valid_dl <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>valid_ds, batch_size<span class="token operator">=</span>bs * 2<span class="token punctuation">)</span>
</code></pre> 
<p>将在每个 epoch 结束时计算并打印验证损失。</p> 
<blockquote> 
 <p>(请注意，总是在训练之前调用model.train()，并在推断之前调用model.eval()，因为诸如nn.BatchNorm2d和nn.Dropout之类的图层会使用它们，以确保这些不同阶段的行为正确。）</p> 
</blockquote> 
<pre><code class="prism language-bash">model, opt <span class="token operator">=</span> get_model<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>epochs<span class="token punctuation">)</span>:
    model.train<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> xb, yb <span class="token keyword">in</span> train_dl:
        pred <span class="token operator">=</span> model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> loss_func<span class="token punctuation">(</span>pred, yb<span class="token punctuation">)</span>

        loss.backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt.step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt.zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>

    model.eval<span class="token punctuation">(</span><span class="token punctuation">)</span>
    with torch.no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>:
        valid_loss <span class="token operator">=</span> sum<span class="token punctuation">(</span>loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>, yb<span class="token punctuation">)</span> <span class="token keyword">for</span> xb, yb <span class="token keyword">in</span> valid_dl<span class="token punctuation">)</span>

    print<span class="token punctuation">(</span>epoch, valid_loss / len<span class="token punctuation">(</span>valid_dl<span class="token punctuation">))</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code class="prism language-bash">0 tensor<span class="token punctuation">(</span>0.3143<span class="token punctuation">)</span>
1 tensor<span class="token punctuation">(</span>0.3289<span class="token punctuation">)</span>
</code></pre> 
<h4><a id="42__fit_get_data_506"></a>4.2. 创建 fit(）和 get_data(）</h4> 
<p>现在，将自己进行一些重构。 由于我们经历了两次相似的过程来计算训练集和验证集的损失，因此我们将其设为自己的函数loss_batch，该函数可计算一批损失。</p> 
<p>我们将优化器传入训练集中，并使用它执行反向传播。 对于验证集，我们没有通过优化程序，因此该方法不会执行反向传播。</p> 
<pre><code class="prism language-bash">def loss_batch<span class="token punctuation">(</span>model, loss_func, xb, yb, opt<span class="token operator">=</span>None<span class="token punctuation">)</span>:
    loss <span class="token operator">=</span> loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>, yb<span class="token punctuation">)</span>

    <span class="token keyword">if</span> opt is not None:
        loss.backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt.step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt.zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> loss.item<span class="token punctuation">(</span><span class="token punctuation">)</span>, len<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>
</code></pre> 
<p><code>fit</code>运行必要的操作来训练我们的模型，并计算每个时期的训练和验证损失。</p> 
<pre><code class="prism language-bash"><span class="token function">import</span> numpy as np

def fit<span class="token punctuation">(</span>epochs, model, loss_func, opt, train_dl, valid_dl<span class="token punctuation">)</span>:
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>epochs<span class="token punctuation">)</span>:
        model.train<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> xb, yb <span class="token keyword">in</span> train_dl:
            loss_batch<span class="token punctuation">(</span>model, loss_func, xb, yb, opt<span class="token punctuation">)</span>

        model.eval<span class="token punctuation">(</span><span class="token punctuation">)</span>
        with torch.no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>:
            losses, nums <span class="token operator">=</span> zip<span class="token punctuation">(</span>
                *<span class="token punctuation">[</span>loss_batch<span class="token punctuation">(</span>model, loss_func, xb, yb<span class="token punctuation">)</span> <span class="token keyword">for</span> xb, yb <span class="token keyword">in</span> valid_dl<span class="token punctuation">]</span>
            <span class="token punctuation">)</span>
        val_loss <span class="token operator">=</span> np.sum<span class="token punctuation">(</span>np.multiply<span class="token punctuation">(</span>losses, nums<span class="token punctuation">))</span> / np.sum<span class="token punctuation">(</span>nums<span class="token punctuation">)</span>

        print<span class="token punctuation">(</span>epoch, val_loss<span class="token punctuation">)</span>
</code></pre> 
<p><code>get_data</code>返回用于训练和验证集的数据加载器。</p> 
<pre><code class="prism language-bash">def get_data<span class="token punctuation">(</span>train_ds, valid_ds, bs<span class="token punctuation">)</span>:
    <span class="token keyword">return</span> <span class="token punctuation">(</span>
        DataLoader<span class="token punctuation">(</span>train_ds, batch_size<span class="token operator">=</span>bs, shuffle<span class="token operator">=</span>True<span class="token punctuation">)</span>,
        DataLoader<span class="token punctuation">(</span>valid_ds, batch_size<span class="token operator">=</span>bs * 2<span class="token punctuation">)</span>,
    <span class="token punctuation">)</span>
</code></pre> 
<p>现在，我们获取数据加载器和拟合模型的整个过程可以在 3 行代码中运行：</p> 
<pre><code class="prism language-bash">train_dl, valid_dl <span class="token operator">=</span> get_data<span class="token punctuation">(</span>train_ds, valid_ds, bs<span class="token punctuation">)</span>
model, opt <span class="token operator">=</span> get_model<span class="token punctuation">(</span><span class="token punctuation">)</span>
fit<span class="token punctuation">(</span>epochs, model, loss_func, opt, train_dl, valid_dl<span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code class="prism language-bash">0 0.34373447265625
1 0.31163097116947175
</code></pre> 
<blockquote> 
 <p><strong>可以使用这些基本的 3 行代码来训练各种各样的模型。</strong></p> 
</blockquote> 
<h3><a id="5__CNN_562"></a>5. 切换到 CNN</h3> 
<p>现在，我们将构建具有三个卷积层的神经网络。 由于上一节中的所有函数都不包含任何有关模型组合的内容，因此我们将能够使用它们来训练 CNN，而无需进行任何修改。</p> 
<p>我们将使用 Pytorch 的预定义 Conv2d 类作为我们的卷积层。 我们定义具有 3 个卷积层的 CNN。 每个卷积后跟一个 ReLU。 最后，我们执行平均池化。 <code>(请注意，view是 numpy 的reshape的 PyTorch 版本）</code></p> 
<pre><code class="prism language-bash">class Mnist_CNN<span class="token punctuation">(</span>nn.Module<span class="token punctuation">)</span>:
    def __init__<span class="token punctuation">(</span>self<span class="token punctuation">)</span>:
        super<span class="token punctuation">(</span><span class="token punctuation">)</span>.__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self.conv1 <span class="token operator">=</span> nn.Conv2d<span class="token punctuation">(</span>1, 16, kernel_size<span class="token operator">=</span>3, stride<span class="token operator">=</span>2, padding<span class="token operator">=</span>1<span class="token punctuation">)</span>
        self.conv2 <span class="token operator">=</span> nn.Conv2d<span class="token punctuation">(</span>16, 16, kernel_size<span class="token operator">=</span>3, stride<span class="token operator">=</span>2, padding<span class="token operator">=</span>1<span class="token punctuation">)</span>
        self.conv3 <span class="token operator">=</span> nn.Conv2d<span class="token punctuation">(</span>16, 10, kernel_size<span class="token operator">=</span>3, stride<span class="token operator">=</span>2, padding<span class="token operator">=</span>1<span class="token punctuation">)</span>

    def forward<span class="token punctuation">(</span>self, xb<span class="token punctuation">)</span>:
        xb <span class="token operator">=</span> xb.view<span class="token punctuation">(</span>-1, 1, 28, 28<span class="token punctuation">)</span>
        xb <span class="token operator">=</span> F.relu<span class="token punctuation">(</span>self.conv1<span class="token punctuation">(</span>xb<span class="token punctuation">))</span>
        xb <span class="token operator">=</span> F.relu<span class="token punctuation">(</span>self.conv2<span class="token punctuation">(</span>xb<span class="token punctuation">))</span>
        xb <span class="token operator">=</span> F.relu<span class="token punctuation">(</span>self.conv3<span class="token punctuation">(</span>xb<span class="token punctuation">))</span>
        xb <span class="token operator">=</span> F.avg_pool2d<span class="token punctuation">(</span>xb, 4<span class="token punctuation">)</span>
        <span class="token keyword">return</span> xb.view<span class="token punctuation">(</span>-1, xb.size<span class="token punctuation">(</span>1<span class="token punctuation">))</span>

lr <span class="token operator">=</span> 0.1
</code></pre> 
<p>动量是随机梯度下降的一种变体，它也考虑了以前的更新，通常可以加快训练速度。</p> 
<pre><code class="prism language-bash">model <span class="token operator">=</span> Mnist_CNN<span class="token punctuation">(</span><span class="token punctuation">)</span>
opt <span class="token operator">=</span> optim.SGD<span class="token punctuation">(</span>model.parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>, lr<span class="token operator">=</span>lr, momentum<span class="token operator">=</span>0.9<span class="token punctuation">)</span>

fit<span class="token punctuation">(</span>epochs, model, loss_func, opt, train_dl, valid_dl<span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code class="prism language-bash">0 0.32315067479610443
1 0.2475940626323223
</code></pre> 
<h3><a id="6_nnSequential_597"></a>6. nn.Sequential</h3> 
<p><code>torch.nn</code>还有另一个灵活的类，可以用来简化我们的代码： <code>Sequential</code> 。 <code>Sequential</code>对象以顺序方式运行其中包含的每个模块。 这是编写神经网络的一种简单方法。</p> 
<p>要利用此优势，我们需要能够从给定的函数轻松定义自定义层。 例如，PyTorch 没有视图图层，我们需要为网络创建一个图层。 Lambda将创建一个层，然后在使用Sequential定义网络时可以使用该层。</p> 
<pre><code class="prism language-bash">class Lambda<span class="token punctuation">(</span>nn.Module<span class="token punctuation">)</span>:
    def __init__<span class="token punctuation">(</span>self, func<span class="token punctuation">)</span>:
        super<span class="token punctuation">(</span><span class="token punctuation">)</span>.__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self.func <span class="token operator">=</span> func

    def forward<span class="token punctuation">(</span>self, x<span class="token punctuation">)</span>:
        <span class="token keyword">return</span> self.func<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

def preprocess<span class="token punctuation">(</span>x<span class="token punctuation">)</span>:
    <span class="token keyword">return</span> x.view<span class="token punctuation">(</span>-1, 1, 28, 28<span class="token punctuation">)</span>
</code></pre> 
<p>用Sequential创建的模型很简单：</p> 
<pre><code class="prism language-bash">model <span class="token operator">=</span> nn.Sequential<span class="token punctuation">(</span>
    Lambda<span class="token punctuation">(</span>preprocess<span class="token punctuation">)</span>,
    nn.Conv2d<span class="token punctuation">(</span>1, 16, kernel_size<span class="token operator">=</span>3, stride<span class="token operator">=</span>2, padding<span class="token operator">=</span>1<span class="token punctuation">)</span>,
    nn.ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>,
    nn.Conv2d<span class="token punctuation">(</span>16, 16, kernel_size<span class="token operator">=</span>3, stride<span class="token operator">=</span>2, padding<span class="token operator">=</span>1<span class="token punctuation">)</span>,
    nn.ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>,
    nn.Conv2d<span class="token punctuation">(</span>16, 10, kernel_size<span class="token operator">=</span>3, stride<span class="token operator">=</span>2, padding<span class="token operator">=</span>1<span class="token punctuation">)</span>,
    nn.ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>,
    nn.AvgPool2d<span class="token punctuation">(</span>4<span class="token punctuation">)</span>,
    Lambda<span class="token punctuation">(</span>lambda x: x.view<span class="token punctuation">(</span>x.size<span class="token punctuation">(</span>0<span class="token punctuation">)</span>, -1<span class="token punctuation">))</span>,
<span class="token punctuation">)</span>

opt <span class="token operator">=</span> optim.SGD<span class="token punctuation">(</span>model.parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>, lr<span class="token operator">=</span>lr, momentum<span class="token operator">=</span>0.9<span class="token punctuation">)</span>

fit<span class="token punctuation">(</span>epochs, model, loss_func, opt, train_dl, valid_dl<span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code class="prism language-bash">0 0.4177794136047363
1 0.2814746533513069
</code></pre> 
<h3><a id="7__DataLoader_637"></a>7. 包装 DataLoader</h3> 
<p>虽然我们的 CNN 网络很简洁，但是它只能在 MNIST 数据集上面有效，因为</p> 
<ul><li>MNIST 数据集假设输入为 28 * 28 长向量</li><li>MNIST 数据集假设 CNN 的最终网格尺寸为 4 * 4(这是因为我们使用的平均池化卷积核的大小）</li></ul> 
<p>让我们摆脱这两个假设，因此我们的模型需要适用于任何 2d 单通道图像。 首先，我们可以删除初始的 Lambda 层，但将数据预处理移至生成器中：</p> 
<pre><code class="prism language-bash">def preprocess<span class="token punctuation">(</span>x, y<span class="token punctuation">)</span>:
    <span class="token keyword">return</span> x.view<span class="token punctuation">(</span>-1, 1, 28, 28<span class="token punctuation">)</span>, y

class WrappedDataLoader:
    def __init__<span class="token punctuation">(</span>self, dl, func<span class="token punctuation">)</span>:
        self.dl <span class="token operator">=</span> dl
        self.func <span class="token operator">=</span> func

    def __len__<span class="token punctuation">(</span>self<span class="token punctuation">)</span>:
        <span class="token keyword">return</span> len<span class="token punctuation">(</span>self.dl<span class="token punctuation">)</span>

    def __iter__<span class="token punctuation">(</span>self<span class="token punctuation">)</span>:
        batches <span class="token operator">=</span> iter<span class="token punctuation">(</span>self.dl<span class="token punctuation">)</span>
        <span class="token keyword">for</span> b <span class="token keyword">in</span> batches:
            yield <span class="token punctuation">(</span>self.func<span class="token punctuation">(</span>*b<span class="token punctuation">))</span>

train_dl, valid_dl <span class="token operator">=</span> get_data<span class="token punctuation">(</span>train_ds, valid_ds, bs<span class="token punctuation">)</span>
train_dl <span class="token operator">=</span> WrappedDataLoader<span class="token punctuation">(</span>train_dl, preprocess<span class="token punctuation">)</span>
valid_dl <span class="token operator">=</span> WrappedDataLoader<span class="token punctuation">(</span>valid_dl, preprocess<span class="token punctuation">)</span>
</code></pre> 
<p>接下来，我们可以将<code>nn.AvgPool2d</code>替换为<code>nn.AdaptiveAvgPool2d</code>，<strong>这使我们可以定义所需的输出张量的大小，而不是所需的输入张量的大小</strong>。 结果，我们的模型将适用于任何大小的输入。</p> 
<pre><code class="prism language-bash">model <span class="token operator">=</span> nn.Sequential<span class="token punctuation">(</span>
    nn.Conv2d<span class="token punctuation">(</span>1, 16, kernel_size<span class="token operator">=</span>3, stride<span class="token operator">=</span>2, padding<span class="token operator">=</span>1<span class="token punctuation">)</span>,
    nn.ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>,
    nn.Conv2d<span class="token punctuation">(</span>16, 16, kernel_size<span class="token operator">=</span>3, stride<span class="token operator">=</span>2, padding<span class="token operator">=</span>1<span class="token punctuation">)</span>,
    nn.ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>,
    nn.Conv2d<span class="token punctuation">(</span>16, 10, kernel_size<span class="token operator">=</span>3, stride<span class="token operator">=</span>2, padding<span class="token operator">=</span>1<span class="token punctuation">)</span>,
    nn.ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>,
    nn.AdaptiveAvgPool2d<span class="token punctuation">(</span>1<span class="token punctuation">)</span>,
    Lambda<span class="token punctuation">(</span>lambda x: x.view<span class="token punctuation">(</span>x.size<span class="token punctuation">(</span>0<span class="token punctuation">)</span>, -1<span class="token punctuation">))</span>,
<span class="token punctuation">)</span>

opt <span class="token operator">=</span> optim.SGD<span class="token punctuation">(</span>model.parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>, lr<span class="token operator">=</span>lr, momentum<span class="token operator">=</span>0.9<span class="token punctuation">)</span>

fit<span class="token punctuation">(</span>epochs, model, loss_func, opt, train_dl, valid_dl<span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code class="prism language-bash">0 0.37449371428489686
1 0.2191312262892723
</code></pre> 
<h3><a id="8__GPU_688"></a>8. 使用 GPU</h3> 
<p>如果您足够幸运地能够使用具有 CUDA 功能的 GPU(您可以从大多数云提供商处以每小时$ 0.50 的价格租用一个 GPU），则可以使用它来加速代码。</p> 
<ol><li>首先检查您的 GPU 是否在 Pytorch 中正常工作：<pre><code class="prism language-bash">print<span class="token punctuation">(</span>torch.cuda.is_available<span class="token punctuation">(</span><span class="token punctuation">))</span>
</code></pre> 输出结果：<pre><code class="prism language-bash">True
</code></pre> </li><li>然后为其创建一个设备对象：<pre><code class="prism language-bash">dev <span class="token operator">=</span> torch.device<span class="token punctuation">(</span>
    <span class="token string">"cuda"</span><span class="token punctuation">)</span> <span class="token keyword">if</span> torch.cuda.is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> torch.device<span class="token punctuation">(</span><span class="token string">"cpu"</span><span class="token punctuation">)</span>
</code></pre> </li><li>让我们更新preprocess，将批次移至 GPU：<pre><code class="prism language-bash">def preprocess<span class="token punctuation">(</span>x, y<span class="token punctuation">)</span>:
    <span class="token keyword">return</span> x.view<span class="token punctuation">(</span>-1, 1, 28, 28<span class="token punctuation">)</span>.to<span class="token punctuation">(</span>dev<span class="token punctuation">)</span>, y.to<span class="token punctuation">(</span>dev<span class="token punctuation">)</span>

train_dl, valid_dl <span class="token operator">=</span> get_data<span class="token punctuation">(</span>train_ds, valid_ds, bs<span class="token punctuation">)</span>
train_dl <span class="token operator">=</span> WrappedDataLoader<span class="token punctuation">(</span>train_dl, preprocess<span class="token punctuation">)</span>
valid_dl <span class="token operator">=</span> WrappedDataLoader<span class="token punctuation">(</span>valid_dl, preprocess<span class="token punctuation">)</span>
</code></pre> </li><li>最后，我们可以将模型移至 GPU：<pre><code class="prism language-bash">model.to<span class="token punctuation">(</span>dev<span class="token punctuation">)</span>
opt <span class="token operator">=</span> optim.SGD<span class="token punctuation">(</span>model.parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>, lr<span class="token operator">=</span>lr, momentum<span class="token operator">=</span>0.9<span class="token punctuation">)</span>
</code></pre> </li></ol> 
<p>您应该发现它现在运行得更快：</p> 
<pre><code class="prism language-bash">fit<span class="token punctuation">(</span>epochs, model, loss_func, opt, train_dl, valid_dl<span class="token punctuation">)</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code class="prism language-bash">0 0.1909765040397644
1 0.180943009185791
</code></pre> 
<h3><a id="9__726"></a>9. 总结</h3> 
<p>现在，我们有了一个通用的数据管道和训练循环，您可以将其用于使用 Pytorch 训练多种类型的模型。 要了解现在可以轻松进行模型训练，请查看 mnist_sample 示例笔记本。</p> 
<p>当然，您需要添加很多内容，例如数据增强，超参数调整，监控训练，转移学习等。 这些功能在 fastai 库中可用，该库是使用本教程中所示的相同设计方法开发的，为希望进一步推广模型的从业人员提供了自然的下一步。</p> 
<p>该篇文章开始时将通过示例分别说明torch.nn，torch.optim，Dataset和DataLoader。 因此，让我们总结一下我们所看到的：</p> 
<ul><li>torch.nn<br> Module：创建一个类似函数行为功能的，但可以包含状态(例如神经网络层权重）的可调用对象。它知道它包含的Parameter，并且可以将其所有梯度归零，通过其循环进行权重更新等 。<br> Parameter：张量的包装器，它告诉Module具有在反向传播期间需要更新的权重。仅更新具有 require_grad 属性集的张量<br> functional：一个模块(通常按照常规导入到F名称空间中），包含激活函数，损失函数等。以及卷积和线性层之类的无状态版本。</li><li>torch.optim：包含其中SGD之类的优化程序，这些优化程序可以在反向传播期间更新权重参数</li><li>Dataset：一个具有__len__和__getitem__的抽象接口对象，包括 Pytorch 提供的类，例如TensorDataset</li><li>DataLoader：获取任何Dataset并创建一个迭代器，该迭代器返回批量数据。</li></ul> 
<p>参考链接：</p> 
<ol><li><a href="http://deeplearning.net/data/mnist/" rel="nofollow">http://deeplearning.net/data/mnist/</a></li><li><a href="https://pytorch.org/tutorials/beginner/nn_tutorial.html" rel="nofollow">https://pytorch.org/tutorials/beginner/nn_tutorial.html</a></li><li><a href="https://pytorch.apachecn.org/docs/1.4/53.html" rel="nofollow">https://pytorch.apachecn.org/docs/1.4/53.html</a></li></ol>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/00f1becf86e27283c2ee4fcf6c0a0a1e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">information_schema.columns字段说明，获取数据库表所有列信息</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/8378a3e833f4d1553708d6991fa2fea0/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">引用&amp;指针</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>