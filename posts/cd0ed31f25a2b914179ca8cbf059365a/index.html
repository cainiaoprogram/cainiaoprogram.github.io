<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>HDFS编程 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="HDFS编程" />
<meta property="og:description" content="实验2：HDFS编程 实验步骤1 【实验介绍】 实验内容
本实验利用 Hadoop 提供的 Java API 进行编程对 HDFS 进行操作。
实验目标
通过本实验掌握利用 Hadoop 提供的 Java API 进行编程对 HDFS 进行操作。
实验知识点
Hadoop Java API
HDFS 目录操作
实验环境
Hadoop 2.7.1
IDEA 11.0.10
CourseGrading在线实验环境
工作目录：~/Desktop/workspace/hdfs_pro
预备知识
Hadoop 基本操作
MapReduce 编程基础
HDFS 基本操作
Java 编程基础
实验步骤2 【实验原理】HDFS文件系统编程框架 以下代码为HDFS文件系统的编程框架：
01 import org.apache.hadoop.conf.Configuration; 02 import org.apache.hadoop.fs.FileSystem; 04 public class FileExist { 05 public static void main(String[] args) { 06 try { 07 Configuration conf = new Configuration(); 08 FileSystem fs = FileSystem." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/cd0ed31f25a2b914179ca8cbf059365a/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-11-06T16:41:31+08:00" />
<meta property="article:modified_time" content="2023-11-06T16:41:31+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">HDFS编程</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h5>实验2：HDFS编程</h5> 
<ul><li><a href="https://course.educg.net/exp/doexpDeskDocker.jsp?libCenter=false&amp;desktopParam=d3d3LmVkdWNnLm5ldDpOREk0WVRZNVkyUTNNR1V5TUROaE1EUTBaak00TUdKbFlqZzBNakV6TlRVOmRtNWpjR0Z6YzNkdmNtUTpZMjkxY25ObExtVmtkV05uTG01bGRBOg&amp;judgeParam=KZeGZDh56wDZDHY07xQFYquM47UWKVd0AXsPevmodOkKzCDrFRX084VjrF_2eOMPx1SNjd4ODWs&amp;assignID=12781&amp;guideID=121967&amp;procIndex=0#" rel="nofollow" title="实验步骤1">实验步骤1</a> <strong>【实验介绍】</strong></li></ul> 
<hr> 
<ol><li> <p>实验内容</p> <p>本实验利用 Hadoop 提供的 Java API 进行编程对 HDFS 进行操作。</p> </li><li> <p>实验目标</p> <p>通过本实验掌握利用 Hadoop 提供的 Java API 进行编程对 HDFS 进行操作。</p> </li><li> <p>实验知识点</p> <p>Hadoop Java API</p> <p>HDFS 目录操作</p> </li><li> <p>实验环境</p> <p>Hadoop 2.7.1</p> <p>IDEA 11.0.10</p> <p>CourseGrading在线实验环境</p> <p>工作目录：~/Desktop/workspace/hdfs_pro</p> </li></ol> 
<ol><li> <p>预备知识</p> <p>Hadoop 基本操作</p> <p>MapReduce 编程基础</p> <p>HDFS 基本操作</p> <p>Java 编程基础</p> </li><li><a href="https://course.educg.net/exp/doexpDeskDocker.jsp?libCenter=false&amp;desktopParam=d3d3LmVkdWNnLm5ldDpOREk0WVRZNVkyUTNNR1V5TUROaE1EUTBaak00TUdKbFlqZzBNakV6TlRVOmRtNWpjR0Z6YzNkdmNtUTpZMjkxY25ObExtVmtkV05uTG01bGRBOg&amp;judgeParam=KZeGZDh56wDZDHY07xQFYquM47UWKVd0AXsPevmodOkKzCDrFRX084VjrF_2eOMPx1SNjd4ODWs&amp;assignID=12781&amp;guideID=121967&amp;procIndex=0#" rel="nofollow" title="实验步骤2">实验步骤2</a> <strong>【实验原理】HDFS文件系统编程框架</strong></li><li> 
  <hr><p>以下代码为HDFS文件系统的编程框架：</p> <pre><code>01 import org.apache.hadoop.conf.Configuration;
02 import org.apache.hadoop.fs.FileSystem;
04 public class FileExist {
05     public static void main(String[] args) {
06         try {
07             Configuration conf = new Configuration();
08             FileSystem fs = FileSystem.get(conf);
09         }catch(Exception e) {
10             e.printStackTrace();
11         }
12     }
13 }
</code></pre> <p>上述代码的第7行到第8行为根据配置（Configuration）创建文件系统。文件编程的接口大多都是基于FileSystem类提供的方法进行的，在获得fs对象后，就可以对文件进行操作了。</p> <p>FileSystem类中各方法的详细含义可参考：<a href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html" rel="nofollow" title="FileSystem (Apache Hadoop Main 3.3.6 API)">FileSystem (Apache Hadoop Main 3.3.6 API)</a></p> <p></p> </li></ol> 
<ul><li><a href="https://course.educg.net/exp/doexpDeskDocker.jsp?libCenter=false&amp;desktopParam=d3d3LmVkdWNnLm5ldDpOREk0WVRZNVkyUTNNR1V5TUROaE1EUTBaak00TUdKbFlqZzBNakV6TlRVOmRtNWpjR0Z6YzNkdmNtUTpZMjkxY25ObExtVmtkV05uTG01bGRBOg&amp;judgeParam=KZeGZDh56wDZDHY07xQFYquM47UWKVd0AXsPevmodOkKzCDrFRX084VjrF_2eOMPx1SNjd4ODWs&amp;assignID=12781&amp;guideID=121967&amp;procIndex=0#" rel="nofollow" title="实验步骤3">实验步骤3</a> <strong>【大数据集群使用说明】</strong></li></ul> 
<hr> 
<p>大数据集群和云桌面实验环境是联动的，在实验平台对云桌面实验环境进行创建、停止、启动、还原等操作时，会同时对大数据集群进行相应的操作。</p> 
<p>接下来，以1主3从的4节点大数据集群为例进行说明。</p> 
<p><strong>1. 大数据集群的创建</strong></p> 
<p>当你第一次进入云桌面实验环境时，平台会在后台服务器上为你创建云桌面环境，创建完毕后（这时候你在浏览器上可以看到云桌面了），平台会继续创建大数据集群。</p> 
<p>由于第一次创建大数据集群时，需要进行集群的初始化操作。因此，集群创建有一定的延迟（大约30秒）。</p> 
<p>注意：你需要等待集群成功初始化后，才可以在云桌面实验环境中进行大数据实验的相关操作。</p> 
<p>你可以通过点击实验手册上栏右侧的<code>更多--&gt;容器启动日志</code>来查看集群是否初始化成功，如下图所示：</p> 
<p></p> 
<p class="img-center"><img alt="" height="702" src="https://images2.imgbox.com/86/3a/qKCfwO6V_o.png" width="1118"></p> 
<p>创建大数据集群时产生的日志在“=== docker create ===”分割线下方，如下图所示：</p> 
<p></p> 
<p class="img-center"><img alt="" height="1200" src="https://images2.imgbox.com/1f/31/VgEPsdGg_o.png" width="1200"></p> 
<p>当你在集群创建日志中（需要下拉浏览器滚动条）能看到如下图所示的提示时，就说明集群已经创建成功了。</p> 
<p></p> 
<p class="img-center"><img alt="" height="1004" src="https://images2.imgbox.com/43/0e/UfZQQxpf_o.png" width="1200"></p> 
<p>如果未看到该信息，你可以通过再次点击<code>更多--&gt;容器启动日志</code>的方式刷新日志信息，直到确认集群成功创建后，才可以继续做大数据实验。</p> 
<p><strong>2. 大数据集群的停止</strong></p> 
<p>当你退出云桌面实验环境页面（关闭页面或者跳转到其他页面）超过30分钟时，平台会自动关闭云桌面实验环境。</p> 
<p>同时，平台也会同步地关闭你的大数据集群。</p> 
<p><strong>3. 大数据集群的启动</strong></p> 
<p>当你在云桌面实验环境处于关闭状态打开云桌面实验环境时，平台会自动启动你的云桌面环境，同时，平台也会同步启动你的大数据集群。</p> 
<p>启动大数据集群时产生的日志在“=== docker start ===”分割线下方，如下图所示：</p> 
<p></p> 
<p class="img-center"><img alt="" height="737" src="https://images2.imgbox.com/17/69/WSCgqf0j_o.png" width="1200"></p> 
<p>当你在集群启动日志中能看到如下图所示的提示时，就说明集群已经启动成功了。</p> 
<p></p> 
<p class="img-center"><img alt="" height="897" src="https://images2.imgbox.com/a3/da/NWZ46gz8_o.png" width="1200"></p> 
<p>如果未看到该信息，你可以通过再次点击<code>更多--&gt;容器启动日志</code>的方式刷新日志信息，直到确认集群成功启动后，才可以继续做大数据实验。</p> 
<p><strong>4. 大数据集群的重启</strong></p> 
<p>当你点击实验手册上栏右侧的<code>更多--&gt;重新启动</code>重启云桌面实验环境时，实验平台也会自动重启大数据集群。</p> 
<p>实验平台会首先对大数据集群进行<code>停止</code>操作，然后再进行<code>启动</code>操作。</p> 
<p>因此，你可以通过查看集群的启动日志来确认集群是否启动成功。</p> 
<p><strong>5. 大数据集群的还原</strong></p> 
<p>当你点击实验手册上栏右侧的<code>更多--&gt;桌面还原</code>还原云桌面实验环境时，实验平台也会自动还原大数据集群。</p> 
<p>实验平台会首先对大数据集群进行<code>停止</code>操作，然后再进行<code>删除</code>操作，最后再进行<code>创建</code>操作。</p> 
<p>因此，你可以通过查看集群的创建日志来确认还原后的集群是否创建成功。</p> 
<p><strong>6. 访问大数据集群</strong></p> 
<p>实验平台自动实现了云桌面实验环境到大数据集群各节点的<code>ssh</code>无密码访问。因此，你打开终端后，可以直接用<code>ssh</code>命令登录各节点。</p> 
<p>在云桌面实验环境中，点击鼠标右键，选择<code>Open Terminal Here</code>打开<code>Linux</code>终端，如下图所示。</p> 
<p></p> 
<p class="img-center"><img alt="" height="1200" src="https://images2.imgbox.com/61/25/3ShFNAc3_o.png" width="1200"></p> 
<p>通过命令<code>ssh master</code>可无密码登录<code>master</code>节点（通过<code>exit</code>命令可退出登录），如下图所示：</p> 
<p></p> 
<p class="img-center"><img alt="" height="1200" src="https://images2.imgbox.com/6a/26/QDTNUQ1Z_o.png" width="1200"></p> 
<p>通过命令<code>ssh slave1</code>可无密码登录<code>slave1</code>节点，如下图所示。</p> 
<p></p> 
<p class="img-center"><img alt="" height="1200" src="https://images2.imgbox.com/39/7c/7Y7XyREF_o.png" width="1200"></p> 
<p>同理，通过命令<code>ssh slave2</code>可无密码登录<code>slave2</code>节点，通过命令<code>ssh slave3</code>可无密码登录<code>slave3</code>节点。</p> 
<p>可通过<code>cat /etc/hosts</code>查看当前集群的配置，下图所示的<code>hosts</code>文件说明当前的大数据集群是包含1个主节点和3个从节点的4节点集群。</p> 
<p></p> 
<p class="img-center"><img alt="" height="1200" src="https://images2.imgbox.com/b9/c1/2lmpFgot_o.png" width="1200"></p> 
<p>如果大数据集群是1主9从的10节点集群，那你用<code>cat /etc/hosts</code>命令应该可以看到一个<code>master</code>节点和9个<code>slave</code>节点（<code>slave1</code>-<code>slave9</code>）。</p> 
<p></p> 
<ul><li><a href="https://course.educg.net/exp/doexpDeskDocker.jsp?libCenter=false&amp;desktopParam=d3d3LmVkdWNnLm5ldDpOREk0WVRZNVkyUTNNR1V5TUROaE1EUTBaak00TUdKbFlqZzBNakV6TlRVOmRtNWpjR0Z6YzNkdmNtUTpZMjkxY25ObExtVmtkV05uTG01bGRBOg&amp;judgeParam=KZeGZDh56wDZDHY07xQFYquM47UWKVd0AXsPevmodOkKzCDrFRX084VjrF_2eOMPx1SNjd4ODWs&amp;assignID=12781&amp;guideID=121967&amp;procIndex=0#" rel="nofollow" title="实验步骤4">实验步骤4</a> <strong>【实验准备】</strong></li></ul> 
<hr> 
<h2 id="开启服务">开启服务</h2> 
<p>  【新版环境】集群中提供了大数据服务的启动脚本。如下所示：</p> 
<p></p> 
<p class="img-center"><img alt="" height="426" src="https://images2.imgbox.com/bd/77/VnQKw0qi_o.jpg" width="1024"></p> 
<p>  以<code>hadoop</code>服务为例，如下操作：</p> 
<pre><code>(master) &gt; bash /scripts/hadoop/start-hadoop.sh
</code></pre> 
<p>  验证如下：</p> 
<p></p> 
<p class="img-center"><img alt="" height="323" src="https://images2.imgbox.com/25/ed/U1i5asmG_o.jpg" width="1025"></p> 
<p></p> 
<p class="img-center"><img alt="" height="306" src="https://images2.imgbox.com/c5/39/fpKZiSWk_o.jpg" width="1020"></p> 
<h2 id="创建工作目录">创建工作目录</h2> 
<p>本实验的工作目录为<code>~/Desktop/workspace/hdfs_pro</code>，使用以下命令创建和初始化工作目录：</p> 
<pre><code>root@cg:~/Desktop# mkdir -p ~/Desktop/workspace/hdfs_pro
root@cg:~/Desktop# cd ~/Desktop/workspace/hdfs_pro
root@cg:~/Desktop/workspace/hdfs_pro# 
</code></pre> 
<p></p> 
<p class="img-center"><img alt="" height="76" src="https://images2.imgbox.com/3e/44/ULa3n6OU_o.png" width="803"></p> 
<hr> 
<h2 id="创建idea工程">创建IDEA工程</h2> 
<p>接下来创建IDEA工程：</p> 
<p>启动IDEA环境。</p> 
<p></p> 
<p class="img-center"><img alt="" height="740" src="https://images2.imgbox.com/74/92/FL0wdEUW_o.jpg" width="993"></p> 
<p></p> 
<p class="img-center"><img alt="" height="657" src="https://images2.imgbox.com/dc/4b/dYBC0Jkt_o.png" width="1177"></p> 
<p>在项目名称（Project Name）处填入<code>hdfs_pro</code>，将工程位置选择为本实验的工作目录，再点击<code>Finish</code>。</p> 
<p>如下图所示：</p> 
<p></p> 
<p class="img-center"><img alt="" height="727" src="https://images2.imgbox.com/b2/e4/c2Nj8go7_o.png" width="1000"></p> 
<p><strong>导入依赖</strong></p> 
<p>依次点击：<code>File -&gt; Project Structure...</code></p> 
<p></p> 
<p class="img-center"><img alt="" height="334" src="https://images2.imgbox.com/8e/af/YJBHZHGH_o.png" width="380"></p> 
<p>Modules -&gt; Dependencies, 点击+号添加 JARs or Directories...</p> 
<p></p> 
<p class="img-center"><img alt="" height="699" src="https://images2.imgbox.com/68/c3/cfU4o0pI_o.png" width="1092"></p> 
<p>依次添加以下文件：</p> 
<ul><li>/opt/module/hadoop-2.7.1/share/hadoop/common</li><li>/opt/module/hadoop-2.7.1/share/hadoop/common/lib</li><li>/opt/module/hadoop-2.7.1/share/hadoop/hdfs</li><li>/opt/module/hadoop-2.7.1/share/hadoop/httpfs</li><li>/opt/module/hadoop-2.7.1/share/hadoop/kms</li><li>/opt/module/hadoop-2.7.1/share/hadoop/mapreduce</li><li>/opt/module/hadoop-2.7.1/share/hadoop/tools</li><li>/opt/module/hadoop-2.7.1/share/hadoop/yarn</li></ul> 
<p></p> 
<p class="img-center"><img alt="" height="728" src="https://images2.imgbox.com/59/84/5aFllT0V_o.jpg" width="1200"></p> 
<p></p> 
<h2 id="一、创建目录">一、创建目录</h2> 
<h3 id="1、相关接口说明">1、相关接口说明</h3> 
<p>创建目录可以使用<code>FileSystem</code>的<code>mkdirs</code>方法，该方法的含义如下：</p> 
<ol><li> <p>函数原型：<code>public boolean mkdirs(Path f) throws IOException</code></p> </li><li> <p>函数功能：调用该方法，根据f指定的路径创建目录。目录的权限为默认权限。</p> </li><li> <p>参数说明：<code>f</code>，<code>Path</code>对象。表示要创建的目录的路径。</p> </li><li> <p>返回值：如果目录成功创建，返回<code>true</code>。</p> </li><li> <p>异常：如果遇到IO故障，抛出<code>IOException</code>异常。</p> </li></ol> 
<p>mkdirs还有一个带有目录权限参数的版本，其原型为：</p> 
<p><code>public abstract boolean mkdirs(Path f, FsPermission permission) throws IOException</code></p> 
<h3 id="2、完整实验代码">2、完整实验代码</h3> 
<p>将 <code>hdfs_pro</code> 项目下 <code>src/main/java</code> 目录新建名为 <code>CreateDir</code> 的类</p> 
<p>（选中<code>java</code>文件夹<code>-&gt;File-&gt;new-&gt;Java Class-&gt;</code>在<code>name</code>选项中填入 <code>CreateDir-&gt;Finish</code>）</p> 
<p>该实验的完整实验代码如下：</p> 
<pre><code>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import java.util.Scanner;
import java.net.URI;

public class CreateDir {
    public static void main(String[] args) {
        try {
            Scanner sc = new Scanner(System.in);
            String dirPath = '/'+sc.next();
            FileSystem fs = FileSystem.get(new URI("hdfs://master:9000"), new Configuration());
            Path hdfsPath = new Path(dirPath);
            if(fs.mkdirs(hdfsPath)){
                System.out.println("Directory "+ dirPath +" has been created successfully!");
            }
        }catch(Exception e) {
            e.printStackTrace();
        }
    }
}
</code></pre> 
<p>将该代码拷贝到<code>CreateDir.java</code>文件中。如下图所示：</p> 
<p></p> 
<p class="img-center"><img alt="" height="780" src="https://images2.imgbox.com/2b/2f/YqUl6uwa_o.png" width="1200"></p> 
<p>以上代码中主要调用<code>fs</code>的<code>mkdirs</code>方法来创建目录，如果目录创建成功，会输出相应提示信息。</p> 
<h3 id="3、运行结果分析">3、运行结果分析</h3> 
<p>在<code>CreateDir.java</code>上，点击右键，选择<code>Run</code> ，执行程序。</p> 
<p></p> 
<p class="img-center"><img alt="" height="606" src="https://images2.imgbox.com/e5/2c/HowmX4VX_o.png" width="651"></p> 
<p>输入目录名<code>newdir</code>。</p> 
<p>运行结果的截图如下：</p> 
<p></p> 
<p class="img-center"><img alt="" height="202" src="https://images2.imgbox.com/32/af/8DTtpke7_o.jpg" width="1072"></p> 
<p>如图所示，目录<code>newDir</code> 已经被成功创建。</p> 
<hr> 
<p></p> 
<h2 id="二、目录存在性判断">二、目录存在性判断</h2> 
<h3 id="1、相关接口说明-1">1、相关接口说明</h3> 
<p>判断文件是否存在需要使用<code>FileSystem</code>的<code>exists</code>方法，该方法的详细含义如下：</p> 
<p>方法名：<code>exists</code></p> 
<p>方法原型：<code>public boolean exists(Path f) throws IOException</code></p> 
<p>接口功能：检查某个路径所指的目录是否存在。</p> 
<p>接口说明：参数<code>f</code>的含义为源路径。如果目录存在，返回值为<code>true</code>。如果IO故障会抛出<code>IOException</code>异常。</p> 
<h3 id="2、完整实验代码-1">2、完整实验代码</h3> 
<p>将 <code>hdfs_pro</code> 项目下 <code>src/main/java</code> 目录新建名为 <code>DirExist</code> 的类</p> 
<p>（选中<code>java</code>文件夹<code>-&gt;File-&gt;new-&gt;Java Class-&gt;</code>在<code>name</code>选项中填入<code> DirExist-&gt;Finish</code>）</p> 
<p>该实验的完整代码如下所示：</p> 
<pre><code>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import java.util.Scanner;
import java.net.URI;

public class DirExist {
    public static void main(String[] args) {
        try {
            Scanner sc = new Scanner(System.in);
            String dirName = '/'+sc.next();
            FileSystem fs = FileSystem.get(new URI("hdfs://master:9000"), new Configuration());
            if(fs.exists(new Path(dirName ))) {
                System.out.println("Directory Exists!");
            } else {
                System.out.println("Directory not Exists!");
            }
        }catch(Exception e) {
            e.printStackTrace();
        }
    }
}
</code></pre> 
<h3 id="3、运行结果分析-1">3、运行结果分析</h3> 
<p>使用和上一节相同的方法运行该代码。</p> 
<p>输入正确目录名：<code>newDir</code></p> 
<p>结果如下：</p> 
<p></p> 
<p class="img-center"><img alt="" height="241" src="https://images2.imgbox.com/51/41/frckBqeh_o.jpg" width="1033"></p> 
<p>输入错误目录名：<code>Newdir</code></p> 
<p>结果如下：</p> 
<p></p> 
<p class="img-center"><img alt="" height="237" src="https://images2.imgbox.com/67/90/uZq1cdrl_o.jpg" width="1054"></p> 
<p>如图所示，程序判断无误。</p> 
<p></p> 
<hr> 
<p></p> 
<h2 id="三、列出目录中的内容">三、列出目录中的内容</h2> 
<h3 id="1、相关接口说明-2">1、相关接口说明</h3> 
<p>在HDFS文件系统上浏览某个目录中子文件和子目录时，需要使用<code>FileSystem</code>类提供的<code>listStatus</code>方法，该方法将返回该目录下所有子文件和子目录的详细信息，包括文件的长度、块大小、备份数、修改时间、所有者以及权限等信息，这些信息都被封装在<code>FileStatus</code>对象中。调用<code>listStatus</code>方法时需要提供目录的路径，<code>listStatus</code>方法的详细说明如下：</p> 
<p>函数原型：<code>public abstract FileStatus[] listStatus(Path f) throws FileNotFoundException, IOException</code></p> 
<p>函数功能：根据输入参数<code>f</code>所指定的目录，列出该目录下所有子文件/子目录的详细信息。注意，该接口不保证返回的文件/目录信息是有序的。</p> 
<p>函数参数：<code>f</code>，指定目录的路径。</p> 
<p>返回值：<code>f</code>所指定的目录下所有子文件/子目录的详细信息。</p> 
<p>异常：两种异常，<code>FileNotFoundException</code>和<code>IOException</code>。当所指定的目录不存在时，抛出<code>FileNotFoundException</code>异常。当遇到IO故障时，返回<code>IOException</code>异常。</p> 
<h3 id="2、完整实验代码-2">2、完整实验代码</h3> 
<p>将<code> hdfs_pro</code>项目下 <code>src/main/java</code>目录新建名为 <code>ListFiles </code>的类</p> 
<p>（选中<code>java</code>文件夹<code>-&gt;File-&gt;new-&gt;Java Class-&gt;</code>在<code>name</code>选项中填入 <code>ListFiles-&gt;Finish</code>）</p> 
<p>该实验的完整代码如下：</p> 
<pre><code>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.FileUtil;
import org.apache.hadoop.fs.Path;

import java.net.URI;
import java.util.Scanner;

public class ListFiles {
    public static void main(String[] args) {
        try {
            Scanner sc = new Scanner(System.in);
            String filePath = sc.next();
            FileSystem fs = FileSystem.get(new URI("hdfs://master:9000"), new Configuration());
            Path srcPath = new Path(filePath);
            FileStatus[] stats = fs.listStatus(srcPath);
            Path[] paths = FileUtil.stat2Paths(stats);
            for(Path p : paths)
                System.out.println(p.getName());
        }catch(Exception e) {
            e.printStackTrace();
        }
    }
}
</code></pre> 
<h3 id="3、运行结果分析-2">3、运行结果分析</h3> 
<p>使用和上节相同的方法运行代码。</p> 
<p>输入根目录：<code>/</code></p> 
<p>运行结果如下：</p> 
<p></p> 
<p class="img-center"><img alt="" height="227" src="https://images2.imgbox.com/ed/52/1X2dfy1v_o.jpg" width="1069"></p> 
<p>如图，程序成功列出了根目录下的所有文件。</p> 
<p></p> 
<hr> 
<p></p> 
<h2 id="四、删除目录">四、删除目录</h2> 
<h3 id="1、相关接口说明-3">1、相关接口说明</h3> 
<p>删除文件可以使用<code>FileSystem</code>的<code>delete</code>接口，该接口的含义如下：</p> 
<ol><li> <p>函数原型：<code>public abstract boolean delete(Path f,boolean recursive) throws IOException</code></p> </li><li> <p>函数功能：删除文件或者目录。</p> </li><li> <p>参数说明：<code>f</code>，要删除的文件或者目录的路径。<code>recursive</code>，是否需要递归删除。如果是删除目录的话，将该参数设置为<code>true</code>。否则，设置为<code>false</code>.</p> </li><li> <p>返回值：如果成功删除，则返回<code>true</code>。否则，返回<code>false</code>。</p> </li><li> <p>如果遇到IO故障，会抛出<code>IOException</code>。</p> </li></ol> 
<h3 id="2、完整实验代码-3">2、完整实验代码</h3> 
<p>将<code> hdfs_pro</code>项目下 <code>src/main/java</code>目录新建名为 <code>DeleteDir</code>的类</p> 
<p>（选中<code>java</code>文件夹<code>-&gt;File-&gt;new-&gt; Java Class-&gt;</code>在<code>name</code>选项中填入<code> DeleteDir-&gt;Finish</code>）</p> 
<p>该实验的完整代码如下所示：</p> 
<pre><code>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

import java.net.URI;
import java.util.Scanner;

public class DeleteDir {
    public static void main(String[] args) {
        try {
            Scanner sc = new Scanner(System.in);
            String dirPath = '/'+sc.next();
            FileSystem fs = FileSystem.get(new URI("hdfs://master:9000"), new Configuration());
            Path hdfsPath = new Path(dirPath);
            if(fs.delete(hdfsPath,true)){
                System.out.println("Directory "+ dirPath +" has been deleted successfully!");
            }
        }catch(Exception e) {
            e.printStackTrace();
        }
    }
}
</code></pre> 
<h3 id="3、运行结果分析-3">3、运行结果分析</h3> 
<p>使用和上节相同的方法运行代码。</p> 
<p>输入目录名：<code>newDir</code></p> 
<p>运行结果如下：</p> 
<p></p> 
<p class="img-center"><img alt="" height="222" src="https://images2.imgbox.com/7f/6d/7iWsXCR1_o.jpg" width="1048"></p> 
<p>可以看到，之前创建的 <code>newDir</code> 目录被成功删除。</p> 
<p>可以通过上一节中的<code>ListFiles</code>类进行验证。</p> 
<ul><li><a href="https://course.educg.net/exp/doexpDeskDocker.jsp?libCenter=false&amp;desktopParam=d3d3LmVkdWNnLm5ldDpOREk0WVRZNVkyUTNNR1V5TUROaE1EUTBaak00TUdKbFlqZzBNakV6TlRVOmRtNWpjR0Z6YzNkdmNtUTpZMjkxY25ObExtVmtkV05uTG01bGRBOg&amp;judgeParam=KZeGZDh56wDZDHY07xQFYquM47UWKVd0AXsPevmodOkKzCDrFRX084VjrF_2eOMPx1SNjd4ODWs&amp;assignID=12781&amp;guideID=121967&amp;procIndex=0#" rel="nofollow" title="实验步骤6">实验步骤6</a> <strong>【实验步骤】文件的相关操作</strong></li></ul> 
<hr> 
<h2 id="一、创建文件">一、创建文件</h2> 
<h3>1、相关接口说明</h3> 
<p>使用<code>FileSystem</code>的<code>create</code>函数可以创建文件，根据参数的不同，<code>create</code>函数有以下几种重载类型：</p> 
<pre><code>public FSDataOutputStream create(Path f) throws IOException

public FSDataOutputStream create(Path f,boolean overwrite) throws IOException

public FSDataOutputStream create(Path f,Progressable progress) throws IOException

public FSDataOutputStream create(Path f,short replication) throws IOException

public FSDataOutputStream create(Path f,short replication,Progressable progress) throws IOException

public FSDataOutputStream create(Path f,boolean overwrite,int bufferSize) throws IOException

public FSDataOutputStream create(Path f,boolean overwrite,int bufferSize,Progressable progress) throws IOException

public FSDataOutputStream create(Path f,boolean overwrite,int bufferSize,short replication,long blockSize) throws IOException

public FSDataOutputStream create(Path f,boolean overwrite,int bufferSize,short replication,long blockSize,Progressable progress)throws IOException

public abstract FSDataOutputStream create(Path f,FsPermission permission,boolean overwrite,int bufferSize,short replication,long blockSize,Progressable progress) throws IOException

public FSDataOutputStream create(Path f,FsPermission permission,EnumSet&lt;CreateFlag&gt; flags,int bufferSize,short replication,long blockSize,Progressable progress) throws IOException

public FSDataOutputStream create(Path f,FsPermission permission,EnumSet&lt;CreateFlag&gt; flags,int bufferSize, short replication,long blockSize,Progressable progress, org.apache.hadoop.fs.Options.ChecksumOpt checksumOpt) throws IOException
</code></pre> 
<p>上述接口中，各参数的含义分别如下：</p> 
<p><code>f</code>，要打开的文件名，默认会覆盖已经存在的文件。</p> 
<p><code>overwrite</code>，如果要创建的文件已经存在，是否覆盖。设置为<code>true</code>时，覆盖；为<code>false</code>时，不覆盖。</p> 
<p><code>progress</code>，用于汇报进度信息。</p> 
<p><code>replication</code>，设置文件块的副本数量。</p> 
<p><code>bufferSize</code>，所使用的缓冲区的大小。</p> 
<p><code>blockSize</code>，块大小。</p> 
<p><code>permission</code>，设置文件的权限。</p> 
<p><code>flags</code>，指定文件创建标志，文件创建标志包括：<code>CREATE</code>，<code>APPEND</code>，<code>OVERWRITE</code> ，<code>SYNC_BLOCK </code>，<code>LAZY_PERSIST</code> ，<code>APPEND_NEWBLOCK</code>等。</p> 
<h3>2、完整实验代码</h3> 
<p>将<code>hdfs_pro</code>项目下<code>src/main/java</code>目录新建名为<code> CreateFile</code> 的类</p> 
<p>（选中<code>java</code>文件夹<code>-&gt;File-&gt;new-&gt;Java Class-&gt;</code>在<code>name</code>选项中填入<code>CreateFile-&gt;Finish</code>）</p> 
<p>此次实验的完整代码如下所示：</p> 
<pre><code>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

import java.net.URI;
import java.util.Scanner;

public class CreateFile {
    public static void main(String[] args) {
        try {
            Scanner sc = new Scanner(System.in);
            String filePath = '/'+sc.next();
            FileSystem fs = FileSystem.get(new URI("hdfs://master:9000"), new Configuration());
            Path hdfsPath = new Path(filePath);
            fs.create(hdfsPath);
        }catch(Exception e) {
            e.printStackTrace();
        }
    }
}
</code></pre> 
<h3>3、运行结果分析</h3> 
<p>使用和上节相同的方法运行代码。</p> 
<p>输入文件名：<code>newfile.txt</code></p> 
<p>运行结果如下：</p> 
<p></p> 
<p class="img-center"><img alt="" height="209" src="https://images2.imgbox.com/22/02/Huk30q82_o.jpg" width="1064"></p> 
<p>在云桌面终端，输入命令验证</p> 
<pre><code>root@cg:~/Desktop/workspace/hdfs_pro# hadoop fs -ls /
</code></pre> 
<p></p> 
<p class="img-center"><img alt="" height="138" src="https://images2.imgbox.com/e1/07/0QpJz1vJ_o.jpg" width="1007"></p> 
<p>如图所示，文件已经成功被创建。</p> 
<p></p> 
<hr> 
<p></p> 
<h2 id="二、文件存在性判断">二、文件存在性判断</h2> 
<h3>1、相关接口说明</h3> 
<p>判断文件是否存在需要使用<code>FileSystem</code>的<code>exists</code>方法，该方法的详细含义如下：</p> 
<p>方法名：<code>exists</code></p> 
<p>方法原型：<code>public boolean exists(Path f) throws IOException</code></p> 
<p>接口功能：检查某个路径所指的文件是否存在。</p> 
<p>接口说明：参数<code>f</code>的含义为源路径。如果文件存在，返回值为<code>true</code>。如果IO故障会抛出<code>IOException</code>异常。</p> 
<h3>2、完整实验代码</h3> 
<p>将<code>hdfs_pro</code>项目下<code>src/main/java</code>目录新建名为<code>FileExist</code> 的类</p> 
<p>（选中<code>java</code>文件夹<code>-&gt;File-&gt;new-&gt;Java Class-&gt;</code>在<code>name</code>选项中填入<code>FileExist-&gt;Finish</code>）</p> 
<p>该实验的完整代码如下：</p> 
<pre><code>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

import java.net.URI;
import java.util.Scanner;

public class FileExist {
    public static void main(String[] args) {
        try {
            Scanner sc = new Scanner(System.in);
            String fileName = '/'+sc.next();
            FileSystem fs = FileSystem.get(new URI("hdfs://master:9000"), new Configuration());

            if(fs.exists(new Path(fileName))) {
                System.out.println("File Exists!");
            } else {
                System.out.println("File not Exists!");
            }
        }catch(Exception e) {
            e.printStackTrace();
        }
    }
}
</code></pre> 
<h3>3、运行结果分析</h3> 
<p>点击<code>Run</code>运行代码。</p> 
<p>输入文件名：<code>newfile.txt</code></p> 
<p>运行结果如下：</p> 
<p></p> 
<p class="img-center"><img alt="" height="254" src="https://images2.imgbox.com/77/de/U4mJk3Af_o.jpg" width="1052"></p> 
<p>可以发现<code>newfile.txt</code>存在。</p> 
<p></p> 
<hr> 
<p></p> 
<h2 id="三、文件写">三、文件写</h2> 
<h3>1、相关接口说明</h3> 
<p>HDFS不支持文件的随机写，写文件的方式有两种：1）文件不存在，创建文件之后，开始对文件的内容进行写入。2）文件存在，打开文件，在文件尾部追加写。</p> 
<p>对于第一种方式，由于调用<code>create</code>方法后会返回<code>FSDataOutputStream</code>对象，使用该对象对文件进行写操作。第二种方式，使用<code>FileSystem</code>类的<code>append</code>接口，该接口也会返回<code>FSDataOutputStream</code>对象，同样使用该对象可对文件进行追加操作。</p> 
<p><code>create</code>方法在文件创建实验中已经进行了详细说明，这里对<code>FSDataOutputStream</code>的相关常用方法进行说明，<code>FSDataOutputStream</code>有三个常用的方法，分别为<code>write</code>，<code>flush</code>，<code>close</code>函数。<code>write</code>将数据写入到文件中，<code>flush</code>将数据缓存在内存中的数据更新到磁盘，<code>close</code>则关闭流对象。</p> 
<p><code>FileSystem</code>的<code>append</code>函数详细说明如下：</p> 
<p>函数原型：<code>public FSDataOutputStream append(Path f) throws IOException</code></p> 
<p>函数功能：在一个已经存在的文件尾部追加数据。</p> 
<p>函数参数：<code>f</code>，文件路径。</p> 
<p>返回值：<code>FSDataOutputStream</code>对象。</p> 
<p>异常：遇到IO故障时，抛出<code>IOException</code>异常。</p> 
<h3>2、完整实验代码</h3> 
<p>将<code>hdfs_pro</code>项目下<code>src/main/java</code>目录新建名为<code> WriteFile</code> 的类</p> 
<p>（选中<code>java</code>文件夹<code>-&gt;File-&gt;new-&gt;Java Class-&gt;</code>在<code>name</code>选项中填入<code>WriteFile-&gt;Finish</code>）</p> 
<p>该实验的完整代码如下所示：</p> 
<pre><code>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

import java.net.URI;
import java.util.Scanner;

public class WriteFile {
    public static void main(String[] args) {
        try {
            Scanner sc = new Scanner(System.in);
            String filePath = '/'+sc.next();
            FileSystem fs = FileSystem.get(new URI("hdfs://master:9000"), new Configuration());

            Path srcPath = new Path(filePath);
            FSDataOutputStream os = fs.create(srcPath,true,1024,(short)1,(long)(1&lt;&lt;26));
            String str = "Hello, this is a sentence that should be written into the file.\n";
            os.write(str.getBytes());
            os.flush();
            os.close();

            os = fs.append(srcPath);
            str = "Hello, this is another sentence that should be written into the file.\n";
            os.write(str.getBytes());
            os.flush();
            os.close();
        }catch(Exception e) {
            e.printStackTrace();
        }
    }
}
</code></pre> 
<p>该代码中，文件的写入分为两部分，第一部分使用<code>create</code>返回的<code>FSDataOutputStream</code>对象进行写入，第二部分使用<code>append</code>返回的<code>FSDataOutputStream</code>对象进行写入。</p> 
<h3>3、运行结果分析</h3> 
<p>点击<code>Run</code>运行代码。</p> 
<p>输入文件名：<code>newfile.txt</code></p> 
<p>运行结果如下：</p> 
<p></p> 
<p class="img-center"><img alt="" height="228" src="https://images2.imgbox.com/2e/9a/QW0qsUuQ_o.jpg" width="1045"></p> 
<p>在云桌面终端，输入命令验证</p> 
<pre><code>root@cg:~/Desktop/workspace/hdfs_pro# hadoop fs -cat /newfile.txt
</code></pre> 
<p></p> 
<p class="img-center"><img alt="" height="109" src="https://images2.imgbox.com/59/6c/IxIN1Bni_o.jpg" width="934"></p> 
<p>如图所示，文件成功写入。</p> 
<p></p> 
<hr> 
<p></p> 
<h2 id="四、文件读">四、文件读</h2> 
<h3>1、相关接口说明</h3> 
<p>如果要读取<code>HDFS</code>上的文件，可以使用<code>open</code>方法。<code>open</code>方法会返回一个<code>FSDataInputStream</code>对象，使用该对象可对文件进行读操作。<code>open</code>函数详细说明如下：</p> 
<p>函数原型：<code>public FSDataInputStream open(Path f) throws IOException</code></p> 
<p>函数功能：打开<code>Path</code>对象<code>f</code>指定的路径的文件。</p> 
<p>参数说明：<code>f</code>，要打开的文件。</p> 
<p>返回值：<code>FSDataInputStream</code>对象，利用<code>FSDataInputStream</code>对象可对文件进行读操作。</p> 
<p>异常：遇到IO故障时，将抛出<code>IOException</code>。</p> 
<p><code>open</code>方法还有一个带有<code>bufferSize</code>参数的重载版本，该方法的原型为：</p> 
<pre><code>public abstract FSDataInputStream open(Path f,int bufferSize) throws IOException
</code></pre> 
<p>其中，<code>bufferSize</code>的含义为读取过程中所使用的缓冲区的大小。</p> 
<h3>2、完整实验代码</h3> 
<p>将<code>hdfs_pro</code>项目下<code>src/main/java</code>目录新建名为<code>ReadFile</code> 的类</p> 
<p>（选中<code>java</code>文件夹<code>-&gt;File-&gt;new-&gt;Java Class-&gt;</code>在<code>name</code>选项中填入<code>ReadFile-&gt;Finish</code>）</p> 
<p>该实验的完整代码如下：</p> 
<pre><code>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

import java.net.URI;
import java.util.Scanner;

public class ReadFile {
    public static void main(String[] args) {
        try {
            Scanner sc = new Scanner(System.in);
            String filePath = '/'+sc.next();
            FileSystem fs = FileSystem.get(new URI("hdfs://master:9000"), new Configuration());
            
            Path srcPath = new Path(filePath);

            FSDataInputStream is = fs.open(srcPath);
            while(true) {
                String line = is.readLine();
                if(line == null) {
                    break;
                }
                System.out.println(line);
            }
            is.close();
        }catch(Exception e) {
            e.printStackTrace();
        }
    }
}
</code></pre> 
<h3>3、运行结果分析</h3> 
<p>点击<code>Run</code>运行代码。</p> 
<p>输入文件名：<code>newfile.txt</code></p> 
<p>运行结果如下：</p> 
<p></p> 
<p class="img-center"><img alt="" height="255" src="https://images2.imgbox.com/99/f9/N21arT8n_o.jpg" width="1043"></p> 
<p>如图，文件中的内容成功被读取。</p> 
<p></p> 
<hr> 
<p></p> 
<h2 id="五、文件重命名">五、文件重命名</h2> 
<h3 id="1、相关接口说明-4">1、相关接口说明</h3> 
<p>文件重命名可以使用<code>FileSystem</code>的<code>rename</code>方法，该方法的详细说明如下：</p> 
<p>函数原型：<code>public abstract boolean rename(Path src,Path dst)throws IOException</code></p> 
<p>函数功能：将路径<code>src</code>重命名为路径<code>dst</code>。</p> 
<p>参数：<code>src</code>，将被重命名的路径。<code>dst</code>，重命名后的路径。</p> 
<p>返回值：如果重命名成功，返回<code>true</code>；否则，返回<code>false</code>；</p> 
<p>异常：如果遇到IO故障，抛出<code>IOException</code>异常。</p> 
<h3 id="2、完整实验代码-4">2、完整实验代码</h3> 
<p>将<code>hdfs_pro</code>项目下<code>src/main/java</code>目录新建名为<code>Rename</code> 的类</p> 
<p>（选中<code>java</code>文件夹<code>-&gt;File-&gt;new-&gt;Java Class-&gt;</code>在<code>name</code>选项中填入<code>Rename-&gt;Finish</code>）</p> 
<p>该实验的完整代码如下：</p> 
<pre><code>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

import java.net.URI;
import java.util.Scanner;

public class Rename {
    public static void main(String[] args) {
        try {
            Scanner sc = new Scanner(System.in);
            String srcStrPath = '/'+sc.next();
            String dstStrPath = '/'+sc.next();
            FileSystem fs = FileSystem.get(new URI("hdfs://master:9000"), new Configuration());
            Path srcPath = new Path(srcStrPath);
            Path dstPath = new Path(dstStrPath);
            if(fs.rename(srcPath,dstPath)) {
                System.out.println("rename from " + srcStrPath + " to " + dstStrPath + "successfully!");
            }
        }catch(Exception e) {
            e.printStackTrace();
        }
    }
}
</code></pre> 
<h3 id="3、运行结果分析-4">3、运行结果分析</h3> 
<p>点击<code>Run</code>运行代码。</p> 
<p>输入文件名与新的文件名：</p> 
<pre><code>newfile.txt
file.txt
</code></pre> 
<p>运行结果如下：</p> 
<p></p> 
<p class="img-center"><img alt="" height="247" src="https://images2.imgbox.com/8d/78/a2tYdndQ_o.jpg" width="1051"></p> 
<p>在云桌面终端，输入命令验证</p> 
<pre><code>root@cg:~/Desktop/workspace/hdfs_pro# hadoop fs -ls /
</code></pre> 
<p></p> 
<p class="img-center"><img alt="" height="126" src="https://images2.imgbox.com/c8/0c/SqZERDd6_o.jpg" width="943"></p> 
<p>如图可以发现文件已经重命名。</p> 
<p></p> 
<hr> 
<p></p> 
<h2 id="六、文件删除">六、文件删除</h2> 
<h3 id="1、相关接口说明-5">1、相关接口说明</h3> 
<p>删除文件可以使用<code>FileSystem</code>的<code>delete</code>接口，该接口的含义如下：</p> 
<ol><li> <p>函数原型：<code>public abstract boolean delete(Path f,boolean recursive) throws IOException</code></p> </li><li> <p>函数功能：删除文件或者目录。</p> </li><li> <p>参数说明：<code>f</code>，要删除的文件或者目录的路径。<code>recursive</code>，是否需要递归删除。如果是删除目录的话，将该参数设置为<code>true</code>。否则，设置为<code>false</code>.</p> </li><li> <p>返回值：如果成功删除，则返回<code>true</code>。否则，返回<code>false</code>。</p> </li><li> <p>如果遇到IO故障，会抛出<code>IOException</code>。</p> </li></ol> 
<h3 id="2、完整实验代码-5">2、完整实验代码</h3> 
<p>将<code>hdfs_pro</code>项目下<code>src/main/java</code>目录新建名为<code>DeleteFile</code> 的类</p> 
<p>（选中<code>java</code>文件夹<code>-&gt;File-&gt;new-&gt;Java Class-&gt;</code>在<code>name</code>选项中填入<code>DeleteFile-&gt;Finish</code>）</p> 
<p>该实验的完整代码如下：</p> 
<pre><code>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

import java.net.URI;
import java.util.Scanner;

public class DeleteFile {
    public static void main(String[] args) {
        try {
            Scanner sc = new Scanner(System.in);
            String filePath = '/'+sc.next();
            FileSystem fs = FileSystem.get(new URI("hdfs://master:9000"), new Configuration());
            
            Path hdfsPath = new Path(filePath);
            if(fs.delete(hdfsPath,false)){
                System.out.println("File "+ filePath +" has been deleted successfully!");
            }
        }catch(Exception e) {
            e.printStackTrace();
        }
    }
}
</code></pre> 
<h3 id="3、运行结果分析-5">3、运行结果分析</h3> 
<p>点击<code>Run</code>运行代码。</p> 
<p>输入文件名：<code>file.txt</code></p> 
<p>运行结果如下：</p> 
<p></p> 
<p class="img-center"><img alt="" height="257" src="https://images2.imgbox.com/13/d9/IzszRuYT_o.jpg" width="1050"></p> 
<p>在云桌面终端，输入命令验证</p> 
<pre><code>root@cg:~/Desktop/workspace/hdfs_pro# hadoop fs -ls /
</code></pre> 
<p></p> 
<p class="img-center"><img alt="" height="117" src="https://images2.imgbox.com/6a/68/0XhEvGba_o.jpg" width="1000"></p> 
<p><code>file.txt</code> 已经被删除。</p> 
<p></p> 
<ul><li><a href="https://course.educg.net/exp/doexpDeskDocker.jsp?libCenter=false&amp;desktopParam=d3d3LmVkdWNnLm5ldDpOREk0WVRZNVkyUTNNR1V5TUROaE1EUTBaak00TUdKbFlqZzBNakV6TlRVOmRtNWpjR0Z6YzNkdmNtUTpZMjkxY25ObExtVmtkV05uTG01bGRBOg&amp;judgeParam=KZeGZDh56wDZDHY07xQFYquM47UWKVd0AXsPevmodOkKzCDrFRX084VjrF_2eOMPx1SNjd4ODWs&amp;assignID=12781&amp;guideID=121967&amp;procIndex=0#" rel="nofollow" title="实验步骤7">实验步骤7</a> <strong>【实验作业1】自己动手实现HDFS Shell</strong></li></ul> 
<hr> 
<p>基于已经学习到的Hadoop API编程知识，自己动手实现一个简单的HDFS Shell程序，程序名称为<strong>HShell</strong>，要求能够支持以下功能：</p> 
<p>1.使用<code>HShell -cp 本地路径 HDFS路径</code>，将文件从Linux本地文件系统拷贝到HDFS指定路径上。</p> 
<p>2.使用<code>HShell -rm 路径</code>删除文件</p> 
<p>3.使用<code>HShell -rm -r 路径</code>删除目录</p> 
<p>4.使用<code>HShell -cp -r 本地目录路径 HDFS路径</code>，将目录从Linux本地拷贝到HDFS指定路径上。</p> 
<p>5.使用<code>HShell -list 路径</code>显示某个文件的信息或者某个目录的信息</p> 
<p>6.使用<code>HShell -mv 路径 路径</code>移动文件或者重命名文件</p> 
<p>7.使用<code>HShell -find 文件名 目录</code>实现在目录下递归查找某个文件名的文件</p> 
<p><img alt="" height="603" src="https://images2.imgbox.com/d6/af/j1n37oBc_o.png" width="1052"><img alt="" height="751" src="https://images2.imgbox.com/5c/e2/I186eRbr_o.png" width="1198"><img alt="" height="685" src="https://images2.imgbox.com/95/0d/pP1Fcftj_o.png" width="1074"><img alt="" height="787" src="https://images2.imgbox.com/da/f7/A8GYfe5x_o.png" width="1200"><img alt="" height="711" src="https://images2.imgbox.com/b7/ee/cJ2e16Xi_o.png" width="1197"><img alt="" height="173" src="https://images2.imgbox.com/34/3f/obtoDw2N_o.png" width="1120"><img alt="" height="702" src="https://images2.imgbox.com/3b/65/6pezwxT3_o.png" width="1172"><img alt="" height="225" src="https://images2.imgbox.com/af/96/GasauWUw_o.png" width="1167"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/1acb210b7c8ebaf4920a324292df4894/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">VMware安装Ubuntu22.04教程</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/5f83c180dd5eec3e0f2011382ba76c11/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">CSDN付费专栏，写文章变现的机会来了！</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>