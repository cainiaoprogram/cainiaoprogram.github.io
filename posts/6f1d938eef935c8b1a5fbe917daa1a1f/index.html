<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Pytorch笔记 之 torch.nn 模块简介 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Pytorch笔记 之 torch.nn 模块简介" />
<meta property="og:description" content="文章目录 torch.nnbeforenn.functionalnn.Module &amp; nn.Parameternn.Lineartorch.optimDataLoaderAdd Validationnn.SequentialUsing GPU torch.nn import torch.nn as nn
参考翻译 What is torch.nn really?
主要是对 PyTorch 框架的各模块进行简要介绍
一定程度上是 PyTorch 的入门笔记
假设已经对神经网络相关基础知识有了一定了解
（或实现过机器学习梯度下降相关代码）
before PyTorch 使用 torch.tensor，需要将数据进行转换
import torch x_train, y_train, x_valid, y_valid = map( torch.tensor, (x_train, y_train, x_valid, y_valid) ) x_train.shape x_train.min() x_train.max() map(function, iterable, …)
return iterable
nn.functional import torch.nn.functional as F
包含 torch.nn 库中所有函数
同时包含大量 loss 和 activation function
import torch.nn.functional as F loss_func = F." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/6f1d938eef935c8b1a5fbe917daa1a1f/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-09-04T20:36:37+08:00" />
<meta property="article:modified_time" content="2019-09-04T20:36:37+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Pytorch笔记 之 torch.nn 模块简介</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><ul><li><a href="#torchnn_1" rel="nofollow">torch.nn</a></li><li><ul><li><a href="#before_10" rel="nofollow">before</a></li><li><a href="#nnfunctional_27" rel="nofollow">nn.functional</a></li><li><a href="#nnModule__nnParameter_138" rel="nofollow">nn.Module &amp; nn.Parameter</a></li><li><a href="#nnLinear_144" rel="nofollow">nn.Linear</a></li><li><a href="#torchoptim_146" rel="nofollow">torch.optim</a></li><li><a href="#DataLoader_156" rel="nofollow">DataLoader</a></li><li><a href="#Add_Validation_187" rel="nofollow">Add Validation</a></li><li><a href="#nnSequential_255" rel="nofollow">nn.Sequential</a></li><li><a href="#Using_GPU_286" rel="nofollow">Using GPU</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h3><a id="torchnn_1"></a>torch.nn</h3> 
<p><code>import torch.nn as nn</code></p> 
<blockquote> 
 <p>参考翻译 <a href="https://pytorch.org/tutorials/beginner/nn_tutorial.html" rel="nofollow">What is torch.nn really?</a><br> 主要是对 PyTorch 框架的各模块进行简要介绍<br> 一定程度上是 PyTorch 的入门笔记<br> 假设已经对神经网络相关基础知识有了一定了解<br> （或实现过机器学习梯度下降相关代码）</p> 
</blockquote> 
<h4><a id="before_10"></a>before</h4> 
<p>PyTorch 使用 <code>torch.tensor</code>，需要将数据进行转换</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch

x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> x_valid<span class="token punctuation">,</span> y_valid <span class="token operator">=</span> <span class="token builtin">map</span><span class="token punctuation">(</span>
    torch<span class="token punctuation">.</span>tensor<span class="token punctuation">,</span>
    <span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> x_valid<span class="token punctuation">,</span> y_valid<span class="token punctuation">)</span>
<span class="token punctuation">)</span>
x_train<span class="token punctuation">.</span>shape
x_train<span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
x_train<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<blockquote> 
 <p>map(function, iterable, …)<br> return iterable</p> 
</blockquote> 
<h4><a id="nnfunctional_27"></a>nn.functional</h4> 
<p><code>import torch.nn.functional as F</code><br> 包含 <code>torch.nn</code> 库中所有函数<br> 同时包含大量 loss 和 activation function</p> 
<pre><code>import torch.nn.functional as F

loss_func = F.cross_entropy
loss = loss_func(model(x), y)

loss.backward()
</code></pre> 
<p>其中 <code>loss.backward()</code> 更新模型的梯度，包括 weights 和 bias</p> 
<p><a href="https://www.zhihu.com/question/66782101" rel="nofollow">PyTorch 中，nn 与 nn.functional 有什么区别？</a></p> 
<blockquote> 
 <p><code>nn.functional.xxx</code> 是函数接口，<code>nn.Xxx</code> 是 <code>.nn.functional.xxx</code> 的类封装，并且<code>nn.Xxx</code> 都继承于一个共同祖先 <code>nn.Module</code><br> <code>nn.Xxx</code> 除了具有 <code>nn.functional.xxx</code> 功能之外，内部附带 <code>nn.Module</code> 相关的属性和方法，eg. <code>train()</code>, <code>eval()</code>, <code>load_state_dict</code>, <code>state_dict</code></p> 
</blockquote> 
<ul><li>两者的调用方式不同</li></ul> 
<p><code>nn.Xxx</code> ，实例化 -&gt; 函数调用 -&gt; 传入数据</p> 
<pre><code class="prism language-python">inputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span>
conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
out <span class="token operator">=</span> conv<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
</code></pre> 
<p><code>nn.functional.xxx</code> 传入数据 <strong>和 weight、bias 等其他参数</strong></p> 
<pre><code class="prism language-python">weight <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
bias <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span>
out <span class="token operator">=</span> nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> weight<span class="token punctuation">,</span> bias<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre> 
<ul><li>能否和 <code>nn.Sequential</code> 结合使用</li></ul> 
<p><code>nn.Xxx</code> 继承于 <code>nn.Module</code>，能够很好的与 <code>nn.Sequential</code> 结合使用</p> 
<pre><code class="prism language-python">fm_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>num_features<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Droput<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span>
</code></pre> 
<p>而 <code>nn.functional.xxx</code> 无法与 <code>nn.Sequential</code> 结合使用</p> 
<ul><li>是否需要自己定义和管理 weight 和 bias 等参数<br> <code>nn.Xxx</code> 不需要自己定义和管理weight</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">CNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>CNN<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>cnn1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>relu1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>maxpool1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>cnn2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>relu2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>maxpool2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>linear1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4</span> <span class="token operator">*</span> <span class="token number">4</span> <span class="token operator">*</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>maxpool1<span class="token punctuation">(</span>self<span class="token punctuation">.</span>relu1<span class="token punctuation">(</span>self<span class="token punctuation">.</span>cnn1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>maxpool2<span class="token punctuation">(</span>self<span class="token punctuation">.</span>relu2<span class="token punctuation">(</span>self<span class="token punctuation">.</span>cnn2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>linear1<span class="token punctuation">(</span>out<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> out
</code></pre> 
<p><code>nn.functional.xxx</code> 需要自己定义 weight，每次调用的时候都需要手动传入 weight，不利于代码复用</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">CNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""docstring for CNN"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>CNN<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>cnn1_weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bias1_weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>cnn2_weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bias2_weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>linear1_weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span> <span class="token operator">*</span> <span class="token number">4</span> <span class="token operator">*</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bias3_weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        out <span class="token operator">=</span> F<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>cnn1_weight<span class="token punctuation">,</span> self<span class="token punctuation">.</span>bias1_weight<span class="token punctuation">)</span>
        out <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        out <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        out <span class="token operator">=</span> F<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>out<span class="token punctuation">,</span> self<span class="token punctuation">.</span>cnn2_weight<span class="token punctuation">,</span> self<span class="token punctuation">.</span>bias2_weight<span class="token punctuation">)</span>
        out <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        out <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        out <span class="token operator">=</span> F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>out<span class="token punctuation">,</span> self<span class="token punctuation">.</span>linear1_weight<span class="token punctuation">,</span> self<span class="token punctuation">.</span>bias3_weight<span class="token punctuation">)</span>

</code></pre> 
<blockquote> 
 <p>上述两中定义方式得到的 CNN 功能都是相同的<br> PyTorch 官方推荐：</p> 
 <ol><li>具有学习参数的(eg. conv2d, linear, batch_norm) 采用 <code>nn.Xxx</code></li><li>没有学习参数的(eg. maxpool, loss_func, activation func) 等根据个人选择使用 <code>nn.functional.xxx</code> 或 <code>nn.Xxx</code></li><li>最后，关于 dropout，强烈推荐使用 <code>nn.Xxx</code> 方式，因为一般情况下只有训练阶段才进行 dropout，在 eval 阶段不会进行 dropout。使用<code>nn.Xxx</code> 方法定义 dropout，在调用 <code>model.eval()</code> 之后，model 中所有的 dropout layer 都关闭，但以 <code>nn.functional.dropout</code> 方式定义 dropout，在调用 <code>model.eval()</code> 之后并不能关闭 dropout。需要使用 <code>F.dropout(x, trainig=self.training</code>。</li></ol> 
</blockquote> 
<h4><a id="nnModule__nnParameter_138"></a>nn.Module &amp; nn.Parameter</h4> 
<p>继承 <code>nn.Module</code>，构造一个保存 weights，bias 和具有前向传播方法(forward step)的类<br> <code>nn.Module</code> 有大量属性和方法(eg. <code>.parameters()</code> 和 <code>.zero_grad()</code>)</p> 
<h4><a id="nnLinear_144"></a>nn.Linear</h4> 
<h4><a id="torchoptim_146"></a>torch.optim</h4> 
<p><code>torch.optim</code> 有各种优化算法，可以使用优化器的 <code>step</code> 来进行前向传播，而不用人工的更新所有参数</p> 
<pre><code>opt.step()
opt.zero_grad()
</code></pre> 
<p><code>optim.zero_grad()</code> 将所有的梯度置为 0，需要在下个批次计算梯度之前调用</p> 
<h4><a id="DataLoader_156"></a>DataLoader</h4> 
<p><code>TensorDataset</code> 是 Dataset 的 tensor 包装</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> TensorDataset

train_ds <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>
</code></pre> 
<p><code>DataLoader</code> 用于管理 batches，便于迭代</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader

train_dl <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>train_ds<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">)</span>
</code></pre> 
<p>迭代训练</p> 
<pre><code class="prism language-python">model<span class="token punctuation">,</span> opt <span class="token operator">=</span> get_model<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> xb<span class="token punctuation">,</span> yb <span class="token keyword">in</span> train_dl<span class="token punctuation">:</span>
        pred <span class="token operator">=</span> model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> loss_func<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> yb<span class="token punctuation">)</span>

        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span><span class="token punctuation">,</span> yb<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="Add_Validation_187"></a>Add Validation</h4> 
<p>在训练过程中计算并打印每个 epoch 的 validation loss</p> 
<pre><code class="prism language-python">model<span class="token punctuation">,</span> opt <span class="token operator">=</span> get_model<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token comment"># 训练前</span>
    model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> xb<span class="token punctuation">,</span> yb <span class="token keyword">in</span> train_dl<span class="token punctuation">:</span>
        pred <span class="token operator">=</span> model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> loss_func<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> yb<span class="token punctuation">)</span>

        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 训练后，验证前</span>
    <span class="token comment"># 确保 nn.BatchNorm2d 和 nn.Dropout 采取适当的行为（关闭）</span>
    model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        valid_loss <span class="token operator">=</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span><span class="token punctuation">,</span> yb<span class="token punctuation">)</span> <span class="token keyword">for</span> xb<span class="token punctuation">,</span> yb <span class="token keyword">in</span> valid_dl<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>epoch<span class="token punctuation">,</span> valid_loss <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>valid_dl<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>为了简化代码，增强可读性，可以构建 <code>fit()</code> 和 <code>get_data()</code> 函数</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">get_data</span><span class="token punctuation">(</span>train_ds<span class="token punctuation">,</span> valid_ds<span class="token punctuation">,</span> bs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span>
        DataLoader<span class="token punctuation">(</span>train_ds<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>bs<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        DataLoader<span class="token punctuation">(</span>valid_ds<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>bs <span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">loss_batch</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> loss_func<span class="token punctuation">,</span> xb<span class="token punctuation">,</span> yb<span class="token punctuation">,</span> opt<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    loss <span class="token operator">=</span> loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span><span class="token punctuation">,</span> yb<span class="token punctuation">)</span>

    <span class="token keyword">if</span> opt <span class="token keyword">is</span> <span class="token operator">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>xb<span class="token punctuation">)</span>

<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token keyword">def</span> <span class="token function">fit</span><span class="token punctuation">(</span>epochs<span class="token punctuation">,</span> model<span class="token punctuation">,</span> loss_func<span class="token punctuation">,</span> opt<span class="token punctuation">,</span> train_dl<span class="token punctuation">,</span> valid_dl<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment"># 遍历 batch 中的每个样本</span>
        <span class="token keyword">for</span> xb<span class="token punctuation">,</span> yb <span class="token keyword">in</span> train_dl<span class="token punctuation">:</span>
            loss_batch<span class="token punctuation">(</span>model<span class="token punctuation">,</span> loss_func<span class="token punctuation">,</span> xb<span class="token punctuation">,</span> yb<span class="token punctuation">,</span> opt<span class="token punctuation">)</span>

        model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            losses<span class="token punctuation">,</span> nums <span class="token operator">=</span> <span class="token builtin">zip</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">[</span>loss_batch<span class="token punctuation">(</span>model<span class="token punctuation">,</span> loss_func<span class="token punctuation">,</span> xb<span class="token punctuation">,</span> yb<span class="token punctuation">)</span> <span class="token keyword">for</span> xb<span class="token punctuation">,</span> yb <span class="token keyword">in</span> valid_dl<span class="token punctuation">]</span><span class="token punctuation">)</span>
        val_loss <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span>losses<span class="token punctuation">,</span> nums<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>nums<span class="token punctuation">)</span>

        <span class="token keyword">print</span><span class="token punctuation">(</span>epoch<span class="token punctuation">,</span> val_loss<span class="token punctuation">)</span>
</code></pre> 
<p>主要代码简化为</p> 
<pre><code class="prism language-python">train_dl<span class="token punctuation">,</span> valid_dl <span class="token operator">=</span> get_data<span class="token punctuation">(</span>train_ds<span class="token punctuation">,</span> valid_ds<span class="token punctuation">,</span> bs<span class="token punctuation">)</span>
model<span class="token punctuation">,</span> opt <span class="token operator">=</span> get_model<span class="token punctuation">(</span><span class="token punctuation">)</span>
fit<span class="token punctuation">(</span>epochs<span class="token punctuation">,</span> model<span class="token punctuation">,</span> loss_func<span class="token punctuation">,</span> opt<span class="token punctuation">,</span> train_dl<span class="token punctuation">,</span> valid_dl<span class="token punctuation">)</span>
</code></pre> 
<h4><a id="nnSequential_255"></a>nn.Sequential</h4> 
<blockquote> 
 <p>参考 Keras 中的 Sequential Model</p> 
</blockquote> 
<pre><code class="prism language-python">model <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    Lambda<span class="token punctuation">(</span>preprocess<span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    Lambda<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span>
</code></pre> 
<p>其中，可以 PyTorch 没有提供 view layer，需要构造（Sequential中的Lambda）</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Lambda</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> func<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Lambda<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>func <span class="token operator">=</span> func

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>func<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">preprocess</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="Using_GPU_286"></a>Using GPU</h4> 
<blockquote> 
 <p>GPU 和 CPU 训练的模型的加载不一样，参数需要设置</p> 
</blockquote> 
<p>首先，判断 GPU 是否可以使用</p> 
<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>使用指定 GPU</p> 
<pre><code class="prism language-python">dev <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span><span class="token punctuation">)</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cpu"</span><span class="token punctuation">)</span>
</code></pre> 
<p>将数据（batch）移到GPU（使用 <code>.to(torch.device("cuda"))</code> 或 <code>.cuda()</code>）</p> 
<pre><code class="prism language-python">xb<span class="token punctuation">.</span>to<span class="token punctuation">(</span>dev<span class="token punctuation">)</span> <span class="token comment"># xb.cuda()</span>
yb<span class="token punctuation">.</span>to<span class="token punctuation">(</span>dev<span class="token punctuation">)</span> <span class="token comment"># yb.cuda()</span>
</code></pre> 
<p>最后，需要将模型移到 GPU</p> 
<pre><code class="prism language-python">model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>dev<span class="token punctuation">)</span> <span class="token comment"># model.cuda()</span>
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e6f75739bf19f9c8d532bc4e28927a12/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">超详细黑苹果安装图文教程送EFI配置合集及系统</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/4b22958a851e04ea16582a33163b5fce/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">解决Python开发中，Pycharm中无法使用中文输入法问题</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>