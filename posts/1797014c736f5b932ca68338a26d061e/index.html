<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>UNet网络解读 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="UNet网络解读" />
<meta property="og:description" content="UNet解读 UNet论文UNet的简介代码解读DoubleConv模块Down模块Up模块OutConv模块 整个UNet参考资料 UNet论文 UNet论文地址
UNet的简介 UNet是一个对称的网络结构，左侧为下采样，右侧为上采样；下采样为encoder，上采样为decoder;四条灰色的平行线，就是在上采样的过程中，融合下采样过程的特征图的通道，Concat 原理就是：一本大小为10cm10cm的书，厚度为3cm的书本（10103）的A书，和一本大小为10cm10cm，厚度为4cm的B书（10103）将A书和B书，边缘对齐的摞在一起，这样就可以得到一个大小10107的一摞书了所以对feature map,一个大小为256*256*64的feature map(w为256，h为256，c为64)，和一个大小为256*256*32的feature map进行Concat融合，你就会得到一个大小为256*256*96的feature map在实际使用中，Concat融合的两个feature map的大小不一定相同，例如25625664的feature map和24024032的feature map进行Concat 两种方法 1.将大的256*256*64的feature map进行裁剪，裁剪为240*240*64的feature map，比如上下左右，各舍弃8 pixel，裁剪后再进行Concat，得到24024096的feature map。2.将小的240*240*32的feature map进行padding操作，padding为256*256*32的feature map，比如上下左右，各补8 pixel，padding后再进行Concat，得到25625696的feature map。 UNet采用的Concat方案就是第二种，将小的feature map进行padding，padding的方式是补0，一种常规的常量填充。（详细看代码Up） 代码解读 组成U-Net的模型块主要有如下几个部分：
1）每个子块内部的两次卷积（Double Convolution）
2）左侧模型块之间的下采样连接，即最大池化（Max pooling）
3）右侧模型块之间的上采样连接（Up sampling）
4）输出层的处理（OutConv）
DoubleConv模块 两次卷积操作： class DoubleConv(nn.Module): # mid_channel是第一次conv的out和第二次conv的输入 def __init__(self, in_channels, out_channels, mid_channels=None): super().__init__() if not mid_channels: mid_channels = out_channels self.double_conv = nn.Sequential( # 大小 (高宽 &#43; 2*padding - kernel_size)/stride &#43; 1 # (572 &#43; 2*0 - 3 )/1 &#43;1 = 570 # 通道1-&gt;64 nn." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/1797014c736f5b932ca68338a26d061e/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-11-24T21:41:16+08:00" />
<meta property="article:modified_time" content="2022-11-24T21:41:16+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">UNet网络解读</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>UNet解读</h4> 
 <ul><li><a href="#UNet_1" rel="nofollow">UNet论文</a></li><li><a href="#UNet_3" rel="nofollow">UNet的简介</a></li><li><a href="#_18" rel="nofollow">代码解读</a></li><li><ul><li><a href="#DoubleConv_29" rel="nofollow">DoubleConv模块</a></li><li><a href="#Down_68" rel="nofollow">Down模块</a></li><li><a href="#Up_85" rel="nofollow">Up模块</a></li><li><a href="#OutConv_135" rel="nofollow">OutConv模块</a></li></ul> 
  </li><li><a href="#UNet_147" rel="nofollow">整个UNet</a></li><li><a href="#_185" rel="nofollow">参考资料</a></li></ul> 
</div> 
<p></p> 
<h2><a id="UNet_1"></a>UNet论文</h2> 
<p><a href="https://arxiv.org/pdf/1505.04597.pdf" rel="nofollow">UNet论文地址</a></p> 
<h2><a id="UNet_3"></a>UNet的简介</h2> 
<p><img src="https://images2.imgbox.com/cd/47/jgbVxvGe_o.png" alt="UNET"></p> 
<ul><li>UNet是一个<code>对称</code>的网络结构，<code>左侧为下采样，右侧为上采样</code>；</li><li>下采样为encoder，上采样为decoder;</li><li>四条灰色的平行线，就是在上采样的过程中，融合下采样过程的特征图的通道，Concat 
  <ul><li><code>原理</code>就是：一本大小为10cm<em>10cm的书，厚度为3cm的书本（10</em>10<em>3）的A书，和一本大小为10cm</em>10cm，厚度为4cm的B书（10<em>10</em>3）</li><li>将A书和B书，边缘对齐的摞在一起，这样就可以得到一个大小10<em>10</em>7的一摞书了</li><li>所以对feature map,一个大小为<code>256*256*64</code>的feature map(w为256，h为256，c为64)，和一个大小为<code>256*256*32</code>的feature map进行Concat融合，你就会得到一个大小为<code>256*256*96</code>的feature map</li><li>在实际使用中，Concat融合的两个feature map的大小不一定相同，例如256<em>256</em>64的feature map和240<em>240</em>32的feature map进行Concat 
    <ul><li>两种方法 
      <ul><li>1.将大的<code>256*256*64</code>的feature map进行裁剪，裁剪为<code>240*240*64</code>的feature map，<code>比如上下左右，各舍弃8 pixel，裁剪后再进行Concat</code>，得到240<em>240</em>96的feature map。</li><li>2.将小的<code>240*240*32</code>的feature map进行<code>padding操作</code>，padding为<code>256*256*32</code>的feature map，<code>比如上下左右，各补8 pixel，padding后再进行Concat</code>，得到256<em>256</em>96的feature map。</li></ul> </li><li>UNet采用的Concat方案就是<code>第二种</code>，将小的feature map进行padding，padding的方式是<code>补0</code>，一种<code>常规</code>的常量填充。（详细看代码Up）</li></ul> </li></ul> </li></ul> 
<h2><a id="_18"></a>代码解读</h2> 
<ul><li> <p>组成U-Net的模型块主要有如下几个部分：</p> 
  <ul><li> <p>1）每个子块内部的两次卷积（Double Convolution）</p> </li><li> <p>2）左侧模型块之间的下采样连接，即最大池化（Max pooling）</p> </li><li> <p>3）右侧模型块之间的上采样连接（Up sampling）</p> </li><li> <p>4）输出层的处理（OutConv）</p> </li></ul> </li></ul> 
<h3><a id="DoubleConv_29"></a>DoubleConv模块</h3> 
<ul><li>两次卷积操作：</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">DoubleConv</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
   <span class="token comment"># mid_channel是第一次conv的out和第二次conv的输入</span>
   <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> mid_channels<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
       <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
       <span class="token keyword">if</span> <span class="token keyword">not</span> mid_channels<span class="token punctuation">:</span>
           mid_channels <span class="token operator">=</span> out_channels
       self<span class="token punctuation">.</span>double_conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
           <span class="token comment"># 大小 (高宽 + 2*padding - kernel_size)/stride + 1</span>
           <span class="token comment"># (572 + 2*0 - 3 )/1 +1 = 570</span>
           <span class="token comment"># 通道1-&gt;64</span>
           nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> mid_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
           <span class="token comment"># 帮助网络训练， 对输入数据做规范化，称为Covariate shift</span>
           <span class="token comment"># BatchNorm后是不改变输入的shape</span>
           <span class="token comment"># num_features: 输入维度，也就是数据的特征维度；</span>
           <span class="token comment"># eps： 是在分母上加的一个值，是为了防止分母为0的情况，让其能正常计算；</span>
           <span class="token comment"># affine： 是仿射变化，将,分别初始化为1和0；</span>
           <span class="token comment"># nn.BatchNorm2d是对channel做归一化处理，也就是对批次内的特征进行归一化</span>
           <span class="token comment"># 加快收敛，防止梯度爆炸和消失</span>
           nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>mid_channels<span class="token punctuation">)</span><span class="token punctuation">,</span>
           <span class="token comment"># inplace = True 时,会修改输入对象的值,所以打印出对象存储地址相同,类似于C语言的址传递</span>
           <span class="token comment"># inplace = False 时,不会修改输入对象的值,而是返回一个新创建的对象,所以打印出对象存储地址不同,类似于C语言的值传递</span>
           nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
           <span class="token comment"># (570 + 2*0 - 3 )/1 +1 = 568</span>
           <span class="token comment"># 设置padding=1(原始的是设置为0，会改变)经过卷积后不会改变特征层的大小，这也是现在主流的实现方式</span>
           nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>mid_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
           nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">)</span><span class="token punctuation">,</span>
           nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
       <span class="token punctuation">)</span>
   
   <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
       x <span class="token operator">=</span> self<span class="token punctuation">.</span>double_conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
       <span class="token keyword">return</span> x
</code></pre> 
<ul><li>注意代码里面的卷积是如何计算的<code>（ (高宽 + 2*padding - kernel_size)/stride + 1），如U图中的初始：(572 + 2*0 - 3 )/1 +1 = 570</code></li></ul> 
<h3><a id="Down_68"></a>Down模块</h3> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Down</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>maxpool_conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token comment"># 扩大通道64-&gt;128</span>
            DoubleConv<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>maxpool_conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="Up_85"></a>Up模块</h3> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Up</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> bilinear<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 上采样需要插值，双</span>
        <span class="token keyword">if</span> bilinear<span class="token punctuation">:</span>
            <span class="token comment"># 两倍</span>
            self<span class="token punctuation">.</span>up <span class="token operator">=</span> nn<span class="token punctuation">.</span>Upsample<span class="token punctuation">(</span>scale_factor<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'bilinear'</span><span class="token punctuation">,</span> align_corners<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>conv <span class="token operator">=</span> DoubleConv<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> in_channels <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
        	<span class="token comment"># 反卷积</span>
            self<span class="token punctuation">.</span>up <span class="token operator">=</span> nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> in_channels <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>conv <span class="token operator">=</span> DoubleConv<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x1<span class="token punctuation">,</span> x2<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x1 <span class="token operator">=</span> self<span class="token punctuation">.</span>up<span class="token punctuation">(</span>x1<span class="token punctuation">)</span>
        <span class="token comment"># input is CHW</span>
        <span class="token comment"># 算出相差多少</span>
        diffY <span class="token operator">=</span> x2<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">-</span> x1<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>
        diffX <span class="token operator">=</span> x2<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">-</span> x1<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span>
        <span class="token comment"># F.pad是pytorch内置的tensor扩充函数，便于对数据集图像或中间层特征进行维度扩充</span>
        <span class="token comment"># 最后一维padding，第一个元素代表左边padding的个数，第二个元素代表右边padding的个数</span>
        <span class="token comment"># input：需要扩充的tensor，可以是图像数据，抑或是特征矩阵数据</span>
        <span class="token comment"># pad：扩充维度，用于预先定义出某维度上的扩充参数</span>
        <span class="token comment"># mode：扩充方法，’constant‘, ‘reflect’ or ‘replicate’三种模式，分别表示常量，反射，复制</span>
        <span class="token comment"># value：扩充时指定补充值，但是value只在mode='constant’有效，</span>
        <span class="token comment"># 即使用value填充在扩充出的新维度位置，而在’reflect’和’replicate’模式下，value不可赋值</span>
        <span class="token comment"># https://www.modb.pro/db/227153</span>
        x1 <span class="token operator">=</span> F<span class="token punctuation">.</span>pad<span class="token punctuation">(</span>x1<span class="token punctuation">,</span> <span class="token punctuation">[</span>diffX <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">,</span> diffX <span class="token operator">-</span> diffX <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">,</span> diffY <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">,</span> diffY <span class="token operator">-</span> diffY <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>x2<span class="token punctuation">,</span> x1<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre> 
<ul><li><code>__init__</code>初始化函数里定义的<code>上采样方法</code>以及<code>卷积采用DoubleConv</code>。 
  <ul><li> <p>上采样，定义了两种方法：<code>Upsample</code>和<code>ConvTranspose2d</code>，也就是<code>双线性插值</code>和<code>反卷积</code>。</p> 
    <ul><li> <p>双线性插值:<br> <img src="https://images2.imgbox.com/a7/8f/v4llptgs_o.png" alt="双线性插值"></p> </li><li> <p>简单地讲：已知Q11、Q12、Q21、Q22四个点坐标，通过Q11和Q21求R1，再通过Q12和Q22求R2，最后通过R1和R2求P，这个过程就是双线性插值。</p> </li><li> <p>对于一个feature map而言，其实就是在像素点中间补点，补的点的值是多少，是由相邻像素点的值决定的。</p> </li></ul> </li><li> <p>反卷积：</p> 
    <ul><li> <p>就是反着卷积</p> </li><li> <p><img src="https://images2.imgbox.com/2c/ee/VKqNHXRv_o.gif" alt="反卷积"></p> </li><li> <p>下面的蓝色为原始图片，周围白色的虚线方块为padding结果，通常为0，上面绿色为卷积后的图片。</p> </li><li> <p>这个示意图，就是一个从<code>2*2的feature map-&gt;4*4的feature map过程</code>。</p> </li><li> <p>在<code>forward前向传播函数中</code>，<code>x1</code>接收的是<code>上采样的数据</code>，<code>x2</code>接收的是<code>特征融合的数据</code>。特征融合方法就是，上文提到的，<code>先对小的feature map进行padding，再进行concat</code>。</p> </li></ul> </li></ul> </li></ul> 
<h3><a id="OutConv_135"></a>OutConv模块</h3> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">OutConv</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>OutConv<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 和super().__init__()一样</span>
        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre> 
<h2><a id="UNet_147"></a>整个UNet</h2> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">UNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n_channels<span class="token punctuation">,</span> n_classes<span class="token punctuation">,</span> bilinear<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>UNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>n_channels <span class="token operator">=</span> n_channels
        self<span class="token punctuation">.</span>n_classes <span class="token operator">=</span> n_classes
        self<span class="token punctuation">.</span>bilinear <span class="token operator">=</span> bilinear
        
        <span class="token comment"># 最左边</span>
        self<span class="token punctuation">.</span>inc <span class="token operator">=</span> DoubleConv<span class="token punctuation">(</span>n_channels<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span>
        <span class="token comment"># 下采样(里面包括下采样完的两次卷积)</span>
        self<span class="token punctuation">.</span>down1 <span class="token operator">=</span> Down<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>down2 <span class="token operator">=</span> Down<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>down3 <span class="token operator">=</span> Down<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span>
        <span class="token comment"># 插值方式</span>
        factor <span class="token operator">=</span> <span class="token number">2</span> <span class="token keyword">if</span> bilinear <span class="token keyword">else</span> <span class="token number">1</span>
        self<span class="token punctuation">.</span>down4 <span class="token operator">=</span> Down<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">1024</span> <span class="token operator">//</span> factor<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>up1 <span class="token operator">=</span> Up<span class="token punctuation">(</span><span class="token number">1024</span><span class="token punctuation">,</span> <span class="token number">512</span> <span class="token operator">//</span> factor<span class="token punctuation">,</span> bilinear<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>up2 <span class="token operator">=</span> Up<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">512</span> <span class="token operator">//</span> factor<span class="token punctuation">,</span> bilinear<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>up3 <span class="token operator">=</span> Up<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">256</span> <span class="token operator">//</span> factor<span class="token punctuation">,</span> bilinear<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>up4 <span class="token operator">=</span> Up<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> bilinear<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>outc <span class="token operator">=</span> OutConv<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> n_classes<span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x1 <span class="token operator">=</span> self<span class="token punctuation">.</span>inc<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x2 <span class="token operator">=</span> self<span class="token punctuation">.</span>down1<span class="token punctuation">(</span>x1<span class="token punctuation">)</span>
        x3 <span class="token operator">=</span> self<span class="token punctuation">.</span>down2<span class="token punctuation">(</span>x2<span class="token punctuation">)</span>
        x4 <span class="token operator">=</span> self<span class="token punctuation">.</span>down3<span class="token punctuation">(</span>x3<span class="token punctuation">)</span>
        x5 <span class="token operator">=</span> self<span class="token punctuation">.</span>down4<span class="token punctuation">(</span>x4<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>up1<span class="token punctuation">(</span>x5<span class="token punctuation">,</span> x4<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>up2<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x3<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>up3<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x2<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>up4<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x1<span class="token punctuation">)</span>
        logits <span class="token operator">=</span>self<span class="token punctuation">.</span>outc<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> logits
</code></pre> 
<h2><a id="_185"></a>参考资料</h2> 
<ul><li><a href="https://blog.csdn.net/Y0W1as5eg37urFdS/article/details/122206366">UNet语义分割</a></li><li><a href="https://www.modb.pro/db/227153" rel="nofollow">F.pad的使用</a></li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/2eccd5ff1e06792a8f1216c99c6c4002/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Python基本数据类型</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d258fc1f2963dbd0991df56a0c0f3b75/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">DW运用HTML&#43;CSS建网站（2）成品</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>