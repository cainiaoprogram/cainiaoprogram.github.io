<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>logstash使用总结 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="logstash使用总结" />
<meta property="og:description" content="最近在寻找从kafka读取数据，同步数据写入ElasticSearch中的通用ELK方案。其中 logstash最方便简单，总结一下。
安装 下载
下载位置
Past Releases of Elastic Stack Software | Elastic
注意：下载版本和ElasticSearch的版本保持一致。es版本可以通过http://ip:9200/ 查看。
管道配置 Logstash管道通常有三个阶段：输入（input）→ 过滤器(filter)→ 输出(output)。输入生成事件，过滤器修改它们，输出将它们发送到其他地方。
input 读取kafka数据 input { kafka { bootstrap_servers =&gt; &#34;192.168.10.153:9092&#34; group_id =&gt; &#34;logstash_test&#34; auto_offset_reset =&gt; &#34;latest&#34; topics =&gt; [&#34;log_info&#34;] codec =&gt; json { ##添加json插件 charset =&gt; &#34;UTF-8&#34; } } } LogStash多实例并行消费kafka 1.设置相同topic
2.设置相同groupid
3.设置不同clientid
4.input 的这个参数 consumer_threads =&gt; 10 多实列相加最好等于 topic分区数
如果一个logstash得参数大于topic，则topic数据都会被这个logstash消费掉
配置示例：
input { kafka { bootstrap_servers =&gt; &#34;192.168.10.153:9092&#34; group_id =&gt; &#34;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/0f41579f52d1781422dc2dc45a2916a5/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-08-11T08:57:11+08:00" />
<meta property="article:modified_time" content="2023-08-11T08:57:11+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">logstash使用总结</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>最近在寻找从kafka读取数据，同步数据写入ElasticSearch中的通用ELK方案。其中 logstash最方便简单，总结一下。</p> 
<h2>安装</h2> 
<p>下载</p> 
<p>下载位置</p> 
<p><a href="https://www.elastic.co/cn/downloads/past-releases#logstash" rel="nofollow" title="Past Releases of Elastic Stack Software | Elastic">Past Releases of Elastic Stack Software | Elastic</a></p> 
<p>注意：下载版本和ElasticSearch的版本保持一致。es版本可以通过<a href="http://192.168.10.153:9200/" rel="nofollow" title="http://ip:9200/">http://ip:9200/</a> 查看。</p> 
<p></p> 
<h2>管道配置</h2> 
<p>Logstash管道通常有三个阶段：输入（input）→ 过滤器(filter)→ 输出(output)。输入生成事件，过滤器修改它们，输出将它们发送到其他地方。</p> 
<p></p> 
<h3>input</h3> 
<h4>读取kafka数据</h4> 
<pre><code>input {
  kafka {
    bootstrap_servers =&gt; "192.168.10.153:9092"
    group_id =&gt; "logstash_test"
    auto_offset_reset =&gt; "latest"
    topics =&gt; ["log_info"]
    codec =&gt; json {  ##添加json插件
      charset =&gt; "UTF-8"
    }
  }
}
</code></pre> 
<h4 id="articleContentId">LogStash多实例并行消费kafka</h4> 
<blockquote> 
 <p>1.设置相同topic<br> 2.设置相同groupid<br> 3.设置不同clientid<br> 4.input 的这个参数 consumer_threads =&gt; 10 多实列相加最好等于 topic分区数<br> 如果一个logstash得参数大于topic，则topic数据都会被这个logstash消费掉</p> 
</blockquote> 
<p>配置示例：</p> 
<pre><code>input {
  kafka {
    bootstrap_servers =&gt; "192.168.10.153:9092"
    group_id =&gt; "logstash_test"
    client_id =&gt; 1
    auto_offset_reset =&gt; "latest"
    topics =&gt; ["log_info"]
    codec =&gt; json {  ##添加json插件
      charset =&gt; "UTF-8"
    }
  }
}
</code></pre> 
<p>测试：</p> 
<p>按要求启动多个logstash实例，然后批量发送一批数据进入kafka,如果多个实例中都可以看到消费输出，则说明LogStash多实例并行消费kafka配置生效。</p> 
<p>批量发送可以用如下脚本</p> 
<pre><code>cat log.txt | ./bin/kafka-console-producer.sh --bootstrap-server 192.168.10.153:9092 --topic log_info</code></pre> 
<p>log.txt</p> 
<pre><code>{"title":"aa","author":"bbbb","itemId":12336,"site":"dafadf","time":"2023-01-01 01:00:00"}
{"title":"bb","author":"bbbb","itemId":12337,"site":"dafadf","time":"2023-01-01 01:00:00"}
{"title":"cc","author":"bbbb","itemId":12338,"site":"dafadf","time":"2023-01-01 01:00:00"}</code></pre> 
<p>来源：<a href="https://blog.csdn.net/jyb199158/article/details/82841731" title="LogStash多实例并行消费kafka_logstash 多实例 消费kafka 重复消费_林沂梵的博客-CSDN博客">LogStash多实例并行消费kafka_logstash 多实例 消费kafka 重复消费_林沂梵的博客-CSDN博客</a> </p> 
<p></p> 
<h4>吞吐能力调优</h4> 
<p>1.调整consumer_threads</p> 
<p>2.调整work数</p> 
<p>在logstash消费kafka数据时，<strong>consumer_threads参数用于指定从kafka中读取数据的线程数</strong>，即同时从kafka中读取数据的数量。该参数的值越大，logstash从kafka读取数据的速度就越快。但是，如果该值过大，可能会导致系统性能下降。</p> 
<p>与此不同的是，<strong>work参数则是指定logstash中并行执行的worker数</strong>，即同时进行过滤、处理数据的线程数。该参数的值越大，logstash处理数据的能力就越强。但同样地，如果该值过大，可能会导致系统性能下降。</p> 
<p>因此，consumer_threads参数是用于调整从kafka中读取数据的速度，而work参数则是用于调整logstash的整体处理能力。</p> 
<p>样例：</p> 
<pre><code>input {
  kafka {
    bootstrap_servers =&gt; "192.168.10.153:9092"
    group_id =&gt; "logstash_test"
    auto_offset_reset =&gt; "latest"
    topics =&gt; ["log_info"]
	consumer_threads =&gt; 2
	workers =&gt; 5
    codec =&gt; json {  ##添加json插件
      charset =&gt; "UTF-8"
    }
  }
}
</code></pre> 
<p>3.调整queue.type</p> 
<p>logstash中的queue.type参数用于指定队列的类型，目前支持两种类型：memory和persisted。</p> 
<ul><li>memory：使用内存作为队列存储方式，数据仅在内存中存储，<strong>适用于数据量较小的场景</strong>。</li><li>persisted：使用磁盘作为队列存储方式，会将数据存储到磁盘文件中，<strong>适用于数据量较大的场景</strong>。</li></ul> 
<p>queue.type的默认值是memory，如果需要使用persisted类型的队列，需要指定文件路径和文件名。</p> 
<p></p> 
<h3>filter</h3> 
<h4>解决@timestamp相差8小时问题</h4> 
<p>1.@timestamp为当前时间</p> 
<pre><code>filter {
 ruby {   
   code =&gt; "event.set('timestamp', event.get('@timestamp').time.localtime + 8*60*60)"   
 }  
 ruby {  
   code =&gt; "event.set('@timestamp',event.get('timestamp'))"  
 }  
 mutate {  
   remove_field =&gt; ["timestamp"]  
 } 
    
}
</code></pre> 
<p></p> 
<p>2.用时间字段覆盖@timestamp </p> 
<pre><code>filter {
    date {
        match =&gt; ["time", "yyyy-MM-dd HH:mm:ss"]
        target =&gt; "@timestamp"
    }
	
 ruby {   
   code =&gt; "event.set('timestamp', event.get('@timestamp').time.localtime + 8*60*60)"   
 }  
 ruby {  
   code =&gt; "event.set('@timestamp',event.get('timestamp'))"  
 }  
 mutate {  
   remove_field =&gt; ["timestamp"]  
 } 
    
}
</code></pre> 
<p>3.使用格式化后的时间字符串</p> 
<pre><code>filter {
    date {
        match =&gt; ["time", "yyyy-MM-dd HH:mm:ss"]
        target =&gt; "timetest"
    }

ruby {
    code =&gt; "event.set('daytime', ( event.get('timetest').time.localtime + 8*60*60).strftime('%Y-%m-%d'))"
 }

 mutate {
   remove_field =&gt; ["timetest"]
 }

}
</code></pre> 
<p></p> 
<h3> output</h3> 
<h4>按自定义模板输出到elasticsearch。</h4> 
<p>如下实现了取@timestamp的天，动态创建index索引</p> 
<p>以itemId字段作为索引id</p> 
<p><code>lush_size</code> 和 <code>idle_flush_time</code> 两个参数共同控制 Logstash 向 Elasticsearch 发送批量数据的行为。以上面示例来说：Logstash 会努力攒到 5条数据一次性发送出去，但是如果 5秒钟内也没攒够 5条，Logstash 还是会以当前攒到的数据量发一次。</p> 
<p>从 5.0 开始，这个行为有了另一个前提：<code>flush_size</code> 的大小不能超过 Logstash 运行时的命令行参数设置的 <code>batch_size</code>，否则将以 <code>batch_size</code> 为批量发送的大小。</p> 
<pre><code>output {
  elasticsearch {
   flush_size =&gt; 5
   idle_flush_time =&gt; 5
    hosts =&gt; ["http://192.168.10.153:9200"]
    index =&gt; "log_info-%{+YYYY.MM.dd}"
	document_type =&gt; "log_type"  
	document_id =&gt; "%{itemId}"
    template =&gt; "/root/logstash-5.4.1/config/temp_log_info.json"  #Elasticsearh模板路径
    template_name =&gt; "log_info_tmp"  #Elasticsearh模板名称
    template_overwrite =&gt; true
  }
  stdout {
        codec =&gt; json_lines
    }
}
</code></pre> 
<p>temp_log_info.json</p> 
<pre><code>{
      "template":"log_info*",
     "mappings":{
        "article":{
            "dynamic":"strict",
            "_all":{
                "enabled":false
            },
            "properties":{
                "title":{
                    "type":"string",
                    "index":"analyzed",
                    "analyzer":"ik_max_word",
                    "search_analyzer":"ik_max_word"
                },
                "author":{
                    "type":"string",
                    "index":"no"
                },
				"itemId":{
                    "type":"long"
                },
				 "site":{
                    "type":"keyword"
                },
                "time":{
                    "type":"date",
                    "index":"not_analyzed",
                    "format":"yyyy-MM-dd HH:mm:ss"
                }
            }
        }
    }
}</code></pre> 
<p> 来源：</p> 
<p><a href="https://www.wenjiangs.com/doc/wgtwsbetm" rel="nofollow" title="output配置 - elasticsearch - ELK Stack 中文指南 - 开发文档 - 文江博客">output配置 - elasticsearch - ELK Stack 中文指南 - 开发文档 - 文江博客</a></p> 
<p></p> 
<h4>根据不同来源写到不同索引</h4> 
<pre><code>input {
    file {
        path =&gt; "/usr/local/my.log"  
        start_position =&gt; "beginning" 
        type =&gt; "infolog"
        sincedb_path =&gt; "/dev/null"
    }
    file {
        path =&gt; "/usr/local/my1.log"  
        start_position =&gt; "beginning" 
        type =&gt; "errlog"
        sincedb_path =&gt; "/dev/null"
    }
 
}
filter {
      json {
         source =&gt; "message"
      }
      date {
        match =&gt; ["timestamp", "dd/MMM/yyyy:HH:mm:ss Z"] #匹配timestamp字段
        target =&gt; "@timestamp"  #将匹配到的数据写到@timestamp字段中
      }
}
 
output {
       if [type] == "infolog" {
         elasticsearch {
            hosts =&gt; ["test:9200"]
            index =&gt; "infolog-%{+YYYY.MM.dd}"
        }
       } else if [type] == "errlog" {
         elasticsearch {
            hosts =&gt; ["test:9200"]
            index =&gt; "errlog-%{+YYYY.MM.dd}"
        }
       }
 
}</code></pre> 
<p>来源：<a href="https://blog.csdn.net/qq_41214487/article/details/120062142?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=logstash%20%E6%A0%B9%E6%8D%AE%E4%B8%8D%E5%90%8C%E6%95%B0%E6%8D%AE%E6%BA%90%20%E5%8A%A8%E6%80%81%E5%88%9B%E5%BB%BA%E7%B4%A2%E5%BC%95&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-7-120062142.142%5Ev88%5Einsert_down1,239%5Ev2%5Einsert_chatgpt&amp;spm=1018.2226.3001.4187" title="logstash配置不同类型日志写到不同索引_logstash 索引配置_皮特猫.的博客-CSDN博客">logstash配置不同类型日志写到不同索引_logstash 索引配置_皮特猫.的博客-CSDN博客</a></p> 
<p></p> 
<h2>启动</h2> 
<p>普通启动</p> 
<blockquote> 
 <p>./bin/logstash -f ./config/test.conf</p> 
</blockquote> 
<p>自动重新加载配置文件</p> 
<blockquote> 
 <p>bin/logstash -f apache.config --config.reload.automatic </p> 
</blockquote> 
<p>启动多个实体</p> 
<p>修改config/logstash.yml</p> 
<blockquote> 
 <p>path.data: /path/to/data/directory</p> 
</blockquote> 
<p>注意：在设置 <code>path.data</code> 的时候，需要确保 Logstash 进程对该目录有读写权限。同时如果你运行了多个 Logstash 实例，需要保证每个实例的 <code>path.data</code> 目录是不同的，以便避免数据冲突。</p> 
<p></p> 
<p>带认证的es入库</p> 
<pre><code>input {
  kafka {
    bootstrap_servers =&gt; "kafka_host:9092"  # 替换为Kafka的主机和端口
    topics =&gt; ["topic_name"]  # 替换为要消费的Kafka主题名称
    group_id =&gt; "logstash_consumer"
    codec =&gt; json
  }
}

output {
  elasticsearch {
    hosts =&gt; ["http://elasticsearch_host:9200"]  # 替换为Elasticsearch的主机和端口
    user =&gt; "aaa"  # Elasticsearch的用户名
    password =&gt; "ccc"  # Elasticsearch的密码
    index =&gt; "your_index_name"  # 替换为要写入的Elasticsearch索引名称
    document_id =&gt; "%{id}"  # 替换为JSON数据中表示文档ID的字段名称
  }
}</code></pre> 
<p></p> 
<h2>测试</h2> 
<p>启动生产者：</p> 
<pre><code>./bin/kafka-console-producer.sh --bootstrap-server 192.168.10.153:9092 --topic log_info</code></pre> 
<p>插入测试数据：</p> 
<pre><code>{"title":"aa","author":"bbbb","itemId":12335,"site":"dafadf","time":"2023-01-01 01:00:00"}</code></pre> 
<p>#大批量测试用这种更方便 </p> 
<pre><code class="hljs">cat log.txt | ./bin/kafka-console-producer.sh --bootstrap-server 192.168.10.153:9092 --topic log_info</code></pre> 
<h2>一次logstash-7.3.2同步实践</h2> 
<p>#创建模板</p> 
<pre><code class="hljs">curl -u 'elastic:xxx' -X PUT --header 'Content-Type: application/json' --header 'Accept: application/json'  'http://10.x.x.x:9200/_template/tmp_news' -d@tmp_news.json</code></pre> 
<p>tmp_news.json </p> 
<pre><code class="hljs">{
    "template":"news_*",
     "aliases": {
         "news_total": {}
      },
    "mappings": {
                "properties": {
                "content":{
                    "type":"text",
                    "analyzer":"ik_max_word",
                    "search_analyzer":"ik_max_word"
                },
                "data_id":{
                    "type":"keyword"
                },
                "uid":{
                    "type":"keyword"
                },
                "group_id":{
                    "type":"keyword"
                },
                "pubtime":{
                    "type":"date",
                    "format":"yyyy-MM-dd HH:mm:ss"
                },
                "insert_time":{
                    "type":"date",
                    "format":"yyyy-MM-dd HH:mm:ss"
                }
    }
  }

}</code></pre> 
<pre><code class="hljs">curl -u 'elastic:xxx' -X PUT --header 'Content-Type: application/json' --header 'Accept: application/json'  'http://10.x.x.x:9200/_template/tmp_tg' -d@tmp_tg.json
</code></pre> 
<p>tmp_tg.json</p> 
<pre><code class="hljs">{
    "template":"tg_*",
     "aliases": {
         "tg_total": {}
      },
    "mappings": {
                "properties": {
                "content":{
                    "type":"text",
                    "analyzer":"ik_max_word",
                    "search_analyzer":"ik_max_word"
                },
                "data_id":{
                    "type":"keyword"
                },
                "uid":{
                    "type":"keyword"
                },
                "group_id":{
                    "type":"keyword"
                },
                "pubtime":{
                    "type":"date",
                    "format":"yyyy-MM-dd HH:mm:ss"
                },
                "insert_time":{
                    "type":"date",
                    "format":"yyyy-MM-dd HH:mm:ss"
                }
    }
  }

}
</code></pre> 
<p>#测试模板是否生效</p> 
<pre><code class="hljs">curl -u 'elastic:xxx' -X PUT --header 'Content-Type: application/json' --header 'Accept: application/json' 'http://10.x.x.x:9200/news_test'</code></pre> 
<p>#查看实体</p> 
<pre><code class="hljs">curl  -u 'elastic:xxx' -XGET 'http://10.x.x.x:9200/_cat/indices?v'</code></pre> 
<p>#查看mapping</p> 
<pre><code class="hljs">curl -u 'elastic:xxx' -XGET 'http://10.x.x.x:9200/news_test/_mapping?pretty'</code></pre> 
<p>#查看索引的别名</p> 
<pre><code class="hljs">curl -u 'elastic:xxx' -XGET '10.x.x.x:9200/news_test/_alias'</code></pre> 
<p>#查看模板</p> 
<pre><code class="hljs">curl -u 'elastic:xxx' -XGET http://10.x.x.x:9200/_template/tmp_tg?pretty</code></pre> 
<p>#删除模板</p> 
<pre><code class="hljs">curl -u 'elastic:xxx' -XDELETE 10.x.x.x:9200/_template/tmp_tg</code></pre> 
<p>#准备logstash配置</p> 
<p>news_tmp.conf</p> 
<pre><code class="hljs">input {
  kafka {
    bootstrap_servers =&gt; "10.x.x.x:9092"
    topics =&gt; ["line_new_3"]
    group_id =&gt; "logstash_consumer_news"
    codec =&gt; json
  }
}

filter {
 ruby {   
   code =&gt; "event.set('timestamp', event.get('@timestamp').time.localtime + 8*60*60)"   
 }  
 ruby {  
   code =&gt; "event.set('@timestamp',event.get('timestamp'))"  
 }  
 mutate {  
   remove_field =&gt; ["timestamp"]  
 } 
    
}

output {
  elasticsearch {
    hosts =&gt; ["http://10.x.x.x:9200"]
    index =&gt; "news_%{+YYYY-MM}" 
    user =&gt; "elastic"
    password =&gt; "xxx"
    document_id =&gt; "%{data_id}"
    template =&gt; "/data/es7/tmp_news.json"  #Elasticsearh模板路径
    template_name =&gt; "tmp_news"  #Elasticsearh模板名称
    template_overwrite =&gt; true
  }
  stdout {
        codec =&gt; json_lines
    }
}</code></pre> 
<p>tg_tmp.conf</p> 
<pre><code class="hljs">input {
  kafka {
    bootstrap_servers =&gt; "10.x.x.x:9092"
    topics =&gt; ["tggv1_3"]
    group_id =&gt; "logstash_consumer_tg"
    codec =&gt; json
  }
}

filter {
 ruby {   
   code =&gt; "event.set('timestamp', event.get('@timestamp').time.localtime + 8*60*60)"   
 }  
 ruby {  
   code =&gt; "event.set('@timestamp',event.get('timestamp'))"  
 }  
 mutate {  
   remove_field =&gt; ["timestamp"]  
 } 
    
}

output {
  elasticsearch {
    hosts =&gt; ["http://10.x.x.x:9200"]
    index =&gt; "tg_%{+YYYY-MM}" 
    user =&gt; "elastic"
    password =&gt; "xxx"
    document_id =&gt; "%{data_id}"
    template =&gt; "/data/es7/tmp_tg.json"  #Elasticsearh模板路径
    template_name =&gt; "tmp_tg"  #Elasticsearh模板名称
    template_overwrite =&gt; true
  }
  stdout {
        codec =&gt; json_lines
    }
}</code></pre> 
<p>#启动配置</p> 
<pre><code class="hljs">./bin/logstash -f config/news_tmp.conf</code></pre> 
<pre><code class="hljs">./bin/logstash -f config/tg_tmp.conf</code></pre> 
<p>#查看实体</p> 
<pre><code class="hljs">curl  -u 'elastic:xxx' -XGET 'http://10.x.x.x:9200/_cat/indices?v'</code></pre> 
<p>#查看索引创建是否正常</p> 
<pre><code class="hljs">curl -u 'elastic:xxx' -XGET 'http://10.x.x.x:9200/news_2023-08/_mapping?pretty'</code></pre> 
<p>#查看数据否正常</p> 
<pre><code class="hljs">curl  -u 'elastic:xxx' -X GET "http://10.x.x.x:9200/news_2023-08/_doc/005eadb0b289abef5f02d553bb07f164"</code></pre> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/67d0d3c939e919bb96e2f312bdb6a2b4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Java知识点--常用类--String</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d0687382bdacf40d74fd8d13fd26561a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Pycharm Run 控制台打印中文乱码</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>