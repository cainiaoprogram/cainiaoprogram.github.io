<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>人工智能|机器学习——循环神经网络的简洁实现 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="人工智能|机器学习——循环神经网络的简洁实现" />
<meta property="og:description" content="循环神经网络的简洁实现 如何使用深度学习框架的高级API提供的函数更有效地实现相同的语言模型。 我们仍然从读取时光机器数据集开始。
import torch from torch import nn from torch.nn import functional as F from d2l import torch as d2l batch_size, num_steps = 32, 35 train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps) 定义模型 高级API提供了循环神经网络的实现。 我们构造一个具有256个隐藏单元的单隐藏层的循环神经网络层rnn_layer。 事实上，我们还没有讨论多层循环神经网络的意义。 现在仅需要将多层理解为一层循环神经网络的输出被用作下一层循环神经网络的输入就足够了。
num_hiddens = 256 rnn_layer = nn.RNN(len(vocab), num_hiddens) 我们使用张量来初始化隐状态，它的形状是（隐藏层数，批量大小，隐藏单元数）。
state = torch.zeros((1, batch_size, num_hiddens)) state.shape torch.Size([1, 32, 256]) 通过一个隐状态和一个输入，我们就可以用更新后的隐状态计算输出。 需要强调的是，rnn_layer的“输出”（Y）不涉及输出层的计算： 它是指每个时间步的隐状态，这些隐状态可以用作后续输出层的输入。
X = torch.rand(size=(num_steps, batch_size, len(vocab))) Y, state_new = rnn_layer(X, state) Y.shape, state_new.shape (torch.Size([35, 32, 256]), torch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/b63d43b76cfe5e5d09c0c13a91f2402e/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-11-25T22:28:04+08:00" />
<meta property="article:modified_time" content="2023-11-25T22:28:04+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">人工智能|机器学习——循环神经网络的简洁实现</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h4>循环神经网络的简洁实现</h4> 
<blockquote> 
 <p>如何使用<a href="https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6&amp;spm=1001.2101.3001.7020" title="深度学习框架">深度学习框架</a>的高级API提供的函数更有效地实现相同的语言模型。 我们仍然从读取时光机器数据集开始。</p> 
</blockquote> 
<pre><code class="language-python">import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l
 
batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</code></pre> 
<h4>定义模型</h4> 
<blockquote> 
 <p>高级API提供了循环神经网络的实现。 我们构造一个具有256个隐藏单元的单隐藏层的循环神经网络层<code>rnn_layer</code>。 事实上，我们还没有讨论多层循环神经网络的意义。 现在仅需要将多层理解为一层循环神经网络的输出被用作下一层循环神经网络的输入就足够了。</p> 
</blockquote> 
<pre><code class="language-python">num_hiddens = 256
rnn_layer = nn.RNN(len(vocab), num_hiddens)</code></pre> 
<blockquote> 
 <p>我们使用张量来初始化隐状态，它的形状是（隐藏层数，批量大小，隐藏单元数）。</p> 
</blockquote> 
<pre><code class="language-python">state = torch.zeros((1, batch_size, num_hiddens))
state.shape

torch.Size([1, 32, 256])</code></pre> 
<blockquote> 
 <p>通过一个隐状态和一个输入，我们就可以用更新后的隐状态计算输出。 需要强调的是，<code>rnn_layer</code>的“输出”（<code>Y</code>）不涉及输出层的计算： 它是指每个时间步的隐状态，这些隐状态可以用作后续输出层的输入。</p> 
</blockquote> 
<pre><code class="language-python">X = torch.rand(size=(num_steps, batch_size, len(vocab)))
Y, state_new = rnn_layer(X, state)
Y.shape, state_new.shape

 (torch.Size([35, 32, 256]), torch.Size([1, 32, 256]))</code></pre> 
<blockquote> 
 <p>我们为一个完整的循环神经网络模型定义了一个<code>RNNModel</code>类。 注意，<code>rnn_layer</code>只包含隐藏的循环层，我们还需要创建一个单独的输出层。</p> 
</blockquote> 
<pre><code class="language-python">#@save
class RNNModel(nn.Module):
    """循环神经网络模型"""
    def __init__(self, rnn_layer, vocab_size, **kwargs):
        super(RNNModel, self).__init__(**kwargs)
        self.rnn = rnn_layer
        self.vocab_size = vocab_size
        self.num_hiddens = self.rnn.hidden_size
        # 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1
        if not self.rnn.bidirectional:
            self.num_directions = 1
            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)
        else:
            self.num_directions = 2
            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)
 
    def forward(self, inputs, state):
        X = F.one_hot(inputs.T.long(), self.vocab_size)
        X = X.to(torch.float32)
        Y, state = self.rnn(X, state)
        # 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)
        # 它的输出形状是(时间步数*批量大小,词表大小)。
        output = self.linear(Y.reshape((-1, Y.shape[-1])))
        return output, state
 
    def begin_state(self, device, batch_size=1):
        if not isinstance(self.rnn, nn.LSTM):
            # nn.GRU以张量作为隐状态
            return  torch.zeros((self.num_directions * self.rnn.num_layers,
                                 batch_size, self.num_hiddens),
                                device=device)
        else:
            # nn.LSTM以元组作为隐状态
            return (torch.zeros((
                self.num_directions * self.rnn.num_layers,
                batch_size, self.num_hiddens), device=device),
                    torch.zeros((
                        self.num_directions * self.rnn.num_layers,
                        batch_size, self.num_hiddens), device=device))</code></pre> 
<h4> 训练与预测</h4> 
<blockquote> 
 <p>在训练模型之前，让我们基于一个具有随机权重的模型进行预测。</p> 
</blockquote> 
<pre><code class="language-python">device = d2l.try_gpu()
net = RNNModel(rnn_layer, vocab_size=len(vocab))
net = net.to(device)
d2l.predict_ch8('time traveller', 10, net, vocab, device)</code></pre> 
<blockquote> 
 <p> 很明显，这种模型根本不能输出好的结果。 接下来，我们使用定义的超参数调用<code>train_ch8</code>，并且使用高级API训练模型。 </p> 
</blockquote> 
<pre><code class="language-python">num_epochs, lr = 500, 1
d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, device)</code></pre> 
<blockquote> 
 <p>perplexity 1.3, 404413.8 tokens/sec on cuda:0 time travellerit would be remarkably convenient for the historia travellery of il the hise fupt might and st was it loflers</p> 
</blockquote> 
<p><img alt="" height="592" src="https://images2.imgbox.com/dc/2e/PoE4RVGP_o.png" width="902"></p> 
<blockquote> 
 <p>由于深度学习框架的高级API对代码进行了更多的优化， 该模型在较短的时间内达到了较低的困惑度。  </p> 
</blockquote>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c3438d014b73159894ff7f91819b5aae/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">前端路由hash和history的六大区别</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d1a7f586618763305aace3fb212f62e4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">51单片机从零开始入门教程（DS1302实时时钟篇）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>