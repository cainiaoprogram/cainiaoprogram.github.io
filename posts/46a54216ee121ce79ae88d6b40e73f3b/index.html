<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>[Graph Embedding] DeepWalk 论文笔记 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="[Graph Embedding] DeepWalk 论文笔记" />
<meta property="og:description" content="文章目录 [Graph Embedding] DeepWalkIntroductionRandom WalksGraph Embedding 与 NLPDeepWalk 算法原理总结 [Graph Embedding] DeepWalk Introduction 图表示学习是将图(Graph/Network)中节点在一个低维的连续的向量空间中表示出来，这种表示能够保留节点与邻居的相似性、节点的社区成员属性等。得到节点的向量表示就可以喂给下游的学习模型进行节点分类，边预测等任务。
DeepWalk作为图表示学习的经典方法之一，核心方法就是通过对一定数量的节点做RandomWalk（随机游走），然后将得到的节点序列作为顶点的向量输入表示，通过使用skip-gram model 进行学习，得到最终的表示。下图为对一个空手道俱乐部成员的social relationship Graph表示到一个二维空间。
Random Walks 所谓随机游走（random walk）就是从一个节点开始随机的选择下一个与当前的节点相邻的节点（提前设定随机游走结束的条件），最终形成一条节点序列。截断随机游走（truncated random walks）就是指长度固定的随机游走。使用随机游走的好处有：
并发性。随机游走可以同时在图上的多个节点上同时进行。适应性。由于图的结构是随时间动态变化的，所以当图变化的时候，不需要对所有节点重新进行随机游走，只需要对与图中变化了的子图有关的节点重新进行随机游走得到新的向量表示就可以。
Graph Embedding 与 NLP 作者发现了对graph上的节点进行随机游走的节点频率与NLP（自然语言处理）中word在文章中的出现频率都满足一样的power-law（幂律分布）。如下图所示：
既然网络（Graph/Network）的特征特性与自然语言处理那么相近，那么我们是否可以将NLP中的词向量的模型用在Graph Embedding中，这就是本文的核心观念。
要想了解DeepWalk的算法原理，需要提前了解NLP中Word Embedding 和 Word2Vec的基本内容，并且要理解其中的一种算法——SkipGram。
DeepWalk 算法原理 DeepWalk算法主要由2部分组成，第一部分是产生节点的随机游走序列，第二部分是参数的更新。算法的流程图如下图所示。
第2步是在构造Hierarchical Softmax（层次Softmax）所需的二叉树，算法中使用到Hierarchical Softmax加速算法的学习，这里不多加阐述。第3步是总的迭代次数，共γ次。每一次迭代（第4步到8步）中，对Graph中的每一个节点进行一次random walk，然后将生成的节点序列作为输入向量，输入SkipGram算法中进行参数的学习。
参数更新如下图所示：
第3步是需要优化的函数，即使得当顶点为 v j v_j vj​时，它所在的random walk序列中的 [ j − w : j &#43; w ] [j - w : j &#43; w] [j−w:j&#43;w]中除了 v j v_j vj​的顶点出现的条件概率最大化（这里使用到的是NLP中的SkipGram算法）。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/46a54216ee121ce79ae88d6b40e73f3b/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-10-26T21:32:58+08:00" />
<meta property="article:modified_time" content="2020-10-26T21:32:58+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">[Graph Embedding] DeepWalk 论文笔记</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#Graph_Embedding_DeepWalk_1" rel="nofollow">[Graph Embedding] DeepWalk</a></li><li><ul><li><a href="#Introduction_3" rel="nofollow">Introduction</a></li><li><a href="#Random_Walks_12" rel="nofollow">Random Walks</a></li><li><a href="#Graph_Embedding__NLP_19" rel="nofollow">Graph Embedding 与 NLP</a></li><li><a href="#DeepWalk__30" rel="nofollow">DeepWalk 算法原理</a></li><li><a href="#_50" rel="nofollow">总结</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="Graph_Embedding_DeepWalk_1"></a>[Graph Embedding] DeepWalk</h2> 
<h3><a id="Introduction_3"></a>Introduction</h3> 
<p>图表示学习是将图(Graph/Network)中节点在一个低维的连续的向量空间中表示出来，这种表示能够保留节点与邻居的相似性、节点的社区成员属性等。得到节点的向量表示就可以喂给下游的学习模型进行节点分类，边预测等任务。</p> 
<p>DeepWalk作为图表示学习的经典方法之一，核心方法就是通过对一定数量的节点做RandomWalk（随机游走），然后将得到的节点序列作为顶点的向量输入表示，通过使用skip-gram model 进行学习，得到最终的表示。下图为对一个空手道俱乐部成员的social relationship Graph表示到一个二维空间。</p> 
<p><img src="https://images2.imgbox.com/ed/a5/OJmjhZw4_o.png" alt="在这里插入图片描述"><br>  </p> 
<h3><a id="Random_Walks_12"></a>Random Walks</h3> 
<p>所谓随机游走（random walk）就是从一个节点开始随机的选择下一个与当前的节点相邻的节点（提前设定随机游走结束的条件），最终形成一条节点序列。截断随机游走（truncated random walks）就是指长度固定的随机游走。使用随机游走的好处有：</p> 
<ol><li><strong>并发性</strong>。随机游走可以同时在图上的多个节点上同时进行。</li><li><strong>适应性</strong>。由于图的结构是随时间动态变化的，所以当图变化的时候，不需要对所有节点重新进行随机游走，只需要对与图中变化了的子图有关的节点重新进行随机游走得到新的向量表示就可以。<br>  </li></ol> 
<h3><a id="Graph_Embedding__NLP_19"></a>Graph Embedding 与 NLP</h3> 
<p>作者发现了对graph上的节点进行随机游走的节点频率与NLP（自然语言处理）中word在文章中的出现频率都满足一样的power-law（幂律分布）。如下图所示：</p> 
<p><img src="https://images2.imgbox.com/e1/16/kF09TvSF_o.png" alt="在这里插入图片描述"></p> 
<p>既然网络（Graph/Network）的特征特性与自然语言处理那么相近，那么我们是否可以将NLP中的词向量的模型用在Graph Embedding中，这就是本文的核心观念。</p> 
<p>要想了解DeepWalk的算法原理，需要提前了解NLP中Word Embedding 和 Word2Vec的基本内容，并且要理解其中的一种算法——SkipGram。<br>  </p> 
<h3><a id="DeepWalk__30"></a>DeepWalk 算法原理</h3> 
<p>DeepWalk算法主要由2部分组成，第一部分是产生节点的随机游走序列，第二部分是参数的更新。算法的流程图如下图所示。</p> 
<p><img src="https://images2.imgbox.com/a3/a1/QJCE3l4h_o.png" alt="在这里插入图片描述"></p> 
<p>第2步是在构造Hierarchical Softmax（层次Softmax）所需的二叉树，算法中使用到Hierarchical Softmax加速算法的学习，这里不多加阐述。第3步是总的迭代次数，共γ次。每一次迭代（第4步到8步）中，对Graph中的每一个节点进行一次random walk，然后将生成的节点序列作为输入向量，输入SkipGram算法中进行参数的学习。</p> 
<p>参数更新如下图所示：</p> 
<p><img src="https://images2.imgbox.com/8c/f8/9vaDUsQn_o.png" alt="在这里插入图片描述"></p> 
<p>第3步是需要优化的函数，即使得当顶点为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          v 
         
        
          j 
         
        
       
      
        v_j 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.716668em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span></span></span></span></span>时，它所在的random walk序列中的<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         [ 
        
       
         j 
        
       
         − 
        
       
         w 
        
       
         : 
        
       
         j 
        
       
         + 
        
       
         w 
        
       
         ] 
        
       
      
        [j - w : j + w] 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault" style="margin-right: 0.05724em;">j</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.85396em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.05724em;">j</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="mclose">]</span></span></span></span></span>中除了<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          v 
         
        
          j 
         
        
       
      
        v_j 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.716668em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span></span></span></span></span>的顶点出现的条件概率最大化（这里使用到的是NLP中的SkipGram算法）。</p> 
<p>第4步采用SGD（随机梯度下降）进行参数的迭代更新.</p> 
<p>在计算<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          P 
         
        
          r 
         
        
       
         ( 
        
        
        
          u 
         
        
          k 
         
        
       
         ∣ 
        
       
         Φ 
        
       
         ( 
        
        
        
          v 
         
        
          j 
         
        
       
         ) 
        
       
         ) 
        
       
      
        P_r(u_k | \Phi(v_j)) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.03611em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: -0.13889em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">u</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord">∣</span><span class="mord">Φ</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span>时，为了减少算法的时间复杂度，作者使用到的是Hierarchical Softmax（层次Softmax），这也是NLP中词向量用到的一个重要方法，这里不做过多阐述，如果感兴趣，可以阅读原论文4.2.2节。<br>  </p> 
<h3><a id="_50"></a>总结</h3> 
<p>最后用一个图来形象概括DeepWalk的整个算法流程（引自阿里论文）。<br> <img src="https://images2.imgbox.com/91/6b/ZmalVUE1_o.png" alt="在这里插入图片描述"></p> 
<p> <br> <strong>论文原文</strong>：<a href="http://www.perozzi.net/publications/14_kdd_deepwalk.pdf" rel="nofollow">DeepWalk: Online Learning of Social Representations</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e48784f7caa5af0c91f2aaaf2a3deff3/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">五、Python复习教程（重点）-爬虫框架实战</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/5c2fd61c264289ba140b496ec25d3f94/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">简单登录&#43;注册&#43;验证码页面 前后端交互（前端加后端 AJAX局部判断 DRuid连接池 MySQL DBUtils）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>