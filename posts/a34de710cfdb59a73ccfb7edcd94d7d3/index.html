<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>深度学习--面试题目 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="深度学习--面试题目" />
<meta property="og:description" content="● BatchNormalization的作用
参考回答：
神经网络在训练的时候随着网络层数的加深,激活函数的输入值的整体分布逐渐往激活函数的取值区间上下限靠近,从而导致在反向传播时低层的神经网络的梯度消失。而BatchNormalization的作用是通过规范化的手段,将越来越偏的分布拉回到标准化的分布,使得激活函数的输入值落在激活函数对输入比较敏感的区域,从而使梯度变大,加快学习收敛速度,避免梯度消失的问题。
● 梯度消失
参考回答：
在神经网络中，当前面隐藏层的学习速率低于后面隐藏层的学习速率，即随着隐藏层数目的增加，分类准确率反而下降了。这种现象叫做消失的梯度问题。
● 循环神经网络，为什么好?
参考回答：
循环神经网络模型（RNN）是一种节点定向连接成环的人工神经网络，是一种反馈神经网络，RNN利用内部的记忆来处理任意时序的输入序列，并且在其处理单元之间既有内部的反馈连接又有前馈连接，这使得RNN可以更加容易处理不分段的文本等。
● 什么是Group Convolution
参考回答：
若卷积神将网络的上一层有N个卷积核,则对应的通道数也为N。设群数目为M,在进行卷积操作的时候,将通道分成M份,每个group对应N/M个通道,然后每个group卷积完成后输出叠在一起,作为当前层的输出通道。
● 什么是RNN
参考回答：
一个序列当前的输出与前面的输出也有关,在RNN网络结构中中,隐藏层的输入不仅包括输入层的输出还包含上一时刻隐藏层的输出,网络会对之前的信息进行记忆并应用于当前的输入计算中。
● 训练过程中,若一个模型不收敛,那么是否说明这个模型无效?导致模型不收敛的原因有哪些?
参考回答：
并不能说明这个模型无效,导致模型不收敛的原因可能有数据分类的标注不准确,样本的信息量太大导致模型不足以fit整个样本空间。学习率设置的太大容易产生震荡,太小会导致不收敛。可能复杂的分类任务用了简单的模型。数据没有进行归一化的操作。
● 图像处理中锐化和平滑的操作
参考回答：
锐化就是通过增强高频分量来减少图像中的模糊,在增强图像边缘的同时也增加了图像的噪声。
平滑与锐化相反,过滤掉高频分量,减少图像的噪声是图片变得模糊。
● VGG使用33卷积核的优势是什么?
参考回答：
2个33的卷积核串联和55的卷积核有相同的感知野,前者拥有更少的参数。多个33的卷积核比一个较大尺寸的卷积核有更多层的非线性函数,增加了非线性表达,使判决函数更具有判决性。
● Relu比Sigmoid的效果好在哪里?
参考回答：
Sigmoid的导数只有在0的附近时有较好的激活性,而在正负饱和区域的梯度趋向于0,从而产生梯度弥散的现象,而relu在大于0的部分梯度为常数,所以不会有梯度弥散现象。Relu的导数计算的更快。Relu在负半区的导数为0,所以神经元激活值为负时,梯度为0,此神经元不参与训练,具有稀疏性。
● 问题：神经网络中权重共享的是？
参考回答：
卷积神经网络、循环神经网络
解析：通过网络结构直接解释
● 问题：神经网络激活函数？
参考回答：
sigmod、tanh、relu
解析：需要掌握函数图像，特点，互相比较，优缺点以及改进方法
● 问题：在深度学习中，通常会finetuning已有的成熟模型，再基于新数据，修改最后几层神经网络权值，为什么？
参考回答：
实践中的数据集质量参差不齐，可以使用训练好的网络来进行提取特征。把训练好的网络当做特征提取器。
● 问题：画GRU结构图
参考回答：
GRU有两个门：更新门，输出门
解析：如果不会画GRU，可以画LSTM或者RNN。再或者可以讲解GRU与其他两个网络的联系和区别。不要直接就说不会。
● Attention机制的作用
参考回答：
减少处理高维输入数据的计算负担,结构化的选取输入的子集,从而降低数据的维度。让系统更加容易的找到输入的数据中与当前输出信息相关的有用信息,从而提高输出的质量。帮助类似于decoder这样的模型框架更好的学到多种内容模态之间的相互关系。
● Lstm和Gru的原理
参考回答：
Lstm由输入门,遗忘门,输出门和一个cell组成。第一步是决定从cell状态中丢弃什么信息,然后在决定有多少新的信息进入到cell状态中,最终基于目前的cell状态决定输出什么样的信息。
Gru由重置门和跟新门组成,其输入为前一时刻隐藏层的输出和当前的输入,输出为下一时刻隐藏层的信息。重置门用来计算候选隐藏层的输出,其作用是控制保留多少前一时刻的隐藏层。跟新门的作用是控制加入多少候选隐藏层的输出信息,从而得到当前隐藏层的输出。
● 什么是dropout
参考回答：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/a34de710cfdb59a73ccfb7edcd94d7d3/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-02-26T15:45:02+08:00" />
<meta property="article:modified_time" content="2020-02-26T15:45:02+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深度学习--面试题目</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>● BatchNormalization的作用<br> <em>参考回答：</em><br> 神经网络在训练的时候随着网络层数的加深,激活函数的输入值的整体分布逐渐往激活函数的取值区间上下限靠近,从而导致在反向传播时低层的神经网络的梯度消失。而BatchNormalization的作用是通过规范化的手段,将越来越偏的分布拉回到标准化的分布,使得激活函数的输入值落在激活函数对输入比较敏感的区域,从而使梯度变大,加快学习收敛速度,避免梯度消失的问题。</p> 
<p>● 梯度消失<br> <em>参考回答：</em><br> 在神经网络中，当前面隐藏层的学习速率低于后面隐藏层的学习速率，即随着隐藏层数目的增加，分类准确率反而下降了。这种现象叫做消失的梯度问题。</p> 
<p>● 循环神经网络，为什么好?<br> <em>参考回答：</em><br> 循环神经网络模型（RNN）是一种节点定向连接成环的人工神经网络，是一种反馈神经网络，RNN利用内部的记忆来处理任意时序的输入序列，并且在其处理单元之间既有内部的反馈连接又有前馈连接，这使得RNN可以更加容易处理不分段的文本等。</p> 
<p>● 什么是Group Convolution<br> <em>参考回答：</em><br> 若卷积神将网络的上一层有N个卷积核,则对应的通道数也为N。设群数目为M,在进行卷积操作的时候,将通道分成M份,每个group对应N/M个通道,然后每个group卷积完成后输出叠在一起,作为当前层的输出通道。</p> 
<p>● 什么是RNN<br> <em>参考回答：</em><br> 一个序列当前的输出与前面的输出也有关,在RNN网络结构中中,隐藏层的输入不仅包括输入层的输出还包含上一时刻隐藏层的输出,网络会对之前的信息进行记忆并应用于当前的输入计算中。</p> 
<p>● 训练过程中,若一个模型不收敛,那么是否说明这个模型无效?导致模型不收敛的原因有哪些?<br> <em>参考回答：</em><br> 并不能说明这个模型无效,导致模型不收敛的原因可能有数据分类的标注不准确,样本的信息量太大导致模型不足以fit整个样本空间。学习率设置的太大容易产生震荡,太小会导致不收敛。可能复杂的分类任务用了简单的模型。数据没有进行归一化的操作。</p> 
<p>● 图像处理中锐化和平滑的操作<br> <em>参考回答：</em><br> 锐化就是通过增强高频分量来减少图像中的模糊,在增强图像边缘的同时也增加了图像的噪声。<br> 平滑与锐化相反,过滤掉高频分量,减少图像的噪声是图片变得模糊。</p> 
<p>● VGG使用3<em>3卷积核的优势是什么?<br> <em>参考回答：</em><br> 2个3</em>3的卷积核串联和5<em>5的卷积核有相同的感知野,前者拥有更少的参数。多个3</em>3的卷积核比一个较大尺寸的卷积核有更多层的非线性函数,增加了非线性表达,使判决函数更具有判决性。</p> 
<p>● Relu比Sigmoid的效果好在哪里?<br> <em>参考回答：</em><br> Sigmoid的导数只有在0的附近时有较好的激活性,而在正负饱和区域的梯度趋向于0,从而产生梯度弥散的现象,而relu在大于0的部分梯度为常数,所以不会有梯度弥散现象。Relu的导数计算的更快。Relu在负半区的导数为0,所以神经元激活值为负时,梯度为0,此神经元不参与训练,具有稀疏性。</p> 
<p>● 问题：神经网络中权重共享的是？<br> <em>参考回答：</em><br> 卷积神经网络、循环神经网络<br> 解析：通过网络结构直接解释</p> 
<p>● 问题：神经网络激活函数？<br> <em>参考回答：</em><br> sigmod、tanh、relu<br> 解析：需要掌握函数图像，特点，互相比较，优缺点以及改进方法</p> 
<p>● 问题：在深度学习中，通常会finetuning已有的成熟模型，再基于新数据，修改最后几层神经网络权值，为什么？<br> <em>参考回答：</em><br> 实践中的数据集质量参差不齐，可以使用训练好的网络来进行提取特征。把训练好的网络当做特征提取器。</p> 
<p>● 问题：画GRU结构图<br> <em>参考回答：</em><img src="https://images2.imgbox.com/44/5e/xr09vS4w_o.png" alt="在这里插入图片描述"><br> GRU有两个门：更新门，输出门<br> 解析：如果不会画GRU，可以画LSTM或者RNN。再或者可以讲解GRU与其他两个网络的联系和区别。不要直接就说不会。</p> 
<p>● Attention机制的作用<br> <em>参考回答：</em><br> 减少处理高维输入数据的计算负担,结构化的选取输入的子集,从而降低数据的维度。让系统更加容易的找到输入的数据中与当前输出信息相关的有用信息,从而提高输出的质量。帮助类似于decoder这样的模型框架更好的学到多种内容模态之间的相互关系。</p> 
<p>● Lstm和Gru的原理<br> <em>参考回答：</em><br> Lstm由输入门,遗忘门,输出门和一个cell组成。第一步是决定从cell状态中丢弃什么信息,然后在决定有多少新的信息进入到cell状态中,最终基于目前的cell状态决定输出什么样的信息。<br> Gru由重置门和跟新门组成,其输入为前一时刻隐藏层的输出和当前的输入,输出为下一时刻隐藏层的信息。重置门用来计算候选隐藏层的输出,其作用是控制保留多少前一时刻的隐藏层。跟新门的作用是控制加入多少候选隐藏层的输出信息,从而得到当前隐藏层的输出。</p> 
<p>● 什么是dropout<br> <em>参考回答：</em><br> 在神经网络的训练过程中,对于神经单元按一定的概率将其随机从网络中丢弃,从而达到对于每个mini-batch都是在训练不同网络的效果,防止过拟合。</p> 
<p>● LSTM每个门的计算公式<br> <em>参考回答：</em></p> 
<p>遗忘门:<img src="https://images2.imgbox.com/55/c8/CLVX1fkM_o.png" alt="在这里插入图片描述"></p> 
<p>输入门:<img src="https://images2.imgbox.com/99/4d/9GgR9ze9_o.png" alt="在这里插入图片描述"></p> 
<p>输出门:<img src="https://images2.imgbox.com/c3/ea/ZI417p9s_o.png" alt="在这里插入图片描述"></p> 
<p>● DropConnect的原理<br> <em>参考回答：</em><br> 防止过拟合方法的一种,与dropout不同的是,它不是按概率将隐藏层的节点输出清0,而是对每个节点与之相连的输入权值以一定的概率清0。</p> 
<p>● 深度学习了解多少，有看过底层代码吗？caffe,tf?</p> 
<p>● 除了GMM-HMM，你了解深度学习在语音识别中的应用吗？<br> <em>参考回答：</em><br> 讲了我用的过DNN-HMM，以及与GMM-HMM的联系与区别；然后RNN+CTC，这里我只是了解，大概讲了一下CTC损失的原理；然后提了一下CNN+LSTM。</p> 
<p>● 用过哪些移动端深度学习框架？<br> <em>参考回答：</em><br> 开源的有：小米的MACE，骁龙的SNPE，腾讯的FeatherCNN和ncnn，百度的mobile-deep-learning(MDL)；caffe、tensorflow lite都有移动端，只是可能没有上面的框架效率高。据传还有支付宝的xNN，商汤的PPL，不过都是自用，未开源。</p> 
<p>● Caffe：整体架构说一下，新加一个层需要哪些步骤，卷积是怎么实现的，多卡机制，数据并行还是模型并行？<br> <em>参考回答：</em><br> Caffe是深度学习的一个框架，Caffe框架主要包括五个组件：Blob、Solver、Net、Layer、Proto；框架结构如下图所示。这五大组件可以分为两个部分：第一部分，Blob、Layer和Net，这三个组件使得Caffe构成基于自己的模块化的模型，caffe是逐层地定义一个net，而net是从数据输入层到损失曾自下而上定义整个模型，Blob在caffe中是处理和传递实际数据的数据封装包；第二部分：Solver和Proto，这两个模型分别用于协调模型的优化以及用于网络模型的结构定义、存储和读取的方式（Layer-By-Layer）定义Net，而贯穿所有Nets的结构就是caffe框架或模型；对于Layer而言，输入的是Blob数据封装包格式的实际数据，当采用该框架进行训练时，也就是Solver调优模型，则需要Proto这种网络模型的结构定义、存储和读取。<br> 总体来说，caffe是通过Layer</p> 
<p>Caffe中卷积运算的原理</p> 
<p>俗话说，一图胜千言，首先先给出原理示意图，为了方便理解，这里以二维核为例<br> <img src="https://images2.imgbox.com/8c/a6/oq7uAmPJ_o.png" alt="在这里插入图片描述"><br> 滑动窗口在图像中每滑动一个地方，将图像中该滑动窗口图像展开为一列，所有列组成图中的滑动窗口矩阵，这里假设pad=1,stride=1,K=3,则滑动窗口矩阵每行大小为W<em>H,一共K</em>K行.</p> 
<p>每个核展开为一行，N个核形成的核矩阵大小为N<em>K</em>K。</p> 
<p>最后将核矩阵和滑动窗口矩阵相乘，每一行就是一个特征图，N个卷积核形成N个特征图。</p> 
<p>扩展到三维核<br> <img src="https://images2.imgbox.com/23/6f/1o2oh86H_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/f6/54/P2Zjgnk3_o.png" alt="在这里插入图片描述"><br> 三维核就是多了一个通道的概念，原理与二维核一样。<br> caffe支持多GPU并行了，原理比较简单，就是每个GPU分别算一个batch，n个GPU，实际的batchsize就是n*batch，比如原来用一个GPU，batchsize设置成256，现在用4个GPU，把batchsize设置成64，和原来的一个GPU的运算是等价的。</p> 
<p>实际使用的时候基本不用设置，和原来一样编译好就可以用了。命令就是在-gpu 后面对多个GPU号用逗号隔开，比如-gpu 1,2,3,4 就是同时使用1-4共4个GPU，GPU编号可以不连续，或者直接用-gpu all，就是使用所有的GPU。</p> 
<p>Caffe是数据并行的。</p> 
<p>● BN层的作用，为什么要在后面加伽马和贝塔，不加可以吗<br> <em>参考回答：</em><br> BN层的作用是把一个batch内的所有数据，从不规范的分布拉到正态分布。这样做的好处是使得数据能够分布在激活函数的敏感区域，敏感区域即为梯度较大的区域，因此在反向传播的时候能够较快反馈误差传播。</p> 
<p>● 梯度消失，梯度爆炸的问题，<br> <em>参考回答：</em><br> 激活函数的原因，由于梯度求导的过程中梯度非常小，无法有效反向传播误差，造成梯度消失的问题。</p> 
<p>● Adam<br> <em>参考回答：</em><br> Adam 算法和传统的随机梯度下降不同。随机梯度下降保持单一的学习率（即 alpha）更新所有的权重，学习率在训练过程中并不会改变。而 Adam 通过计算梯度的一阶矩估计和二阶矩估计而为不同的参数设计独立的自适应性学习率。</p> 
<p>● attention机制<br> <em>参考回答：</em><br> Attention简单理解就是权重分配，。以seq2seq中的attention公式作为讲解。就是对输入的每个词分配一个权重，权重的计算方式为与解码端的隐含层时刻作比较，得到的权重的意义就是权重越大，该词越重要。最终加权求和。</p> 
<p>● RNN梯度消失问题,为什么LSTM和GRU可以解决此问题<br> <em>参考回答：</em><br> RNN由于网络较深,后面层的输出误差很难影响到前面层的计算,RNN的某一单元主要受它附近单元的影响。而LSTM因为可以通过阀门记忆一些长期的信息,相应的也就保留了更多的梯度。而GRU也可通过重置和更新两个阀门保留长期的记忆,也相对解决了梯度消失的问题。</p> 
<p>● GAN网络的思想<br> <em>参考回答：</em><br> GAN用一个生成模型和一个判别模型,判别模型用于判断给定的图片是不是真实的图片,生成模型自己生成一张图片和想要的图片很像,开始时两个模型都没有训练,然后两个模型一起进行对抗训练,生成模型产生图片去欺骗判别模型,判别模型去判别真假,最终两个模型在训练过程中,能力越来越强最终达到稳态。</p> 
<p>● 1*1的卷积作用<br> <em>参考回答：</em><br> 实现跨通道的交互和信息整合,实现卷积核通道数的降维和升维,可以实现多个feature map的线性组合,而且可是实现与全连接层的等价效果。</p> 
<p>● 怎么提升网络的泛化能力<br> <em>参考回答：</em><br> 从数据上提升性能:收集更多的数据,对数据做缩放和变换,特征组合和重新定义问题。<br> 从算法调优上提升性能:用可靠的模型诊断工具对模型进行诊断,权重的初始化,用小的随机数初始化权重。对学习率进行调节,尝试选择合适的激活函数,调整网络的拓扑结构,调节batch和epoch的大小,添加正则化的方法,尝试使用其它的优化方法,使用early stopping。</p> 
<p>● 什么是seq2seq model<br> <em>参考回答：</em><br> Seq2seq属于encoder-decoder结构的一种,利用两个RNN,一个作为encoder一个作为decoder。Encoder负责将输入序列压缩成指定长度的向量,这个向量可以看作这段序列的语义,而decoder负责根据语义向量生成指定的序列。</p> 
<p>● 激活函数的作用<br> <em>参考回答：</em><br> 激活函数是用来加入非线性因素的,提高神经网络对模型的表达能力,解决线性模型所不能解决的问题。</p> 
<p>● 为什么用relu就不用sigmoid了<br> <em>参考回答：</em><br> Sigmoid的导数只有在0的附近时有比较好的激活性,在正负饱和区域的梯度都接近0，会导致梯度弥散。而relu函数在大于0的部分梯度为常数,不会产生梯度弥散现象。Relu函数在负半区导数为0,也就是说这个神经元不会经历训练,就是所谓稀疏性。而且relu函数的导数计算的更快。</p> 
<p>● 讲一下基于WFST的静态解码网络的语音识别流程？<br> <em>参考回答：</em><br> 从语音特征开始讲起，我讲了MFCC和LPC的原理以及提取过程，这一部分讲的很细，然后讲了viterbi解码过程，最后概述了一下HCLG.fst构建流程</p> 
<p>● 目标检测了解吗，Faster RCNN跟RCNN有什么区别<br> <em>参考回答：</em><br> 目标检测，也叫目标提取，是一种基于目标几何和统计特征的图像分割，它将目标的分割和识别合二为一，其准确性和实时性是整个系统的一项重要能力。尤其是在复杂场景中，需要对多个目标进行实时处理时，目标自动提取和识别就显得特别重要。<br> 随着计算机技术的发展和计算机视觉原理的广泛应用，利用计算机图像处理技术对目标进行实时跟踪研究越来越热门，对目标进行动态实时跟踪定位在智能化交通系统、智能监控系统、军事目标检测及医学导航手术中手术器械定位等方面具有广泛的应用价值。</p> 
<table><thead><tr><th></th><th>使用方法</th><th>缺点</th><th>改进</th></tr></thead><tbody><tr><td>CNN</td><td>1、SS提取RP ；2、CNN提取特征；3、SVM分类；4、BB盒回归</td><td>1、 训练步骤繁琐（微调网络+训练SVM+训练bbox）；2、训练、测试均速度慢 ；3、训练占空间</td><td>1、 从DPM HSC的34.3%直接提升到了66%（mAP）；2、引入RP+CNN</td></tr><tr><td>Faster R-CNN</td><td>1、RPN提取RP；2、CNN提取特征；3、softmax分类；4、多任务损失函数边框回归。</td><td>1、 还是无法达到实时检测目标；2、 获取region proposal，再对每个proposal分类计算量还是比较大。</td><td>1、 提高了检测精度和速度；2、 真正实现端到端的目标检测框架；3、 生成建议框仅需约10ms。</td></tr></tbody></table> 
<p>● SPP，YOLO了解吗？<br> <em>参考回答：</em><br> SPP-Net简介：<br> SPP-Net主要改进有下面两个：<br> 1）.共享卷积计算、2）.空间金字塔池化<br> 在SPP-Net中同样由这几个部分组成：<br> ss算法、CNN网络、SVM分类器、bounding box<br> ss算法的区域建议框同样在原图上生成，但是却在Conv5上提取，当然由于尺寸的变化，在Conv5层上提取时要经过尺度变换，这是它R-CNN最大的不同，也是SPP-Net能够大幅缩短时长的原因。因为它充分利用了卷积计算，也就是每张图片只卷积一次，但是这种改进带来了一个新的问题，由于ss算法生成的推荐框尺度是不一致的，所以在cov5上提取到的特征尺度也是不一致的，这样是没有办法做全尺寸卷积的（Alexnet）。<br> 所以SPP-Net需要一种算法，这种算法能够把不一致的输入产生统一的输出，这就SPP，即空间金字塔池化，由它替换R-CNN中的pooling层，除此之外，它和R-CNN就一样了。<br> YOLO详解：<br> YOLO的名字You only look once正是自身特点的高度概括。YOLO的核心思想在于将目标检测作为回归问题解决 ，YOLO首先将图片划分成SxS个区域，注意这个区域的概念不同于上文提及将图片划分成N个区域扔进detector这里的区域不同。上文提及的区域是真的将图片进行剪裁，或者说把图片的某个局部的像素扔进detector，而这里的划分区域，只的是逻辑上的划分。</p> 
<p>● 梯度消失梯度爆炸怎么解决<br> <em>参考回答：</em><br> 1）、使用 ReLU、LReLU、ELU、maxout 等激活函数<br> sigmoid函数的梯度随着x的增大或减小和消失，而ReLU不会。<br> 2）、使用批规范化<br> 通过规范化操作将输出信号x规范化到均值为0，方差为1保证网络的稳定性。从上述分析分可以看到，反向传播式子中有w的存在，所以w的大小影响了梯度的消失和爆炸，Batch Normalization 就是通过对每一层的输出规范为均值和方差一致的方法，消除了w带来的放大缩小的影响，进而解决梯度消失和爆炸的问题。</p> 
<p>● RNN容易梯度消失，怎么解决？<br> <em>参考回答：</em><br> 1）、梯度裁剪（Clipping Gradient）<br> 既然在BP过程中会产生梯度消失（就是偏导无限接近0，导致长时记忆无法更新），那么最简单粗暴的方法，设定阈值，当梯度小于阈值时，更新的梯度为阈值。<br> 优点：简单粗暴<br> 缺点：很难找到满意的阈值<br> 2）、LSTM（Long Short-Term Memory）<br> 一定程度上模仿了长时记忆，相比于梯度裁剪，最大的优点就是，自动学习在什么时候可以将error反向传播，自动控制哪些是需要作为记忆存储在LSTM cell中。一般长时记忆模型包括写入，读取，和忘记三个过程对应到LSTM中就变成了input_gate,output_gate,forget_gate,三个门，范围在0到1之间，相当于对输入输出进行加权的学习，利用大量数据来自动学习加权的参数（即学习了哪些错误可以用BP更新参数）。具体的公式表达：<br> <img src="https://images2.imgbox.com/fb/8a/F3ineohp_o.png" alt="在这里插入图片描述"><br> 优点：模型自动学习更新参数</p> 
<p>● LSTM跟RNN有啥区别<br> <em>参考回答：</em><br> LSTM与RNN的比较<br> RNN在处理long term memory的时候存在缺陷，因此LSTM应运而生。LSTM是一种变种的RNN，它的精髓在于引入了细胞状态这样一个概念，不同于RNN只考虑最近的状态，LSTM的细胞状态会决定哪些状态应该被留下来，哪些状态应该被遗忘。<br> 下面来看一些RNN和LSTM内部结构的不同：<br> RNN<br> <img src="https://images2.imgbox.com/dd/0e/UI9f2CMw_o.png" alt="在这里插入图片描述"><br> LSTM<br> 由上面两幅图可以观察到，LSTM结构更为复杂，在RNN中，将过去的输出和当前的输入concatenate到一起，通过tanh来控制两者的输出，它只考虑最近时刻的状态。在RNN中有两个输入和一个输出。</p> 
<p>而LSTM为了能记住长期的状态，在RNN的基础上增加了一路输入和一路输出，增加的这一路就是细胞状态，也就是途中最上面的一条通路。事实上整个LSTM分成了三个部分：</p> 
<p>1）哪些细胞状态应该被遗忘</p> 
<p>2）哪些新的状态应该被加入</p> 
<p>3）根据当前的状态和现在的输入，输出应该是什么</p> 
<p>下面来分别讨论：</p> 
<p>1）哪些细胞状态应该被遗忘</p> 
<p>这部分功能是通过sigmoid函数实现的，也就是最左边的通路。根据输入和上一时刻的输出来决定当前细胞状态是否有需要被遗忘的内容。举个例子，如果之前细胞状态中有主语，而输入中又有了主语，那么原来存在的主语就应该被遗忘。concatenate的输入和上一时刻的输出经过sigmoid函数后，越接近于0被遗忘的越多，越接近于1被遗忘的越少。</p> 
<p>2）哪些新的状态应该被加入</p> 
<p>继续上面的例子，新进来的主语自然就是应该被加入到细胞状态的内容，同理也是靠sigmoid函数来决定应该记住哪些内容。但是值得一提的是，需要被记住的内容并不是直接</p> 
<p>concatenate的输入和上一时刻的输出，还要经过tanh，这点应该也是和RNN保持一致。并且需要注意，此处的sigmoid和前一步的sigmoid层的w和b不同，是分别训练的层。</p> 
<p>细胞状态在忘记了该忘记的，记住了该记住的之后，就可以作为下一时刻的细胞状态输入了。</p> 
<p>3）根据当前的状态和现在的输入，输出应该是什么</p> 
<p>这是最右侧的通路，也是通过sigmoid函数做门，对第二步求得的状态做tanh后的结果过滤，从而得到最终的预测结果。</p> 
<p>事实上，LSTM就是在RNN的基础上，增加了对过去状态的过滤，从而可以选择哪些状态对当前更有影响，而不是简单的选择最近的状态。</p> 
<p>在这之后，研究人员们实现了各种LSTM的变种网络。不变的是，通常都会用sigmoid函数做门，筛选状态或者输入。并且输出都是要经过tanh函数。具体为什么要用这两个函数，由于刚接触还不能给出一定的解释，日后理解了再补充。</p> 
<p>● 卷积层和池化层有什么区别</p> 
<table><thead><tr><th></th><th>卷积层</th><th>池化层</th></tr></thead><tbody><tr><td>功能</td><td>提取特征</td><td>压缩特征图，提取主要特征</td></tr><tr><td>操作</td><td>可惜是二维的，对于三维数据比如RGB图像（3通道），卷积核的深度必须同输入的通道数，输出的通道数等于卷积核的个数。卷积操作会改变输入特征图的通道数。</td><td>池化只是在二维数据上操作的，因此不改变输入的通道数。对于多通道的输入，这一点和卷积区别很大。</td></tr><tr><td>特性</td><td>权值共享：减少了参数的数量，并利用了图像目标的位置无关性。稀疏连接：输出的每个值只依赖于输入的部分值。</td><td></td></tr></tbody></table> 
<p>● 防止过拟合有哪些方法<br> <em>参考回答：</em><br> 1）Dropout ；2）加L1/L2正则化；3）BatchNormalization ；4）网络bagging</p> 
<p>● dropout咋回事讲讲<br> <em>参考回答：</em><br> Dropout的目标是在指数 级数量的神经网络上近似这个过程。Dropout训练与Bagging训练不太一样。在Bagging的情况下,所有模型是独立的。<br> 在Dropout的情况下,模型是共享参数的,其中每个模型继承的父神经网络参 数的不同子集。参数共享使得在有限可用的内存下代表指数数量的模型变得可能。 在Bagging的情况下,每一个模型在其相应训练集上训练到收敛。<br> 在Dropout的情况下,通常大部分模型都没有显式地被训练,通常该模型很大,以致到宇宙毁灭都不 能采样所有可能的子网络。取而代之的是,可能的子网络的一小部分训练单个步骤,参数共享导致剩余的子网络能有好的参数设定。</p> 
<p>● relu<br> <em>参考回答：</em><br> 在深度神经网络中，通常使用一种叫修正线性单元(Rectified linear unit，ReLU）作为神经元的激活函数。ReLU起源于神经科学的研究：2001年，Dayan、Abott从生物学角度模拟出了脑神经元接受信号更精确的激活模型，如下图：<br> <img src="https://images2.imgbox.com/3d/fe/Hdac3RVh_o.png" alt="在这里插入图片描述"><br> 其中横轴是时间(ms)，纵轴是神经元的放电速率(Firing Rate)。同年，Attwell等神经科学家通过研究大脑的能量消耗过程，推测神经元的工作方式具有稀疏性和分布性；2003年Lennie等神经科学家估测大脑同时被激活的神经元只有1~4%，这进一步表明了神经元的工作稀疏性。而对于ReLU函数而言，类似表现是如何体现的？其相比于其他线性函数(如purlin)和非线性函数(如sigmoid、双曲正切)又有何优势？下面请各位看官容我慢慢道来。</p> 
<p>首先，我们来看一下ReLU激活函数的形式，如下图：<br> <img src="https://images2.imgbox.com/84/e7/fGEJ9nRg_o.png" alt="在这里插入图片描述"><br> 从上图不难看出，ReLU函数其实是分段线性函数，把所有的负值都变为0，而正值不变，这种操作被成为单侧抑制。可别小看这个简单的操作，正因为有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。尤其体现在深度神经网络模型(如CNN)中，当模型增加N层之后，理论上ReLU神经元的激活率将降低2的N次方倍。这里或许有童鞋会问：ReLU的函数图像为什么一定要长这样？反过来，或者朝下延伸行不行？其实还不一定要长这样。只要能起到单侧抑制的作用，无论是镜面翻转还是180度翻转，最终神经元的输出也只是相当于加上了一个常数项系数，并不影响模型的训练结果。之所以这样定，或许是为了契合生物学角度，便于我们理解吧。</p> 
<p>那么问题来了：这种稀疏性有何作用？换句话说，我们为什么需要让神经元稀疏？不妨举栗子来说明。当看名侦探柯南的时候，我们可以根据故事情节进行思考和推理，这时用到的是我们的大脑左半球；而当看蒙面唱将时，我们可以跟着歌手一起哼唱，这时用到的则是我们的右半球。左半球侧重理性思维，而右半球侧重感性思维。也就是说，当我们在进行运算或者欣赏时，都会有一部分神经元处于激活或是抑制状态，可以说是各司其职。再比如，生病了去医院看病，检查报告里面上百项指标，但跟病情相关的通常只有那么几个。与之类似，当训练一个深度分类模型的时候，和目标相关的特征往往也就那么几个，因此通过ReLU实现稀疏后的模型能够更好地挖掘相关特征，拟合训练数据。</p> 
<p>此外，相比于其它激活函数来说，ReLU有以下优势：对于线性函数而言，ReLU的表达能力更强，尤其体现在深度网络中；而对于非线性函数而言，ReLU由于非负区间的梯度为常数，因此不存在梯度消失问题(Vanishing Gradient Problem)，使得模型的收敛速度维持在一个稳定状态。这里稍微描述一下什么是梯度消失问题：当梯度小于1时，预测值与真实值之间的误差每传播一层会衰减一次，如果在深层模型中使用sigmoid作为激活函数，这种现象尤为明显，将导致模型收敛停滞不前。</p> 
<p>● 神经网络为啥用交叉熵。<br> <em>参考回答：</em><br> 通过神经网络解决多分类问题时，最常用的一种方式就是在最后一层设置n个输出节点，无论在浅层神经网络还是在CNN中都是如此，比如，在AlexNet中最后的输出层有1000个节点，而即便是ResNet取消了全连接层，也会在最后有一个1000个节点的输出层。<br> 一般情况下，最后一个输出层的节点个数与分类任务的目标数相等。假设最后的节点数为N，那么对于每一个样例，神经网络可以得到一个N维的数组作为输出结果，数组中每一个维度会对应一个类别。在最理想的情况下，如果一个样本属于k，那么这个类别所对应的的输出节点的输出值应该为1，而其他节点的输出都为0，即[0,0,1,0,….0,0]，这个数组也就是样本的Label，是神经网络最期望的输出结果，交叉熵就是用来判定实际的输出与期望的输出的接近程度。</p> 
<p>● 注意力公式<br> <em>参考回答：</em><br> Soft attention、global attention、动态attention<br> Hard attention<br> 静态attention“半软半硬”的attention （local attention）<br> 强制前向attention</p> 
<p>● 论文flow情况<br> <em>参考回答：</em><br> 谈谈自己投稿的论文，论文投稿级别，论文内容，用到的方法，对比方法等</p> 
<p>● Flappy.Bird开发者,怎么利用DNQ方法强化学习你的游戏AI<br> <em>参考回答：</em><br> 强化学习是机器学习里面的一个分支。它强调如何基于环境而行动，以取得最大化的预期收益。其灵感来源于心理学中的行为主义理论，既有机体如何在环境给予的奖励或者惩罚的刺激下，逐步形成对刺激的预期，产生能够最大利益的习惯性行为。结构简图如下：<br> <img src="https://images2.imgbox.com/b4/87/llpL23Se_o.png" alt="在这里插入图片描述"><br> 因为强化学习考虑到了自主个体、环境、奖励等因素，所以很多人包括强化学习的研究者Richard Sutton 都认为它是人工智能中最高层的模型，其它深度学习、机器学习模型都是它的子系统。在围棋界先后打败世界冠军的李世乭和柯洁额alphaGo就使用了强化学习模型，也正是这两次比赛，把人工智能这个概念传递给了大众。使用的是卷积神经网络结构。</p> 
<p>● LeNet-5结构<br> <em>参考回答：</em><br> 输入层：32∗3232∗32的图片，也就是相当于10241024个神经元</p> 
<p>C1层:选取66个特征卷积核，大小为5∗55∗5(不包含偏置),得到66<br> 个特征图，每个特征图的大小为32−5+1=2832−5+1=28，也就是神经元的个数由10241024减小到了28∗28=78428∗28=784。<br> 输入层与C1层之间的参数:6∗(5∗5+1)6∗(5∗5+1),对于卷积层C1，每个像素都与前一层的5∗55∗5个像素和11个bias有连接，有6∗(5∗5+1)∗(28∗28)6∗(5∗5+1)∗(28∗28)个连接</p> 
<p>S2层:池化,是一个下采样层（为什么是下采样？利用图像局部相关性的原理，对图像进行子抽样，可以减少数据处理量同时保留有用信息），有66个14∗1414∗14的特征图，特征图中的每个单元与C1中相对应特征图的2∗22∗2邻域相连接。S2S2层每个单元对应C1C1中44个求和，乘以一个可训练参数，再加上一个可训练偏置。</p> 
<p>C1与S2之间的参数:每一个2∗22∗2求和，然后乘以一个参数，加上一个偏置，共计2∗6=122∗6=12个参数。S2S2中的每个像素都与C1C1中的2∗22∗2个像素和11个偏置相连接，所以有6∗5∗14∗14=58806∗5∗14∗14=5880个连接</p> 
<p>C3层:选取卷积核大小为5∗55∗5,得到新的图片大小为10∗1010∗10我们知道S2包含：6张14∗146张14∗14大小的图片，我们希望这一层得到的结果是：16张10∗1016张10∗10的图片。这1616张图片的每一张，是通过S2S2的66张图片进行加权组合得到的，具体是怎么组合的呢？</p> 
<p>S2与C3之间的组合</p> 
<p>前66个feature map与S2S2层相连的33个feature map相连接，后面66个feature map与S2层相连的4个S2层相连的4个feature map相连接，后面33个feature map与S2S2层部分不相连的44个feature map相连接，最后一个与S2S2层的所有feature map相连。卷积核大小依然为5∗55∗5，总共有6∗（3∗5∗5+1）6∗（3∗5∗5+1）+6∗（4∗5∗5+1）6∗（4∗5∗5+1）+3∗（4∗5∗5+1）3∗（4∗5∗5+1）+1∗（6∗5∗5+1）=15161∗（6∗5∗5+1）=1516个参数。而图像大小为10∗1010∗10，所以共有151600151600个连接。<br> <img src="https://images2.imgbox.com/c6/70/abXS30SX_o.png" alt="在这里插入图片描述"><br> S4层<br> 池化，窗口大小为2∗22∗2,有1616个特征图，总共有3232个参数<br> C3与S4之间的参数<br> 16∗（25∗4+25）=200016∗（25∗4+25）=2000个连接</p> 
<p>C5层<br> 总共120120个feature map，每个feature map与S4S4层所有的feature map相连接，卷积核大小是5∗55∗5，而S4S4层的feature map的大小也是5∗55∗5，所以C5C5的feature map就变成了1个点，共计有120（25∗16+1）=48120120（25∗16+1）=48120个参数。</p> 
<p>F6层<br> 全连接<br> F6F6相当于MLP中的隐含层，有8484个节点，所以有84∗（120+1）=1016484∗（120+1）=10164个参数。F6F6层采用了正切函数。</p> 
<p>输出层<br> 采用了RBF函数，即径向欧式距离函数</p> 
<p>● 推导LSTM正向传播和单向传播过程<br> <em>参考回答：</em><br> 前向推导过程：<br> <img src="https://images2.imgbox.com/51/86/anoADwzh_o.png" alt="在这里插入图片描述"><br> 反向推导过程：<br> <img src="https://images2.imgbox.com/ed/c6/ljdbfQ3k_o.png" alt="在这里插入图片描述"><br> ● LSTM原理，与GRU区别<br> <em>参考回答：</em><br> LSTM算法全称为Long short-term memory，是一种特定形式的RNN（Recurrent neural network，循环神经网络），而RNN是一系列能够处理序列数据的神经网络的总称。<br> RNN在处理长期依赖（时间序列上距离较远的节点）时会遇到巨大的困难，因为计算距离较远的节点之间的联系时会涉及雅可比矩阵的多次相乘，这会带来梯度消失（经常发生）或者梯度膨胀（较少发生）的问题，这样的现象被许多学者观察到并独立研究。为了解决该问题，研究人员提出LSTM。</p> 
<p>LSTM是门限RNN，其单一节点的结构如下图1所示。LSTM的巧妙之处在于通过增加输入门限，遗忘门限和输出门限，使得自循环的权重是变化的，这样一来在模型参数固定的情况下，不同时刻的积分尺度可以动态改变，从而避免了梯度消失或者梯度膨胀的问题。<br> <img src="https://images2.imgbox.com/88/7c/MdMYyx8q_o.png" alt="在这里插入图片描述"><br> 图1 LSTM的CELL示意图</p> 
<p>根据LSTM网络的结构，每个LSTM单元的计算公式如下图2所示，其中Ft表示遗忘门限，It表示输入门限，Ct表示前一时刻cell状态、Ct表示cell状态（这里就是循环发生的地方），Ot表示输出门限，Ht表示当前单元的输出，Ht-1表示前一时刻单元的输出。<br> <img src="https://images2.imgbox.com/be/f9/9oaLK7p0_o.png" alt="在这里插入图片描述"><br> 图2 LSTM计算公式</p> 
<p>与GRU区别：1）GRU和LSTM的性能在很多任务上不分伯仲。2）GRU 参数更少因此更容易收敛，但是数据集很大的情况下，LSTM表达性能更好。3）从结构上来说，GRU只有两个门（update和reset），LSTM有三个门（forget，input，output），GRU直接将hidden state 传给下一个单元，而LSTM则用memory cell 把hidden state 包装起来。</p> 
<p>● LSTM和Naive RNN的区别<br> <em>参考回答：</em><br> RNN和LSTM内部结构的不同：<br> <img src="https://images2.imgbox.com/bd/92/dmWUrCkY_o.png" alt="在这里插入图片描述"><br> RNN<br> <img src="https://images2.imgbox.com/11/89/x4fzGWYH_o.png" alt="在这里插入图片描述"><br> LSTM</p> 
<p>由上面两幅图可以观察到，LSTM结构更为复杂，在RNN中，将过去的输出和当前的输入concatenate到一起，通过tanh来控制两者的输出，它只考虑最近时刻的状态。在RNN中有两个输入和一个输出。</p> 
<p>而LSTM为了能记住长期的状态，在RNN的基础上增加了一路输入和一路输出，增加的这一路就是细胞状态，也就是途中最上面的一条通路。事实上整个LSTM分成了三个部分：</p> 
<p>1）哪些细胞状态应该被遗忘</p> 
<p>2）哪些新的状态应该被加入</p> 
<p>3）根据当前的状态和现在的输入，输出应该是什么</p> 
<p>下面来分别讨论：</p> 
<p>1）哪些细胞状态应该被遗忘<br> 这部分功能是通过sigmoid函数实现的，也就是最左边的通路。根据输入和上一时刻的输出来决定当前细胞状态是否有需要被遗忘的内容。举个例子，如果之前细胞状态中有主语，而输入中又有了主语，那么原来存在的主语就应该被遗忘。concatenate的输入和上一时刻的输出经过sigmoid函数后，越接近于0被遗忘的越多，越接近于1被遗忘的越少。</p> 
<p>2）哪些新的状态应该被加入<br> 继续上面的例子，新进来的主语自然就是应该被加入到细胞状态的内容，同理也是靠sigmoid函数来决定应该记住哪些内容。但是值得一提的是，需要被记住的内容并不是直接concatenate的输入和上一时刻的输出，还要经过tanh，这点应该也是和RNN保持一致。并且需要注意，此处的sigmoid和前一步的sigmoid层的w和b不同，是分别训练的层。细胞状态在忘记了该忘记的，记住了该记住的之后，就可以作为下一时刻的细胞状态输入了。</p> 
<p>3）根据当前的状态和现在的输入，输出应该是什么<br> 这是最右侧的通路，也是通过sigmoid函数做门，对第二步求得的状态做tanh后的结果过滤，从而得到最终的预测结果。事实上，LSTM就是在RNN的基础上，增加了对过去状态的过滤，从而可以选择哪些状态对当前更有影响，而不是简单的选择最近的状态。</p> 
<p>● 神经网络为啥用交叉熵。<br> <em>参考回答</em>：<br> 通过神经网络解决多分类问题时，最常用的一种方式就是在最后一层设置n个输出节点，无论在浅层神经网络还是在CNN中都是如此，比如，在AlexNet中最后的输出层有1000个节点，而即便是ResNet取消了全连接层，也会在最后有一个1000个节点的输出层。<br> 一般情况下，最后一个输出层的节点个数与分类任务的目标数相等。假设最后的节点数为N，那么对于每一个样例，神经网络可以得到一个N维的数组作为输出结果，数组中每一个维度会对应一个类别。在最理想的情况下，如果一个样本属于k，那么这个类别所对应的的输出节点的输出值应该为1，而其他节点的输出都为0，即[0,0,1,0,….0,0]，这个数组也就是样本的Label，是神经网络最期望的输出结果，交叉熵就是用来判定实际的输出与期望的输出的接近程度。</p> 
<p>● 注意力公式<br> <em>参考回答：</em><br> Soft attention、global attention、动态attention<br> Hard attention</p> 
<p>“半软半硬”的attention （local attention）</p> 
<p>静态attention</p> 
<p>强制前向attention<br> <img src="https://images2.imgbox.com/25/b0/Ux7vD3w9_o.png" alt="在这里插入图片描述"></p> 
<p>● Inception Score 评价指标介绍<br> <em>参考回答：</em><br> 定义：<br> <img src="https://images2.imgbox.com/33/b4/d5Q77ohi_o.png" alt="在这里插入图片描述"><br> 推导出上式的意义：<br> <img src="https://images2.imgbox.com/60/f7/bxzae64s_o.png" alt="在这里插入图片描述"><br> 故要使得生成图像的inception score高，就需要</p> 
<p>1.最大化H(y);也就是对于输入的样本，通过inception_v3模型后的类别要均衡，衡量模式坍塌。</p> 
<p>2.最小化H(y|x);说明对于输入的样本，通过inception_v3模型后预测某类别的置信度要高，衡量图片生成的质量。</p> 
<p>● 使用的 CNN 模型权重之间有关联吗？<br> <em>参考回答：</em><br> 权重之间有关联。CNN是权重共享，减少了参数的数量。<br> 简单来说就是用一个卷积核来和一个图像来进行卷积，记住是同一个卷积核，不改变卷积核的值。这样可以减少权值参数。共享就是一个图片对卷积核是共同享有的。对于一个100<em>100像素的图像，如果我们用一个神经元来对图像进行操作，这个神经元大小就是100</em>100=10000，单如果我们使用10<em>10的卷积核，我们虽然需要计算多次，但我们需要的参数只有10</em>10=100个，加上一个偏向b，一共只需要101个参数。我们取得图像大小还是100<em>100。如果我们取得图像比较大，它的参数将会更加多。我们通过10</em>10的卷积核对图像进行特征提取，这样我们就得到一个Feature Map。</p> 
<p>一个卷积核只能提取一个特征，所以我们需要多几个卷积核，假设我们有6个卷积核，我们就会得到6个Feature Map，将这6个Feature Map组成一起就是一个神经元。这6个Feature Map我们需要101*6=606个参数。这个值和10000比还是比较小的。如果像之前的神经网络, 两两相连, 需要 28x28 = 784 输入层, 加上第一个隐藏层30个神经元, 则需要784x30再加上30个b, 总共23,550个参数! 多了40倍的参数。</p> 
<p>5、百度实习：1）模型压缩方法；2）CPM 模型压缩用了哪些方法；3）压缩效果（体积、指标、部署）；4）Kaggle 比赛，比赛背景，怎么进行数据清洗，类别平衡，相近类别重分类，最终成绩是多少，觉得跟前几名差距在哪，有没有尝试过集成的方法；5）人脸项目，大概流程，GPU 加速的地方，两个网络的训练过程，级联网络的 inference 过程，能同时检测多个人脸吗？多尺度缩放怎么处理，resize 自己写？只是检测吗，有没有识别？或者其他</p> 
<p>● CycleGAN 原理介绍一下<br> <em>参考回答：</em><br> CycleGAN其实就是一个A→B单向GAN加上一个B→A单向GAN。两个GAN共享两个生成器，然后各自带一个判别器，所以加起来总共有两个判别器和两个生成器。一个单向GAN有两个loss，而CycleGAN加起来总共有四个loss。CycleGAN论文的原版原理图和公式如下，其实理解了单向GAN那么CycleGAN已经很好理解。<br> <img src="https://images2.imgbox.com/a9/a6/2itbYvei_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/1b/75/k59oHbue_o.png" alt="在这里插入图片描述"><br> 下面放一张网友们自制的CycleGAN示意图，比论文原版的更加直观，出处见水印。<br> <img src="https://images2.imgbox.com/3b/f0/8GQrKf8l_o.png" alt="在这里插入图片描述"></p> 
<p>● 训练 GAN 的时候有没有遇到什么问题<br> <em>参考回答：</em><br> 遇到GAN训练不稳定问题。通过Wasserstein GAN来解决这个问题。WGAN前作分析了Ian Goodfellow提出的原始GAN两种形式各自的问题，第一种形式等价在最优判别器下等价于最小化生成分布与真实分布之间的JS散度，由于随机生成分布很难与真实分布有不可忽略的重叠以及JS散度的突变特性，使得生成器面临梯度消失的问题；第二种形式在最优判别器下等价于既要最小化生成分布与真实分布直接的KL散度，又要最大化其JS散度，相互矛盾，导致梯度不稳定，而且KL散度的不对称性使得生成器宁可丧失多样性也不愿丧失准确性，导致collapse mode现象。<br> WGAN前作针对分布重叠问题提出了一个过渡解决方案，通过对生成样本和真实样本加噪声使得两个分布产生重叠，理论上可以解决训练不稳定的问题，可以放心训练判别器到接近最优，但是未能提供一个指示训练进程的可靠指标，也未做实验验证。</p> 
<p>WGAN本作引入了Wasserstein距离，由于它相对KL散度与JS散度具有优越的平滑特性，理论上可以解决梯度消失问题。接着通过数学变换将Wasserstein距离写成可求解的形式，利用一个参数数值范围受限的判别器神经网络来最大化这个形式，就可以近似Wasserstein距离。在此近似最优判别器下优化生成器使得Wasserstein距离缩小，就能有效拉近生成分布与真实分布。WGAN既解决了训练不稳定的问题，也提供了一个可靠的训练进程指标，而且该指标确实与生成样本的质量高度相关。</p> 
<p>● CPM 模型压缩怎么做的？有压过 OpenPose 吗？<br> <em>参考回答：</em><br> 预测和图像特征计算模块可以被深度网络架构来取代，其中图像和组织特征的表达可以从数据中直接学习。卷积架构让全局可导，因此可以CPM所有阶段联合训练。CPM可以描述为在PM隐含空间模型框架下的卷积架构。<br> 1）用局部图线索来进行关键定位</p> 
<p>第一阶段只用局部图线索来预测部件信任度。figure 2c展示用本地图信息的部件检测的深度网络。先序哦是局部的因为第一阶段感知野只是输出像素附近的一小块。我们用5层卷机网络组成的结构（尾部是量个1x`1卷积层的全卷积架构）。实践中，为了得到一定精度，我们把图片标准化为368x368，感受野是160x160.网络可以看成让深度网络在图像上滑动，并将160x160中局部图像线索回归至代表了各个部件在各个位置的score的P+1大小输出向量。</p> 
<p>2）基于空间环境信息的级联预测</p> 
<p>对于性状稳定的头和肩膀，检测效果很好，然而人体骨架的连接处准确率就很低，因为形状差异很大。部件周围的信任映射，虽然有噪声，但是很有价值。figure 3中，当检测右手肘时，右肩膀的信任映射达到高峰，可以成为一个很强的线索。后续阶段的预测器（gt）可以用图位置z附近含有噪声的信任映射里的空间组织信息（fai），并且利用“部件的几何设定都是恒定的”这一事实来提高改善预测。</p> 
<p>第二个阶段，分类器g2接收特征x2和前一阶段fai的输入。前一阶段不同部件的位置z附近的空间区域产生信任映射，特征方程是把信任映射出的特点编码。CPM不用显式方程来计算环境特征，而是定义含有前一阶段信任度的fai作为预测机的感受野。</p> 
<p>这个网络的设计为了在第二阶段输出层得到一个足够大的感知野，可以学习复杂和长距离的部件关系。通过应用迁移阶段的输出层特征（而不是用图模型的显式方程），后续卷积层自由结合最有预测力的特征，来形成环境信息。第一阶段的信任映射来自用小感知野来检验局部图像的网络。第二阶段，我们设计了一个极大扩充的等价感知野。大感知野可以用两种方法实现：牺牲准确度的池化，增加参数为代价的加大卷积核大小，或者冒着可能让反传消失风险增加网络层数。我们选择增加卷积层，在8x降维热力图上达到大感知野，让我们尽可能减少参数数量。8步网络更容易获得大感知野，它和4步网络表现一样好（在高精确度区域也是）。我们也在PM之后图像特征上映射上重复了类似架构，让空间组织依赖图像而且允许错误关联。</p> 
<p>我们发现，感受野变大，准确性也变大。通过一系列实验，figure 4的准确度随着感受野的变化曲线，改变感受野只通过改变结构而不是增加参数。准确度随着感受野变大而变大，在250像素饱和，这也大概是归一化物体的大小。这说明，网络确实让远距离物体关系编码，并且这是有益的。我们最好的数据集中，我们把图像归一化为368x368，基于第一级信任映射的第二级感知野输出是31x31,这和原始图片的400x400像素等价，其半径可以覆盖任何部件。当阶段增多，有效感知野就会变大。我们有6个阶段。</p> 
<p>3）用CPM学习</p> 
<p>这个深度架构可以有许多层。训练这个网可能让梯度消失，就是反向传播在中间层会减弱。pm级联预测框架有一个自然的解决这个问题的方法。我们不断激励这个网络，通过在每个阶段t的输出定义一个损失函数，让预测的和实际信任映射的距离最小化。部件p理想的信任映射是bp，通过把p部件的最可能点设定在ground truth位置。</p> 
<p>压缩过OpenPose，效果还可以。</p> 
<p>● 用过哪些 Optimizer，效果如何<br> <em>参考回答：</em><br> 1）SGD；2）Momentum；3）Nesterov；4）Adagrad；5）Adadelta；6）RMSprop；7）Adam；8）Adamax；9）Nadam。（1）对于稀疏数据，尽量使用学习率可自适应的算法，不用手动调节，而且最好采用默认参数。（2）SGD通常训练时间最长，但是在好的初始化和学习率调度方案下，结果往往更可靠。但SGD容易困在鞍点，这个缺点也不能忽略。（3）如果在意收敛的速度，并且需要训练比较深比较复杂的网络时，推荐使用学习率自适应的优化方法。（4）Adagrad，Adadelta和RMSprop是比较相近的算法，表现都差不多。（5）在能使用带动量的RMSprop或者Adam的地方，使用Nadam往往能取得更好的效果。</p> 
<p>● 图像基础：传统图像处理方法知道哪些，图像对比度增强说一下<br> <em>参考回答：</em><br> 数字图像处理常用方法：<br> 1）图像变换：由于图像阵列很大，直接在空间域中进行处理，涉及计算量很大。因此，往往采用各种图像变换的方法，如傅立叶变换、沃尔什变换、离散余弦变换等间接处理技术，将空间域的处理转换为变换域处理，不仅可减少计算量，而且可获得更有效的处理（如傅立叶变换可在频域中进行数字滤波处理）。目前新兴研究的小波变换在时域和频域中都具有良好的局部化特性，它在图像处理中也有着广泛而有效的应用。</p> 
<p>2）图像编码压缩：图像编码压缩技术可减少描述图像的数据量（即比特数），以便节省图像传输、处理时间和减少所占用的存储器容量。压缩可以在不失真的前提下获得，也可以在允许的失真条件下进行。编码是压缩技术中最重要的方法，它在图像处理技术中是发展最早且比较成熟的技术。</p> 
<p>3）图像增强和复原：图像增强和复原的目的是为了提高图像的质量，如去除噪声，提高图像的清晰度等。图像增强不考虑图像降质的原因，突出图像中所感兴趣的部分。如强化图像高频分量，可使图像中物体轮廓清晰，细节明显；如强化低频分量可减少图像中噪声影响。图像复原要求对图像降质的原因有一定的了解，一般讲应根据降质过程建立“降质模型”，再采用某种滤波方法，恢复或重建原来的图像。</p> 
<p>4）图像分割：图像分割是数字图像处理中的关键技术之一。图像分割是将图像中有意义的特征部分提取出来，其有意义的特征有图像中的边缘、区域等，这是进一步进行图像识别、分析和理解的基础。虽然目前已研究出不少边缘提取、区域分割的方法，但还没有一种普遍适用于各种图像的有效方法。因此，对图像分割的研究还在不断深入之中，是目前图像处理中研究的热点之一。</p> 
<p>5）图像描述：图像描述是图像识别和理解的必要前提。作为最简单的二值图像可采用其几何特性描述物体的特性，一般图像的描述方法采用二维形状描述，它有边界描述和区域描述两类方法。对于特殊的纹理图像可采用二维纹理特征描述。随着图像处理研究的深入发展，已经开始进行三维物体描述的研究，提出了体积描述、表面描述、广义圆柱体描述等方法。</p> 
<p>6）图像分类（识别）：图像分类（识别）属于模式识别的范畴，其主要内容是图像经过某些预处理（增强、复原、压缩）后，进行图像分割和特征提取，从而进行判决分类。图像分类常采用经典的模式识别方法，有统计模式分类和句法（结构）模式分类，近年来新发展起来的模糊模式识别和人工神经网络模式分类在图像识别中也越来越受到重视。</p> 
<p>全局对比度增强</p> 
<ol><li>直方图均衡化 Histogram Equalization</li></ol> 
<p>算法：</p> 
<p>1）根据图像灰度计算灰度概率密度函数PDF</p> 
<p>2）计算累积概率分布函数CDF</p> 
<p>3）将CDF归一化到原图灰度取值范围，如[0,255]。</p> 
<p>4）之后CDF四舍五入取整，得到灰度转换函数sk=T(rk)</p> 
<p>5）将CDF作为转换函数，将灰度为rk的点转换为sk灰度</p> 
<ol start="2"><li>直方图匹配 Histogram Matching</li></ol> 
<p>算法：</p> 
<p>1）根据图像计算概率密度分布pr®；</p> 
<p>2）根据pr®计算累计分布函数sk=T(rk)；</p> 
<p>3）根据给定的目标分布pz(z)计算累计分布函数G(zq)；</p> 
<p>4）对于每一个k，找到一个q，使得G(zq)约等于sk；</p> 
<p>5）将原图中灰度为k的点变为灰度q；</p> 
<p>局部对比度增强</p> 
<ol><li> <p>邻域直方图均衡：将全局直方图均衡的思想应用于邻域直方图处理中。</p> </li><li> <p>邻域直方图匹配：将全局直方图匹配的思想应用于邻域直方图处理中。</p> </li><li> <p>邻域统计方法</p> </li></ol> 
<p>算法</p> 
<p>1）初始化：增强常数E，灰度下阈值k0，标准差下阈值k1，标准差上阈值k2，窗口半宽s；</p> 
<p>2）计算图像灰度均值MG和灰度标准差σG；</p> 
<p>3）对于每一个像素，计算邻域（大小为2∗step+1的方块）内灰度均值ML和标准差σL；</p> 
<p>4）如果ML&lt;=k0∗MGML&lt;=k0∗MG并且k1∗σG&lt;=σL&lt;=k2∗σG，将像素灰度乘以E。</p> 
<p>● 介绍一下图像的高频、低频部分，知道哪些图像补全的方法<br> <em>参考回答：</em><br> 图像的频率：灰度值变化剧烈程度的指标，是灰度在平面空间上的梯度。<br> （1）什么是低频?</p> 
<p>低频就是颜色缓慢地变化,也就是灰度缓慢地变化,就代表着那是连续渐变的一块区域,这部分就是低频. 对于一幅图像来说，除去高频的就是低频了，也就是边缘以内的内容为低频，而边缘内的内容就是图像的大部分信息，即图像的大致概貌和轮廓，是图像的近似信息。</p> 
<p>（2）什么是高频?</p> 
<p>反过来, 高频就是频率变化快.图像中什么时候灰度变化快?就是相邻区域之间灰度相差很大,这就是变化得快.图像中,一个影像与背景的边缘部位,通常会有明显的差别,也就是说变化那条边线那里,灰度变化很快,也即是变化频率高的部位.因此，图像边缘的灰度值变化快，就对应着频率高，即高频显示图像边缘。图像的细节处也是属于灰度值急剧变化的区域，正是因为灰度值的急剧变化，才会出现细节。</p> 
<p>另外噪声（即噪点）也是这样,在一个像素所在的位置,之所以是噪点,就是因为它与正常的点颜色不一样了，也就是说该像素点灰度值明显不一样了,也就是灰度有快速地变化了,所以是高频部分，因此有噪声在高频这么一说。</p> 
<p>图像补全的方法：</p> 
<p>Region Filling and Object Removal by Exemplar-Based Image Inpainting</p> 
<p>算法的流程大致如下：</p> 
<p>1）对待补全区域边界的像素依次计算补全的优先度(priority)，这个优先度主要考虑2个因素。一个是周围像素可信度高的位置要优先补，另一个是位于图像梯度变化剧烈的位置要优先补。综合二者得到所有优先度之后，挑选优先度最高的像素来补</p> 
<p>2）对于上一步找到的待补全像素，考虑它周围的一个小patch(比如3*3)。在图像已知部分搜索所有的patch，找到最相似的patch</p> 
<p>3）用找到的best match来补全未知部分，并更新相关数值</p> 
<p>但是我们也不难发现这个方法存在的问题：如果图像已知部分找不到相似的patch，那算法将无法进行；这个方法只适用于补全背景以低频信息和重复性纹理为主的图像；搜索相似的patch计算复杂度非常高，算法运行效率低。</p> 
<p>Scene Completion Using Millions of Photographs</p> 
<p>算法的大致流程如下：</p> 
<p>1）从Flickr上下载两百万图片构建数据库，以”landscape””city””park”等关键词搜索户外场景的图片。</p> 
<p>2）对于一张待补全图像，从数据库中挑选200个场景最相似的图片，这里使用gist scene descriptor和图像下采样到4*4作为匹配的特征向量。</p> 
<p>3）将补全区域边界外80个pixel的区域作为context。对于每一张匹配的图像，搜索所有的平移空间和3个尺度的scale空间，根据context部分的匹配误差，选择最佳的补全位置；之后利用graph-cut算法求解最佳的融合边界。</p> 
<p>4）利用标准的泊松融合处理融合边界。</p> 
<p>5）将前几步的匹配cost和graph-cut的cost加起来，返回cost最小的20的结果供用户挑选。</p> 
<p>Context Encoders: Feature Learning by Inpainting</p> 
<p>文章提出的网络结构如下，包括3个部分：Encoder, Channel-wise fully-connected layer, Decoder。Encoder的结构直接借鉴了AlexNet前5层的卷积层结构，具体结构如下。输入的crop尺寸是227Í227，卷积之后得到的feature map结构是256层6 Í 6。所有的weight都随机初始化。</p> 
<p>Channel-wise fully-connected layer是对普通fc层的一种改进。之所以加入fc层是为了使feature map每一层的信息可以在内部交流。但传统的fc层参数太多，因此作者提出可以在fc中去掉feature map层间的信息交流，从而减少参数规模。在fc之后会接一个stride为1的卷积层，来实现层间的信息交流。</p> 
<p>Decoder的目的是将压缩的feature map一步步放大，恢复到原始图片的尺寸。文章提出采用5个up-convolutional层，每层后接一个RELU。上采样的结构如下。</p> 
<p>● 百度实习：模型压缩的大方向。CPM 模型怎么压缩的，做了哪些工作？<br> 参考回答：</p> 
<p>预测和图像特征计算模块可以被深度网络架构来取代，其中图像和组织特征的表达可以从数据中直接学习。卷积架构让全局可导，因此可以CPM所有阶段联合训练。CPM可以描述为在PM隐含空间模型框架下的卷积架构。<br> 1）用局部图线索来进行关键定位</p> 
<p>第一阶段只用局部图线索来预测部件信任度。figure 2c展示用本地图信息的部件检测的深度网络。先序哦是局部的因为第一阶段感知野只是输出像素附近的一小块。我们用5层卷机网络组成的结构（尾部是量个1x`1卷积层的全卷积架构）。实践中，为了得到一定精度，我们把图片标准化为368x368，感受野是160x160.网络可以看成让深度网络在图像上滑动，并将160x160中局部图像线索回归至代表了各个部件在各个位置的score的P+1大小输出向量。</p> 
<p>2）基于空间环境信息的级联预测</p> 
<p>对于性状稳定的头和肩膀，检测效果很好，然而人体骨架的连接处准确率就很低，因为形状差异很大。部件周围的信任映射，虽然有噪声，但是很有价值。figure 3中，当检测右手肘时，右肩膀的信任映射达到高峰，可以成为一个很强的线索。后续阶段的预测器（gt）可以用图位置z附近含有噪声的信任映射里的空间组织信息（fai），并且利用“部件的几何设定都是恒定的”这一事实来提高改善预测。</p> 
<p>第二个阶段，分类器g2接收特征x2和前一阶段fai的输入。前一阶段不同部件的位置z附近的空间区域产生信任映射，特征方程是把信任映射出的特点编码。CPM不用显式方程来计算环境特征，而是定义含有前一阶段信任度的fai作为预测机的感受野。</p> 
<p>这个网络的设计为了在第二阶段输出层得到一个足够大的感知野，可以学习复杂和长距离的部件关系。通过应用迁移阶段的输出层特征（而不是用图模型的显式方程），后续卷积层自由结合最有预测力的特征，来形成环境信息。第一阶段的信任映射来自用小感知野来检验局部图像的网络。第二阶段，我们设计了一个极大扩充的等价感知野。大感知野可以用两种方法实现：牺牲准确度的池化，增加参数为代价的加大卷积核大小，或者冒着可能让反传消失风险增加网络层数。我们选择增加卷积层，在8x降维热力图上达到大感知野，让我们尽可能减少参数数量。8步网络更容易获得大感知野，它和4步网络表现一样好（在高精确度区域也是）。我们也在PM之后图像特征上映射上重复了类似架构，让空间组织依赖图像而且允许错误关联。</p> 
<p>我们发现，感受野变大，准确性也变大。通过一系列实验，figure 4的准确度随着感受野的变化曲线，改变感受野只通过改变结构而不是增加参数。准确度随着感受野变大而变大，在250像素饱和，这也大概是归一化物体的大小。这说明，网络确实让远距离物体关系编码，并且这是有益的。我们最好的数据集中，我们把图像归一化为368x368，基于第一级信任映射的第二级感知野输出是31x31,这和原始图片的400x400像素等价，其半径可以覆盖任何部件。当阶段增多，有效感知野就会变大。我们有6个阶段。</p> 
<p>3）用CPM学习<br> 这个深度架构可以有许多层。训练这个网可能让梯度消失，就是反向传播在中间层会减弱。pm级联预测框架有一个自然的解决这个问题的方法。我们不断激励这个网络，通过在每个阶段t的输出定义一个损失函数，让预测的和实际信任映射的距离最小化。部件p理想的信任映射是bp，通过把p部件的最可能点设定在ground truth位置。</p> 
<p>● Depthwise 卷积实际速度与理论速度差距较大，解释原因。<br> <em>参考回答：</em><br> 首先，caffe原先的gpu实现group convolution很糟糕，用for循环每次算一个卷积，速度极慢。第二，cudnn7.0及之后直接支持group convolution，但本人实测，速度比github上几个直接写cuda kernel计算的dw convolution速度慢。例如对于n=128, c=512, h=32, w=32, group=512的卷积跑100次，cudnn 7.0里的group convolution需要4秒多，而DepthwiseConvolution大概只需要1秒。<br> 分析了一下dw convolution与普通convolution的理论计算复杂度，举例如下：</p> 
<p>卷积1：普通卷积，输入为64<em>64</em>256，输出为64<em>64</em>256，卷积核大小为3<em>3。参数为3</em>3<em>256</em>256=590K，计算量为64<em>64</em>256<em>3</em>3<em>256=2.42G，计算过程的工作集内存总量（输入输出数据+参数）为64</em>64<em>256</em>2 + 3<em>3</em>256*256 = 2.69M。</p> 
<p>卷积2：dw卷积，输入为64<em>64</em>256，输出为64<em>64</em>256，卷积核大小为3<em>3。参数为3</em>3<em>256=2.3K个，计算量为64</em>64<em>256</em>3<em>3=9.44M，计算过程的工作集内存总量为64</em>64<em>256</em>2 + 3<em>3</em>256=2.10M。</p> 
<p>卷积3：普通卷积，输入为64<em>64</em>16，输出为64<em>64</em>16，卷积核大小为3<em>3。参数为3</em>3<em>16</em>16=2.3K个，计算量为64<em>64</em>16<em>3</em>3<em>16=9.44M，计算过程的工作集内存总量为64</em>64<em>16</em>2 + 3<em>3</em>16*16=133K。</p> 
<p>可以看到卷积2肯定比卷积1快，因为计算量下降到1/256了，但卷积2实际上无法达到卷积1的256倍速度（我记得我测得结果大概是快10倍左右），因为工作集内存大小并没有显著降低。卷积2也无法达到卷积3的速度，因为虽然FLOPS相同，但工作集内存大小相差了很多倍，因此单位数据的计算密度小很多，很难充分利用GPU上的计算单元。</p> 
<p>● RetinaNet 的大致结构画一下<br> <em>参考回答：</em><br> <img src="https://images2.imgbox.com/00/75/WViVmtvO_o.png" alt="在这里插入图片描述"><br> ● RetinaNet为什么比SSD效果好<br> <em>参考回答：</em><br> SSD 在训练期间重新采样目标类和背景类的比率，这样它就不会被图像背景淹没。RetinaNet采用另一种方法来减少训练良好的类的损失。因此，只要该模型能够很好地检测背景，就可以减少其损失并重新增强对目标类的训练。所以RetinaNet比SSD 效果好。</p> 
<p>原文来自：鸿钧老祖<br> 原文链接：<br> （1）https://www.cnblogs.com/zhangyang520/p/10969960.html<br> （2）<br> https://www.cnblogs.com/zhangyang520/p/10969966.html<br> （3）https://www.cnblogs.com/zhangyang520/p/10969968.html</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/736c87785cd4e2d1a873bbe9636267e4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">2019 CS224N Assignment 2: word2vec</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7ae33fcc496590ab035454782cca9dc0/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Win 10 常用快捷键</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>