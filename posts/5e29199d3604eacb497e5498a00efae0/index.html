<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>机器学习:lightGBM算法 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="机器学习:lightGBM算法" />
<meta property="og:description" content="lightGBM是基于GBDT的一个算法。因为传统的GBDT在每一次迭代时需要多次遍历整个训练数据，所以GBDT的训练会受内存等的限制。尤其在处理工业级海量数据的数据时，GBDT不能满足其需求。而lightGBM支持高效率的并行训练，并且具有更快的迭代速度，更低的内存消耗、更好的准确率，支持分布式可以快速处理海量数据。lightGBM的优缺点主要如下：
1.优点 XGBoost也是GBDT框架下的一个算法。相对于XGBoost, lightGBM的优点主要集中在内存和速度两个方面。
lightGBM采用基于直方图的决策树算法，该算法将遍历样本转化为遍历直方图。既降低了内存使用率又可以利用直方图做差的方式降低计算复杂度；而XGBoost采用的是基于预排序的决策树算法，不仅需要额外的空间保存特征排序结果，而且需要遍历每一个分割点计算分裂增益，计算代价大。ligthGBM算法在训练过程中采用单边梯度算法(GOSS)过滤掉梯度小的样本，减少了大量的计算。GOSS先将要进行分裂的特征的所有绝对值大小进行降序排序，选取绝对值最大的 a × 100 % a\times 100\% a×100%个数据，然后在剩下的较小梯度数据中随机选择 b × 100 % b\times 100\% b×100%个数据。接着将这 b × 100 % b\times 100\% b×100%个数据乘以一个常数 1 − a b \frac{1-a}{b} b1−a​，这样算法既可以更关注训练不足的样本，又不用担心会改变原数据集的分布。lightGBM采用了leaf-wise算法的增长策略构建树，减少了很多不必要的计算。XGBoost采用按层(level-wise)生长策略，该策略遍历一次数据可以同时分裂同一层的叶子，虽然这样方便进行多线程优化，控制模型复杂度并且不容易过拟合，但这种策略不加区分低对待同一层的叶子，非常低效。而lightGBM采用的leaf-wise算法每次只对当前分裂增益最大的叶子结点进行分裂。这种方法很容易长出比较深的树从而导致过拟合，所以一般会在树的最大深度上加一个限制。lightGBM采用优化后的特征并行、数据并行方法加速计算，当数据量非常大的时候还可以采用投票并行的策略。lightGBM对缓存进行了优化，增加了缓存命中率。lightGBM算法在训练过程可以将两两互斥或冲突率低的特征捆绑成一个特征进行处理，减少了特征数量，降低了内存消耗(使用图着色原理选择要捆绑在一起的特征)。lightGBM可以直接处理类别型特征。 2.缺点 可能会长比深的决策树，产生过拟合。lightGBM是基于偏差的算法，所以会对异常值比较敏感。在寻找最优解的时候，没有将最优解是全部特征的综合这一理念考虑进去。 3.案例 这里使用lightGBM的sklearn接口。下面以LGBMClassifier()为例，介绍其中的主要参数。
boosting_type:‘gbdt’(传统的GBDT模型)、‘dart’、‘goss’、‘rf’(随机森林)
dart: Dropouts meet Multiple Additive Regression Trees. 在每棵树的迭代过程中不再单单去拟合前一棵树的残差，而是从前面已有的树中采样一部分树，组合成一个新的树，然后去拟合这部分的残差，从而使后面的树贡献变大一些。
goss：Gradient-based One-Side Sampling，单边梯度采样。目的是丢弃一些对计算增益没有帮助的样本留下有帮助的，降低计算复杂度。num_leaves：基学习器的最大叶子节点数。max_depth: 基学习器的最大树深度。若 ≤ 0 \leq 0 ≤0则意味着对树深度不加限制。learning_rate: 学习率。n_estimators：学习器的个数。subsample_for_bin: 构成bins的样本数。
lightGBM中使用基于直方图算法的决策树算法。对每个特征进行直方图统计，然后根据直方图的离散值，遍历寻找最优的分割点。objective: ‘binary’、‘multiclass’class_weight: 该参数主要用于多分类任务中；在二分类任务中，可以使用is_unbalance或者scale_pos_weight。colsample_bytree:构造每棵树时特征的抽样比率。reg_alpha: L1正则化项。reg_lambda: L2正则化项。silent: 是否打印每次的运行结果。 LGBMClassifier()的fit()方法中也有几个重要参数需要说明。具体如下：
eval_metric:评价指标。分类任务默认使用’logloss‘。lightGBM中常用的评价指标有：
categorical_feature: 类别特征。lightGBM是目前唯一能直接处理类别特征的算法。在lightGBM中类别特征不需要在用one-hot进行转换了。 import pandas as pd from lightgbm import LGBMClassifier from sklearn." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/5e29199d3604eacb497e5498a00efae0/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-07-11T10:58:03+08:00" />
<meta property="article:modified_time" content="2021-07-11T10:58:03+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">机器学习:lightGBM算法</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>  lightGBM是基于GBDT的一个算法。因为传统的GBDT在每一次迭代时需要多次遍历整个训练数据，所以GBDT的训练会受内存等的限制。尤其在处理工业级海量数据的数据时，GBDT不能满足其需求。而lightGBM支持高效率的并行训练，并且具有更快的迭代速度，更低的内存消耗、更好的准确率，支持分布式可以快速处理海量数据。lightGBM的优缺点主要如下：</p> 
<h6><a id="1_1"></a>1.优点</h6> 
<p>  XGBoost也是GBDT框架下的一个算法。相对于XGBoost, lightGBM的优点主要集中在内存和速度两个方面。</p> 
<ul><li>lightGBM采用基于直方图的决策树算法，该算法将遍历样本转化为遍历直方图。既降低了内存使用率又可以利用直方图做差的方式降低计算复杂度；而XGBoost采用的是基于预排序的决策树算法，不仅需要额外的空间保存特征排序结果，而且需要遍历每一个分割点计算分裂增益，计算代价大。</li><li>ligthGBM算法在训练过程中采用单边梯度算法(GOSS)过滤掉梯度小的样本，减少了大量的计算。GOSS先将要进行分裂的特征的所有绝对值大小进行降序排序，选取绝对值最大的<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          a 
         
        
          × 
         
        
          100 
         
        
          % 
         
        
       
         a\times 100\% 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.66666em; vertical-align: -0.08333em;"></span><span class="mord mathdefault">a</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.80556em; vertical-align: -0.05556em;"></span><span class="mord">1</span><span class="mord">0</span><span class="mord">0</span><span class="mord">%</span></span></span></span></span>个数据，然后在剩下的较小梯度数据中随机选择<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          b 
         
        
          × 
         
        
          100 
         
        
          % 
         
        
       
         b\times 100\% 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.77777em; vertical-align: -0.08333em;"></span><span class="mord mathdefault">b</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.80556em; vertical-align: -0.05556em;"></span><span class="mord">1</span><span class="mord">0</span><span class="mord">0</span><span class="mord">%</span></span></span></span></span>个数据。接着将这<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          b 
         
        
          × 
         
        
          100 
         
        
          % 
         
        
       
         b\times 100\% 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.77777em; vertical-align: -0.08333em;"></span><span class="mord mathdefault">b</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.80556em; vertical-align: -0.05556em;"></span><span class="mord">1</span><span class="mord">0</span><span class="mord">0</span><span class="mord">%</span></span></span></span></span>个数据乘以一个常数<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
          
          
            1 
           
          
            − 
           
          
            a 
           
          
         
           b 
          
         
        
       
         \frac{1-a}{b} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.19011em; vertical-align: -0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.845108em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">b</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">−</span><span class="mord mathdefault mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>，这样算法既可以更关注训练不足的样本，又不用担心会改变原数据集的分布。</li><li>lightGBM采用了leaf-wise算法的增长策略构建树，减少了很多不必要的计算。XGBoost采用按层(level-wise)生长策略，该策略遍历一次数据可以同时分裂同一层的叶子，虽然这样方便进行多线程优化，控制模型复杂度并且不容易过拟合，但这种策略不加区分低对待同一层的叶子，非常低效。而lightGBM采用的leaf-wise算法每次只对当前分裂增益最大的叶子结点进行分裂。这种方法很容易长出比较深的树从而导致过拟合，所以一般会在树的最大深度上加一个限制。</li><li>lightGBM采用优化后的特征并行、数据并行方法加速计算，当数据量非常大的时候还可以采用投票并行的策略。</li><li>lightGBM对缓存进行了优化，增加了缓存命中率。</li><li>lightGBM算法在训练过程可以将两两互斥或冲突率低的特征捆绑成一个特征进行处理，减少了特征数量，降低了内存消耗(使用图着色原理选择要捆绑在一起的特征)。</li><li>lightGBM可以直接处理类别型特征。</li></ul> 
<h6><a id="2_10"></a>2.缺点</h6> 
<ul><li>可能会长比深的决策树，产生过拟合。</li><li>lightGBM是基于偏差的算法，所以会对异常值比较敏感。</li><li>在寻找最优解的时候，没有将最优解是全部特征的综合这一理念考虑进去。</li></ul> 
<h6><a id="3_15"></a>3.案例</h6> 
<p>  这里使用lightGBM的sklearn接口。下面以LGBMClassifier()为例，介绍其中的主要参数。<br> <img src="https://images2.imgbox.com/77/d6/jGFtB53X_o.png" alt="在这里插入图片描述"></p> 
<ul><li>boosting_type:‘gbdt’(传统的GBDT模型)、‘dart’、‘goss’、‘rf’(随机森林)<br> dart: Dropouts meet Multiple Additive Regression Trees. 在每棵树的迭代过程中不再单单去拟合前一棵树的残差，而是从前面已有的树中采样一部分树，组合成一个新的树，然后去拟合这部分的残差，从而使后面的树贡献变大一些。<br> goss：Gradient-based One-Side Sampling，单边梯度采样。目的是丢弃一些对计算增益没有帮助的样本留下有帮助的，降低计算复杂度。</li><li>num_leaves：基学习器的最大叶子节点数。</li><li>max_depth: 基学习器的最大树深度。若<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          ≤ 
         
        
          0 
         
        
       
         \leq 0 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.77194em; vertical-align: -0.13597em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">0</span></span></span></span></span>则意味着对树深度不加限制。</li><li>learning_rate: 学习率。</li><li>n_estimators：学习器的个数。</li><li>subsample_for_bin: 构成bins的样本数。<br> lightGBM中使用基于直方图算法的决策树算法。对每个特征进行直方图统计，然后根据直方图的离散值，遍历寻找最优的分割点。</li><li>objective: ‘binary’、‘multiclass’</li><li>class_weight: 该参数主要用于多分类任务中；在二分类任务中，可以使用is_unbalance或者scale_pos_weight。</li><li>colsample_bytree:构造每棵树时特征的抽样比率。</li><li>reg_alpha: L1正则化项。</li><li>reg_lambda: L2正则化项。</li><li>silent: 是否打印每次的运行结果。</li></ul> 
<p>LGBMClassifier()的fit()方法中也有几个重要参数需要说明。具体如下：<br> <img src="https://images2.imgbox.com/0b/b4/H6jZvlDv_o.png" alt="在这里插入图片描述"></p> 
<ul><li>eval_metric:评价指标。分类任务默认使用’logloss‘。lightGBM中常用的评价指标有：<br> <img src="https://images2.imgbox.com/68/54/qBMBcVcp_o.png" alt="在这里插入图片描述"></li><li>categorical_feature: 类别特征。lightGBM是目前唯一能直接处理类别特征的算法。在lightGBM中类别特征不需要在用one-hot进行转换了。</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">from</span> lightgbm <span class="token keyword">import</span> LGBMClassifier
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> roc_auc_score<span class="token punctuation">,</span>accuracy_score
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>impute <span class="token keyword">import</span> SimpleImputer
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> GridSearchCV

data<span class="token operator">=</span>pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">'Titanic.train.csv'</span><span class="token punctuation">)</span>

<span class="token comment">#删除无用字段</span>
data<span class="token operator">=</span>data<span class="token punctuation">.</span>drop<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'PassengerId'</span><span class="token punctuation">,</span><span class="token string">'Name'</span><span class="token punctuation">,</span><span class="token string">'Ticket'</span><span class="token punctuation">,</span><span class="token string">'Cabin'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token comment">#对Age字段的缺失值用均值进行填充。也可以使用模型预测缺失值。</span>
imputer<span class="token operator">=</span>SimpleImputer<span class="token punctuation">(</span>strategy<span class="token operator">=</span><span class="token string">'mean'</span><span class="token punctuation">)</span>
data<span class="token punctuation">[</span><span class="token string">'Age'</span><span class="token punctuation">]</span><span class="token operator">=</span>imputer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">'Age'</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment">#把object型转换category型(lightGBM不接受object型)</span>
<span class="token keyword">for</span> col <span class="token keyword">in</span> data<span class="token punctuation">.</span>columns<span class="token punctuation">:</span>
    <span class="token keyword">if</span> data<span class="token punctuation">[</span>col<span class="token punctuation">]</span><span class="token punctuation">.</span>dtype<span class="token operator">==</span><span class="token string">'object'</span><span class="token punctuation">:</span>
        data<span class="token punctuation">[</span>col<span class="token punctuation">]</span><span class="token operator">=</span>data<span class="token punctuation">[</span>col<span class="token punctuation">]</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'category'</span><span class="token punctuation">)</span>

x<span class="token operator">=</span>data<span class="token punctuation">.</span>drop<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'Survived'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
y<span class="token operator">=</span>data<span class="token punctuation">[</span><span class="token string">'Survived'</span><span class="token punctuation">]</span>

x_train<span class="token punctuation">,</span>x_test<span class="token punctuation">,</span>y_train<span class="token punctuation">,</span>y_test<span class="token operator">=</span>train_test_split<span class="token punctuation">(</span>x<span class="token punctuation">,</span>y<span class="token punctuation">,</span>test_size<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span>random_state<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

params<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token string">'max_depth'</span><span class="token punctuation">:</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token string">'learning_rate'</span><span class="token punctuation">:</span><span class="token punctuation">[</span><span class="token number">0.05</span><span class="token punctuation">,</span><span class="token number">0.1</span><span class="token punctuation">,</span><span class="token number">0.15</span><span class="token punctuation">,</span><span class="token number">0.2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token string">'n_estimators'</span><span class="token punctuation">:</span><span class="token punctuation">[</span><span class="token number">30</span><span class="token punctuation">,</span><span class="token number">50</span><span class="token punctuation">,</span><span class="token number">80</span><span class="token punctuation">,</span><span class="token number">100</span><span class="token punctuation">]</span><span class="token punctuation">}</span>

estimator<span class="token operator">=</span>LGBMClassifier<span class="token punctuation">(</span>objective<span class="token operator">=</span><span class="token string">'binary'</span><span class="token punctuation">,</span>reg_alpha<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>
                                reg_lambda<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span>
gbm<span class="token operator">=</span>GridSearchCV<span class="token punctuation">(</span>estimator<span class="token punctuation">,</span>params<span class="token punctuation">)</span>
gbm<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span>y_train<span class="token punctuation">,</span>early_stopping_rounds<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>
        categorical_feature<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'Sex'</span><span class="token punctuation">,</span><span class="token string">'Embarked'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        eval_metric<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'auc'</span><span class="token punctuation">,</span><span class="token string">'binary'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        eval_set<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">(</span>x_train<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        eval_names<span class="token operator">=</span><span class="token string">'train'</span><span class="token punctuation">,</span>
        verbose<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>

model<span class="token operator">=</span>gbm<span class="token punctuation">.</span>best_estimator_
y_pred<span class="token operator">=</span>model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span>
roc_auc<span class="token operator">=</span>roc_auc_score<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span>model<span class="token punctuation">.</span>predict_proba<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
accuracy<span class="token operator">=</span>accuracy_score<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span>y_pred<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"lightGBM模型的准确率为{:.3f},AUC={:.3f}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>accuracy<span class="token punctuation">,</span>roc_auc<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>其运行结果如下：</p> 
<blockquote> 
 <p>lightGBM模型的准确率为0.849,AUC=0.904</p> 
</blockquote> 
<h6><a id="_90"></a>参考资料</h6> 
<ol><li><a href="https://zhuanlan.zhihu.com/p/99069186" rel="nofollow">https://zhuanlan.zhihu.com/p/99069186</a></li><li><a href="https://lightgbm.readthedocs.io/en/latest/index.html" rel="nofollow">https://lightgbm.readthedocs.io/en/latest/index.html</a></li></ol>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/5c82d829232a74fb8500919785f7b8d3/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">汉诺塔游戏设计-c&#43;&#43;课程设计</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/de42af9efc178784eba35a24004e9bb2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">安装framework 3.5出现0x800F0922的解决方法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>