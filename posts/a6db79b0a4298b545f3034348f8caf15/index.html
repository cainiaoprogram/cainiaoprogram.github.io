<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>LayerDropout方法 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="LayerDropout方法" />
<meta property="og:description" content="简介：
一种结构化的dropout形式，它在训练过程中具有正则化效果，并允许在推理时有效地剪枝。
方法的核心是在训练过程中，通过随机丢弃模型权重，从大模型中抽取小的子网络，如Dropout或Dropconnect，这具有使网络对后续剪枝具有鲁棒性的优点 。
贡献：
•LayerDrop使非常深的transformer正则化并稳定其训练，从而在各种基准测试中获得最先进的性能。
•可以在测试时从一个预先训练的大模型中自动提取任何深度的小而有效的模型，而不需要进行精细调整。
•LayerDrop易于实现。
在BART模型上实现的代码：
for idx, decoder_layer in enumerate(self.layers): # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description) if output_hidden_states: all_hidden_states &#43;= (hidden_states,) dropout_probability = random.uniform(0, 1) if self.training and (dropout_probability &lt; self.layerdrop): continue past_key_value = past_key_values[idx] if past_key_values is not None else None if getattr(self.config, &#34;gradient_checkpointing&#34;, False) and self.training: if use_cache: logger.warning( &#34;`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting &#34; &#34;`use_cache=False`...&#34; ) use_cache = False def create_custom_forward(module): def custom_forward(*inputs): # None for past_key_value return module(*inputs, output_attentions, use_cache) return custom_forward layer_outputs = torch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/a6db79b0a4298b545f3034348f8caf15/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-10-25T13:11:49+08:00" />
<meta property="article:modified_time" content="2022-10-25T13:11:49+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">LayerDropout方法</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>简介：</p> 
<p>一种结构化的dropout形式，它在训练过程中具有正则化效果，并允许在推理时有效地剪枝。</p> 
<p><img alt="" height="574" src="https://images2.imgbox.com/0a/30/DxpFI4sv_o.png" width="1200"></p> 
<p> 方法的核心是在训练过程中，通过随机丢弃模型权重，从大模型中抽取小的子网络，如Dropout或Dropconnect，这具有使网络对后续剪枝具有鲁棒性的优点 。</p> 
<p>贡献：</p> 
<p>        •LayerDrop使非常深的transformer正则化并稳定其训练，从而在各种基准测试中获得最先进的性能。</p> 
<p>         •可以在测试时从一个预先训练的大模型中自动提取任何深度的小而有效的模型，而不需要进行精细调整。</p> 
<p>         •LayerDrop易于实现。</p> 
<p>在BART模型上实现的代码：</p> 
<pre><code class="language-python">        for idx, decoder_layer in enumerate(self.layers):
            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)
            if output_hidden_states:
                all_hidden_states += (hidden_states,)
            dropout_probability = random.uniform(0, 1)
            if self.training and (dropout_probability &lt; self.layerdrop):
                continue

            past_key_value = past_key_values[idx] if past_key_values is not None else None

            if getattr(self.config, "gradient_checkpointing", False) and self.training:

                if use_cache:
                    logger.warning(
                        "`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting "
                        "`use_cache=False`..."
                    )
                    use_cache = False

                def create_custom_forward(module):
                    def custom_forward(*inputs):
                        # None for past_key_value
                        return module(*inputs, output_attentions, use_cache)

                    return custom_forward

                layer_outputs = torch.utils.checkpoint.checkpoint(
                    create_custom_forward(decoder_layer),
                    hidden_states,
                    attention_mask,
                    encoder_hidden_states,
                    encoder_attention_mask,
                    head_mask[idx] if head_mask is not None else None,
                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,
                    None,
                )
            else:

                layer_outputs = decoder_layer(
                    hidden_states,
                    attention_mask=attention_mask,
                    encoder_hidden_states=encoder_hidden_states,
                    encoder_attention_mask=encoder_attention_mask,
                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),
                    cross_attn_layer_head_mask=(
                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None
                    ),
                    past_key_value=past_key_value,
                    output_attentions=output_attentions,
                    use_cache=use_cache,
                )
            hidden_states = layer_outputs[0]

            if use_cache:
                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)

            if output_attentions:
                all_self_attns += (layer_outputs[1],)

                if encoder_hidden_states is not None:
                    all_cross_attentions += (layer_outputs[2],)</code></pre> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/3ca7fbe67c23b55371c0694f66fa9b6d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">（已解决）传值问题：前端发送请求后，后端接收的参数多了“=”</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b34315f1743664450352006016246707/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">常见的js面试题之递归算法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>