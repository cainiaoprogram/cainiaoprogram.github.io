<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>NLP基础——中文分词 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="NLP基础——中文分词" />
<meta property="og:description" content="简介 分词是自然语言处理（NLP）中的一个基本任务，它涉及将连续的文本序列切分成多个有意义的单元，这些单元通常被称为“词”或“tokens”。在英语等使用空格作为自然分隔符的语言中，分词相对简单，因为大部分情况下只需要根据空格和标点符号来切分文本。
然而，在汉语等语言中，并没有明显的单词界限标记（如空格），因此汉语分词比较复杂。汉字序列必须被正确地切割成有意义的词组合。例如，“我爱北京天安门”，应该被正确地划分为“我/爱/北京/天安门”。
方法 中文分词技术主要可以归类为以下几种方法：
基于字符串匹配的方法：这种方法依赖一个预先定义好的字典来匹配和确定句子中最长能够匹配上的字符串。这包括正向最大匹配法、逆向最大匹配法以及双向最大匹配法。
基于理解的方法：通过模拟人类理解句子含义进行分词，考虑上下文、句法结构和其他信息。
基于统计学习模型：利用机器学习算法从大量已经人工标注好了分词结果的数据集里学习如何进行有效地分词。常见算法包括隐马尔可夫模型(HMM)、条件随机场(CRF)以及近年来流行起来基于深度学习框架构建神经网络模型(RNNs、CNNs、LSTMs、Transformer、BERT等)。
混合方法：结合以上几种不同策略以提高精确度和鲁棒性。
基于规则: 通过制定一系列规则手动或半自动地进行文字断开, 这通常需要专业知识并且效率不高, 但可以在特定情境下发挥作用。
Python栗子 基于字符串匹配，最大前向匹配，代码如下
def max_match_segmentation(text, dictionary): max_word_length = max(len(word) for word in dictionary) start = 0 segmentation = [] while start &lt; len(text): for length in range(max_word_length, 0, -1): if length &gt; len(text) - start: continue word = text[start:start &#43; length] if word in dictionary: segmentation.append(word) start &#43;= length break else: # 如果没有找到，则按单字切分 segmentation.append(text[start]) start &#43;= 1 return segmentation # 示例字典和用法： dictionary = {&#34;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/e389b852e7223347b18f583c5f809e64/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-03T23:45:52+08:00" />
<meta property="article:modified_time" content="2024-01-03T23:45:52+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">NLP基础——中文分词</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-github-gist">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h4><a id="_0"></a>简介</h4> 
<p>分词是自然语言处理（NLP）中的一个基本任务，它涉及将连续的文本序列切分成多个有意义的单元，这些单元通常被称为“词”或“tokens”。在英语等使用空格作为自然分隔符的语言中，分词相对简单，因为大部分情况下只需要根据空格和标点符号来切分文本。</p> 
<p>然而，在汉语等语言中，并没有明显的单词界限标记（如空格），因此汉语分词比较复杂。汉字序列必须被正确地切割成有意义的词组合。例如，“我爱北京天安门”，应该被正确地划分为“我/爱/北京/天安门”。</p> 
<h4><a id="_4"></a>方法</h4> 
<p>中文分词技术主要可以归类为以下几种方法：</p> 
<ol><li> <p>基于字符串匹配的方法：这种方法依赖一个预先定义好的字典来匹配和确定句子中最长能够匹配上的字符串。这包括正向最大匹配法、逆向最大匹配法以及双向最大匹配法。</p> </li><li> <p>基于理解的方法：通过模拟人类理解句子含义进行分词，考虑上下文、句法结构和其他信息。</p> </li><li> <p>基于统计学习模型：利用机器学习算法从大量已经人工标注好了分词结果的数据集里学习如何进行有效地分词。常见算法包括隐马尔可夫模型(HMM)、条件随机场(CRF)以及近年来流行起来基于深度学习框架构建神经网络模型(RNNs、CNNs、LSTMs、Transformer、BERT等)。</p> </li><li> <p>混合方法：结合以上几种不同策略以提高精确度和鲁棒性。</p> </li><li> <p>基于规则: 通过制定一系列规则手动或半自动地进行文字断开, 这通常需要专业知识并且效率不高, 但可以在特定情境下发挥作用。</p> </li></ol> 
<h4><a id="Python_16"></a>Python栗子</h4> 
<p>基于字符串匹配，最大前向匹配，代码如下</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">max_match_segmentation</span><span class="token punctuation">(</span>text<span class="token punctuation">,</span> dictionary<span class="token punctuation">)</span><span class="token punctuation">:</span>
    max_word_length <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>word<span class="token punctuation">)</span> <span class="token keyword">for</span> word <span class="token keyword">in</span> dictionary<span class="token punctuation">)</span>
    start <span class="token operator">=</span> <span class="token number">0</span>
    segmentation <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

    <span class="token keyword">while</span> start <span class="token operator">&lt;</span> <span class="token builtin">len</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> length <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>max_word_length<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> length <span class="token operator">&gt;</span> <span class="token builtin">len</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span> <span class="token operator">-</span> start<span class="token punctuation">:</span>
                <span class="token keyword">continue</span>
            word <span class="token operator">=</span> text<span class="token punctuation">[</span>start<span class="token punctuation">:</span>start <span class="token operator">+</span> length<span class="token punctuation">]</span>
            <span class="token keyword">if</span> word <span class="token keyword">in</span> dictionary<span class="token punctuation">:</span>
                segmentation<span class="token punctuation">.</span>append<span class="token punctuation">(</span>word<span class="token punctuation">)</span>
                start <span class="token operator">+=</span> length
                <span class="token keyword">break</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>  <span class="token comment"># 如果没有找到，则按单字切分</span>
            segmentation<span class="token punctuation">.</span>append<span class="token punctuation">(</span>text<span class="token punctuation">[</span>start<span class="token punctuation">]</span><span class="token punctuation">)</span>
            start <span class="token operator">+=</span> <span class="token number">1</span>

    <span class="token keyword">return</span> segmentation

<span class="token comment"># 示例字典和用法：</span>
dictionary <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token string">"我"</span><span class="token punctuation">,</span> <span class="token string">"爱"</span><span class="token punctuation">,</span> <span class="token string">"北京"</span><span class="token punctuation">,</span> <span class="token string">"天安门"</span><span class="token punctuation">}</span>
text_to_segment <span class="token operator">=</span> <span class="token string">"我爱北京天安门"</span>

segments <span class="token operator">=</span> max_match_segmentation<span class="token punctuation">(</span>text_to_segment<span class="token punctuation">,</span> dictionary<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"分词结果："</span><span class="token punctuation">,</span> <span class="token string">"/ "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>segments<span class="token punctuation">)</span><span class="token punctuation">)</span>

</code></pre> 
<h4><a id="_50"></a>构建思路（如何实现基于统计的分词方法）</h4> 
<p>基于统计学习的中文分词方法其核心思想是从大量已经分词的文本（语料库）中学习如何将连续的汉字序列切分成有意义的词汇。通常包括以下几个步骤：</p> 
<ol><li> <p><strong>语料库准备</strong>：收集并整理一定量的已经进行过人工分词处理的文本数据，作为训练集。</p> </li><li> <p><strong>特征提取</strong>：从训练数据中提取有助于模型学习和预测的特征。在传统统计模型中，这些特征可能包括：</p> 
  <ul><li>字符及其邻近字符</li><li>词性标注信息</li><li>字符组合频率</li></ul> </li><li> <p><strong>概率模型选择</strong>：选择合适的统计概率模型来估算不同切分方式出现的概率。常见模型包括：</p> 
  <ul><li>隐马尔可夫模型（HMM）</li><li>条件随机场（CRF）</li><li>最大熵模型</li><li>支持向量机（SVM）</li></ul> </li><li> <p><strong>参数估计与训练</strong>：利用选定的统计学习算法对特征和标签进行建模，并通过算法调整参数以最大化某种性能指标或者最小化误差。</p> </li><li> <p><strong>解码与优化</strong>：使用如Viterbi算法等解码技术找到给定字序列下最可能对应的词序列。</p> </li><li> <p><strong>评估与调整</strong>：通过交叉验证、留出验证或引入开发集等方式，在非训练数据上评价分词效果，并据此调整特征或者优化参数。</p> </li><li> <p><strong>迭代改进</strong>: 在实际应用过程中，根据反馈持续追踪新出现单字、新兴流行语等元素，更新语料库并重新训练以保证系统性能不断提升。</p> </li></ol> 
<p>基于统计学习方法进行中文分词具有较强实用性和广泛适用性。它不依赖复杂规则体系，而是通过从数据本身“学会”如何正确地将句子划分为单个单词或短语。</p> 
<h4><a id="_76"></a>分词参考链接</h4> 
<ul><li>tokenizer <a href="https://huggingface.co/docs/tokenizers" rel="nofollow">https://huggingface.co/docs/tokenizers</a></li><li>微型中文分词器 <a href="https://github.com/howl-anderson/MicroTokenizer">https://github.com/howl-anderson/MicroTokenizer</a></li><li>中文分词jieba <a href="https://github.com/fxsjy/jieba">https://github.com/fxsjy/jieba</a></li><li>THULAC：一个高效的中文词法分析工具包<a href="https://github.com/thunlp/THULAC-Python">https://github.com/thunlp/THULAC-Python</a></li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/75d2db33b69bc80a107e8d93f0739c49/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">一起学docker（六）| Dockerfile自定义镜像 &#43; 微服务模块实战</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/91d108e1daf9f69752868e9a7784d0a9/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">74HC595驱动数码管程序</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>