<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>读论文（二） - BERT - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="读论文（二） - BERT" />
<meta property="og:description" content="Introduction 预训练的语言模型，在改进自然语言处理任务方面非常有效。包括句子级别的任务（自然语言推理和释义）也包括分词级别的任务（NER和问答）。
将预训练的语言表示应用于下游任务有两种现有策略：基于特征（feature-based）与微调（fine-tuning）。（这两种方法在预训练期间共享相同的目标函数，它们使用单​​向语言模型来学习通用语言表示。）
基于特征：例如 ELMo使用特定于任务的架构，其中包括预训练的表示作为附加特征。
微调：引入了最少的任务特定参数，并通过简单地微调所有预训练参数来对下游任务进行训练。
但是，当前的技术限制了预训练表示的能力，特别是对于微调方法。主要限制是标准语言模型是单向的，这限制了可在预训练期间使用的架构的选择。这样的限制对于句子级任务来说是次优的，并且在将基于微调的方法应用于令牌级任务（例如问答）时可能非常有害。（提出问题）
BERT 所有总结的bert的知识点都在这：http://t.csdn.cn/YsF9N
Experiment 我们展示了 11 个 NLP 任务的 BERT 微调结果。（属于4类）
第一类任务运行结果：
Ablation Studies(消融实验) 我们首先考察 NSP 任务带来的影响。在表 5 中，我们表明移除 NSP 会显着损害 QNLI、MNLI 和 SQuAD 1.1 的性能。接下来，我们通过比较“No NSP”与“LTR &amp; No NSP”来评估训练双向表示的影响。 LTR 模型在所有任务上的表现都比 MLM 模型差，在 MRPC 和 SQuAD 上的下降幅度很大。
在本节中，我们探讨了模型大小对微调任务准确性的影响。我们训练了许多具有不同层数、隐藏单元和注意力头的 BERT 模型，同时使用与前面描述的相同的超参数和训练过程。选定 GLUE 任务的结果如表 6 所示。可以看到，更大的模型会导致所有四个数据集的准确度得到严格的提高。
最近由于使用语言模型进行迁移学习的经验改进表明，丰富的、无监督的预训练是许多语言理解系统不可或缺的一部分。特别是，这些结果使即使是低资源任务也能从深度单向架构中受益。我们的主要贡献是将这些发现进一步推广到深度双向架构，允许相同的预训练模型成功处理广泛的 NLP 任务。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/e6fd3c2222a1995b4e3d165a1d052025/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-10-17T15:43:38+08:00" />
<meta property="article:modified_time" content="2022-10-17T15:43:38+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">读论文（二） - BERT</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3><strong><span style="background-color:#ffd900;">Introduction</span></strong></h3> 
<p>        预训练的语言模型，在改进自然语言处理任务方面非常有效。包括句子级别的任务<em>（自然语言推理和释义）</em>也包括分词级别的任务（<em>NER和问答</em>）。</p> 
<p>        将预训练的语言表示应用于下游任务有两种现有策略：<strong>基于特征（feature-based）与微调（fine-tuning）</strong>。（这两种方法在预训练期间共享相同的目标函数，它们使用单​​向语言模型来学习通用语言表示。）</p> 
<p>        <em>基于特征：例如 ELMo使用特定于任务的架构，其中包括预训练的表示作为附加特征。</em></p> 
<p><em>        微调：</em>引入了最少的任务特定参数，并通过简单地微调所有预训练参数来对下游任务进行训练。</p> 
<p>       但是，当前的技术限制了预训练表示的能力，特别是对于微调方法。主要限制是标准语言模型是单向的，这限制了可在预训练期间使用的架构的选择。这样的限制对于句子级任务来说是次优的，并且在将基于微调的方法应用于令牌级任务（例如问答）时可能非常有害。（<strong><span style="background-color:#a2e043;">提出问题</span></strong>）</p> 
<h3><span style="background-color:#ffd900;">BERT</span>  </h3> 
<p>       所有总结的bert的知识点都在这：<a href="http://t.csdn.cn/YsF9N" rel="nofollow" title="http://t.csdn.cn/YsF9N">http://t.csdn.cn/YsF9N</a></p> 
<h3><span style="background-color:#ffd900;">Experiment</span></h3> 
<p> 我们展示了 11 个 NLP 任务的 BERT 微调结果。（属于4类）</p> 
<p><img alt="" src="https://images2.imgbox.com/4d/19/dPbJAkea_o.jpg"></p> 
<p> 第一类任务运行结果：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/ef/a9/hyrWhXa1_o.png"></p> 
<h3><span style="background-color:#ffd900;">Ablation Studies(消融实验)</span></h3> 
<p style="text-align:center;"> <img alt="" src="https://images2.imgbox.com/c1/f6/Zhdx7Bi2_o.png"></p> 
<p>         我们首先考察 NSP 任务带来的影响。在表 5 中，我们表明移除 NSP 会显着损害 QNLI、MNLI 和 SQuAD 1.1 的性能。接下来，我们通过比较“No NSP”与“LTR &amp; No NSP”来评估训练双向表示的影响。 LTR 模型在所有任务上的表现都比 MLM 模型差，在 MRPC 和 SQuAD 上的下降幅度很大。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/e9/9a/wGAD2fgL_o.png"></p> 
<p>        在本节中，我们探讨了模型大小对微调任务准确性的影响。我们训练了许多具有不同层数、隐藏单元和注意力头的 BERT 模型，同时使用与前面描述的相同的超参数和训练过程。选定 GLUE 任务的结果如表 6 所示。可以看到，更大的模型会导致所有四个数据集的准确度得到严格的提高。</p> 
<p>        最近由于使用语言模型进行迁移学习的经验改进表明，丰富的、无监督的预训练是许多语言理解系统不可或缺的一部分。特别是，这些结果使即使是低资源任务也能从深度单向架构中受益。我们的主要贡献是将这些发现进一步推广到深度双向架构，允许相同的预训练模型成功处理广泛的 NLP 任务。</p> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c1f6d64c1d42f2fc63031a02cfa955c6/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">SpringBoot项目使用docker-maven-plugin插件构建docker镜像以及推送到docker hub或docker registry私服</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/36aa6ec283cfd7aac6a8a8dac99fba2a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">网易云ncm,QQ音乐qmc,mgg,mflac，酷狗kgm解锁转换为flac格式</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>