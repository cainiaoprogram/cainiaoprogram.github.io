<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>opencv特征检测和匹配 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="opencv特征检测和匹配" />
<meta property="og:description" content="opencv可以检测图像的主要特征，然后提取这些特征，使其成为图像描述符，利用这些图像描述符来搜索数据库里，进行图像的检测
1.特征检测算法
算法：
Harris：用于检测角点SIFT：用于检测斑点SURF：用于检测斑点FAST：用于检测角点BRIEF：用于检测斑点ORB：代表带有方向的FAST算法与具有旋转不变性的BRIEF算法 方法：
暴力匹配法基于FLANN的匹配法 2.cornerHarris角点检测
import cv2 import numpy as np img = cv2.imread(&#39;E:\\opencv3\\charpter6\\chess_board.jpg&#39;) gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) gray = np.float32(gray)#为了使用cornerHarris函数，需要将棋盘图像转化为灰度个事 dst = cv2.cornerHarris(gray,4,21,0.04)#从灰度图像做处理，提取角点信息 #第二个参数：参数值越小，标记点的大小越小 #第三个参数：参数值只能在3-31之间，而且必须是奇数，表示敏感度，越小，敏感度越强，会把很多细节的东西也算成角点 #第三个参数：自由参数,取值参数为 [0,04,0.06] img[dst&gt;0.01*dst.max()] = [0,0,255] while (True): cv2.imshow(&#39;result&#39;,img) if cv2.waitKey(5) &amp; 0xff == ord(&#39;q&#39;): break cv2.destroyAllWindows() dst&gt;0.01*dst.max()这么多返回是满足条件的dst索引值，根据索引值来设置这个点的颜色，这里是设定一个阈值　当大于这个阈值分数的都可以判定为角点
3.DOG和SIFT进行特征提取与描述
cornerHarris函数可以很好的检测到角点，这与焦点的特性有关，即使图片旋转，也能检测到角点，但是如果图像压缩了或者增大了，那么cornerHarris检测的角点信息就可能遗漏，图像越小越有可能丢失掉角点信息
这使我们可以用一种与图像比例无关的角点检测方法：SIFT
该函数在图像尺度变化时依然能输出同样的结果，但是SIFT并不检测关键点，关键点是由DOG检测的，但是SIFT会通过一个特征向量来描述特征点周围的区域情况
DOG（Differen Of Gaussians）
DOG是对同一副图像使用不同的高斯滤波器所得到的结果，前面有提到过cv2.GaussianBlur()低通滤波的模糊处理效果，DOG技术可以有效地检测到边缘，做种结果是感兴趣的区域（关键点）
import cv2 import numpy as np impath = &#39;E:\\opencv3\\charpter6\\chess_board.jpg&#39; img = cv2.imread(impath) gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) sift = cv2." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/ee508fcd30ced8eb1110313b61b04614/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-06-03T17:18:39+08:00" />
<meta property="article:modified_time" content="2019-06-03T17:18:39+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">opencv特征检测和匹配</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atelier-sulphurpool-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>opencv可以检测图像的主要特征，然后提取这些特征，使其成为图像描述符，利用这些图像描述符来搜索数据库里，进行图像的检测</p> 
<p><strong>1.特征检测算法</strong></p> 
<p><strong>算法：</strong></p> 
<ul><li>Harris：用于检测角点</li><li>SIFT：用于检测斑点</li><li>SURF：用于检测斑点</li><li>FAST：用于检测角点</li><li>BRIEF：用于检测斑点</li><li>ORB：代表带有方向的FAST算法与具有旋转不变性的BRIEF算法</li></ul> 
<p><strong>方法：</strong></p> 
<ul><li>暴力匹配法</li><li>基于FLANN的匹配法</li></ul> 
<p><strong>2.cornerHarris角点检测</strong></p> 
<pre><code>import cv2
import numpy as np


img = cv2.imread('E:\\opencv3\\charpter6\\chess_board.jpg')
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
gray = np.float32(gray)#为了使用cornerHarris函数，需要将棋盘图像转化为灰度个事
dst = cv2.cornerHarris(gray,4,21,0.04)#从灰度图像做处理，提取角点信息
#第二个参数：参数值越小，标记点的大小越小
#第三个参数：参数值只能在3-31之间，而且必须是奇数，表示敏感度，越小，敏感度越强，会把很多细节的东西也算成角点
#第三个参数：自由参数,取值参数为 [0,04,0.06]

img[dst&gt;0.01*dst.max()] = [0,0,255]

while (True):
    cv2.imshow('result',img)
    if cv2.waitKey(5) &amp; 0xff == ord('q'):
        break

cv2.destroyAllWindows()
</code></pre> 
<p><img src="https://images2.imgbox.com/af/f5/mpKef5YU_o.png" alt="在这里插入图片描述"><br> dst&gt;0.01*dst.max()这么多返回是满足条件的dst索引值，根据索引值来设置这个点的颜色，这里是设定一个阈值　当大于这个阈值分数的都可以判定为角点</p> 
<p><strong>3.DOG和SIFT进行特征提取与描述</strong><br> cornerHarris函数可以很好的检测到角点，这与焦点的特性有关，即使图片旋转，也能检测到角点，但是如果图像压缩了或者增大了，那么cornerHarris检测的角点信息就可能遗漏，图像越小越有可能丢失掉角点信息</p> 
<p>这使我们可以用一种与图像比例无关的角点检测方法：<strong>SIFT</strong></p> 
<p>该函数在图像尺度变化时依然能输出同样的结果，但是SIFT并不检测关键点，关键点是由DOG检测的，但是SIFT会通过一个特征向量来描述特征点周围的区域情况</p> 
<p><strong>DOG（Differen Of Gaussians）</strong><br> DOG是对同一副图像使用不同的高斯滤波器所得到的结果，前面有提到过cv2.GaussianBlur()低通滤波的模糊处理效果，DOG技术可以有效地检测到边缘，做种结果是感兴趣的区域（关键点）</p> 
<pre><code>import cv2
import numpy as np

impath = 'E:\\opencv3\\charpter6\\chess_board.jpg'
img = cv2.imread(impath)
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

sift = cv2.xfeatures2d.SIFT_create()
keypoints,descriptor = sift.detectAndCompute(gray,None)
#返回的是关键点信息和描述符

img = cv2.drawKeypoints(image = img,
                        outImage = img,
                        keypoints = keypoints,
                        flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS,
                        color = (51,163,236))

cv2.imshow('sift_keypoints',img)

while(True):
    if cv2.waitKey(5) &amp; 0xff == ord('q'):
        break
    
cv2.destroyAllWindows()
</code></pre> 
<p><img src="https://images2.imgbox.com/80/e8/cEtz7mRK_o.png" alt="在这里插入图片描述"></p> 
<pre><code>sift = cv2.xfeatures2d.SIFT_create()
keypoints,descriptor = sift.detectAndCompute(gray,None)
</code></pre> 
<p>这里创建了一个SIFT对象，SIFT对象会使用DOG检测关键点，并对灰度图像进行计算，计算每个关键点周围的区域特征向量，总的来说就是进行两个主要的操作：检测，计算。</p> 
<p>这里得到的<strong>keypoints</strong>：<br> <img src="https://images2.imgbox.com/0b/28/p7G8xql4_o.png" alt="在这里插入图片描述"><br> 一共有840个特征点，每个特征点有六个属性：<br> **pt：**表示图像中关键点的x，y坐标<br> keypoints[3].pt<br> Out[3]: (9.074405670166016, 35.200592041015625)</p> 
<p>**size：**表示特征的直径<br> keypoints[3].size<br> Out[4]: 5.260268211364746</p> 
<p>**angle：**表示特征的方向<br> keypoints[3].angle<br> Out[5]: 0.021240234375</p> 
<p>**response：**表示关键点的强度，有的特征会比其他特征好一些，response可以查看评估特征强度<br> keypoints[10].response<br> Out[7]: 0.053790051490068436</p> 
<p>**octave：**表示所在金字塔的层级<br> keypoints[10].octave<br> Out[8]: 9372159</p> 
<p>**class_id：**表示关键点的id<br> keypoints[10].class_id<br> Out[10]: -1</p> 
<p>这里的<strong>descriptor</strong>：<br> 表示计算的特征点周围的特征向量</p> 
<p>descriptor<br> Out[11]:<br> array([[ 1., 1., 12., …, 0., 0., 1.],<br> [ 0., 0., 0., …, 2., 1., 5.],<br> [ 0., 0., 0., …, 0., 0., 7.],<br> …,<br> [ 2., 0., 0., …, 67., 19., 38.],<br> [20., 0., 0., …, 0., 0., 0.],<br> [20., 2., 0., …, 0., 0., 0.]], dtype=float32)<br> 这是一个（840，128）的矩阵，每个特征点都用一个128维的特征向量表示</p> 
<p>画图时用的：</p> 
<pre><code>cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS
</code></pre> 
<p>表示画图时每个关键点都绘制圆圈和方向</p> 
<p><strong>4.Hessian算法和SURF进行特征的提取和检测</strong></p> 
<p>SURF特征检测算法比SIFT算法快好几倍，它吸收了SIFT算法的思想</p> 
<p>SURF与SIFT原理类似：<br> SUFT采用Hessian算法检测关键点，用SURF提取特征；<br> SIFT采用DOG算法检测关键点，用SIFT提取特征。</p> 
<pre><code>import cv2
import numpy as np

impath = 'E:\\opencv3\\charpter6\\chess_board.jpg'
img = cv2.imread(impath)
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

threshold = 8000 #阈值，阈值越小特征点越多

surt = cv2.xfeatures2d.SURF_create(float(threshold))
keypoints,descriptor = surt.detectAndCompute(gray,None)
#返回的是关键点信息和描述符

img = cv2.drawKeypoints(image = img,
                        outImage = img,
                        keypoints = keypoints,
                        flags = 4,
                        color = (51,163,236))

cv2.imshow('surf_keypoints',img)

while(True):
    if cv2.waitKey(5) &amp; 0xff == ord('q'):
        break
    
cv2.destroyAllWindows()
                        
</code></pre> 
<p>SURF跟SIFT比较就是多了一个阈值，这个值越高，能识别的特征就越少，可以用试探法来得到最优检测。<br> <img src="https://images2.imgbox.com/6b/86/VX8qttkt_o.png" alt="在这里插入图片描述"><br> SIFT和SURF的比较：<br> <a href="https://blog.csdn.net/jwh_bupt/article/details/6567452">参考文献1</a></p> 
<p><strong>5.FAST算法</strong><br> FAST 全称 Features from accelerated segment test,一种用于<strong>角点检测的算法</strong>，该算法的原理是取图像中检测点，以该点为圆心的周围的16个像素点判断检测点是否为角点。通俗的讲就是中心的的像素值比大部分周围的像素值要亮一个阈值 或者 暗一个阈值则为角点。</p> 
<p><img src="https://images2.imgbox.com/21/8e/ZcD3yY56_o.png" alt="在这里插入图片描述"><br> 对于检测点p，若周围的16个像素点中有N个连续的点的像素值都比其小一个阈值 或 大 一个阈值，则该点可作为角点。即img[x] &lt; img[p] - threshold 或 img[x] &gt; img[p] + threshold</p> 
<p><strong>6.BRIEF算法</strong><br> BRIEF并不是检测算法，它只是一个<strong>描述符</strong>。<br> SIFT和SURF的核心是detectAndCompute（）函数，该函数包括两个步骤：检测和计算</p> 
<p><strong>关键点描述符</strong>是图像的一种表示，可以通过比较两个图片的关键点描述符，找出共同之处，因此描述符是现在作为特征匹配的一种方法</p> 
<p>BRIEF是目前最快的描述符。</p> 
<p><strong>7.暴力匹配法（Brute-Force）</strong><br> 暴力匹配法是一种<strong>描述符匹配方法</strong>。原理是，比较两个描述符，产生匹配列表。<strong>暴力</strong>是指该算法不涉及优化，第一个描述符的所有特征都用来跟第二个描述符比较，每次比较算出一个距离值，距离值最小的被认为是匹配的。</p> 
<h3><a id="_196"></a>综上所述</h3> 
<p>Harris，FAST都是用来检测角点的<br> SIFT（中的DOG），SURF（中的Hessian）是检测斑点的（关键点区域）<br> BRIEF也是检测斑点的。<br> ORB特征是基于FAST和BRIEF算法的，也是一种检测特征的算法</p> 
<p>检测到了这些图像描述符，就需要进行一个匹配的操作，匹配的操作有：<br> 暴力匹配法，FLANN匹配。<br> ORB特征匹配、K近邻特征匹配都是用暴力匹配法作为匹配器（BFMatcher ）的</p> 
<p><strong>8.ORB特征匹配</strong></p> 
<p>ORB特征匹配可以得到的结果：</p> 
<ul><li>给FAST增加一个快速的准确的方向分量</li><li>能高效的计算带方向的BRIEF特征</li><li>在旋转的方式下，还能使用BRIEF查询图像之间的匹配情况</li></ul> 
<pre><code>import cv2
import numpy as np
import matplotlib.pyplot as plt

img1 = cv2.imread('chess_board.jpg',cv2.IMREAD_GRAYSCALE)
img2 = cv2.imread('chess_min.jpg',cv2.IMREAD_GRAYSCALE)


orb = cv2.ORB_create()
kp1,des1 = orb.detectAndCompute(img1,None)
kp2,des2 = orb.detectAndCompute(img2,None)

bf = cv2.BFMatcher(cv2.NORM_HAMMING,crossCheck = True)
matches = bf.match(des1,des2)
matches = sorted(matches,key = lambda x:x.distance)

img3 = cv2.drawMatches(img1,kp1,img2,kp2,matches[:40],img2,flags = 2)


plt.imshow(img3)
plt.show()

plt.imsave('orb.jpg',img3)
</code></pre> 
<pre><code>img1 = cv2.imread('chess_board.jpg',cv2.IMREAD_GRAYSCALE)
</code></pre> 
<p>imread（）的第二个参数也可以用0代替，这里的参数还可以填其他的：<br> IMREAD_ANYCOLOR 4<br> IMREAD_ANYDEPTH 2<br> IMREAD_COLOR 1<br> IMREAD_GRAYSCALE 0<br> IMREAD_LOAD_GDAL 8<br> IMREAD_UNCHANGED -1</p> 
<pre><code>orb = cv2.ORB_create()
kp1,des1 = orb.detectAndCompute(img1,None)
kp2,des2 = orb.detectAndCompute(img2,None)
</code></pre> 
<p>对查询的图像和训练图片都要进行检测，然后计算关键点和描述符<br> kp1有500个关键点，kp2有392个关键点<br> des1，des2分别是这些关键点的特征向量，直接暴力匹配，遍历描述符，在满足一定置信度下，显示前n个匹配的描述符。</p> 
<pre><code>bf = cv2.BFMatcher(cv2.NORM_HAMMING,crossCheck = True)
matches = bf.match(des1,des2)
matches = sorted(matches,key = lambda x:x.distance)
</code></pre> 
<p>这是暴力匹配的过程</p> 
<p>原始图片：<br> <img src="https://images2.imgbox.com/d0/a9/S97B5UxP_o.png" alt="在这里插入图片描述"><br> 局部图片：<br> <img src="https://images2.imgbox.com/9d/c3/SR1WgVFs_o.png" alt="在这里插入图片描述"><br> 匹配图：<br> <img src="https://images2.imgbox.com/92/98/zgb8kJ0i_o.png" alt="在这里插入图片描述"><br> <strong>9.K-近邻匹配</strong><br> KNN是一种匹配检测算法。</p> 
<pre><code>import numpy as np
import cv2
from matplotlib import pyplot as plt

img1 = cv2.imread('chess_min.jpg',cv2.IMREAD_GRAYSCALE)#查询图像
img2 = cv2.imread('chess_board.jpg',cv2.IMREAD_GRAYSCALE)#训练图像

#开始创建orb特征检测器和描述符
orb = cv2.ORB_create()
kp1,des1 = orb.detectAndCompute(img1,None)
kp2,des2 = orb.detectAndCompute(img2,None)

#使用默认参数的BFMatch()
bf = cv2.BFMatcher()
matches = bf.knnMatch(des1,des2,k = 2)###knnMatch用knnMatch返回k个匹配

###drawMatchesKnn绘制匹配
img3 = cv2.drawMatchesKnn(img1,kp1,img2,kp2,matches,img2,flags = 2)
#先填写查询图像，后填写训练图像

plt.imshow(img3)
plt.show()

plt.imsave('knn.jpg',img3)
</code></pre> 
<p><strong>cv2.BFMatcher</strong>创建一个 BF-Matcher 对象，它有两个可选参数。</p> 
<p>第一个是用来指定要使用的距离测试类型，默认值为 cv2.Norm_L2。对于使用二进制描述符的 ORB,BRIEF,BRISK算法等,要使用 cv2.NORM_HAMMING,这样就会返回两个测试对象之间的汉明距离。如果 ORB 算法的参数设置为 V T A_K==3 或 4,normType就应该设置成 cv2.NORM_HAMMING2。</p> 
<p>第二个参数是布尔变量 crossCheck,默认值为 False。如果设置为True,匹配条件就会更加严格,只有到 A 中的第 i 个特征点与 B 中的第 j 个特征点距离最近,并且 B 中的第 j 个特征点到 A 中的第 i 个特征点也是最近(A 中没有其他点到 j 的距离更近)时才会返回最佳匹配(i,j)。也就是这两个特征点要互相匹配才行。这样就能提供统一的结果。</p> 
<p>BFMatcher 对象具有两个方法, <strong>BFMatcher.match()</strong> 和 <strong>BFMatcher.knnMatch()</strong>。第一个方法会返回最佳匹配。第二个方法为每个关键点返回 k 个最佳匹配(降序排列之后取前 k 个),其中 k 是由用户设定的。<br> <img src="https://images2.imgbox.com/ae/2e/1OGWPK67_o.png" alt="在这里插入图片描述"></p> 
<p>采用比率测试来过滤掉不满足用户定义条件的匹配：</p> 
<pre><code># 应用比率测试(ratio test)
good = []
for m, n in matches:
    if m.distance &lt; 0.75 * n.distance:
        good.append([m])
 
img3 = cv2.drawMatchesKnn(img1, kp1, img2, kp2, good, None, flags=2)

plt.imshow(img3)
plt.show()

plt.imsave('knn_ratio_test.jpg',img3)
</code></pre> 
<p><img src="https://images2.imgbox.com/0e/25/Ns41gd1t_o.png" alt="在这里插入图片描述"></p> 
<pre><code>np.shape(matches)
Out[8]: (500, 2)
</code></pre> 
<p>matches有500个匹配，每个匹配匹配两个特征点</p> 
<pre><code>matches[0]
Out[12]: [&lt;DMatch 000001E132944E70&gt;, &lt;DMatch 000001E132A96BB0&gt;]
</code></pre> 
<p>cv2.DMatch是opencv中匹配函数(例如：knnMatch)返回的用于匹配关键点描述符的类，这个DMatch 对象具有下列属性：</p> 
<p>• DMatch.distance - 描述符之间的距离。越小越好。<br> • DMatch.trainIdx - 目标图像中描述符的索引。<br> • DMatch.queryIdx - 查询图像中描述符的索引。<br> • DMatch.imgIdx - 目标图像的索引。</p> 
<p>if m.distance &lt; 0.75 * n.distance<br> 这里的参数越小，匹配越严格，匹配对数越少</p> 
<pre><code>matches[0][0].distance
Out[13]: 379.7446594238281

matches[0][1].distance
Out[14]: 398.87591552734375
</code></pre> 
<p>这里good：<br> [[&lt;DMatch 000001E1343B4450&gt;],<br> [&lt;DMatch 000001E1343A7470&gt;],<br> [&lt;DMatch 000001E1343C15D0&gt;],<br> [&lt;DMatch 000001E1343C17B0&gt;],<br> [&lt;DMatch 000001E1343C1BD0&gt;],<br> [&lt;DMatch 000001E1343A9070&gt;],<br> [&lt;DMatch 000001E1343A93D0&gt;],<br> [&lt;DMatch 000001E1343A9790&gt;],<br> [&lt;DMatch 000001E1343A97D0&gt;],<br> [&lt;DMatch 000001E1343A99B0&gt;],<br> [&lt;DMatch 000001E1343A9CD0&gt;],<br> [&lt;DMatch 000001E1343B5190&gt;],<br> [&lt;DMatch 000001E1343B5210&gt;],<br> [&lt;DMatch 000001E1343B5470&gt;],<br> [&lt;DMatch 000001E1343B5AD0&gt;],<br> [&lt;DMatch 000001E1343B5D70&gt;],<br> [&lt;DMatch 000001E1343C4790&gt;]]</p> 
<p>如果要访问queryIdx，trainIdx，imgIdx</p> 
<pre><code>good[4][0].queryIdx
Out[69]: 146
</code></pre> 
<pre><code>matches[0][0].queryIdx
Out[72]: 0
</code></pre> 
<p>一定要保证访问属性的是DMatch而不是[DMatch]</p> 
<p><img src="https://images2.imgbox.com/3e/9c/mgWeqyUr_o.png" alt="在这里插入图片描述"></p> 
<p><strong>10.FLANN匹配</strong><br> FLANN比其他的最近邻搜索软件快10倍，基于FLANN的匹配非常准确，快速方便。</p> 
<pre><code>import numpy as np
import cv2
from matplotlib import pyplot as plt

queryImage = cv2.imread('chess_min.jpg',0)#查询图像
trainingImage = cv2.imread('chess_board.jpg',0)#训练图像

#创建一个SIFT对象，检测并计算
sift = cv2.xfeatures2d.SIFT_create()
kp1,des1 = sift.detectAndCompute(queryImage,None)
kp2,des2 = sift.detectAndCompute(trainingImage,None)


#创建FLANN匹配器
FLANN_INDEX_KDTREE = 0
indexParams = dict(algorithm = FLANN_INDEX_KDTREE,tree = 5)
searchParams = dict(checks = 50)#指定索引树被遍历多少次，次数越多，计算时间越长，越精确

flann = cv2.FlannBasedMatcher(indexParams,searchParams)
matches = flann.knnMatch(des1,des2,k = 2)

#准备一个空的mask画good匹配
matchesMask = [[0,0] for i in range(len(matches))]

for i,(m,n) in enumerate(matches):
    if m.distance &lt; 0.7*n.distance:
        matchesMask[i] = [1,0]
        
drawParams = dict(matchColor = (0,255,0),
                  singlePointColor = (255,0,0),
                  matchesMask = matchesMask,
                  flags = 0)#设置匹配的颜色，特征点的颜色

resultImage = cv2.drawMatchesKnn(queryImage,kp1,trainingImage,kp2,matches,None,**drawParams)
#用指定的颜色上色，就不用彩色的那种了

plt.imshow(resultImage)
plt.show()

plt.imsave('flann.jpg',resultImage)
</code></pre> 
<p><img src="https://images2.imgbox.com/ae/ea/C0XPaDNL_o.png" alt="在这里插入图片描述"></p> 
<p>FLANN匹配器有两个参数：indexParams，searchParams，这两个参数在python中以字典形式进行参数传递，C++中以结构体形式进行参数传递。<br> indexParams：需要我们配置要使用的算法，这里用的是随机k-d树算法（The Randomized k-d TreeAlgorithm）（kd-trees可以指定待处理核密度树的数量，最理想的数量在1-16之间），当然也可以使用其他的算法，如：LinearIndex，KTreeIndex，KMeansIndex,CompositeIndex,AutotuneIndex。<a href="https://blog.csdn.net/qq_36387683/article/details/80578480">参考文献2</a><br> SearchParams：这是一个字典，它用来指定递归遍历的次数。值越高结果越准确，但是消耗的时间也越多。如果想修改这个值，可以传入参数。</p> 
<p>这里5kd-trees和50checks能在合理的精度范围内，并且读时间完成</p> 
<pre><code>for m,n in matches:
    print(m)
    print(n)
</code></pre> 
<p>得到的是<br> &lt;DMatch 000001E1343994D0&gt;<br> &lt;DMatch 000001E1343994F0&gt;<br> &lt;DMatch 000001E134399510&gt;<br> &lt;DMatch 000001E134399530&gt;<br> &lt;DMatch 000001E134399550&gt;<br> &lt;DMatch 000001E134399570&gt;<br> &lt;DMatch 000001E134399590&gt;<br> &lt;DMatch 000001E1343995B0&gt;<br> &lt;DMatch 000001E1343995D0&gt;<br> &lt;DMatch 000001E1343995F0&gt;<br> &lt;DMatch 000001E134399610&gt;<br> &lt;DMatch 000001E134399630&gt;<br> &lt;DMatch 000001E134399650&gt;</p> 
<pre><code>for i,(m,n) in enumerate(matches):
    print(i)
    print(m)
    print(n)
</code></pre> 
<p>692<br> &lt;DMatch 000001E134399470&gt;<br> &lt;DMatch 000001E134399490&gt;<br> 693<br> &lt;DMatch 000001E1343994B0&gt;<br> &lt;DMatch 000001E1343994D0&gt;<br> 694<br> &lt;DMatch 000001E1343994F0&gt;<br> &lt;DMatch 000001E134399510&gt;<br> 695<br> &lt;DMatch 000001E134399530&gt;<br> &lt;DMatch 000001E134399550&gt;<br> 696<br> &lt;DMatch 000001E134399570&gt;<br> &lt;DMatch 000001E134399590&gt;<br> 697<br> &lt;DMatch 000001E1343995B0&gt;<br> &lt;DMatch 000001E1343995D0&gt;<br> 698<br> &lt;DMatch 000001E1343995F0&gt;<br> &lt;DMatch 000001E134399610&gt;<br> 699<br> &lt;DMatch 000001E134399630&gt;<br> &lt;DMatch 000001E134399650&gt;</p> 
<pre><code>if m.distance &lt; 0.7*n.distance
</code></pre> 
<p>这里丢弃掉任何距离大于0.7的值，可以避免90%的错误匹配</p> 
<p><strong>cv2.drawMatches()与cv2.drawMatchesKnn()区别</strong><br> cv2.drawMatches()使用的点对集是一维的，（N，）<br> cv2.drawMatchesKnn()使用的点对集是二维的，（N，2）或者（N，1）</p> 
<pre><code>#np.shape(good) is (646,)
#np.shape(matches) is (6202, 2)
</code></pre> 
<pre><code>cv2.drawMatches(queryImage,kp1,trainingImage,kp2,good,None,**draw_params)
</code></pre> 
<pre><code>cv2.drawMatchesKnn(img1, kp1, img2, kp2, matches, None, flags=2)
</code></pre> 
<p><strong>FLANN的单应性匹配</strong><br> 什么叫单应性：<br> 单应性是一个条件，该条件表示，当两幅图像中有一副图像出现了投影几遍（变形了，不是正常的矩形了）时，他们还能匹配</p> 
<pre><code>import numpy as np
import cv2
from matplotlib import pyplot as plt

queryImage = cv2.imread('queryImage.jpg',0)#查询图像
trainingImage = cv2.imread('trainingImage.jpg',0)#训练图像

MIN_MATCH_COUNT = 10

#创建一个sift对象检测和计算特征点
sift = cv2.xfeatures2d.SIFT_create()
kp1,des1 = sift.detectAndCompute(queryImage,None)
kp2,des2 = sift.detectAndCompute(trainingImage,None)

#创建FLANN匹配器
FLANN_INDEX_KDTREE = 0
indexParams = dict(algorithm = FLANN_INDEX_KDTREE,tree = 5)
searchParams = dict(checks = 50)
#指定索引树被遍历多少次，次数越多，计算时间越长，越精确

flann = cv2.FlannBasedMatcher(indexParams,searchParams)
matches = flann.knnMatch(des1,des2,k = 2)

good = []
for m, n in matches:
    if m.distance &lt; 0.7 * n.distance:
        good.append(m)
        
if len(good) &gt; MIN_MATCH_COUNT:
    #如果好的匹配大于MIN_MATCH_COUNT个数的话
    #就在原始图像和训练图像中把这些匹配的关键点m找出来
    #获取关键点的坐标
    src_pts = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1,1,2)
    dst_pts = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1,1,2)
    #这里的m是[DMatch]是一个列表，所以要去列表

#    print('src_pts',src_pts)
#    print('dst_pts',dst_pts)
 
    #单应性 
    #计算变换矩阵和MASK，由关键点坐标匹配得到的变换矩阵  
    M,mask = cv2.findHomography(src_pts,dst_pts,cv2.RANSAC,5.0)
    matchesMask = mask.ravel().tolist()
    
    #对第二张图片计算相对于原始目标的投影畸变，绘制边框
    h,w = queryImage.shape
#    print(h,w)
    pts = np.float32([[0,0],[0,h-1],[w-1,h-1],[w-1,0]]).reshape(-1,1,2)
    dst = cv2.perspectiveTransform(pts,M)
    
    trainingImage = cv2.polylines(trainingImage,[np.int32(dst)],True,255,3,cv2.LINE_AA)

else:
    print('Not enough matches are found - %d/%d' %(len(good),MIN_MATCH_COUNT))
    matchesMask = None
    
draw_params = dict(matchColor = (0,255,0),#匹配线画成绿色
                  singlePointColor = None,
                  matchesMask = matchesMask,
                  flags = 2)#只画线不画匹配点，这里颜色用的RGB标准

resultImage = cv2.drawMatches(queryImage,kp1,trainingImage,kp2,good,None,**draw_params)
#只画好的匹配，good,这里用drawMatches而不是drawMatchesKnn，因为这里是good而不是matches
#np.shape(good) is (646,)
#np.shape(matches) is (6202, 2)

plt.imshow(resultImage)
plt.show()

plt.imsave('flann_mask.jpg',resultImage)
</code></pre> 
<p><img src="https://images2.imgbox.com/d4/3f/ov6lBgR4_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_593"></a>综上所述</h3> 
<p>匹配方法有两种：暴力匹配法（Brute-Force）和FLANN法</p> 
<p>实际代码中<br> <strong>暴力匹配法</strong>由<strong>BFMatcher对象</strong>实现暴力匹配：</p> 
<pre><code>bf = cv2.BFMatcher(cv2.NORM_HAMMING,crossCheck = True)
matches = bf.match(des1,des2)
</code></pre> 
<pre><code>bf = cv2.BFMatcher()
matches = bf.knnMatch(des1,des2,k = 2)###knnMatch用knnMatch返回k个匹配
</code></pre> 
<p><strong>FLANN算法</strong>由<strong>FlannBasedMatcher</strong>实现：</p> 
<pre><code>flann = cv2.FlannBasedMatcher(indexParams,searchParams)
matches = flann.knnMatch(des1,des2,k = 2)
</code></pre> 
<p>而在特征检测器方面</p> 
<p><strong>暴力匹配</strong>多用于<strong>orb特征</strong>的匹配中：</p> 
<pre><code>orb = cv2.ORB_create()
kp1,des1 = orb.detectAndCompute(img1,None)
kp2,des2 = orb.detectAndCompute(img2,None)
</code></pre> 
<p><strong>FLANN</strong>多用于<strong>SIFT特征</strong>的匹配中：</p> 
<pre><code>sift = cv2.xfeatures2d.SIFT_create()
kp1,des1 = sift.detectAndCompute(queryImage,None)
kp2,des2 = sift.detectAndCompute(trainingImage,None)
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/77d4db3bc00924085f28755662aac65d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">软件构造Lab2实验总结</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/945f3fc449518a73b9f5f32868db466c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">lambda</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>