<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>vits官方gituhb项目--模型构建 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="vits官方gituhb项目--模型构建" />
<meta property="og:description" content="在完成VITS论文学习后，对github上的官方仓库进行学习，帮助理解算法实现过程中的一些细节；仓库代码基于pytorch实现，链接为https://github.com/jaywalnut310/vits。本笔记主要针对项目中模型构建部分代码进行注释解析，主要涉及仓库项目中的model.py、modules.py、attentions.py文件。
文章目录 model.pymodules.pyattentions.py model.py 本文件基于各个小模块，对整个VITS的各个一阶模块进行定义，最后集成为多周期判别器/MultiPeriodDiscriminator和生成器/SynthesizerTrn，从训练方式来讲是使用了对抗训练，所以要同时训练判别器和生成器，而生成器，主体是基于VAE的架构，其中又涉及到Flow；特别是本文提出的随机时长预测器，也是一个基于VAE架构，又涉及Flow的一个模块。在VAE和FLOW中涉及大量公式推导。对于随机时长预测器的代码，最好对应论文补充材料中的B.3部分中的结构图对比学习，更加直观；随机时长预测器应用的变分数据增广和变分反量化，可参考论文VFlow和Flow&#43;&#43;。具体代码如下：
import copy import math import torch from torch import nn from torch.nn import functional as F import commons import modules import attentions import monotonic_align from torch.nn import Conv1d, ConvTranspose1d, AvgPool1d, Conv2d from torch.nn.utils import weight_norm, remove_weight_norm, spectral_norm from commons import init_weights, get_padding # 基于flow的vae架构，优化的是最大似然估计，是学习一个分布；推理时，是从分布中采样音素的持续时间，从而具有随机性，只使用基于flow的解码器预测时长 class StochasticDurationPredictor(nn.Module): def __init__(self, in_channels, filter_channels, kernel_size, p_dropout, n_flows=4, gin_channels=0): &#39;&#39;&#39; @param in_channels:输入的维度，时长预测器的输出是文本编码器后的隐向量h_{text} @param filter_channels: @param kernel_size: @param p_dropout: @param n_flows: @param gin_channels: &#39;&#39;&#39; super()." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/0a2711838b8f0f7bd063bfdf337685f2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-12-10T17:18:43+08:00" />
<meta property="article:modified_time" content="2022-12-10T17:18:43+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">vits官方gituhb项目--模型构建</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>在完成VITS论文学习后，对github上的官方仓库进行学习，帮助理解算法实现过程中的一些细节；仓库代码基于pytorch实现，链接为<a href="https://github.com/jaywalnut310/vits">https://github.com/jaywalnut310/vits</a>。本笔记主要针对项目中模型构建部分代码进行注释解析，主要涉及仓库项目中的model.py、modules.py、attentions.py文件。</p> 
<p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#modelpy_4" rel="nofollow">model.py</a></li><li><a href="#modulespy_611" rel="nofollow">modules.py</a></li><li><a href="#attentionspy_1016" rel="nofollow">attentions.py</a></li></ul> 
</div> 
<p></p> 
<h2><a id="modelpy_4"></a>model.py</h2> 
<p>本文件基于各个小模块，对整个VITS的各个一阶模块进行定义，最后集成为多周期判别器/MultiPeriodDiscriminator和生成器/SynthesizerTrn，从训练方式来讲是使用了对抗训练，所以要同时训练判别器和生成器，而生成器，主体是基于VAE的架构，其中又涉及到Flow；特别是本文提出的随机时长预测器，也是一个基于VAE架构，又涉及Flow的一个模块。在VAE和FLOW中涉及大量公式推导。<mark>对于随机时长预测器的代码，最好对应论文补充材料中的B.3部分中的结构图对比学习，更加直观；随机时长预测器应用的变分数据增广和变分反量化，可参考论文VFlow和Flow++</mark>。具体代码如下：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> copy
<span class="token keyword">import</span> math
<span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> functional <span class="token keyword">as</span> F

<span class="token keyword">import</span> commons
<span class="token keyword">import</span> modules
<span class="token keyword">import</span> attentions
<span class="token keyword">import</span> monotonic_align

<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> Conv1d<span class="token punctuation">,</span> ConvTranspose1d<span class="token punctuation">,</span> AvgPool1d<span class="token punctuation">,</span> Conv2d
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>utils <span class="token keyword">import</span> weight_norm<span class="token punctuation">,</span> remove_weight_norm<span class="token punctuation">,</span> spectral_norm
<span class="token keyword">from</span> commons <span class="token keyword">import</span> init_weights<span class="token punctuation">,</span> get_padding


<span class="token comment"># 基于flow的vae架构，优化的是最大似然估计，是学习一个分布；推理时，是从分布中采样音素的持续时间，从而具有随机性，只使用基于flow的解码器预测时长</span>
<span class="token keyword">class</span> <span class="token class-name">StochasticDurationPredictor</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> filter_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> p_dropout<span class="token punctuation">,</span> n_flows<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> gin_channels<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">'''
        @param in_channels:输入的维度，时长预测器的输出是文本编码器后的隐向量h_{text}
        @param filter_channels:
        @param kernel_size:
        @param p_dropout:
        @param n_flows:
        @param gin_channels:
        '''</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        filter_channels <span class="token operator">=</span> in_channels  <span class="token comment"># it needs to be removed from future version.</span>
        self<span class="token punctuation">.</span>in_channels <span class="token operator">=</span> in_channels
        self<span class="token punctuation">.</span>filter_channels <span class="token operator">=</span> filter_channels
        self<span class="token punctuation">.</span>kernel_size <span class="token operator">=</span> kernel_size
        self<span class="token punctuation">.</span>p_dropout <span class="token operator">=</span> p_dropout
        self<span class="token punctuation">.</span>n_flows <span class="token operator">=</span> n_flows  <span class="token comment"># flow块的个数</span>
        self<span class="token punctuation">.</span>gin_channels <span class="token operator">=</span> gin_channels

        self<span class="token punctuation">.</span>log_flow <span class="token operator">=</span> modules<span class="token punctuation">.</span>Log<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>flows <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 论文图5(a)中的Flow g_{theta}</span>
        self<span class="token punctuation">.</span>flows<span class="token punctuation">.</span>append<span class="token punctuation">(</span>modules<span class="token punctuation">.</span>ElementwiseAffine<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 先添加一个仿射变换层</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n_flows<span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>flows<span class="token punctuation">.</span>append<span class="token punctuation">(</span>modules<span class="token punctuation">.</span>ConvFlow<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> filter_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> n_layers<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>flows<span class="token punctuation">.</span>append<span class="token punctuation">(</span>modules<span class="token punctuation">.</span>Flip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 将输出维度进行反转，使得本次变换的维度数据在下次flow中参与变换</span>

        <span class="token comment"># 条件编码器由两个1x1卷积和一个DDSConv残差块组成</span>
        self<span class="token punctuation">.</span>post_pre <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> filter_channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>post_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>filter_channels<span class="token punctuation">,</span> filter_channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>post_convs <span class="token operator">=</span> modules<span class="token punctuation">.</span>DDSConv<span class="token punctuation">(</span>filter_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> n_layers<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> p_dropout<span class="token operator">=</span>p_dropout<span class="token punctuation">)</span>

        <span class="token comment"># 后验编码器模块有四个神经样条流，论文图5(a)中的Posterior Encoder</span>
        self<span class="token punctuation">.</span>post_flows <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>post_flows<span class="token punctuation">.</span>append<span class="token punctuation">(</span>modules<span class="token punctuation">.</span>ElementwiseAffine<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 添加四个神经样条流模块</span>
            self<span class="token punctuation">.</span>post_flows<span class="token punctuation">.</span>append<span class="token punctuation">(</span>modules<span class="token punctuation">.</span>ConvFlow<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> filter_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> n_layers<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>post_flows<span class="token punctuation">.</span>append<span class="token punctuation">(</span>modules<span class="token punctuation">.</span>Flip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token comment"># 条件编码器由两个1x1卷积和一个DDSConv残差块组成</span>
        self<span class="token punctuation">.</span>pre <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> filter_channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>filter_channels<span class="token punctuation">,</span> filter_channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>convs <span class="token operator">=</span> modules<span class="token punctuation">.</span>DDSConv<span class="token punctuation">(</span>filter_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> n_layers<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> p_dropout<span class="token operator">=</span>p_dropout<span class="token punctuation">)</span>

        <span class="token keyword">if</span> gin_channels <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>cond <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>gin_channels<span class="token punctuation">,</span> filter_channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 条件信息</span>

    <span class="token comment"># 参考论文附录b.3和图5理解</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x_mask<span class="token punctuation">,</span> w<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> g<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> reverse<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> noise_scale<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>detach<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># 在随机时常预测器中x的梯度与文本编码器中分离，使其不会影响文本编码器，此处的x为h_{text}</span>

        <span class="token comment"># 经过一个条件编码器变换；该条件编码器变换不管是正向还是逆向都需要计算，故在下面的if语句外</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pre<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">if</span> g <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            g <span class="token operator">=</span> torch<span class="token punctuation">.</span>detach<span class="token punctuation">(</span>g<span class="token punctuation">)</span>
            x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>cond<span class="token punctuation">(</span>g<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>convs<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x_mask<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">*</span> x_mask  <span class="token comment"># 此处的x即论文中持续时间预测部分公式中的c_{text}</span>

        <span class="token keyword">if</span> <span class="token keyword">not</span> reverse<span class="token punctuation">:</span>  <span class="token comment"># 正向，训练</span>
            flows <span class="token operator">=</span> self<span class="token punctuation">.</span>flows
            <span class="token keyword">assert</span> w <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span>  <span class="token comment"># w记为训练时传入的时长信息，尺寸为[batch_szie, 1, t_t]，t_t是与c_{text}长度一致</span>

            logdet_tot_q <span class="token operator">=</span> <span class="token number">0</span>
            <span class="token comment"># 对时长进行预处理，经过条件编码器变换</span>
            h_w <span class="token operator">=</span> self<span class="token punctuation">.</span>post_pre<span class="token punctuation">(</span>w<span class="token punctuation">)</span>
            h_w <span class="token operator">=</span> self<span class="token punctuation">.</span>post_convs<span class="token punctuation">(</span>h_w<span class="token punctuation">,</span> x_mask<span class="token punctuation">)</span>
            h_w <span class="token operator">=</span> self<span class="token punctuation">.</span>post_proj<span class="token punctuation">(</span>h_w<span class="token punctuation">)</span> <span class="token operator">*</span> x_mask  <span class="token comment"># 此处的h_w即论文中持续时间预测部分公式中的d</span>

            <span class="token comment"># 初始化高斯噪声，后验编码器使用该噪声转换为两个随机变量v和u，故尺寸是[batch_szie, 2, t_t]，中间维度是2，其他维度与d一致</span>
            e_q <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>w<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> w<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token operator">=</span>x<span class="token punctuation">.</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span> <span class="token operator">*</span> x_mask
            z_q <span class="token operator">=</span> e_q
            <span class="token keyword">for</span> flow <span class="token keyword">in</span> self<span class="token punctuation">.</span>post_flows<span class="token punctuation">:</span>
                z_q<span class="token punctuation">,</span> logdet_q <span class="token operator">=</span> flow<span class="token punctuation">(</span>z_q<span class="token punctuation">,</span> x_mask<span class="token punctuation">,</span> g<span class="token operator">=</span><span class="token punctuation">(</span>x <span class="token operator">+</span> h_w<span class="token punctuation">)</span><span class="token punctuation">)</span>
                logdet_tot_q <span class="token operator">+=</span> logdet_q

            z_u<span class="token punctuation">,</span> z1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>z_q<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            u <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>z_u<span class="token punctuation">)</span> <span class="token operator">*</span> x_mask  <span class="token comment"># 此时，已获得论文中持续时间预测部分公式中的u、v(即z1)</span>
            z0 <span class="token operator">=</span> <span class="token punctuation">(</span>w <span class="token operator">-</span> u<span class="token punctuation">)</span> <span class="token operator">*</span> x_mask  <span class="token comment"># 相当于d-u</span>
            <span class="token comment"># 后验编码器的对数似然</span>
            logdet_tot_q <span class="token operator">+=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">(</span>F<span class="token punctuation">.</span>logsigmoid<span class="token punctuation">(</span>z_u<span class="token punctuation">)</span> <span class="token operator">+</span> F<span class="token punctuation">.</span>logsigmoid<span class="token punctuation">(</span><span class="token operator">-</span>z_u<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> x_mask<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
            logq <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.5</span> <span class="token operator">*</span> <span class="token punctuation">(</span>math<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> math<span class="token punctuation">.</span>pi<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>e_q <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> x_mask<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">-</span> logdet_tot_q

            logdet_tot <span class="token operator">=</span> <span class="token number">0</span>
            z0<span class="token punctuation">,</span> logdet <span class="token operator">=</span> self<span class="token punctuation">.</span>log_flow<span class="token punctuation">(</span>z0<span class="token punctuation">,</span> x_mask<span class="token punctuation">)</span>
            logdet_tot <span class="token operator">+=</span> logdet
            z <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>z0<span class="token punctuation">,</span> z1<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token keyword">for</span> flow <span class="token keyword">in</span> flows<span class="token punctuation">:</span>
                z<span class="token punctuation">,</span> logdet <span class="token operator">=</span> flow<span class="token punctuation">(</span>z<span class="token punctuation">,</span> x_mask<span class="token punctuation">,</span> g<span class="token operator">=</span>x<span class="token punctuation">,</span> reverse<span class="token operator">=</span>reverse<span class="token punctuation">)</span>
                logdet_tot <span class="token operator">=</span> logdet_tot <span class="token operator">+</span> logdet
            <span class="token comment"># nll为负对数似然，nll是negative log likelihood的缩写</span>
            nll <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token number">0.5</span> <span class="token operator">*</span> <span class="token punctuation">(</span>math<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> math<span class="token punctuation">.</span>pi<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>z <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> x_mask<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">-</span> logdet_tot
            <span class="token keyword">return</span> nll <span class="token operator">+</span> logq  <span class="token comment"># [b]，直接相当于损失</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>  <span class="token comment"># 逆向，预测，只需要走先验分布的flow部分</span>
            flows <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">reversed</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>flows<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 将g_{theta}中的所有flow倒排</span>
            flows <span class="token operator">=</span> flows<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span>flows<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>  <span class="token comment"># remove a useless vflow，把倒数第二个vflow删除了</span>
            z <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token operator">=</span>x<span class="token punctuation">.</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span> <span class="token operator">*</span> noise_scale  <span class="token comment"># 初始化高斯噪声</span>
            <span class="token keyword">for</span> flow <span class="token keyword">in</span> flows<span class="token punctuation">:</span>
                z <span class="token operator">=</span> flow<span class="token punctuation">(</span>z<span class="token punctuation">,</span> x_mask<span class="token punctuation">,</span> g<span class="token operator">=</span>x<span class="token punctuation">,</span> reverse<span class="token operator">=</span>reverse<span class="token punctuation">)</span>
            z0<span class="token punctuation">,</span> z1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>z<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            logw <span class="token operator">=</span> z0
            <span class="token keyword">return</span> logw  <span class="token comment"># 时长的对数，预测音频时为文本序列预测对应的时长</span>


<span class="token comment"># 固定的时长预测器，主要由一维卷积组成</span>
<span class="token keyword">class</span> <span class="token class-name">DurationPredictor</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> filter_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> p_dropout<span class="token punctuation">,</span> gin_channels<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>in_channels <span class="token operator">=</span> in_channels
        self<span class="token punctuation">.</span>filter_channels <span class="token operator">=</span> filter_channels
        self<span class="token punctuation">.</span>kernel_size <span class="token operator">=</span> kernel_size
        self<span class="token punctuation">.</span>p_dropout <span class="token operator">=</span> p_dropout
        self<span class="token punctuation">.</span>gin_channels <span class="token operator">=</span> gin_channels

        self<span class="token punctuation">.</span>drop <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p_dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> filter_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> padding<span class="token operator">=</span>kernel_size <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm_1 <span class="token operator">=</span> modules<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>filter_channels<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>filter_channels<span class="token punctuation">,</span> filter_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> padding<span class="token operator">=</span>kernel_size <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm_2 <span class="token operator">=</span> modules<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>filter_channels<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>filter_channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> gin_channels <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>cond <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>gin_channels<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 条件信息</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x_mask<span class="token punctuation">,</span> g<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>detach<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">if</span> g <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            g <span class="token operator">=</span> torch<span class="token punctuation">.</span>detach<span class="token punctuation">(</span>g<span class="token punctuation">)</span>
            x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>cond<span class="token punctuation">(</span>g<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv_1<span class="token punctuation">(</span>x <span class="token operator">*</span> x_mask<span class="token punctuation">)</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>drop<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv_2<span class="token punctuation">(</span>x <span class="token operator">*</span> x_mask<span class="token punctuation">)</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm_2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>drop<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>proj<span class="token punctuation">(</span>x <span class="token operator">*</span> x_mask<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x <span class="token operator">*</span> x_mask


<span class="token comment"># 文本编码器</span>
<span class="token keyword">class</span> <span class="token class-name">TextEncoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>
                 n_vocab<span class="token punctuation">,</span>
                 out_channels<span class="token punctuation">,</span>
                 hidden_channels<span class="token punctuation">,</span>
                 filter_channels<span class="token punctuation">,</span>
                 n_heads<span class="token punctuation">,</span>
                 n_layers<span class="token punctuation">,</span>
                 kernel_size<span class="token punctuation">,</span>
                 p_dropout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>n_vocab <span class="token operator">=</span> n_vocab
        self<span class="token punctuation">.</span>out_channels <span class="token operator">=</span> out_channels
        self<span class="token punctuation">.</span>hidden_channels <span class="token operator">=</span> hidden_channels
        self<span class="token punctuation">.</span>filter_channels <span class="token operator">=</span> filter_channels
        self<span class="token punctuation">.</span>n_heads <span class="token operator">=</span> n_heads
        self<span class="token punctuation">.</span>n_layers <span class="token operator">=</span> n_layers
        self<span class="token punctuation">.</span>kernel_size <span class="token operator">=</span> kernel_size
        self<span class="token punctuation">.</span>p_dropout <span class="token operator">=</span> p_dropout

        self<span class="token punctuation">.</span>emb <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>n_vocab<span class="token punctuation">,</span> hidden_channels<span class="token punctuation">)</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>emb<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">,</span> hidden_channels <span class="token operator">**</span> <span class="token operator">-</span><span class="token number">0.5</span><span class="token punctuation">)</span>
        <span class="token comment"># 基于Transformer的编码部分</span>
        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> attentions<span class="token punctuation">.</span>Encoder<span class="token punctuation">(</span>
            hidden_channels<span class="token punctuation">,</span>
            filter_channels<span class="token punctuation">,</span>
            n_heads<span class="token punctuation">,</span>
            n_layers<span class="token punctuation">,</span>
            kernel_size<span class="token punctuation">,</span>
            p_dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>hidden_channels<span class="token punctuation">,</span> out_channels <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># pointwise的一维卷积，相当于一个一个1×1卷积，空间尺寸没变，通道数改变</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x_lengths<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>emb<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">*</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_channels<span class="token punctuation">)</span>  <span class="token comment"># [b, t, h]，将数值化的token转为word embedding，会进行缩放</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [b, h, t]</span>
        x_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>commons<span class="token punctuation">.</span>sequence_mask<span class="token punctuation">(</span>x_lengths<span class="token punctuation">,</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>  <span class="token comment"># [b, 1, t]</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>x <span class="token operator">*</span> x_mask<span class="token punctuation">,</span> x_mask<span class="token punctuation">)</span>  <span class="token comment"># 文本编码后的隐向量</span>
        stats <span class="token operator">=</span> self<span class="token punctuation">.</span>proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">*</span> x_mask

        m<span class="token punctuation">,</span> logs <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>stats<span class="token punctuation">,</span> self<span class="token punctuation">.</span>out_channels<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># m是论文图1中projection后的均值，logs是标准差的对数</span>
        <span class="token keyword">return</span> x<span class="token punctuation">,</span> m<span class="token punctuation">,</span> logs<span class="token punctuation">,</span> x_mask  <span class="token comment"># 此处x相当于h_{text}</span>


<span class="token comment"># 耦合结构的flow，即论文图1中的Flow，f_{theta}。会将x拆成x1、x2，x1经过网络预测一个s和t，y1=s*x1+t，y2=x2，最后的输出就是将y1和y2拼接起来</span>
<span class="token keyword">class</span> <span class="token class-name">ResidualCouplingBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>
                 channels<span class="token punctuation">,</span>
                 hidden_channels<span class="token punctuation">,</span>
                 kernel_size<span class="token punctuation">,</span>
                 dilation_rate<span class="token punctuation">,</span>
                 n_layers<span class="token punctuation">,</span>
                 n_flows<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>
                 gin_channels<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>channels <span class="token operator">=</span> channels
        self<span class="token punctuation">.</span>hidden_channels <span class="token operator">=</span> hidden_channels
        self<span class="token punctuation">.</span>kernel_size <span class="token operator">=</span> kernel_size
        self<span class="token punctuation">.</span>dilation_rate <span class="token operator">=</span> dilation_rate
        self<span class="token punctuation">.</span>n_layers <span class="token operator">=</span> n_layers
        self<span class="token punctuation">.</span>n_flows <span class="token operator">=</span> n_flows
        self<span class="token punctuation">.</span>gin_channels <span class="token operator">=</span> gin_channels

        self<span class="token punctuation">.</span>flows <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n_flows<span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>flows<span class="token punctuation">.</span>append<span class="token punctuation">(</span>
                modules<span class="token punctuation">.</span>ResidualCouplingLayer<span class="token punctuation">(</span>channels<span class="token punctuation">,</span> hidden_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> dilation_rate<span class="token punctuation">,</span> n_layers<span class="token punctuation">,</span>
                                              gin_channels<span class="token operator">=</span>gin_channels<span class="token punctuation">,</span> mean_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 仿射耦合层</span>
            self<span class="token punctuation">.</span>flows<span class="token punctuation">.</span>append<span class="token punctuation">(</span>modules<span class="token punctuation">.</span>Flip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 将上一层仿射耦合层的数据左右调换，两部分的数据翻转，防止一致都是同一部分的数据进行变换</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x_mask<span class="token punctuation">,</span> g<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> reverse<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> reverse<span class="token punctuation">:</span>  <span class="token comment"># 正向，训练</span>
            <span class="token keyword">for</span> flow <span class="token keyword">in</span> self<span class="token punctuation">.</span>flows<span class="token punctuation">:</span>
                x<span class="token punctuation">,</span> _ <span class="token operator">=</span> flow<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x_mask<span class="token punctuation">,</span> g<span class="token operator">=</span>g<span class="token punctuation">,</span> reverse<span class="token operator">=</span>reverse<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>  <span class="token comment"># 逆向，预测</span>
            <span class="token keyword">for</span> flow <span class="token keyword">in</span> <span class="token builtin">reversed</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>flows<span class="token punctuation">)</span><span class="token punctuation">:</span>
                x <span class="token operator">=</span> flow<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x_mask<span class="token punctuation">,</span> g<span class="token operator">=</span>g<span class="token punctuation">,</span> reverse<span class="token operator">=</span>reverse<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x


<span class="token comment"># 后验编码器</span>
<span class="token keyword">class</span> <span class="token class-name">PosteriorEncoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>
                 in_channels<span class="token punctuation">,</span>
                 out_channels<span class="token punctuation">,</span>
                 hidden_channels<span class="token punctuation">,</span>
                 kernel_size<span class="token punctuation">,</span>
                 dilation_rate<span class="token punctuation">,</span>
                 n_layers<span class="token punctuation">,</span>
                 gin_channels<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>in_channels <span class="token operator">=</span> in_channels
        self<span class="token punctuation">.</span>out_channels <span class="token operator">=</span> out_channels
        self<span class="token punctuation">.</span>hidden_channels <span class="token operator">=</span> hidden_channels
        self<span class="token punctuation">.</span>kernel_size <span class="token operator">=</span> kernel_size
        self<span class="token punctuation">.</span>dilation_rate <span class="token operator">=</span> dilation_rate
        self<span class="token punctuation">.</span>n_layers <span class="token operator">=</span> n_layers
        self<span class="token punctuation">.</span>gin_channels <span class="token operator">=</span> gin_channels

        <span class="token comment"># 主要由一维卷积组成</span>
        self<span class="token punctuation">.</span>pre <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> hidden_channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 编码器是WaveNet残差块</span>
        self<span class="token punctuation">.</span>enc <span class="token operator">=</span> modules<span class="token punctuation">.</span>WN<span class="token punctuation">(</span>hidden_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> dilation_rate<span class="token punctuation">,</span> n_layers<span class="token punctuation">,</span> gin_channels<span class="token operator">=</span>gin_channels<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>hidden_channels<span class="token punctuation">,</span> out_channels <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x_lengths<span class="token punctuation">,</span> g<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        从音频的线性普x_{lin}中学习后验分布
        @param x:音频的线性谱
        @param x_lengths:线性谱的长度
        @param g:global condition，可接受说话人的身份，即speaker的id对应的embedding
        @return:
        """</span>
        x_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>commons<span class="token punctuation">.</span>sequence_mask<span class="token punctuation">(</span>x_lengths<span class="token punctuation">,</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pre<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">*</span> x_mask  <span class="token comment"># 预处理</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>enc<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x_mask<span class="token punctuation">,</span> g<span class="token operator">=</span>g<span class="token punctuation">)</span>  <span class="token comment"># 编码</span>
        stats <span class="token operator">=</span> self<span class="token punctuation">.</span>proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">*</span> x_mask
        m<span class="token punctuation">,</span> logs <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>stats<span class="token punctuation">,</span> self<span class="token punctuation">.</span>out_channels<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 预测后验分布，m是均值，logs是标准差的对数</span>
        z <span class="token operator">=</span> <span class="token punctuation">(</span>m <span class="token operator">+</span> torch<span class="token punctuation">.</span>randn_like<span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">*</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>logs<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> x_mask  <span class="token comment"># 从预测的分布中进行重参数采样得到z</span>
        <span class="token keyword">return</span> z<span class="token punctuation">,</span> m<span class="token punctuation">,</span> logs<span class="token punctuation">,</span> x_mask


<span class="token comment"># 基于后验编码器采样生成的z生成音频波形；主要目的是进行上采样，由反卷积和残差模块组成；</span>
<span class="token comment"># 其实就是HiFiGAN V1的生成器，由多组转置卷积和多感受野融合模块组成</span>
<span class="token keyword">class</span> <span class="token class-name">Generator</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> initial_channel<span class="token punctuation">,</span> resblock<span class="token punctuation">,</span> resblock_kernel_sizes<span class="token punctuation">,</span> resblock_dilation_sizes<span class="token punctuation">,</span> upsample_rates<span class="token punctuation">,</span>
                 upsample_initial_channel<span class="token punctuation">,</span> upsample_kernel_sizes<span class="token punctuation">,</span> gin_channels<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Generator<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>num_kernels <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>resblock_kernel_sizes<span class="token punctuation">)</span>  <span class="token comment"># 卷积核数量</span>
        self<span class="token punctuation">.</span>num_upsamples <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>upsample_rates<span class="token punctuation">)</span>  <span class="token comment"># 上采样比例</span>
        self<span class="token punctuation">.</span>conv_pre <span class="token operator">=</span> Conv1d<span class="token punctuation">(</span>initial_channel<span class="token punctuation">,</span> upsample_initial_channel<span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>
        resblock <span class="token operator">=</span> modules<span class="token punctuation">.</span>ResBlock1 <span class="token keyword">if</span> resblock <span class="token operator">==</span> <span class="token string">'1'</span> <span class="token keyword">else</span> modules<span class="token punctuation">.</span>ResBlock2  <span class="token comment"># 选择何种“多感受野融合/MRF模块”</span>

        self<span class="token punctuation">.</span>ups <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>u<span class="token punctuation">,</span> k<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span>upsample_rates<span class="token punctuation">,</span> upsample_kernel_sizes<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>ups<span class="token punctuation">.</span>append<span class="token punctuation">(</span>weight_norm<span class="token punctuation">(</span>
                ConvTranspose1d<span class="token punctuation">(</span>upsample_initial_channel <span class="token operator">//</span> <span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">**</span> i<span class="token punctuation">)</span><span class="token punctuation">,</span> upsample_initial_channel <span class="token operator">//</span> <span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">**</span> <span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                k<span class="token punctuation">,</span> u<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token punctuation">(</span>k <span class="token operator">-</span> u<span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 添加转置卷积</span>

        self<span class="token punctuation">.</span>resblocks <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>ups<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            ch <span class="token operator">=</span> upsample_initial_channel <span class="token operator">//</span> <span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">**</span> <span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token keyword">for</span> j<span class="token punctuation">,</span> <span class="token punctuation">(</span>k<span class="token punctuation">,</span> d<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span>resblock_kernel_sizes<span class="token punctuation">,</span> resblock_dilation_sizes<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>resblocks<span class="token punctuation">.</span>append<span class="token punctuation">(</span>resblock<span class="token punctuation">(</span>ch<span class="token punctuation">,</span> k<span class="token punctuation">,</span> d<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 添加MRF模块</span>

        self<span class="token punctuation">.</span>conv_post <span class="token operator">=</span> Conv1d<span class="token punctuation">(</span>ch<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ups<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>init_weights<span class="token punctuation">)</span>  <span class="token comment"># 参数初始化</span>

        <span class="token keyword">if</span> gin_channels <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>cond <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>gin_channels<span class="token punctuation">,</span> upsample_initial_channel<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> g<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv_pre<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># 先一维卷积预处理</span>
        <span class="token keyword">if</span> g <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>cond<span class="token punctuation">(</span>g<span class="token punctuation">)</span>  <span class="token comment"># 叠加条件</span>

        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_upsamples<span class="token punctuation">)</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> F<span class="token punctuation">.</span>leaky_relu<span class="token punctuation">(</span>x<span class="token punctuation">,</span> modules<span class="token punctuation">.</span>LRELU_SLOPE<span class="token punctuation">)</span>  <span class="token comment"># 激活</span>
            x <span class="token operator">=</span> self<span class="token punctuation">.</span>ups<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># 上采样</span>
            xs <span class="token operator">=</span> <span class="token boolean">None</span>
            <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_kernels<span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token keyword">if</span> xs <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                    xs <span class="token operator">=</span> self<span class="token punctuation">.</span>resblocks<span class="token punctuation">[</span>i <span class="token operator">*</span> self<span class="token punctuation">.</span>num_kernels <span class="token operator">+</span> j<span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>
                <span class="token keyword">else</span><span class="token punctuation">:</span>
                    xs <span class="token operator">+=</span> self<span class="token punctuation">.</span>resblocks<span class="token punctuation">[</span>i <span class="token operator">*</span> self<span class="token punctuation">.</span>num_kernels <span class="token operator">+</span> j<span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            x <span class="token operator">=</span> xs <span class="token operator">/</span> self<span class="token punctuation">.</span>num_kernels
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>leaky_relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># 激活</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv_post<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># 一维卷积后处理</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># 再激活</span>

        <span class="token keyword">return</span> x

    <span class="token keyword">def</span> <span class="token function">remove_weight_norm</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Removing weight norm...'</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> l <span class="token keyword">in</span> self<span class="token punctuation">.</span>ups<span class="token punctuation">:</span>
            remove_weight_norm<span class="token punctuation">(</span>l<span class="token punctuation">)</span>
        <span class="token keyword">for</span> l <span class="token keyword">in</span> self<span class="token punctuation">.</span>resblocks<span class="token punctuation">:</span>
            l<span class="token punctuation">.</span>remove_weight_norm<span class="token punctuation">(</span><span class="token punctuation">)</span>


<span class="token comment"># 周期判别器</span>
<span class="token keyword">class</span> <span class="token class-name">DiscriminatorP</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> period<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> use_spectral_norm<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>DiscriminatorP<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>period <span class="token operator">=</span> period
        self<span class="token punctuation">.</span>use_spectral_norm <span class="token operator">=</span> use_spectral_norm  <span class="token comment"># 是否进行谱归一化</span>
        <span class="token comment"># 基于self.use_spectral_norm设置是进行权重归一化还是谱归一化</span>
        norm_f <span class="token operator">=</span> weight_norm <span class="token keyword">if</span> use_spectral_norm <span class="token operator">==</span> <span class="token boolean">False</span> <span class="token keyword">else</span> spectral_norm
        <span class="token comment"># 存放多个二维卷积，输出尺寸逐渐扩大</span>
        self<span class="token punctuation">.</span>convs <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>
            norm_f<span class="token punctuation">(</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>kernel_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>stride<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token punctuation">(</span>get_padding<span class="token punctuation">(</span>kernel_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            norm_f<span class="token punctuation">(</span>Conv2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>kernel_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>stride<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token punctuation">(</span>get_padding<span class="token punctuation">(</span>kernel_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            norm_f<span class="token punctuation">(</span>Conv2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>kernel_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>stride<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token punctuation">(</span>get_padding<span class="token punctuation">(</span>kernel_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            norm_f<span class="token punctuation">(</span>Conv2d<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">1024</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>kernel_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>stride<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token punctuation">(</span>get_padding<span class="token punctuation">(</span>kernel_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            norm_f<span class="token punctuation">(</span>Conv2d<span class="token punctuation">(</span><span class="token number">1024</span><span class="token punctuation">,</span> <span class="token number">1024</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>kernel_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token punctuation">(</span>get_padding<span class="token punctuation">(</span>kernel_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv_post <span class="token operator">=</span> norm_f<span class="token punctuation">(</span>Conv2d<span class="token punctuation">(</span><span class="token number">1024</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 最后将尺寸再降维1</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        fmap <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

        <span class="token comment"># 1d to 2d</span>
        b<span class="token punctuation">,</span> c<span class="token punctuation">,</span> t <span class="token operator">=</span> x<span class="token punctuation">.</span>shape
        <span class="token keyword">if</span> t <span class="token operator">%</span> self<span class="token punctuation">.</span>period <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>  <span class="token comment"># pad first  # 如果长度t不是周期period的整数倍</span>
            n_pad <span class="token operator">=</span> self<span class="token punctuation">.</span>period <span class="token operator">-</span> <span class="token punctuation">(</span>t <span class="token operator">%</span> self<span class="token punctuation">.</span>period<span class="token punctuation">)</span>  <span class="token comment"># 计算距离period整数倍的差值</span>
            x <span class="token operator">=</span> F<span class="token punctuation">.</span>pad<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> n_pad<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"reflect"</span><span class="token punctuation">)</span>  <span class="token comment"># 用0进行pad</span>
            t <span class="token operator">=</span> t <span class="token operator">+</span> n_pad  <span class="token comment"># 长度也相应增加</span>
        <span class="token comment"># 将最后的一维长度以period维周期折叠为二维，如[1, 2, 3, 4, 5, 6]，period为2，则转换为[[1, 2, 3], [4, 5, 6]]</span>
        <span class="token comment"># 然后对得到的二维数据进行二维卷积计算</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>b<span class="token punctuation">,</span> c<span class="token punctuation">,</span> t <span class="token operator">//</span> self<span class="token punctuation">.</span>period<span class="token punctuation">,</span> self<span class="token punctuation">.</span>period<span class="token punctuation">)</span>  <span class="token comment"># 尺寸转为四维，[b, c, t//period, period]</span>

        <span class="token keyword">for</span> l <span class="token keyword">in</span> self<span class="token punctuation">.</span>convs<span class="token punctuation">:</span>
            x <span class="token operator">=</span> l<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># 二维卷积计算</span>
            x <span class="token operator">=</span> F<span class="token punctuation">.</span>leaky_relu<span class="token punctuation">(</span>x<span class="token punctuation">,</span> modules<span class="token punctuation">.</span>LRELU_SLOPE<span class="token punctuation">)</span>  <span class="token comment"># 使用泄露的relu作为激活函数，尺寸为[b, c, ]???</span>
            fmap<span class="token punctuation">.</span>append<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># 记录尺寸升维过程中所有的feature map</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv_post<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># 将x从最高维度1024降至1维</span>
        fmap<span class="token punctuation">.</span>append<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 将x从第1维到最后一维推平，尺寸为[b, c*t//period*period]-&gt;[b, c*t]</span>

        <span class="token keyword">return</span> x<span class="token punctuation">,</span> fmap


<span class="token comment"># 尺度判别器</span>
<span class="token keyword">class</span> <span class="token class-name">DiscriminatorS</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> use_spectral_norm<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>DiscriminatorS<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 基于self.use_spectral_norm设置是进行权重归一化还是谱归一化</span>
        norm_f <span class="token operator">=</span> weight_norm <span class="token keyword">if</span> use_spectral_norm <span class="token operator">==</span> <span class="token boolean">False</span> <span class="token keyword">else</span> spectral_norm
        <span class="token comment"># 存放多个一维卷积，输出尺寸逐渐扩大</span>
        self<span class="token punctuation">.</span>convs <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>
            norm_f<span class="token punctuation">(</span>Conv1d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            norm_f<span class="token punctuation">(</span>Conv1d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">41</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> groups<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            norm_f<span class="token punctuation">(</span>Conv1d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">41</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> groups<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            norm_f<span class="token punctuation">(</span>Conv1d<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">1024</span><span class="token punctuation">,</span> <span class="token number">41</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> groups<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            norm_f<span class="token punctuation">(</span>Conv1d<span class="token punctuation">(</span><span class="token number">1024</span><span class="token punctuation">,</span> <span class="token number">1024</span><span class="token punctuation">,</span> <span class="token number">41</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> groups<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            norm_f<span class="token punctuation">(</span>Conv1d<span class="token punctuation">(</span><span class="token number">1024</span><span class="token punctuation">,</span> <span class="token number">1024</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv_post <span class="token operator">=</span> norm_f<span class="token punctuation">(</span>Conv1d<span class="token punctuation">(</span><span class="token number">1024</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        fmap <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment"># 记录每个卷积层输出的feature map</span>

        <span class="token keyword">for</span> l <span class="token keyword">in</span> self<span class="token punctuation">.</span>convs<span class="token punctuation">:</span>
            x <span class="token operator">=</span> l<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            x <span class="token operator">=</span> F<span class="token punctuation">.</span>leaky_relu<span class="token punctuation">(</span>x<span class="token punctuation">,</span> modules<span class="token punctuation">.</span>LRELU_SLOPE<span class="token punctuation">)</span>
            fmap<span class="token punctuation">.</span>append<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv_post<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        fmap<span class="token punctuation">.</span>append<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> x<span class="token punctuation">,</span> fmap


<span class="token comment"># 多周期判别器</span>
<span class="token keyword">class</span> <span class="token class-name">MultiPeriodDiscriminator</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> use_spectral_norm<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>MultiPeriodDiscriminator<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        periods <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">]</span>

        <span class="token comment"># 只使用了一个尺度判别器，没有对波形序列进行平均池化</span>
        discs <span class="token operator">=</span> <span class="token punctuation">[</span>DiscriminatorS<span class="token punctuation">(</span>use_spectral_norm<span class="token operator">=</span>use_spectral_norm<span class="token punctuation">)</span><span class="token punctuation">]</span>
        <span class="token comment"># 添加多个周期判别器</span>
        discs <span class="token operator">=</span> discs <span class="token operator">+</span> <span class="token punctuation">[</span>DiscriminatorP<span class="token punctuation">(</span>i<span class="token punctuation">,</span> use_spectral_norm<span class="token operator">=</span>use_spectral_norm<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> periods<span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>discriminators <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span>discs<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> y<span class="token punctuation">,</span> y_hat<span class="token punctuation">)</span><span class="token punctuation">:</span>
        y_d_rs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        y_d_gs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        fmap_rs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        fmap_gs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> i<span class="token punctuation">,</span> d <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>discriminators<span class="token punctuation">)</span><span class="token punctuation">:</span>
            y_d_r<span class="token punctuation">,</span> fmap_r <span class="token operator">=</span> d<span class="token punctuation">(</span>y<span class="token punctuation">)</span>  <span class="token comment"># 子判别器对一个batch中真实波形数据的判别和feature map</span>
            y_d_g<span class="token punctuation">,</span> fmap_g <span class="token operator">=</span> d<span class="token punctuation">(</span>y_hat<span class="token punctuation">)</span>  <span class="token comment"># 子判别器对一个batch中生成波形数据的判别和feature map</span>
            y_d_rs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>y_d_r<span class="token punctuation">)</span>
            y_d_gs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>y_d_g<span class="token punctuation">)</span>
            fmap_rs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>fmap_r<span class="token punctuation">)</span>
            fmap_gs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>fmap_g<span class="token punctuation">)</span>
        <span class="token comment"># y_d_rs,和y_d_gs记录所有子判别器对batch中真实波形和生成波形的判别结果</span>
        <span class="token comment"># fmap_rs, fmap_gs记录所有子判别器多的所有卷积层对真实波形和生成波形的中间特征，即feature map</span>
        <span class="token keyword">return</span> y_d_rs<span class="token punctuation">,</span> y_d_gs<span class="token punctuation">,</span> fmap_rs<span class="token punctuation">,</span> fmap_gs


<span class="token comment"># 从文本到波形</span>
<span class="token keyword">class</span> <span class="token class-name">SynthesizerTrn</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Synthesizer for Training
    """</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>
                 n_vocab<span class="token punctuation">,</span>
                 spec_channels<span class="token punctuation">,</span>
                 segment_size<span class="token punctuation">,</span>
                 inter_channels<span class="token punctuation">,</span>
                 hidden_channels<span class="token punctuation">,</span>
                 filter_channels<span class="token punctuation">,</span>
                 n_heads<span class="token punctuation">,</span>
                 n_layers<span class="token punctuation">,</span>
                 kernel_size<span class="token punctuation">,</span>
                 p_dropout<span class="token punctuation">,</span>
                 resblock<span class="token punctuation">,</span>
                 resblock_kernel_sizes<span class="token punctuation">,</span>
                 resblock_dilation_sizes<span class="token punctuation">,</span>
                 upsample_rates<span class="token punctuation">,</span>
                 upsample_initial_channel<span class="token punctuation">,</span>
                 upsample_kernel_sizes<span class="token punctuation">,</span>
                 n_speakers<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
                 gin_channels<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
                 use_sdp<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                 <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>

        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>n_vocab <span class="token operator">=</span> n_vocab
        self<span class="token punctuation">.</span>spec_channels <span class="token operator">=</span> spec_channels
        self<span class="token punctuation">.</span>inter_channels <span class="token operator">=</span> inter_channels
        self<span class="token punctuation">.</span>hidden_channels <span class="token operator">=</span> hidden_channels
        self<span class="token punctuation">.</span>filter_channels <span class="token operator">=</span> filter_channels
        self<span class="token punctuation">.</span>n_heads <span class="token operator">=</span> n_heads
        self<span class="token punctuation">.</span>n_layers <span class="token operator">=</span> n_layers
        self<span class="token punctuation">.</span>kernel_size <span class="token operator">=</span> kernel_size
        self<span class="token punctuation">.</span>p_dropout <span class="token operator">=</span> p_dropout
        self<span class="token punctuation">.</span>resblock <span class="token operator">=</span> resblock
        self<span class="token punctuation">.</span>resblock_kernel_sizes <span class="token operator">=</span> resblock_kernel_sizes
        self<span class="token punctuation">.</span>resblock_dilation_sizes <span class="token operator">=</span> resblock_dilation_sizes
        self<span class="token punctuation">.</span>upsample_rates <span class="token operator">=</span> upsample_rates
        self<span class="token punctuation">.</span>upsample_initial_channel <span class="token operator">=</span> upsample_initial_channel
        self<span class="token punctuation">.</span>upsample_kernel_sizes <span class="token operator">=</span> upsample_kernel_sizes
        self<span class="token punctuation">.</span>segment_size <span class="token operator">=</span> segment_size
        self<span class="token punctuation">.</span>n_speakers <span class="token operator">=</span> n_speakers
        self<span class="token punctuation">.</span>gin_channels <span class="token operator">=</span> gin_channels

        self<span class="token punctuation">.</span>use_sdp <span class="token operator">=</span> use_sdp
        <span class="token comment"># 文本编码器，同时会获得文本的先验分布</span>
        self<span class="token punctuation">.</span>enc_p <span class="token operator">=</span> TextEncoder<span class="token punctuation">(</span>n_vocab<span class="token punctuation">,</span>
                                 inter_channels<span class="token punctuation">,</span>
                                 hidden_channels<span class="token punctuation">,</span>
                                 filter_channels<span class="token punctuation">,</span>
                                 n_heads<span class="token punctuation">,</span>
                                 n_layers<span class="token punctuation">,</span>
                                 kernel_size<span class="token punctuation">,</span>
                                 p_dropout<span class="token punctuation">)</span>
        <span class="token comment"># 解码器或音频波形生成器</span>
        self<span class="token punctuation">.</span>dec <span class="token operator">=</span> Generator<span class="token punctuation">(</span>inter_channels<span class="token punctuation">,</span> resblock<span class="token punctuation">,</span> resblock_kernel_sizes<span class="token punctuation">,</span> resblock_dilation_sizes<span class="token punctuation">,</span> upsample_rates<span class="token punctuation">,</span>
                             upsample_initial_channel<span class="token punctuation">,</span> upsample_kernel_sizes<span class="token punctuation">,</span> gin_channels<span class="token operator">=</span>gin_channels<span class="token punctuation">)</span>
        <span class="token comment"># 后验编码器，从音频线性谱中学习隐向量z</span>
        self<span class="token punctuation">.</span>enc_q <span class="token operator">=</span> PosteriorEncoder<span class="token punctuation">(</span>spec_channels<span class="token punctuation">,</span> inter_channels<span class="token punctuation">,</span> hidden_channels<span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span>
                                      gin_channels<span class="token operator">=</span>gin_channels<span class="token punctuation">)</span>
        <span class="token comment"># 论文图1中的Flow f_θ，提高先验分布的表达能力</span>
        self<span class="token punctuation">.</span>flow <span class="token operator">=</span> ResidualCouplingBlock<span class="token punctuation">(</span>inter_channels<span class="token punctuation">,</span> hidden_channels<span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> gin_channels<span class="token operator">=</span>gin_channels<span class="token punctuation">)</span>

        <span class="token keyword">if</span> use_sdp<span class="token punctuation">:</span>  <span class="token comment"># 使用随机时间预测器</span>
            self<span class="token punctuation">.</span>dp <span class="token operator">=</span> StochasticDurationPredictor<span class="token punctuation">(</span>hidden_channels<span class="token punctuation">,</span> <span class="token number">192</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> gin_channels<span class="token operator">=</span>gin_channels<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>dp <span class="token operator">=</span> DurationPredictor<span class="token punctuation">(</span>hidden_channels<span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> gin_channels<span class="token operator">=</span>gin_channels<span class="token punctuation">)</span>

        <span class="token keyword">if</span> n_speakers <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>emb_g <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>n_speakers<span class="token punctuation">,</span> gin_channels<span class="token punctuation">)</span>  <span class="token comment"># 构建speaker的嵌入层，作为condition</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x_lengths<span class="token punctuation">,</span> y<span class="token punctuation">,</span> y_lengths<span class="token punctuation">,</span> sid<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 此处的m_q和s_q表示先验分布是以f_θ(z)为随机变量的高斯分布，还要经过一个逆向的flow才能转变成随机变量为z的高斯分布</span>
        x<span class="token punctuation">,</span> m_p<span class="token punctuation">,</span> logs_p<span class="token punctuation">,</span> x_mask <span class="token operator">=</span> self<span class="token punctuation">.</span>enc_p<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x_lengths<span class="token punctuation">)</span>  <span class="token comment"># 先验分布只以文本作为输入，与条件无关；基于文本预测得到f_θ(z)的均值和标准差</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>n_speakers <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
            g <span class="token operator">=</span> self<span class="token punctuation">.</span>emb_g<span class="token punctuation">(</span>sid<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [b, h, 1]</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            g <span class="token operator">=</span> <span class="token boolean">None</span>
        <span class="token comment"># y是音频的线性谱，经过后验编码器获得z，m_q和s_q即后验分布的均值和标准差</span>
        z<span class="token punctuation">,</span> m_q<span class="token punctuation">,</span> logs_q<span class="token punctuation">,</span> y_mask <span class="token operator">=</span> self<span class="token punctuation">.</span>enc_q<span class="token punctuation">(</span>y<span class="token punctuation">,</span> y_lengths<span class="token punctuation">,</span> g<span class="token operator">=</span>g<span class="token punctuation">)</span>  <span class="token comment"># 后验分布的输入除音频外，还有speaker的嵌入层，是条件相关的</span>
        z_p <span class="token operator">=</span> self<span class="token punctuation">.</span>flow<span class="token punctuation">(</span>z<span class="token punctuation">,</span> y_mask<span class="token punctuation">,</span> g<span class="token operator">=</span>g<span class="token punctuation">)</span>  <span class="token comment"># 从后验分布中得到的z经过正向的flow得到f_θ(z)，计算KL散度需要</span>

        <span class="token comment"># 基于动态规划的MAS算法将文本长度与音频谱图长度对齐</span>
        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># negative cross-entropy</span>
            s_p_sq_r <span class="token operator">=</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span> <span class="token operator">*</span> logs_p<span class="token punctuation">)</span>  <span class="token comment"># [b, d, t]</span>
            neg_cent1 <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.5</span> <span class="token operator">*</span> math<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> math<span class="token punctuation">.</span>pi<span class="token punctuation">)</span> <span class="token operator">-</span> logs_p<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token comment"># [b, 1, t_s]</span>
            neg_cent2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.5</span> <span class="token operator">*</span> <span class="token punctuation">(</span>z_p <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                     s_p_sq_r<span class="token punctuation">)</span>  <span class="token comment"># [b, t_t, d] x [b, d, t_s] = [b, t_t, t_s]</span>
            neg_cent3 <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>z_p<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>m_p <span class="token operator">*</span> s_p_sq_r<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># [b, t_t, d] x [b, d, t_s] = [b, t_t, t_s]</span>
            neg_cent4 <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.5</span> <span class="token operator">*</span> <span class="token punctuation">(</span>m_p <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">*</span> s_p_sq_r<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token comment"># [b, 1, t_s]</span>
            neg_cent <span class="token operator">=</span> neg_cent1 <span class="token operator">+</span> neg_cent2 <span class="token operator">+</span> neg_cent3 <span class="token operator">+</span> neg_cent4

            attn_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>x_mask<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">*</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>y_mask<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token comment"># attn的尺寸[batch_szie, 1, t_s, t_t]</span>
            attn <span class="token operator">=</span> monotonic_align<span class="token punctuation">.</span>maximum_path<span class="token punctuation">(</span>neg_cent<span class="token punctuation">,</span> attn_mask<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 值为0，1的对齐矩阵</span>

        w <span class="token operator">=</span> attn<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># [batch_szie, 1, t_t]，w记为训练时通过MAS算法搜索到的对齐矩阵，或者说音素的时长信息</span>
        <span class="token comment"># l_length表示loss_length</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_sdp<span class="token punctuation">:</span>
            l_length <span class="token operator">=</span> self<span class="token punctuation">.</span>dp<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x_mask<span class="token punctuation">,</span> w<span class="token punctuation">,</span> g<span class="token operator">=</span>g<span class="token punctuation">)</span>
            l_length <span class="token operator">=</span> l_length <span class="token operator">/</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>x_mask<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment"># 确定时长的编码器模块是由一维卷积组成的，就是将编码后的x送到时长预测器中预测每个文本对应的时长，然后与w损失</span>
            logw_ <span class="token operator">=</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>w <span class="token operator">+</span> <span class="token number">1e-6</span><span class="token punctuation">)</span> <span class="token operator">*</span> x_mask
            logw <span class="token operator">=</span> self<span class="token punctuation">.</span>dp<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x_mask<span class="token punctuation">,</span> g<span class="token operator">=</span>g<span class="token punctuation">)</span>
            l_length <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">(</span>logw <span class="token operator">-</span> logw_<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">/</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>x_mask<span class="token punctuation">)</span>  <span class="token comment"># for averaging</span>

        <span class="token comment"># expand prior</span>
        <span class="token comment"># 对齐之前，m_p的尺寸[batch_size, feat_dim, t_t]，logs_p的尺寸[batch_size, feat_dim, t_t]</span>
        m_p <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>attn<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> m_p<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># [batch_szie, feat_dim, t_s]</span>
        logs_p <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>attn<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> logs_p<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># [batch_szie, feat_dim, t_s]</span>

        z_slice<span class="token punctuation">,</span> ids_slice <span class="token operator">=</span> commons<span class="token punctuation">.</span>rand_slice_segments<span class="token punctuation">(</span>z<span class="token punctuation">,</span> y_lengths<span class="token punctuation">,</span> self<span class="token punctuation">.</span>segment_size<span class="token punctuation">)</span>  <span class="token comment"># 从一段音频总截取一部分进行训练</span>
        o <span class="token operator">=</span> self<span class="token punctuation">.</span>dec<span class="token punctuation">(</span>z_slice<span class="token punctuation">,</span> g<span class="token operator">=</span>g<span class="token punctuation">)</span>  <span class="token comment"># 相当于音频的解码器，基于后验编码器采样的z生成音频数据；需要传入speaker的嵌入层</span>
        <span class="token keyword">return</span> o<span class="token punctuation">,</span> l_length<span class="token punctuation">,</span> attn<span class="token punctuation">,</span> ids_slice<span class="token punctuation">,</span> x_mask<span class="token punctuation">,</span> y_mask<span class="token punctuation">,</span> <span class="token punctuation">(</span>z<span class="token punctuation">,</span> z_p<span class="token punctuation">,</span> m_p<span class="token punctuation">,</span> logs_p<span class="token punctuation">,</span> m_q<span class="token punctuation">,</span> logs_q<span class="token punctuation">)</span>

    <span class="token comment"># 生成音频</span>
    <span class="token keyword">def</span> <span class="token function">infer</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x_lengths<span class="token punctuation">,</span> sid<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> noise_scale<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> length_scale<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> noise_scale_w<span class="token operator">=</span><span class="token number">1.</span><span class="token punctuation">,</span> max_len<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        x<span class="token punctuation">,</span> m_p<span class="token punctuation">,</span> logs_p<span class="token punctuation">,</span> x_mask <span class="token operator">=</span> self<span class="token punctuation">.</span>enc_p<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x_lengths<span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>n_speakers <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
            g <span class="token operator">=</span> self<span class="token punctuation">.</span>emb_g<span class="token punctuation">(</span>sid<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [b, h, 1]</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            g <span class="token operator">=</span> <span class="token boolean">None</span>

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_sdp<span class="token punctuation">:</span>
            logw <span class="token operator">=</span> self<span class="token punctuation">.</span>dp<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x_mask<span class="token punctuation">,</span> g<span class="token operator">=</span>g<span class="token punctuation">,</span> reverse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> noise_scale<span class="token operator">=</span>noise_scale_w<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            logw <span class="token operator">=</span> self<span class="token punctuation">.</span>dp<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x_mask<span class="token punctuation">,</span> g<span class="token operator">=</span>g<span class="token punctuation">)</span>
        w <span class="token operator">=</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>logw<span class="token punctuation">)</span> <span class="token operator">*</span> x_mask <span class="token operator">*</span> length_scale
        w_ceil <span class="token operator">=</span> torch<span class="token punctuation">.</span>ceil<span class="token punctuation">(</span>w<span class="token punctuation">)</span>
        y_lengths <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp_min<span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>w_ceil<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        y_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>commons<span class="token punctuation">.</span>sequence_mask<span class="token punctuation">(</span>y_lengths<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>x_mask<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
        attn_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>x_mask<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">*</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>y_mask<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        attn <span class="token operator">=</span> commons<span class="token punctuation">.</span>generate_path<span class="token punctuation">(</span>w_ceil<span class="token punctuation">,</span> attn_mask<span class="token punctuation">)</span>

        m_p <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>attn<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> m_p<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># [b, t', t], [b, t, d] -&gt; [b, d, t']</span>
        logs_p <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>attn<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> logs_p<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>
                                                                                 <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># [b, t', t], [b, t, d] -&gt; [b, d, t']</span>

        z_p <span class="token operator">=</span> m_p <span class="token operator">+</span> torch<span class="token punctuation">.</span>randn_like<span class="token punctuation">(</span>m_p<span class="token punctuation">)</span> <span class="token operator">*</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>logs_p<span class="token punctuation">)</span> <span class="token operator">*</span> noise_scale
        z <span class="token operator">=</span> self<span class="token punctuation">.</span>flow<span class="token punctuation">(</span>z_p<span class="token punctuation">,</span> y_mask<span class="token punctuation">,</span> g<span class="token operator">=</span>g<span class="token punctuation">,</span> reverse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        o <span class="token operator">=</span> self<span class="token punctuation">.</span>dec<span class="token punctuation">(</span><span class="token punctuation">(</span>z <span class="token operator">*</span> y_mask<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>max_len<span class="token punctuation">]</span><span class="token punctuation">,</span> g<span class="token operator">=</span>g<span class="token punctuation">)</span>
        <span class="token keyword">return</span> o<span class="token punctuation">,</span> attn<span class="token punctuation">,</span> y_mask<span class="token punctuation">,</span> <span class="token punctuation">(</span>z<span class="token punctuation">,</span> z_p<span class="token punctuation">,</span> m_p<span class="token punctuation">,</span> logs_p<span class="token punctuation">)</span>
	
	<span class="token comment"># 语音转换</span>
    <span class="token keyword">def</span> <span class="token function">voice_conversion</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> y<span class="token punctuation">,</span> y_lengths<span class="token punctuation">,</span> sid_src<span class="token punctuation">,</span> sid_tgt<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>n_speakers <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">"n_speakers have to be larger than 0."</span>
        g_src <span class="token operator">=</span> self<span class="token punctuation">.</span>emb_g<span class="token punctuation">(</span>sid_src<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 源speaker的embedding</span>
        g_tgt <span class="token operator">=</span> self<span class="token punctuation">.</span>emb_g<span class="token punctuation">(</span>sid_tgt<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 目标speaker的embedding</span>
        z<span class="token punctuation">,</span> m_q<span class="token punctuation">,</span> logs_q<span class="token punctuation">,</span> y_mask <span class="token operator">=</span> self<span class="token punctuation">.</span>enc_q<span class="token punctuation">(</span>y<span class="token punctuation">,</span> y_lengths<span class="token punctuation">,</span> g<span class="token operator">=</span>g_src<span class="token punctuation">)</span>  <span class="token comment"># 以源speaker为条件通过训练后的后验编码器预测分布，采样z</span>
        z_p <span class="token operator">=</span> self<span class="token punctuation">.</span>flow<span class="token punctuation">(</span>z<span class="token punctuation">,</span> y_mask<span class="token punctuation">,</span> g<span class="token operator">=</span>g_src<span class="token punctuation">)</span>  <span class="token comment"># 还是以源speaker为条件，将采样的z经过flow转换为f_θ(z)</span>
        z_hat <span class="token operator">=</span> self<span class="token punctuation">.</span>flow<span class="token punctuation">(</span>z_p<span class="token punctuation">,</span> y_mask<span class="token punctuation">,</span> g<span class="token operator">=</span>g_tgt<span class="token punctuation">,</span> reverse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token comment"># 再以目标speaker作为条件，将f_θ(z)通过逆flow转换为z_hat</span>
        o_hat <span class="token operator">=</span> self<span class="token punctuation">.</span>dec<span class="token punctuation">(</span>z_hat <span class="token operator">*</span> y_mask<span class="token punctuation">,</span> g<span class="token operator">=</span>g_tgt<span class="token punctuation">)</span>  <span class="token comment"># 以目标speaker作为条件，基于z_hat重建音频，实现voice conversion</span>
        <span class="token keyword">return</span> o_hat<span class="token punctuation">,</span> y_mask<span class="token punctuation">,</span> <span class="token punctuation">(</span>z<span class="token punctuation">,</span> z_p<span class="token punctuation">,</span> z_hat<span class="token punctuation">)</span>

</code></pre> 
<h2><a id="modulespy_611"></a>modules.py</h2> 
<p>模型构建过程中flow、随机时长预测器中需要的一些小型模块在本文件中定义，具体代码如下：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> copy
<span class="token keyword">import</span> math
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> scipy
<span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> functional <span class="token keyword">as</span> F

<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> Conv1d<span class="token punctuation">,</span> ConvTranspose1d<span class="token punctuation">,</span> AvgPool1d<span class="token punctuation">,</span> Conv2d
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>utils <span class="token keyword">import</span> weight_norm<span class="token punctuation">,</span> remove_weight_norm

<span class="token keyword">import</span> commons
<span class="token keyword">from</span> commons <span class="token keyword">import</span> init_weights<span class="token punctuation">,</span> get_padding
<span class="token keyword">from</span> transforms <span class="token keyword">import</span> piecewise_rational_quadratic_transform

LRELU_SLOPE <span class="token operator">=</span> <span class="token number">0.1</span>


<span class="token keyword">class</span> <span class="token class-name">LayerNorm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> channels<span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>channels <span class="token operator">=</span> channels
        self<span class="token punctuation">.</span>eps <span class="token operator">=</span> eps

        self<span class="token punctuation">.</span>gamma <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>channels<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>beta <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>channels<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>layer_norm<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>channels<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>gamma<span class="token punctuation">,</span> self<span class="token punctuation">.</span>beta<span class="token punctuation">,</span> self<span class="token punctuation">.</span>eps<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">ConvReluNorm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> hidden_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> n_layers<span class="token punctuation">,</span> p_dropout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>in_channels <span class="token operator">=</span> in_channels
        self<span class="token punctuation">.</span>hidden_channels <span class="token operator">=</span> hidden_channels
        self<span class="token punctuation">.</span>out_channels <span class="token operator">=</span> out_channels
        self<span class="token punctuation">.</span>kernel_size <span class="token operator">=</span> kernel_size
        self<span class="token punctuation">.</span>n_layers <span class="token operator">=</span> n_layers
        self<span class="token punctuation">.</span>p_dropout <span class="token operator">=</span> p_dropout
        <span class="token keyword">assert</span> n_layers <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">"Number of layers should be larger than 0."</span>

        self<span class="token punctuation">.</span>conv_layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm_layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv_layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> hidden_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> padding<span class="token operator">=</span>kernel_size <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm_layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>LayerNorm<span class="token punctuation">(</span>hidden_channels<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>relu_drop <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p_dropout<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n_layers <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>conv_layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>hidden_channels<span class="token punctuation">,</span> hidden_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> padding<span class="token operator">=</span>kernel_size <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>norm_layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>LayerNorm<span class="token punctuation">(</span>hidden_channels<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>hidden_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>proj<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>proj<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x_org <span class="token operator">=</span> x
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv_layers<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>x <span class="token operator">*</span> x_mask<span class="token punctuation">)</span>
            x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm_layers<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            x <span class="token operator">=</span> self<span class="token punctuation">.</span>relu_drop<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> x_org <span class="token operator">+</span> self<span class="token punctuation">.</span>proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x <span class="token operator">*</span> x_mask


<span class="token comment"># 带洞的深度可分离卷积，用于随机时长预测器构建条件编码器</span>
<span class="token keyword">class</span> <span class="token class-name">DDSConv</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Dilated and Depth-Separable Convolution
    """</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> n_layers<span class="token punctuation">,</span> p_dropout<span class="token operator">=</span><span class="token number">0.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>channels <span class="token operator">=</span> channels
        self<span class="token punctuation">.</span>kernel_size <span class="token operator">=</span> kernel_size  <span class="token comment"># 卷积核大小</span>
        self<span class="token punctuation">.</span>n_layers <span class="token operator">=</span> n_layers  <span class="token comment"># 层数</span>
        self<span class="token punctuation">.</span>p_dropout <span class="token operator">=</span> p_dropout

        self<span class="token punctuation">.</span>drop <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p_dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>convs_sep <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 存放可分离卷积的列表</span>
        self<span class="token punctuation">.</span>convs_1x1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 存放1x1卷积的列表</span>
        self<span class="token punctuation">.</span>norms_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norms_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
            dilation <span class="token operator">=</span> kernel_size <span class="token operator">**</span> i
            padding <span class="token operator">=</span> <span class="token punctuation">(</span>kernel_size <span class="token operator">*</span> dilation <span class="token operator">-</span> dilation<span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span>
            self<span class="token punctuation">.</span>convs_sep<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>channels<span class="token punctuation">,</span> channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span>
                                            groups<span class="token operator">=</span>channels<span class="token punctuation">,</span> dilation<span class="token operator">=</span>dilation<span class="token punctuation">,</span> padding<span class="token operator">=</span>padding<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 可分离卷积</span>
            self<span class="token punctuation">.</span>convs_1x1<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>channels<span class="token punctuation">,</span> channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 1x1卷积</span>
            self<span class="token punctuation">.</span>norms_1<span class="token punctuation">.</span>append<span class="token punctuation">(</span>LayerNorm<span class="token punctuation">(</span>channels<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># layernorm1</span>
            self<span class="token punctuation">.</span>norms_2<span class="token punctuation">.</span>append<span class="token punctuation">(</span>LayerNorm<span class="token punctuation">(</span>channels<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># layernorm2</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x_mask<span class="token punctuation">,</span> g<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> g <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> x <span class="token operator">+</span> g  <span class="token comment"># 叠加条件</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
            y <span class="token operator">=</span> self<span class="token punctuation">.</span>convs_sep<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>x <span class="token operator">*</span> x_mask<span class="token punctuation">)</span>  <span class="token comment"># 先进行可分离卷积</span>
            y <span class="token operator">=</span> self<span class="token punctuation">.</span>norms_1<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span>  <span class="token comment"># layernorm</span>
            y <span class="token operator">=</span> F<span class="token punctuation">.</span>gelu<span class="token punctuation">(</span>y<span class="token punctuation">)</span>  <span class="token comment"># 激活</span>
            y <span class="token operator">=</span> self<span class="token punctuation">.</span>convs_1x1<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span>  <span class="token comment"># 1x1卷积</span>
            y <span class="token operator">=</span> self<span class="token punctuation">.</span>norms_2<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span>  <span class="token comment"># layernorm</span>
            y <span class="token operator">=</span> F<span class="token punctuation">.</span>gelu<span class="token punctuation">(</span>y<span class="token punctuation">)</span>  <span class="token comment"># 激活</span>
            y <span class="token operator">=</span> self<span class="token punctuation">.</span>drop<span class="token punctuation">(</span>y<span class="token punctuation">)</span>  <span class="token comment"># dropout</span>
            x <span class="token operator">=</span> x <span class="token operator">+</span> y  <span class="token comment"># residual连接</span>
        <span class="token keyword">return</span> x <span class="token operator">*</span> x_mask


<span class="token comment"># WaveNet的残差模块；不断提高一维膨胀卷积的膨胀系数，不断增大感受野，卷积后的结果一部分元素加到下一层的输入，另一部分元素加到最终的输出</span>
<span class="token keyword">class</span> <span class="token class-name">WN</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> dilation_rate<span class="token punctuation">,</span> n_layers<span class="token punctuation">,</span> gin_channels<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> p_dropout<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>WN<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">assert</span> <span class="token punctuation">(</span>kernel_size <span class="token operator">%</span> <span class="token number">2</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>hidden_channels <span class="token operator">=</span> hidden_channels
        self<span class="token punctuation">.</span>kernel_size <span class="token operator">=</span> kernel_size<span class="token punctuation">,</span>
        self<span class="token punctuation">.</span>dilation_rate <span class="token operator">=</span> dilation_rate
        self<span class="token punctuation">.</span>n_layers <span class="token operator">=</span> n_layers
        self<span class="token punctuation">.</span>gin_channels <span class="token operator">=</span> gin_channels
        self<span class="token punctuation">.</span>p_dropout <span class="token operator">=</span> p_dropout

        self<span class="token punctuation">.</span>in_layers <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>res_skip_layers <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>drop <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p_dropout<span class="token punctuation">)</span>

        <span class="token keyword">if</span> gin_channels <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>
            cond_layer <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>gin_channels<span class="token punctuation">,</span> <span class="token number">2</span> <span class="token operator">*</span> hidden_channels <span class="token operator">*</span> n_layers<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>cond_layer <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>weight_norm<span class="token punctuation">(</span>cond_layer<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'weight'</span><span class="token punctuation">)</span>  <span class="token comment"># 条件层</span>

        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
            dilation <span class="token operator">=</span> dilation_rate <span class="token operator">**</span> i  <span class="token comment"># 膨胀系数增大</span>
            padding <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token punctuation">(</span>kernel_size <span class="token operator">*</span> dilation <span class="token operator">-</span> dilation<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">)</span>
            in_layer <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>hidden_channels<span class="token punctuation">,</span> <span class="token number">2</span> <span class="token operator">*</span> hidden_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span>
                                       dilation<span class="token operator">=</span>dilation<span class="token punctuation">,</span> padding<span class="token operator">=</span>padding<span class="token punctuation">)</span>
            in_layer <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>weight_norm<span class="token punctuation">(</span>in_layer<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'weight'</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>in_layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>in_layer<span class="token punctuation">)</span>

            <span class="token comment"># last one is not necessary</span>
            <span class="token keyword">if</span> i <span class="token operator">&lt;</span> n_layers <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">:</span>
                res_skip_channels <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> hidden_channels
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                res_skip_channels <span class="token operator">=</span> hidden_channels

            res_skip_layer <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>hidden_channels<span class="token punctuation">,</span> res_skip_channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            res_skip_layer <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>weight_norm<span class="token punctuation">(</span>res_skip_layer<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'weight'</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>res_skip_layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>res_skip_layer<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x_mask<span class="token punctuation">,</span> g<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        output <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        n_channels_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>IntTensor<span class="token punctuation">(</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>hidden_channels<span class="token punctuation">]</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> g <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            g <span class="token operator">=</span> self<span class="token punctuation">.</span>cond_layer<span class="token punctuation">(</span>g<span class="token punctuation">)</span>  <span class="token comment"># 条件</span>

        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
            x_in <span class="token operator">=</span> self<span class="token punctuation">.</span>in_layers<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            <span class="token keyword">if</span> g <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                cond_offset <span class="token operator">=</span> i <span class="token operator">*</span> <span class="token number">2</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>hidden_channels
                g_l <span class="token operator">=</span> g<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> cond_offset<span class="token punctuation">:</span>cond_offset <span class="token operator">+</span> <span class="token number">2</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>hidden_channels<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                g_l <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>x_in<span class="token punctuation">)</span>

            acts <span class="token operator">=</span> commons<span class="token punctuation">.</span>fused_add_tanh_sigmoid_multiply<span class="token punctuation">(</span>x_in<span class="token punctuation">,</span> g_l<span class="token punctuation">,</span> n_channels_tensor<span class="token punctuation">)</span>
            acts <span class="token operator">=</span> self<span class="token punctuation">.</span>drop<span class="token punctuation">(</span>acts<span class="token punctuation">)</span>

            res_skip_acts <span class="token operator">=</span> self<span class="token punctuation">.</span>res_skip_layers<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>acts<span class="token punctuation">)</span>
            <span class="token keyword">if</span> i <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>n_layers <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">:</span>
                res_acts <span class="token operator">=</span> res_skip_acts<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>self<span class="token punctuation">.</span>hidden_channels<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>
                x <span class="token operator">=</span> <span class="token punctuation">(</span>x <span class="token operator">+</span> res_acts<span class="token punctuation">)</span> <span class="token operator">*</span> x_mask
                output <span class="token operator">=</span> output <span class="token operator">+</span> res_skip_acts<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_channels<span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                output <span class="token operator">=</span> output <span class="token operator">+</span> res_skip_acts
        <span class="token keyword">return</span> output <span class="token operator">*</span> x_mask

    <span class="token comment"># 清除权重参数</span>
    <span class="token keyword">def</span> <span class="token function">remove_weight_norm</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>gin_channels <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>
            torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>remove_weight_norm<span class="token punctuation">(</span>self<span class="token punctuation">.</span>cond_layer<span class="token punctuation">)</span>
        <span class="token keyword">for</span> l <span class="token keyword">in</span> self<span class="token punctuation">.</span>in_layers<span class="token punctuation">:</span>
            torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>remove_weight_norm<span class="token punctuation">(</span>l<span class="token punctuation">)</span>
        <span class="token keyword">for</span> l <span class="token keyword">in</span> self<span class="token punctuation">.</span>res_skip_layers<span class="token punctuation">:</span>
            torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>remove_weight_norm<span class="token punctuation">(</span>l<span class="token punctuation">)</span>


<span class="token comment"># 一种MRF层，用于音频解码器</span>
<span class="token keyword">class</span> <span class="token class-name">ResBlock1</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>ResBlock1<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>convs1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>
            weight_norm<span class="token punctuation">(</span>Conv1d<span class="token punctuation">(</span>channels<span class="token punctuation">,</span> channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span>dilation<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                               padding<span class="token operator">=</span>get_padding<span class="token punctuation">(</span>kernel_size<span class="token punctuation">,</span> dilation<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            weight_norm<span class="token punctuation">(</span>Conv1d<span class="token punctuation">(</span>channels<span class="token punctuation">,</span> channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span>dilation<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                               padding<span class="token operator">=</span>get_padding<span class="token punctuation">(</span>kernel_size<span class="token punctuation">,</span> dilation<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            weight_norm<span class="token punctuation">(</span>Conv1d<span class="token punctuation">(</span>channels<span class="token punctuation">,</span> channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span>dilation<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                               padding<span class="token operator">=</span>get_padding<span class="token punctuation">(</span>kernel_size<span class="token punctuation">,</span> dilation<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>convs1<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>init_weights<span class="token punctuation">)</span>  <span class="token comment"># 初始化参数</span>

        self<span class="token punctuation">.</span>convs2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>
            weight_norm<span class="token punctuation">(</span>Conv1d<span class="token punctuation">(</span>channels<span class="token punctuation">,</span> channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
                               padding<span class="token operator">=</span>get_padding<span class="token punctuation">(</span>kernel_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            weight_norm<span class="token punctuation">(</span>Conv1d<span class="token punctuation">(</span>channels<span class="token punctuation">,</span> channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
                               padding<span class="token operator">=</span>get_padding<span class="token punctuation">(</span>kernel_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            weight_norm<span class="token punctuation">(</span>Conv1d<span class="token punctuation">(</span>channels<span class="token punctuation">,</span> channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
                               padding<span class="token operator">=</span>get_padding<span class="token punctuation">(</span>kernel_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>convs2<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>init_weights<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> c1<span class="token punctuation">,</span> c2 <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>convs1<span class="token punctuation">,</span> self<span class="token punctuation">.</span>convs2<span class="token punctuation">)</span><span class="token punctuation">:</span>
            xt <span class="token operator">=</span> F<span class="token punctuation">.</span>leaky_relu<span class="token punctuation">(</span>x<span class="token punctuation">,</span> LRELU_SLOPE<span class="token punctuation">)</span>
            <span class="token keyword">if</span> x_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                xt <span class="token operator">=</span> xt <span class="token operator">*</span> x_mask
            xt <span class="token operator">=</span> c1<span class="token punctuation">(</span>xt<span class="token punctuation">)</span>
            xt <span class="token operator">=</span> F<span class="token punctuation">.</span>leaky_relu<span class="token punctuation">(</span>xt<span class="token punctuation">,</span> LRELU_SLOPE<span class="token punctuation">)</span>
            <span class="token keyword">if</span> x_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                xt <span class="token operator">=</span> xt <span class="token operator">*</span> x_mask
            xt <span class="token operator">=</span> c2<span class="token punctuation">(</span>xt<span class="token punctuation">)</span>
            x <span class="token operator">=</span> xt <span class="token operator">+</span> x  <span class="token comment"># residual连接</span>
        <span class="token keyword">if</span> x_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> x <span class="token operator">*</span> x_mask
        <span class="token keyword">return</span> x

    <span class="token keyword">def</span> <span class="token function">remove_weight_norm</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> l <span class="token keyword">in</span> self<span class="token punctuation">.</span>convs1<span class="token punctuation">:</span>
            remove_weight_norm<span class="token punctuation">(</span>l<span class="token punctuation">)</span>
        <span class="token keyword">for</span> l <span class="token keyword">in</span> self<span class="token punctuation">.</span>convs2<span class="token punctuation">:</span>
            remove_weight_norm<span class="token punctuation">(</span>l<span class="token punctuation">)</span>


<span class="token comment"># 一种MRF层，用于音频解码器</span>
<span class="token keyword">class</span> <span class="token class-name">ResBlock2</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>ResBlock2<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>convs <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>
            weight_norm<span class="token punctuation">(</span>Conv1d<span class="token punctuation">(</span>channels<span class="token punctuation">,</span> channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span>dilation<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                               padding<span class="token operator">=</span>get_padding<span class="token punctuation">(</span>kernel_size<span class="token punctuation">,</span> dilation<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            weight_norm<span class="token punctuation">(</span>Conv1d<span class="token punctuation">(</span>channels<span class="token punctuation">,</span> channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span>dilation<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                               padding<span class="token operator">=</span>get_padding<span class="token punctuation">(</span>kernel_size<span class="token punctuation">,</span> dilation<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>convs<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>init_weights<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> c <span class="token keyword">in</span> self<span class="token punctuation">.</span>convs<span class="token punctuation">:</span>
            xt <span class="token operator">=</span> F<span class="token punctuation">.</span>leaky_relu<span class="token punctuation">(</span>x<span class="token punctuation">,</span> LRELU_SLOPE<span class="token punctuation">)</span>
            <span class="token keyword">if</span> x_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                xt <span class="token operator">=</span> xt <span class="token operator">*</span> x_mask
            xt <span class="token operator">=</span> c<span class="token punctuation">(</span>xt<span class="token punctuation">)</span>
            x <span class="token operator">=</span> xt <span class="token operator">+</span> x  <span class="token comment"># residual连接</span>
        <span class="token keyword">if</span> x_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> x <span class="token operator">*</span> x_mask
        <span class="token keyword">return</span> x

    <span class="token keyword">def</span> <span class="token function">remove_weight_norm</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> l <span class="token keyword">in</span> self<span class="token punctuation">.</span>convs<span class="token punctuation">:</span>
            remove_weight_norm<span class="token punctuation">(</span>l<span class="token punctuation">)</span>


<span class="token comment"># 正向，计算x的对数；逆向，计算x的反对数</span>
<span class="token keyword">class</span> <span class="token class-name">Log</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x_mask<span class="token punctuation">,</span> reverse<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> reverse<span class="token punctuation">:</span>
            <span class="token comment"># torch.clamp_min是给x设置了一个下限，如果x中的元素小于1e-5，就将该位置的值改为1e-5</span>
            y <span class="token operator">=</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>clamp_min<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">1e-5</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> x_mask
            logdet <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token operator">-</span>y<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
            <span class="token keyword">return</span> y<span class="token punctuation">,</span> logdet
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">*</span> x_mask
            <span class="token keyword">return</span> x


<span class="token comment"># 数据在指定维度上翻转</span>
<span class="token keyword">class</span> <span class="token class-name">Flip</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> reverse<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>flip<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> reverse<span class="token punctuation">:</span>
            logdet <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>dtype<span class="token operator">=</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>x<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
            <span class="token keyword">return</span> x<span class="token punctuation">,</span> logdet
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> x


<span class="token comment"># 按位计算的仿射变换层</span>
<span class="token keyword">class</span> <span class="token class-name">ElementwiseAffine</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> channels<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>channels <span class="token operator">=</span> channels  <span class="token comment"># 输入的通道数</span>
        self<span class="token punctuation">.</span>m <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 偏移量</span>
        self<span class="token punctuation">.</span>logs <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 缩放量的对数</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x_mask<span class="token punctuation">,</span> reverse<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> reverse<span class="token punctuation">:</span>  <span class="token comment"># 正向</span>
            y <span class="token operator">=</span> self<span class="token punctuation">.</span>m <span class="token operator">+</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>self<span class="token punctuation">.</span>logs<span class="token punctuation">)</span> <span class="token operator">*</span> x  <span class="token comment"># y = s * x + m</span>
            y <span class="token operator">=</span> y <span class="token operator">*</span> x_mask
            logdet <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>logs <span class="token operator">*</span> x_mask<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 计算本次变化的对数行列式</span>
            <span class="token keyword">return</span> y<span class="token punctuation">,</span> logdet
        <span class="token keyword">else</span><span class="token punctuation">:</span>  <span class="token comment"># 逆向</span>
            x <span class="token operator">=</span> <span class="token punctuation">(</span>x <span class="token operator">-</span> self<span class="token punctuation">.</span>m<span class="token punctuation">)</span> <span class="token operator">*</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>self<span class="token punctuation">.</span>logs<span class="token punctuation">)</span> <span class="token operator">*</span> x_mask  <span class="token comment"># x = (x - m) / s</span>
            <span class="token keyword">return</span> x


<span class="token comment"># 残差耦合层，使用的时候选择只预测平移量，是一个对数似然不增加的flow变换，或者说体积不变</span>
<span class="token keyword">class</span> <span class="token class-name">ResidualCouplingLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>
                 channels<span class="token punctuation">,</span>
                 hidden_channels<span class="token punctuation">,</span>
                 kernel_size<span class="token punctuation">,</span>
                 dilation_rate<span class="token punctuation">,</span>
                 n_layers<span class="token punctuation">,</span>
                 p_dropout<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
                 gin_channels<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
                 mean_only<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">assert</span> channels <span class="token operator">%</span> <span class="token number">2</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">"channels should be divisible by 2"</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>channels <span class="token operator">=</span> channels
        self<span class="token punctuation">.</span>hidden_channels <span class="token operator">=</span> hidden_channels
        self<span class="token punctuation">.</span>kernel_size <span class="token operator">=</span> kernel_size
        self<span class="token punctuation">.</span>dilation_rate <span class="token operator">=</span> dilation_rate
        self<span class="token punctuation">.</span>n_layers <span class="token operator">=</span> n_layers
        self<span class="token punctuation">.</span>half_channels <span class="token operator">=</span> channels <span class="token operator">//</span> <span class="token number">2</span>
        self<span class="token punctuation">.</span>mean_only <span class="token operator">=</span> mean_only

        self<span class="token punctuation">.</span>pre <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>self<span class="token punctuation">.</span>half_channels<span class="token punctuation">,</span> hidden_channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 一维卷积</span>
        self<span class="token punctuation">.</span>enc <span class="token operator">=</span> WN<span class="token punctuation">(</span>hidden_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> dilation_rate<span class="token punctuation">,</span> n_layers<span class="token punctuation">,</span> p_dropout<span class="token operator">=</span>p_dropout<span class="token punctuation">,</span>
                      gin_channels<span class="token operator">=</span>gin_channels<span class="token punctuation">)</span>  <span class="token comment"># 相当于Glow论文中的NN，VITS中使用的是WaveNet的残差模块</span>
        self<span class="token punctuation">.</span>post <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>hidden_channels<span class="token punctuation">,</span> self<span class="token punctuation">.</span>half_channels <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">-</span> mean_only<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>post<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>post<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x_mask<span class="token punctuation">,</span> g<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> reverse<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        x0<span class="token punctuation">,</span> x1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>half_channels<span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 先将输入分成两份</span>
        <span class="token comment"># 使用x0去预测偏移量m和所放量s</span>
        h <span class="token operator">=</span> self<span class="token punctuation">.</span>pre<span class="token punctuation">(</span>x0<span class="token punctuation">)</span> <span class="token operator">*</span> x_mask
        h <span class="token operator">=</span> self<span class="token punctuation">.</span>enc<span class="token punctuation">(</span>h<span class="token punctuation">,</span> x_mask<span class="token punctuation">,</span> g<span class="token operator">=</span>g<span class="token punctuation">)</span>
        stats <span class="token operator">=</span> self<span class="token punctuation">.</span>post<span class="token punctuation">(</span>h<span class="token punctuation">)</span> <span class="token operator">*</span> x_mask
        <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>mean_only<span class="token punctuation">:</span>  <span class="token comment"># mean_only的意思是只预测平移量/m，不预测伸缩量/s</span>
            m<span class="token punctuation">,</span> logs <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>stats<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>half_channels<span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>  <span class="token comment"># 只预测缩放量</span>
            m <span class="token operator">=</span> stats
            logs <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>m<span class="token punctuation">)</span>  <span class="token comment"># 设置s的log为0，即s为1，表示经过这样的flow，其对数似然没有增量</span>

        <span class="token keyword">if</span> <span class="token keyword">not</span> reverse<span class="token punctuation">:</span>  <span class="token comment"># 正向，训练</span>
            x1 <span class="token operator">=</span> m <span class="token operator">+</span> x1 <span class="token operator">*</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>logs<span class="token punctuation">)</span> <span class="token operator">*</span> x_mask  <span class="token comment"># 使用m和s对x1进行变换</span>
            x <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>x0<span class="token punctuation">,</span> x1<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 拼接x0和x1得到最终输出x</span>
            logdet <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>logs<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 变换的对数行列式</span>
            <span class="token keyword">return</span> x<span class="token punctuation">,</span> logdet
        <span class="token keyword">else</span><span class="token punctuation">:</span>  <span class="token comment"># 逆向，预测</span>
            x1 <span class="token operator">=</span> <span class="token punctuation">(</span>x1 <span class="token operator">-</span> m<span class="token punctuation">)</span> <span class="token operator">*</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>logs<span class="token punctuation">)</span> <span class="token operator">*</span> x_mask
            x <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>x0<span class="token punctuation">,</span> x1<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token keyword">return</span> x


<span class="token comment"># 神经样条流；随机时长预测器中的后验编码器和标准化流包含4层神经样条流/neural spline flows，用于构建随机时长预测器中的基于flow的先验、后验编码器</span>
<span class="token keyword">class</span> <span class="token class-name">ConvFlow</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> filter_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> n_layers<span class="token punctuation">,</span> num_bins<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> tail_bound<span class="token operator">=</span><span class="token number">5.0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>in_channels <span class="token operator">=</span> in_channels  <span class="token comment"># 输入通道</span>
        self<span class="token punctuation">.</span>filter_channels <span class="token operator">=</span> filter_channels  <span class="token comment"># 过滤器个数，即1×1卷积计算后的输出维度</span>
        self<span class="token punctuation">.</span>kernel_size <span class="token operator">=</span> kernel_size  <span class="token comment"># DDSConv卷积核尺寸</span>
        self<span class="token punctuation">.</span>n_layers <span class="token operator">=</span> n_layers
        self<span class="token punctuation">.</span>num_bins <span class="token operator">=</span> num_bins
        self<span class="token punctuation">.</span>tail_bound <span class="token operator">=</span> tail_bound
        self<span class="token punctuation">.</span>half_channels <span class="token operator">=</span> in_channels <span class="token operator">//</span> <span class="token number">2</span>  <span class="token comment"># 一半的通道数，因为会将输入分为两部分</span>

        self<span class="token punctuation">.</span>pre <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>self<span class="token punctuation">.</span>half_channels<span class="token punctuation">,</span> filter_channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>convs <span class="token operator">=</span> DDSConv<span class="token punctuation">(</span>filter_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> n_layers<span class="token punctuation">,</span> p_dropout<span class="token operator">=</span><span class="token number">0.</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>filter_channels<span class="token punctuation">,</span> self<span class="token punctuation">.</span>half_channels <span class="token operator">*</span> <span class="token punctuation">(</span>num_bins <span class="token operator">*</span> <span class="token number">3</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>proj<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>proj<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x_mask<span class="token punctuation">,</span> g<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> reverse<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        x0<span class="token punctuation">,</span> x1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>half_channels<span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 输入分为两部分</span>
        h <span class="token operator">=</span> self<span class="token punctuation">.</span>pre<span class="token punctuation">(</span>x0<span class="token punctuation">)</span>
        h <span class="token operator">=</span> self<span class="token punctuation">.</span>convs<span class="token punctuation">(</span>h<span class="token punctuation">,</span> x_mask<span class="token punctuation">,</span> g<span class="token operator">=</span>g<span class="token punctuation">)</span>  <span class="token comment"># g是条件</span>
        h <span class="token operator">=</span> self<span class="token punctuation">.</span>proj<span class="token punctuation">(</span>h<span class="token punctuation">)</span> <span class="token operator">*</span> x_mask

        b<span class="token punctuation">,</span> c<span class="token punctuation">,</span> t <span class="token operator">=</span> x0<span class="token punctuation">.</span>shape
        h <span class="token operator">=</span> h<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>b<span class="token punctuation">,</span> c<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># [b, cx?, t] -&gt; [b, c, t, ?]</span>

        unnormalized_widths <span class="token operator">=</span> h<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>self<span class="token punctuation">.</span>num_bins<span class="token punctuation">]</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>filter_channels<span class="token punctuation">)</span>
        unnormalized_heights <span class="token operator">=</span> h<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_bins<span class="token punctuation">:</span><span class="token number">2</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>num_bins<span class="token punctuation">]</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>filter_channels<span class="token punctuation">)</span>
        unnormalized_derivatives <span class="token operator">=</span> h<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>num_bins<span class="token punctuation">:</span><span class="token punctuation">]</span>

        <span class="token comment"># 基于原始x1和由x0预测的值得到的一些参数计算新的x1，并计算转换过程中的对数行列式</span>
        x1<span class="token punctuation">,</span> logabsdet <span class="token operator">=</span> piecewise_rational_quadratic_transform<span class="token punctuation">(</span>x1<span class="token punctuation">,</span>
                                                               unnormalized_widths<span class="token punctuation">,</span>
                                                               unnormalized_heights<span class="token punctuation">,</span>
                                                               unnormalized_derivatives<span class="token punctuation">,</span>
                                                               inverse<span class="token operator">=</span>reverse<span class="token punctuation">,</span>
                                                               tails<span class="token operator">=</span><span class="token string">'linear'</span><span class="token punctuation">,</span>
                                                               tail_bound<span class="token operator">=</span>self<span class="token punctuation">.</span>tail_bound<span class="token punctuation">)</span>

        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>x0<span class="token punctuation">,</span> x1<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> x_mask  <span class="token comment"># 将x0和x1拼接组成x</span>
        logdet <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>logabsdet <span class="token operator">*</span> x_mask<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 计算对数行列式</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> reverse<span class="token punctuation">:</span>  <span class="token comment"># 正向</span>
            <span class="token keyword">return</span> x<span class="token punctuation">,</span> logdet
        <span class="token keyword">else</span><span class="token punctuation">:</span>  <span class="token comment"># 逆向</span>
            <span class="token keyword">return</span> x

</code></pre> 
<h2><a id="attentionspy_1016"></a>attentions.py</h2> 
<p>生成器中的文本编码器是基于Transformer架构，主要代码在本文件中</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> copy
<span class="token keyword">import</span> math
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> functional <span class="token keyword">as</span> F

<span class="token keyword">import</span> commons
<span class="token keyword">import</span> modules
<span class="token keyword">from</span> modules <span class="token keyword">import</span> LayerNorm


<span class="token comment"># Transformer架构的编码器</span>
<span class="token keyword">class</span> <span class="token class-name">Encoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_channels<span class="token punctuation">,</span> filter_channels<span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> n_layers<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> p_dropout<span class="token operator">=</span><span class="token number">0.</span><span class="token punctuation">,</span> window_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>
                 <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>hidden_channels <span class="token operator">=</span> hidden_channels
        self<span class="token punctuation">.</span>filter_channels <span class="token operator">=</span> filter_channels
        self<span class="token punctuation">.</span>n_heads <span class="token operator">=</span> n_heads
        self<span class="token punctuation">.</span>n_layers <span class="token operator">=</span> n_layers
        self<span class="token punctuation">.</span>kernel_size <span class="token operator">=</span> kernel_size
        self<span class="token punctuation">.</span>p_dropout <span class="token operator">=</span> p_dropout
        self<span class="token punctuation">.</span>window_size <span class="token operator">=</span> window_size

        self<span class="token punctuation">.</span>drop <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p_dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>attn_layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm_layers_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ffn_layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm_layers_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>attn_layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>MultiHeadAttention<span class="token punctuation">(</span>hidden_channels<span class="token punctuation">,</span> hidden_channels<span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> p_dropout<span class="token operator">=</span>p_dropout<span class="token punctuation">,</span>
                                                       window_size<span class="token operator">=</span>window_size<span class="token punctuation">)</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>norm_layers_1<span class="token punctuation">.</span>append<span class="token punctuation">(</span>LayerNorm<span class="token punctuation">(</span>hidden_channels<span class="token punctuation">)</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>ffn_layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>
                FFN<span class="token punctuation">(</span>hidden_channels<span class="token punctuation">,</span> hidden_channels<span class="token punctuation">,</span> filter_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> p_dropout<span class="token operator">=</span>p_dropout<span class="token punctuation">)</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>norm_layers_2<span class="token punctuation">.</span>append<span class="token punctuation">(</span>LayerNorm<span class="token punctuation">(</span>hidden_channels<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
        attn_mask <span class="token operator">=</span> x_mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">*</span> x_mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> x <span class="token operator">*</span> x_mask
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
            y <span class="token operator">=</span> self<span class="token punctuation">.</span>attn_layers<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> attn_mask<span class="token punctuation">)</span>
            y <span class="token operator">=</span> self<span class="token punctuation">.</span>drop<span class="token punctuation">(</span>y<span class="token punctuation">)</span>
            x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm_layers_1<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>x <span class="token operator">+</span> y<span class="token punctuation">)</span>

            y <span class="token operator">=</span> self<span class="token punctuation">.</span>ffn_layers<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> x_mask<span class="token punctuation">)</span>
            y <span class="token operator">=</span> self<span class="token punctuation">.</span>drop<span class="token punctuation">(</span>y<span class="token punctuation">)</span>
            x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm_layers_2<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>x <span class="token operator">+</span> y<span class="token punctuation">)</span>
        x <span class="token operator">=</span> x <span class="token operator">*</span> x_mask
        <span class="token keyword">return</span> x


<span class="token comment"># Transformer架构的解码器</span>
<span class="token keyword">class</span> <span class="token class-name">Decoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_channels<span class="token punctuation">,</span> filter_channels<span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> n_layers<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> p_dropout<span class="token operator">=</span><span class="token number">0.</span><span class="token punctuation">,</span>
                 proximal_bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> proximal_init<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>hidden_channels <span class="token operator">=</span> hidden_channels
        self<span class="token punctuation">.</span>filter_channels <span class="token operator">=</span> filter_channels
        self<span class="token punctuation">.</span>n_heads <span class="token operator">=</span> n_heads
        self<span class="token punctuation">.</span>n_layers <span class="token operator">=</span> n_layers
        self<span class="token punctuation">.</span>kernel_size <span class="token operator">=</span> kernel_size
        self<span class="token punctuation">.</span>p_dropout <span class="token operator">=</span> p_dropout
        self<span class="token punctuation">.</span>proximal_bias <span class="token operator">=</span> proximal_bias
        self<span class="token punctuation">.</span>proximal_init <span class="token operator">=</span> proximal_init

        self<span class="token punctuation">.</span>drop <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p_dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>self_attn_layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm_layers_0 <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>encdec_attn_layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm_layers_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ffn_layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm_layers_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>self_attn_layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>
                MultiHeadAttention<span class="token punctuation">(</span>hidden_channels<span class="token punctuation">,</span> hidden_channels<span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> p_dropout<span class="token operator">=</span>p_dropout<span class="token punctuation">,</span>
                                   proximal_bias<span class="token operator">=</span>proximal_bias<span class="token punctuation">,</span> proximal_init<span class="token operator">=</span>proximal_init<span class="token punctuation">)</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>norm_layers_0<span class="token punctuation">.</span>append<span class="token punctuation">(</span>LayerNorm<span class="token punctuation">(</span>hidden_channels<span class="token punctuation">)</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>encdec_attn_layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>
                MultiHeadAttention<span class="token punctuation">(</span>hidden_channels<span class="token punctuation">,</span> hidden_channels<span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> p_dropout<span class="token operator">=</span>p_dropout<span class="token punctuation">)</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>norm_layers_1<span class="token punctuation">.</span>append<span class="token punctuation">(</span>LayerNorm<span class="token punctuation">(</span>hidden_channels<span class="token punctuation">)</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>ffn_layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>
                FFN<span class="token punctuation">(</span>hidden_channels<span class="token punctuation">,</span> hidden_channels<span class="token punctuation">,</span> filter_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> p_dropout<span class="token operator">=</span>p_dropout<span class="token punctuation">,</span> causal<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>norm_layers_2<span class="token punctuation">.</span>append<span class="token punctuation">(</span>LayerNorm<span class="token punctuation">(</span>hidden_channels<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x_mask<span class="token punctuation">,</span> h<span class="token punctuation">,</span> h_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        x: decoder input
        h: encoder output
        """</span>
        self_attn_mask <span class="token operator">=</span> commons<span class="token punctuation">.</span>subsequent_mask<span class="token punctuation">(</span>x_mask<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token operator">=</span>x<span class="token punctuation">.</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
        encdec_attn_mask <span class="token operator">=</span> h_mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">*</span> x_mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> x <span class="token operator">*</span> x_mask
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
            y <span class="token operator">=</span> self<span class="token punctuation">.</span>self_attn_layers<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> self_attn_mask<span class="token punctuation">)</span>
            y <span class="token operator">=</span> self<span class="token punctuation">.</span>drop<span class="token punctuation">(</span>y<span class="token punctuation">)</span>
            x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm_layers_0<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>x <span class="token operator">+</span> y<span class="token punctuation">)</span>

            y <span class="token operator">=</span> self<span class="token punctuation">.</span>encdec_attn_layers<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> h<span class="token punctuation">,</span> encdec_attn_mask<span class="token punctuation">)</span>
            y <span class="token operator">=</span> self<span class="token punctuation">.</span>drop<span class="token punctuation">(</span>y<span class="token punctuation">)</span>
            x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm_layers_1<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>x <span class="token operator">+</span> y<span class="token punctuation">)</span>

            y <span class="token operator">=</span> self<span class="token punctuation">.</span>ffn_layers<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> x_mask<span class="token punctuation">)</span>
            y <span class="token operator">=</span> self<span class="token punctuation">.</span>drop<span class="token punctuation">(</span>y<span class="token punctuation">)</span>
            x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm_layers_2<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>x <span class="token operator">+</span> y<span class="token punctuation">)</span>
        x <span class="token operator">=</span> x <span class="token operator">*</span> x_mask
        <span class="token keyword">return</span> x


<span class="token comment"># 多头自注意力</span>
<span class="token keyword">class</span> <span class="token class-name">MultiHeadAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> p_dropout<span class="token operator">=</span><span class="token number">0.</span><span class="token punctuation">,</span> window_size<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> heads_share<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                 block_length<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> proximal_bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> proximal_init<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">assert</span> channels <span class="token operator">%</span> n_heads <span class="token operator">==</span> <span class="token number">0</span>

        self<span class="token punctuation">.</span>channels <span class="token operator">=</span> channels
        self<span class="token punctuation">.</span>out_channels <span class="token operator">=</span> out_channels
        self<span class="token punctuation">.</span>n_heads <span class="token operator">=</span> n_heads
        self<span class="token punctuation">.</span>p_dropout <span class="token operator">=</span> p_dropout
        self<span class="token punctuation">.</span>window_size <span class="token operator">=</span> window_size
        self<span class="token punctuation">.</span>heads_share <span class="token operator">=</span> heads_share
        self<span class="token punctuation">.</span>block_length <span class="token operator">=</span> block_length
        self<span class="token punctuation">.</span>proximal_bias <span class="token operator">=</span> proximal_bias
        self<span class="token punctuation">.</span>proximal_init <span class="token operator">=</span> proximal_init
        self<span class="token punctuation">.</span>attn <span class="token operator">=</span> <span class="token boolean">None</span>

        self<span class="token punctuation">.</span>k_channels <span class="token operator">=</span> channels <span class="token operator">//</span> n_heads
        self<span class="token punctuation">.</span>conv_q <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>channels<span class="token punctuation">,</span> channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv_k <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>channels<span class="token punctuation">,</span> channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv_v <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>channels<span class="token punctuation">,</span> channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv_o <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>drop <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p_dropout<span class="token punctuation">)</span>

        <span class="token keyword">if</span> window_size <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            n_heads_rel <span class="token operator">=</span> <span class="token number">1</span> <span class="token keyword">if</span> heads_share <span class="token keyword">else</span> n_heads
            rel_stddev <span class="token operator">=</span> self<span class="token punctuation">.</span>k_channels <span class="token operator">**</span> <span class="token operator">-</span><span class="token number">0.5</span>
            self<span class="token punctuation">.</span>emb_rel_k <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>n_heads_rel<span class="token punctuation">,</span> window_size <span class="token operator">*</span> <span class="token number">2</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>k_channels<span class="token punctuation">)</span> <span class="token operator">*</span> rel_stddev<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>emb_rel_v <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>n_heads_rel<span class="token punctuation">,</span> window_size <span class="token operator">*</span> <span class="token number">2</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>k_channels<span class="token punctuation">)</span> <span class="token operator">*</span> rel_stddev<span class="token punctuation">)</span>

        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv_q<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv_k<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv_v<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
        <span class="token keyword">if</span> proximal_init<span class="token punctuation">:</span>
            <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>conv_k<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>copy_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv_q<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
                self<span class="token punctuation">.</span>conv_k<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>copy_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv_q<span class="token punctuation">.</span>bias<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> c<span class="token punctuation">,</span> attn_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        q <span class="token operator">=</span> self<span class="token punctuation">.</span>conv_q<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        k <span class="token operator">=</span> self<span class="token punctuation">.</span>conv_k<span class="token punctuation">(</span>c<span class="token punctuation">)</span>
        v <span class="token operator">=</span> self<span class="token punctuation">.</span>conv_v<span class="token punctuation">(</span>c<span class="token punctuation">)</span>

        x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>attn <span class="token operator">=</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> v<span class="token punctuation">,</span> mask<span class="token operator">=</span>attn_mask<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv_o<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

    <span class="token keyword">def</span> <span class="token function">attention</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># reshape [b, d, t] -&gt; [b, n_h, t, d_k]</span>
        b<span class="token punctuation">,</span> d<span class="token punctuation">,</span> t_s<span class="token punctuation">,</span> t_t <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token operator">*</span>key<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> query<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        query <span class="token operator">=</span> query<span class="token punctuation">.</span>view<span class="token punctuation">(</span>b<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>k_channels<span class="token punctuation">,</span> t_t<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
        key <span class="token operator">=</span> key<span class="token punctuation">.</span>view<span class="token punctuation">(</span>b<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>k_channels<span class="token punctuation">,</span> t_s<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
        value <span class="token operator">=</span> value<span class="token punctuation">.</span>view<span class="token punctuation">(</span>b<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>k_channels<span class="token punctuation">,</span> t_s<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>

        scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>query <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>k_channels<span class="token punctuation">)</span><span class="token punctuation">,</span> key<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>window_size <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token keyword">assert</span> t_s <span class="token operator">==</span> t_t<span class="token punctuation">,</span> <span class="token string">"Relative attention is only available for self-attention."</span>
            key_relative_embeddings <span class="token operator">=</span> self<span class="token punctuation">.</span>_get_relative_embeddings<span class="token punctuation">(</span>self<span class="token punctuation">.</span>emb_rel_k<span class="token punctuation">,</span> t_s<span class="token punctuation">)</span>
            rel_logits <span class="token operator">=</span> self<span class="token punctuation">.</span>_matmul_with_relative_keys<span class="token punctuation">(</span>query <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>k_channels<span class="token punctuation">)</span><span class="token punctuation">,</span> key_relative_embeddings<span class="token punctuation">)</span>
            scores_local <span class="token operator">=</span> self<span class="token punctuation">.</span>_relative_position_to_absolute_position<span class="token punctuation">(</span>rel_logits<span class="token punctuation">)</span>
            scores <span class="token operator">=</span> scores <span class="token operator">+</span> scores_local
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>proximal_bias<span class="token punctuation">:</span>
            <span class="token keyword">assert</span> t_s <span class="token operator">==</span> t_t<span class="token punctuation">,</span> <span class="token string">"Proximal bias is only available for self-attention."</span>
            scores <span class="token operator">=</span> scores <span class="token operator">+</span> self<span class="token punctuation">.</span>_attention_bias_proximal<span class="token punctuation">(</span>t_s<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token operator">=</span>scores<span class="token punctuation">.</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>scores<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
        <span class="token keyword">if</span> mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            scores <span class="token operator">=</span> scores<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>mask <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1e4</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> self<span class="token punctuation">.</span>block_length <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                <span class="token keyword">assert</span> t_s <span class="token operator">==</span> t_t<span class="token punctuation">,</span> <span class="token string">"Local attention is only available for self-attention."</span>
                block_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>scores<span class="token punctuation">)</span><span class="token punctuation">.</span>triu<span class="token punctuation">(</span><span class="token operator">-</span>self<span class="token punctuation">.</span>block_length<span class="token punctuation">)</span><span class="token punctuation">.</span>tril<span class="token punctuation">(</span>self<span class="token punctuation">.</span>block_length<span class="token punctuation">)</span>
                scores <span class="token operator">=</span> scores<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>block_mask <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1e4</span><span class="token punctuation">)</span>
        p_attn <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>scores<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [b, n_h, t_t, t_s]</span>
        p_attn <span class="token operator">=</span> self<span class="token punctuation">.</span>drop<span class="token punctuation">(</span>p_attn<span class="token punctuation">)</span>
        output <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>p_attn<span class="token punctuation">,</span> value<span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>window_size <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            relative_weights <span class="token operator">=</span> self<span class="token punctuation">.</span>_absolute_position_to_relative_position<span class="token punctuation">(</span>p_attn<span class="token punctuation">)</span>
            value_relative_embeddings <span class="token operator">=</span> self<span class="token punctuation">.</span>_get_relative_embeddings<span class="token punctuation">(</span>self<span class="token punctuation">.</span>emb_rel_v<span class="token punctuation">,</span> t_s<span class="token punctuation">)</span>
            output <span class="token operator">=</span> output <span class="token operator">+</span> self<span class="token punctuation">.</span>_matmul_with_relative_values<span class="token punctuation">(</span>relative_weights<span class="token punctuation">,</span> value_relative_embeddings<span class="token punctuation">)</span>
        output <span class="token operator">=</span> output<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>b<span class="token punctuation">,</span> d<span class="token punctuation">,</span> t_t<span class="token punctuation">)</span>  <span class="token comment"># [b, n_h, t_t, d_k] -&gt; [b, d, t_t]</span>
        <span class="token keyword">return</span> output<span class="token punctuation">,</span> p_attn

    <span class="token keyword">def</span> <span class="token function">_matmul_with_relative_values</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        x: [b, h, l, m]
        y: [h or 1, m, d]
        ret: [b, h, l, d]
        """</span>
        ret <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> ret

    <span class="token keyword">def</span> <span class="token function">_matmul_with_relative_keys</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        x: [b, h, l, d]
        y: [h or 1, m, d]
        ret: [b, h, l, m]
        """</span>
        ret <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> ret

    <span class="token keyword">def</span> <span class="token function">_get_relative_embeddings</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> relative_embeddings<span class="token punctuation">,</span> length<span class="token punctuation">)</span><span class="token punctuation">:</span>
        max_relative_position <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>window_size <span class="token operator">+</span> <span class="token number">1</span>
        <span class="token comment"># Pad first before slice to avoid using cond ops.</span>
        pad_length <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span>length <span class="token operator">-</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>window_size <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
        slice_start_position <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>window_size <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">-</span> length<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
        slice_end_position <span class="token operator">=</span> slice_start_position <span class="token operator">+</span> <span class="token number">2</span> <span class="token operator">*</span> length <span class="token operator">-</span> <span class="token number">1</span>
        <span class="token keyword">if</span> pad_length <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
            padded_relative_embeddings <span class="token operator">=</span> F<span class="token punctuation">.</span>pad<span class="token punctuation">(</span>
                relative_embeddings<span class="token punctuation">,</span>
                commons<span class="token punctuation">.</span>convert_pad_shape<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>pad_length<span class="token punctuation">,</span> pad_length<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            padded_relative_embeddings <span class="token operator">=</span> relative_embeddings
        used_relative_embeddings <span class="token operator">=</span> padded_relative_embeddings<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> slice_start_position<span class="token punctuation">:</span>slice_end_position<span class="token punctuation">]</span>
        <span class="token keyword">return</span> used_relative_embeddings

    <span class="token keyword">def</span> <span class="token function">_relative_position_to_absolute_position</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        x: [b, h, l, 2*l-1]
        ret: [b, h, l, l]
        """</span>
        batch<span class="token punctuation">,</span> heads<span class="token punctuation">,</span> length<span class="token punctuation">,</span> _ <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># Concat columns of pad to shift from relative to absolute indexing.</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>pad<span class="token punctuation">(</span>x<span class="token punctuation">,</span> commons<span class="token punctuation">.</span>convert_pad_shape<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token comment"># Concat extra elements so to add up to shape (len+1, 2*len-1).</span>
        x_flat <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token punctuation">[</span>batch<span class="token punctuation">,</span> heads<span class="token punctuation">,</span> length <span class="token operator">*</span> <span class="token number">2</span> <span class="token operator">*</span> length<span class="token punctuation">]</span><span class="token punctuation">)</span>
        x_flat <span class="token operator">=</span> F<span class="token punctuation">.</span>pad<span class="token punctuation">(</span>x_flat<span class="token punctuation">,</span> commons<span class="token punctuation">.</span>convert_pad_shape<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> length <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token comment"># Reshape and slice out the padded elements.</span>
        x_final <span class="token operator">=</span> x_flat<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token punctuation">[</span>batch<span class="token punctuation">,</span> heads<span class="token punctuation">,</span> length <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span> <span class="token operator">*</span> length <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>length<span class="token punctuation">,</span> length <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
        <span class="token keyword">return</span> x_final

    <span class="token keyword">def</span> <span class="token function">_absolute_position_to_relative_position</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        x: [b, h, l, l]
        ret: [b, h, l, 2*l-1]
        """</span>
        batch<span class="token punctuation">,</span> heads<span class="token punctuation">,</span> length<span class="token punctuation">,</span> _ <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># padd along column</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>pad<span class="token punctuation">(</span>x<span class="token punctuation">,</span> commons<span class="token punctuation">.</span>convert_pad_shape<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> length <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        x_flat <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token punctuation">[</span>batch<span class="token punctuation">,</span> heads<span class="token punctuation">,</span> length <span class="token operator">**</span> <span class="token number">2</span> <span class="token operator">+</span> length <span class="token operator">*</span> <span class="token punctuation">(</span>length <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token comment"># add 0's in the beginning that will skew the elements after reshape</span>
        x_flat <span class="token operator">=</span> F<span class="token punctuation">.</span>pad<span class="token punctuation">(</span>x_flat<span class="token punctuation">,</span> commons<span class="token punctuation">.</span>convert_pad_shape<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>length<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        x_final <span class="token operator">=</span> x_flat<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token punctuation">[</span>batch<span class="token punctuation">,</span> heads<span class="token punctuation">,</span> length<span class="token punctuation">,</span> <span class="token number">2</span> <span class="token operator">*</span> length<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
        <span class="token keyword">return</span> x_final

    <span class="token keyword">def</span> <span class="token function">_attention_bias_proximal</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> length<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Bias for self-attention to encourage attention to close positions.
        Args:
          length: an integer scalar.
        Returns:
          a Tensor with shape [1, 1, length, length]
        """</span>
        r <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>length<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
        diff <span class="token operator">=</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>r<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">-</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>r<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span>torch<span class="token punctuation">.</span>log1p<span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span>diff<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">FFN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> filter_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> p_dropout<span class="token operator">=</span><span class="token number">0.</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                 causal<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>in_channels <span class="token operator">=</span> in_channels
        self<span class="token punctuation">.</span>out_channels <span class="token operator">=</span> out_channels
        self<span class="token punctuation">.</span>filter_channels <span class="token operator">=</span> filter_channels
        self<span class="token punctuation">.</span>kernel_size <span class="token operator">=</span> kernel_size
        self<span class="token punctuation">.</span>p_dropout <span class="token operator">=</span> p_dropout
        self<span class="token punctuation">.</span>activation <span class="token operator">=</span> activation
        self<span class="token punctuation">.</span>causal <span class="token operator">=</span> causal

        <span class="token keyword">if</span> causal<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>padding <span class="token operator">=</span> self<span class="token punctuation">.</span>_causal_padding
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>padding <span class="token operator">=</span> self<span class="token punctuation">.</span>_same_padding

        self<span class="token punctuation">.</span>conv_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> filter_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>filter_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>drop <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p_dropout<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv_1<span class="token punctuation">(</span>self<span class="token punctuation">.</span>padding<span class="token punctuation">(</span>x <span class="token operator">*</span> x_mask<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>activation <span class="token operator">==</span> <span class="token string">"gelu"</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> x <span class="token operator">*</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token number">1.702</span> <span class="token operator">*</span> x<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> torch<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>drop<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv_2<span class="token punctuation">(</span>self<span class="token punctuation">.</span>padding<span class="token punctuation">(</span>x <span class="token operator">*</span> x_mask<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x <span class="token operator">*</span> x_mask

    <span class="token keyword">def</span> <span class="token function">_causal_padding</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>kernel_size <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> x
        pad_l <span class="token operator">=</span> self<span class="token punctuation">.</span>kernel_size <span class="token operator">-</span> <span class="token number">1</span>
        pad_r <span class="token operator">=</span> <span class="token number">0</span>
        padding <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>pad_l<span class="token punctuation">,</span> pad_r<span class="token punctuation">]</span><span class="token punctuation">]</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>pad<span class="token punctuation">(</span>x<span class="token punctuation">,</span> commons<span class="token punctuation">.</span>convert_pad_shape<span class="token punctuation">(</span>padding<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

    <span class="token keyword">def</span> <span class="token function">_same_padding</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>kernel_size <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> x
        pad_l <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>kernel_size <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span>
        pad_r <span class="token operator">=</span> self<span class="token punctuation">.</span>kernel_size <span class="token operator">//</span> <span class="token number">2</span>
        padding <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>pad_l<span class="token punctuation">,</span> pad_r<span class="token punctuation">]</span><span class="token punctuation">]</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>pad<span class="token punctuation">(</span>x<span class="token punctuation">,</span> commons<span class="token punctuation">.</span>convert_pad_shape<span class="token punctuation">(</span>padding<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

</code></pre> 
<p>本笔记主要记录vits官方仓库中模型构建相关代码。因为主体架构涉及VAE和Flow的原因，各种分布和KL散度的计算等都涉及很多公式推导，但代码中是直接计算的；作者在注释时对公式部分的代码与论文公式部分进行了关联，但肯定还是存在原理解释不清晰的问题，后续如果条件成熟，会对具体的原理推导进行记录。本笔记主要是对代码进行详细的注释，读者若发现问题或错误，请评论指出，互相学习。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/3642e775d142dd01f631b45fd1813724/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">哥德巴赫猜想（C语言）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a4b9056511e1d8f64f533a44b9df6441/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">机器学习笔记：第2章 模型评估与选择</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>