<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【EAI 005】EmbodiedGPT：通过具身思维链进行视觉语言预训练的具身智能大模型 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【EAI 005】EmbodiedGPT：通过具身思维链进行视觉语言预训练的具身智能大模型" />
<meta property="og:description" content="论文描述：EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought
论文作者：Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, Ping Luo
作者单位：The University of Hong Kong, Shanghai AI Laboratory, Noah’s Ark Laboratory
论文原文：https://arxiv.org/abs/2305.15021
论文出处：NeurIPS 2023 spotlight
论文被引：42（01/05/2024）
项目主页：https://embodiedgpt.github.io/
论文代码：https://github.com/EmbodiedGPT/EmbodiedGPT_Pytorch
Abstract 具身人工智能（Embodied AI，EAI）是机器人技术的一个重要前沿领域，它能够为机器人规划和执行行动序列，以完成物理环境中的长周期（long-horizon）任务。本文提出了 EmbodiedGPT，它是一种端到端多模态具身人工智能基础模型，赋予具身Agent多模态理解和执行能力。为此：
i）我们制作了一个大规模的具身规划数据集，称为 EgoCOT。该数据集包括从 Ego4D 数据集中精心挑选的视频以及相应的高质量语言指令。具体来说，我们利用思维链（Chain of Thoughts，CoT）模式生成一系列子目标，以实现有效的具身规划。ii）我们为 EmbodiedGPT 引入了一种高效的训练方法，通过前缀微调（Prefix Tuning）使 7B 大语言模型（LLM）适应 EgoCOT 数据集，从而生成高质量的规划。iii）我们引入了一种从 LLM 生成的规划查询中提取任务相关特征的范式，从而在高层次（High-Level）规划和低层次（Low-Level）控制之间形成闭环。 广泛的实验表明，EmbodiedGPT 在具身任务上非常有效，包括具身规划（embodied planning），具身控制（embodied control），视觉描述（visual captioning）和视觉问答（visual question answering）。EmbodiedGPT 通过提取更有效的特征，显著提高了具身控制任务的成功率。与使用 Ego4D 数据集进行微调的 BLIP-2 基准相比，EmbodiedGPT 在 Franka Kitchen 基准上的成功率提高了 1." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/be24cf056deeafe3c609c39148b1456d/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-08T00:25:57+08:00" />
<meta property="article:modified_time" content="2024-01-08T00:25:57+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【EAI 005】EmbodiedGPT：通过具身思维链进行视觉语言预训练的具身智能大模型</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <blockquote> 
 <p>论文描述：EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought<br> 论文作者：Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, Ping Luo<br> 作者单位：The University of Hong Kong, Shanghai AI Laboratory, Noah’s Ark Laboratory<br> 论文原文：https://arxiv.org/abs/2305.15021<br> 论文出处：NeurIPS 2023 spotlight<br> 论文被引：42（01/05/2024）<br> 项目主页：https://embodiedgpt.github.io/<br> 论文代码：https://github.com/EmbodiedGPT/EmbodiedGPT_Pytorch</p> 
</blockquote> 
<h2><a id="Abstract_9"></a>Abstract</h2> 
<p>具身人工智能（Embodied AI，EAI）是机器人技术的一个重要前沿领域，它能够为机器人规划和执行行动序列，以完成物理环境中的长周期（long-horizon）任务。本文提出了 EmbodiedGPT，它是一种端到端多模态具身人工智能基础模型，赋予具身Agent多模态理解和执行能力。为此：</p> 
<ul><li>i）我们制作了一个大规模的具身规划数据集，称为 EgoCOT。该数据集包括从 Ego4D 数据集中精心挑选的视频以及相应的高质量语言指令。具体来说，我们利用思维链（Chain of Thoughts，CoT）模式生成一系列子目标，以实现有效的具身规划。</li><li>ii）我们为 EmbodiedGPT 引入了一种高效的训练方法，通过前缀微调（Prefix Tuning）使 7B 大语言模型（LLM）适应 EgoCOT 数据集，从而生成高质量的规划。</li><li>iii）我们引入了一种从 LLM 生成的规划查询中提取任务相关特征的范式，从而在高层次（High-Level）规划和低层次（Low-Level）控制之间形成闭环。</li></ul> 
<p>广泛的实验表明，<strong>EmbodiedGPT 在具身任务上非常有效，包括具身规划（embodied planning），具身控制（embodied control），视觉描述（visual captioning）和视觉问答（visual question answering）</strong>。EmbodiedGPT 通过提取更有效的特征，显著提高了具身控制任务的成功率。与使用 Ego4D 数据集进行微调的 BLIP-2 基准相比，EmbodiedGPT 在 Franka Kitchen 基准上的成功率提高了 1.6 倍，在 Meta-World 基准上的成功率提高了 1.3 倍。</p> 
<h2><a id="1_Introduction_19"></a>1 Introduction</h2> 
<p><strong>具身人工智能任务，如具身规划，具身 VQA 和具身控制，旨在赋予机器人在其环境中感知，推理和行动的能力，使其能够根据实时观察结果执行长周期（long-horizon）规划和自主行动（actions）</strong>。最近，GPT4 [1] 和 PaLM-E [2] 等大型语言模型（LLM）显示出了良好的语言理解，推理和思维链能力。这些进步为开发能够处理自然语言指令，执行多模态思维链，并在物理环境中规划行动的机器人提供了新的可能性。</p> 
<p>大规模数据集在训练大型语言模型方面发挥着重要作用。例如，OpenCLIP 在 LAION-2B 数据集[3]上训练其 ViT-G/14 模型，该数据集包含 2B 个图像语言对。通用视觉语言任务可以从互联网上获取大量弱标签图像对，而<strong>具身人工智能任务则不同，它需要机器人领域的第一视角（Egocentric）数据。此外，精确规划需要结构化的语言指令，而这通常需要大量的人工努力和成本</strong>。这给收集高质量的多模态具身数据带来了挑战。一些研究人员[4, 5, 6, 7]探索利用模拟器创建大规模的具身数据集，但模拟与真实世界之间仍存在巨大差距。最近的研究[8, 9, 10]也在探索通过高效的微调策略（如 LoRA[11]）将预先训练好的 LLMs 适应到新的领域。然而，仍有几个问题有待解决：</p> 
<ul><li>如何将 LLMs 应用于可能面临巨大领域差距的机器人领域</li><li>如何利用思维链能力进行结构化规划</li><li>如何以端到端的方式将输出语言规划用于下游操纵任务</li></ul> 
<p><strong>为了解决上述难题，我们在这项工作中首先建立了一个大规模的具身规划数据集，称为 EgoCOT，其特征是思维链规划指令</strong>。它包含从 Ego4D 数据集[16]中精心挑选的第一视角的视频和相应的高质量分步语言指令，这些指令由机器生成，然后经过基于语义的过滤，最后由人工验证。此外，我们还创建了 EgoVQA 数据集，作为 Ego4D 数据集的扩展，重点关注第一视角的人-物交互视频问答任务，旨在提供更广泛的第一视角的多模态数据。</p> 
<p><img src="https://images2.imgbox.com/72/f7/SHFcholq_o.png" alt="在这里插入图片描述"></p> 
<p><strong>在 EgoCOT 和 EgoVQA 的基础上，我们提出了一种端到端的多模态具身基础模型，称为 EmbodiedGPT，它能以更自然，更直观的方式与物理世界进行交互，并执行许多具身任务，如图 1 所示，例如具身规划，具身 VQA 和具身控制</strong>。EmbodiedGPT 包含四个共同工作的集成模块，其中包括：</p> 
<ul><li>i) 一个用于编码当前观察视觉特征的冻结视觉模型；</li><li>ii) 一个用于执行自然语言的冻结语言模型，以完成问答，图像描述和具身规划任务；</li><li>iii) 一个带有语言映射层的 embodied-former，用于对齐视觉和体现指令，并利用生成的规划提取与任务相关的实例级特征，以便进行底层控制；</li><li>iv）策略网络（policy network），负责根据任务相关特征生成底层动作，使 Agent 能够有效地与环境互动。</li></ul> 
<p>为了进一步提高 EmbodiedGPT 在生成包含子目标序列的可靠规划方面的性能，我们对冻结语言模型进行了前缀微调（Prefix Tuning），以鼓励生成更多可执行规划。</p> 
<p>我们的方法具有以下核心优势：</p> 
<ul><li>i) 生成的规划在物体部件级别（如机械臂的抓手或门的把手），表现出很强的可执行性和粒度性，并体现在子目标序列中；</li><li>ii) EgoCOT 数据集基于开源的大规模数据集构建，与在专有机器人数据上训练的 PaLM-E [2] 模型相比，具有更强的可扩展性。EgoCOT数据集和EmbodiedGPT模型都将开源。</li><li>iii) <strong>EmbodiedGPT 形成了从高层次规划到低层次控制的闭环，实现了高层次规划和低层次控制的无缝集成，提供了高效的任务性能和对各种任务的适应性</strong>。</li></ul> 
<p>为了实现这一目标，我们通过视觉观察和生成的具身规划之间的交叉关注，利用 embodied-former 来查询与任务相关的实例级特征。这使得策略网络只需不到 25 次演示（demonstrations）就能完成低层次（Low-Level）控制任务。</p> 
<p>我们的贡献可归纳如下：</p> 
<ul><li>i) 我们为具身人工智能建立了端到端的多模态基础模型 EmbodiedGPT，该模型具有 "思维链 "能力，使具身Agent能够以更自然，更直观的方式与物理世界交互。</li><li>ii) 我们开发了两个数据集 EgoCOT 和 EgoVQA，其中包括来自 Ego4D 数据集的 2 亿个带注释的视频以及相应的详细规划指令和 VQA 数据。这些数据集首先由机器生成，然后基于语义进行过滤，最后由人工验证以进行质量控制。</li><li>iii) 我们介绍了 EmbodiedGPT，这是一种经济高效的训练方法，也是从 LLM 生成的规划查询中提取任务相关特征的范例，从而在高层次规划和低层次控制之间形成闭环。</li></ul> 
<p>我们通过在多个具身任务（包括具身控制，具身规划，视频描述和视频问答）上取得最先进或相当的性能，证明了我们的方法的有效性。值得注意的是，与在Ego4D数据集上进行微调的BLIP-2[17]和专为操作任务设计的R3M[12]相比，EmbodiedGPT在Franka Kitchen[14]基准测试中的表现优于这两个模型，优势分别为22.1%和5.5%。同样，在 Meta-World [14] 基准上，EmbodiedGPT 分别以 22.5% 和 4.2% 的优势超过了这两个模型。</p> 
<h2><a id="2_Related_Work_59"></a>2 Related Work</h2> 
<h3><a id="21_Vision_Language_Pretraining_with_large_scale_foundation_model_61"></a>2.1 Vision Language Pre-training with large scale foundation model</h3> 
<p>视觉语言预训练的重点是加强视觉观察与自然语言之间的联系。其目标是开发能够更好地理解和处理视觉内容的模型，例如识别物体和动作，以及生成描述性文本。随着模型变得越来越大，端到端预训练的计算费用也随之增加，因此需要模块化的视觉语言预训练方法。这些方法巧妙地使用预训练模型，在视觉语言预训练期间将其冻结，以节省计算成本。例如，Uniter [18]，Oscar [19]，VinVL [20] 和 LiT [21]等模型冻结了图像编码器，而 Frozen [22] 和 VGPT [23] 则冻结了语言模型。此外，Flamingo [24] 和 BLIP-2 [17] 同时使用冻结图像编码器和语言模型，在性能和计算效率之间取得了平衡。<strong>由于缺乏多模态具身规划的开源数据，以前的工作难以进行详细的任务分解，也缺乏生成精确和可执行规划的能力</strong>。为了解决这个问题，我们创建了 EgoCOT 数据集，并<strong>开发了一个具身思维链视觉语言预训练框架，以提高多模态模型在具身推理和规划方面的能力</strong>。</p> 
<h3><a id="22_Egocentric_Video_Datasets_65"></a>2.2 Egocentric Video Datasets.</h3> 
<p>第一视角的视频（Egocentric videos）是利用可穿戴式摄像头拍摄的，它提供了日常活动的自然视角，并提出了一些具有挑战性的研究问题 [25, 26, 27]。多年来，人们创建了多个第一视角的视频数据集，包括 [28, 29, 30]。然而，第一视角的视频收集成本高昂，而且以前的数据集往往规模较小，只针对特定领域。最近，<strong>一个大规模的第一视角视频数据集 Ego4D [16] 已经发布，并被用于具身表征学习。该数据集由来自 9 个国家 74 个地点的 931 人收集的 3,670 小时视频组成，视频配有旁白</strong>。对于具身人工智能任务而言，从大量，多样的第一视角的人类视频中学习，已成为一种很有前途的方法，可以获得用于控制此类任务的普遍有用的视觉表征。例如，</p> 
<ul><li>R3M[12]通过结合时间对比学习和视频语言对齐，利用 Ego4D 人类视频数据集开发了一种稀疏而紧凑的视觉表征。</li><li>VIP [31]利用 Ego4D 数据集学习目标条件机器人操纵的通用奖励函数。</li></ul> 
<h3><a id="23_Large_Foundation_Model_Assistant_System_72"></a>2.3 Large Foundation Model Assistant System</h3> 
<p>大规模多模态语言模型（LLMs）的最新进展，如 GPT-3 [32] 和 GPT-4 [1]，产生了各种能够理解多种信息模式的模型。该领域主要采用两种方法：</p> 
<ul><li>系统协作（systematic collaboration）：<strong>系统协作方法涉及协调多个视觉模型或工具与语言模型，将视觉信息与文本描述结合起来</strong>。例如 Visual ChatGPT [33]，MM-REACT [34] 和 HuggingGPT [35]。然而，这种方法<strong>受限于固定模块化模型的准确性和容量，可能导致错误累积</strong>。</li><li>端到端训练模型：旨在为多模态任务提供统一的模型。例如， 
  <ul><li>Flamingo [24] 通过冻结预训练的视觉编码器和语言模型，将视觉和语言结合起来。</li><li>BLIP-2 [13]引入了 Q-Former，将来自冻结视觉编码器的视觉特征与大型语言模型相统一。</li><li>MiniGPT-4 [36] 和 LLaVA [37] 等模型将经过指令微调的语言模型与来自冻结视觉骨干的视觉特征相对齐。</li><li>VideoChat[38]，mPLUG-Owl[39]和 X-LLM[40]进一步扩大了对视频输入的支持。</li><li>PaLM-E [41] 是第一个大型具身多模态模型，它直接结合了传感器模态的特征以提高真实世界的性能，并利用其大规模日常机器人数据进行了训练[42]。</li></ul> </li></ul> 
<p>与 PaLM-E 相比，EmbodiedGPT 更为紧凑，大小仅为 10B，并可额外支持视频描述，视频问答和根据演示视频进行规划。此外，我们还形成了一个从高层次（High-Level）规划到低层次（Low-Level）控制的闭环系统。</p> 
<h2><a id="3_Method_86"></a>3 Method</h2> 
<p><img src="https://images2.imgbox.com/36/ab/OfemV7Fu_o.png" alt="在这里插入图片描述"></p> 
<p><strong>具身基础模型（embodied foundation model）的目标是通过准确感知环境，识别相关物体，分析其空间关系以及制定详细的任务规划，模仿人类的感知能力以及与环境的交互能力</strong>。为实现这一目标，EmbodiedGPT 采用了预先训练好的 vision transformer 作为视觉编码器，并采用预先训练好的 LLaMA [43] 模型作为语言模型。如图 2 所示，</p> 
<ul><li>Embodied-former 是视觉域和语言域之间的桥梁，它首先通过涉及视觉标记，文本查询和可学习的具身查询的基于注意力的交互，从视觉模型的输出中提取紧凑的视觉特征，然后通过语言映射层将其映射到语言模态。这些嵌入会被发送到冻结的 LLaMA [43] 语言模型，用于视觉描述，视觉问答和具身规划。</li><li>然后，生成的规划将用于从由视觉模型通过 embodied-former 的一般视觉标记（tokens）中查询高度相关的特征。这些特征可用于生成低层次（Low-Level）控制指令，通过下游策略网络执行任务。</li><li>为了提高一系列具身任务的性能，我们引入了一种新颖的视频语言预训练范式，利用认知思维链从第一视角的视频输入中生成具身规划。我们将这一任务表述为标准的视觉问答（VQA）任务，以 “how to do the task that + original caption” 作为问题，以具身规划作为答案。这一框架丰富了具身规划和标准视觉问答任务的数据，鼓励 embodied-former 捕捉更适合具身控制任务的特定任务特征。</li></ul> 
<h3><a id="31_Framework_97"></a>3.1 Framework</h3> 
<p><strong>训练过程包括三个阶段，每个阶段都旨在逐步提升推理和规划能力。前两个阶段的重点是对基本认知和反应技能进行预训练，而第三阶段则是利用 EgoCOT 上第一视角的视频-文本数据训练具身AI任务</strong>。</p> 
<ul><li> <p>第一阶段重点放在<strong>图像-文本对话对齐的预训练</strong>上，包括使用三个数据集：COCO Caption [44]，来自 CC3M 的 59.5 万个经过精细过滤的图像-文本对[45]，以及使用 BLIP-2 对 LAION-400M 进行 re-captioning 处理后获得的 49.1 万个经过过滤的图像-文本对[17]。这一阶段的主要目标是<strong>在保持视觉和语言模型参数冻结以节省计算资源的同时，预先训练 Embodied-former 和 language projection</strong>。</p> </li><li> <p>第二阶段的目标是<strong>增强模型理解和生成更复杂句子的能力，并提高其推理能力</strong>。为此，我们更新了language projection和prefix language adapter，并利用 LLaVA_Instruct_150K [46] 提供的 Complex_Reasoning_77k 和多回合对话数据集。</p> </li><li> <p>利用 EgoCOT 进行具身思维链训练：第三阶段首先使用 Conv3D [47] 将第二阶段预训练的视觉模型迁移到视频编码器，视频的时间偏移为 2，总帧数为 8。然后，我们引入思维链视觉语言预训练范式，即模型将视频的 8 个关键帧作为输入，同时输入任务描述，具身规划和结构化动名词对摘要（verb-noun pairs summary），以便根据提示（如清单 1）进行推理。为避免过拟合，我们提供了一个具有相同含义的不同指令的提示集（prompt set）。<strong>在这一阶段对 patch embedding，language projection layer，prefix language adapter 进行微调，以更好地捕捉时间信息</strong>。</p> </li></ul> 
<h3><a id="32_Model_Architecture_106"></a>3.2 Model Architecture</h3> 
<p>Embodied-former 用 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         E 
        
       
         ( 
        
       
         ⋅ 
        
       
         ) 
        
       
      
        \mathcal{E}(\cdot) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathcal" style="margin-right: 0.0894em;">E</span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span></span></span></span></span> 表示，它是视觉输入 xvis 和冻结语言模型之间的桥梁，是向语言模型提供最相关视觉数据的信息瓶颈（information bottleneck）。Embodied-former由两个子模块组成：</p> 
<ul><li>一个用于从图像输入中提取特征，记为 Evis : xvis → yvis</li><li>一个用于从文本输入中提取特征，记为 Etxt : xtxt → ytxt</li></ul> 
<p>我们使用 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         N 
        
       
      
        N 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.109em;">N</span></span></span></span></span> 个可学习的具身查询嵌入 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          y 
         
         
         
           q 
          
         
           u 
          
         
           e 
          
         
           r 
          
         
           y 
          
         
        
       
      
        y_{query} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7167em; vertical-align: -0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">q</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">ery</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span></span></span></span></span> 作为 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         E 
        
       
      
        \mathcal{E} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathcal" style="margin-right: 0.0894em;">E</span></span></span></span></span> 的输入，通过交叉注意层与 xvis 交互，通过自注意层与 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          x 
         
         
         
           t 
          
         
           x 
          
         
           t 
          
         
        
       
      
        x_{txt} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5806em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">x</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span> 交互。我们将输出查询表示记为 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         z 
        
       
         ∈ 
        
        
        
          R 
         
         
         
           N 
          
         
           × 
          
         
           D 
          
         
        
       
      
        z∈\mathbb{R}^{N ×D} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5782em; vertical-align: -0.0391em;"></span><span class="mord mathnormal" style="margin-right: 0.044em;">z</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.8413em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.109em;">N</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right: 0.0278em;">D</span></span></span></span></span></span></span></span></span></span></span></span></span> ，其中 D 是 embeddings 的维度。z 的维度明显小于视觉特征的维度。然后将输出查询嵌入转化为 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          z 
         
        
          ′ 
         
        
       
         ∈ 
        
        
        
          R 
         
         
         
           N 
          
         
           × 
          
          
          
            D 
           
          
            ′ 
           
          
         
        
       
      
        z^′∈ \mathbb{R}^{N ×D^′} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.791em; vertical-align: -0.0391em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.044em;">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7519em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.9425em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9425em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.109em;">N</span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0278em;">D</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8278em;"><span class="" style="top: -2.931em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>，其维度 D′与语言模态中 LLM 的文本嵌入维度相同。这种转换由一个映射函数来完成，该函数用 M 表示：z → z′，通过一个全连接（FC）层的线性投影来完成。投影嵌入的 z′ 可作为语言模型的软视觉提示，将整个交互解耦为视觉-查询交互和查询-文本交互。语言模型以 z′ 和文本提示（如 Listing 1 所示）为输入，推断出最终的具身规划。对于旨在产生与环境互动的行动的底层控制，具身规划 xplan 被用作 Embodied-former 的输入文本，以查询与任务相关的实例级特征 zinstance = E(xvis, xplan, yquery)。随后，Agent能够生成控制指令，如伺服器的转角，表示为 a = g(zinstance, zglobal)。该函数结合了特定实例信息 zinstance 和全局上下文信息 zglobal。全局上下文使用 ResNet50 模型[48]进行推断，该模型已在 ImageNet[49]上进行了预训练，并采用了全局平均池化技术。<strong>这里，g(-) 表示策略网络，它是一个多层感知器 (MLP) [50] 映射函数。策略网络的输出由具体的可执行动作组成，如笛卡尔坐标系中的位置和速度</strong>。更多实施细节见附录 A。</p> 
<p><img src="https://images2.imgbox.com/77/2e/NwriDYAX_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="33_Training_Settings_118"></a>3.3 Training Settings</h3> 
<p>我们采用了与 BLIP-2[17] 相同的预训练图像编码器。具体来说，我们使用 EVA-CLIP 中的 ViT-G/14 模型[51]，并移除其最后一层，使用倒数第二层的输出特性。</p> 
<p>对于冻结语言模型，我们采用了预先训练好的 LLaMA-7B 模型[43]，并使用 ShareGPT 数据集和 GPT-4 生成的 52K 英语指令遵循数据集[52]对其进行了微调。然后，我们将经过微调的语言模型作为视觉语言预训练的冻结语言模型。</p> 
<p>此外，在预训练过程中，我们还将冻结的 ViT [53] 和语言模型的参数数据类型转换为 FP16，以提高效率。</p> 
<h3><a id="34_Creating_EgoCOT_and_EgoVQA_Dataset_126"></a>3.4 Creating EgoCOT and EgoVQA Dataset</h3> 
<p>对于我们的 EgoCOT 数据集，我们从 Ego4D 数据集[16]中获取了基本数据，其中包括 9,645 个未经修剪的视频，视频时长从 5 秒到 7 小时不等。为了准备好这些数据，我们进行了两个阶段的数据清理。</p> 
<p>在第一阶段，我们过滤掉了缺少旁白或旁白很短的视频（分别占文本的 7.4% 和 0.9%），以及标签不确定的视频（占文本的 4.0%）。我们还排除了没有人与物体互动的视频，例如看电视或走路的视频。经过这一阶段后，我们得到了 290 万小时的视频，其中包含 385 万段叙述，这些叙述来自 129 个不同的场景，涵盖 2927 小时的视频。</p> 
<p>为了生成成对的描述（captions），具身规划和相应的视频片段，我们使用了 EgoVLP 框架[54]来分割视频。旁白是由一系列句子 T0, … , Tn 组成的，并带有精确的时间戳 t0, … , tn，表示所描述事件发生的时间。对于每个带有时间戳 ti 的旁白 Ti，我们通过确定其开始和结束时间点，将其与片段 Vi 配对：</p> 
<p><img src="https://images2.imgbox.com/78/25/N3tKmcun_o.png" alt="在这里插入图片描述"></p> 
<p>其中，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          β 
         
        
          i 
         
        
       
         = 
        
        
        
          ∑ 
         
         
         
           j 
          
         
           = 
          
         
           0 
          
         
         
         
           n 
          
         
           − 
          
         
           1 
          
         
        
       
         ( 
        
        
        
          t 
         
         
         
           j 
          
         
           + 
          
         
           1 
          
         
        
       
         − 
        
        
        
          t 
         
        
          j 
         
        
       
         ) 
        
       
         / 
        
       
         n 
        
       
      
        β_i = \sum^{n-1}_{j=0} (t_{j+1} - t_j ) / n 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0528em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0528em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.3898em; vertical-align: -0.4358em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position: relative; top: 0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.954em;"><span class="" style="top: -2.4003em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">0</span></span></span></span><span class="" style="top: -3.2029em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.4358em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em;">j</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1.0361em; vertical-align: -0.2861em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">/</span><span class="mord mathnormal">n</span></span></span></span></span> 是一个可调参数，等于给定视频中连续叙述之间的平均时间距离。相反，α 是一个比例因子，是根据 EgoCOT 数据集中所有视频中所有 βi 的平均值计算得出的（α = 4.9 秒）。对于每个视频片段，我们都会为 ChatGPT [55] 提供提示和相应的描述，以生成合理而详细的具身规划。描述通常是简短的介绍，如 “C opens a drawer.”。我们使用 ChatGPT 根据描述生成思维链，并将其整理成动名词对列表，如：</p> 
<pre><code class="prism language-python">plans<span class="token punctuation">:</span> grasp the handle <span class="token keyword">with</span> the gripper <span class="token keyword">and</span> pull the handle<span class="token punctuation">;</span> actions<span class="token punctuation">:</span> <span class="token number">1.</span> grasp<span class="token punctuation">(</span>handle<span class="token punctuation">,</span> gripper<span class="token punctuation">)</span> <span class="token number">2.</span> pull<span class="token punctuation">(</span>handle<span class="token punctuation">)</span><span class="token punctuation">.</span>
</code></pre> 
<p>我们用来生成 EgoCOT 数据集的提示如 Listing 2 所示。为了提高生成的思维链的多样性，我们采用了 0.9 的 temperature 参数和 0.95 的 top-p 参数。对于每个提示，我们都进行了五次采样迭代。</p> 
<p><img src="https://images2.imgbox.com/8d/64/tIKz13S2_o.png" alt="在这里插入图片描述"></p> 
<p>Post-procedure.</p> 
<p>为确保生成的规划指令（planning instructions）的质量，我们进行了第二阶段的数据清洗。</p> 
<ul><li>我们使用 CLIP 模型[56]来评估视频和文本对之间的相似性。对于每段视频，我们将其与五个潜在的具身规划进行比较，并选择相似度最高的一个作为具身规划的相应标签。</li><li>然后，我们进一步进行数据清理，过滤掉相似度低于阈值的视频-描述-规划对（video-caption-planning pairs）。我们剔除了视频和描述之间以及视频和规划之间相似度较低的数据，以确保为 EgoCOT 数据集提供最高质量的数据。</li></ul> 
<p>对于视频片段的每个关键帧，我们使用 CLIP 模型将文本数据 T 和图像数据 I 都编码到一个共享的嵌入空间中。相似度使用余弦相似度函数计算，即 S(yT, yI ) = (yT · yI) / (∥yT∥yI∥)，其中 S(yT, yI ) 表示文本和图像之间的相似度，yT 和 yI 是各自的嵌入空间。鉴于每个视频都包含多个关键帧，因此每个视频的相似性得分都是一个集合。这种集合策略有助于缓解单个帧之间的差异问题，并确保对整体相似性的测量更稳健，更有代表性。具有 n 个关键帧的视频 V 与文本数据 T 之间的集合相似性得分由以下公式给出：</p> 
<p><img src="https://images2.imgbox.com/3e/bd/cp5KcWvk_o.png" alt="在这里插入图片描述"></p> 
<p>其中，E(V, T ) 是集合相似度得分，S(yT i, yI i) 是第 i 个关键帧的相似度得分，n 是关键帧的总数。我们还专门为第一视角的人-物互动视频问答任务创建了 EgoVQA 数据集，以丰富训练数据。对于 Ego4D 数据集中的每个描述，我们使用 ChatGPT 生成五个 QA 对。<strong>为确保相关性，我们通过设计 Listing 3 所示的提示来引导 ChatGPT 关注核心关键动词和名词</strong>。制作 EgoVQA 时的采样模式与 EgoCOT 相同。</p> 
<p><img src="https://images2.imgbox.com/55/22/MxYFhLfc_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="4_Experiments_165"></a>4 Experiments</h2> 
<p>在本节中，我们将对多模态基础模型和 EmbodiedGPT 进行全面评估，包括视觉描述，具身规划和控制等各种任务。</p> 
<h3><a id="Evaluation_on_image_input_tasks_169"></a>Evaluation on image input tasks.</h3> 
<p>为了评估生成描述的质量和对给定图像的规划，我们对 30 名参与者进行了用户研究。该研究包括来自 MS-COCO 数据集[44]的 10 个图像描述任务案例，在不同具身人工智能模拟器中的 5 个具身规划场景，以及 5 个带有相应具身规划任务的真实世界场景。参与者被要求从五个维度对不同端到端模型生成的描述进行评分，评分标准从1到10不等，这五个维度分别是：</p> 
<ul><li>物体识别准确性</li><li>空间关系理解</li><li>答案冗余度</li><li>规划合理性</li><li>规划可执行性</li></ul> 
<p><img src="https://images2.imgbox.com/8b/de/BvC4E3NN_o.png" alt="在这里插入图片描述"></p> 
<p>表 1 显示了所有参与者对不同模型的平均得分。结果表明，尽管语言模型中只有 7B 个参数，但 <strong>EmbodiedGPT 在物体识别和空间关系理解方面达到了与 LLaVA-13B 模型相当的水平</strong>。此外，EmbodiedGPT 生成的冗余内容较少，与给定的具身人工智能任务相关，并能生成最合理和可执行的规划输出。我们还将 EmbodiedGPT 的性能与 Visual ChatGPT [33] 进行了比较，后者采用分层方法，将多个预先训练好的视觉模型和语言模型结合起来回答问题。在虚拟家庭[57]基准测试中，Visual ChatGPT 使用视觉描述模型生成密集的描述，然后将其传入 ChatGPT 以得出解决方案。如图 3 所示，Visual ChatGPT 由于仅依赖描述模型提取视觉信息的局限性，未能找到衣架，导致与 EmbodiedGPT 等端到端模型相比性能较差。<strong>这些发现凸显了采用统一的端到端模型比依赖多个阶段的分层方法更有优势</strong>。</p> 
<p><img src="https://images2.imgbox.com/63/43/fA4TC9H5_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="Evaluation_on_video_input_embodied_AI_tasks_187"></a>Evaluation on video input embodied AI tasks.</h3> 
<p>我们在标准具身人工智能基准 Franka Kitchen [14]和 Meta-World [15] 上评估了视频的识别能力和模型在具身控制任务中的规划能力。</p> 
<ul><li>Meta-World 提供了一系列需要复杂物体操作技能的高难度任务，包括在钉子上组装一个戒指，在垃圾箱之间挑选并放置一个木块，按下按钮，打开抽屉和敲击钉子。</li><li>Franka Kitchen 基准侧重于推开右门，打开橱柜，打开电灯，转动炉灶旋钮和打开微波炉等任务。如图 4 所示，给定演示视频后，EmbodiedGPT 可以准确解释具身控制任务，并提供分步规划。输出的规划被输入 EmbodiedGPT 的 Embodied-former 模块，以查询高度相关的特征，作为策略网络的输入，而底层动作则由策略网络生成，以与环境互动（更多可视化效果见附录 B）。</li></ul> 
<p><img src="https://images2.imgbox.com/f4/73/gNHSAp9S_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="Evaluation_on_embodied_control_tasks_197"></a>Evaluation on embodied control tasks.</h3> 
<p>在具身控制任务方面，我们将我们的模型与 R3M[12] 进行了比较，R3M 是这两项基准测试中最先进的方法，我们还将其与一个名为 BLIP-2[Ego4D] 的消融版本进行了比较，BLIP-2[Ego4D]与 EmbodiedGPT 具有相同的结构和参数数量，但只在视频描述任务中使用 Ego4D 数据集进行了微调，而没有加入 EgoCOT。<strong>在所有实验中，策略网络都是通过对少量演示数据进行少量学习而获得的</strong>。有两种设置，一种是使用 10 个演示数据，另一种是使用 25 个演示数据。我们报告了在 100 次随机评估中的成功率，每个基准分别在 5 个种子和 2 个不同摄像头视角下的 5 个任务中仅进行视觉观察。如图 5 和图 6 所示，EmbodiedGPT 的表现优于基线方法，证明了使用 EgoCOT 学习的有效性。</p> 
<p><img src="https://images2.imgbox.com/3d/06/jPPaauol_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/c9/8a/tfRW0878_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="Ablation_study_207"></a>Ablation study.</h3> 
<p><img src="https://images2.imgbox.com/6f/a5/oaXIH9OC_o.png" alt="在这里插入图片描述"></p> 
<p><strong>我们进行了消融研究，以分析思维链训练模式的有效性以及闭环控制设计的重要性</strong>。表 2 结果表明，使用 EgoCOT 方法与仅使用 EGO4D 描述任务进行训练相比，成功率有了显著提高。<strong>此外，闭环设计也是必要的，因为生成的规划包含具体而相关的子目标信息，而这些信息对于控制任务至关重要</strong>。</p> 
<p>总之，EmbodiedGPT 在生成合理规划，从视觉输入中准确提取任务相关特征以及执行与环境交互的低层次（Low-Level）动作方面表现出了很强的能力。消融实验表明，<strong>基于 EgoCOT 的训练范式和从具身规划到低层次（Low-Level）控制的闭环设计都极大地促进了 EmbodiedGPT 性能的提高</strong>。</p> 
<h2><a id="5_Conclusion_216"></a>5 Conclusion</h2> 
<p>在本文中，我们介绍了 EmbodiedGPT，这是一种端到端的多模态具身人工智能基础模型，可使Agent执行逐步规划和低层次（Low-Level）命令。为此，我们创建了一个名为 EgoCOT 的大规模具身规划数据集，并开发了一种高效的训练方法，利用前缀微调（Prefix Tuning）生成具有思维链的高质量规划。此外，我们的具身控制范例还能无缝协调高层次（High-Level）规划和低层次（Low-Level）控制。广泛的实验证明，EmbodiedGPT 在不同的具身任务上都很有效，达到了最先进或相当的性能。我们相信，EmbodiedGPT 代表着向开发更智能的具身人工智能Agent迈出的重要一步。</p> 
<p>Future works and limitations: 由于计算资源有限，EmbodiedGPT 冻结了视觉和语言模型的参数。所有模块的联合训练和探索其他模态（如语音）可能是未来的工作方向。目前，我们还没有预见到明显的不良伦理或社会影响。</p> 
<h2><a id="A_Implementation_details_222"></a>A Implementation details</h2> 
<h2><a id="B_More_demos_of_EmbodiedGPT_224"></a>B More demos of EmbodiedGPT</h2> 
<h3><a id="B1_Visual_Captioning_226"></a>B.1 Visual Captioning</h3> 
<h3><a id="B2_Embodied_Planning_with_image_input_228"></a>B.2 Embodied Planning with image input</h3> 
<h2><a id="C_Evaluation_metric_and_scoring_criteria_for_user_study_230"></a>C Evaluation metric and scoring criteria for user study</h2> 
<h2><a id="D_Insight_about_the_prompt_designing_for_multimodal_large_model_232"></a>D Insight about the prompt designing for multi-modal large model</h2>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/329d314dbae04c4fdd0cefa46c2c077c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【EAI 006】ChatGPT for Robotics：将 ChatGPT 应用于机器人任务的提示词工程研究</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/17efffcbfbf4869359ba128104bf4cb8/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">代码随想录day60:贪心算法｜84.柱状图中最大的矩形</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>