<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>多卡训练中的BN(BatchNorm) - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="多卡训练中的BN(BatchNorm)" />
<meta property="og:description" content="现有框架BatchNorm的实现(DP,DDP)都是只考虑了single gpu。也就是说BN使用的均值和标准差是单个gpu算的，相当于缩小了batchsize。 对于比较消耗显存的训练任务时，往往单卡上的相对批量过小，影响模型的收敛效果。 之前在在图像语义分割的实验中，就发现使用大模型的效果反而变差，实际上就是BN在作怪。 跨卡同步 Batch Normalization 可以使用全局的样本进行归一化，这样相当于‘增大‘了批量大小，这样训练效果不再受到使用 GPU 数量的影响。 最近在图像分割、物体检测的论文中，使用跨卡BN也会显著地提高实验效果，所以跨卡 BN 已然成为竞赛刷分、发论文的必备神器。
但是为什么跨卡同步BN没有成为主流？
1）因为没有sync的需求，因为对于大多数vision问题，单gpu上的mini-batch已经够大了，完全不会影响结果。
2）影响训练速度，BN layer通常是在网络结构里面广泛使用的，这样每次都同步一下GPUs，十分影响训练速度。
从使用的经验来看，如果多卡训练没开BN同步，那么所得的结果和使用单GPU训练得到的结果基本一致，但是训练速度会得到提升。
而如果开了BN同步，训练结果会有将近一个点的提升
当然如果多卡训，但是单张卡上的batchsize就很大了，这样是开BN同步就没有太大作用
开了BN之后会让训练速度变慢，大概是不开5秒，开了8秒的样子" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/541d1678ccb69e83728a4478d7f8d3b4/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-01-04T23:00:48+08:00" />
<meta property="article:modified_time" content="2023-01-04T23:00:48+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">多卡训练中的BN(BatchNorm)</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <blockquote> 
 <h4><strong>     现有框架BatchNorm的实现(DP,DDP)都是只考虑了single gpu。也就是说BN使用的均值和标准差是单个gpu算的，相当于缩小了batchsize。</strong></h4> 
 <p></p> 
 <p>      <u>对于比较消耗显存的训练任务时，往往单卡上的相对批量过小，影响模型的收敛效果。</u> 之前在在图像语义分割的实验中，就发现使用大模型的效果反而变差，实际上就是BN在作怪。 <u>跨卡同步 Batch Normalization 可以使用全局的样本进行归一化，这样相当于‘增大‘了批量大小，这样训练效果不再受到使用 GPU 数量的影响。 最近在图像分割、物体检测的论文中，使用跨卡BN也会显著地提高实验效果，所以跨卡 BN 已然成为竞赛刷分、发论文的必备神器。</u></p> 
 <p></p> 
 <p>但是为什么跨卡同步BN没有成为主流？</p> 
 <p>      1）因为没有sync的需求，因为对于大多数vision问题，单gpu上的mini-batch已经够大了，完全不会影响结果。</p> 
 <p>     2）影响训练速度，BN layer通常是在网络结构里面广泛使用的，这样每次都同步一下GPUs，十分影响训练速度。</p> 
 <p></p> 
 <p>从使用的经验来看，如果多卡训练没开BN同步，那么所得的结果和使用单GPU训练得到的结果基本一致，但是训练速度会得到提升。</p> 
 <p>而如果开了BN同步，训练结果会有将近一个点的提升</p> 
 <p>当然如果多卡训，但是单张卡上的batchsize就很大了，这样是开BN同步就没有太大作用</p> 
 <p>开了BN之后会让训练速度变慢，大概是不开5秒，开了8秒的样子</p> 
 <p></p> 
 <p></p> 
 <p><img alt="" height="286" src="https://images2.imgbox.com/d1/32/O6MwWwfO_o.png" width="331"></p> 
 <p></p> 
 <p></p> 
 <p></p> 
 <p></p> 
 <p></p> 
 <p></p> 
</blockquote>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d6bf55153ce1d75380706cece5127a94/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【SpringBoot集成SpringSecurity】</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/5055cd70594aa0824ff36f4a11a3615d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">SREXT 实现分析</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>