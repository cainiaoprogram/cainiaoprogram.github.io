<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>论文阅读笔记-DEEP COMPRESSION - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="论文阅读笔记-DEEP COMPRESSION" />
<meta property="og:description" content="本次阅读的论文是斯坦福大学韩松的博士论文《DEEP COMPRESSION: COMPRESSING DEEP NEURAL
NETWORKS WITH PRUNING, TRAINED QUANTIZATION
AND HUFFMAN CODING》
一、Abstract Why? 近年来，神经网络在人们实际生活中取得了巨大成功，但是，神经网络这类算法存在着一个鲜明的特点，它是一种计算密集型和存储密集型的算法，这就导致神经网络很难在嵌入式设备上部署，本文就旨在解决这一个限制。
How？ 作者在文章中提出了一种叫做“Deep Compression&#34;的方法，主要分为三个步骤：pruning, trained quantization and Huffman coding。剪枝（pruning)就是将权重中一些相对不重要的参数剪去（设为0），量化（trained quantization)则是用低比特的数据表示高比特的数据，Huffman coding则是一种常见的数据无损的压缩算法。
Result 通过Deep Compression， 在ImageNet数据集上, 作者将AlexNet模型的存储所需空间缩小了35倍（240MB-&gt;6.9MB)，并且没有精度损失，同样，将VGG-16模型所占用的存储空间缩小了49倍（552MB-&gt;11.3MB)，同样也没有精度上的损失。
二、INTRODUCTION 近年来,深度神经网络已经成为了计算机视觉领域十分有力的一个工具，但是，这些强大的深度神经网络都有相当数量的参数，对存储空间和存储带宽提出了很高的要求。
1、在手机等移动工具上，一般都存在这一个问题，例如在苹果的APP STORE上，大于100MB的文件必须在连接wifi的情况下才能下载，这就妨碍了深度神经网络在手机等设备上的部署。 2、手机等移动设备电池续航能力有限，而且散热也差，而深度神经网络在运行时需要频繁的访存以及乘累加运算，这些操作的功耗都是非常高的，因此这一点也限制了深度神经网络的部署。 文章针对这种情况，主要实现以下目标：
1、解决存储空间占用过大的问题
2、解决功耗过高的问题
于是，作者提出了deep compression的方法，将模型的大小压缩至原来的数十倍，使得模型参数能完全放在SRAM内，免去了对DRAM的访问，大大降低了功耗，最终实现了很高的加速比和功率效率。
三、Methods Network Pruning 如何剪枝 剪枝是一种能有效压缩模型大小和防止过拟合的方法，所谓剪枝，就是剪去一些不重要的连接：设置某个阈值，低于每个阈值的权重都被永久置为0，不再参与今后的计算。每次剪枝之后，我们还可以对模型进行retrain，微调权重，这样不断迭代，直至结果满意为止。
在本文中，我们通过剪枝将AlexNet模型的大小缩减了9倍，将VGG-16的模型大小缩减了13倍。
剪枝后参数的存储 剪枝之后，模型中绝大多数参数都会被置为0，因此，剪枝之后的权重矩阵是一个稀疏矩阵，针对稀疏矩阵的存储方式有很多，例如CSR、CSC等，本文作者采用CSR来存储稀疏化后的权重。
如上图所示，AA数组存储的是稀疏矩阵中非零元素的值，JA数组存储A中每一行第一个非零元素在AA中的索引，JA中最后一个元素为非零元素个数&#43;1，而IC数组存储的是AA中对应元素的列号，若设A中非零元素有a个，A矩阵有n行，那么采用CSR存储只需要2a&#43;n&#43;1个数据即可。
那么如果有乘法A*v，则操作如下：
for(i=1;i&lt;=n;i&#43;&#43;){
y0=0;
for(k=JA[i];k&lt;JA[i&#43;1];k&#43;&#43;)
y0&#43;=AA[k]*v[IC[k]];
y[i]=y0;
}
TRAINED QUANTIZATION AND WEIGHT SHARING 量化即用低比特的权重来代替高比特的权重，权重共享则是指让多个权重共享一个值。本文在剪枝之后，即对模型进行量化和权值共享，并且重新训练微调模型。
如图，是我们进行量化和权值共享的示意图，在上图中，输入神经元为4个，权重矩阵为4x4，我们将这16个权重分为4组，每组的颜色相同，并且它们共享同一个值，这些共享的值则没有必要使用32bit来表示，因为他们可能的取值很少，因此可以用低bit来表示这些共享的全值。下方的矩阵则是梯度矩阵，按照权重矩阵的分组对梯度矩阵分组，相同组内的梯度值相加，乘以学习率后对相应的共享权值进行更新。
在本文中，作者对剪枝后的AlexNet模型进行了量化，将卷积层权重量化为8bit，全连接层权重量化为5bit。
为了计算这种方法的压缩率，作者假定权重有k个组，组的索引则需要 l o g 2 ( k ) log_2(k) log2​(k)bit,整个权重矩阵共有n个权重，每个权重用b比特表示，那么压缩率为" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/691850fcadcad4b7e0d6209ab8dc9bae/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-09-20T20:35:23+08:00" />
<meta property="article:modified_time" content="2020-09-20T20:35:23+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">论文阅读笔记-DEEP COMPRESSION</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>本次阅读的论文是斯坦福大学韩松的博士论文《DEEP COMPRESSION: COMPRESSING DEEP NEURAL<br> NETWORKS WITH PRUNING, TRAINED QUANTIZATION<br> AND HUFFMAN CODING》</p> 
<h2><a id="Abstract_3"></a>一、Abstract</h2> 
<h3><a id="Why_4"></a>Why?</h3> 
<p>近年来，神经网络在人们实际生活中取得了巨大成功，但是，神经网络这类算法存在着一个鲜明的特点，它是一种计算密集型和存储密集型的算法，这就导致神经网络很难在嵌入式设备上部署，本文就旨在解决这一个限制。</p> 
<h3><a id="How_6"></a>How？</h3> 
<p>作者在文章中提出了一种叫做“Deep Compression"的方法，主要分为三个步骤：pruning, trained quantization and Huffman coding。剪枝（pruning)就是将权重中一些相对不重要的参数剪去（设为0），量化（trained quantization)则是用低比特的数据表示高比特的数据，Huffman coding则是一种常见的数据无损的压缩算法。</p> 
<h3><a id="Result_8"></a>Result</h3> 
<p>通过Deep Compression， 在ImageNet数据集上, 作者将AlexNet模型的存储所需空间缩小了35倍（240MB-&gt;6.9MB)，并且没有精度损失，同样，将VGG-16模型所占用的存储空间缩小了49倍（552MB-&gt;11.3MB)，同样也没有精度上的损失。</p> 
<h2><a id="INTRODUCTION_10"></a>二、INTRODUCTION</h2> 
<p>近年来,深度神经网络已经成为了计算机视觉领域十分有力的一个工具，但是，这些强大的深度神经网络都有相当数量的参数，对存储空间和存储带宽提出了很高的要求。</p> 
<h4><a id="1APP_STORE100MBwifi_12"></a>1、在手机等移动工具上，一般都存在这一个问题，例如在苹果的APP STORE上，大于100MB的文件必须在连接wifi的情况下才能下载，这就妨碍了深度神经网络在手机等设备上的部署。</h4> 
<h4><a id="2_13"></a>2、手机等移动设备电池续航能力有限，而且散热也差，而深度神经网络在运行时需要频繁的访存以及乘累加运算，这些操作的功耗都是非常高的，因此这一点也限制了深度神经网络的部署。</h4> 
<p>文章针对这种情况，主要实现以下目标：<br> 1、解决存储空间占用过大的问题<br> 2、解决功耗过高的问题<br> 于是，作者提出了deep compression的方法，将模型的大小压缩至原来的数十倍，使得模型参数能完全放在SRAM内，免去了对DRAM的访问，大大降低了功耗，最终实现了很高的加速比和功率效率。</p> 
<h2><a id="Methods_18"></a>三、Methods</h2> 
<h3><a id="Network_Pruning_19"></a>Network Pruning</h3> 
<h4><a id="_20"></a>如何剪枝</h4> 
<p>剪枝是一种能有效压缩模型大小和防止过拟合的方法，所谓剪枝，就是剪去一些不重要的连接：设置某个阈值，低于每个阈值的权重都被永久置为0，不再参与今后的计算。每次剪枝之后，我们还可以对模型进行retrain，微调权重，这样不断迭代，直至结果满意为止。<br> 在本文中，我们通过剪枝将AlexNet模型的大小缩减了9倍，将VGG-16的模型大小缩减了13倍。</p> 
<h4><a id="_23"></a>剪枝后参数的存储</h4> 
<p>剪枝之后，模型中绝大多数参数都会被置为0，因此，剪枝之后的权重矩阵是一个稀疏矩阵，针对稀疏矩阵的存储方式有很多，例如CSR、CSC等，本文作者采用CSR来存储稀疏化后的权重。<br> <img src="https://images2.imgbox.com/1d/e1/0sAHy9ZU_o.png" alt="在这里插入图片描述"><br> 如上图所示，AA数组存储的是稀疏矩阵中非零元素的值，JA数组存储A中每一行第一个非零元素在AA中的索引，JA中最后一个元素为非零元素个数+1，而IC数组存储的是AA中对应元素的列号，若设A中非零元素有a个，A矩阵有n行，那么采用CSR存储只需要2a+n+1个数据即可。<br> 那么如果有乘法A*v，则操作如下：<br> for(i=1;i&lt;=n;i++){<!-- --><br> y0=0;<br> for(k=JA[i];k&lt;JA[i+1];k++)<br> y0+=AA[k]*v[IC[k]];<br> y[i]=y0;<br> }</p> 
<h3><a id="TRAINED_QUANTIZATION_AND_WEIGHT_SHARING_34"></a>TRAINED QUANTIZATION AND WEIGHT SHARING</h3> 
<p>量化即用低比特的权重来代替高比特的权重，权重共享则是指让多个权重共享一个值。本文在剪枝之后，即对模型进行量化和权值共享，并且重新训练微调模型。<br> <img src="https://images2.imgbox.com/72/34/7ytvlWL4_o.png" alt="在这里插入图片描述"><br> 如图，是我们进行量化和权值共享的示意图，在上图中，输入神经元为4个，权重矩阵为4x4，我们将这16个权重分为4组，每组的颜色相同，并且它们共享同一个值，这些共享的值则没有必要使用32bit来表示，因为他们可能的取值很少，因此可以用低bit来表示这些共享的全值。下方的矩阵则是梯度矩阵，按照权重矩阵的分组对梯度矩阵分组，相同组内的梯度值相加，乘以学习率后对相应的共享权值进行更新。<br> 在本文中，作者对剪枝后的AlexNet模型进行了量化，将卷积层权重量化为8bit，全连接层权重量化为5bit。<br> 为了计算这种方法的压缩率，作者假定权重有k个组，组的索引则需要<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         l 
        
       
         o 
        
        
        
          g 
         
        
          2 
         
        
       
         ( 
        
       
         k 
        
       
         ) 
        
       
      
        log_2(k) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.03148em;">k</span><span class="mclose">)</span></span></span></span></span>bit,整个权重矩阵共有n个权重，每个权重用b比特表示，那么压缩率为<br> <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         r 
        
       
         = 
        
        
         
          
          
            n 
           
          
            b 
           
          
          
          
            n 
           
          
            l 
           
          
            o 
           
           
           
             g 
            
           
             2 
            
           
          
            ( 
           
          
            k 
           
          
            ) 
           
          
            + 
           
          
            k 
           
          
            b 
           
          
         
        
       
      
        r=\dfrac{nb}{nlog_2(k)+kb} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.02778em;">r</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 2.30744em; vertical-align: -0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.37144em;"><span class="" style="top: -2.314em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault">n</span><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.03148em;">k</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord mathdefault" style="margin-right: 0.03148em;">k</span><span class="mord mathdefault">b</span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault">n</span><span class="mord mathdefault">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.936em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p> 
<h4><a id="how_to_weight_sharing_41"></a>how to weight sharing</h4> 
<p>作者在本文中采用k-means聚类算法进行权值的共享，设有n个权重，目标是分为k组，那么每组中所有的权重都共享一个权值。</p> 
<h4><a id="initialization_of_shared_weights_43"></a>initialization of shared weights</h4> 
<p>在进行k均值聚类时，初始的k个聚类中心的选取对聚类结果有较大影响，本文介绍了三种初始化聚类中心的方法：</p> 
<h5><a id="1Forgy_random_initialization_45"></a>1、Forgy (random) initialization</h5> 
<p>在数据集中随机选取k个点作为k个聚类中心。</p> 
<h5><a id="2Densitybased_initialization_47"></a>2、Density-based initialization</h5> 
<p>基于密度进行选取。</p> 
<h5><a id="3Linear_initialization_49"></a>3、Linear initialization</h5> 
<p>在[xmin,xmax]区间内等距离的选取k个点作为聚类中心。</p> 
<h4><a id="feedforwarded_and_backpropagation_51"></a>feed-forwarded and back-propagation</h4> 
<h3><a id="Huffman_Coding_52"></a>Huffman Coding</h3> 
<p>略</p> 
<h2><a id="Experiments_54"></a>四、Experiments</h2> 
<p>作者一共剪枝、量化、哈夫曼编码了4个神经网络，两个基于MNIST，两个基于ImageNet。</p> 
<h3><a id="LENET300100_AND_LENET5_ON_MNIST_56"></a>LENET-300-100 AND LENET-5 ON MNIST</h3> 
<p>LeNet-300-100是一个全连接神经网络，共有两个隐藏层，分别有300和100个神经元，而LeNet-5则是一个卷积神经网络，有两个卷积层和两个全连接层，他们在MNIST的错误率分别为1.6%和0.8%。<br> 下表是四个神经网络的实验结果<br> <img src="https://images2.imgbox.com/a5/ba/2xe3y0cB_o.png" alt="在这里插入图片描述"><br> 下面两个表是两个Lenet的压缩情况<br> <img src="https://images2.imgbox.com/fe/38/T9X6MSsC_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="ALEXNET_ON_IMAGENET_62"></a>ALEXNET ON IMAGENET</h3> 
<p>下表是AlexNet的实验结果，压缩前后模型精度基本无损失。<br> <img src="https://images2.imgbox.com/5a/1e/gx8tjlp5_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="VGG16_ON_IMAGENET_66"></a>VGG-16 ON IMAGENET</h3> 
<p>下表是VGG-16的实验结果。<br> <img src="https://images2.imgbox.com/c5/11/kO07oCZs_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="Discussion_69"></a>五、Discussion</h2> 
<h3><a id="PRUNING_AND_QUANTIZATION_WORKING_TOGETHER_70"></a>PRUNING AND QUANTIZATION WORKING TOGETHER</h3> 
<p>作者通过实验发现，剪枝和量化一起使用的效果要好于剪枝或量化单独使用时的效果。<br> 下图是不同压缩方法下模型准确率和压缩率的关系，可以看到，剪枝和量化一起使用时的效果是最佳的。<br> <img src="https://images2.imgbox.com/79/be/AYRkSqxI_o.png" alt="在这里插入图片描述"><br> 下图则是量化前是否进行剪枝对最终量化结果的影响。<br> <img src="https://images2.imgbox.com/2e/68/5cqJSYJp_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="CENTROID_INITIALIZATION_76"></a>CENTROID INITIALIZATION</h3> 
<p>作者比较了三种聚类中心初始化方法对模型最终结果的影响，如下图<br> <img src="https://images2.imgbox.com/5a/94/OwPu2zxb_o.png" alt="在这里插入图片描述"><br> 可以看到，uniform init优于其余两种方法。</p> 
<h3><a id="SPEEDUP_AND_ENERGY_EFFICIENCY_80"></a>SPEEDUP AND ENERGY EFFICIENCY</h3> 
<p>作者指出，deep compression主要用于对延迟比较敏感的场合，需要设备作出实时的反应，因此，batchsize不等于1是不符合实际应用场景的，下面讨论batchsize=1下的情况。<br> 我们知道，在卷积神经网络中，FC层占据了整个网络超过90%的权重参数，因而本文的模型压缩方法主要还是对FC层起作用，为了比较Deep Compression前后的差别，我们把主要目光聚焦在全连接层，作者在三个平台上进行了实验，分别是<br> 1.NVIDIA GeForce GTX Titan X as desktop processor<br> 2.Intel Core i7 5930K as desktop processor<br> 3.NVIDIA Tegra K1 as mobile processor<br> 下图显示了不同平台上剪枝后模型的Speeding up</p> 
<p><img src="https://images2.imgbox.com/0d/35/2pE7WntD_o.png" alt="在这里插入图片描述"><br> 下图显示了不同平台上剪枝前后的Energy Efficiency<br> <img src="https://images2.imgbox.com/97/1a/O8a25ITh_o.png" alt="在这里插入图片描述"><br> 此外，作者还讨论了是否进行Batch Process的不同，如果进行Batch，那么每次读入一个矩阵，并和权重矩阵相乘，访存为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         O 
        
       
         ( 
        
        
        
          n 
         
        
          2 
         
        
       
         ) 
        
       
      
        O(n^2) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.06411em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>，计算为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         O 
        
       
         ( 
        
        
        
          n 
         
        
          3 
         
        
       
         ) 
        
       
      
        O(n^3) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.06411em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>,则存储-计算比为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         O 
        
       
         ( 
        
       
         1 
        
       
         / 
        
       
         n 
        
       
         ) 
        
       
      
        O(1/n) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.02778em;">O</span><span class="mopen">(</span><span class="mord">1</span><span class="mord">/</span><span class="mord mathdefault">n</span><span class="mclose">)</span></span></span></span></span>,而如果不进行Batch，那么每次读入的是一个向量，和权重矩阵相乘，类似的可以得到存储-计算比为O(1)，由此可见，不进行Batch时存储-计算比较低，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
         
         
           S 
          
         
           o 
          
         
             
          
         
           r 
          
         
           e 
          
         
           d 
          
         
           u 
          
         
           c 
          
         
           i 
          
         
           n 
          
         
           g 
          
         
             
          
         
           t 
          
         
           h 
          
         
           e 
          
         
             
          
         
           m 
          
         
           e 
          
         
           m 
          
         
           o 
          
         
           r 
          
         
           y 
          
         
             
          
         
           f 
          
         
           o 
          
         
           o 
          
         
           t 
          
         
           p 
          
         
           r 
          
         
           i 
          
         
           n 
          
         
           t 
          
         
             
          
         
           i 
          
         
           s 
          
         
             
          
         
           c 
          
         
           r 
          
         
           i 
          
         
           t 
          
         
           i 
          
         
           c 
          
         
           a 
          
         
           l 
          
         
             
          
         
           f 
          
         
           o 
          
         
           r 
          
         
             
          
         
           t 
          
         
           h 
          
         
           e 
          
         
             
          
         
           n 
          
         
           o 
          
         
           n 
          
         
           − 
          
         
           b 
          
         
           a 
          
         
           t 
          
         
           c 
          
         
           h 
          
         
           i 
          
         
           n 
          
         
           g 
          
         
             
          
         
           c 
          
         
           a 
          
         
           s 
          
         
           e 
          
         
           . 
          
         
        
       
      
        \color{#FF0000}{So~ reducing ~the ~memory ~footprint ~is ~critical~ for~ the ~non-batching~ case.} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord" style="color: rgb(255, 0, 0);"><span class="mord mathdefault" style="margin-right: 0.05764em; color: rgb(255, 0, 0);">S</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">o</span><span class="mspace nobreak" style="color: rgb(255, 0, 0);"><span style="color: rgb(255, 0, 0);"> </span></span><span class="mord mathdefault" style="margin-right: 0.02778em; color: rgb(255, 0, 0);">r</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">e</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">d</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">u</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">c</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">i</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">n</span><span class="mord mathdefault" style="margin-right: 0.03588em; color: rgb(255, 0, 0);">g</span><span class="mspace nobreak" style="color: rgb(255, 0, 0);"><span style="color: rgb(255, 0, 0);"> </span></span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">t</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">h</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">e</span><span class="mspace nobreak" style="color: rgb(255, 0, 0);"><span style="color: rgb(255, 0, 0);"> </span></span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">m</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">e</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">m</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">o</span><span class="mord mathdefault" style="margin-right: 0.02778em; color: rgb(255, 0, 0);">r</span><span class="mord mathdefault" style="margin-right: 0.03588em; color: rgb(255, 0, 0);">y</span><span class="mspace nobreak" style="color: rgb(255, 0, 0);"><span style="color: rgb(255, 0, 0);"> </span></span><span class="mord mathdefault" style="margin-right: 0.10764em; color: rgb(255, 0, 0);">f</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">o</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">o</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">t</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">p</span><span class="mord mathdefault" style="margin-right: 0.02778em; color: rgb(255, 0, 0);">r</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">i</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">n</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">t</span><span class="mspace nobreak" style="color: rgb(255, 0, 0);"><span style="color: rgb(255, 0, 0);"> </span></span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">i</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">s</span><span class="mspace nobreak" style="color: rgb(255, 0, 0);"><span style="color: rgb(255, 0, 0);"> </span></span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">c</span><span class="mord mathdefault" style="margin-right: 0.02778em; color: rgb(255, 0, 0);">r</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">i</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">t</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">i</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">c</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">a</span><span class="mord mathdefault" style="margin-right: 0.01968em; color: rgb(255, 0, 0);">l</span><span class="mspace nobreak" style="color: rgb(255, 0, 0);"><span style="color: rgb(255, 0, 0);"> </span></span><span class="mord mathdefault" style="margin-right: 0.10764em; color: rgb(255, 0, 0);">f</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">o</span><span class="mord mathdefault" style="margin-right: 0.02778em; color: rgb(255, 0, 0);">r</span><span class="mspace nobreak" style="color: rgb(255, 0, 0);"><span style="color: rgb(255, 0, 0);"> </span></span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">t</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">h</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">e</span><span class="mspace nobreak" style="color: rgb(255, 0, 0);"><span style="color: rgb(255, 0, 0);"> </span></span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">n</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">o</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">n</span><span class="mspace" style="color: rgb(255, 0, 0); margin-right: 0.222222em;"></span><span class="mbin" style="color: rgb(255, 0, 0);">−</span><span class="mspace" style="color: rgb(255, 0, 0); margin-right: 0.222222em;"></span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">b</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">a</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">t</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">c</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">h</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">i</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">n</span><span class="mord mathdefault" style="margin-right: 0.03588em; color: rgb(255, 0, 0);">g</span><span class="mspace nobreak" style="color: rgb(255, 0, 0);"><span style="color: rgb(255, 0, 0);"> </span></span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">c</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">a</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">s</span><span class="mord mathdefault" style="color: rgb(255, 0, 0);">e</span><span class="mord" style="color: rgb(255, 0, 0);">.</span></span></span></span></span></span></p> 
<h3><a id="RATIO_OF_WEIGHTS_INDEX_AND_CODEBOOK_92"></a>RATIO OF WEIGHTS, INDEX AND CODEBOOK</h3>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a15525da1111eee9c990bcab260c4a04/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">基于服务器部署的OCR在线识别应用</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d3500f4a2281bea64de9aab61471d63a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">median函数的使用方法_如何在Google表格中使用MEDIAN函数</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>