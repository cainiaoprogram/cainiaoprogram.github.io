<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Fully-Convolutional Siamese Networks for Object Tracking基于全卷积孪生网络的目标跟踪算法SiameseFC - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Fully-Convolutional Siamese Networks for Object Tracking基于全卷积孪生网络的目标跟踪算法SiameseFC" />
<meta property="og:description" content="1.论文相关 Bertinetto, Luca, et al. &#34;Fully-convolutional siamese networks for object tracking.&#34; European conference on computer vision. Springer, Cham, 2016.
之前写的论文总结更多的偏向于论文翻译，以后写的论文总结要更偏向于总结
2.论文内容 2.1摘要 任意目标的跟踪问题，传统方法实施起来基本上是使用单独的视频本身作为训练集，然后学习一个目标的表观模型。尽管这种方法很成功，但是这种在线学习方式限制了可以学到模型的丰富性。近来，很多人尝试探索深度卷积网络强大的表达能力。然而，由于目标跟踪中目标事先是不知道的，所以每次都需要在线执行一次随机梯度下降算法来调整网络权重，这严重影响了跟踪系统的速度。在这篇论文里，我们使用了一种新颖的全卷积孪生网络来做为基本的跟踪算法，网络以端到端的形式在用来进行目标检测的视频数据集ILSVRC15上训练而成。我们的跟踪器远远超过了实时性的要求，操作起来还很简单，并且在多个benchmark上都达到了当前最优的性能。
2.2引入 跟踪问题是在视频的第一帧确定一个单独的目标来进行跟踪，目标是任意给定的，所以这就无法收集数据来训练一个特定的检测器来进行跟踪。
今年来，很多成功的范例基本都是使用视频里提取出来的例子，以在线的形式学习一个目标的表观模型出来。这种方法的盛行很大程度上归功于一些已有算法的成功，比如TLD,Struck，KCF。然而，很明显的一个效率不高的地方在于，使用当前视频派生出来的数据仅仅能学习到相对简单的模型。尽管计算机视觉中深度卷积网络的应用无处不在，但是由于缺乏监督数据以及跟踪的实时性要求，通过深度学习在每一个视频学到一个简单的检测器这种方法都难以应用。
最近很多工作志在通过使用预训练模型来解决上述问题。这些方法中，要么是使用网络内部某一层作为特征，然后再使用浅显的方法来跟踪，比如相关滤波；要么是使用SGD方法来微调多层网络。浅显的方法不能充分利用端到端学习的益处，而使用SGD微调虽然能到达时下最优效果，但是却难以达到实时性的要求。
我们提倡另一种替代性的方法。这个方法在初始离线阶段把训练网络更多地看成一个通用的相似性学习问题。这篇论文的关键贡献就在于证明这个方法在benchmark上可以达到非常有竞争性的性能，并且运行时的帧率远超实时性的要求。具体点讲，我们训练了一个孪生网络在一个较大的搜索区域搜索样本图片。本文另一个贡献在于，孪生网络是一个关于搜索区域的全卷积网络，而最后目标位置的估计我们通过计算两个输入的交叉相关，然后再进行插值得到，密集而且高效。
这种相似度学习方法之前是被忽略的，因为跟踪领域难以获得大量的标记数据集。事实上，直到最近，我们能获得的数据集也仅仅包含几百个标注视频。然而，我们相信ILSVRC视频目标检测数据集的出现让训练这么一个模型成为可能。为了公平起见，VOT委员会禁止既使用视频来训练网络，又用这个视频来验证网络。而我们证明了我们模型从ImageNet视频到ALOV/OTB/VOT视频的泛化能力，我们保留跟踪的benchmarkmark视频单独用来测试我们的性能。
3.跟踪中的深度相似性学习 跟踪一个任意目标可以被当做一种相似性学习。我们要学习到一个函数，函数比较样本z和搜索区域x，然后返回一个得分图。得分图和搜索区域x尺寸相同。得分高，说明这个区域和z相似，反之说明不相似。要找到z在新一帧中的位置，我们只需要把所有的可能位置都计算一下相似度即可。在实验中，我们使用目标初始的表观作为z就足够了。
******************上面还是像翻译，下面我要多总结**********************
那么用什么来模拟这么一个函数f呢？当然是最近很火的深度卷积网络了，因为他早已经在计算机视觉领域取得了广泛成功。而使用深度卷积的相似性度量函数可以作为一个很典型的孪生结构。孪生网络如同一个特征提取器，同时提取z和x图片的特征，然后将提取到的特征送入另一个函数g，那么我们这个相似性度量函数其实就是。函数g可以是一个很简单的距离度量或者相似度度量。这种深度孪生网络早已被广泛应用与人脸确认，关键描述点学习，one-shot字符识别。
3.1网络的总体结构 网络总体结构如上图所示，孪生网络其实就是一个特征提取器，它提取z和x特征之后，送到相似度函数里计算一下相似度。本文的相似度函数是使用交叉相关，公式如下，
其实就是将作为一个卷积核，在上进行卷积，相似度大的地方，那么自然响应值就大，那自然也就可以当做是目标z在x中的位置了。
那岂不是随便找一个特征提取器，提取一个特征一做，然后再一卷积都可以了？也许可以，但是由于本文的孪生网络是以端到端的形式学习出来的，那么可以认为，它训练出来的这个特征提取器，提取的特征更适合做卷积来获得最后的相似度得分图。而其他的特征提取器提取到的特征可能就不太适合用卷积来获得相似度响应图。
跟踪的方法就很好想到了，把上一帧目标的位置作为中心，在下一帧附近计算响应图。响应值最大的位置相对于中心的偏移再乘以步长，那就是目标在下一帧的真实位置了。为了应对尺度变化，作者在进行跟踪的时候也同时使用了多种尺寸来进行搜索。
3.2使用图片来训练 作者的训练方式是非常有创意的。网络最后的输出，其实相当于一个判别式方法，用正负样本对来训练网络。搜索图片x中的每一个候选子窗口，其实相当于一个样本，而它的得分，输出的就是它是正/负样本的概率。使用逻辑回归来表示的话，这就是一个应用逻辑回归的典型二分类问题，那么逻辑损失就可以表示为下式：
其中，v是候选位置的得分，而y是它的真实类别，y属于｛1，-1｝。这个推导其实很简单，分别表示出来逻辑回归分类时正负样本对应的概率和，那么损失函数就是和，结合y属于｛1，-1｝稍微整理一下就可以把这两个式子统一为作者给出的形式。
而训练的时候网络的最终损失函数如下，
简单来说就是搜索区域x的所有候选位置的平均损失。
训练样本对（z，x）从标注视频数据集中获得，如下图
只要x和z在视频里相隔不超过T帧，那么都可以作为一个训练对。图片要进行归一化，但是不能破坏长宽比，而是用背景补充。至于y怎么确定呢，也就是说什么叫做正样本什么叫做负样本呢？定义如下
R是我们定义的半径，c是目标的中心，k是网络最终总步长。上面的意思就是说在255*255这张图片上，只要和目标的距离不超过R，那就算正样本，否则就是负样本。
3.3孪生网络部分的结构 pooling是最大值pooling，每个线性层之后都batch normalization，除了conv5每个卷积层之后都有relu，等等，具体设置可以查看论文。
4.总结 网络的总体思路如上，但是在训练的时候数据集还有些其他处理，比如填充背景的大小，等等。总之网络速度快，性能好。有什么缺点呢？" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/14d3e8e8d9098363dcc71b1935a9e5b2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-03-12T21:35:53+08:00" />
<meta property="article:modified_time" content="2018-03-12T21:35:53+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Fully-Convolutional Siamese Networks for Object Tracking基于全卷积孪生网络的目标跟踪算法SiameseFC</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h4><span style="font-size:18px;">1.论文相关</span></h4> 
<p><span style="font-size:16px;"></span></p> 
<p><span style="font-size:16px;"><span style="color:rgb(34,34,34);font-family:Arial, sans-serif;text-align:left;background-color:rgb(255,255,255);">Bertinetto, Luca, et al. "Fully-convolutional siamese networks for object tracking." </span><span style="color:rgb(34,34,34);font-family:Arial, sans-serif;text-align:left;background-color:rgb(255,255,255);">European conference on computer vision</span><span style="color:rgb(34,34,34);font-family:Arial, sans-serif;text-align:left;background-color:rgb(255,255,255);">. Springer, Cham, 2016.</span><br></span></p> 
<p style="text-align:left;"><span style="font-family:Arial, sans-serif;color:#222222;"><span style="background-color:rgb(255,255,255);"><span style="font-size:16px;">之前写的论文总结更多的偏向于论文翻译，以后写的论文总结要更偏向于总结</span></span></span></p> 
<h4><span style="font-size:18px;">2.论文内容</span></h4> 
<h5>2.1摘要</h5> 
<p>        任意目标的跟踪问题，传统方法实施起来基本上是使用单独的视频本身作为训练集，然后学习一个目标的表观模型。尽管这种方法很成功，但是这种在线学习方式限制了可以学到模型的丰富性。近来，很多人尝试探索深度卷积网络强大的表达能力。然而，由于目标跟踪中目标事先是不知道的，所以每次都需要在线执行一次随机梯度下降算法来调整网络权重，这严重影响了跟踪系统的速度。在这篇论文里，我们使用了一种新颖的全卷积孪生网络来做为基本的跟踪算法，网络以端到端的形式在用来进行目标检测的视频数据集ILSVRC15上训练而成。我们的跟踪器远远超过了实时性的要求，操作起来还很简单，并且在多个benchmark上都达到了当前最优的性能。</p> 
<h5>2.2引入</h5> 
<p>        跟踪问题是在视频的第一帧确定一个单独的目标来进行跟踪，目标是任意给定的，所以这就无法收集数据来训练一个特定的检测器来进行跟踪。<br></p> 
<p>        今年来，很多成功的范例基本都是使用视频里提取出来的例子，以在线的形式学习一个目标的表观模型出来。这种方法的盛行很大程度上归功于一些已有算法的成功，比如TLD,Struck，KCF。然而，很明显的一个效率不高的地方在于，使用当前视频派生出来的数据仅仅能学习到相对简单的模型。尽管计算机视觉中深度卷积网络的应用无处不在，但是由于缺乏监督数据以及跟踪的实时性要求，通过深度学习在每一个视频学到一个简单的检测器这种方法都难以应用。<br></p> 
<p>        最近很多工作志在通过使用预训练模型来解决上述问题。这些方法中，要么是使用网络内部某一层作为特征，然后再使用浅显的方法来跟踪，比如相关滤波；要么是使用SGD方法来微调多层网络。浅显的方法不能充分利用端到端学习的益处，而使用SGD微调虽然能到达时下最优效果，但是却难以达到实时性的要求。<br></p> 
<p>       我们提倡另一种替代性的方法。这个方法在初始离线阶段把训练网络更多地看成一个通用的相似性学习问题。这篇论文的关键贡献就在于证明这个方法在benchmark上可以达到非常有竞争性的性能，并且运行时的帧率远超实时性的要求。具体点讲，我们训练了一个孪生网络在一个较大的搜索区域搜索样本图片。本文另一个贡献在于，孪生网络是一个关于搜索区域的全卷积网络，而最后目标位置的估计我们通过计算两个输入的交叉相关，然后再进行插值得到，密集而且高效。<br></p> 
<p>        这种相似度学习方法之前是被忽略的，因为跟踪领域难以获得大量的标记数据集。事实上，直到最近，我们能获得的数据集也仅仅包含几百个标注视频。然而，我们相信ILSVRC视频目标检测数据集的出现让训练这么一个模型成为可能。为了公平起见，VOT委员会禁止既使用视频来训练网络，又用这个视频来验证网络。而我们证明了我们模型从ImageNet视频到ALOV/OTB/VOT视频的泛化能力，我们保留跟踪的benchmarkmark视频单独用来测试我们的性能。<br></p> 
<h4>3.跟踪中的深度相似性学习</h4> 
<p style="text-align:justify;">        跟踪一个任意目标可以被当做一种相似性学习。我们要学习到一个函数<img src="https://images2.imgbox.com/cf/c3/yMe0wOnP_o.png" alt="">，函数比较样本z和搜索区域x，然后返回一个得分图。得分图和搜索区域x尺寸相同。得分高，说明这个区域和z相似，反之说明不相似。要找到z在新一帧中的位置，我们只需要把所有的可能位置都计算一下相似度即可。在实验中，我们使用目标初始的表观作为z就足够了。<br></p> 
<p style="text-align:justify;">******************上面还是像翻译，下面我要多总结**********************</p> 
<p style="text-align:justify;">        那么用什么来模拟这么一个函数f呢？当然是最近很火的深度卷积网络了，因为他早已经在计算机视觉领域取得了广泛成功。而使用深度卷积的相似性度量函数可以作为一个很典型的孪生结构。孪生网络如同一个特征提取器<img src="https://images2.imgbox.com/77/38/G1jDJEqC_o.png" alt="">，同时提取z和x图片的特征，然后将提取到的特征送入另一个函数g，那么我们这个相似性度量函数其实就是<img src="https://images2.imgbox.com/06/bc/kXn0ZV8M_o.png" alt="">。函数g可以是一个很简单的距离度量或者相似度度量。这种深度孪生网络早已被广泛应用与人脸确认，关键描述点学习，one-shot字符识别。</p> 
<h5 style="text-align:justify;">3.1网络的总体结构</h5> 
<p style="text-align:center;"><img src="https://images2.imgbox.com/da/99/q2MaY8YQ_o.png" alt=""><br></p> 
<p style="text-align:justify;">网络总体结构如上图所示，孪生网络其实就是一个特征提取器，它提取z和x特征之后，送到相似度函数里计算一下相似度。本文的相似度函数是使用交叉相关，公式如下，</p> 
<p style="text-align:center;"><img src="https://images2.imgbox.com/d6/fc/3s3QoDTE_o.png" alt=""><br></p> 
<p style="text-align:justify;">其实就是将<img src="https://images2.imgbox.com/cb/e7/zLukw34f_o.png" alt="">作为一个卷积核，在<img src="https://images2.imgbox.com/3c/83/6ey9pL2Q_o.png" alt="">上进行卷积，相似度大的地方，那么自然响应值就大，那自然也就可以当做是目标z在x中的位置了。</p> 
<p style="text-align:justify;">        那岂不是随便找一个特征提取器，提取一个特征一做，然后再一卷积都可以了？也许可以，但是由于本文的孪生网络是以端到端的形式学习出来的，那么可以认为，它训练出来的这个特征提取器，提取的特征更适合做卷积来获得最后的相似度得分图。而其他的特征提取器提取到的特征可能就不太适合用卷积来获得相似度响应图。</p> 
<p style="text-align:justify;">        跟踪的方法就很好想到了，把上一帧目标的位置作为中心，在下一帧附近计算响应图。响应值最大的位置相对于中心的偏移再乘以步长，那就是目标在下一帧的真实位置了。为了应对尺度变化，作者在进行跟踪的时候也同时使用了多种尺寸来进行搜索。<br></p> 
<h5 style="text-align:justify;">3.2使用图片来训练</h5> 
<p>        作者的训练方式是非常有创意的。网络最后的输出，其实相当于一个判别式方法，用正负样本对来训练网络。搜索图片x中的每一个候选子窗口，其实相当于一个样本，而它的得分，输出的就是它是正/负样本的概率。使用逻辑回归来表示的话，这就是一个应用逻辑回归的典型二分类问题，那么逻辑损失就可以表示为下式：</p> 
<p style="text-align:center;"><img src="https://images2.imgbox.com/d3/9c/RLpJqQ8u_o.png" alt=""><br></p> 
<p style="text-align:justify;">其中，v是候选位置的得分，而y是它的真实类别，y属于｛1，-1｝。这个推导其实很简单，分别表示出来逻辑回归分类时正负样本对应的概率<img src="https://images2.imgbox.com/38/af/SluqawE0_o.png" alt="">和<img src="https://images2.imgbox.com/6c/bb/7Lytwoix_o.png" alt="">，那么损失函数就是<img src="https://images2.imgbox.com/05/5c/w9YBwCqA_o.png" alt="">和<img src="https://images2.imgbox.com/86/15/T2Pvz1Ch_o.png" alt="">，结合y属于｛1，-1｝稍微整理一下就可以把这两个式子统一为作者给出的形式。</p> 
<p style="text-align:justify;">        而训练的时候网络的最终损失函数如下，<br></p> 
<p style="text-align:center;"><img src="https://images2.imgbox.com/de/2e/pm7Y1oh7_o.png" alt=""><br></p> 
<p style="text-align:justify;">        简单来说就是搜索区域x的所有候选位置的平均损失。<br></p> 
<p style="text-align:justify;">        训练样本对（z，x）从标注视频数据集中获得，如下图<br></p> 
<p style="text-align:center;"><img src="https://images2.imgbox.com/07/a6/7RZepm3q_o.png" alt=""><img src="https://images2.imgbox.com/2e/76/kX33ukX2_o.png" alt=""><br></p> 
<p style="text-align:justify;">只要x和z在视频里相隔不超过T帧，那么都可以作为一个训练对。图片要进行归一化，但是不能破坏长宽比，而是用背景补充。至于y怎么确定呢，也就是说什么叫做正样本什么叫做负样本呢？定义如下</p> 
<p style="text-align:center;"><img src="https://images2.imgbox.com/dd/4a/5FLDBJkh_o.png" alt=""><br></p> 
<p style="text-align:justify;">R是我们定义的半径，c是目标的中心，k是网络最终总步长。上面的意思就是说在255*255这张图片上，只要和目标的距离不超过R，那就算正样本，否则就是负样本。</p> 
<h5 style="text-align:justify;">3.3孪生网络部分的结构</h5> 
<p style="text-align:center;"><img src="https://images2.imgbox.com/c2/2f/rJDD41uo_o.png" alt=""></p> 
<p style="text-align:justify;">pooling是最大值pooling，每个线性层之后都batch normalization，除了conv5每个卷积层之后都有relu，等等，具体设置可以查看论文。</p> 
<h4 style="text-align:justify;">4.总结</h4> 
<p>网络的总体思路如上，但是在训练的时候数据集还有些其他处理，比如填充背景的大小，等等。总之网络速度快，性能好。有什么缺点呢？</p> 
<p style="text-align:justify;"><br></p> 
<p style="text-align:justify;"><br></p> 
<p style="text-align:justify;"><br></p> 
<p style="text-align:justify;"><br></p> 
<p style="text-align:justify;"><br></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c549ac85339abfca7b8f9301dea561af/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">“刷脸出入” 人脸识别门禁强化出入安全</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/af4462e5083957cd9c6a8ec15da0832d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">zabbix web应用架构分析</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>