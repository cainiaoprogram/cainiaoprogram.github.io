<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Scrapy 下载器中间件、spider中间件 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Scrapy 下载器中间件、spider中间件" />
<meta property="og:description" content="Scrapy 官方文档 ( 下载器中间件 )
：https://doc.scrapy.org/en/latest/topics/downloader-middleware.html：https://www.osgeo.cn/scrapy/topics/downloader-middleware.html Scrapy 扩展中间件: 针对特定响应状态码，使用代理重新请求：https://www.cnblogs.com/my8100/p/scrapy_middleware_autoproxy.html
https://www.baidu.com/s?wd=中间件状态码不等于200重新请求
1、下载器 中间件 ( Downloader Middleware ) 下载器中间件是介于 Scrapy 的 request/response 处理的钩子框架。 是用于全局修改Scrapy request 和 response 的一个轻量、底层的系统。
Downloader Middleware 即 下载中间件。它是处于Scrapy的 Engine 和 Downloader 之间的处理模块。在 Engine 把从 Scheduler 获取的 Request 发送给 Downloader 的过程中，以及Downloader 把 Response 发送回 Engine 的过程中，Request 和 Response 都会经过 Downloader Middleware 的处理。
也就是 DownloaderMiddleware 在整个架构中起作用的位置是以下两个
Engine从Scheduler获取Request发送给Downloader，在Request被 Engine 发送给Downloader执行下载之前，DownloaderMiddleware可以对Request进行修改。Downloader执行Request后生成Response，在Response被Engine发送给Spider之前，也就是在Resposne被Spider解析之前，Downloder Middleware可以对Response进行修改 DownloderMiddleware 在整个爬虫执行过程中能起到非常重要的作用，功能十分强大。可以修改User-Agent、处理重定向、设置代理、失败重试、设置Cookie 等。
Downloader Middleware 的用法非常简单，只要实现 process_request、process_response、process_exception 中的任意一个方法即可，同时不同方法的返回值不同，其产生的效果也不同。
关于 调用 顺序" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/6444076d992198bc3ae219afb8d50de9/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-18T21:02:52+08:00" />
<meta property="article:modified_time" content="2023-10-18T21:02:52+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Scrapy 下载器中间件、spider中间件</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p></p> 
<p>Scrapy 官方文档 ( 下载器中间件 )</p> 
<ul><li>：<a href="https://doc.scrapy.org/en/latest/topics/downloader-middleware.html" rel="nofollow" title="https://doc.scrapy.org/en/latest/topics/downloader-middleware.html">https://doc.scrapy.org/en/latest/topics/downloader-middleware.html</a></li><li>：<a href="https://www.osgeo.cn/scrapy/topics/downloader-middleware.html" rel="nofollow" title="https://www.osgeo.cn/scrapy/topics/downloader-middleware.html">https://www.osgeo.cn/scrapy/topics/downloader-middleware.html</a></li></ul> 
<p>Scrapy 扩展中间件: 针对特定响应状态码，使用代理重新请求：<a href="https://www.cnblogs.com/my8100/p/scrapy_middleware_autoproxy.html" rel="nofollow" title="https://www.cnblogs.com/my8100/p/scrapy_middleware_autoproxy.html">https://www.cnblogs.com/my8100/p/scrapy_middleware_autoproxy.html</a></p> 
<p><a href="https://www.baidu.com/s?wd=%E4%B8%AD%E9%97%B4%E4%BB%B6%E7%8A%B6%E6%80%81%E7%A0%81%E4%B8%8D%E7%AD%89%E4%BA%8E200%E9%87%8D%E6%96%B0%E8%AF%B7%E6%B1%82" rel="nofollow" title="https://www.baidu.com/s?wd=中间件状态码不等于200重新请求">https://www.baidu.com/s?wd=中间件状态码不等于200重新请求</a></p> 
<p></p> 
<p></p> 
<p></p> 
<h2>1、下载器 中间件 ( Downloader Middleware )</h2> 
<p></p> 
<p>下载器中间件是介于 Scrapy 的 request/response 处理的钩子框架。 是用于全局修改Scrapy request 和 response 的一个轻量、底层的系统。</p> 
<blockquote> 
 <p><span style="color:#fe2c24;">Downloader Middleware 即 下载中间件。它是处于Scrapy的 Engine 和 Downloader 之间的处理模块。在 Engine 把从 Scheduler 获取的 Request 发送给 Downloader 的过程中，以及Downloader 把 Response 发送回 Engine 的过程中，Request 和 Response 都会经过 Downloader Middleware 的处理。</span></p> 
 <p>也就是 DownloaderMiddleware 在整个架构中起作用的位置是以下两个</p> 
 <ul><li>Engine从Scheduler获取Request发送给Downloader，在Request被 Engine 发送给Downloader执行下载之前，DownloaderMiddleware可以对Request进行修改。</li><li>Downloader执行Request后生成Response，在Response被Engine发送给Spider之前，也就是在Resposne被Spider解析之前，Downloder Middleware可以对Response进行修改</li></ul> 
 <p>DownloderMiddleware 在整个爬虫执行过程中能起到非常重要的作用，功能十分强大。可以修改User-Agent、处理重定向、设置代理、失败重试、设置Cookie 等。</p> 
</blockquote> 
<p>Downloader Middleware 的用法非常简单，只要实现 process_request、process_response、process_exception 中的任意一个方法即可，同时不同方法的返回值不同，其产生的效果也不同。</p> 
<p>关于 调用 顺序</p> 
<ul><li>由于 Request 是从 Engine 发送给 Downloader 的，并且优先级数字越小的Downloader Middleware越靠近 Engine，所以优先级数字越小的 Dowloader Middleware的process_request 方法越先被调用。</li><li>process_response 方法则相反，由于Response 是由Downloder 发送给 Engine的，优先级数字越大的 Downloader Middleware 越靠近 Downloader，所以优先级数字越大的 Downloader Middleware 的 process_response 越先被调用</li><li>在 process_request 方法中，当返回值是 Response 对象时，更低优先级的Downloader Middleware 的process_request和process_exception方法不会被继续调用，每个 Downloader Middleware 的 process_response 方法而被依次调用。调用完之后，直接将 Response对象发送给Spider来处理。</li></ul> 
<p>如果想将自定义的 Downloader Middleware 添加到项目中，不要直接修改 DOWNLOADER_MIDDLEWARES_BASE 变量。Scrapy 提供了另外一个设置变量 DOWNLOADER_MIDDLEWARES，直接修改这个变量就可以添加自己定义的 Downloader Middleware，以及禁用 DOWNLOADER_MIDDLEWARES_BASE里面定义的 Downloader Middleware了。</p> 
<p></p> 
<p><span style="color:#7c79e5;"><strong>1. 如果完全没有中间件，爬虫的流程如下图所示。</strong></span></p> 
<p style="text-align:center;"><img alt="" class="has" src="https://images2.imgbox.com/af/59/NMM3t7x4_o.jpg"></p> 
<p><span style="color:#7c79e5;"><strong>2. 使用了中间件以后，爬虫的流程如下图所示。</strong></span></p> 
<p style="text-align:center;"><img alt="" class="has" src="https://images2.imgbox.com/5a/0f/dlJipdTv_o.jpg"></p> 
<p></p> 
<p></p> 
<h3>激活下载器中间件</h3> 
<p></p> 
<p>要激活下载器中间件组件，将其加入到 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/settings.html#std:setting-DOWNLOADER_MIDDLEWARES" rel="nofollow" title="DOWNLOADER_MIDDLEWARES">DOWNLOADER_MIDDLEWARES</a> 设置中。 该设置是一个字典(dict)，键为中间件类的路径，值为其中间件的顺序(order)。</p> 
<p>这里是一个例子:</p> 
<pre class="has"><code class="language-html">DOWNLOADER_MIDDLEWARES = {
    'myproject.middlewares.CustomDownloaderMiddleware': 543,
}
</code></pre> 
<p><a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/settings.html#std:setting-DOWNLOADER_MIDDLEWARES" rel="nofollow" title="DOWNLOADER_MIDDLEWARES">DOWNLOADER_MIDDLEWARES</a><strong><span style="color:#f33b45;"> 设置会与 Scrapy 定义的 </span></strong><a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/settings.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE" rel="nofollow" title="DOWNLOADER_MIDDLEWARES_BASE">DOWNLOADER_MIDDLEWARES_BASE</a><strong><span style="color:#f33b45;"> 设置合并(</span><span style="color:#7c79e5;">但不是覆盖</span><span style="color:#f33b45;">)， </span>而后根据顺序(order)进行排序，最后得到启用中间件的有序列表<span style="color:#f33b45;">: 第一个中间件是最靠近引擎的，最后一个中间件是最靠近下载器的。</span></strong></p> 
<p>关于如何分配中间件的顺序请查看 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/settings.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE" rel="nofollow" title="DOWNLOADER_MIDDLEWARES_BASE">DOWNLOADER_MIDDLEWARES_BASE</a> 设置，而后根据您想要放置中间件的位置选择一个值。 由于每个中间件执行不同的动作，您的中间件可能会依赖于之前(或者之后)执行的中间件，因此顺序是很重要的。</p> 
<p>如果您想禁止内置的(在 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/settings.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE" rel="nofollow" title="DOWNLOADER_MIDDLEWARES_BASE">DOWNLOADER_MIDDLEWARES_BASE</a> 中设置并默认启用的)中间件， 您必须在项目的 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/settings.html#std:setting-DOWNLOADER_MIDDLEWARES" rel="nofollow" title="DOWNLOADER_MIDDLEWARES">DOWNLOADER_MIDDLEWARES</a> 设置中定义该中间件，并将其值赋为 None 。 例如，如果您想要关闭user-agent中间件:</p> 
<pre class="has"><code class="language-html">DOWNLOADER_MIDDLEWARES = {
    'myproject.middlewares.CustomDownloaderMiddleware': 543,
    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,
}
</code></pre> 
<p>最后，请注意，有些中间件需要通过特定的设置来启用。更多内容请查看相关中间件文档。</p> 
<p></p> 
<p></p> 
<h3>编写您自己的下载器中间件</h3> 
<p></p> 
<p>编写下载器中间件十分简单。每个中间件组件是一个定义了以下一个或多个方法的Python类：</p> 
<p><span style="color:#7c79e5;"><strong>class </strong></span>scrapy.downloadermiddlewares.DownloaderMiddleware</p> 
<p></p> 
<p></p> 
<h4><span style="color:#f33b45;"><strong>process_request(request, spider)：</strong></span></h4> 
<p></p> 
<p>当每个 request 通过下载中间件时，该方法被调用。</p> 
<p>参数：</p> 
<ul><li><strong>request</strong> (<a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/request-response.html#scrapy.http.Request" rel="nofollow" title="Request">Request</a> 对象) – 处理的request</li><li><strong>spider</strong> (<a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/spiders.html#scrapy.spiders.Spider" rel="nofollow" title="Spider">Spider</a> 对象) – 该request对应的spider</li></ul> 
<p>返回值：<a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" rel="nofollow" title="process_request()">process_request()</a> 必须返回其中之一:</p> 
<ul><li>返回 <code>None</code> 。如果其返回 <code>None</code> ，Scrapy将继续处理该request，执行其他的中间件的相应方法，直到合适的下载器处理函数(download handler)被调用， 该request被执行(其response被下载)。</li><li>返回一个 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/request-response.html#scrapy.http.Response" rel="nofollow" title="Response">Response</a> 对象。如果其返回 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/request-response.html#scrapy.http.Response" rel="nofollow" title="Response">Response</a> 对象，Scrapy将不会调用 <em>任何</em> 其他的 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" rel="nofollow" title="process_request()">process_request()</a> 或 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" rel="nofollow" title="process_exception()">process_exception()</a> 方法，或相应地下载函数； 其将返回该response。 已安装的中间件的 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" rel="nofollow" title="process_response()">process_response()</a> 方法则会在每个response返回时被调用。</li><li>返回一个 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/request-response.html#scrapy.http.Request" rel="nofollow" title="Request">Request</a> 对象。如果其返回 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/request-response.html#scrapy.http.Request" rel="nofollow" title="Request">Request</a> 对象，Scrapy则停止调用 process_request方法并重新调度返回的request。当新返回的request被执行后， 相应地中间件链将会根据下载的response被调用。</li><li>或 raise <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/exceptions.html#scrapy.exceptions.IgnoreRequest" rel="nofollow" title="IgnoreRequest">IgnoreRequest</a> 。如果其raise一个 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/exceptions.html#scrapy.exceptions.IgnoreRequest" rel="nofollow" title="IgnoreRequest">IgnoreRequest</a> 异常，则安装的下载中间件的 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" rel="nofollow" title="process_exception()">process_exception()</a> 方法会被调用。如果没有任何一个方法处理该异常， 则request的errback(<code>Request.errback</code>)方法会被调用。如果没有代码处理抛出的异常， 则该异常被忽略且不记录(不同于其他异常那样)。</li></ul> 
<p>解释：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/c8/d8/BlKz7fqE_o.png"></p> 
<p></p> 
<p></p> 
<h4><span style="color:#f33b45;"><strong> process_response(request, response, spider)：</strong></span></h4> 
<p>参数：</p> 
<ul><li><strong>request</strong> (<a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/request-response.html#scrapy.http.Request" rel="nofollow" title="Request">Request</a> 对象) – response所对应的request</li><li><strong>response</strong> (<a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/request-response.html#scrapy.http.Response" rel="nofollow" title="Response">Response</a> 对象) – 被处理的response</li><li><strong>spider</strong> (<a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/spiders.html#scrapy.spiders.Spider" rel="nofollow" title="Spider">Spider</a> 对象) – response所对应的spider</li></ul> 
<p>返回值：<a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" rel="nofollow" title="process_request()">process_request()</a> 必须返回以下之一:</p> 
<ul><li>返回一个 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/request-response.html#scrapy.http.Response" rel="nofollow" title="Response">Response</a> 对象。如果其返回一个 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/request-response.html#scrapy.http.Response" rel="nofollow" title="Response">Response</a> (可以与传入的response相同，也可以是全新的对象)， 该response会被在链中的其他中间件的 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" rel="nofollow" title="process_response()">process_response()</a> 方法处理。</li><li>返回一个 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/request-response.html#scrapy.http.Request" rel="nofollow" title="Request">Request</a> 对象。如果其返回一个 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/request-response.html#scrapy.http.Request" rel="nofollow" title="Request">Request</a> 对象，则中间件链停止， 返回的request会被重新调度下载。处理类似于 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" rel="nofollow" title="process_request()">process_request()</a> 返回request所做的那样。</li><li>或 raise 一个 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/exceptions.html#scrapy.exceptions.IgnoreRequest" rel="nofollow" title="IgnoreRequest">IgnoreRequest</a> 异常。如果其抛出一个 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/exceptions.html#scrapy.exceptions.IgnoreRequest" rel="nofollow" title="IgnoreRequest">IgnoreRequest</a> 异常，则调用request的errback(<code>Request.errback</code>)。 如果没有代码处理抛出的异常，则该异常被忽略且不记录(不同于其他异常那样)。</li></ul> 
<p>解释：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/31/70/58BU44I4_o.png"></p> 
<p></p> 
<p></p> 
<h4><span style="color:#f33b45;"><strong>process_exception(request, exception, spider)：</strong></span></h4> 
<p></p> 
<p>当下载处理器 ( download handler ) 或 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" rel="nofollow" title="process_request()">process_request()</a> ( 下载中间件 ) 抛出异常 ( 包括 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/exceptions.html#scrapy.exceptions.IgnoreRequest" rel="nofollow" title="IgnoreRequest">IgnoreRequest</a> 异常) 时， Scrapy 调用 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" rel="nofollow" title="process_exception()">process_exception()</a> 。</p> 
<p>参数：</p> 
<ul><li><strong>request</strong> (是 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/request-response.html#scrapy.http.Request" rel="nofollow" title="Request">Request</a> 对象) – 产生异常的request</li><li><strong>exception</strong> (<code>Exception</code> 对象) – 抛出的异常</li><li><strong>spider</strong> (<a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/spiders.html#scrapy.spiders.Spider" rel="nofollow" title="Spider">Spider</a> 对象) – request对应的spider</li></ul> 
<p>返回值：<a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" rel="nofollow" title="process_exception()">process_exception()</a> 应该返回以下之一:</p> 
<ul><li>返回 <code>None</code> 。如果其返回 <code>None</code> ，Scrapy将会继续处理该异常，接着调用已安装的其他中间件的 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" rel="nofollow" title="process_exception()">process_exception()</a> 方法，直到所有中间件都被调用完毕，则调用默认的异常处理。</li><li>返回一个 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/request-response.html#scrapy.http.Response" rel="nofollow" title="Response">Response</a> 对象。如果其返回一个 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/request-response.html#scrapy.http.Response" rel="nofollow" title="Response">Response</a> 对象，则已安装的中间件链的 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" rel="nofollow" title="process_response()">process_response()</a> 方法被调用。Scrapy将不会调用任何其他中间件的 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" rel="nofollow" title="process_exception()">process_exception()</a> 方法。</li><li>或者一个 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/request-response.html#scrapy.http.Request" rel="nofollow" title="Request">Request</a> 对象。如果其返回一个 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/request-response.html#scrapy.http.Request" rel="nofollow" title="Request">Request</a> 对象， 则返回的request将会被重新调用下载。这将停止中间件的 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" rel="nofollow" title="process_exception()">process_exception()</a> 方法执行，就如返回一个response的那样。</li></ul> 
<p>解释：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/3f/bc/RVgv5XhD_o.png"></p> 
<p></p> 
<p></p> 
<h4><span style="color:#f33b45;"><strong>from_crawler(cls, crawler)：</strong></span></h4> 
<p></p> 
<p>If present, this classmethod is called to create a middleware instance from a <a href="http://doc.scrapy.org/en/latest/topics/api.html#scrapy.crawler.Crawler" rel="nofollow" title="Crawler">Crawler</a>. It must return a new instance of the middleware. Crawler object provides access to all Scrapy core components like settings and signals; it is a way for middleware to access them and hook its functionality into Scrapy.</p> 
<table><tbody><tr><th>Parameters:</th><td><strong>crawler</strong> (<a href="http://doc.scrapy.org/en/latest/topics/api.html#scrapy.crawler.Crawler" rel="nofollow" title="Crawler">Crawler</a> object) – crawler that uses this middleware</td></tr></tbody></table> 
<p><strong>scrapy 中的 from_crawler</strong>：<a href="https://www.jianshu.com/p/e9ec5d7b6204" rel="nofollow" title="（1）scrapy中的from_crawler - 简书">（1）scrapy中的from_crawler - 简书</a></p> 
<p>解释：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/9a/d3/xQzH7ZFC_o.png"></p> 
<p></p> 
<p></p> 
<p></p> 
<h4>示例：代理中间件</h4> 
<p></p> 
<p>在爬虫开发中，更换代理IP是非常常见的情况，有时候每一次访问都需要随机选择一个代理IP来进行。<br> 中间件本身是一个Python的类，只要爬虫每次访问网站之前都先“经过”这个类，它就能给请求换新的代理IP，这样就能实现动态改变代理。<br> 在创建一个Scrapy工程以后，工程文件夹下会有一个 middlewares.py 文件，打开以后其内容如下图所示。</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/da/fe/5YWxTq7R_o.jpg"></p> 
<p>Scrapy 自动生成的这个文件名称为 middlewares.py，名字后面的 s 表示复数，说明这个文件里面可以放很多个中间件。可以看到有一个  <span style="color:#f33b45;"><strong>SpiderMiddleware </strong></span>（爬虫中间件）中间件 和 <span style="color:#f33b45;"><strong>DownloaderMiddleware </strong></span>（下载中间件）中间件</p> 
<p></p> 
<p>在middlewares.py中添加下面一段代码（可以在 <strong>下载中间件这个类</strong> 里面写，也可以把 <strong>爬虫中间件</strong> 和 <strong>下载中间件</strong> 这两个类删了，自己写个 下载中间件的类。推荐 自己单写一个类 作为 下载中间件）：</p> 
<pre class="has"><code class="language-python"># -*- coding: utf-8 -*-

# Define here the models for your spider middleware
#
# See documentation in:
# https://doc.scrapy.org/en/latest/topics/spider-middleware.html


import random
from scrapy.conf import settings
from scrapy.utils.project import get_project_settings


class ProxyMiddleware(object):

    def process_request(self, request, spider):
        proxy_1 = random.choice(settings['PROXIES'])  # 方法 1
        proxy_2 = random.choice(get_project_settings()['PROXIES'])  # 方法 2
        request.meta['proxy'] = proxy_1</code></pre> 
<p>打开 setting.py 添加 代理 ，并激活 这个代理中间件：</p> 
<p>需要注意的是，代理IP是有类型的，需要先看清楚是 HTTP型 的代理IP还是 HTTPS型 的代理IP。</p> 
<pre class="has"><code>DOWNLOADER_MIDDLEWARES = {
    'test_spider.middlewares.ProxyMiddleware': 543,
    # 'test_spider.middlewares.Custom_B_DownloaderMiddleware': 643,
    # 'test_spider.middlewares.Custom_B_DownloaderMiddleware': None,
}

PROXIES = [
    'https://114.217.243.25:8118',
    'https://125.37.175.233:8118',
    'http://1.85.116.218:8118'
]</code></pre> 
<p><span style="color:#f33b45;"><strong>DOWNLOADER_MIDDLEWARES </strong>其实就是一个字典</span>，字典的Key就是用点分隔的中间件路径，后面的数字表示这种中间件的顺序。由于中间件是按顺序运行的，因此如果遇到后一个中间件依赖前一个中间件的情况，中间件的顺序就至关重要。<br> 如何确定后面的数字应该怎么写呢？最简单的办法就是从543开始，逐渐加一，这样一般不会出现什么大问题。如果想把中间件做得更专业一点，那就需要知道Scrapy自带中间件的顺序，如图下图所示 （ <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/settings.html#std:setting-DOWNLOADER_MIDDLEWARES" rel="nofollow" title="DOWNLOADER_MIDDLEWARES">DOWNLOADER_MIDDLEWARES</a> ）。</p> 
<p style="text-align:center;"><img alt="" class="has" src="https://images2.imgbox.com/a2/d5/VSY68Eeg_o.jpg"></p> 
<p><span style="color:#f33b45;"><strong>数字越小的中间件越先执行</strong></span><span style="color:#7c79e5;"><strong>（数字越小，越靠近引擎，数字越大越靠近下载器，所以数字越小的，processrequest()优先处理；数字越大的，process_response()优先处理；若需要关闭某个中间件直接设为None即可）</strong></span>，例如Scrapy自带的第1个中间件RobotsTxtMiddleware，它的作用是首先查看settings.py中ROBOTSTXT_OBEY 这一项的配置是True还是False。如果是True，表示要遵守Robots.txt协议，它就会检查将要访问的网址能不能被运行访问，如果不被允许访问，那么直接就取消这一次请求，接下来的和这次请求有关的各种操作全部都不需要继续了。<br> 开发者自定义的中间件，会被按顺序插入到Scrapy自带的中间件中。爬虫会按照从100～900的顺序依次运行所有的中间件。直到所有中间件全部运行完成，或者遇到某一个中间件而取消了这次请求。</p> 
<p></p> 
<p>Scrapy 其实自带了 UA 中间件（UserAgentMiddleware）、代理中间件（HttpProxyMiddleware）和重试中间件（RetryMiddleware）。所以，从“原则上”说，要自己开发这3个中间件，需要先禁用Scrapy里面自带的这3个中间件。要禁用Scrapy的中间件，需要在settings.py里面将这个中间件的顺序设为None：</p> 
<pre class="has"><code>DOWNLOADER_MIDDLEWARES = {
    'test_spider.middlewares.ProxyMiddleware': 543,
    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': None,
    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': None	
}

PROXIES = [
    'https://114.217.243.25:8118',
    'https://125.37.175.233:8118',
    'http://1.85.116.218:8118'
]</code></pre> 
<p>为什么说“原则上”应该禁用呢？先查看Scrapy自带的代理中间件的源代码，如下图所示：</p> 
<p style="text-align:center;"><img alt="" class="has" src="https://images2.imgbox.com/b8/d6/U3YqpsGF_o.jpg"></p> 
<p>从上图可以看出，如果Scrapy发现这个请求已经被设置了代理，那么这个中间件就会什么也不做，直接返回。因此虽然Scrapy自带的这个代理中间件顺序为750，比开发者自定义的代理中间件的顺序543大，但是它并不会覆盖开发者自己定义的代理信息，所以即使不禁用系统自带的这个代理中间件也没有关系。</p> 
<p><strong>代理中间件的可用代理列表不一定非要写在settings.py里面，也可以将它们写到数据库或者Redis中。一个可行的自动更换代理的爬虫系统，应该有如下的3个功能。</strong></p> 
<ul><li>1. 有一个小爬虫ProxySpider去各大代理网站爬取免费代理并验证，将可以使用的代理IP保存到数据库中。</li><li>2. 在ProxyMiddlerware的process_request中，每次从数据库里面随机选择一条代理IP地址使用。</li><li>3. 周期性验证数据库中的无效代理，及时将其删除。由于免费代理极其容易失效，因此如果有一定开发预算的话，建议购买专业代理机构的代理服务，高速而稳定。</li></ul> 
<p></p> 
<p><strong>scrapy 中对接 selenium</strong></p> 
<pre class="has"><code class="language-python">from scrapy.http import HtmlResponse
from selenium import webdriver
from selenium.common.exceptions import TimeoutException
from gp.configs import *


class ChromeDownloaderMiddleware(object):

    def __init__(self):
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')  # 设置无界面
        if CHROME_PATH:
            options.binary_location = CHROME_PATH
        if CHROME_DRIVER_PATH:
            self.driver = webdriver.Chrome(chrome_options=options, executable_path=CHROME_DRIVER_PATH)  # 初始化Chrome驱动
        else:
            self.driver = webdriver.Chrome(chrome_options=options)  # 初始化Chrome驱动

    def __del__(self):
        self.driver.close()

    def process_request(self, request, spider):
        try:
            print('Chrome driver begin...')
            self.driver.get(request.url)  # 获取网页链接内容
            return HtmlResponse(url=request.url, body=self.driver.page_source, request=request, encoding='utf-8',
                                status=200)  # 返回HTML数据
        except TimeoutException:
            return HtmlResponse(url=request.url, request=request, encoding='utf-8', status=500)
        finally:
            print('Chrome driver end...')</code></pre> 
<p></p> 
<p></p> 
<h4>示例：UA (user-agent) 中间件</h4> 
<p></p> 
<p>Scrapy学习篇（十一）之设置随机User-Agent：<a href="https://www.cnblogs.com/cnkai/p/7401343.html" rel="nofollow" title="https://www.cnblogs.com/cnkai/p/7401343.html">https://www.cnblogs.com/cnkai/p/7401343.html</a></p> 
<p>开发UA中间件和开发代理中间件几乎一样，它也是从 settings.py 配置好的 UA 列表中随机选择一项，加入到请求头中。代码如下：</p> 
<pre class="has"><code class="language-python">class UAMiddleware(object):

    def process_request(self, request, spider):
        ua = random.choice(settings['USER_AGENT_LIST'])
        request.headers['User-Agent'] = ua</code></pre> 
<p>比IP更好的是，UA不会存在失效的问题，所以只要收集几十个UA，就可以一直使用。常见的UA如下：</p> 
<pre class="has"><code class="language-python">USER_AGENT_LIST = [
    "Mozilla/5.0 (Linux; U; Android 2.3.6; en-us; Nexus S Build/GRK39F) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1",
    "Avant Browser/1.2.789rel1 (http://www.avantbrowser.com)",
    "Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.5 (KHTML, like Gecko) Chrome/4.0.249.0 Safari/532.5",
    "Mozilla/5.0 (Windows; U; Windows NT 5.2; en-US) AppleWebKit/532.9 (KHTML, like Gecko) Chrome/5.0.310.0 Safari/532.9",
    "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/534.7 (KHTML, like Gecko) Chrome/7.0.514.0 Safari/534.7",
    "Mozilla/5.0 (Windows; U; Windows NT 6.0; en-US) AppleWebKit/534.14 (KHTML, like Gecko) Chrome/9.0.601.0 Safari/534.14",
    "Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.14 (KHTML, like Gecko) Chrome/10.0.601.0 Safari/534.14",
    "Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.20 (KHTML, like Gecko) Chrome/11.0.672.2 Safari/534.20",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.27 (KHTML, like Gecko) Chrome/12.0.712.0 Safari/534.27",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.24 Safari/535.1",
    "Mozilla/5.0 (Windows NT 6.0) AppleWebKit/535.2 (KHTML, like Gecko) Chrome/15.0.874.120 Safari/535.2",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.7 (KHTML, like Gecko) Chrome/16.0.912.36 Safari/535.7",
    "Mozilla/5.0 (Windows; U; Windows NT 6.0 x64; en-US; rv:1.9pre) Gecko/2008072421 Minefield/3.0.2pre",
    "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.10) Gecko/2009042316 Firefox/3.0.10",
    "Mozilla/5.0 (Windows; U; Windows NT 6.0; en-GB; rv:1.9.0.11) Gecko/2009060215 Firefox/3.0.11 (.NET CLR 3.5.30729)",
    "Mozilla/5.0 (Windows; U; Windows NT 6.0; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6 GTB5",
    "Mozilla/5.0 (Windows; U; Windows NT 5.1; tr; rv:1.9.2.8) Gecko/20100722 Firefox/3.6.8 ( .NET CLR 3.5.30729; .NET4.0E)",
    "Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1",
    "Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:2.0.1) Gecko/20100101 Firefox/4.0.1",
    "Mozilla/5.0 (Windows NT 5.1; rv:5.0) Gecko/20100101 Firefox/5.0",
    "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:6.0a2) Gecko/20110622 Firefox/6.0a2",
    "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:7.0.1) Gecko/20100101 Firefox/7.0.1",
    "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:2.0b4pre) Gecko/20100815 Minefield/4.0b4pre",
    "Mozilla/4.0 (compatible; MSIE 5.5; Windows NT 5.0 )",
    "Mozilla/4.0 (compatible; MSIE 5.5; Windows 98; Win 9x 4.90)",
    "Mozilla/5.0 (Windows; U; Windows XP) Gecko MultiZilla/1.6.1.0a",
    "Mozilla/2.02E (Win95; U)",
    "Mozilla/3.01Gold (Win95; I)",
    "Mozilla/4.8 [en] (Windows NT 5.1; U)",
    "Mozilla/5.0 (Windows; U; Win98; en-US; rv:1.4) Gecko Netscape/7.1 (ax)",
    "HTC_Dream Mozilla/5.0 (Linux; U; Android 1.5; en-ca; Build/CUPCAKE) AppleWebKit/528.5  (KHTML, like Gecko) Version/3.1.2 Mobile Safari/525.20.1",
    "Mozilla/5.0 (hp-tablet; Linux; hpwOS/3.0.2; U; de-DE) AppleWebKit/534.6 (KHTML, like Gecko) wOSBrowser/234.40.1 Safari/534.6 TouchPad/1.0",
    "Mozilla/5.0 (Linux; U; Android 1.5; en-us; sdk Build/CUPCAKE) AppleWebkit/528.5  (KHTML, like Gecko) Version/3.1.2 Mobile Safari/525.20.1",
    "Mozilla/5.0 (Linux; U; Android 2.1; en-us; Nexus One Build/ERD62) AppleWebKit/530.17 (KHTML, like Gecko) Version/4.0 Mobile Safari/530.17",
    "Mozilla/5.0 (Linux; U; Android 2.2; en-us; Nexus One Build/FRF91) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1",
    "Mozilla/5.0 (Linux; U; Android 1.5; en-us; htc_bahamas Build/CRB17) AppleWebKit/528.5  (KHTML, like Gecko) Version/3.1.2 Mobile Safari/525.20.1",
    "Mozilla/5.0 (Linux; U; Android 2.1-update1; de-de; HTC Desire 1.19.161.5 Build/ERE27) AppleWebKit/530.17 (KHTML, like Gecko) Version/4.0 Mobile Safari/530.17",
    "Mozilla/5.0 (Linux; U; Android 2.2; en-us; Sprint APA9292KT Build/FRF91) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1",
    "Mozilla/5.0 (Linux; U; Android 1.5; de-ch; HTC Hero Build/CUPCAKE) AppleWebKit/528.5  (KHTML, like Gecko) Version/3.1.2 Mobile Safari/525.20.1",
    "Mozilla/5.0 (Linux; U; Android 2.2; en-us; ADR6300 Build/FRF91) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1",
    "Mozilla/5.0 (Linux; U; Android 2.1; en-us; HTC Legend Build/cupcake) AppleWebKit/530.17 (KHTML, like Gecko) Version/4.0 Mobile Safari/530.17",
    "Mozilla/5.0 (Linux; U; Android 1.5; de-de; HTC Magic Build/PLAT-RC33) AppleWebKit/528.5  (KHTML, like Gecko) Version/3.1.2 Mobile Safari/525.20.1 FirePHP/0.3",
    "Mozilla/5.0 (Linux; U; Android 1.6; en-us; HTC_TATTOO_A3288 Build/DRC79) AppleWebKit/528.5  (KHTML, like Gecko) Version/3.1.2 Mobile Safari/525.20.1",
    "Mozilla/5.0 (Linux; U; Android 1.0; en-us; dream) AppleWebKit/525.10  (KHTML, like Gecko) Version/3.0.4 Mobile Safari/523.12.2",
    "Mozilla/5.0 (Linux; U; Android 1.5; en-us; T-Mobile G1 Build/CRB43) AppleWebKit/528.5  (KHTML, like Gecko) Version/3.1.2 Mobile Safari 525.20.1",
    "Mozilla/5.0 (Linux; U; Android 1.5; en-gb; T-Mobile_G2_Touch Build/CUPCAKE) AppleWebKit/528.5  (KHTML, like Gecko) Version/3.1.2 Mobile Safari/525.20.1",
    "Mozilla/5.0 (Linux; U; Android 2.0; en-us; Droid Build/ESD20) AppleWebKit/530.17 (KHTML, like Gecko) Version/4.0 Mobile Safari/530.17",
    "Mozilla/5.0 (Linux; U; Android 2.2; en-us; Droid Build/FRG22D) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1",
    "Mozilla/5.0 (Linux; U; Android 2.0; en-us; Milestone Build/ SHOLS_U2_01.03.1) AppleWebKit/530.17 (KHTML, like Gecko) Version/4.0 Mobile Safari/530.17",
    "Mozilla/5.0 (Linux; U; Android 2.0.1; de-de; Milestone Build/SHOLS_U2_01.14.0) AppleWebKit/530.17 (KHTML, like Gecko) Version/4.0 Mobile Safari/530.17",
    "Mozilla/5.0 (Linux; U; Android 3.0; en-us; Xoom Build/HRI39) AppleWebKit/525.10  (KHTML, like Gecko) Version/3.0.4 Mobile Safari/523.12.2",
    "Mozilla/5.0 (Linux; U; Android 0.5; en-us) AppleWebKit/522  (KHTML, like Gecko) Safari/419.3",
    "Mozilla/5.0 (Linux; U; Android 1.1; en-gb; dream) AppleWebKit/525.10  (KHTML, like Gecko) Version/3.0.4 Mobile Safari/523.12.2",
    "Mozilla/5.0 (Linux; U; Android 2.0; en-us; Droid Build/ESD20) AppleWebKit/530.17 (KHTML, like Gecko) Version/4.0 Mobile Safari/530.17",
    "Mozilla/5.0 (Linux; U; Android 2.1; en-us; Nexus One Build/ERD62) AppleWebKit/530.17 (KHTML, like Gecko) Version/4.0 Mobile Safari/530.17",
    "Mozilla/5.0 (Linux; U; Android 2.2; en-us; Sprint APA9292KT Build/FRF91) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1",
    "Mozilla/5.0 (Linux; U; Android 2.2; en-us; ADR6300 Build/FRF91) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1",
    "Mozilla/5.0 (Linux; U; Android 2.2; en-ca; GT-P1000M Build/FROYO) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1",
    "Mozilla/5.0 (Linux; U; Android 3.0.1; fr-fr; A500 Build/HRI66) AppleWebKit/534.13 (KHTML, like Gecko) Version/4.0 Safari/534.13",
    "Mozilla/5.0 (Linux; U; Android 3.0; en-us; Xoom Build/HRI39) AppleWebKit/525.10  (KHTML, like Gecko) Version/3.0.4 Mobile Safari/523.12.2",
    "Mozilla/5.0 (Linux; U; Android 1.6; es-es; SonyEricssonX10i Build/R1FA016) AppleWebKit/528.5  (KHTML, like Gecko) Version/3.1.2 Mobile Safari/525.20.1",
    "Mozilla/5.0 (Linux; U; Android 1.6; en-us; SonyEricssonX10i Build/R1AA056) AppleWebKit/528.5  (KHTML, like Gecko) Version/3.1.2 Mobile Safari/525.20.1",
]</code></pre> 
<p>test_spider.py （<span style="color:#f33b45;"><strong>使用 的是 scrapy-redis 的 RedisSpider，需要从 redis 读取 url</strong></span>）：</p> 
<pre class="has"><code class="language-python">#!/usr/bin/python3
# -*- coding: utf-8 -*-
# @Author      : 
# @File        : mao_yan_spider.py
# @Software    : PyCharm
# @description : XXX


from scrapy import Spider
from scrapy_redis.spiders import RedisSpider


class TestSpider(RedisSpider):
    name = 'test'
    redis_key = 'start_urls:{0}'.format(name)
    # start_urls = ['http://exercise.kingname.info/exercise_middleware_ua']

    def parse(self, response):
        print('response text : {0}'.format(response.text))
        pass
</code></pre> 
<p>setting.py 配置 （）：</p> 
<p style="text-align:center;"><img alt="" class="has" src="https://images2.imgbox.com/02/5e/nXGLiM8v_o.jpg"></p> 
<p>test.py:</p> 
<pre class="has"><code class="language-python">#!/usr/bin/python3
# -*- coding: utf-8 -*-
# @Author      : 
# @File        : test_s.py
# @Software    : PyCharm
# @description : XXX


import redis
from scrapy import cmdline


def add_test_task():
    r = redis.Redis(host='127.0.0.1', port=6379)
    for i in range(10):
        url = 'http://exercise.kingname.info/exercise_middleware_ua'
        r.lpush('start_urls:test', url)


if __name__ == "__main__":
    add_test_task()
    cmdline.execute("scrapy crawl test".split())
    pass
</code></pre> 
<p>运行 test.py 截图：</p> 
<p style="text-align:center;"><img alt="" class="has" src="https://images2.imgbox.com/a4/05/0KiFbKmI_o.jpg"></p> 
<p>可以看到 请求的 user-agent 是变化的。</p> 
<pre class="has"><code>from faker import Faker

class UserAgent_Middleware():

    def process_request(self, request, spider):
        f = Faker()
        agent = f.firefox()
        request.headers['User-Agent'] = agent</code></pre> 
<p></p> 
<h4 style="background-color:transparent;">示例：对接 selenium</h4> 
<pre><code class="language-python">from scrapy.http import HtmlResponse
from selenium import webdriver
import time

class SeleniumMiddleware(object):
    def process_request(self, request, spider):
        url = request.url
        browser = webdriver.Chrome()
        browser.get(url)
        time,sleep(5)
        html = browser.page source
        browser,close()
        return HtmlResponse(
            url=request.url,
            body=html,
            request=request,
            encoding='utf-8,
            status=200
        )</code></pre> 
<p>上面实现的 SeleniumMiddleware 太简单，有一下缺点：</p> 
<ul><li>Chrome初始化的时候没有指定任何参数比如 headless、proxy等，而且没有把参数可配置化</li><li>没有实现异常处理，比如出现TimeException后如何进行重试。</li><li>加载过程简单指定了固定的等待时间，没有设置等待某一特定节点。</li><li>没有设置Cookie、执行JavaScript、截图等一系列扩展功能。</li><li>整个爬取过程变成了阻塞式爬取，同一时刻只有一个页面能被爬取，爬取效率大大降低</li></ul> 
<p>催佬写了一个Python 包，对以上的 SeleniumMiddleware 做了一些优化</p> 
<ul><li>Chrome的初始化参数可配置，可以通过全局settings配置或Request对象配置</li><li>实现了异常处理，出现了加载异常会按照 Scrapy 的重试逻辑进行重试。</li><li>加载过程可以指定特定节点进行等待，节点加载出来之后立即继续向下执行。</li><li>增加了设置 Cookie、执行 JavaScript、截图、代理设置等一系列功能并将参数可配置化。</li><li>将爬取过程改为非阻塞式，同一时刻支持多个浏览器同时加载并可通过 CONCURRENTREQUESTS 控制。</li><li>增加了SeleniumRequest，定义Request更加方便，而且支持多个扩展参数。</li><li>增加了 WebDriver 反屏蔽功能，将浏览器伪装成正常的浏览器防止被检测</li></ul> 
<p>包名叫 GerapySelenium，安装：pip3 install gerapy-selenium<br> 安装后只需要启用对应的 Downloader Middleware 并改写 Request 为 SeleniumRequest 即可;<br> DOWNLOADER_MIDDLEWARES = {<!-- --><br>     'gerapy_selenium.downloadermiddlewares.SeleniumMiddleware':543<br> }</p> 
<p>CONCURRENTREQUESTS = 6<br> 将并发量修改为了6，这样在爬取过程中就会同时使用 Chrome 渲染6个页面了，如果电脑性能还可以的话，可以将数字调得更大些。</p> 
<p>在Spider中，还需要修改 Request 为 SeleniumRequest，同时还可以增加一些其他的配置，比如通过 wait_for 来等待某一特定节点加载出来，比如原来的:<br> yield Request(start url,callback=self.parse_index)<br> 就可以修改为:<br> yield SeleniumRequest(start_url, callback=self.parse_index,wait_for='.item .name')</p> 
<p>示例：<a class="link-info" href="https://github.com/Python3WebSpider/ScrapySeleniumDemo" title="https://github.com/Python3WebSpider/ScrapySeleniumDemo">https://github.com/Python3WebSpider/ScrapySeleniumDemo</a></p> 
<p></p> 
<h4>示例：Scrapy 对接 Splash</h4> 
<p>：<a class="link-info" href="https://github.com/Python3WebSpider/ScrapySplashDemo" title="https://github.com/Python3WebSpider/ScrapySplashDemo">https://github.com/Python3WebSpider/ScrapySplashDemo</a></p> 
<p></p> 
<h4>示例：Scrapy 对接 Pyppeteer</h4> 
<p>：<a class="link-info" href="https://github.com/Python3WebSpider/ScrapyPyppeteerDemo" title="https://github.com/Python3WebSpider/ScrapyPyppeteerDemo">https://github.com/Python3WebSpider/ScrapyPyppeteerDemo</a></p> 
<p></p> 
<h4 id="开发cookies中间件">Cookies 中间件</h4> 
<p>对于需要登录的网站，可以使用Cookies来保持登录状态。那么如果单独写一个小程序，用Selenium持续不断地用不同的账号登录网站，就可以得到很多不同的Cookies。由于Cookies本质上就是一段文本，所以可以把这段文本放在Redis里面。这样一来，当Scrapy爬虫请求网页时，可以从Redis中读取Cookies并给爬虫换上。这样爬虫就可以一直保持登录状态。</p> 
<p>以下面这个练习页面为例：<a href="http://exercise.kingname.info/exercise_login_success" rel="nofollow" title="exercise login">exercise login</a></p> 
<p>如果直接用Scrapy访问，得到的是登录界面的源代码，如下图所示。</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/97/13/3jsSR1uv_o.png"></p> 
<p>现在，使用中间件，可以实现完全不改动这个loginSpider.py里面的代码，就打印出登录以后才显示的内容。</p> 
<p>首先开发一个小程序，通过Selenium登录这个页面，并将网站返回的Headers保存到Redis中。这个小程序的代码如下图所示。</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/0f/69/N55PdbYZ_o.png"></p> 
<p>这段代码的作用是使用Selenium和ChromeDriver填写用户名和密码，实现登录练习页面，然后将登录以后的Cookies转换为JSON格式的字符串并保存到Redis中。</p> 
<p>接下来，再写一个中间件，用来从Redis中读取Cookies，并把这个Cookies给Scrapy使用：</p> 
<pre class="has"><code class="language-python">class LoginMiddleware(object):
    def __init__(self):
        self.client = redis.StrictRedis()
    
    def process_request(self, request, spider):
        if spider.name == 'loginSpider':
            cookies = json.loads(self.client.lpop('cookies').decode())
            request.cookies = cookies</code></pre> 
<p>设置了这个中间件以后，爬虫里面的代码不需要做任何修改就可以成功得到登录以后才能看到的HTML，如图12-12所示。</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/c9/9f/8APyQKwn_o.png"></p> 
<p>如果有某网站的100个账号，那么单独写一个程序，持续不断地用Selenium和ChromeDriver或者Selenium 和PhantomJS登录，获取Cookies，并将Cookies存放到Redis中。爬虫每次访问都从Redis中读取一个新的Cookies来进行爬取，就大大降低了被网站发现或者封锁的可能性。</p> 
<p>这种方式不仅适用于登录，也适用于验证码的处理。</p> 
<p></p> 
<p></p> 
<h3>内置 下载中间件 参考手册</h3> 
<p></p> 
<p>本页面介绍了Scrapy自带的所有下载中间件。关于如何使用及编写您自己的中间件，请参考 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#topics-downloader-middleware" rel="nofollow" title="downloader middleware usage guide">downloader middleware usage guide</a>.</p> 
<p>关于默认启用的中间件列表(及其顺序)请参考 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/settings.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE" rel="nofollow" title="DOWNLOADER_MIDDLEWARES_BASE">DOWNLOADER_MIDDLEWARES_BASE</a> 设置。</p> 
<p></p> 
<p></p> 
<h4>CookiesMiddleware</h4> 
<p></p> 
<p><span style="color:#7c79e5;"><strong>class </strong></span>scrapy.downloadermiddlewares.cookies.CookiesMiddleware</p> 
<p>该中间件使得爬取需要cookie(例如使用session)的网站成为了可能。 其追踪了web server发送的cookie，并在之后的request中发送回去， 就如浏览器所做的那样。</p> 
<p>以下设置可以用来配置cookie中间件:</p> 
<ul><li><a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#std:setting-COOKIES_ENABLED" rel="nofollow" title="COOKIES_ENABLED">COOKIES_ENABLED</a></li><li><a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#std:setting-COOKIES_DEBUG" rel="nofollow" title="COOKIES_DEBUG">COOKIES_DEBUG</a></li></ul> 
<p>每个 spider 多 cookie session</p> 
<p>0.15 新版功能.</p> 
<p>Scrapy 通过使用 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#std:reqmeta-cookiejar" rel="nofollow" title="cookiejar">cookiejar</a> 作为 Request meta 的 key 来支持单 spider 追踪多 cookie session。 默认情况下其使用一个 cookie jar(session)，不过您可以传递一个标示符来使用多个。</p> 
<p>例如 （ yield 的 每个 Request 都 有一个 meta={'<span style="color:#f33b45;"><strong>cookiejar</strong></span>': i} ，cookiejar 这个字段的 值只是一个标识，只要不为 None 就行，通常 设置为一个整数 。例如 1，或者 True）:</p> 
<pre class="has"><code>for i, url in enumerate(urls): 
    yield scrapy.Request(
        url="http://www.example.com", 
        meta={'cookiejar': i}, 
        callback=self.parse_page
    )</code></pre> 
<p>需要注意的是 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#std:reqmeta-cookiejar" rel="nofollow" title="cookiejar">cookiejar</a> meta key不是”黏性的(sticky)”。 您需要在之后的 每个 request 请求中接着传递。例如:</p> 
<pre class="has"><code class="language-python">def parse_page(self, response):
    # do some processing
    return scrapy.Request("http://www.example.com/otherpage",
        meta={'cookiejar': response.meta['cookiejar']},
        callback=self.parse_other_page)</code></pre> 
<p></p> 
<p><strong>COOKIES_ENABLED</strong></p> 
<p>默认: <code>True</code></p> 
<p>是否启用 cookies middleware。如果关闭，cookies 将不会发送给 web server。</p> 
<p><strong>COOKIES_DEBUG</strong></p> 
<p>默认: <code>False</code></p> 
<p>如果启用，Scrapy将记录所有在request(<code>Cookie</code> 请求头)发送的cookies及response接收到的cookies(<code>Set-Cookie</code> 接收头)。</p> 
<p>下边是启用 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#std:setting-COOKIES_DEBUG" rel="nofollow" title="COOKIES_DEBUG">COOKIES_DEBUG</a> 的记录的样例:</p> 
<pre class="has"><code class="language-html">2011-04-06 14:35:10-0300 [scrapy] INFO: Spider opened
2011-04-06 14:35:10-0300 [scrapy] DEBUG: Sending cookies to: &lt;GET http://www.diningcity.com/netherlands/index.html&gt;
        Cookie: clientlanguage_nl=en_EN
2011-04-06 14:35:14-0300 [scrapy] DEBUG: Received cookies from: &lt;200 http://www.diningcity.com/netherlands/index.html&gt;
        Set-Cookie: JSESSIONID=B~FA4DC0C496C8762AE4F1A620EAB34F38; Path=/
        Set-Cookie: ip_isocode=US
        Set-Cookie: clientlanguage_nl=en_EN; Expires=Thu, 07-Apr-2011 21:21:34 GMT; Path=/
2011-04-06 14:49:50-0300 [scrapy] DEBUG: Crawled (200) &lt;GET http://www.diningcity.com/netherlands/index.html&gt; (referer: None)
[...]
</code></pre> 
<p></p> 
<p></p> 
<h4>DefaultHeadersMiddleware</h4> 
<p></p> 
<p><em>class </em><code>scrapy.downloadermiddlewares.defaultheaders.</code><code>DefaultHeadersMiddleware</code></p> 
<p>该中间件设置 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/settings.html#std:setting-DEFAULT_REQUEST_HEADERS" rel="nofollow" title="DEFAULT_REQUEST_HEADERS">DEFAULT_REQUEST_HEADERS</a> 指定的默认request header。</p> 
<p></p> 
<p></p> 
<h4>DownloadTimeoutMiddleware</h4> 
<p></p> 
<p><em>class </em><code>scrapy.downloadermiddlewares.downloadtimeout.</code><code>DownloadTimeoutMiddleware</code></p> 
<p>该中间件设置 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/settings.html#std:setting-DOWNLOAD_TIMEOUT" rel="nofollow" title="DOWNLOAD_TIMEOUT">DOWNLOAD_TIMEOUT</a> 指定的request下载超时时间.</p> 
<p>Note</p> 
<p>You can also set download timeout per-request using <a href="http://doc.scrapy.org/en/latest/topics/request-response.html#std:reqmeta-download_timeout" rel="nofollow" title="download_timeout">download_timeout</a> Request.meta key; this is supported even when DownloadTimeoutMiddleware is disabled.</p> 
<p></p> 
<p></p> 
<h4>HttpAuthMiddleware</h4> 
<p></p> 
<p><em>class </em><code>scrapy.downloadermiddlewares.httpauth.</code><code>HttpAuthMiddleware</code></p> 
<p>该中间件完成某些使用 <a href="http://en.wikipedia.org/wiki/Basic_access_authentication" rel="nofollow" title="Basic access authentication">Basic access authentication</a> (或者叫HTTP认证)的spider生成的请求的认证过程。</p> 
<p>在 spider 中启用 HTTP 认证，请设置 spider 的 <code>http_user</code> 及 <code>http_pass</code> 属性。</p> 
<p>样例:</p> 
<pre class="has"><code class="language-python">from scrapy.spiders import CrawlSpider

class SomeIntranetSiteSpider(CrawlSpider):

    http_user = 'someuser'
    http_pass = 'somepass'
    name = 'intranet.example.com'

    # .. rest of the spider code omitted ...</code></pre> 
<p></p> 
<p></p> 
<h4>HttpCacheMiddleware</h4> 
<p></p> 
<p><em>class </em><code>scrapy.downloadermiddlewares.httpcache.</code><code>HttpCacheMiddleware</code></p> 
<p>该中间件为所有HTTP request及response提供了底层(low-level)缓存支持。 其由cache存储后端及cache策略组成。</p> 
<p>Scrapy提供了两种HTTP缓存存储后端:</p> 
<blockquote> 
 <ul><li><a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#httpcache-storage-fs" rel="nofollow" title="Filesystem storage backend (默认值)">Filesystem storage backend (默认值)</a></li><li><a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#httpcache-storage-dbm" rel="nofollow" title="DBM storage backend">DBM storage backend</a></li></ul> 
</blockquote> 
<p>您可以使用 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#std:setting-HTTPCACHE_STORAGE" rel="nofollow" title="HTTPCACHE_STORAGE">HTTPCACHE_STORAGE</a> 设定来修改HTTP缓存存储后端。 您也可以实现您自己的存储后端。</p> 
<p>Scrapy提供了两种了缓存策略:</p> 
<blockquote> 
 <ul><li><a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#httpcache-policy-rfc2616" rel="nofollow" title="RFC2616策略">RFC2616策略</a></li><li><a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#httpcache-policy-dummy" rel="nofollow" title="Dummy策略(默认值)">Dummy策略(默认值)</a></li></ul> 
</blockquote> 
<p>您可以使用 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#std:setting-HTTPCACHE_POLICY" rel="nofollow" title="HTTPCACHE_POLICY">HTTPCACHE_POLICY</a> 设定来修改HTTP缓存存储后端。 您也可以实现您自己的存储策略。</p> 
<p></p> 
<p><strong>Dummy策略(默认值)</strong></p> 
<p>该策略不考虑任何HTTP Cache-Control指令。每个request及其对应的response都被缓存。 当相同的request发生时，其不发送任何数据，直接返回response。</p> 
<p>Dummpy策略对于测试spider十分有用。其能使spider运行更快(不需要每次等待下载完成)， 同时在没有网络连接时也能测试。其目的是为了能够回放spider的运行过程， <em>使之与之前的运行过程一模一样</em> 。</p> 
<p>使用这个策略请设置:</p> 
<ul><li><a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#std:setting-HTTPCACHE_POLICY" rel="nofollow" title="HTTPCACHE_POLICY">HTTPCACHE_POLICY</a> 为 <code>scrapy.extensions.httpcache.DummyPolicy</code></li></ul> 
<p></p> 
<p><strong>RFC2616策略</strong></p> 
<p>该策略提供了符合RFC2616的HTTP缓存，例如符合HTTP Cache-Control， 针对生产环境并且应用在持续性运行环境所设置。该策略能避免下载未修改的数据(来节省带宽，提高爬取速度)。</p> 
<p>实现了:</p> 
<ul><li> <p>当 no-store cache-control指令设置时不存储response/request。</p> </li><li> <p>当 no-cache cache-control指定设置时不从cache中提取response，即使response为最新。</p> </li><li> <p>根据 max-age cache-control指令中计算保存时间(freshness lifetime)。</p> </li><li> <p>根据 Expires 指令来计算保存时间(freshness lifetime)。</p> </li><li> <p>根据response包头的 Last-Modified 指令来计算保存时间(freshness lifetime)(Firefox使用的启发式算法)。</p> </li><li> <p>根据response包头的 Age 计算当前年龄(current age)</p> </li><li> <p>根据 Date 计算当前年龄(current age)</p> </li><li> <p>根据response包头的 Last-Modified 验证老旧的response。</p> </li><li> <p>根据response包头的 ETag 验证老旧的response。</p> </li><li> <p>为接收到的response设置缺失的 Date 字段。</p> </li><li> <p>支持request中cache-control指定的 max-stale</p> <p>通过该字段，使得spider完整支持了RFC2616缓存策略，但避免了多次请求下情况下的重验证问题(revalidation on a request-by-request basis). 后者仍然需要HTTP标准进行确定.</p> <p>例子:</p> <p>在Request的包头中添加 Cache-Control: max-stale=600 表明接受未超过600秒的超时时间的response.</p> <p>更多请参考: RFC2616, 14.9.3</p> </li></ul> 
<p>目前仍然缺失:</p> 
<ul><li>Pragma: no-cache 支持 <a href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1" rel="nofollow" title="HTTP/1.1: Header Field Definitions">HTTP/1.1: Header Field Definitions</a></li><li>Vary 字段支持 <a href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6" rel="nofollow" title="HTTP/1.1: Caching in HTTP">HTTP/1.1: Caching in HTTP</a></li><li>当update或delete之后失效相应的response <a href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10" rel="nofollow" title="HTTP/1.1: Caching in HTTP">HTTP/1.1: Caching in HTTP</a></li><li>... 以及其他可能缺失的特性 ..</li></ul> 
<p>使用这个策略，设置:</p> 
<ul><li><a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#std:setting-HTTPCACHE_POLICY" rel="nofollow" title="HTTPCACHE_POLICY">HTTPCACHE_POLICY</a> 为 <code>scrapy.extensions.httpcache.RFC2616Policy</code></li></ul> 
<p></p> 
<p><strong>Filesystem storage backend (默认值)</strong></p> 
<p>文件系统存储后端可以用于HTTP缓存中间件。</p> 
<p>使用该存储端，设置:</p> 
<ul><li><a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#std:setting-HTTPCACHE_STORAGE" rel="nofollow" title="HTTPCACHE_STORAGE">HTTPCACHE_STORAGE</a> 为 <code>scrapy.extensions.httpcache.FilesystemCacheStorage</code></li></ul> 
<p>每个request/response组存储在不同的目录中，包含下列文件:</p> 
<blockquote> 
 <ul><li><code>request_body</code> - the plain request body</li><li><code>request_headers</code> - the request headers (原始HTTP格式)</li><li><code>response_body</code> - the plain response body</li><li><code>response_headers</code> - the request headers (原始HTTP格式)</li><li><code>meta</code> - 以Python <code>repr()</code> 格式(grep-friendly格式)存储的该缓存资源的一些元数据。</li><li><code>pickled_meta</code> - 与 <code>meta</code> 相同的元数据，不过使用pickle来获得更高效的反序列化性能。</li></ul> 
</blockquote> 
<p>目录的名称与request的指纹(参考 <code>scrapy.utils.request.fingerprint</code>)有关，而二级目录是为了避免在同一文件夹下有太多文件 (这在很多文件系统中是十分低效的)。目录的例子:</p> 
<pre class="has"><code class="language-html">/path/to/cache/dir/example.com/72/72811f648e718090f041317756c03adb0ada46c7
</code></pre> 
<p></p> 
<p><strong>DBM storage backend</strong></p> 
<p>0.13 新版功能.</p> 
<p>同时也有 <a href="http://en.wikipedia.org/wiki/Dbm" rel="nofollow" title="DBM">DBM</a> 存储后端可以用于HTTP缓存中间件。</p> 
<p>默认情况下，其采用 <a href="https://docs.python.org/library/anydbm.html" rel="nofollow" title="anydbm">anydbm</a> 模块，不过您也可以通过 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#std:setting-HTTPCACHE_DBM_MODULE" rel="nofollow" title="HTTPCACHE_DBM_MODULE">HTTPCACHE_DBM_MODULE</a> 设置进行修改。</p> 
<p>使用该存储端，设置:</p> 
<ul><li><a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#std:setting-HTTPCACHE_STORAGE" rel="nofollow" title="HTTPCACHE_STORAGE">HTTPCACHE_STORAGE</a> 为 <code>scrapy.extensions.httpcache.DbmCacheStorage</code></li></ul> 
<p></p> 
<p><strong>LevelDB storage backend</strong></p> 
<p>0.23 新版功能.</p> 
<p>A <a href="http://code.google.com/p/leveldb/" rel="nofollow" title="LevelDB">LevelDB</a> storage backend is also available for the HTTP cache middleware.</p> 
<p>This backend is not recommended for development because only one process can access LevelDB databases at the same time, so you can’t run a crawl and open the scrapy shell in parallel for the same spider.</p> 
<p>In order to use this storage backend:</p> 
<ul><li>set <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#std:setting-HTTPCACHE_STORAGE" rel="nofollow" title="HTTPCACHE_STORAGE">HTTPCACHE_STORAGE</a> to <code>scrapy.extensions.httpcache.LeveldbCacheStorage</code></li><li>install <a href="https://pypi.python.org/pypi/leveldb" rel="nofollow" title="LevelDB python bindings">LevelDB python bindings</a> like <code>pip install leveldb</code></li></ul> 
<p></p> 
<p><strong>HTTPCache中间件设置</strong></p> 
<p><a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware" rel="nofollow" title="HttpCacheMiddleware">HttpCacheMiddleware</a> 可以通过以下设置进行配置:</p> 
<p></p> 
<p><strong>HTTPCACHE_ENABLED</strong></p> 
<p>0.11 新版功能.</p> 
<p>默认: <code>False</code></p> 
<p>HTTP缓存是否开启。</p> 
<p>在 0.11 版更改: 在0.11版本前，是使用 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#std:setting-HTTPCACHE_DIR" rel="nofollow" title="HTTPCACHE_DIR">HTTPCACHE_DIR</a> 来开启缓存。</p> 
<p></p> 
<p><strong>HTTPCACHE_EXPIRATION_SECS</strong></p> 
<p>默认: <code>0</code></p> 
<p>缓存的request的超时时间，单位秒。</p> 
<p>超过这个时间的缓存request将会被重新下载。如果为0，则缓存的request将永远不会超时。</p> 
<p>在 0.11 版更改: 在0.11版本前，0的意义是缓存的request永远超时。</p> 
<p></p> 
<p><strong>HTTPCACHE_DIR</strong></p> 
<p>默认: <code>'httpcache'</code></p> 
<p>存储(底层的)HTTP缓存的目录。如果为空，则HTTP缓存将会被关闭。 如果为相对目录，则相对于项目数据目录(project data dir)。更多内容请参考 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/commands.html#topics-project-structure" rel="nofollow" title="默认的Scrapy项目结构">默认的Scrapy项目结构</a> 。</p> 
<p></p> 
<p><strong>HTTPCACHE_IGNORE_HTTP_CODES</strong></p> 
<p>0.10 新版功能.</p> 
<p>默认: <code>[]</code></p> 
<p>不缓存设置中的HTTP返回值(code)的request。</p> 
<p></p> 
<p><strong>HTTPCACHE_IGNORE_MISSING</strong></p> 
<p>默认: <code>False</code></p> 
<p>如果启用，在缓存中没找到的request将会被忽略，不下载。</p> 
<p></p> 
<p><strong>HTTPCACHE_IGNORE_SCHEMES</strong></p> 
<p>0.10 新版功能.</p> 
<p>默认: <code>['file']</code></p> 
<p>不缓存这些URI标准(scheme)的response。</p> 
<p></p> 
<p><strong>HTTPCACHE_STORAGE</strong></p> 
<p>默认: <code>'scrapy.extensions.httpcache.FilesystemCacheStorage'</code></p> 
<p>实现缓存存储后端的类。</p> 
<p></p> 
<p><strong>HTTPCACHE_DBM_MODULE</strong></p> 
<p>0.13 新版功能.</p> 
<p>默认: <code>'anydbm'</code></p> 
<p>在 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#httpcache-storage-dbm" rel="nofollow" title="DBM存储后端">DBM存储后端</a> 的数据库模块。 该设定针对DBM后端。</p> 
<p></p> 
<p><strong>HTTPCACHE_POLICY</strong></p> 
<p>0.18 新版功能.</p> 
<p>默认: <code>'scrapy.extensions.httpcache.DummyPolicy'</code></p> 
<p>实现缓存策略的类。</p> 
<p></p> 
<p><strong>HTTPCACHE_GZIP</strong></p> 
<p>0.25 新版功能.</p> 
<p>默认: <code>False</code></p> 
<p>如果启用，scrapy将会使用gzip压缩所有缓存的数据. 该设定只针对文件系统后端(Filesystem backend)有效。</p> 
<p></p> 
<p></p> 
<h4>HttpCompressionMiddleware</h4> 
<p></p> 
<p><em>class </em><code>scrapy.downloadermiddlewares.httpcompression.</code><code>HttpCompressionMiddleware</code></p> 
<p>该中间件提供了对压缩(gzip, deflate)数据的支持。</p> 
<p><strong>HttpCompressionMiddleware Settings</strong></p> 
<p><strong>COMPRESSION_ENABLED</strong></p> 
<p>默认: <code>True</code></p> 
<p>Compression Middleware(压缩中间件)是否开启。</p> 
<p></p> 
<p></p> 
<h4>ChunkedTransferMiddleware</h4> 
<p></p> 
<p><em>class </em><code>scrapy.downloadermiddlewares.chunked.</code><code>ChunkedTransferMiddleware</code></p> 
<p>该中间件添加了对 <a href="http://en.wikipedia.org/wiki/Chunked_transfer_encoding" rel="nofollow" title="chunked transfer encoding">chunked transfer encoding</a> 的支持。</p> 
<p></p> 
<p></p> 
<h4>HttpProxyMiddleware</h4> 
<p></p> 
<p>0.8 新版功能.</p> 
<p><em>class </em><code>scrapy.downloadermiddlewares.httpproxy.</code><code>HttpProxyMiddleware</code></p> 
<p>该中间件提供了对request设置HTTP代理的支持。您可以通过在 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/request-response.html#scrapy.http.Request" rel="nofollow" title="Request">Request</a> 对象中设置 <code>proxy</code> 元数据来开启代理。</p> 
<p>类似于Python标准库模块 <a href="https://docs.python.org/library/urllib.html" rel="nofollow" title="urllib">urllib</a> 及 <a href="https://docs.python.org/library/urllib2.html" rel="nofollow" title="urllib2">urllib2</a> ，其使用了下列环境变量:</p> 
<ul><li><code>http_proxy</code></li><li><code>https_proxy</code></li><li><code>no_proxy</code></li></ul> 
<p>您也可以针对每个请求设置 <code>proxy</code> 元数据, 其形式类似于 <code>http://some_proxy_server:port</code>.</p> 
<p></p> 
<p></p> 
<h4>RedirectMiddleware</h4> 
<p></p> 
<p><em>class </em><code>scrapy.downloadermiddlewares.redirect.</code><code>RedirectMiddleware</code></p> 
<p>该中间件根据 response 的状态处理重定向的request。</p> 
<p id="std:reqmeta-redirect_urls">通过该中间件的(被重定向的)request的url可以通过 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/request-response.html#scrapy.http.Request.meta" rel="nofollow" title="Request.meta">Request.meta</a> 的 <code>redirect_urls</code> 键找到。</p> 
<p><a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#scrapy.downloadermiddlewares.redirect.RedirectMiddleware" rel="nofollow" title="RedirectMiddleware">RedirectMiddleware</a> 可以通过下列设置进行配置(更多内容请参考设置文档):</p> 
<ul><li><a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#std:setting-REDIRECT_ENABLED" rel="nofollow" title="REDIRECT_ENABLED">REDIRECT_ENABLED</a></li><li><a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/settings.html#std:setting-REDIRECT_MAX_TIMES" rel="nofollow" title="REDIRECT_MAX_TIMES">REDIRECT_MAX_TIMES</a></li></ul> 
<p id="std:reqmeta-dont_redirect">如果 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/request-response.html#scrapy.http.Request.meta" rel="nofollow" title="Request.meta">Request.meta</a> 包含 <code>dont_redirect</code> 键，则该 request 将会被此中间件忽略。</p> 
<p>如果想要处理一些 <strong>重定向状态码 </strong>在你的 spider 中，你可以 spider 的属性 <code>handle_httpstatus_list </code>列出。</p> 
<p>例如，如果想要 重定向中间件 忽略 301 和 302 的 response（通过其他的状态码）你可以这样写：</p> 
<pre class="has"><code class="language-python">class MySpider(CrawlSpider):
    handle_httpstatus_list = [301, 302]</code></pre> 
<p><a href="http://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request.meta" rel="nofollow" title="Request.meta">Request.meta</a> 的键 <code>handle_httpstatus_list</code>   能被用来指定 每个 request 的 response 的状态码应该被允许通过。</p> 
<p>也可以设置 meta 的 key 为 <code>handle_httpstatus_all</code>  值为 True 来允许 一个 请求的所有 response 通过</p> 
<p></p> 
<p><strong>RedirectMiddleware settings</strong></p> 
<p><strong>REDIRECT_ENABLED</strong></p> 
<p>0.13 新版功能.</p> 
<p>默认: <code>True</code></p> 
<p>是否启用Redirect中间件。</p> 
<p></p> 
<p><strong>REDIRECT_MAX_TIMES</strong></p> 
<p>默认: <code>20</code></p> 
<p>单个request被重定向的最大次数。</p> 
<p></p> 
<p></p> 
<h4>MetaRefreshMiddleware</h4> 
<p></p> 
<p><em>class </em><code>scrapy.downloadermiddlewares.redirect.</code><code>MetaRefreshMiddleware</code></p> 
<p>该中间件根据meta-refresh html标签处理request重定向。</p> 
<p><a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware" rel="nofollow" title="MetaRefreshMiddleware">MetaRefreshMiddleware</a> 可以通过以下设定进行配置 (更多内容请参考设置文档)。</p> 
<ul><li><a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#std:setting-METAREFRESH_ENABLED" rel="nofollow" title="METAREFRESH_ENABLED">METAREFRESH_ENABLED</a></li><li><code>METAREFRESH_MAXDELAY</code></li></ul> 
<p>该中间件遵循 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#scrapy.downloadermiddlewares.redirect.RedirectMiddleware" rel="nofollow" title="RedirectMiddleware">RedirectMiddleware</a> 描述的 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/settings.html#std:setting-REDIRECT_MAX_TIMES" rel="nofollow" title="REDIRECT_MAX_TIMES">REDIRECT_MAX_TIMES</a> 设定，<a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#std:reqmeta-dont_redirect" rel="nofollow" title="dont_redirect">dont_redirect</a> 及 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#std:reqmeta-redirect_urls" rel="nofollow" title="redirect_urls">redirect_urls</a> meta key。</p> 
<p><strong>MetaRefreshMiddleware settings</strong></p> 
<p><strong>METAREFRESH_ENABLED</strong></p> 
<p>0.17 新版功能.</p> 
<p>默认: <code>True</code></p> 
<p>Meta Refresh中间件是否启用。</p> 
<p><strong>REDIRECT_MAX_METAREFRESH_DELAY</strong></p> 
<p>默认: <code>100</code></p> 
<p>跟进重定向的最大 meta-refresh 延迟(单位:秒)。</p> 
<p></p> 
<p></p> 
<h4>RetryMiddleware</h4> 
<p></p> 
<p><em>class </em><code>scrapy.downloadermiddlewares.retry.</code><code>RetryMiddleware</code></p> 
<p>该中间件将重试可能由于临时的问题，例如连接超时或者HTTP 500错误导致失败的页面。</p> 
<p>爬取进程会收集失败的页面并在最后，spider爬取完所有正常(不失败)的页面后重新调度。 一旦没有更多需要重试的失败页面，该中间件将会发送一个信号(retry_complete)， 其他插件可以监听该信号。</p> 
<p><a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#scrapy.downloadermiddlewares.retry.RetryMiddleware" rel="nofollow" title="RetryMiddleware">RetryMiddleware</a> 可以通过下列设定进行配置 (更多内容请参考设置文档):</p> 
<ul><li><a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#std:setting-RETRY_ENABLED" rel="nofollow" title="RETRY_ENABLED">RETRY_ENABLED</a></li><li><a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#std:setting-RETRY_TIMES" rel="nofollow" title="RETRY_TIMES">RETRY_TIMES</a></li><li><a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#std:setting-RETRY_HTTP_CODES" rel="nofollow" title="RETRY_HTTP_CODES">RETRY_HTTP_CODES</a></li></ul> 
<p>关于HTTP错误的考虑:</p> 
<p>如果根据HTTP协议，您可能想要在设定 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/downloader-middleware.html#std:setting-RETRY_HTTP_CODES" rel="nofollow" title="RETRY_HTTP_CODES">RETRY_HTTP_CODES</a> 中移除400错误。 该错误被默认包括是由于这个代码经常被用来指示服务器过载(overload)了。而在这种情况下，我们想进行重试。</p> 
<p id="std:reqmeta-dont_retry">如果 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/request-response.html#scrapy.http.Request.meta" rel="nofollow" title="Request.meta">Request.meta</a> 包含 <code>dont_retry</code> 键， 该request将会被本中间件忽略。</p> 
<p><strong>RetryMiddleware Settings</strong></p> 
<p><strong>RETRY_ENABLED</strong></p> 
<p>0.13 新版功能.</p> 
<p>默认: <code>True</code></p> 
<p>Retry Middleware是否启用。</p> 
<p><strong>RETRY_TIMES</strong></p> 
<p>默认: <code>2</code></p> 
<p>包括第一次下载，最多的重试次数</p> 
<p><strong>RETRY_HTTP_CODES</strong></p> 
<p>默认: <code>[500, 502, 503, 504, 400, 408]</code></p> 
<p>重试的response 返回值(code)。其他错误(DNS查找问题、连接失败及其他)则一定会进行重试。</p> 
<p></p> 
<p></p> 
<h4>RobotsTxtMiddleware</h4> 
<p></p> 
<p><em>class </em><code>scrapy.downloadermiddlewares.robotstxt.</code><code>RobotsTxtMiddleware</code></p> 
<p>该中间件过滤所有robots.txt eclusion standard中禁止的request。</p> 
<p>确认该中间件及 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/settings.html#std:setting-ROBOTSTXT_OBEY" rel="nofollow" title="ROBOTSTXT_OBEY">ROBOTSTXT_OBEY</a> 设置被启用以确保Scrapy尊重robots.txt。</p> 
<p>警告</p> 
<p>记住, 如果您在一个网站中使用了多个并发请求， Scrapy仍然可能下载一些被禁止的页面。这是由于这些页面是在robots.txt被下载前被请求的。 这是当前robots.txt中间件已知的限制，并将在未来进行修复。</p> 
<p></p> 
<p></p> 
<h4>DownloaderStats</h4> 
<p></p> 
<p><em>class </em><code>scrapy.downloadermiddlewares.stats.</code><code>DownloaderStats</code></p> 
<p>保存所有通过的request、response及exception的中间件。</p> 
<p>您必须启用 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/settings.html#std:setting-DOWNLOADER_STATS" rel="nofollow" title="DOWNLOADER_STATS">DOWNLOADER_STATS</a> 来启用该中间件。</p> 
<p></p> 
<p></p> 
<h4>UserAgentMiddleware</h4> 
<p></p> 
<p><em>class </em><code>scrapy.downloadermiddlewares.useragent.</code><code>UserAgentMiddleware</code></p> 
<p>用于覆盖spider的默认user agent的中间件。</p> 
<p>要使得spider能覆盖默认的user agent，其 user_agent 属性必须被设置。</p> 
<p></p> 
<p></p> 
<h4>AjaxCrawlMiddleware</h4> 
<p></p> 
<p><em>class </em><code>scrapy.downloadermiddlewares.ajaxcrawl.</code><code>AjaxCrawlMiddleware</code></p> 
<p>根据meta-fragment html标签查找 ‘AJAX可爬取’ 页面的中间件。查看 <a href="https://developers.google.com/webmasters/ajax-crawling/docs/getting-started" rel="nofollow" title="https://developers.google.com/webmasters/ajax-crawling/docs/getting-started">https://developers.google.com/webmasters/ajax-crawling/docs/getting-started</a> 来获得更多内容。</p> 
<p>注解</p> 
<p>即使没有启用该中间件，Scrapy仍能查找类似于 <code>'http://example.com/!#foo=bar'</code> 这样的’AJAX可爬取’页面。 AjaxCrawlMiddleware是针对不具有 <code>'!#'</code> 的URL，通常发生在’index’或者’main’页面中。</p> 
<p><strong>AjaxCrawlMiddleware 设置</strong></p> 
<p><strong>AJAXCRAWL_ENABLED</strong></p> 
<p>0.21 新版功能.</p> 
<p>默认: <code>False</code></p> 
<p>AjaxCrawlMiddleware是否启用。您可能需要针对 <a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/broad-crawls.html#topics-broad-crawls" rel="nofollow" title="通用爬虫">通用爬虫</a> 启用该中间件。</p> 
<p></p> 
<p><strong>HttpProxyMiddleware settings</strong></p> 
<p><strong>HTTPPROXY_ENABLED</strong></p> 
<p>Default: <code>True</code></p> 
<p>Whether or not to enable the <code>HttpProxyMiddleware</code>.</p> 
<p><strong>HTTPPROXY_AUTH_ENCODING</strong></p> 
<p>Default: <code>"latin-1"</code></p> 
<p>The default encoding for proxy authentication on <code>HttpProxyMiddleware</code>.</p> 
<p></p> 
<p></p> 
<p></p> 
<h2>2、spider 中间件 ( spider Middleware )</h2> 
<p></p> 
<p>Spider Middleware，中文可以翻译为爬虫中间件，它是处于 Spider 和 Engine 之间的处理模块。当 Downloader 生成 Response 之后，Response 会被发送给 Spider，在发送给 Spider 之前，Response 会首先经过 Spider Middleware的处理，当 Spider 处理生成 Item 和 Request 之后，Item 和 Request 还会经过 Spider Middleware 的处理</p> 
<p>SpiderMiddleware有如下3个作用</p> 
<ul><li>Downioader 生成 Response 之后，Engine 会将其发送给 Spider 进行解析，在 Response 发送给 Spider 之前，可以借助 Spider Middleware 对 Response 进行处理。</li><li>Spider 生成 Request 之后会被发送至 Engine，然后 Request 会被转发到 Scheduler，在 Request 被发送给 Engine 之前，可以借助 SpiderMiddleware 对 Request 进行处理</li><li>Spider 生成 Item 之后会被发送至 Engine，然后 Item 会被转发到 ltemPipeline，在 Item 被发送给 Engine 之前，可以借助 SpiderMiddleware 对 Item 进行处理。</li></ul> 
<p>总的来说，Spider Middleware 可以用来处理输入给 Spider 的 Response 和 Spider 输出的 Item 以及 Request。</p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e234f48a7ab1e085259319b012c65620/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Python基础 给玻璃球工作室的新生们</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1751fadcf2e6fedf6f5f39ed0acc838c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Java面试题一</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>