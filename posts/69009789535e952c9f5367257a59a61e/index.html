<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>一张图转3D质量起飞！GitHub刚建空仓就有300&#43;人赶来标星 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="一张图转3D质量起飞！GitHub刚建空仓就有300&#43;人赶来标星" />
<meta property="og:description" content="梦晨 发自 凹非寺
量子位 | 公众号 QbitAI 最新“只用一张图转3D”方法火了，高保真那种。
对比之前一众方法，算得上跨越式提升。（新方法在最后一行）
挑出一个结果放大来看，几何结构细节丰富，渲染分辨率也高达1024x1024。
新方法Magic123，来自KAUST、Snap和牛津联合团队，一作为KAUST博士生钱国成。
只需输入单个图像，不光生成高质量3D网格，连有视觉吸引力的纹理也一起打包生成。
甚至论文刚挂在arXiv上，代码还没来得及上传时，就已经有300&#43;人赶来标星码住（顺便催更）。
从粗到精，两阶段方案 以往2D转3D最常见方法就是NeRF。但NeRF不光占显存高，分辨率还低。
论文中指出，即使资源效率更高的Instant-NGP方案在16G显存GPU上也只能达到128x128的分辨率。
为进一步提高3D内容的质量，团队在NeRF之后引入了第二阶段，采用DMTet算法将分辨率提高到1024x1024，并且细化NeRF得出的几何结构和纹理。
对于仅有一张的2D参考图像，首先使用现成的Dense Prediction Transformer模型进行分割，再使用预训练的MiDaS提取深度图，用于后续优化。
然后进入第一步粗阶段，采用Instant-NGP并对其进行优化，快速推理并重建复杂几何，但不需要太高分辨率，点到为止即可。
在第二步精细阶段，在用内存效率高的DMTet方法细化和解耦3D模型。DMTet是一种混合了SDF体素和Mesh网格的表示方法，生成可微分的四面体网格。
并且在两个阶段中都使用Textural inversion来保证生成与输入一致的几何形状和纹理。
团队将输入图像分为常见对象（如玩具熊）、不太常见对象（如两个叠在一起的甜甜圈）、不常见对象（如龙雕像）3种。
发现仅使用2D先验信息可以生成更复杂的3D结构，但与输入图像的一致性不高。
仅使用3D先验信息能产生精确但缺少细节的几何体。
团队建议综合使用2D和3D先验，并经过反复试验，最终找到了二者的平衡点。
2D先验信息使用了Stable Diffusion 1.5，3D先验信息使用了哥伦比亚大学/丰田研究所提出的Zero-1-to-3。
在定性比较中，结合两种先验信息的Magic123方法取得了最好的效果。
在定量比较中，评估了Magic123在NeRF4和RealFusion15数据集上的表现，与之前SOTA方法相比在所有指标上取得Top-1成绩。
那么Magic123方法有没有局限性呢？
也有。
在论文最后，团队指出整个方法都建立在“假设参考图像是正视图”的基础上，输入其他角度的图像会导致生成的几何性质较差。
比如从上方拍摄桌子上的食物，就不适合用这个方法了。
另外由于使用了SDS损失，Magic123倾向于生成过度饱和的纹理。尤其是在精细阶段，更高分辨率会放大这种问题。
项目主页：
https://guochengqian.github.io/project/magic123/
论文：
https://arxiv.org/abs/2303.11328
GitHub：
https://github.com/guochengqian/Magic123
参考链接：
[1]https://twitter.com/_akhaliq/status/1675684794653351936" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/69009789535e952c9f5367257a59a61e/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-23T12:01:57+08:00" />
<meta property="article:modified_time" content="2023-07-23T12:01:57+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">一张图转3D质量起飞！GitHub刚建空仓就有300&#43;人赶来标星</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <h6>梦晨 发自 凹非寺<br>量子位 | 公众号 QbitAI</h6> 
 <p style="text-align:left;">最新<strong>“只用一张图转3D”</strong>方法火了，高保真那种。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/80/b0/FEXXAmHZ_o.gif" alt="596b7080cd1337186a0da90a289548ed.gif"></p> 
 <p style="text-align:left;">对比之前一众方法，算得上<strong>跨越式提升</strong>。（新方法在最后一行）</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/5a/36/EMk73Dze_o.gif" alt="c0b9a6a27a0dbba15a8a6f218d94a2c2.gif"></p> 
 <p style="text-align:left;">挑出一个结果放大来看，几何结构细节丰富，渲染分辨率也高达<strong>1024x1024</strong>。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/14/83/GEg3yJnX_o.png" alt="dcdcce24214d5bf1167cc0df8926fb78.png"></p> 
 <p style="text-align:left;">新方法<strong>Magic123</strong>，来自KAUST、Snap和牛津联合团队，一作为KAUST博士生钱国成。</p> 
 <p style="text-align:left;">只需输入单个图像，不光生成高质量3D网格，连有视觉吸引力的纹理也一起打包生成。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/1a/be/AxbfQ9q5_o.png" alt="ca7bce56139fa65657c5de38c988e0ea.png"></p> 
 <p style="text-align:left;">甚至论文刚挂在arXiv上，代码还没来得及上传时，就已经有300+人赶来标星码住（顺便催更）。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/ed/b8/xAxGCeNf_o.png" alt="d5ba009f7598cae8a958e453052ea433.png"></p> 
 <h3>从粗到精，两阶段方案</h3> 
 <p style="text-align:left;">以往2D转3D最常见方法就是NeRF。但NeRF不光占显存高，分辨率还低。</p> 
 <p style="text-align:left;">论文中指出，即使资源效率更高的Instant-NGP方案在16G显存GPU上也只能达到<strong>128x128</strong>的分辨率。</p> 
 <p style="text-align:left;">为进一步提高3D内容的质量，团队在NeRF之后引入了第二阶段，采用DMTet算法将分辨率提高到<strong>1024x1024</strong>，并且细化NeRF得出的几何结构和纹理。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/68/18/B7pRei6c_o.png" alt="28d291e24fd060d5c8f50a7d75ffccca.png"></p> 
 <p style="text-align:left;">对于仅有一张的2D参考图像，首先使用现成的Dense Prediction Transformer模型进行分割，再使用预训练的MiDaS提取深度图，用于后续优化。</p> 
 <p style="text-align:left;">然后进入第一步<strong>粗阶段</strong>，采用Instant-NGP并对其进行优化，快速推理并重建复杂几何，但不需要太高分辨率，点到为止即可。</p> 
 <p style="text-align:left;">在第二步<strong>精细阶段</strong>，在用内存效率高的DMTet方法细化和解耦3D模型。DMTet是一种混合了SDF体素和Mesh网格的表示方法，生成可微分的四面体网格。</p> 
 <p style="text-align:left;">并且在两个阶段中都使用Textural inversion来保证生成与输入一致的几何形状和纹理。</p> 
 <p style="text-align:left;">团队将输入图像分为常见对象（如玩具熊）、不太常见对象（如两个叠在一起的甜甜圈）、不常见对象（如龙雕像）3种。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/53/d5/1D7b3nTG_o.png" alt="1fb278baf4777da787436ff80bfa1df4.png"></p> 
 <p style="text-align:left;">发现仅使用2D先验信息可以生成更复杂的3D结构，但与输入图像的一致性不高。</p> 
 <p style="text-align:left;">仅使用3D先验信息能产生精确但缺少细节的几何体。</p> 
 <p style="text-align:left;">团队建议<strong>综合使用2D和3D先验</strong>，并经过反复试验，最终找到了二者的平衡点。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/12/5b/QjpJAwA2_o.png" alt="e2f1c8d6109c28d07fe6ff94df06416d.png"></p> 
 <p style="text-align:left;">2D先验信息使用了Stable Diffusion 1.5，3D先验信息使用了哥伦比亚大学/丰田研究所提出的Zero-1-to-3。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/61/32/BxDIyl5c_o.png" alt="318f03a4661bbd219f9c29fb632d104a.png"></p> 
 <p style="text-align:left;">在定性比较中，结合两种先验信息的Magic123方法取得了最好的效果。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/15/cb/x0VszkGP_o.png" alt="62633c3be22a32bb87b002d7c570ca69.png"></p> 
 <p style="text-align:left;">在定量比较中，评估了Magic123在NeRF4和RealFusion15数据集上的表现，与之前SOTA方法相比在<strong>所有指标上取得Top-1成绩</strong>。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/98/0f/nL9yXZf0_o.png" alt="580711e9971a418f9ded82d9fa7bf1c8.png"></p> 
 <p style="text-align:left;">那么Magic123方法有没有局限性呢？</p> 
 <p style="text-align:left;">也有。</p> 
 <p style="text-align:left;">在论文最后，团队指出整个方法都建立在<strong>“假设参考图像是正视图”</strong>的基础上，输入其他角度的图像会导致生成的几何性质较差。</p> 
 <p style="text-align:left;">比如从上方拍摄桌子上的食物，就不适合用这个方法了。</p> 
 <p style="text-align:left;">另外由于使用了SDS损失，<strong>Magic123倾向于生成过度饱和的纹理</strong>。尤其是在精细阶段，更高分辨率会放大这种问题。</p> 
 <p style="text-align:left;">项目主页：<br>https://guochengqian.github.io/project/magic123/</p> 
 <p style="text-align:left;">论文：<br>https://arxiv.org/abs/2303.11328</p> 
 <p style="text-align:left;">GitHub：<br>https://github.com/guochengqian/Magic123</p> 
 <p style="text-align:left;">参考链接：<br>[1]https://twitter.com/_akhaliq/status/1675684794653351936</p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/fd06e7d8b4e9171ee338fb6382ef31fa/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">docker || 启动mysql容器</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/55aaa91f7260ad8d02eec142a719c7cf/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Linux安装jdk，Tomcat，mysql</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>