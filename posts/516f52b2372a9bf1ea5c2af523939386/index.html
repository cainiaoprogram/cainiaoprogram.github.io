<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Hebb学习规则与Hopfield神经网络 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Hebb学习规则与Hopfield神经网络" />
<meta property="og:description" content="前言 本文简单介绍了一下Hebb学习规则和Hopfield神经网络，并讨论了二者的特性和意义，以及它们之间的联系。最后代码实现了一个简单Hopfield网络，能够记忆并联想回忆出一张图片。
Hebb学习规则 Hebb学习规则是一种神经网络连接权值的调整方法，它基于心理学家D.O.Hebb提出的“突触修正”假设。该假设指出当该突触前神经元和后神经元同时兴奋或抑制时，则该突触连接增强；反过来，若同一时刻两者状态相反，则突触连接减弱。神经元模型以及具体的调整公式如下：
Δ W j = η f ( W j T X ) X \Delta W_j = \eta f (W_j^T X)X ΔWj​=ηf(WjT​X)X
其中， η \eta η为学习率， f ( W j T X ) f (W_j^T X) f(WjT​X)为神经元 j 的输出， f ( ) f() f()为激活函数，一般为 s g n ( ) sgn() sgn()函数，即大于某一阈值输出值为1，否则输出值为-1。
使用这个规则更新突触连接权值后，当突触前神经元再次发出同样的刺激时，后神经元更容易兴奋或抑制。具体可参考巴甫洛夫的条件反射实验，通过同时刺激狗相应的听觉和控制分泌唾液腺体的神经元，人为地增强了两者的连接。通过重复刺激，最后就在两个原本毫不相干的神经元中建立了很强的突触连接，制造出了全新地条件反射。
Hopfield神经网络 Hopfield网络是一种单层反馈神经网络，具有联想记忆的功能。在使用时通过灌输式学习的方法对其进行训练，即完成记忆，然后在推理阶段通过反馈机制对网络状态进行更新，最后就可以实现联想的功能。Hopfield神经网络可分为离散型(DHNN)和连续型（CHNN）两种，这里主要介绍DHNN。
离散型Hopfield神经网络 离散型Hopfield神经网络（DHNN）的状态是随其迭代的轮数而改变的，所以其状态在时间轴上是离散的，这与下面的CHNN不同。其网络结构如下：
可以发现，每个神经元的输出都将作为所有神经元的输入，即当下任何一个神经元的状态（输出）都将影响所有神经元的下一个状态。有时候DHNN中的神经元没有自反馈，这在具体实现时直接将权重矩阵中对角线上的元素置为0即可。在实际测试中发现，对简单任务有无自反馈连接并不影响DHNN的功能和最终结果。
每个神经元的输出有1和-1两种情况，分别对应着神经元的兴奋和抑制状态，而整个网络的状态由所有神经元的状态决定，因此该网络总共就有 2 n 2^n 2n种状态，其中 n n n为神经元个数。
DHNN功能的实现分为两个阶段：记忆阶段和联想阶段。在记忆阶段输入想要记忆的模式，通过特定的规则调整连接权值，然后在联想阶段给出相似的输入，网络通过迭代能够逐渐收敛于已经记忆的模式，就实现了联想的功能。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/516f52b2372a9bf1ea5c2af523939386/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-09-07T11:37:48+08:00" />
<meta property="article:modified_time" content="2021-09-07T11:37:48+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Hebb学习规则与Hopfield神经网络</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_0"></a>前言</h2> 
<p>本文简单介绍了一下Hebb学习规则和Hopfield神经网络，并讨论了二者的特性和意义，以及它们之间的联系。最后代码实现了一个简单Hopfield网络，能够记忆并联想回忆出一张图片。</p> 
<h2><a id="Hebb_2"></a>Hebb学习规则</h2> 
<p>Hebb学习规则是一种神经网络连接权值的调整方法，它基于心理学家D.O.Hebb提出的“突触修正”假设。该假设指出当该突触前神经元和后神经元同时兴奋或抑制时，则该突触连接增强；反过来，若同一时刻两者状态相反，则突触连接减弱。神经元模型以及具体的调整公式如下：<br> <img src="https://images2.imgbox.com/b6/16/tWRna40V_o.png" alt="在这里插入图片描述"><br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          Δ 
         
         
         
           W 
          
         
           j 
          
         
        
          = 
         
        
          η 
         
        
          f 
         
        
          ( 
         
         
         
           W 
          
         
           j 
          
         
           T 
          
         
        
          X 
         
        
          ) 
         
        
          X 
         
        
       
         \Delta W_j = \eta f (W_j^T X)X 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.969438em; vertical-align: -0.286108em;"></span><span class="mord">Δ</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: -0.13889em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.27444em; vertical-align: -0.383108em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">η</span><span class="mord mathdefault" style="margin-right: 0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.891331em;"><span class="" style="top: -2.453em; margin-left: -0.13889em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.05724em;">j</span></span></span><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.383108em;"><span class=""></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span></span></span></span></span></span></p> 
<p>其中，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         η 
        
       
      
        \eta 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">η</span></span></span></span></span>为学习率，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         f 
        
       
         ( 
        
        
        
          W 
         
        
          j 
         
        
          T 
         
        
       
         X 
        
       
         ) 
        
       
      
        f (W_j^T X) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.2361em; vertical-align: -0.394772em;"></span><span class="mord mathdefault" style="margin-right: 0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.841331em;"><span class="" style="top: -2.44134em; margin-left: -0.13889em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.05724em;">j</span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.394772em;"><span class=""></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span><span class="mclose">)</span></span></span></span></span>为神经元 <em><strong>j</strong></em> 的输出，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         f 
        
       
         ( 
        
       
         ) 
        
       
      
        f() 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.10764em;">f</span><span class="mopen">(</span><span class="mclose">)</span></span></span></span></span>为激活函数，一般为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         s 
        
       
         g 
        
       
         n 
        
       
         ( 
        
       
         ) 
        
       
      
        sgn() 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault" style="margin-right: 0.03588em;">g</span><span class="mord mathdefault">n</span><span class="mopen">(</span><span class="mclose">)</span></span></span></span></span>函数，即大于某一阈值输出值为1，否则输出值为-1。<br> 使用这个规则更新突触连接权值后，当突触前神经元再次发出同样的刺激时，后神经元更容易兴奋或抑制。具体可参考巴甫洛夫的条件反射实验，通过同时刺激狗相应的听觉和控制分泌唾液腺体的神经元，<strong>人为地增强了两者的连接</strong>。通过重复刺激，最后就在两个<strong>原本毫不相干的神经元</strong>中建立了很强的突触连接，<strong>制造出了全新地条件反射</strong>。</p> 
<h2><a id="Hopfield_11"></a>Hopfield神经网络</h2> 
<p>Hopfield网络是一种单层反馈神经网络，具有联想记忆的功能。在使用时通过灌输式学习的方法对其进行训练，即完成记忆，然后在推理阶段通过反馈机制对网络状态进行更新，最后就可以实现联想的功能。Hopfield神经网络可分为离散型(DHNN)和连续型（CHNN）两种，这里主要介绍DHNN。</p> 
<h3><a id="Hopfield_13"></a>离散型Hopfield神经网络</h3> 
<p>离散型Hopfield神经网络（DHNN）的状态是随其迭代的轮数而改变的，所以其状态在时间轴上是离散的，这与下面的CHNN不同。其网络结构如下：<br> <img src="https://images2.imgbox.com/b3/71/SdTXH5DA_o.png" alt="在这里插入图片描述"><br> 可以发现，每个神经元的输出都将作为所有神经元的输入，即当下任何一个神经元的状态（输出）都将影响所有神经元的下一个状态。有时候DHNN中的神经元没有自反馈，这在具体实现时直接将权重矩阵中对角线上的元素置为0即可。在实际测试中发现，对简单任务有无自反馈连接并不影响DHNN的功能和最终结果。<br> 每个神经元的输出有1和-1两种情况，分别对应着神经元的兴奋和抑制状态，而整个网络的状态由所有神经元的状态决定，因此该网络总共就有<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          2 
         
        
          n 
         
        
       
      
        2^n 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.664392em; vertical-align: 0em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.664392em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span></span></span></span></span></span></span></span>种状态，其中<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         n 
        
       
      
        n 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault">n</span></span></span></span></span>为神经元个数。<br> DHNN功能的实现分为两个阶段：记忆阶段和联想阶段。在记忆阶段输入想要记忆的模式，通过特定的规则调整连接权值，然后在联想阶段给出相似的输入，网络通过迭代能够逐渐收敛于已经记忆的模式，就实现了联想的功能。</p> 
<h4><a id="DHNN_19"></a>DHNN的稳定性</h4> 
<p>由网络工作状态的分析可知，DHNN实质上是一个离散的非线性动力学系统。网络从初态<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         X 
        
       
         ( 
        
       
         0 
        
       
         ) 
        
       
      
        X(0) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span><span class="mopen">(</span><span class="mord">0</span><span class="mclose">)</span></span></span></span></span>开始，若经有限次迭代后，其状态不再发生变化，即<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         X 
        
       
         ( 
        
       
         t 
        
       
         + 
        
       
         1 
        
       
         ) 
        
       
         = 
        
       
         X 
        
       
         ( 
        
       
         t 
        
       
         ) 
        
       
      
        X(t+1) = X(t) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mclose">)</span></span></span></span></span>，则称该网络是稳定的。</p> 
<h4><a id="_21"></a>吸引子和能量函数</h4> 
<p>网络达到稳定时的状态<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         X 
        
       
      
        X 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span></span></span></span></span>，称为网络的吸引子。一个动力学系统的最终行为是由它的吸引子决定的。吸引子即为DHNN实现信息存储与联想回忆功能的基础。若把需要记忆的样本信息存储于网络不同的吸引子，当输入含有部分记忆信息的样本时，网络的演变过程便是从部分信息寻找全部信息，即联想回忆的过程。DHNN中有个能量函数的概念，公式如下：<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          E 
         
        
          = 
         
        
          − 
         
         
         
           1 
          
         
           2 
          
         
         
         
           X 
          
         
           T 
          
         
        
          W 
         
        
          X 
         
        
          + 
         
         
         
           X 
          
         
           T 
          
         
        
          T 
         
        
       
         E = - \frac{1}{2}X^TWX + X^TT 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.05764em;">E</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 2.00744em; vertical-align: -0.686em;"></span><span class="mord">−</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.32144em;"><span class="" style="top: -2.314em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord">2</span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.686em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.891331em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.891331em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.891331em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right: 0.13889em;">T</span></span></span></span></span></span></p> 
<p>其中<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         T 
        
       
      
        T 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.13889em;">T</span></span></span></span></span>为每个神经元的激活阈值，简单起见，在后续的实验中直接全部设为0。 随着迭代轮数的增加，能量函数最终会收敛于最小值，即吸引子。 注意，这里<strong>迭代过程中改变的是网络的状态</strong>，即每个神经元的输出（1或-1），并不是权值矩阵。权值矩阵在记忆阶段已经设置好了，<strong>之后就不需要调整了</strong>。并且能量函数的最小值是在<strong>状态空间</strong>中寻找的，并不是在权值空间，这与Back-propagation算法中寻找损失函数的最小值不同。<br> DHNN在联想阶段状态的调整方式有两种：同步式和异步式。同步式即在一次迭代中，调整所有神经元的输出状态；异步式则是在一次迭代中，随机或按一定顺序只调整一个神经元的状态。同步式调整计算速度快，能够充分发挥硬件平台的并行计算能力，但保证网络能够收敛到吸引子的条件会更加苛刻，异步式调整则正好相反。<br> 每个DHNN记忆的容量有限，当只记忆一个模式时，若全值矩阵满足一定条件，联想阶段就可以保证网络能够收敛到所记忆的模式。但当需要记忆多个模式时，若<strong>记忆模式不两两正交</strong>，则在设置权值矩阵时每个样本之间会产生干扰，具体表现为在状态空间的能量函数可能会出现局部极小值，这就会导致在联想阶段网络无法收敛到吸引子。<br> 对于记忆了多个模式的DHNN，在联想阶段网络最终收敛到哪一个吸引子不仅与其初始状态有关，也与神经元状态调整的顺序（即整个系统状态改变的路径）有关。</p> 
<h4><a id="Hebb_31"></a>网络权值的设计与Hebb规则</h4> 
<p>权值矩阵的设置公式如下：<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          W 
         
        
          = 
         
         
         
           ∑ 
          
          
          
            p 
           
          
            = 
           
          
            1 
           
          
         
           p 
          
         
        
          η 
         
        
          [ 
         
         
         
           X 
          
         
           p 
          
         
        
          ( 
         
         
         
           X 
          
         
           p 
          
         
         
         
           ) 
          
         
           T 
          
         
        
          ] 
         
        
       
         W = \sum_{p = 1}^{p} \eta[X^p(X^p)^T] 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 3.10173em; vertical-align: -1.40322em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.69851em;"><span class="" style="top: -1.88289em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.05em;"><span class="pstrut" style="height: 3.05em;"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span><span class="" style="top: -4.34711em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.40322em;"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">η</span><span class="mopen">[</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.714392em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.714392em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.891331em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.13889em;">T</span></span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span></span></span></p> 
<p>或将神经元的自连权值设为0，取消自反馈连接：<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          W 
         
        
          = 
         
         
         
           ∑ 
          
          
          
            p 
           
          
            = 
           
          
            1 
           
          
         
           p 
          
         
        
          η 
         
        
          [ 
         
         
         
           X 
          
         
           p 
          
         
        
          ( 
         
         
         
           X 
          
         
           p 
          
         
         
         
           ) 
          
         
           T 
          
         
        
          − 
         
        
          I 
         
        
          ] 
         
        
       
         W = \sum_{p = 1}^{p} \eta[X^p(X^p)^T - I] 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 3.10173em; vertical-align: -1.40322em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.69851em;"><span class="" style="top: -1.88289em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.05em;"><span class="pstrut" style="height: 3.05em;"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span><span class="" style="top: -4.34711em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.40322em;"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">η</span><span class="mopen">[</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.714392em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.714392em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.891331em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.13889em;">T</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.07847em;">I</span><span class="mclose">]</span></span></span></span></span></span></p> 
<p>式中<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         p 
        
       
      
        p 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span class="mord mathdefault">p</span></span></span></span></span>为需要记忆的模式的数量，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         η 
        
       
      
        \eta 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">η</span></span></span></span></span>为学习率。这个权值设置方法其实并不符合上面公式中Hebb学习规则的定义，因为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         Δ 
        
       
         W 
        
       
      
        \Delta W 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord">Δ</span><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span></span></span></span></span>并不是网络输入与输出的乘积。但它所达到的效果与Hebb规则是一致的，都能让在记忆阶段输入的样本信息保留在权值矩阵中，并在之后的联想阶段输入同样的样本时，<strong>每个神经元更容易兴奋或抑制</strong>，即整个网络更容易变成记忆的状态。只不过在这个应用中我们希望<strong>DHNN直接输出输入数据</strong>，因此上述公式中将<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         f 
        
       
         ( 
        
        
        
          W 
         
        
          T 
         
        
       
         X 
        
       
         ) 
        
       
      
        f(W^TX) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.09133em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.841331em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span><span class="mclose">)</span></span></span></span></span>变为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         X 
        
       
      
        X 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span></span></span></span></span>。</p> 
<h3><a id="Hopfield_43"></a>连续型Hopfield神经网络</h3> 
<p>连续型Hopfield神经网络（CHNN）的基本结构与DHNN相似，但CHNN中个输入输出量均是随时间连续变化的模拟量，一般使用常系数微分方程或模拟电子线路来描述。具体实现涉及到硬件电路方面的设计与分析，这里就不再展开了。</p> 
<h2><a id="_45"></a>代码实现与实验结果</h2> 
<p>接下来实现了一个简单的DHNN，并对其进行了一些简单的测试。网络的功能很简单，就是在记忆阶段输入一张或几张图片，然后将其遮盖住一部分后再输入网络，让网络在联想阶段恢复原图。</p> 
<h3><a id="_47"></a>代码实现</h3> 
<pre><code class="prism language-python"><span class="token comment">#创建一个DHNN类</span>
<span class="token keyword">class</span> <span class="token class-name">DHNN</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n<span class="token punctuation">,</span> <span class="token builtin">iter</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>n <span class="token operator">=</span> n											<span class="token comment">#神经元个数</span>
        self<span class="token punctuation">.</span><span class="token builtin">iter</span> <span class="token operator">=</span> <span class="token builtin">iter</span>									<span class="token comment">#联想阶段迭代次数</span>

    <span class="token keyword">def</span> <span class="token function">memory</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> epoch<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>					<span class="token comment">#记忆</span>
        self<span class="token punctuation">.</span>W <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>n<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epoch<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">for</span> x <span class="token keyword">in</span> X<span class="token punctuation">:</span>
                x <span class="token operator">=</span> np<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>n<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
                xT <span class="token operator">=</span> np<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>n<span class="token punctuation">)</span><span class="token punctuation">)</span>
                <span class="token comment"># self.W += lr * (x * xT)                   #神经元有自反馈</span>
                self<span class="token punctuation">.</span>W <span class="token operator">+=</span> lr <span class="token operator">*</span> <span class="token punctuation">(</span>x <span class="token operator">*</span> xT <span class="token operator">-</span> np<span class="token punctuation">.</span>eye<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment">#神经元无自反馈</span>
    
    <span class="token keyword">def</span> <span class="token function">recall</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>									<span class="token comment">#联想</span>
        x <span class="token operator">=</span> np<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>n<span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">)</span>
        energy_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.5</span> <span class="token operator">*</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>W<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">*</span> x<span class="token punctuation">)</span><span class="token punctuation">]</span>
        <span class="token comment"># for iter in range(self.iter):                     #同步调节方式</span>
        <span class="token comment">#     ilf = self.W.dot(x)</span>
        <span class="token comment">#     x[ilf&gt;0] = 1</span>
        <span class="token comment">#     x[ilf&lt;0] = -1</span>
        <span class="token comment">#     energy = -0.5 * np.sum(self.W.dot(x) * x)</span>
        <span class="token comment">#     energy_list.append(energy)</span>
        <span class="token keyword">for</span> <span class="token builtin">iter</span> <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span><span class="token builtin">iter</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                       <span class="token comment">#异步调节方式</span>
            <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>NET_SIZE<span class="token punctuation">)</span><span class="token punctuation">:</span>
                i <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span>NET_SIZE<span class="token punctuation">)</span>
                ilf <span class="token operator">=</span> self<span class="token punctuation">.</span>W<span class="token punctuation">[</span>i<span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>dot<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
                <span class="token keyword">if</span> ilf <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
                    x<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>
                <span class="token keyword">elif</span> ilf <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
                    x<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span>
            energy <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">0.5</span> <span class="token operator">*</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>W<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">*</span> x<span class="token punctuation">)</span>
            energy_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>energy<span class="token punctuation">)</span>
        <span class="token keyword">return</span> np<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>n<span class="token operator">**</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>n<span class="token operator">**</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> energy_list
</code></pre> 
<p>在这个Demo中，神经元的个数即为输入输出图像像素点的个数，即每个像素点对应一个神经元。神经元的两种输出状态对应像素点的颜色：1为白，-1为黑，因此<strong>此网络只能记忆联想二值图像</strong>。</p> 
<h3><a id="_86"></a>实验结果</h3> 
<p>记忆阶段和联想阶段输入的图像如下：<br> <img src="https://images2.imgbox.com/95/64/KA4SrWRE_o.png" alt="在这里插入图片描述"><br> 联想阶段迭代不同的次数，输出图像如下：<br> <img src="https://images2.imgbox.com/b0/79/XOh84sY5_o.png" alt="在这里插入图片描述"></p> 
<p>能量函数随迭代次数变化如下：<br> <img src="https://images2.imgbox.com/b7/62/zO3EOiTt_o.png" alt="在这里插入图片描述"><br> 可见随着迭代轮数的增加，能量函数逐渐下降并最终于收敛于最小值（对应吸引子），相应的输出图像也越来越趋近于记忆图像。</p> 
<h3><a id="_95"></a>问题与讨论</h3> 
<p>上述实验中DHNN只记忆了一个模式，即整个系统只有一个吸引子，发现网络在回忆阶段能够收敛到吸引子，实验结果很好。但当DHNN记忆了多个模式时，在记忆阶段设置权值矩阵时由于样本之间会有相互干扰，因此在联想阶段网络往往会陷入局部极值，即无法收敛到任一吸引子，这种情况在实验中很常见。此外，当记忆的两个模式相似度较大时，DHNN也有可能发生联想错误，产生<strong>串扰</strong>。这些问题需要引入<strong>玻尔兹曼机和受限玻尔兹曼机</strong>来解决。</p> 
<h2><a id="_97"></a>参考文献</h2> 
<p>韩力群, 施彦. 人工神经网络理论及应用[M]. 北京: 机械工业出版社, 2016.</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/280e1b1d4523f6001002355e8d43f912/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Derivative finite-differencing step was artificially reduced to be within bound constraints.</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c3bec0c92242cedc153247e9a97e040a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">python解决读取文件时中文乱码的解决方案</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>