<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>softmax的loss和gradient推导过程 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="softmax的loss和gradient推导过程" />
<meta property="og:description" content="softmax的loss和gradient推导过程 相信搞deeplearning的各位大牛都很熟悉softmax了，用来对得分矩阵做归一化得到概率的一种分类手段，我这两天在做cs231n的作业，新手上路，只作为自己的学习足迹记录，还望各位大佬多多包涵。
简单介绍Softmax的loss计算Softmax的grad计算naive loopvectorization 简单介绍 这个公式是大家非常熟悉的，其实就是对于神经网络最后一层的结果进行指数概率的归一化，其中Li求得的是对于每个样本而言，它在所有类别中，被分类位正确的概率，syi代表样本被正确分类的评分，sj代表样本被分类为j的评分。 其实对于loss的求解很简单，主要的难度就在于对于梯度的求解。
loss的计算 根据上面的定义其实很容易计算loss,把所有样本i的Li加起来就是最终的结果，需要注意的是L的维度，和分数矩阵相同，最后再计算的时候用numpy.sum函数求矩阵的所有元素之和即可。另外需要考虑的就是归一化和正则化，归一化除以训练样本数就行。做作业的时候说的是用L2正则化，也就是把regularization_rate*W*W加在loss后面即可。
梯度grad的计算 以上这个式子的第一步其实是我们熟悉的链式求导法则，第二步是矩阵的求导法则，s是x与w的点乘结果，如果对某一个求导，结果就是另一个的转置。清楚了以上这个式子就可以进行正常的推导了。
单纯求导的结果会是这样，我们可以看出和s有关的有两个参数sj和syi，分别对应着正确分类的分数和其他所有分类的分数。
终于到了激动人心的推导过程。上面说了要分为正确的Syi和错误的Sj两种情况来计算，也就是j=yi和j≠yi两种。首先看j≠yi。这个时候我们是对sj来求导，那syi就是一个常数，分子后面的被减数的求和项只剩下sj的指数，这样求到之后得出的是probability(j)。
接下来用同样的思想，对syi来求导数，sj就是未知数。可以看出结果是probability(i)-1。
代码块 关于以上的求解，算法分为两种，一种是对于矩阵每个元素的细致计算，通过循环来实现，也叫做naive算法，这种算法直接按照公式去做，还是很简单的直接上代码：
def softmax_loss_naive(W, X, y, reg): &#34;&#34;&#34; Softmax loss function, naive implementation (with loops) Inputs have dimension D, there are C classes, and we operate on minibatches of N examples. Inputs: - W: A numpy array of shape (D, C) containing weights. - X: A numpy array of shape (N, D) containing a minibatch of data." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/33420ada2ca114f0f925d0288d8fae4e/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2017-09-25T16:55:58+08:00" />
<meta property="article:modified_time" content="2017-09-25T16:55:58+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">softmax的loss和gradient推导过程</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2 id="softmax的loss和gradient推导过程">softmax的loss和gradient推导过程</h2> 
<p>  相信搞deeplearning的各位大牛都很熟悉softmax了，用来对得分矩阵做归一化得到概率的一种分类手段，我这两天在做cs231n的作业，新手上路，只作为自己的学习足迹记录，还望各位大佬多多包涵。</p> 
<ul><li><strong>简单介绍</strong></li><li><strong>Softmax的loss计算</strong></li><li><strong>Softmax的grad计算</strong></li><li><strong>naive loop</strong></li><li><strong>vectorization</strong></li></ul> 
<hr> 
<h3 id="简单介绍">简单介绍</h3> 
<p><img src="https://images2.imgbox.com/c5/c6/0PoCmbUb_o.png" alt="每个样本的loss" title=""></p> 
<p>  这个公式是大家非常熟悉的，其实就是对于神经网络最后一层的结果进行指数概率的归一化，其中Li求得的是对于每个样本而言，它在所有类别中，被分类位正确的概率，syi代表样本被正确分类的评分，sj代表样本被分类为j的评分。 其实对于loss的求解很简单，主要的难度就在于对于梯度的求解。</p> 
<h3 id="loss的计算">loss的计算</h3> 
<p>  根据上面的定义其实很容易计算loss,把所有样本i的Li加起来就是最终的结果，需要注意的是L的维度，和分数矩阵相同，最后再计算的时候用numpy.sum函数求矩阵的所有元素之和即可。另外需要考虑的就是归一化和正则化，归一化除以训练样本数就行。做作业的时候说的是用L2正则化，也就是把regularization_rate*W*W加在loss后面即可。</p> 
<p><img src="https://images2.imgbox.com/2d/94/aecEvjhc_o.png" alt="这里写图片描述" title=""></p> 
<h3 id="梯度grad的计算">梯度grad的计算</h3> 
<p><img src="https://images2.imgbox.com/ce/65/BZCvoSUC_o.png" alt="链式求导和矩阵求导的规则" title=""></p> 
<p>  以上这个式子的第一步其实是我们熟悉的链式求导法则，第二步是矩阵的求导法则，s是x与w的点乘结果，如果对某一个求导，结果就是另一个的转置。清楚了以上这个式子就可以进行正常的推导了。</p> 
<p><img src="https://images2.imgbox.com/76/19/Amm3x3BR_o.png" alt="这里写图片描述" title=""></p> 
<p>  单纯求导的结果会是这样，我们可以看出和s有关的有两个参数sj和syi，分别对应着正确分类的分数和其他所有分类的分数。</p> 
<p>  终于到了激动人心的推导过程。上面说了要分为正确的Syi和错误的Sj两种情况来计算，也就是j=yi和j≠yi两种。首先看j≠yi。这个时候我们是对sj来求导，那syi就是一个常数，分子后面的被减数的求和项只剩下sj的指数，这样求到之后得出的是probability(j)。</p> 
<p><img src="https://images2.imgbox.com/32/96/3zMVCK1a_o.png" alt="这里写图片描述" title=""></p> 
<p>  接下来用同样的思想，对syi来求导数，sj就是未知数。可以看出结果是probability(i)-1。</p> 
<p><img src="https://images2.imgbox.com/3c/60/FQzUViZ2_o.png" alt="这里写图片描述" title=""></p> 
<h4 id="代码块">代码块</h4> 
<p>  关于以上的求解，算法分为两种，一种是对于矩阵每个元素的细致计算，通过循环来实现，也叫做naive算法，这种算法直接按照公式去做，还是很简单的直接上代码：</p> 
<pre class="prettyprint"><code class="language-python hljs "><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">softmax_loss_naive</span><span class="hljs-params">(W, X, y, reg)</span>:</span>
  <span class="hljs-string">"""
  Softmax loss function, naive implementation (with loops)
  Inputs have dimension D, there are C classes, and we operate on minibatches
  of N examples.
  Inputs:
  - W: A numpy array of shape (D, C) containing weights.
  - X: A numpy array of shape (N, D) containing a minibatch of data.
  - y: A numpy array of shape (N,) containing training labels; y[i] = c means
    that X[i] has label c, where 0 &lt;= c &lt; C.
  - reg: (float) regularization strength

  Returns a tuple of:
  - loss as single float
  - gradient with respect to weights W; an array of same shape as W
  """</span>
  <span class="hljs-comment"># Initialize the loss and gradient to zero.</span>
  loss = <span class="hljs-number">0.0</span>
  dW = np.zeros_like(W)
  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(X.shape[<span class="hljs-number">0</span>]):
      scores = np.dot(X[i],W)
      scores -= np.max(scores)     <span class="hljs-comment">#规范化数据</span>
      exp_scores = np.exp(scores)
      s_y_i = np.exp(scores[y[i]])
      sum_exp_scores_i = np.sum(exp_scores)
      L_i = -np.log(s_y_i/sum_exp_scores_i) <span class="hljs-comment">#计算Li</span>
      <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(W.shape[<span class="hljs-number">1</span>]):
          <span class="hljs-keyword">if</span> j == y[i]:  <span class="hljs-comment">#正确列的修正</span>
              dW[:,y[i]] += (-<span class="hljs-number">1</span>+s_y_i/sum_exp_scores_i)*X[i].T
          <span class="hljs-keyword">else</span>:
              dW[:,j] += (np.exp(scores[j])/sum_exp_scores_i)*X[i].T    <span class="hljs-comment">#错误列的修正</span>
      loss += L_i
  loss /= X.shape[<span class="hljs-number">0</span>]
  dW /= X.shape[<span class="hljs-number">0</span>]
  loss += <span class="hljs-number">0.5</span>*reg*np.sum(W*W)   <span class="hljs-comment">#regularization</span>
  dW += reg*W
  <span class="hljs-keyword">return</span> loss, dW</code></pre> 
<pre class="prettyprint"><code class="language-python hljs "><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">softmax_loss_vectorized_modify</span><span class="hljs-params">(W, X, y, reg)</span>:</span>
  <span class="hljs-string">"""
  Softmax loss function, vectorized version.
  Inputs and outputs are the same as softmax_loss_naive.
  """</span>
  <span class="hljs-comment"># Initialize the loss and gradient to zero.</span>
  loss = <span class="hljs-number">0.0</span>
  scores = X.dot(W)
  dW = np.zeros_like(scores)
  s_max = np.reshape(np.max(scores,axis=<span class="hljs-number">1</span>),(X.shape[<span class="hljs-number">0</span>],<span class="hljs-number">1</span>))
  scores -= s_max   <span class="hljs-comment">#对于每一行样本规范化数据</span>
  prob = np.exp(scores)/np.sum(np.exp(scores),axis=<span class="hljs-number">1</span>,keepdims=<span class="hljs-keyword">True</span>)
  correct = np.zeros_like(prob)
  correct[range(X.shape[<span class="hljs-number">0</span>]),y] = <span class="hljs-number">1.0</span>  <span class="hljs-comment">#把每个样本的正确分类位置为1</span>
  loss +=  -np.sum(correct*np.log(prob))/X.shape[<span class="hljs-number">0</span>] + <span class="hljs-number">0.5</span>*reg*np.sum(W*W)   <span class="hljs-comment">#loss就是按照公式计算</span>
  dW += (prob-correct)/X.shape[<span class="hljs-number">0</span>] + <span class="hljs-number">0.5</span>*reg*np.sum(W*W)  <span class="hljs-comment">#correct矩阵只有在正确类才为1，错误类为0，也符合上面的推导</span>
  <span class="hljs-keyword">return</span> loss, dW</code></pre> 
<h4 id="总结">总结</h4> 
<p>这是第一次写数学公式加代码过程，博主发现自己的数学还是很渣的，希望所有初学者能和博主一起努力，共勉吧。以后学到的东西都更新到这里。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/1f412752411a839081e9897dea3cb9cb/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Could not resolve all dependencies for configuration &#39;:app:retrolambdaConfi</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/3434aa6c983292cb001fdf8e1443f1b4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Python中numpy数组的合并</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>