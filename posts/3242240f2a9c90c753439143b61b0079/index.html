<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>无卷积步长或池化:用于低分辨率图像和小物体的新 CNN 模块SPD-Conv - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="无卷积步长或池化:用于低分辨率图像和小物体的新 CNN 模块SPD-Conv" />
<meta property="og:description" content="No More Strided Convolutions or Pooling:A New CNN Building Block for Low-Resolution Images and Small Objects 原文地址：
https://arxiv.org/pdf/2208.03641v1.pdf pdf下载： (67条消息) 无卷积步长或池化:用于低分辨率图像和小物体的新CNN模块SPD-Conv-行业报告文档类资源-CSDN文库https://download.csdn.net/download/weixin_53660567/86737435
无卷积步长或池化:用于低分辨率图像和小物体的新 CNN 模块SPD-Conv 摘要
卷积神经网络(CNNs)在图像分类和目标检测等计算机视觉任务中取得了显著的成功。然而，当图像分辨率较低或物体较小时，它们的性能会迅速下降。在本文中，我们指出这根源为现有CNN常见的设计体系结构中一个有缺陷，即使用卷积步长和/或池化层，这导致了细粒度信息的丢失和较低效的特征表示的学习。为此，我们提出了一个名为SPD-Conv的新的CNN构建块来代替每个卷积步长和每个池化层(因此完全消除了它们)。SPD-Conv由一个空间到深度(SPD)层和一个无卷积步长(Conv)层组成，可以应用于大多数CNN体系结构(如果不是全部的话)。我们从两个最具代表性的计算机视觉任务:目标检测和图像分类来解释这个新设计。然后，我们将SPD-Conv应用于YOLOv5和ResNet，创建了新的CNN架构，并通过经验证明，我们的方法明显优于最先进的深度学习模型，特别是在处理低分辨率图像和小物体等更困难的任务时。我们在https://github.com/LabSAINT/SPD-Conv上开放了源代码。
1.介绍
自AlexNet[18]以来，卷积神经网络(CNNs)在许多计算机视觉任务中表现出色。例如在图像分类方面，CNN的知名模型有AlexNet、VGGNet[30]、ResNet[13]等;在目标检测中，包括R-CNN系列[9,28]，YOLO系列[26,4]，SSD [24]，EfficientDet [34]，等等。然而，所有这样的CNN模型在训练和推理中都需要“高质量”的输入(精细图像、中型到大型对象)。例如，AlexNet最初在227×227清晰图像上进行训练和推理，但在将图像分辨率降低到1/4和1/8后，其分类准确率分别下降了14%和30%，[16]。VGGNet和ResNet too[16]上也有类似的情况。在目标检测的情况下，SSD在1/4分辨率的图像或相当于1/4较小尺寸的目标上受到显著的mAP损失34.1，如文献[11]所描述的那样。事实上，小物体检测是一项非常具有挑战性的任务，因为小物体固有的分辨率较低，而且可供模型学习的背景信息也有限。此外，它们经常(不幸地)与同一图像中的大型目标共存，而大型目标往往会主导特征学习过程，从而使小型目标无法被检测到。
在本文中，我们认为这种性能下降的根源在于现有CNN的一个常见的设计缺陷。也就是说，使用卷积步长和/或池化，特别是在CNN体系结构的早期层中。这种设计的负面影响通常不会表现出来，因为大多数被研究的场景都是“和蔼可亲的”，图像有良好的分辨率，物体的大小也适中; 因此，存在大量的冗余像素信息，跨跃卷积和池化可以方便地跳过，模型仍然可以很好地学习特征。然而，在图像模糊或物体很小的更困难的任务中，冗余信息的大量假设不再成立，当前的设计开始遭受细粒度信息丢失和学习特征不足的影响。
为了解决这个问题，我们为CNN提出了一个新的构建模块，称为SPD-Conv，完全替代(从而消除)卷积步长和池化层。SPD- conv是一个空间到深度(SPD)层，后面跟着一个无步长卷积层。SPD层对特征映射X进行下采样，但保留了通道维度中的所有信息，因此没有信息丢失。我们受到了图像转换技术[29]的启发，该技术在将原始图像输入神经网络之前将其缩放，但我们基本上将其推广到整个网络内部和整个网络中的下采样特征映射;此外，我们在每个SPD之后添加了一个无步长卷积操作，以在增加的卷积层中使用可学习参数减少(增加的)通道数量。我们提出的方法既通用又统一，因为SPD-Conv (i)可以应用于大多数(如果不是所有)CNN架构，并且(ii)以相同的方式替代卷积步长和池化。综上所述，本文的贡献如下:
我们在现有的CNN体系结构中发现了一个常见的设计缺陷，并提出了一个名为SPD-Conv的新构建块来代替旧的设计。SPD-Conv下采样不丢失可学习信息，完全摒弃了目前广泛使用的步长卷积和池化操作。SPD-Conv代表了一种通用和统一的方法，可以很容易地应用于大多数(如果不是所有)基于深度学习的计算机视觉任务。利用目标检测和图像分类这两个最具代表性的计算机视觉任务，对SPD-Conv的性能进行了评价。具体而言，我们构建了YOLOv5-SPD、ResNet18-SPD和ResNet50-SPD，并在COCO-2017、Tiny ImageNet和CIFAR-10数据集上对它们进行了评估，并与几种最先进的深度学习模型进行了比较。结果表明，该算法在AP方面有显著提高，并获得了top-1精度，特别是在小物体和低分辨率图像上。如图1所示。SPD-Conv可以很容易地集成到流行的深度学习库中，如PyTorch和TensorFlow，有可能产生更大的影响。我们的源代码可在https://github.com/LabSAINT/SPD-Conv获得。 图1:比较AP中的小目标(APS)。“SPD”表示我们的方法。
本文的其余部分组织如下。第2节介绍了背景并回顾了相关工作。第3节描述了我们提出的方法，第4节介绍了两个使用目标检测和图像分类的案例研究。第5节提供了性能评估。本文的结论在第6部分。
2 前期工作及相关工作
我们首先提供这个领域的概述，更多地关注目标检测，因为它包含了图像分类。
目前最先进的目标检测模型是基于CNN的，可以分为一级和二级检测器，或基于锚框的或无锚框检测器。两阶段检测器首先生成粗区域提取，然后使用一个head(全连接网络)对每个提取进行分类和细化。相比之下，一级检测器跳过区域提取步骤，直接在密集的位置采样上运行检测。基于锚框的方法使用锚框盒，锚框盒是一个预定义的盒子集合，匹配训练数据中对象的宽度和高度，以提高训练过程中的损失收敛性。我们提供了表1，它对一些众所周知的模型进行了分类。
一般而言，一级检测器比二级检测器速度快，基于锚框的模型比无锚框的模型更精确。因此，在后面的案例研究和实验中，我们更多地关注单级和基于锚框的模型，即表1中的第一个行。一个典型的单阶段目标检测模型如图2所示。它由一个基于CNN的视觉特征提取backbone和一个预测每个包含对象的类别和边界框的检测头组成。在这两者之间，添加一个额外的NECK来组合多个尺度的特征，以产生语义上强的特征，用于检测不同大小的目标。
表1:OD模型的分类
图2:一级目标检测通道
2.1小目标检测
传统上，检测小物体和大物体都被视为一个多尺度的物体检测问题。一种经典的方法是图像金字塔[3]，它将输入图像调整为多个尺度，并为每个尺度训练专用检测器。为了提高精度，提出了SNIP[31]，它根据每个探测器中不同的物体大小进行选择性反向传播。SNIPER[32]提高了SNIP的效率，它只处理每个目标实例周围的背景区域，而不是图像金字塔中的每个像素，从而减少了训练时间。采用另一种提高效率的方法，特征金字塔网络FPN[20]在卷积层中利用横向连接将固有的多尺度特征联系在一起，并使用自顶向下的结构将这些特征结合起来。随后引入PANet[22]和BiFPN[34]，利用较短的路径改善FPN的特征信息流。此外，引入SAN[15]将多尺度特征映射到尺度不变换子空间，使检测器对尺度变化具有更强的鲁棒性。所有这些模型都一致使用卷积步长和最大池化，我们完全摆脱了这一点。
2.2低分辨率图像分类
解决这一挑战的早期尝试之一是[6]，它提出了一个端到端CNN模型，在分类之前添加一个超分辨率步骤。随后，[25]提出将从高分辨率训练图像中获得的细粒度知识转移到低分辨率测试图像中。然而，这种方法需要对应于特定应用程序(例如，类)的高分辨率训练图像，而这些图像并不总是可用的。
其他一些研究，如[37]，也需要对高分辨率训练图像的同样要求。最近，[33]提出了一个包含属性级可分离性(属性表示细粒度、分层的类标签)的损失函数，以便模型能够学习特定于类的鉴别特征。然而，细粒度(分层)类标签很难获得，因此限制了该方法的采用。
3.一个新的构建模块:SPD-Conv
SPD- conv由一个空间到深度(SPD)层和一个非跨步卷积层组成。本节将对此进行详细描述。
我们的SPD组件推广了一种(原始)图像转换技术[29]来对CNN内部和整个CNN的特征映射进行下采样，如下所示。
考虑任意大小为S × S × C1的中间特征映射X，将子特征映射序列切片为：
一般来说，给定任何(原始)特征映射X，子映射fx,y由所有特征映射组成特征图X(i, j) ,i &#43; x和j &#43; y可以被比例整除。因此，每个子图按一个比例因子向下采样X。图3(a)(b)(c)给出了一个例子，当scale = 2时，我们得到四个子映射f0,0,f1,0,f0,1,f1,1，它们的形状为(S/2,S/2,C1)并对X进行2倍的下采样。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/3242240f2a9c90c753439143b61b0079/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-10-06T16:17:58+08:00" />
<meta property="article:modified_time" content="2022-10-06T16:17:58+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">无卷积步长或池化:用于低分辨率图像和小物体的新 CNN 模块SPD-Conv</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3><span style="color:#0d0016;">No More Strided Convolutions or Pooling:A New CNN Building Block for Low-Resolution Images and Small Objects</span></h3> 
<h3><span style="color:#0d0016;">原文地址：<br><a href="https://arxiv.org/pdf/2208.03641v1.pdf" rel="nofollow" title="https://arxiv.org/pdf/2208.03641v1.pdf">https://arxiv.org/pdf/2208.03641v1.pdf</a></span></h3> 
<h3><span style="color:#0d0016;"><strong> pdf下载：</strong></span></h3> 
<p><span style="color:#0d0016;"><a class="has-card" href="https://download.csdn.net/download/weixin_53660567/86737435" title="(67条消息) 无卷积步长或池化:用于低分辨率图像和小物体的新CNN模块SPD-Conv-行业报告文档类资源-CSDN文库"><span class="link-card-box"><span class="link-title">(67条消息) 无卷积步长或池化:用于低分辨率图像和小物体的新CNN模块SPD-Conv-行业报告文档类资源-CSDN文库</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/89/0f/HcrFEbfO_o.png" alt="icon-default.png?t=M85B">https://download.csdn.net/download/weixin_53660567/86737435</span></span></a></span></p> 
<h2 style="margin-left:0px;text-align:center;"><span style="color:#0d0016;"><strong>无卷积步长或池化</strong><strong>:</strong><strong>用于低分辨率图像和小物体的新</strong><strong> CNN </strong><strong>模块</strong><strong>SPD-Conv</strong></span></h2> 
<p style="margin-left:0;text-align:center;"><span style="color:#0d0016;"><strong><span style="background-color:#FFFFFF;">摘要</span></strong></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">卷积神经网络</span><span style="background-color:#FFFFFF;">(CNNs)</span><span style="background-color:#FFFFFF;">在图像分类和目标检测等计算机视觉任务中取得了显著的成功。然而，当图像分辨率较低或物体较小时，它们的性能会迅速下降。在本文中，我们指出这根源为现有</span><span style="background-color:#FFFFFF;">CNN</span><span style="background-color:#FFFFFF;">常见的设计体系结构中一个有缺陷，即使用卷积步长和</span><span style="background-color:#FFFFFF;">/</span><span style="background-color:#FFFFFF;">或池化层，这导致了细粒度信息的丢失和较低效的特征表示的学习。为此，我们提出了一个名为</span><span style="background-color:#FFFFFF;">SPD-Conv</span><span style="background-color:#FFFFFF;">的新的</span><span style="background-color:#FFFFFF;">CNN</span><span style="background-color:#FFFFFF;">构建块来代替每个卷积步长和每个池化层</span><span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">因此完全消除了它们</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">。</span><span style="background-color:#FFFFFF;">SPD-Conv</span><span style="background-color:#FFFFFF;">由一个空间到深度</span><span style="background-color:#FFFFFF;">(SPD)</span><span style="background-color:#FFFFFF;">层和一个无卷积步长</span><span style="background-color:#FFFFFF;">(Conv)</span><span style="background-color:#FFFFFF;">层组成，可以应用于大多数</span><span style="background-color:#FFFFFF;">CNN</span><span style="background-color:#FFFFFF;">体系结构</span><span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">如果不是全部的话</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">。我们从两个最具代表性的计算机视觉任务</span><span style="background-color:#FFFFFF;">:</span><span style="background-color:#FFFFFF;">目标检测和图像分类来解释这个新设计。然后，我们将</span><span style="background-color:#FFFFFF;">SPD-Conv</span><span style="background-color:#FFFFFF;">应用于</span><span style="background-color:#FFFFFF;">YOLOv5</span><span style="background-color:#FFFFFF;">和</span><span style="background-color:#FFFFFF;">ResNet</span><span style="background-color:#FFFFFF;">，创建了新的</span><span style="background-color:#FFFFFF;">CNN</span><span style="background-color:#FFFFFF;">架构，并通过经验证明，我们的方法明显优于最先进的深度学习模型，特别是在处理低分辨率图像和小物体等更困难的任务时。我们在</span><span style="background-color:#FFFFFF;">https://github.com/LabSAINT/SPD-Conv</span><span style="background-color:#FFFFFF;">上开放了源代码。</span></span></p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><strong><span style="background-color:#FFFFFF;">1.</span></strong><strong><span style="background-color:#FFFFFF;">介绍</span></strong></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">自</span><span style="background-color:#FFFFFF;">AlexNet[18]</span><span style="background-color:#FFFFFF;">以来，卷积神经网络</span><span style="background-color:#FFFFFF;">(CNNs)</span><span style="background-color:#FFFFFF;">在许多计算机视觉任务中表现出色。例如在图像分类方面，</span><span style="background-color:#FFFFFF;">CNN</span><span style="background-color:#FFFFFF;">的知名模型有</span><span style="background-color:#FFFFFF;">AlexNet</span><span style="background-color:#FFFFFF;">、</span><span style="background-color:#FFFFFF;">VGGNet[30]</span><span style="background-color:#FFFFFF;">、</span><span style="background-color:#FFFFFF;">ResNet[13]</span><span style="background-color:#FFFFFF;">等</span><span style="background-color:#FFFFFF;">;</span><span style="background-color:#FFFFFF;">在目标检测中，包括</span><span style="background-color:#FFFFFF;">R-CNN</span><span style="background-color:#FFFFFF;">系列</span><span style="background-color:#FFFFFF;">[9,28]</span><span style="background-color:#FFFFFF;">，</span><span style="background-color:#FFFFFF;">YOLO</span><span style="background-color:#FFFFFF;">系列</span><span style="background-color:#FFFFFF;">[26,4]</span><span style="background-color:#FFFFFF;">，</span><span style="background-color:#FFFFFF;">SSD [24]</span><span style="background-color:#FFFFFF;">，</span><span style="background-color:#FFFFFF;">EfficientDet [34]</span><span style="background-color:#FFFFFF;">，等等。然而，所有这样的</span><span style="background-color:#FFFFFF;">CNN</span><span style="background-color:#FFFFFF;">模型在训练和推理中都需要</span><span style="background-color:#FFFFFF;">“</span><span style="background-color:#FFFFFF;">高质量</span><span style="background-color:#FFFFFF;">”</span><span style="background-color:#FFFFFF;">的输入</span><span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">精细图像、中型到大型对象</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">。例如，</span><span style="background-color:#FFFFFF;">AlexNet</span><span style="background-color:#FFFFFF;">最初在</span><span style="background-color:#FFFFFF;">227×227</span><span style="background-color:#FFFFFF;">清晰图像上进行训练和推理，但在将图像分辨率降低到</span><span style="background-color:#FFFFFF;">1/4</span><span style="background-color:#FFFFFF;">和</span><span style="background-color:#FFFFFF;">1/8</span><span style="background-color:#FFFFFF;">后，其分类准确率分别下降了</span><span style="background-color:#FFFFFF;">14%</span><span style="background-color:#FFFFFF;">和</span><span style="background-color:#FFFFFF;">30%</span><span style="background-color:#FFFFFF;">，</span><span style="background-color:#FFFFFF;">[16]</span><span style="background-color:#FFFFFF;">。</span><span style="background-color:#FFFFFF;">VGGNet</span><span style="background-color:#FFFFFF;">和</span><span style="background-color:#FFFFFF;">ResNet too[16]</span><span style="background-color:#FFFFFF;">上也有类似的情况。在目标检测的情况下，</span><span style="background-color:#FFFFFF;">SSD</span><span style="background-color:#FFFFFF;">在</span><span style="background-color:#FFFFFF;">1/4</span><span style="background-color:#FFFFFF;">分辨率的图像或相当于</span><span style="background-color:#FFFFFF;">1/4</span><span style="background-color:#FFFFFF;">较小尺寸的目标上受到显著的</span><span style="background-color:#FFFFFF;">mAP</span><span style="background-color:#FFFFFF;">损失</span><span style="background-color:#FFFFFF;">34.1</span><span style="background-color:#FFFFFF;">，如文献</span><span style="background-color:#FFFFFF;">[11]</span><span style="background-color:#FFFFFF;">所描述的那样。事实上，小物体检测是一项非常具有挑战性的任务，因为小物体固有的分辨率较低，而且可供模型学习的背景信息也有限。此外，它们经常</span><span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">不幸地</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">与同一图像中的大型目标共存，而大型目标往往会主导特征学习过程，从而使小型目标无法被检测到。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">在本文中，我们认为这种性能下降的根源在于现有</span><span style="background-color:#FFFFFF;">CNN</span><span style="background-color:#FFFFFF;">的一个常见的设计缺陷。也就是说，使用卷积步长和</span><span style="background-color:#FFFFFF;">/</span><span style="background-color:#FFFFFF;">或池化，特别是在</span><span style="background-color:#FFFFFF;">CNN</span><span style="background-color:#FFFFFF;">体系结构的早期层中。这种设计的负面影响通常不会表现出来，因为大多数被研究的场景都是</span><span style="background-color:#FFFFFF;">“</span><span style="background-color:#FFFFFF;">和蔼可亲的</span><span style="background-color:#FFFFFF;">”</span><span style="background-color:#FFFFFF;">，图像有良好的分辨率，物体的大小也适中</span><span style="background-color:#FFFFFF;">; </span><span style="background-color:#FFFFFF;">因此，存在大量的冗余像素信息，跨跃卷积和池化可以方便地跳过，模型仍然可以很好地学习特征。然而，在图像模糊或物体很小的更困难的任务中，冗余信息的大量假设不再成立，当前的设计开始遭受细粒度信息丢失和学习特征不足的影响。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">为了解决这个问题，我们为</span><span style="background-color:#FFFFFF;">CNN</span><span style="background-color:#FFFFFF;">提出了一个新的构建模块，称为</span><span style="background-color:#FFFFFF;">SPD-Conv</span><span style="background-color:#FFFFFF;">，完全替代</span><span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">从而消除</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">卷积步长和池化层。</span><span style="background-color:#FFFFFF;">SPD- conv</span><span style="background-color:#FFFFFF;">是一个空间到深度</span><span style="background-color:#FFFFFF;">(SPD)</span><span style="background-color:#FFFFFF;">层，后面跟着一个无步长卷积层。</span><span style="background-color:#FFFFFF;">SPD</span><span style="background-color:#FFFFFF;">层对特征映射</span><span style="background-color:#FFFFFF;">X</span><span style="background-color:#FFFFFF;">进行下采样，但保留了通道维度中的所有信息，因此没有信息丢失</span><span style="background-color:#FFFFFF;">。我们受到了图像转换技术</span><span style="background-color:#FFFFFF;">[29]</span><span style="background-color:#FFFFFF;">的启发，该技术在将原始图像输入神经网络之前将其缩放，但我们基本上将其推广到整个网络内部和整个网络中的下采样特征映射</span><span style="background-color:#FFFFFF;">;</span><span style="background-color:#FFFFFF;">此外，我们在每个</span><span style="background-color:#FFFFFF;">SPD</span><span style="background-color:#FFFFFF;">之后添加了一个无步长卷积操作，以在增加的卷积层中使用可学习参数减少</span><span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">增加的</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">通道数量。我们提出的方法既通用又统一，因为</span><span style="background-color:#FFFFFF;">SPD-Conv (i)</span><span style="background-color:#FFFFFF;">可以应用于大多数</span><span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">如果不是所有</span><span style="background-color:#FFFFFF;">)CNN</span><span style="background-color:#FFFFFF;">架构，并且</span><span style="background-color:#FFFFFF;">(ii)</span><span style="background-color:#FFFFFF;">以相同的方式替代卷积步长和池化。综上所述，本文的贡献如下</span><span style="background-color:#FFFFFF;">:</span></span></p> 
<ol><li style="text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">我们在现有的</span><span style="background-color:#FFFFFF;">CNN</span><span style="background-color:#FFFFFF;">体系结构中发现了一个常见的设计缺陷，并提出了一个名为</span><span style="background-color:#FFFFFF;">SPD-Conv</span><span style="background-color:#FFFFFF;">的新构建块来代替旧的设计。</span><span style="background-color:#FFFFFF;">SPD-Conv</span><span style="background-color:#FFFFFF;">下采样不丢失可学习信息，完全摒弃了目前广泛使用的步长卷积和池化操作。</span></span></li><li style="text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">SPD-Conv</span><span style="background-color:#FFFFFF;">代表了一种通用和统一的方法，可以很容易地应用于大多数</span><span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">如果不是所有</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">基于深度学习的计算机视觉任务。</span></span></li><li style="text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">利用目标检测和图像分类这两个最具代表性的计算机视觉任务，对</span><span style="background-color:#FFFFFF;">SPD-Conv</span><span style="background-color:#FFFFFF;">的性能进行了评价。具体而言，我们构建了</span><span style="background-color:#FFFFFF;">YOLOv5-SPD</span><span style="background-color:#FFFFFF;">、</span><span style="background-color:#FFFFFF;">ResNet18-SPD</span><span style="background-color:#FFFFFF;">和</span><span style="background-color:#FFFFFF;">ResNet50-SPD</span><span style="background-color:#FFFFFF;">，并在</span><span style="background-color:#FFFFFF;">COCO-2017</span><span style="background-color:#FFFFFF;">、</span><span style="background-color:#FFFFFF;">Tiny ImageNet</span><span style="background-color:#FFFFFF;">和</span><span style="background-color:#FFFFFF;">CIFAR-10</span><span style="background-color:#FFFFFF;">数据集上对它们进行了评估，并与几种最先进的深度学习模型进行了比较。结果表明，该算法在</span><span style="background-color:#FFFFFF;">AP</span><span style="background-color:#FFFFFF;">方面有显著提高，并获得了</span><span style="background-color:#FFFFFF;">top-1</span><span style="background-color:#FFFFFF;">精度，特别是在小物体和低分辨率图像上。如图</span><span style="background-color:#FFFFFF;">1</span><span style="background-color:#FFFFFF;">所示。</span></span></li><li style="text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">SPD-Conv</span><span style="background-color:#FFFFFF;">可以很容易地集成到流行的深度学习库中，如</span><span style="background-color:#FFFFFF;">PyTorch</span><span style="background-color:#FFFFFF;">和</span><span style="background-color:#FFFFFF;">TensorFlow</span><span style="background-color:#FFFFFF;">，有可能产生更大的影响。我们的源代码可在</span><span style="background-color:#FFFFFF;">https://github.com/LabSAINT/SPD-Conv</span><span style="background-color:#FFFFFF;">获得。</span></span></li></ol> 
<p style="margin-left:0;text-align:center;"><span style="color:#0d0016;"><img alt="" height="339" src="https://images2.imgbox.com/74/4e/dRVUyH7M_o.png" width="957"></span></p> 
<p style="margin-left:0;text-align:center;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">图</span><span style="background-color:#FFFFFF;">1:</span><span style="background-color:#FFFFFF;">比较</span><span style="background-color:#FFFFFF;">AP</span><span style="background-color:#FFFFFF;">中的小目标</span><span style="background-color:#FFFFFF;">(APS)</span><span style="background-color:#FFFFFF;">。</span><span style="background-color:#FFFFFF;">“SPD”</span><span style="background-color:#FFFFFF;">表示我们的方法。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;"><span style="background-color:#FFFFFF;">本文的其余部分组织如下。第</span><span style="background-color:#FFFFFF;">2</span><span style="background-color:#FFFFFF;">节介绍了背景并回顾了相关工作。第</span><span style="background-color:#FFFFFF;">3</span><span style="background-color:#FFFFFF;">节描述了我们提出的方法，第</span><span style="background-color:#FFFFFF;">4</span><span style="background-color:#FFFFFF;">节介绍了两个使用目标检测和图像分类的案例研究。第</span><span style="background-color:#FFFFFF;">5</span><span style="background-color:#FFFFFF;">节提供了性能评估。本文的结论在第</span><span style="background-color:#FFFFFF;">6</span><span style="background-color:#FFFFFF;">部分。</span></span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><strong><span style="background-color:#FFFFFF;">2 </span></strong><strong><span style="background-color:#FFFFFF;">前期工作及相关工作</span></strong></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">我们首先提供这个领域的概述，更多地关注目标检测，因为它包含了图像分类。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">目前最先进的目标检测模型是基于</span><span style="background-color:#FFFFFF;">CNN</span><span style="background-color:#FFFFFF;">的，可以分为一级和二级检测器，或基于锚框的或无锚框检测器。两阶段检测器首先生成粗区域提取，然后使用一个</span><span style="background-color:#FFFFFF;">head(</span><span style="background-color:#FFFFFF;">全连接网络</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">对每个提取进行分类和细化。相比之下，一级检测器跳过区域提取步骤，直接在密集的位置采样上运行检测。基于锚框的方法使用锚框盒，锚框盒是一个预定义的盒子集合，匹配训练数据中对象的宽度和高度，以提高训练过程中的损失收敛性。我们提供了表</span><span style="background-color:#FFFFFF;">1</span><span style="background-color:#FFFFFF;">，它对一些众所周知的模型进行了分类。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">一般而言，一级检测器比二级检测器速度快，基于锚框的模型比无锚框的模型更精确。因此，在后面的案例研究和实验中，我们更多地关注单级和基于锚框的模型，即表</span><span style="background-color:#FFFFFF;">1</span><span style="background-color:#FFFFFF;">中的第一个行。一个典型的单阶段目标检测模型如图</span><span style="background-color:#FFFFFF;">2</span><span style="background-color:#FFFFFF;">所示。它由一个基于</span><span style="background-color:#FFFFFF;">CNN</span><span style="background-color:#FFFFFF;">的视觉特征提取</span><span style="background-color:#FFFFFF;">backbone</span><span style="background-color:#FFFFFF;">和一个预测每个包含对象的类别和边界框的检测头组成。在这两者之间，添加一个额外的</span><span style="background-color:#FFFFFF;">NECK</span><span style="background-color:#FFFFFF;">来组合多个尺度的特征，以产生语义上强的特征，用于检测不同大小的目标。</span></span></p> 
<p style="margin-left:0;text-align:center;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">表</span><span style="background-color:#FFFFFF;">1:OD</span><span style="background-color:#FFFFFF;">模型的分类</span></span></p> 
<p style="margin-left:0;text-align:center;"><span style="color:#0d0016;"><img alt="" height="530" src="https://images2.imgbox.com/e4/0e/hMF3DgRW_o.png" width="888"></span></p> 
<p style="margin-left:0;text-align:center;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">图</span><span style="background-color:#FFFFFF;">2:</span><span style="background-color:#FFFFFF;">一级目标检测通道</span></span></p> 
<p style="margin-left:0;text-align:center;"><span style="color:#0d0016;"><img alt="" height="530" src="https://images2.imgbox.com/f4/92/wkPLQjrs_o.png" width="972"></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><strong><span style="background-color:#FFFFFF;">2.1</span></strong><strong><span style="background-color:#FFFFFF;">小目标检测</span></strong></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">传统上，检测小物体和大物体都被视为一个多尺度的物体检测问题。一种经典的方法是图像金字塔</span><span style="background-color:#FFFFFF;">[3]</span><span style="background-color:#FFFFFF;">，它将输入图像调整为多个尺度，并为每个尺度训练专用检测器。为了提高精度，提出了</span><span style="background-color:#FFFFFF;">SNIP[31]</span><span style="background-color:#FFFFFF;">，它根据每个探测器中不同的物体大小进行选择性反向传播。</span><span style="background-color:#FFFFFF;">SNIPER[32]</span><span style="background-color:#FFFFFF;">提高了</span><span style="background-color:#FFFFFF;">SNIP</span><span style="background-color:#FFFFFF;">的效率，它只处理每个目标实例周围的背景区域，而不是图像金字塔中的每个像素，从而减少了训练时间。采用另一种提高效率的方法，特征金字塔网络</span>FPN<span style="background-color:#FFFFFF;">[20]</span><span style="background-color:#FFFFFF;">在卷积层中利用横向连接将固有的多尺度特征联系在一起，并使用自顶向下的结构将这些特征结合起来。随后引入</span><span style="background-color:#FFFFFF;">PANet[22]</span><span style="background-color:#FFFFFF;">和</span><span style="background-color:#FFFFFF;">BiFPN[34]</span><span style="background-color:#FFFFFF;">，利用较短的路径改善</span><span style="background-color:#FFFFFF;">FPN</span><span style="background-color:#FFFFFF;">的特征信息流。此外，引入</span><span style="background-color:#FFFFFF;">SAN[15]</span><span style="background-color:#FFFFFF;">将多尺度特征映射到尺度不变换子空间，使检测器对尺度变化具有更强的鲁棒性。所有这些模型都一致使用卷积步长和最大池化，我们完全摆脱了这一点。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><strong><span style="background-color:#FFFFFF;">2.2</span></strong><strong><span style="background-color:#FFFFFF;">低分辨率图像分类</span></strong></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">解决这一挑战的早期尝试之一是</span><span style="background-color:#FFFFFF;">[6]</span><span style="background-color:#FFFFFF;">，它提出了一个端到端</span><span style="background-color:#FFFFFF;">CNN</span><span style="background-color:#FFFFFF;">模型，在分类之前添加一个超分辨率步骤。随后，</span><span style="background-color:#FFFFFF;">[25]</span><span style="background-color:#FFFFFF;">提出将从高分辨率训练图像中获得的细粒度知识转移到低分辨率测试图像中。然而，这种方法需要对应于特定应用程序</span><span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">例如，类</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">的高分辨率训练图像，而这些图像并不总是可用的。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">其他一些研究，如</span><span style="background-color:#FFFFFF;">[37]</span><span style="background-color:#FFFFFF;">，也需要对高分辨率训练图像的同样要求。最近，</span><span style="background-color:#FFFFFF;">[33]</span><span style="background-color:#FFFFFF;">提出了一个包含属性级可分离性</span><span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">属性表示细粒度、分层的类标签</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">的损失函数，以便模型能够学习特定于类的鉴别特征。然而，细粒度</span><span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">分层</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">类标签很难获得，因此限制了该方法的采用。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><strong><span style="background-color:#FFFFFF;">3.</span></strong><strong><span style="background-color:#FFFFFF;">一个新的构建模块</span></strong><strong><span style="background-color:#FFFFFF;">:SPD-Conv</span></strong></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">SPD- conv</span><span style="background-color:#FFFFFF;">由一个空间到深度</span><span style="background-color:#FFFFFF;">(SPD)</span><span style="background-color:#FFFFFF;">层和一个非跨步卷积层组成。本节将对此进行详细描述。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">我们的</span><span style="background-color:#FFFFFF;">SPD</span><span style="background-color:#FFFFFF;">组件推广了一种</span><span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">原始</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">图像转换技术</span><span style="background-color:#FFFFFF;">[29]</span><span style="background-color:#FFFFFF;">来对</span><span style="background-color:#FFFFFF;">CNN</span><span style="background-color:#FFFFFF;">内部和整个</span><span style="background-color:#FFFFFF;">CNN</span><span style="background-color:#FFFFFF;">的特征映射进行下采样，如下所示。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">考虑任意大小为</span><span style="background-color:#FFFFFF;">S × S × C1</span><span style="background-color:#FFFFFF;">的中间特征映射</span><span style="background-color:#FFFFFF;">X</span><span style="background-color:#FFFFFF;">，将子特征映射序列切片为：</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><img alt="" height="485" src="https://images2.imgbox.com/56/59/HPSBd71E_o.png" width="1200"></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;">一般来说，给定任何(原始)特征映射<em>X</em>，子映射<em>fx,y</em>由所有特征映射组成特征图<em>X</em>(<em>i, j</em>) ,<em>i </em>+<em> x</em>和<em>j</em> + <em>y</em>可以被比例整除。因此，每个子图按一个比例因子向下采样<em>X</em>。图3(a)(b)(c)给出了一个例子，当scale = 2时，我们得到四个子映射<em>f0,0</em>,<em>f1,0</em>,<em>f0,1</em>,<em>f1,1</em>，它们的形状为(<em>S</em>/2,<em>S</em>/2,<em>C</em>1)并对<em>X</em>进行2倍的下采样。</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">接下来，我们沿着通道维度将这些子特征映射连接起来，从而得到一个特征映射</span><span style="background-color:#FFFFFF;">X’</span><span style="background-color:#FFFFFF;">，它的空间维度减少了一个比例因子，通道维度增加了一个比例因子</span><span style="background-color:#FFFFFF;">2</span><span style="background-color:#FFFFFF;">。</span>  <img alt="" height="860" src="https://images2.imgbox.com/f2/83/buk4MvA1_o.png" width="1200"></span></p> 
<p style="margin-left:0;text-align:center;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">图</span><span style="background-color:#FFFFFF;">3:</span><span style="background-color:#FFFFFF;">当</span><span style="background-color:#FFFFFF;">scale = 2</span><span style="background-color:#FFFFFF;">时</span><span style="background-color:#FFFFFF;">SPD-Conv</span><span style="background-color:#FFFFFF;">的图解</span><span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">详见文本</span><span style="background-color:#FFFFFF;">)</span></span></p> 
<p><span style="color:#0d0016;"> </span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><strong><span style="background-color:#FFFFFF;">3.2 Non-strided</span></strong><strong><span style="background-color:#FFFFFF;">卷积</span></strong></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">在</span><span style="background-color:#FFFFFF;">SPD</span><span style="background-color:#FFFFFF;">特征转换层之后，我们添加一个带有</span><span style="background-color:#FFFFFF;">C2</span><span style="background-color:#FFFFFF;">过滤器的</span><span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">即</span><span style="background-color:#FFFFFF;">stride=1)</span><span style="background-color:#FFFFFF;">无卷积步长层，其中</span><span style="background-color:#FFFFFF;"> </span><em><span style="background-color:#FFFFFF;">C</span></em><em><span style="background-color:#FFFFFF;">2</span></em><span style="background-color:#FFFFFF;">&lt;  </span><em><span style="background-color:#FFFFFF;">scale</span></em><span style="background-color:#FFFFFF;">2</span><em><span style="background-color:#FFFFFF;">C1</span></em><img alt="" height="19" src="https://images2.imgbox.com/c0/e5/KRJlsBym_o.png" width="107"><span style="background-color:#FFFFFF;">，并进一步进行转换</span><em><span style="background-color:#FFFFFF;">X</span></em><span style="background-color:#FFFFFF;">'</span><em><span style="background-color:#FFFFFF;">S</span></em><em><span style="background-color:#FFFFFF;">scale</span></em><span style="background-color:#FFFFFF;">  , </span><em><span style="background-color:#FFFFFF;">S</span></em><em><span style="background-color:#FFFFFF;">scale</span></em><span style="background-color:#FFFFFF;">, </span><em><span style="background-color:#FFFFFF;">scale</span></em><span style="background-color:#FFFFFF;">2</span><em><span style="background-color:#FFFFFF;">C</span></em><em><span style="background-color:#FFFFFF;">1</span></em><span style="background-color:#FFFFFF;">→ </span><em><span style="background-color:#FFFFFF;">X</span></em><span style="background-color:#FFFFFF;">'' (</span><em><span style="background-color:#FFFFFF;">S</span></em><em><span style="background-color:#FFFFFF;">scale</span></em><span style="background-color:#FFFFFF;">  , </span><em><span style="background-color:#FFFFFF;">S</span></em><em><span style="background-color:#FFFFFF;">scale</span></em><span style="background-color:#FFFFFF;">, </span><em><span style="background-color:#FFFFFF;">C</span></em><em><span style="background-color:#FFFFFF;">2</span></em><span style="background-color:#FFFFFF;">)</span><img alt="" height="29" src="https://images2.imgbox.com/e8/7b/LH8RC3Qe_o.png" width="335"><span style="background-color:#FFFFFF;">。我们使用无步长卷积的原因是为了尽可能地保留</span> <span style="background-color:#FFFFFF;">所有的判别特征信息。否则，例如，使用</span><span style="background-color:#FFFFFF;">stride=3</span><span style="background-color:#FFFFFF;">的</span><span style="background-color:#FFFFFF;">3 × 3</span><span style="background-color:#FFFFFF;">过滤器，特征图将</span><span style="background-color:#FFFFFF;">“</span><span style="background-color:#FFFFFF;">缩小</span><span style="background-color:#FFFFFF;">”</span><span style="background-color:#FFFFFF;">，但每个像素</span> <span style="background-color:#FFFFFF;">只采样一次</span><span style="background-color:#FFFFFF;">;</span><span style="background-color:#FFFFFF;">如果</span><span style="background-color:#FFFFFF;">stride=2</span><span style="background-color:#FFFFFF;">，将发生非对称采样，其中偶数行</span><span style="background-color:#FFFFFF;">/</span><span style="background-color:#FFFFFF;">列和奇数行</span><span style="background-color:#FFFFFF;">/</span><span style="background-color:#FFFFFF;">列将被采样不同的次数。一般来说，大于</span><span style="background-color:#FFFFFF;">1</span><span style="background-color:#FFFFFF;">的步长会导致信息的非歧视性损失，尽管在表面上，它转换特征映射</span><em><span style="background-color:#FFFFFF;">X</span></em><span style="background-color:#FFFFFF;">'</span><em><span style="background-color:#FFFFFF;">S</span></em><em><span style="background-color:#FFFFFF;">scale</span></em><span style="background-color:#FFFFFF;">  , </span><em><span style="background-color:#FFFFFF;">S</span></em><em><span style="background-color:#FFFFFF;">scale</span></em><span style="background-color:#FFFFFF;">, </span><em><span style="background-color:#FFFFFF;">scale</span></em><span style="background-color:#FFFFFF;">2</span><em><span style="background-color:#FFFFFF;">C</span></em><em><span style="background-color:#FFFFFF;">1</span></em><span style="background-color:#FFFFFF;">→ <em>X</em>'' (</span><em><span style="background-color:#FFFFFF;">S</span></em><em><span style="background-color:#FFFFFF;">scale</span></em><span style="background-color:#FFFFFF;">  , </span><em><span style="background-color:#FFFFFF;">S</span></em><em><span style="background-color:#FFFFFF;">scale</span></em><span style="background-color:#FFFFFF;">,</span><em><span style="background-color:#FFFFFF;">C</span></em><em><span style="background-color:#FFFFFF;">2</span></em><span style="background-color:#FFFFFF;">)</span><img alt="" height="29" src="https://images2.imgbox.com/29/ce/vjyRbx1P_o.png" width="335"><span style="background-color:#FFFFFF;">(但没有</span><em><span style="background-color:#FFFFFF;">X</span></em><span style="background-color:#FFFFFF;">'</span><img alt="" height="19" src="https://images2.imgbox.com/2b/d7/zNQmPhE6_o.png" width="16"><span style="background-color:#FFFFFF;">)。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><strong><span style="background-color:#FFFFFF;">4.</span></strong><strong><span style="background-color:#FFFFFF;">如何使用</span></strong><strong><span style="background-color:#FFFFFF;">SPD-Conv:</span></strong><strong><span style="background-color:#FFFFFF;">案例研究</span></strong></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">为了解释如何应用我们提出的方法来重新设计</span><span style="background-color:#FFFFFF;">CNN</span><span style="background-color:#FFFFFF;">架构，我们使用了两个最具代表性的计算机视觉模型类别</span><span style="background-color:#FFFFFF;">:</span><span style="background-color:#FFFFFF;">目标检测和图像分类。这几乎没有丧失普遍性</span>,<span style="background-color:#FFFFFF;">CNN</span><span style="background-color:#FFFFFF;">架构使用步长卷积和</span><span style="background-color:#FFFFFF;">/</span><span style="background-color:#FFFFFF;">或池化操作来下采样特征图。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><strong><span style="background-color:#FFFFFF;">4.1</span></strong><strong><span style="background-color:#FFFFFF;">目标检测</span></strong></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">YOLO</span><span style="background-color:#FFFFFF;">是一系列非常流行的目标检测模型，其中我们选择最新的</span><span style="background-color:#FFFFFF;">YOLOv5[14]</span><span style="background-color:#FFFFFF;">进行演示。</span><span style="background-color:#FFFFFF;">YOLOv5</span><span style="background-color:#FFFFFF;">使用</span><span style="background-color:#FFFFFF;">CSPDarknet53[4]</span><span style="background-color:#FFFFFF;">，其中</span><span style="background-color:#FFFFFF;">SPP[12]</span><span style="background-color:#FFFFFF;">模块作为骨干，</span><span style="background-color:#FFFFFF;">PANet[23]</span><span style="background-color:#FFFFFF;">作为</span><span style="background-color:#FFFFFF;">neck</span><span style="background-color:#FFFFFF;">，</span><span style="background-color:#FFFFFF;">YOLOv3 head[26]</span><span style="background-color:#FFFFFF;">作为检测头。此外，它还使用了各种数据增强方法和</span>YOLOv4 <span style="background-color:#FFFFFF;">[4]</span><span style="background-color:#FFFFFF;">中的一些模块进行性能优化。它采用带</span><span style="background-color:#FFFFFF;">sigmoid</span><span style="background-color:#FFFFFF;">层的交叉熵损失计算目标性和分类损失，使用</span><span style="background-color:#FFFFFF;">CIoU</span><span style="background-color:#FFFFFF;">损失函数</span><span style="background-color:#FFFFFF;">[38]</span><span style="background-color:#FFFFFF;">计算定位损失。</span><span style="background-color:#FFFFFF;">CIoU</span><span style="background-color:#FFFFFF;">损耗比</span><span style="background-color:#FFFFFF;">IoU</span><span style="background-color:#FFFFFF;">损耗考虑的细节更多，如边缘重叠、中心距离和宽高比</span><span style="background-color:#FFFFFF;">。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><strong><span style="background-color:#FFFFFF;">YOLOv5-SPD.</span></strong><span style="background-color:#FFFFFF;">我们将第</span><span style="background-color:#FFFFFF;">3</span><span style="background-color:#FFFFFF;">节中描述的方法应用到</span><span style="background-color:#FFFFFF;">YOLOv5</span><span style="background-color:#FFFFFF;">，只需更换</span><span style="background-color:#FFFFFF;">YOLOv5 stride-2</span><span style="background-color:#FFFFFF;">卷积层即可得到</span><span style="background-color:#FFFFFF;">YOLOv5- SPD (</span><span style="background-color:#FFFFFF;">图</span><span style="background-color:#FFFFFF;">4)</span>，<span style="background-color:#FFFFFF;">用</span><span style="background-color:#FFFFFF;">SPD-Conv</span><span style="background-color:#FFFFFF;">构建块取代原有卷积。有</span><span style="background-color:#FFFFFF;">7</span><span style="background-color:#FFFFFF;">个这样的替换实例，因为</span><span style="background-color:#FFFFFF;">YOLOv5</span><span style="background-color:#FFFFFF;">在主干中使用</span><span style="background-color:#FFFFFF;">5</span><span style="background-color:#FFFFFF;">个</span><span style="background-color:#FFFFFF;">stride-2</span><span style="background-color:#FFFFFF;">卷积层对特征图进行</span>25<span style="background-color:#FFFFFF;">倍的下采样，在</span><span style="background-color:#FFFFFF;">neck</span><span style="background-color:#FFFFFF;">使用</span><span style="background-color:#FFFFFF;">2</span><span style="background-color:#FFFFFF;">个</span><span style="background-color:#FFFFFF;">stride-2</span><span style="background-color:#FFFFFF;">卷积层。在</span><span style="background-color:#FFFFFF;">YOLOv5 neck</span><span style="background-color:#FFFFFF;">中，每一次步长卷积后都有一个连接层</span><span style="background-color:#FFFFFF;">;</span><span style="background-color:#FFFFFF;">这并没有改变我们的方法，我们只是将其保持在</span><span style="background-color:#FFFFFF;">SPD</span><span style="background-color:#FFFFFF;">和</span><span style="background-color:#FFFFFF;">Conv</span><span style="background-color:#FFFFFF;">之间。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><strong><span style="background-color:#FFFFFF;">可伸缩性</span></strong><strong><span style="background-color:#FFFFFF;">.</span></strong><span style="background-color:#FFFFFF;">YOLOv5- SPD</span><span style="background-color:#FFFFFF;">可以满足不同的应用程序或硬件需求，以与</span><span style="background-color:#FFFFFF;">YOLOv5</span><span style="background-color:#FFFFFF;">相同的方式轻松放大和缩小。具体来说，我们可以简单地调整</span><span style="background-color:#FFFFFF;">(1)</span><span style="background-color:#FFFFFF;">每个无步长卷积层中的滤波器数量和</span><span style="background-color:#FFFFFF;">/</span><span style="background-color:#FFFFFF;">或</span><span style="background-color:#FFFFFF;">(2)C3</span><span style="background-color:#FFFFFF;">模块的重复次数</span><span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">如图</span><span style="background-color:#FFFFFF;">4</span><span style="background-color:#FFFFFF;">所示</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">，得到不同版本的</span><span style="background-color:#FFFFFF;">YOLOv5-SPD</span><span style="background-color:#FFFFFF;">。第一种方法称为宽度缩放，它将原始宽度</span><em>nw</em><img alt="" height="19" src="https://images2.imgbox.com/30/26/KukJRvfo_o.png" width="19"><span style="background-color:#FFFFFF;"> (通道数量</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">更改为</span><em>[</em><em>n</em><em>w</em> <em>× </em><em>widt</em><em>h</em><em>_</em><em>factor</em><em>]</em>8 <img alt="" height="19" src="https://images2.imgbox.com/e5/67/UIJHwPU8_o.png" width="162"> <span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">四舍四入到</span><span style="background-color:#FFFFFF;">8</span><span style="background-color:#FFFFFF;">的最接近倍数</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">。第二种被称为深度缩放，它改变了原来的深度</span><em>nd</em><img alt="" height="19" src="https://images2.imgbox.com/90/fe/Ld7qzAH8_o.png" width="17"><span style="background-color:#FFFFFF;"> (重复</span><span style="background-color:#FFFFFF;">C3</span><span style="background-color:#FFFFFF;">模块的次数</span><span style="background-color:#FFFFFF;">,</span><span style="background-color:#FFFFFF;">例如，图</span><span style="background-color:#FFFFFF;">4</span><span style="background-color:#FFFFFF;">中</span><span style="background-color:#FFFFFF;">9 × C3</span><span style="background-color:#FFFFFF;">中的</span><span style="background-color:#FFFFFF;">9)</span><span style="background-color:#FFFFFF;">到</span><em>[</em><em>n</em><em>d</em> <em>× depth_factor]</em><img alt="" height="19" src="https://images2.imgbox.com/1a/13/niNqCtdF_o.png" width="150"><span style="background-color:#FFFFFF;">。这样，通过选择不同的宽度/</span><span style="background-color:#FFFFFF;">深度系数，我们得到了</span><em>nano, small, medium</em>, 和<em>large</em><span style="background-color:#FFFFFF;">的</span><span style="background-color:#FFFFFF;">YOLOv5- SPD</span><span style="background-color:#FFFFFF;">，如表</span><span style="background-color:#FFFFFF;">2</span><span style="background-color:#FFFFFF;">所示，其中系数值的选择与</span><span style="background-color:#FFFFFF;">YOLOv5</span><span style="background-color:#FFFFFF;">相同，以便我们在后面的实验中进行比较。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><img alt="" height="1016" src="https://images2.imgbox.com/4c/97/argPRBWO_o.png" width="1188"></span></p> 
<p style="margin-left:0;text-align:center;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">表</span><span style="background-color:#FFFFFF;">2:</span><span style="background-color:#FFFFFF;">扩展</span><span style="background-color:#FFFFFF;">YOLOv5-SPD</span><span style="background-color:#FFFFFF;">以获得适合不同案例的不同版本</span><span style="background-color:#FFFFFF;">.</span></span></p> 
<p style="margin-left:0;text-align:center;"><span style="color:#0d0016;"><img alt="" height="281" src="https://images2.imgbox.com/d0/0f/nNhtStxY_o.png" width="804"></span></p> 
<p style="margin-left:0;text-align:center;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">表</span><span style="background-color:#FFFFFF;">3:</span><span style="background-color:#FFFFFF;">我们的</span><span style="background-color:#FFFFFF;">ResNet18-SPD</span><span style="background-color:#FFFFFF;">和</span><span style="background-color:#FFFFFF;">ResNet50-SPD</span><span style="background-color:#FFFFFF;">架构</span><span style="background-color:#FFFFFF;">.</span></span></p> 
<p style="margin-left:0;text-align:center;"><span style="color:#0d0016;"><img alt="" height="881" src="https://images2.imgbox.com/cc/de/PdfHPpuK_o.png" width="1129"></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><strong><span style="background-color:#FFFFFF;">4.2</span></strong><strong><span style="background-color:#FFFFFF;">图像分类</span></strong></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">分类</span><span style="background-color:#FFFFFF;">CNN</span><span style="background-color:#FFFFFF;">通常从一个</span><span style="background-color:#FFFFFF;">stem</span><span style="background-color:#FFFFFF;">单元开始，该单元由一个</span><span style="background-color:#FFFFFF;">stride-2</span><span style="background-color:#FFFFFF;">卷积和一个池化层组成，以降低四倍的图像分辨率。一个流行的模型是</span><span style="background-color:#FFFFFF;">ResNet[13]</span><span style="background-color:#FFFFFF;">，它赢得了</span><span style="background-color:#FFFFFF;">ILSVRC 2015</span><span style="background-color:#FFFFFF;">年的挑战赛。</span><span style="background-color:#FFFFFF;">ResNet</span><span style="background-color:#FFFFFF;">引入了残差连接，允许训练多达</span><span style="background-color:#FFFFFF;">152</span><span style="background-color:#FFFFFF;">层的网络。它还通过仅使用单个全连接层显著减少了参数的总数。最后采用</span><span style="background-color:#FFFFFF;">softmax</span><span style="background-color:#FFFFFF;">层对类预测进行规范化。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><strong><span style="background-color:#FFFFFF;">ResNet18-SPD</span></strong><strong><span style="background-color:#FFFFFF;">和</span></strong><strong><span style="background-color:#FFFFFF;"> ResNet50-SPD</span></strong><span style="background-color:#FFFFFF;">。</span><span style="background-color:#FFFFFF;">ResNet-18</span><span style="background-color:#FFFFFF;">和</span><span style="background-color:#FFFFFF;">ResNet-50</span><span style="background-color:#FFFFFF;">都使用了总共</span><span style="background-color:#FFFFFF;">4</span><span style="background-color:#FFFFFF;">个</span><span style="background-color:#FFFFFF;">stride-2</span><span style="background-color:#FFFFFF;">卷积和一个最大池化的</span><span style="background-color:#FFFFFF;">stride-2</span><span style="background-color:#FFFFFF;">层</span>，<span style="background-color:#FFFFFF;">对每个输入图像进行</span><span style="background-color:#FFFFFF;">25</span><span style="background-color:#FFFFFF;">倍的采样。应用我们提出的构建块，我们用</span><span style="background-color:#FFFFFF;">SPD-Conv</span><span style="background-color:#FFFFFF;">替换了四个跨步卷积</span><span style="background-color:#FFFFFF;">;</span><span style="background-color:#FFFFFF;">但另一方面，我们只是删除了最大池化层，因为我们的主要目标是低分辨率的图像，在我们的实验中使用的数据集是相当小的图像</span><span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">在</span><span style="background-color:#FFFFFF;">Tiny ImageNet</span><span style="background-color:#FFFFFF;">中</span><span style="background-color:#FFFFFF;">64 × 64</span><span style="background-color:#FFFFFF;">，在</span>CIFAR-10<span style="background-color:#FFFFFF;">中</span><span style="background-color:#FFFFFF;">32 × 32)</span><span style="background-color:#FFFFFF;">，因此池化是不必要的。对于较大的图像，这样的最大池化层仍然可以用</span><span style="background-color:#FFFFFF;">SPD-Conv</span><span style="background-color:#FFFFFF;">以同样的方式替换。这两个新的网络结构如表</span><span style="background-color:#FFFFFF;">3</span><span style="background-color:#FFFFFF;">所示。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><strong><span style="background-color:#FFFFFF;">5.</span></strong><strong><span style="background-color:#FFFFFF;">实验</span></strong></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">本节使用两个代表性的计算机视觉任务</span><span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">目标检测和图像分类</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">评估我们提出的</span><span style="background-color:#FFFFFF;">SPD-Conv</span><span style="background-color:#FFFFFF;">方法。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><strong><span style="background-color:#FFFFFF;">5.1</span></strong><strong><span style="background-color:#FFFFFF;">目标检测</span></strong></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><strong><span style="background-color:#FFFFFF;">数据集和设置</span></strong><strong><span style="background-color:#FFFFFF;">.</span></strong><span style="background-color:#FFFFFF;">我们使用</span><span style="background-color:#FFFFFF;">COCO-2017</span><span style="background-color:#FFFFFF;">数据集</span><span style="background-color:#FFFFFF;">[1]</span><span style="background-color:#FFFFFF;">，它分为</span><span style="background-color:#FFFFFF;">train2017(118,287</span><span style="background-color:#FFFFFF;">张图像</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">和</span><span style="background-color:#FFFFFF;">val2017(5000</span><span style="background-color:#FFFFFF;">张图像</span><span style="background-color:#FFFFFF;">;</span><span style="background-color:#FFFFFF;">也称为</span><span style="background-color:#FFFFFF;">minival)</span><span style="background-color:#FFFFFF;">用于验证，</span><span style="background-color:#FFFFFF;">test2017(40,670</span><span style="background-color:#FFFFFF;">张图像</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">用于测试。我们使用了广泛的最先进的基准模型，如表</span><span style="background-color:#FFFFFF;">4</span><span style="background-color:#FFFFFF;">和表</span><span style="background-color:#FFFFFF;">5</span><span style="background-color:#FFFFFF;">所示。我们给出了</span><span style="background-color:#FFFFFF;">val2017</span><span style="background-color:#FFFFFF;">在不同情况下的平均精度</span><span style="background-color:#FFFFFF;">(AP)</span><span style="background-color:#FFFFFF;">标准度量</span><span style="background-color:#FFFFFF;">IoU</span><span style="background-color:#FFFFFF;">阈值</span><span style="background-color:#FFFFFF;">[0.5:0.95]</span><span style="background-color:#FFFFFF;">和目标大小</span><span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">小、中、大</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">。我们还给出了</span><span style="background-color:#FFFFFF;">test-dev2017(20288</span><span style="background-color:#FFFFFF;">张图片</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">上的</span><span style="background-color:#FFFFFF;">AP</span><span style="background-color:#FFFFFF;">指标，它是</span><span style="background-color:#FFFFFF;">test2017</span><span style="background-color:#FFFFFF;">的一个子集，具有可访问的标签。然而，这些标签并没有公开发布，但需要将所有预测的标签在</span><span style="background-color:#FFFFFF;">JSON</span><span style="background-color:#FFFFFF;">文件中提交给</span><span style="background-color:#FFFFFF;">CodaLab COCO Detection</span><span style="background-color:#FFFFFF;">要求</span><span style="background-color:#FFFFFF;">[2]</span><span style="background-color:#FFFFFF;">检索已计算的指标，我们这样做了。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><strong><span style="background-color:#FFFFFF;">训练</span></strong><strong><span style="background-color:#FFFFFF;">.</span></strong><span style="background-color:#FFFFFF;">我们训练不同版本</span>(nano, small, medium, and large)<span style="background-color:#FFFFFF;">的</span><span style="background-color:#FFFFFF;">YOLOv5-SPD</span><span style="background-color:#FFFFFF;">和</span><span style="background-color:#FFFFFF;">train2017</span><span style="background-color:#FFFFFF;">上的所有基准模型。与大多数其他研究不同的是，我们从头开始训练，不使用迁移学习。这是因为我们希望检查每个模型的真正学习能力，而不被从理想</span><span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">高质量</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">数据集</span><span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">如</span><span style="background-color:#FFFFFF;">ImageNet)</span><span style="background-color:#FFFFFF;">的迁移学习继承的丰富特征表示所掩盖。这是在我们自己的模型</span>(<em>∗</em>-SPD-n/s/m/l)<span style="background-color:#FFFFFF;">和所有现有的</span><span style="background-color:#FFFFFF;">YOLO</span><span style="background-color:#FFFFFF;">系列模型</span><span style="background-color:#FFFFFF;">(v5, X, v4</span><span style="background-color:#FFFFFF;">，以及它们的缩放版本，如</span><span style="background-color:#FFFFFF;">nano, small, large</span><span style="background-color:#FFFFFF;">等</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">上进行的。其他基准模型仍然使用迁移学习，因为我们缺乏资源</span><span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">从头开始训练消耗大量</span><span style="background-color:#FFFFFF;">GPU</span><span style="background-color:#FFFFFF;">时间</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">。然而，请注意，这仅仅意味着这些基准被置于比我们自己的模型更有利的位置，因为它们受益于高质量的数据集。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">我们选择</span>momentum<span style="background-color:#FFFFFF;">为</span><span style="background-color:#FFFFFF;">0.937</span><span style="background-color:#FFFFFF;">，权值衰减为</span><span style="background-color:#FFFFFF;">0.0005</span><span style="background-color:#FFFFFF;">的</span><span style="background-color:#FFFFFF;">SGD</span><span style="background-color:#FFFFFF;">优化器。在三个热身阶段，学习率从</span><span style="background-color:#FFFFFF;">0.0033</span><span style="background-color:#FFFFFF;">线性增加到</span><span style="background-color:#FFFFFF;">0.01</span><span style="background-color:#FFFFFF;">，随后使用余弦衰减策略降低到最终值</span><span style="background-color:#FFFFFF;">0.001</span><span style="background-color:#FFFFFF;">。</span><span style="background-color:#FFFFFF;">nano</span><span style="background-color:#FFFFFF;">和</span><span style="background-color:#FFFFFF;">small</span><span style="background-color:#FFFFFF;">模型训练在</span><span style="background-color:#FFFFFF;">4</span><span style="background-color:#FFFFFF;">个</span><span style="background-color:#FFFFFF;">V-100 32GB GPU</span><span style="background-color:#FFFFFF;">上，</span><span style="background-color:#FFFFFF;">batch size</span><span style="background-color:#FFFFFF;">为</span><span style="background-color:#FFFFFF;">128</span><span style="background-color:#FFFFFF;">，</span><em>medium </em>和 <em>large</em><span style="background-color:#FFFFFF;">训练在</span><span style="background-color:#FFFFFF;">batch size</span><span style="background-color:#FFFFFF;">为</span><span style="background-color:#FFFFFF;">32</span><span style="background-color:#FFFFFF;">。</span>目标和分类损失函数采用<span style="background-color:#FFFFFF;">CIoU</span><span style="background-color:#FFFFFF;">损失</span><span style="background-color:#FFFFFF;">[38]</span><span style="background-color:#FFFFFF;">和交叉熵损失。我们还采用了几种数据增强技术来减轻过拟合，并提高所有模型的性能</span><span style="background-color:#FFFFFF;">;</span><span style="background-color:#FFFFFF;">这些技术包括</span><span style="background-color:#FFFFFF;">(i)</span><span style="background-color:#FFFFFF;">色调、饱和度和值的光度失真，</span><span style="background-color:#FFFFFF;">(ii)</span><span style="background-color:#FFFFFF;">几何失真，如平移、缩放、剪切、左右翻转</span><span style="background-color:#FFFFFF;">fliplr</span><span style="background-color:#FFFFFF;">和上下翻转</span><span style="background-color:#FFFFFF;">flipup</span><span style="background-color:#FFFFFF;">，以及</span><span style="background-color:#FFFFFF;">(iii)</span><span style="background-color:#FFFFFF;">多图像增强技术，如马赛克和切割混合</span><span style="background-color:#FFFFFF;">cutmix</span><span style="background-color:#FFFFFF;">。注意，在推断时不使用数据增强。超参数采用</span><span style="background-color:#FFFFFF;">YOLOv5</span><span style="background-color:#FFFFFF;">，无需重新调优。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><strong> </strong><strong><span style="background-color:#FFFFFF;">结果</span>。</strong><span style="background-color:#FFFFFF;">表</span><span style="background-color:#FFFFFF;">4</span><span style="background-color:#FFFFFF;">给出了</span><span style="background-color:#FFFFFF;">val2017</span><span style="background-color:#FFFFFF;">上的结果，表</span><span style="background-color:#FFFFFF;">5</span><span style="background-color:#FFFFFF;">给出了</span><span style="background-color:#FFFFFF;">test-dev</span><span style="background-color:#FFFFFF;">上的结果。两个表中的</span>APS<em>, </em>APM<em>, </em>APL<span style="background-color:#FFFFFF;">指的是小型</span><span style="background-color:#FFFFFF;">/</span><span style="background-color:#FFFFFF;">中型</span><span style="background-color:#FFFFFF;">/</span><span style="background-color:#FFFFFF;">大型目标的</span><span style="background-color:#FFFFFF;">AP</span><span style="background-color:#FFFFFF;">，不应与模型比例</span>(nano, small,medium, large)<span style="background-color:#FFFFFF;">混淆。两个表中所示的图像分辨率</span><span style="background-color:#FFFFFF;">640 × 640</span><span style="background-color:#FFFFFF;">在目标检测</span><span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">与图像分类相反</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">中并不被认为是高的，因为实际物体上的分辨率要低得多，特别是当目标很小的时候。</span></span></p> 
<p style="margin-left:0;text-align:center;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">表</span><span style="background-color:#FFFFFF;">4:MS-COCO</span><span style="background-color:#FFFFFF;">验证数据集</span><span style="background-color:#FFFFFF;">(val2017)</span><span style="background-color:#FFFFFF;">的比较</span><span style="background-color:#FFFFFF;">.</span></span></p> 
<p style="margin-left:0;text-align:center;"><span style="color:#0d0016;"><img alt="" height="1114" src="https://images2.imgbox.com/a9/e3/MQXw10BG_o.png" width="1200"></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">val2017</span><span style="background-color:#FFFFFF;">结果</span><span style="background-color:#FFFFFF;">.</span><span style="background-color:#FFFFFF;">表</span><span style="background-color:#FFFFFF;">4</span><span style="background-color:#FFFFFF;">按模型比例表示，用水平线分隔</span><span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">最后一组是大目标检测模型</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">。在第一类</span><span style="background-color:#FFFFFF;">nano</span><span style="background-color:#FFFFFF;">模型中，我们的</span><span style="background-color:#FFFFFF;">YOLOv5-SPD-n</span><span style="background-color:#FFFFFF;">的</span><span style="background-color:#FFFFFF;">AP</span><span style="background-color:#FFFFFF;">和</span>APS<span style="background-color:#FFFFFF;">性能都是最好</span><span style="background-color:#FFFFFF;">:</span><span style="background-color:#FFFFFF;">其</span>APS<span style="background-color:#FFFFFF;">比第二</span><span style="background-color:#FFFFFF;">YOLOv5n</span><span style="background-color:#FFFFFF;">高</span><span style="background-color:#FFFFFF;">13.15%</span><span style="background-color:#FFFFFF;">，整体</span><span style="background-color:#FFFFFF;">AP</span><span style="background-color:#FFFFFF;">比第二名</span><span style="background-color:#FFFFFF;">YOLOv5n</span><span style="background-color:#FFFFFF;">高</span><span style="background-color:#FFFFFF;">10.7%</span><span style="background-color:#FFFFFF;">。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">在第二类</span><span style="background-color:#FFFFFF;">small</span><span style="background-color:#FFFFFF;">中，我们的</span><span style="background-color:#FFFFFF;">YOLOv5-SPD-s</span><span style="background-color:#FFFFFF;">在</span><span style="background-color:#FFFFFF;">AP</span><span style="background-color:#FFFFFF;">和</span>APS<span style="background-color:#FFFFFF;">上的表现都是最好的，尽管这次</span><span style="background-color:#FFFFFF;">YOLOX-S</span><span style="background-color:#FFFFFF;">在</span><span style="background-color:#FFFFFF;">AP</span><span style="background-color:#FFFFFF;">上是第二好的。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">在第三种</span>medium<span style="background-color:#FFFFFF;">中，虽然我们的</span><span style="background-color:#FFFFFF;">YOLOv5-SPD-m</span><span style="background-color:#FFFFFF;">仍然优于其他型号，但</span><span style="background-color:#FFFFFF;">AP</span><span style="background-color:#FFFFFF;">性能非常接近。另一方面，我们的</span>APS<span style="background-color:#FFFFFF;">比第二名有更大的获胜优势</span><span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">高出</span><span style="background-color:#FFFFFF;">8.6%)</span><span style="background-color:#FFFFFF;">，这是一个好迹象，因为</span><span style="background-color:#FFFFFF;">SPD-Conv</span><span style="background-color:#FFFFFF;">对于较小的物体和较低的分辨率尤其有利。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">最后，对于</span>large<span style="background-color:#FFFFFF;">模型，</span><span style="background-color:#FFFFFF;">YOLOX-L</span><span style="background-color:#FFFFFF;">获得了最好的</span><span style="background-color:#FFFFFF;">AP</span><span style="background-color:#FFFFFF;">，而我们的</span><span style="background-color:#FFFFFF;">YOLOv5-SPD-l</span><span style="background-color:#FFFFFF;">仅略低</span><span style="background-color:#FFFFFF;">(3%)(</span><span style="background-color:#FFFFFF;">但比底部组中的其他基准模型要好得多</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">。另一方面，我们的</span>APS<span style="background-color:#FFFFFF;">仍然是最高的，这与</span><span style="background-color:#FFFFFF;">SPD-Conv</span><span style="background-color:#FFFFFF;">的上述优势相呼应。</span></span></p> 
<p style="margin-left:0;text-align:center;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">表</span><span style="background-color:#FFFFFF;">5:MS-COCO</span><span style="background-color:#FFFFFF;">测试数据集的比较</span><span style="background-color:#FFFFFF;">(test-dev2017).</span></span></p> 
<p><span style="color:#0d0016;"><img alt="" height="621" src="https://images2.imgbox.com/55/f5/nZhGRo3U_o.png" width="791"></span></p> 
<p><span style="color:#0d0016;"> </span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><strong><span style="background-color:#FFFFFF;">test-dev2017</span></strong><strong><span style="background-color:#FFFFFF;">结果</span></strong><strong><span style="background-color:#FFFFFF;">.</span></strong><span style="background-color:#FFFFFF;">如表</span><span style="background-color:#FFFFFF;">5</span><span style="background-color:#FFFFFF;">所示，我们的</span><span style="background-color:#FFFFFF;">YOLOv5-SPD-n</span><span style="background-color:#FFFFFF;">再次成为</span>APS<span style="background-color:#FFFFFF;">上</span><span style="background-color:#FFFFFF;">nano</span><span style="background-color:#FFFFFF;">模型类别的明显赢家，领先</span><span style="background-color:#FFFFFF;">YOLOv5n 19%</span><span style="background-color:#FFFFFF;">。</span>对于平均<span style="background-color:#FFFFFF;">AP</span><span style="background-color:#FFFFFF;">，虽然看起来似乎</span>EfficientDet-D0<span style="background-color:#FFFFFF;">比我们的表现更好，这是因为</span>EfficientDet<span style="background-color:#FFFFFF;">的参数几乎是我们的两倍，并且使用高分辨率图像进行训练</span><span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">通过转移学习，如单元中的</span><span style="background-color:#FFFFFF;">“Trf”</span><span style="background-color:#FFFFFF;">所示</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">，</span><span style="background-color:#FFFFFF;">AP</span><span style="background-color:#FFFFFF;">值与分辨率高度相关。这种训练的好处也同样反映在小模型类别中。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">尽管其他基准模型获得了这种好处，但我们的方法在下一个类别中，即在</span><span style="background-color:#FFFFFF;">AP</span><span style="background-color:#FFFFFF;">和</span>APS<span style="background-color:#FFFFFF;">上的中型模型中重新获得了最高排名。最后，在</span><span style="background-color:#FFFFFF;">large</span><span style="background-color:#FFFFFF;">类别中，我们</span><span style="background-color:#FFFFFF;">YOLOv5-SPD-l </span><span style="background-color:#FFFFFF;">的</span>APS<span style="background-color:#FFFFFF;">也是性能最好的，在</span><span style="background-color:#FFFFFF;">AP</span><span style="background-color:#FFFFFF;">上的</span><span style="background-color:#FFFFFF;">YOLOX-L</span><span style="background-color:#FFFFFF;">非常接近。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><strong><span style="background-color:#FFFFFF;">总结</span></strong><span style="background-color:#FFFFFF;">.</span><span style="background-color:#FFFFFF;">很明显，通过简单地用我们提出的</span><span style="background-color:#FFFFFF;">SPD-Conv</span><span style="background-color:#FFFFFF;">构建块替换卷积步长和池化层，神经网络可以显著提高其精度，同时保持相同的参数大小水平。当目标很小时，这种改进更加突出，这很好地满足了我们的目标。</span>尽管我们并不是在所有情况下都能够占据第一名的位置，但<span style="background-color:#FFFFFF;">SPD-Conv</span><span style="background-color:#FFFFFF;">是唯一能够始终保持良好表现的方法</span><span style="background-color:#FFFFFF;">;</span><span style="background-color:#FFFFFF;">如果不是表现最好的话，它只是偶尔会成为</span><span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">非常接近的</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">亚军，而在我们的主要目标指标</span>APS<span style="background-color:#FFFFFF;">上，它总是赢家。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">最后，回想一下，我们采用了</span><span style="background-color:#FFFFFF;">YOLOv5</span><span style="background-color:#FFFFFF;">超参数而没有进行返回调优，这意味着我们的模型在进行专门的超参数调优后可能会表现得更好。还记得所有非</span><span style="background-color:#FFFFFF;">YOLO</span><span style="background-color:#FFFFFF;">基准模型</span><span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">和</span><span style="background-color:#FFFFFF;">PP-YOLO)</span><span style="background-color:#FFFFFF;">使用迁移学习进行训练，因此受益于高质量的图像，而我们的没有。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><strong><span style="background-color:#FFFFFF;">视觉对比</span></strong><span style="background-color:#FFFFFF;">。为了视觉和直观的理解，我们使用随机选择的两幅图像提供了两个真实的例子，如图</span><span style="background-color:#FFFFFF;">5</span><span style="background-color:#FFFFFF;">所示。我们比较</span><span style="background-color:#FFFFFF;">YOLOv5-SPD-m</span><span style="background-color:#FFFFFF;">和</span><span style="background-color:#FFFFFF;">YOLOv5m</span><span style="background-color:#FFFFFF;">，因为后者是相应</span><span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">中等</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">类别中所有基准模型中性能最好的。图</span><span style="background-color:#FFFFFF;">5(a)(b)</span><span style="background-color:#FFFFFF;">表明</span><span style="background-color:#FFFFFF;">YOLOv5-SPD-m</span><span style="background-color:#FFFFFF;">能够检测到被遮挡的长颈鹿，</span><span style="background-color:#FFFFFF;">YOLOv5m</span><span style="background-color:#FFFFFF;">没有检测到，图</span><span style="background-color:#FFFFFF;">5(c)(d)</span><span style="background-color:#FFFFFF;">显示</span><span style="background-color:#FFFFFF;">YOLOv5-SPD-m</span><span style="background-color:#FFFFFF;">检测到非常小的目标</span><span style="background-color:#FFFFFF;">(</span><span style="background-color:#FFFFFF;">一张脸和两个长凳</span><span style="background-color:#FFFFFF;">)</span><span style="background-color:#FFFFFF;">，而</span><span style="background-color:#FFFFFF;">YOLOv5m</span><span style="background-color:#FFFFFF;">检测不到。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><img alt="" height="1095" src="https://images2.imgbox.com/dc/ea/q7mVhYGe_o.png" width="1200"><span style="background-color:#FFFFFF;">图5:val2017</span><span style="background-color:#FFFFFF;">的目标检测示例。蓝框表示</span>ground truth<span style="background-color:#FFFFFF;">。红色箭头强调了差异。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><strong><span style="background-color:#FFFFFF;">5.2</span></strong><strong><span style="background-color:#FFFFFF;">图像分类</span></strong></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><strong><span style="background-color:#FFFFFF;">数据集和设置</span></strong><span style="background-color:#FFFFFF;">。对于图像分类任务，我们使用了</span><span style="background-color:#FFFFFF;">Tiny ImageNet[19]</span><span style="background-color:#FFFFFF;">和</span><span style="background-color:#FFFFFF;">CIFAR-10</span><span style="background-color:#FFFFFF;">数据集</span><span style="background-color:#FFFFFF;">[17]</span><span style="background-color:#FFFFFF;">。</span><span style="background-color:#FFFFFF;">Tiny ImageNet</span><span style="background-color:#FFFFFF;">是一个</span><span style="background-color:#FFFFFF;">ILSVRC-2012</span><span style="background-color:#FFFFFF;">分类数据集子集，包含</span><span style="background-color:#FFFFFF;">200</span><span style="background-color:#FFFFFF;">个类。每个类都有</span><span style="background-color:#FFFFFF;">500</span><span style="background-color:#FFFFFF;">张训练图，</span><span style="background-color:#FFFFFF;">50</span><span style="background-color:#FFFFFF;">张验证图，</span><span style="background-color:#FFFFFF;">50</span><span style="background-color:#FFFFFF;">张测试图。每张图片的分辨率为</span><span style="background-color:#FFFFFF;">64×64×3</span><span style="background-color:#FFFFFF;">像素。</span><span style="background-color:#FFFFFF;">CIFAR-10</span><span style="background-color:#FFFFFF;">包含</span><span style="background-color:#FFFFFF;">6</span><span style="background-color:#FFFFFF;">万张分辨率</span><span style="background-color:#FFFFFF;">32 × 32 × 3</span><span style="background-color:#FFFFFF;">的图像，包括</span><span style="background-color:#FFFFFF;">5</span><span style="background-color:#FFFFFF;">万张训练图像和</span><span style="background-color:#FFFFFF;">1</span><span style="background-color:#FFFFFF;">万张测试图像。有</span><span style="background-color:#FFFFFF;">10</span><span style="background-color:#FFFFFF;">个类，每个类有</span><span style="background-color:#FFFFFF;">6000</span><span style="background-color:#FFFFFF;">张图片。我们使用准确率第一作为评价分类性能的指标。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><strong><span style="background-color:#FFFFFF;">训练</span></strong><span style="background-color:#FFFFFF;">。我们在</span><span style="background-color:#FFFFFF;">Tiny ImageNet</span><span style="background-color:#FFFFFF;">上训练</span><span style="background-color:#FFFFFF;">ReseNet18-SPD</span><span style="background-color:#FFFFFF;">模型。我们执行随机网格搜索来优化超参数，包括学习率、批大小、动量、优化器和权值衰减。图</span><span style="background-color:#FFFFFF;">6</span><span style="background-color:#FFFFFF;">展示了使用</span>wandb MLOPs<span style="background-color:#FFFFFF;">生成的样本超参数扫描图。结果是选择</span><span style="background-color:#FFFFFF;">SGD</span><span style="background-color:#FFFFFF;">优化器，学习率为</span><span style="background-color:#FFFFFF;">0.01793</span><span style="background-color:#FFFFFF;">，动量为</span><span style="background-color:#FFFFFF;">0.9447</span><span style="background-color:#FFFFFF;">，最小批大小为</span><span style="background-color:#FFFFFF;">256</span><span style="background-color:#FFFFFF;">，权值衰减正则化为</span><span style="background-color:#FFFFFF;">0.002113</span><span style="background-color:#FFFFFF;">，训练轮次为</span><span style="background-color:#FFFFFF;">200</span><span style="background-color:#FFFFFF;">。接下来，我们</span><span style="background-color:#FFFFFF;">CIFAR-10</span><span style="background-color:#FFFFFF;">上训练我们的</span><span style="background-color:#FFFFFF;">ResNet50-SPD</span><span style="background-color:#FFFFFF;">模型。超参数采用</span><span style="background-color:#FFFFFF;">ResNet50</span><span style="background-color:#FFFFFF;">论文中的超参数，其中</span><span style="background-color:#FFFFFF;">SGD</span><span style="background-color:#FFFFFF;">优化器的初始学习率为</span><span style="background-color:#FFFFFF;">0.1</span><span style="background-color:#FFFFFF;">，动量为</span><span style="background-color:#FFFFFF;">0.9</span><span style="background-color:#FFFFFF;">，批大小为</span><span style="background-color:#FFFFFF;">128</span><span style="background-color:#FFFFFF;">，权值衰减正则化</span><span style="background-color:#FFFFFF;">0.0001</span><span style="background-color:#FFFFFF;">，训练轮次为</span><span style="background-color:#FFFFFF;">200</span><span style="background-color:#FFFFFF;">。对于</span><span style="background-color:#FFFFFF;">ReseNet18-SPD</span><span style="background-color:#FFFFFF;">和</span><span style="background-color:#FFFFFF;">ReseNet50-SPD</span><span style="background-color:#FFFFFF;">，我们使用与</span><span style="background-color:#FFFFFF;">ResNet</span><span style="background-color:#FFFFFF;">中相同的衰减函数，随着</span><span style="background-color:#FFFFFF;">epoch</span><span style="background-color:#FFFFFF;">数的增加而降低学习率。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><img alt="" height="862" src="https://images2.imgbox.com/b4/b7/B2wU8Mtu_o.png" width="1200"></span></p> 
<p style="margin-left:0;text-align:center;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">图</span><span style="background-color:#FFFFFF;">6:</span><span style="background-color:#FFFFFF;">图像分类中的超参数调节</span><span style="background-color:#FFFFFF;">:</span><span style="background-color:#FFFFFF;">使用</span><span style="background-color:#FFFFFF;">wandb</span><span style="background-color:#FFFFFF;">的扫描图。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><strong><span style="background-color:#FFFFFF;">测试</span></strong><span style="background-color:#FFFFFF;">。</span><span style="background-color:#FFFFFF;">Tiny ImageNet</span><span style="background-color:#FFFFFF;">上的准确性是在验证数据集上评估的，因为测试数据集中的基本</span>ground truth<span style="background-color:#FFFFFF;">是不可用的。在测试数据集上计算</span><span style="background-color:#FFFFFF;">CIFAR-10</span><span style="background-color:#FFFFFF;">的精度。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><strong><span style="background-color:#FFFFFF;">结果</span></strong><span style="background-color:#FFFFFF;">。表</span><span style="background-color:#FFFFFF;">6</span><span style="background-color:#FFFFFF;">总结了</span><span style="background-color:#FFFFFF;">top-1</span><span style="background-color:#FFFFFF;">精度的结果。结果表明，我们的模型</span><span style="background-color:#FFFFFF;">ResNet18-SPDResNet50-SPD</span><span style="background-color:#FFFFFF;">明显优于所有其他基准模型。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">最后，我们在图</span><span style="background-color:#FFFFFF;">7</span><span style="background-color:#FFFFFF;">中提供了一个使用</span><span style="background-color:#FFFFFF;">Tiny ImageNet</span><span style="background-color:#FFFFFF;">的可视化插图。给出了</span><span style="background-color:#FFFFFF;">8</span><span style="background-color:#FFFFFF;">个被</span><span style="background-color:#FFFFFF;">ResNet18</span><span style="background-color:#FFFFFF;">错误分类和被</span><span style="background-color:#FFFFFF;">ResNet18-SPD</span><span style="background-color:#FFFFFF;">正确分类的例子</span>。<span style="background-color:#FFFFFF;">这些图像的共同特征是分辨率较低，因此对标准</span><span style="background-color:#FFFFFF;">ResNet</span><span style="background-color:#FFFFFF;">提出了挑战，后者在卷积步长和池化操作中丢失了细粒度信息。</span></span></p> 
<p><span style="color:#0d0016;"> </span></p> 
<p style="margin-left:0;text-align:center;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">表</span><span style="background-color:#FFFFFF;">6:图像分类性能比较</span><img alt="" height="563" src="https://images2.imgbox.com/3a/79/4iTdo6Td_o.png" width="1200"></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><img alt="" height="956" src="https://images2.imgbox.com/fb/a4/fWeTq32i_o.png" width="1200"></span></p> 
<p style="margin-left:0;text-align:center;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;">图</span><span style="background-color:#FFFFFF;">7:</span><span style="background-color:#FFFFFF;">绿色标签</span><span style="background-color:#FFFFFF;">:ground truth</span><span style="background-color:#FFFFFF;">。蓝色标签</span><span style="background-color:#FFFFFF;">:ResNet18-SPD</span><span style="background-color:#FFFFFF;">预测。红色标签</span><span style="background-color:#FFFFFF;">:ResNet-18</span><span style="background-color:#FFFFFF;">预测</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#0d0016;"><strong><span style="background-color:#FFFFFF;">6.</span></strong><strong><span style="background-color:#FFFFFF;">结论</span></strong></span></p> 
<p style="margin-left:0cm;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#FFFFFF;"><span style="background-color:#FFFFFF;">本文指出了现有</span><span style="background-color:#FFFFFF;">CNN</span><span style="background-color:#FFFFFF;">体系结构中一个常见但有缺陷的设计，即使用卷积步长和</span><span style="background-color:#FFFFFF;">/</span><span style="background-color:#FFFFFF;">或池化层。这将导致细粒度特征信息的丢失，特别是在低分辨率图像和小物体上。然后，我们提出了一个名为</span><span style="background-color:#FFFFFF;">SPD-Conv</span><span style="background-color:#FFFFFF;">的新的</span><span style="background-color:#FFFFFF;">CNN</span><span style="background-color:#FFFFFF;">构建块，它完全消除了步长和池化操作，取而代之的是一个空间到深度卷积和一个无步长卷积。这种新设计在保留判别特征信息的同时，具有向下采样特征图的优点</span>。它还代表了一种通用的统一方法，可以很容易地应用于任何CNN体系结构，也可以以同样的方式应用于跨步转换和池。我们提供了两个最具代表性的用例，目标检测和图像分类，并通过广泛的评估证明SPD-Conv在检测和分类精度方面带来了显著的性能改进。我们预计它将广泛惠及研究社区，因为它可以很容易地集成到现有的深度学习框架中，如PyTorch和TensorFlow。</span></span></p> 
<h4 style="margin-left:0cm;text-align:justify;"><span style="color:#0d0016;"><strong><span style="background-color:#FFFFFF;">百度网盘链接：</span></strong></span></h4> 
<blockquote> 
 <p style="margin-left:0cm;text-align:justify;"><span style="color:#0d0016;"><strong><span style="background-color:#FFFFFF;">链接：https://pan.baidu.com/s/1-YnmEU6d_x0N7S1xspUFeA <br> 提取码：yyds</span></strong></span></p> 
</blockquote>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/625c87973600d5defad67b80bd26e02c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Python入门 - 数据类型</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6eca3dcc78eb5ea859b8a3a22f3c3f5d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">基于Matlab的自动控制系统频率法串联校正仿真分析（含仿真代码和结果图像）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>