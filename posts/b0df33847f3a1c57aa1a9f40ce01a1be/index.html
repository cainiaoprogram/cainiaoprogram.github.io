<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【深度学习：Recurrent Neural Networks】循环神经网络（RNN）的简要概述 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【深度学习：Recurrent Neural Networks】循环神经网络（RNN）的简要概述" />
<meta property="og:description" content="【深度学习】循环神经网络（RNN）：连接过去与未来的桥梁 循环神经网络简介什么是循环神经网络 (RNN)？传统 RNN 的架构循环神经网络如何工作？常用激活函数RNN的优点和缺点RNN 的优点：RNN 的缺点： 循环神经网络与前馈神经网络随时间反向传播 (BPTT)标准 RNN 的两个问题RNN 应用基本 Python 实现（RNN 与 Keras）经常问的问题结论 苹果的Siri和谷歌的语音搜索都使用递归神经网络（RNN），这是最先进的顺序数据方法。这是第一个具有内部存储器的算法，可以记住其输入，使其非常适合机器学习中涉及顺序数据的问题。它是在过去几年中导致深度学习取得惊人进步的算法之一。在本文中，我们将介绍递归神经网络的基础知识，以及最紧迫的困难以及如何解决它们。
循环神经网络简介 用于对顺序数据进行建模的深度学习方法是循环神经网络 （RNN）。在注意力模型出现之前，RNN是处理顺序数据的标准建议。深度前馈模型可能需要序列中每个元素的特定参数。它也可能无法泛化为可变长度序列。
递归神经网络对序列的每个元素使用相同的权重，从而减少了参数的数量，并允许模型泛化为不同长度的序列。由于其设计，RNN 泛化到序列数据以外的结构化数据，例如地理或图形数据。
与许多其他深度学习技术一样，循环神经网络相对较旧。它们最初是在 20 世纪 80 年代开发的，但直到最近我们才充分认识到它们的潜力。 20 世纪 90 年代长短期记忆 (LSTM) 的出现，加上计算能力的提高和我们现在必须处理的大量数据，确实将 RNN 推到了最前沿。
什么是循环神经网络 (RNN)？ 神经网络在人工智能、机器学习和深度学习领域模仿人脑的功能，使计算机程序能够识别模式并解决常见问题。
RNN 是一种可用于对序列数据建模的神经网络。 RNN 由前馈网络组成，其行为与人脑相似。简而言之，循环神经网络可以以其他算法无法做到的方式预测顺序数据。
标准神经网络中的所有输入和输出都是相互独立的，但是在某些情况下，例如在预测短语的下一个单词时，前面的单词是必要的，因此必须记住前面的单词。结果，RNN 应运而生，它使用隐藏层来克服这个问题。 RNN 最重要的组成部分是隐藏状态，它记住有关序列的特定信息。
RNN 有一个内存，用于存储有关计算的所有信息。它对每个输入采用相同的设置，因为它通过在所有输入或隐藏层上执行相同的任务来产生相同的结果。
传统 RNN 的架构 RNN 是一种具有隐藏状态并允许将过去的输出用作输入的神经网络。他们通常是这样的：
RNN 架构可能会根据您要解决的问题而有所不同。从具有单个输入和输出的那些到具有多个输入和输出的那些（之间存在差异）。
下面是一些 RNN 架构的示例，可以帮助您更好地理解这一点。
一对一：这里只有一对。传统神经网络使用一对一架构。 一对多：一对多网络中的单个输入可能会产生大量输出。例如，在音乐制作中使用了太多的网络。多对一：在这种情况下，单个输出是通过组合来自不同时间步骤的多个输入来生成的。情感分析和情绪识别使用这样的网络，其中类别标签由单词序列确定。多对多：对于多对多，有多种选择。两个输入产生三个输出。机器翻译系统，例如英语到法语的翻译系统，反之亦然，使用多对多网络。 循环神经网络如何工作？ 循环神经网络中的信息通过循环循环到中间隐藏层。
输入层 x 接收并处理神经网络的输入，然后将其传递到中间层。
中间层 h 可以找到多个隐藏层，每个隐藏层都有自己的激活函数、权重和偏差。如果不同隐藏层的各种参数不受前一层的影响，即神经网络中没有记忆，则可以使用循环神经网络。
不同的激活函数、权重和偏差将由循环神经网络标准化，确保每个隐藏层具有相同的特征。它不会构建大量隐藏层，而是仅创建一个隐藏层并根据需要循环多次。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/b0df33847f3a1c57aa1a9f40ce01a1be/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-01T13:49:33+08:00" />
<meta property="article:modified_time" content="2024-01-01T13:49:33+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【深度学习：Recurrent Neural Networks】循环神经网络（RNN）的简要概述</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>【深度学习】循环神经网络（RNN）：连接过去与未来的桥梁</h4> 
 <ul><li><ul><li><ul><li><a href="#_5" rel="nofollow">循环神经网络简介</a></li><li><a href="#_RNN_14" rel="nofollow">什么是循环神经网络 (RNN)？</a></li><li><a href="#_RNN__25" rel="nofollow">传统 RNN 的架构</a></li><li><a href="#_39" rel="nofollow">循环神经网络如何工作？</a></li><li><a href="#_51" rel="nofollow">常用激活函数</a></li><li><a href="#RNN_62" rel="nofollow">RNN的优点和缺点</a></li><li><ul><li><a href="#RNN__63" rel="nofollow">RNN 的优点：</a></li><li><a href="#RNN__70" rel="nofollow">RNN 的缺点：</a></li></ul> 
    </li><li><a href="#_76" rel="nofollow">循环神经网络与前馈神经网络</a></li><li><a href="#_BPTT_87" rel="nofollow">随时间反向传播 (BPTT)</a></li><li><a href="#_RNN__96" rel="nofollow">标准 RNN 的两个问题</a></li><li><a href="#RNN__107" rel="nofollow">RNN 应用</a></li><li><a href="#_Python_RNN__Keras_118" rel="nofollow">基本 Python 实现（RNN 与 Keras）</a></li><li><a href="#_153" rel="nofollow">经常问的问题</a></li><li><a href="#_161" rel="nofollow">结论</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<p>苹果的Siri和谷歌的语音搜索都使用递归神经网络（RNN），这是最先进的顺序数据方法。这是第一个具有内部存储器的算法，可以记住其输入，使其非常适合机器学习中涉及顺序数据的问题。它是在过去几年中导致深度学习取得惊人进步的算法之一。在本文中，我们将介绍递归神经网络的基础知识，以及最紧迫的困难以及如何解决它们。</p> 
<h4><a id="_5"></a>循环神经网络简介</h4> 
<p>用于对顺序数据进行建模的深度学习方法是循环神经网络 （RNN）。在注意力模型出现之前，RNN是处理顺序数据的标准建议。深度前馈模型可能需要序列中每个元素的特定参数。它也可能无法泛化为可变长度序列。</p> 
<p><img src="https://images2.imgbox.com/4c/da/olrqIuyd_o.png" alt="在这里插入图片描述"><br> 递归神经网络对序列的每个元素使用相同的权重，从而减少了参数的数量，并允许模型泛化为不同长度的序列。由于其设计，RNN 泛化到序列数据以外的结构化数据，例如地理或图形数据。</p> 
<p>与许多其他深度学习技术一样，循环神经网络相对较旧。它们最初是在 20 世纪 80 年代开发的，但直到最近我们才充分认识到它们的潜力。 20 世纪 90 年代长短期记忆 (LSTM) 的出现，加上计算能力的提高和我们现在必须处理的大量数据，确实将 RNN 推到了最前沿。</p> 
<h4><a id="_RNN_14"></a>什么是循环神经网络 (RNN)？</h4> 
<p>神经网络在人工智能、机器学习和深度学习领域模仿人脑的功能，使计算机程序能够识别模式并解决常见问题。</p> 
<p>RNN 是一种可用于对序列数据建模的神经网络。 RNN 由前馈网络组成，其行为与人脑相似。简而言之，循环神经网络可以以其他算法无法做到的方式预测顺序数据。</p> 
<p><img src="https://images2.imgbox.com/3d/e8/lTT5vFZv_o.png" alt="在这里插入图片描述"><br> 标准神经网络中的所有输入和输出都是相互独立的，但是在某些情况下，例如在预测短语的下一个单词时，前面的单词是必要的，因此必须记住前面的单词。结果，RNN 应运而生，它使用隐藏层来克服这个问题。 RNN 最重要的组成部分是隐藏状态，它记住有关序列的特定信息。</p> 
<p>RNN 有一个内存，用于存储有关计算的所有信息。它对每个输入采用相同的设置，因为它通过在所有输入或隐藏层上执行相同的任务来产生相同的结果。</p> 
<h4><a id="_RNN__25"></a>传统 RNN 的架构</h4> 
<p>RNN 是一种具有隐藏状态并允许将过去的输出用作输入的神经网络。他们通常是这样的：</p> 
<p><img src="https://images2.imgbox.com/8c/a1/AQVpIm4H_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/2e/34/XoZvJPu3_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/96/9b/38CZuHs2_o.png" alt="在这里插入图片描述"><br> RNN 架构可能会根据您要解决的问题而有所不同。从具有单个输入和输出的那些到具有多个输入和输出的那些（之间存在差异）。</p> 
<p>下面是一些 RNN 架构的示例，可以帮助您更好地理解这一点。</p> 
<ul><li>一对一：这里只有一对。传统神经网络使用一对一架构。 一对多：一对多网络中的单个输入可能会产生大量输出。例如，在音乐制作中使用了太多的网络。</li><li>多对一：在这种情况下，单个输出是通过组合来自不同时间步骤的多个输入来生成的。情感分析和情绪识别使用这样的网络，其中类别标签由单词序列确定。</li><li>多对多：对于多对多，有多种选择。两个输入产生三个输出。机器翻译系统，例如英语到法语的翻译系统，反之亦然，使用多对多网络。</li></ul> 
<h4><a id="_39"></a>循环神经网络如何工作？</h4> 
<p>循环神经网络中的信息通过循环循环到中间隐藏层。</p> 
<p><img src="https://images2.imgbox.com/6e/2c/rylihsI2_o.gif" alt="在这里插入图片描述"></p> 
<p>输入层 x 接收并处理神经网络的输入，然后将其传递到中间层。</p> 
<p>中间层 h 可以找到多个隐藏层，每个隐藏层都有自己的激活函数、权重和偏差。如果不同隐藏层的各种参数不受前一层的影响，即神经网络中没有记忆，则可以使用循环神经网络。</p> 
<p>不同的激活函数、权重和偏差将由循环神经网络标准化，确保每个隐藏层具有相同的特征。它不会构建大量隐藏层，而是仅创建一个隐藏层并根据需要循环多次。</p> 
<h4><a id="_51"></a>常用激活函数</h4> 
<p>神经元的激活函数决定了它是应该打开还是关闭。非线性函数通常将神经元的输出转换为 0 到 1 或 -1 到 1 之间的数字。</p> 
<p><img src="https://images2.imgbox.com/04/8c/aOURbWE4_o.png" alt="在这里插入图片描述"></p> 
<p>以下是一些最常用的功能：</p> 
<ul><li>Sigmoid：用公式<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          g 
         
        
          ( 
         
        
          z 
         
        
          ) 
         
        
          = 
         
        
          1 
         
        
          / 
         
        
          ( 
         
        
          1 
         
        
          + 
         
         
         
           e 
          
         
           − 
          
         
        
          z 
         
        
          ) 
         
        
       
         g(z) = 1/(1 + e^-z) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">g</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.044em;">z</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord">1/</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1.0213em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7713em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">−</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right: 0.044em;">z</span><span class="mclose">)</span></span></span></span></span>来表达。</li><li>Tanh：用公式 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          g 
         
        
          ( 
         
        
          z 
         
        
          ) 
         
        
          = 
         
        
          ( 
         
         
         
           e 
          
         
           − 
          
         
        
          z 
         
        
          – 
         
         
         
           e 
          
         
           − 
          
         
        
          z 
         
        
          ) 
         
        
          / 
         
        
          ( 
         
         
         
           e 
          
         
           − 
          
         
        
          z 
         
        
          + 
         
         
         
           e 
          
         
           − 
          
         
        
          z 
         
        
          ) 
         
        
       
         g(z) = (e^-z – e^-z)/(e^-z + e^-z) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">g</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.044em;">z</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.0213em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7713em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">−</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right: 0.044em;">z</span><span class="mord">–</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7713em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">−</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right: 0.044em;">z</span><span class="mclose">)</span><span class="mord">/</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7713em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">−</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right: 0.044em;">z</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1.0213em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7713em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">−</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right: 0.044em;">z</span><span class="mclose">)</span></span></span></span></span> 来表达。</li><li>Relu：用公式 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          g 
         
        
          ( 
         
        
          z 
         
        
          ) 
         
        
          = 
         
        
          m 
         
        
          a 
         
        
          x 
         
        
          ( 
         
        
          0 
         
        
          , 
         
        
          z 
         
        
          ) 
         
        
       
         g(z) = max(0 , z) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">g</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.044em;">z</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.044em;">z</span><span class="mclose">)</span></span></span></span></span> 来表达。</li></ul> 
<h4><a id="RNN_62"></a>RNN的优点和缺点</h4> 
<h5><a id="RNN__63"></a>RNN 的优点：</h5> 
<p>有效处理顺序数据，包括文本、语音和时间序列。<br> 与前馈神经网络不同，处理任意长度的输入。<br> 跨时间步共享权重，提高训练效率。</p> 
<h5><a id="RNN__70"></a>RNN 的缺点：</h5> 
<p>容易出现梯度消失和爆炸问题，阻碍学习。<br> 训练可能具有挑战性，尤其是对于长序列。<br> 计算速度比其他神经网络架构慢。</p> 
<h4><a id="_76"></a>循环神经网络与前馈神经网络</h4> 
<p>前馈神经网络只有一条信息流路线：从输入层到输出层，经过隐藏层。数据以直线路径在网络中流动，不会两次经过同一节点。</p> 
<p>RNN 和前馈神经网络之间的信息流如下两图所示。</p> 
<p><img src="https://images2.imgbox.com/02/29/4qXvGuX6_o.png" alt="在这里插入图片描述"><br> 前馈神经网络对接下来发生的事情的预测很差，因为它们没有接收到的信息的记忆。因为前馈网络只是分析当前输入，所以它不知道时间顺序。除了受过的训练之外，它对过去发生的事情没有任何记忆。</p> 
<p>该信息通过循环处于 RNN 循环中。在做出判断之前，它会评估当前的输入以及从过去的输入中学到的知识。另一方面，循环神经网络可能会由于内部记忆而回忆起来。它产生输出，复制它，然后将其返回到网络。</p> 
<h4><a id="_BPTT_87"></a>随时间反向传播 (BPTT)</h4> 
<p>当我们将反向传播算法应用于以时间序列数据作为输入的循环神经网络时，我们将其称为时间反向传播。</p> 
<p>在普通 RNN 中，每次将单个输入发送到网络中，并获得单个输出。另一方面，反向传播使用当前输入和先前输入作为输入。这称为时间步长，一个时间步长将由同时进入 RNN 的多个时间序列数据点组成。</p> 
<p><img src="https://images2.imgbox.com/15/a6/GFGCsEef_o.png" alt="在这里插入图片描述"><br> 神经网络的输出用于计算和收集错误，一旦它在时间集上进行训练并给出输出。然后，网络将回滚，并重新计算和调整权重以解决故障。</p> 
<h4><a id="_RNN__96"></a>标准 RNN 的两个问题</h4> 
<p>RNN 必须克服两个关键挑战，但为了理解它们，必须首先了解什么是梯度。</p> 
<p><img src="https://images2.imgbox.com/6a/78/37LOJ7mf_o.png" alt="在这里插入图片描述"><br> 就其输入而言，梯度是偏导数。如果您不确定这意味着什么，请考虑这一点：梯度量化了当输入稍微改变时函数的输出变化的程度。</p> 
<p>函数的斜率也称为梯度。斜率越陡，模型学习的速度越快，梯度就越高。另一方面，如果斜率为零，模型将停止学习。梯度用于测量所有权重相对于误差变化的变化。</p> 
<ul><li><strong>梯度爆炸：</strong> 当算法无缘无故地给权重赋予荒谬的高优先级时，就会发生梯度爆炸。幸运的是，截断或压缩梯度是解决此问题的简单方法。</li><li><strong>梯度消失：</strong> 当梯度值太小时，就会发生梯度消失，导致模型停止学习或花费太长时间。这是 20 世纪 90 年代的一个大问题，而且它比梯度爆炸更难解决。幸运的是，Sepp Hochreiter 和 Juergen Schmidhuber 的 LSTM 概念解决了这个问题。</li></ul> 
<h4><a id="RNN__107"></a>RNN 应用</h4> 
<p>循环神经网络用于解决涉及序列数据的各种问题。序列数据有许多不同类型，但以下是最常见的：音频、文本、视频、生物序列。</p> 
<p>使用 RNN 模型和序列数据集，您可以解决各种问题，包括：</p> 
<ul><li>Speech recognition 语音识别</li><li>Generation of music 音乐的产生</li><li>Automated Translations 自动翻译</li><li>Analysis of video action 视频动作分析</li><li>Sequence study of the genome and DNA 基因组和DNA的序列研究</li></ul> 
<h4><a id="_Python_RNN__Keras_118"></a>基本 Python 实现（RNN 与 Keras）</h4> 
<p><strong>Import the required libraries 导入所需的库</strong></p> 
<pre><code class="prism language-python3"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token keyword">from</span> tensorflow <span class="token keyword">import</span> keras
<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras <span class="token keyword">import</span> layers
</code></pre> 
<p>这是一个简单的序列模型，它处理整数序列，将每个整数嵌入到 64 维向量中，然后使用 LSTM 层来处理向量序列。</p> 
<pre><code class="prism language-python3">model <span class="token operator">=</span> keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>layers<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>input_dim<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span> output_dim<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>layers<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>summary<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>Output: 输出：</p> 
<blockquote> 
 <p>Model: “sequential”<br> Layer (type) Output Shape Param #<br> =================================================================<br> embedding (Embedding) (None, None, 64) 64000<br> =_________________________________________________________________<br> lstm (LSTM) (None, 128) 98816<br> =_________________________________________________________________<br> dense (Dense) (None, 10) 1290<br> =================================================================<br> Total params: 164,106<br> Trainable params: 164,106<br> Non-trainable params: 0</p> 
</blockquote> 
<h4><a id="_153"></a>经常问的问题</h4> 
<p>Q1. 什么是循环神经网络?<br> 答：循环神经网络 (RNN) 是一种人工神经网络，旨在处理顺序数据，例如时间序列或自然语言。它们具有反馈连接，使它们能够保留先前时间步骤的信息，从而能够捕获时间依赖性。这使得 RNN 非常适合语言建模、语音识别和顺序数据分析等任务。</p> 
<p>Q2. 递归神经网络是如何工作的?<br> 答：循环神经网络 (RNN) 通过逐步处理顺序数据来工作。它维护一个充当存储器的隐藏状态，该状态在每个时间步使用输入数据和先前的隐藏状态进行更新。隐藏状态允许网络从过去的输入中捕获信息，使其适合顺序任务。 RNN 在所有时间步上使用相同的权重集，从而允许它们在整个序列中共享信息。然而，传统的 RNN 存在梯度消失和爆炸问题，这可能会阻碍它们捕获长期依赖关系的能力。</p> 
<h4><a id="_161"></a>结论</h4> 
<p>递归神经网络是一种多功能工具，可用于多种情况。它们被用于语言建模和文本生成器的各种方法中。它们还用于语音识别。</p> 
<p>这种类型的神经网络用于为与卷积神经网络配对时未标记的图像创建标签。这种组合的效果令人难以置信。</p> 
<p>然而，循环神经网络有一个缺陷。他们在学习远程依赖关系方面遇到困难，这意味着他们无法理解由多个步骤分隔的数据之间的关系。</p> 
<p>例如，当预测单词时，我们可能需要更多的上下文，而不仅仅是一个先前的单词。这称为梯度消失问题，可以使用一种称为长短期记忆网络 (LSTM) 的特殊类型的循环神经网络来解决，这是一个更大的主题，将在以后的文章中讨论。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/3e30e6ed0c54e788758a5af3e78fb40b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">认识数据挖掘</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7349d702bb778d74fff4e7992cbc9cc1/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【MyBatis】操作数据库——入门</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>