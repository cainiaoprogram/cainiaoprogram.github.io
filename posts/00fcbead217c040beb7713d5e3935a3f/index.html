<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【python】tensorrt8版本下的onnx转tensorrt engine - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【python】tensorrt8版本下的onnx转tensorrt engine" />
<meta property="og:description" content="背景 最近解决了python版本为啥执行不了trtexec.exe生成的trt文件的bug，是环境里pytorch自带的cuda和trt的冲突，卸了重装了CPU版本就OK了。但是在我尝试解决的过程中出现了问题，环境有点玩坏了，之后用trtexec.exe生成的engine直接摆烂，输出的全是NaN。行吧，那我在python环境里生成吧。但是网上搜索的onnx2tensorrt代码基本上是7代之前的，我之前试了一些跑不出东西来。今天参考官方代码，记录以下tensorrt8在python环境下如何把onnx转为engine。
参考 github代码
简单流程 实际上用不到官方代码的那么多，只需要选一部分就行了：
import tensorrt as trt import os EXPLICIT_BATCH = 1 &lt;&lt; (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH) TRT_LOGGER = trt.Logger() def get_engine(onnx_file_path, engine_file_path=&#34;&#34;): &#34;&#34;&#34;Attempts to load a serialized engine if available, otherwise builds a new TensorRT engine and saves it.&#34;&#34;&#34; def build_engine(): &#34;&#34;&#34;Takes an ONNX file and creates a TensorRT engine to run inference with&#34;&#34;&#34; with trt.Builder(TRT_LOGGER) as builder, builder.create_network( EXPLICIT_BATCH ) as network, builder.create_builder_config() as config, trt.OnnxParser( network, TRT_LOGGER ) as parser, trt." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/00fcbead217c040beb7713d5e3935a3f/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-06-16T17:13:02+08:00" />
<meta property="article:modified_time" content="2022-06-16T17:13:02+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【python】tensorrt8版本下的onnx转tensorrt engine</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h5><a id="_0"></a>背景</h5> 
<p>最近解决了python版本为啥执行不了trtexec.exe生成的trt文件的bug，是环境里pytorch自带的cuda和trt的冲突，卸了重装了CPU版本就OK了。但是在我尝试解决的过程中出现了问题，环境有点玩坏了，之后用trtexec.exe生成的engine直接摆烂，输出的全是NaN。行吧，那我在python环境里生成吧。但是网上搜索的onnx2tensorrt代码基本上是7代之前的，我之前试了一些跑不出东西来。今天参考官方代码，记录以下tensorrt8在python环境下如何把onnx转为engine。</p> 
<h5><a id="_3"></a>参考</h5> 
<p><a href="https://github.com/NVIDIA/TensorRT/blob/master/samples/python/yolov3_onnx/onnx_to_tensorrt.py">github代码</a></p> 
<h5><a id="_6"></a>简单流程</h5> 
<p>实际上用不到官方代码的那么多，只需要选一部分就行了：</p> 
<pre><code class="prism language-bash"><span class="token function">import</span> tensorrt as trt
<span class="token function">import</span> os

EXPLICIT_BATCH <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">&lt;&lt;</span> <span class="token punctuation">(</span>int<span class="token punctuation">)</span><span class="token punctuation">(</span>trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH<span class="token punctuation">)</span>
TRT_LOGGER <span class="token operator">=</span> trt.Logger<span class="token punctuation">(</span><span class="token punctuation">)</span>


def get_engine<span class="token punctuation">(</span>onnx_file_path, <span class="token assign-left variable">engine_file_path</span><span class="token operator">=</span><span class="token string">""</span><span class="token punctuation">)</span>:
    <span class="token string">""</span>"Attempts to load a serialized engine <span class="token keyword">if</span> available, otherwise builds a new TensorRT engine and saves it.<span class="token string">""</span>"

    def build_engine<span class="token punctuation">(</span><span class="token punctuation">)</span>:
        <span class="token string">""</span>"Takes an ONNX <span class="token function">file</span> and creates a TensorRT engine to run inference with<span class="token string">""</span>"
        with trt.Builder<span class="token punctuation">(</span>TRT_LOGGER<span class="token punctuation">)</span> as builder, builder.create_network<span class="token punctuation">(</span>
            EXPLICIT_BATCH
        <span class="token punctuation">)</span> as network, builder.create_builder_config<span class="token punctuation">(</span><span class="token punctuation">)</span> as config, trt.OnnxParser<span class="token punctuation">(</span>
            network, TRT_LOGGER
        <span class="token punctuation">)</span> as parser, trt.Runtime<span class="token punctuation">(</span>
            TRT_LOGGER
        <span class="token punctuation">)</span> as runtime:
            config.max_workspace_size <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">&lt;&lt;</span> <span class="token number">32</span>  <span class="token comment"># 4GB</span>
            builder.max_batch_size <span class="token operator">=</span> <span class="token number">1</span>
            <span class="token comment"># Parse model file</span>
            <span class="token keyword">if</span> not os.path.exists<span class="token punctuation">(</span>onnx_file_path<span class="token punctuation">)</span>:
                print<span class="token punctuation">(</span>
                    <span class="token string">"ONNX file {} not found, please run yolov3_to_onnx.py first to generate it."</span>.format<span class="token punctuation">(</span>onnx_file_path<span class="token punctuation">)</span>
                <span class="token punctuation">)</span>
                exit<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
            print<span class="token punctuation">(</span><span class="token string">"Loading ONNX file from path {}..."</span>.format<span class="token punctuation">(</span>onnx_file_path<span class="token punctuation">))</span>
            with open<span class="token punctuation">(</span>onnx_file_path, <span class="token string">"rb"</span><span class="token punctuation">)</span> as model:
                print<span class="token punctuation">(</span><span class="token string">"Beginning ONNX file parsing"</span><span class="token punctuation">)</span>
                <span class="token keyword">if</span> not parser.parse<span class="token punctuation">(</span>model.read<span class="token punctuation">(</span><span class="token punctuation">))</span>:
                    print<span class="token punctuation">(</span><span class="token string">"ERROR: Failed to parse the ONNX file."</span><span class="token punctuation">)</span>
                    <span class="token keyword">for</span> <span class="token for-or-select variable">error</span> <span class="token keyword">in</span> range<span class="token punctuation">(</span>parser.num_errors<span class="token punctuation">)</span>:
                        print<span class="token punctuation">(</span>parser.get_error<span class="token punctuation">(</span>error<span class="token punctuation">))</span>
                    <span class="token builtin class-name">return</span> None

            <span class="token comment"># # The actual yolov3.onnx is generated with batch size 64. Reshape input to batch size 1</span>
            <span class="token comment"># network.get_input(0).shape = [1, 3, 608, 608]</span>

            print<span class="token punctuation">(</span><span class="token string">"Completed parsing of ONNX file"</span><span class="token punctuation">)</span>
            print<span class="token punctuation">(</span><span class="token string">"Building an engine from file {}; this may take a while..."</span>.format<span class="token punctuation">(</span>onnx_file_path<span class="token punctuation">))</span>
            plan <span class="token operator">=</span> builder.build_serialized_network<span class="token punctuation">(</span>network, config<span class="token punctuation">)</span>
            engine <span class="token operator">=</span> runtime.deserialize_cuda_engine<span class="token punctuation">(</span>plan<span class="token punctuation">)</span>
            print<span class="token punctuation">(</span><span class="token string">"Completed creating Engine"</span><span class="token punctuation">)</span>
            with open<span class="token punctuation">(</span>engine_file_path, <span class="token string">"wb"</span><span class="token punctuation">)</span> as f:
                f.write<span class="token punctuation">(</span>plan<span class="token punctuation">)</span>
            <span class="token builtin class-name">return</span> engine

    <span class="token keyword">if</span> os.path.exists<span class="token punctuation">(</span>engine_file_path<span class="token punctuation">)</span>:
        <span class="token comment"># If a serialized engine exists, use it instead of building an engine.</span>
        print<span class="token punctuation">(</span><span class="token string">"Reading engine from file {}"</span>.format<span class="token punctuation">(</span>engine_file_path<span class="token punctuation">))</span>
        with open<span class="token punctuation">(</span>engine_file_path, <span class="token string">"rb"</span><span class="token punctuation">)</span> as f, trt.Runtime<span class="token punctuation">(</span>TRT_LOGGER<span class="token punctuation">)</span> as runtime:
            <span class="token builtin class-name">return</span> runtime.deserialize_cuda_engine<span class="token punctuation">(</span>f.read<span class="token punctuation">(</span><span class="token punctuation">))</span>
    else:
        <span class="token builtin class-name">return</span> build_engine<span class="token punctuation">(</span><span class="token punctuation">)</span>


def main<span class="token punctuation">(</span><span class="token punctuation">)</span>:
    <span class="token string">""</span>"Create a TensorRT engine <span class="token keyword">for</span> ONNX-based YOLOv3-608 and run inference.<span class="token string">""</span>"

    <span class="token comment"># Try to load a previously generated YOLOv3-608 network graph in ONNX format:</span>
    onnx_file_path <span class="token operator">=</span> <span class="token string">"model.onnx"</span>
    engine_file_path <span class="token operator">=</span> <span class="token string">"model.trt"</span>

    get_engine<span class="token punctuation">(</span>onnx_file_path, engine_file_path<span class="token punctuation">)</span>


<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token builtin class-name">:</span>
    main<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>摘抄下来之后我根据自己的模型删了一些东西，参考的这个模型的onnx好像batch是10，它自己就加了转为1的部分，我手头的模型文件已经是了，就删掉了。动态输入大小我没需要搞，就没看咋加。用pycharm试了下失败了，好像是显存分配有点问题，不知道设置的1&lt;&lt;32的大小是不是有点莽。但是我改到pycharm的终端敲代码执行成功了，就很抽象。</p> 
<pre><code class="prism language-bash"><span class="token punctuation">(</span>mypytorch<span class="token punctuation">)</span> PS F:<span class="token punctuation">\</span>DeepStereo<span class="token punctuation">\</span>AppleShow<span class="token operator"><span class="token file-descriptor important">2</span>&gt;</span> python onnx2trt.py      
onnx2trt.py:20: DeprecationWarning: Use set_memory_pool_limit instead.
  config.max_workspace_size <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">&lt;&lt;</span> <span class="token number">32</span>  <span class="token comment"># 4GB</span>
Loading ONNX <span class="token function">file</span> from path G:<span class="token punctuation">\</span>jupyter<span class="token punctuation">\</span>Model_Zoo<span class="token punctuation">\</span>resources_iter10_modify<span class="token punctuation">\</span>crestereo_combined_iter10_240x320.onnx<span class="token punctuation">..</span>.
Beginning ONNX <span class="token function">file</span> parsing
<span class="token punctuation">[</span>06/16/2022-16:59:16<span class="token punctuation">]</span> <span class="token punctuation">[</span>TRT<span class="token punctuation">]</span> <span class="token punctuation">[</span>W<span class="token punctuation">]</span> onnx2trt_utils.cpp:365: Your ONNX model has been generated with INT64 weights, <span class="token keyword">while</span> TensorRT does not natively support INT64. Attempting to cast down t
o INT32.
Completed parsing of ONNX <span class="token function">file</span>
Building an engine from <span class="token function">file</span> G:<span class="token punctuation">\</span>jupyter<span class="token punctuation">\</span>Model_Zoo<span class="token punctuation">\</span>resources_iter10_modify<span class="token punctuation">\</span>crestereo_combined_iter10_240x320.onnx<span class="token punctuation">;</span> this may take a while<span class="token punctuation">..</span>.
<span class="token punctuation">[</span>06/16/2022-17:03:52<span class="token punctuation">]</span> <span class="token punctuation">[</span>TRT<span class="token punctuation">]</span> <span class="token punctuation">[</span>W<span class="token punctuation">]</span> TensorRT was linked against cuBLAS/cuBLAS LT <span class="token number">11.8</span>.0 but loaded cuBLAS/cuBLAS LT <span class="token number">11.3</span>.1
<span class="token punctuation">[</span>06/16/2022-17:07:42<span class="token punctuation">]</span> <span class="token punctuation">[</span>TRT<span class="token punctuation">]</span> <span class="token punctuation">[</span>W<span class="token punctuation">]</span> TensorRT was linked against cuBLAS/cuBLAS LT <span class="token number">11.8</span>.0 but loaded cuBLAS/cuBLAS LT <span class="token number">11.3</span>.1
<span class="token punctuation">[</span>06/16/2022-17:07:43<span class="token punctuation">]</span> <span class="token punctuation">[</span>TRT<span class="token punctuation">]</span> <span class="token punctuation">[</span>W<span class="token punctuation">]</span> TensorRT was linked against cuBLAS/cuBLAS LT <span class="token number">11.8</span>.0 but loaded cuBLAS/cuBLAS LT <span class="token number">11.3</span>.1
Completed creating Engine
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6cd271b1d7cfb62a8db33da2a636e0f5/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">java桌面端实时分析系统</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/118fc96ab0a472ffd62ab17668d2b922/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">小猫爪：S32K3学习笔记01-S32K3 RTD【MCAL&amp;SDK】的使用和环境搭建</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>