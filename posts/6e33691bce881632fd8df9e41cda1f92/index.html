<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>å¦‚ä½•è®­ç»ƒä¸€ä¸ªç®€å•çš„stable diffusionæ¨¡å‹(é™„è¯¦ç»†æ³¨é‡Šï¼‰ - èœé¸Ÿç¨‹åºå‘˜åšå®¢</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="å¦‚ä½•è®­ç»ƒä¸€ä¸ªç®€å•çš„stable diffusionæ¨¡å‹(é™„è¯¦ç»†æ³¨é‡Šï¼‰" />
<meta property="og:description" content="æ³¨ï¼šä»£ç æ¥è‡ªhttps://github.com/darcula1993/diffusion-models-class-CN/blob/main/unit1/01_introduction_to_diffusers_CN.ipynbÂ æœ¬æ–‡æ˜¯æœ¬äººå­¦ä¹ åçš„çš„å°è¯•ä»¥åŠæ³¨è§£
ä¸€ã€å‡†å¤‡å·¥ä½œ &#34;&#34;&#34; è¿™è¡Œå‘½ä»¤ä½¿ç”¨pipå·¥å…·æ¥å®‰è£…æˆ–å‡çº§å¤šä¸ªPythonåŒ…ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒæ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š -qqï¼šè¿™æ˜¯pipçš„å®‰é™æ¨¡å¼é€‰é¡¹ï¼Œå®ƒä¼šå‡å°‘è¾“å‡ºä¿¡æ¯ï¼Œåªæ˜¾ç¤ºå…³é”®ä¿¡æ¯ï¼Œä½¿å®‰è£…è¿‡ç¨‹æ›´ä¸ºç®€æ´ã€‚ -Uï¼šè¿™æ˜¯pipçš„å‡çº§é€‰é¡¹ï¼Œå®ƒæŒ‡ç¤ºpipå‡çº§å·²ç»å®‰è£…çš„åŒ…åˆ°æœ€æ–°ç‰ˆæœ¬ï¼ˆå¦‚æœå­˜åœ¨æ–°ç‰ˆæœ¬ï¼‰ã€‚ æ¥ä¸‹æ¥ï¼Œåˆ—å‡ºäº†è¦å®‰è£…æˆ–å‡çº§çš„åŒ…ï¼š diffusersï¼šä¸€ä¸ªPythonåŒ… datasetsï¼šHugging Face Transformersåº“çš„ä¸€éƒ¨åˆ†ï¼Œç”¨äºæä¾›å’Œç®¡ç†å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ•°æ®é›†çš„å·¥å…·ã€‚ transformersï¼šHugging Face Transformersåº“ï¼Œæä¾›äº†é¢„è®­ç»ƒçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†å’Œæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚ accelerateï¼šHugging Faceåº“çš„ä¸€éƒ¨åˆ†ï¼Œç”¨äºåŠ é€Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†ã€‚ ftfyï¼šä¸€ä¸ªç”¨äºå¤„ç†Unicodeæ–‡æœ¬çš„Pythonåº“ï¼Œç”¨äºä¿®å¤å’Œæ¸…ç†ä¸è§„èŒƒçš„Unicodeæ–‡æœ¬ã€‚ pyarrowï¼šæ­£å¦‚å‰é¢æ‰€æåˆ°çš„ï¼Œpyarrowæ˜¯ä¸€ä¸ªç”¨äºé«˜æ•ˆå¤„ç†å¤§è§„æ¨¡æ•°æ®é›†çš„Pythonåº“ï¼Œæ”¯æŒåˆ—å¼å­˜å‚¨å’Œè·¨è¯­è¨€äº’æ“ä½œæ€§ã€‚ &#34;&#34;&#34; %pip install -qq -U diffusers datasets transformers accelerate ftfy pyarrow # ç™»å½•hugging face from huggingface_hub import notebook_login notebook_login() æ˜¾ç¤ºä¸‹å›¾åˆ™ç™»é™†æˆåŠŸï¼š
# å®‰è£… Git LFS æ¥ä¸Šä¼ æ¨¡å‹æ£€æŸ¥ç‚¹ï¼š %%capture !sudo apt -qq install git-lfs !git config --global credential.helper store # å¯¼å…¥å°†è¦ä½¿ç”¨çš„åº“ï¼Œå¹¶å®šä¹‰ä¸€äº›æ–¹ä¾¿å‡½æ•°ï¼Œç¨åå°†ä¼šä½¿ç”¨è¿™äº›å‡½æ•° import numpy as np import torch import torch.nn.functional as F from matplotlib import pyplot as plt from PIL import Image def show_images(x): &#34;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/6e33691bce881632fd8df9e41cda1f92/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-20T20:37:58+08:00" />
<meta property="article:modified_time" content="2023-10-20T20:37:58+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="èœé¸Ÿç¨‹åºå‘˜åšå®¢" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">èœé¸Ÿç¨‹åºå‘˜åšå®¢</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">å¦‚ä½•è®­ç»ƒä¸€ä¸ªç®€å•çš„stable diffusionæ¨¡å‹(é™„è¯¦ç»†æ³¨é‡Šï¼‰</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>æ³¨ï¼šä»£ç æ¥è‡ª<a href="https://github.com/darcula1993/diffusion-models-class-CN/blob/main/unit1/01_introduction_to_diffusers_CN.ipynb" title="https://github.com/darcula1993/diffusion-models-class-CN/blob/main/unit1/01_introduction_to_diffusers_CN.ipynb">https://github.com/darcula1993/diffusion-models-class-CN/blob/main/unit1/01_introduction_to_diffusers_CN.ipynb</a>Â </p> 
<p>æœ¬æ–‡æ˜¯æœ¬äººå­¦ä¹ åçš„çš„å°è¯•ä»¥åŠæ³¨è§£</p> 
<h2>ä¸€ã€å‡†å¤‡å·¥ä½œ</h2> 
<pre><code class="language-python">"""

è¿™è¡Œå‘½ä»¤ä½¿ç”¨pipå·¥å…·æ¥å®‰è£…æˆ–å‡çº§å¤šä¸ªPythonåŒ…ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒæ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

-qqï¼šè¿™æ˜¯pipçš„å®‰é™æ¨¡å¼é€‰é¡¹ï¼Œå®ƒä¼šå‡å°‘è¾“å‡ºä¿¡æ¯ï¼Œåªæ˜¾ç¤ºå…³é”®ä¿¡æ¯ï¼Œä½¿å®‰è£…è¿‡ç¨‹æ›´ä¸ºç®€æ´ã€‚

-Uï¼šè¿™æ˜¯pipçš„å‡çº§é€‰é¡¹ï¼Œå®ƒæŒ‡ç¤ºpipå‡çº§å·²ç»å®‰è£…çš„åŒ…åˆ°æœ€æ–°ç‰ˆæœ¬ï¼ˆå¦‚æœå­˜åœ¨æ–°ç‰ˆæœ¬ï¼‰ã€‚

æ¥ä¸‹æ¥ï¼Œåˆ—å‡ºäº†è¦å®‰è£…æˆ–å‡çº§çš„åŒ…ï¼š

diffusersï¼šä¸€ä¸ªPythonåŒ…

datasetsï¼šHugging Face Transformersåº“çš„ä¸€éƒ¨åˆ†ï¼Œç”¨äºæä¾›å’Œç®¡ç†å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ•°æ®é›†çš„å·¥å…·ã€‚

transformersï¼šHugging Face Transformersåº“ï¼Œæä¾›äº†é¢„è®­ç»ƒçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†å’Œæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚

accelerateï¼šHugging Faceåº“çš„ä¸€éƒ¨åˆ†ï¼Œç”¨äºåŠ é€Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†ã€‚

ftfyï¼šä¸€ä¸ªç”¨äºå¤„ç†Unicodeæ–‡æœ¬çš„Pythonåº“ï¼Œç”¨äºä¿®å¤å’Œæ¸…ç†ä¸è§„èŒƒçš„Unicodeæ–‡æœ¬ã€‚

pyarrowï¼šæ­£å¦‚å‰é¢æ‰€æåˆ°çš„ï¼Œpyarrowæ˜¯ä¸€ä¸ªç”¨äºé«˜æ•ˆå¤„ç†å¤§è§„æ¨¡æ•°æ®é›†çš„Pythonåº“ï¼Œæ”¯æŒåˆ—å¼å­˜å‚¨å’Œè·¨è¯­è¨€äº’æ“ä½œæ€§ã€‚
"""

%pip install -qq -U diffusers datasets transformers accelerate ftfy pyarrow</code></pre> 
<pre><code class="language-python"># ç™»å½•hugging face
from huggingface_hub import notebook_login

notebook_login()</code></pre> 
<p>æ˜¾ç¤ºä¸‹å›¾åˆ™ç™»é™†æˆåŠŸï¼š</p> 
<p><img alt="" height="177" src="https://images2.imgbox.com/d9/80/MBmwRk44_o.png" width="730"></p> 
<p></p> 
<pre><code class="language-python"># å®‰è£… Git LFS æ¥ä¸Šä¼ æ¨¡å‹æ£€æŸ¥ç‚¹ï¼š

%%capture
!sudo apt -qq install git-lfs
!git config --global credential.helper store</code></pre> 
<pre><code class="language-python"># å¯¼å…¥å°†è¦ä½¿ç”¨çš„åº“ï¼Œå¹¶å®šä¹‰ä¸€äº›æ–¹ä¾¿å‡½æ•°ï¼Œç¨åå°†ä¼šä½¿ç”¨è¿™äº›å‡½æ•°
import numpy as np
import torch
import torch.nn.functional as F
from matplotlib import pyplot as plt
from PIL import Image


def show_images(x):
  """Given a batch of images x, make a grid and convert to PIL"""
  '''
  è¾“å…¥å‚æ•°ï¼šxï¼Œä¸€ä¸ªæ‰¹é‡çš„å›¾åƒæ•°æ®ï¼ˆé€šå¸¸æ˜¯PyTorchå¼ é‡ï¼‰ã€‚
  åŠŸèƒ½ï¼šå°†è¾“å…¥çš„å›¾åƒæ•°æ®ä»èŒƒå›´(-1, 1)æ˜ å°„åˆ°(0, 1)ï¼Œç„¶åå°†è¿™äº›å›¾åƒæ’åˆ—æˆä¸€ä¸ªç½‘æ ¼ï¼Œå¹¶å°†ç½‘æ ¼è½¬æ¢ä¸ºPILå›¾åƒã€‚
  è¿”å›å€¼ï¼šè¿”å›ä¸€ä¸ªPILå›¾åƒï¼Œå…¶ä¸­åŒ…å«äº†æ’åˆ—å¥½çš„è¾“å…¥å›¾åƒç½‘æ ¼ã€‚
  '''
  x = x * 0.5 + 0.5  # Map from (-1, 1) back to (0, 1)
  grid = torchvision.utils.make_grid(x)
  grid_im = grid.detach().cpu().permute(1, 2, 0).clip(0, 1) * 255
  grid_im = Image.fromarray(np.array(grid_im).astype(np.uint8))
  return grid_im


def make_grid(images, size=64):
  """Given a list of PIL images, stack them together into a line for easy viewing"""
  '''
  è¾“å…¥å‚æ•°ï¼šimagesï¼Œä¸€ä¸ªåŒ…å«å¤šä¸ªPILå›¾åƒçš„åˆ—è¡¨ï¼Œä»¥åŠä¸€ä¸ªå¯é€‰çš„sizeå‚æ•°ï¼Œç”¨äºæŒ‡å®šå›¾åƒçš„å¤§å°ã€‚
  åŠŸèƒ½ï¼šå°†å¤šä¸ªPILå›¾åƒæŒ‰ç…§æŒ‡å®šçš„å¤§å°å †å åœ¨ä¸€è¡Œä¸Šï¼Œä»¥ä¾¿äºæŸ¥çœ‹ã€‚
  è¿”å›å€¼ï¼šè¿”å›ä¸€ä¸ªæ–°çš„PILå›¾åƒï¼Œå…¶ä¸­åŒ…å«äº†å †å åœ¨ä¸€è¡Œä¸Šçš„è¾“å…¥å›¾åƒã€‚
  '''
  output_im = Image.new("RGB", (size * len(images), size))
  for i, im in enumerate(images):
      output_im.paste(im.resize((size, size)), (i * size, 0))
  return output_im


# Mac users may need device = 'mps' (untested)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")</code></pre> 
<h2>äºŒã€ä¸‹è½½è®­ç»ƒæ•°æ®é›†</h2> 
<pre><code class="language-python"># ä¸‹è½½ä¸€ä¸ªæ¥è‡ª Hugging Face Hub çš„å›¾åƒé›†ã€‚å…·ä½“æ¥è¯´ï¼Œæ˜¯ä¸ª 1000 å¼ è´è¶å›¾åƒæ”¶è—é›†
import torchvision  # å¯¼å…¥PyTorchçš„torchvisionåº“
from datasets import load_dataset  # å¯¼å…¥æ•°æ®é›†
from torchvision import transforms  # å¯¼å…¥PyTorchçš„transformsæ¨¡å—

# ä½¿ç”¨load_datasetå‡½æ•°åŠ è½½åä¸º"huggan/smithsonian_butterflies_subset"çš„æ•°æ®é›†ä¸­çš„è®­ç»ƒé›†æ•°æ®
dataset = load_dataset("huggan/smithsonian_butterflies_subset", split="train")

# æˆ–è€…ä»æœ¬åœ°æ–‡ä»¶å¤¹åŠ è½½å›¾åƒæ•°æ®
# dataset = load_dataset("imagefolder", data_dir="path/to/folder")

# æŒ‡å®šå›¾åƒå¤§å°ä¸º32x32åƒç´ 
image_size = 32
# å¦‚æœGPUå†…å­˜ä¸è¶³ï¼Œå¯ä»¥é™ä½æ‰¹æ¬¡å¤§å°
batch_size = 64

# å®šä¹‰æ•°æ®å¢å¼ºæ“ä½œ
preprocess = transforms.Compose(
    [
        transforms.Resize((image_size, image_size)),  # è°ƒæ•´å›¾åƒå¤§å°
        transforms.RandomHorizontalFlip(),  # éšæœºæ°´å¹³ç¿»è½¬ï¼ˆæ•°æ®å¢å¼ºï¼‰
        transforms.ToTensor(),  # å°†å›¾åƒè½¬æ¢ä¸ºå¼ é‡ï¼ˆæ•°å€¼èŒƒå›´ä»0åˆ°1ï¼‰
        transforms.Normalize([0.5], [0.5]),  # å°†å›¾åƒåƒç´ å€¼å½’ä¸€åŒ–åˆ°(-1, 1)çš„èŒƒå›´
    ]
)

# å®šä¹‰ä¸€ä¸ªç”¨äºå¯¹æ•°æ®è¿›è¡Œè½¬æ¢çš„å‡½æ•°
def transform(examples):
    images = [preprocess(image.convert("RGB")) for image in examples["image"]]
    return {"images": images}

# å°†æ•°æ®é›†çš„è½¬æ¢å‡½æ•°è®¾ç½®ä¸ºä¸Šè¿°å®šä¹‰çš„transformå‡½æ•°
dataset.set_transform(transform)

# åˆ›å»ºä¸€ä¸ªæ•°æ®åŠ è½½å™¨ï¼Œç”¨äºä»¥æ‰¹æ¬¡æ–¹å¼æä¾›è½¬æ¢åçš„å›¾åƒæ•°æ®
train_dataloader = torch.utils.data.DataLoader(
    dataset, batch_size=batch_size, shuffle=True  # ä½¿ç”¨æŒ‡å®šçš„æ‰¹æ¬¡å¤§å°å’Œéšæœºæ‰“ä¹±æ•°æ®
)
</code></pre> 
<pre><code class="language-python"># æˆ‘ä»¬å¯ä»¥ä»ä¸­å–å‡ºä¸€æ‰¹å›¾åƒæ•°æ®æ¥çœ‹ä¸€çœ‹ä»–ä»¬æ˜¯ä»€ä¹ˆæ ·å­:

xb = next(iter(train_dataloader))["images"].to(device)[:8]
print("X shape:", xb.shape)
show_images(xb).resize((8 * 64, 64), resample=Image.NEAREST)</code></pre> 
<p>ç»“æœå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p> 
<p><img alt="" height="119" src="https://images2.imgbox.com/f0/4c/ryBokDJK_o.png" width="721"></p> 
<h2>ä¸‰ã€å®šä¹‰ç®¡ç†å™¨</h2> 
<p>æˆ‘ä»¬çš„è®­ç»ƒè®¡åˆ’æ˜¯ï¼Œå–å‡ºè¿™äº›è¾“å…¥å›¾ç‰‡ç„¶åå¯¹å®ƒä»¬å¢æ·»å™ªå£°ï¼Œåœ¨è¿™ä¹‹åæŠŠå¸¦å™ªçš„å›¾ç‰‡é€å…¥æ¨¡å‹ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæˆ‘ä»¬å°†ç”¨æ¨¡å‹çš„é¢„æµ‹å€¼æ¥ä¸æ–­è¿­ä»£å»é™¤è¿™äº›å™ªç‚¹ã€‚åœ¨diffusersä¸­ï¼Œè¿™ä¸¤ä¸ªæ­¥éª¤éƒ½æ˜¯ç”± ç®¡ç†å™¨ï¼ˆè°ƒåº¦å™¨ï¼‰ æ¥å¤„ç†çš„ã€‚å™ªå£°ç®¡ç†å™¨å†³å®šåœ¨ä¸åŒçš„è¿­ä»£å‘¨æœŸæ—¶åˆ†åˆ«åŠ å…¥å¤šå°‘å™ªå£°ã€‚</p> 
<pre><code class="language-python">from diffusers import DDPMScheduler

noise_scheduler = DDPMScheduler(num_train_timesteps=1000)</code></pre> 
<pre><code class="language-python"># ç»˜å›¾æŸ¥çœ‹è¾“å…¥ (x) ä¸å™ªå£°æ˜¯å¦‚ä½•åœ¨ä¸åŒè¿­ä»£å‘¨æœŸä¸­é‡åŒ–å’Œå åŠ çš„
plt.plot(noise_scheduler.alphas_cumprod.cpu() ** 0.5, label=r"${\sqrt{\bar{\alpha}_t}}$")
plt.plot((1 - noise_scheduler.alphas_cumprod.cpu()) ** 0.5, label=r"$\sqrt{(1 - \bar{\alpha}_t)}$")
plt.legend(fontsize="x-large");</code></pre> 
<p><img alt="" height="524" src="https://images2.imgbox.com/f2/96/ec8yiLld_o.png" width="801"></p> 
<pre><code class="language-python"># ä½¿ç”¨noise_scheduler.add_noiseåŠŸèƒ½æ¥æ·»åŠ ä¸åŒç¨‹åº¦çš„å™ªå£°
timesteps = torch.linspace(0, 999, 8).long().to(device)
noise = torch.randn_like(xb)
noisy_xb = noise_scheduler.add_noise(xb, noise, timesteps)
print("Noisy X shape", noisy_xb.shape)
show_images(noisy_xb).resize((8 * 64, 64), resample=Image.NEAREST)</code></pre> 
<p><img alt="" height="116" src="https://images2.imgbox.com/b4/aa/P8tZZxDW_o.png" width="741"></p> 
<h2>å››ã€å®šä¹‰ã€è®­ç»ƒæ¨¡å‹</h2> 
<pre><code class="language-python"># å®šä¹‰æ¨¡å‹
from diffusers import UNet2DModel

# Create a model
model = UNet2DModel(
  sample_size=image_size,  # the target image resolution
  in_channels=3,  # the number of input channels, 3 for RGB images
  out_channels=3,  # the number of output channels
  layers_per_block=2,  # how many ResNet layers to use per UNet block
  block_out_channels=(64, 128, 128, 256),  # More channels -&gt; more parameters
  down_block_types=(
      "DownBlock2D",  # a regular ResNet downsampling block
      "DownBlock2D",
      "AttnDownBlock2D",  # a ResNet downsampling block with spatial self-attention
      "AttnDownBlock2D",
  ),
  up_block_types=(
      "AttnUpBlock2D",
      "AttnUpBlock2D",  # a ResNet upsampling block with spatial self-attention
      "UpBlock2D",
      "UpBlock2D",  # a regular ResNet upsampling block
  ),
)
model.to(device);</code></pre> 
<p>å¼€å§‹è®­ç»ƒæ¨¡å‹Â </p> 
<pre><code class="language-python"># åˆ›å»ºäº†ä¸€ä¸ªå¯¹è±¡ï¼Œç”¨äºè°ƒåº¦å™ªå£°çš„æ·»åŠ ã€‚å‚æ•°æŒ‡å®šäº†è®­ç»ƒçš„æ€»æ­¥æ•°å’Œå™ªå£°çš„å˜åŒ–è§„å¾‹
noise_scheduler = DDPMScheduler(
    num_train_timesteps=1000, beta_schedule="squaredcos_cap_v2"
)

# åˆ›å»ºäº†ä¸€ä¸ªAdamWä¼˜åŒ–å™¨ï¼Œç”¨äºæ›´æ–°æ¨¡å‹çš„å‚æ•°ã€‚è¿”å›æ¨¡å‹çš„å¯è®­ç»ƒå‚æ•°ï¼Œå‚æ•°æŒ‡å®šäº†å­¦ä¹ ç‡ã€‚
optimizer = torch.optim.AdamW(model.parameters(), lr=4e-4)

# åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨æ¯ä¸ªæ­¥éª¤çš„æŸå¤±å€¼ã€‚
losses = []

for epoch in range(30):
  for step, batch in enumerate(train_dataloader):
    # ä»æ‰¹æ¬¡ä¸­è·å–å¹²å‡€çš„å›¾åƒæ•°æ®ï¼Œå¹¶å°†å…¶ç§»åŠ¨åˆ°æŒ‡å®šçš„è®¾å¤‡ï¼ˆä¾‹å¦‚GPUï¼‰ä¸Šè¿›è¡Œè®¡ç®—ã€‚
    clean_images = batch["images"].to(device)
    # ç”Ÿæˆä¸å¹²å‡€å›¾åƒç›¸åŒå½¢çŠ¶çš„å™ªå£°å¼ é‡ï¼Œè¯¥å™ªå£°å°†è¢«æ·»åŠ åˆ°å›¾åƒä¸­ã€‚
    noise = torch.randn(clean_images.shape).to(clean_images.device)
    # è·å–æ‰¹æ¬¡çš„å¤§å°ã€‚
    bs = clean_images.shape[0]

    # ä¸ºæ¯ä¸ªå›¾åƒéšæœºç”Ÿæˆä¸€ä¸ªæ—¶é—´æ­¥é•¿ï¼Œè¯¥æ—¶é—´æ­¥é•¿å°†ç”¨äºç¡®å®šå™ªå£°çš„å˜åŒ–ç¨‹åº¦ã€‚
    timesteps = torch.randint(
      0, noise_scheduler.num_train_timesteps, (bs,), device=clean_images.device
    ).long()

    # æ ¹æ®å™ªå£°è°ƒåº¦å™¨ä¸­çš„æ¯ä¸ªå™ªå£°å¹…åº¦å’Œæ—¶é—´æ­¥é•¿ï¼Œå°†å™ªå£°æ·»åŠ åˆ°å¹²å‡€å›¾åƒä¸­ï¼Œç”Ÿæˆå¸¦æœ‰å™ªå£°çš„å›¾åƒã€‚
    noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)

    # ä½¿ç”¨æ¨¡å‹å¯¹å¸¦æœ‰å™ªå£°çš„å›¾åƒè¿›è¡Œé¢„æµ‹ï¼Œå¾—åˆ°å»å™ªåçš„å›¾åƒã€‚
    noise_pred = model(noisy_images, timesteps, return_dict=False)[0]

    # è®¡ç®—é¢„æµ‹å›¾åƒä¸çœŸå®å™ªå£°ä¹‹é—´çš„å‡æ–¹è¯¯å·®æŸå¤±ï¼Œè®¡ç®—æŸå¤±ç›¸å¯¹äºæ¨¡å‹å‚æ•°çš„æ¢¯åº¦ï¼Œå¹¶å°†å½“å‰æ­¥éª¤çš„æŸå¤±æ·»åŠ åˆ°åˆ—è¡¨ä¸­ã€‚
    loss = F.mse_loss(noise_pred, noise)
    loss.backward(loss)
    losses.append(loss.item())

    # ä½¿ç”¨ä¼˜åŒ–å™¨æ›´æ–°æ¨¡å‹çš„å‚æ•°ï¼Œå¹¶å°†æ¢¯åº¦ç½®é›¶ã€‚
    optimizer.step()
    optimizer.zero_grad()

  # æ¯éš”5ä¸ªepochï¼Œè®¡ç®—æœ€è¿‘ä¸€ä¸ªepochçš„å¹³å‡æŸå¤±ï¼Œå¹¶æ‰“å°å‡ºæ¥ã€‚
  if (epoch + 1) % 5 == 0:
      loss_last_epoch = sum(losses[-len(train_dataloader) :]) / len(train_dataloader)
      print(f"Epoch:{epoch+1}, loss: {loss_last_epoch}")</code></pre> 
<p><img alt="" height="194" src="https://images2.imgbox.com/1a/a1/URrbpplt_o.png" width="877"></p> 
<pre><code class="language-python"># ç»˜åˆ¶ loss æ›²çº¿ï¼Œæˆ‘ä»¬èƒ½çœ‹åˆ°æ¨¡å‹åœ¨ä¸€å¼€å§‹å¿«é€Ÿçš„æ”¶æ•›ï¼Œæ¥ä¸‹æ¥ä»¥ä¸€ä¸ªè¾ƒæ…¢çš„é€Ÿåº¦æŒç»­ä¼˜åŒ–ï¼ˆæˆ‘ä»¬ç”¨å³è¾¹ log åæ ‡è½´çš„è§†å›¾å¯ä»¥çœ‹çš„æ›´æ¸…æ¥šï¼‰ï¼š

fig, axs = plt.subplots(1, 2, figsize=(12, 4))
axs[0].plot(losses)
axs[1].plot(np.log(losses))
plt.show()</code></pre> 
<h2><img alt="" height="429" src="https://images2.imgbox.com/cf/4c/ndeiWcYF_o.png" width="1182"></h2> 
<h2>äº”ã€ç”Ÿæˆå›¾åƒÂ </h2> 
<p>ä¸‹é¢å¼€å§‹ç”Ÿæˆå›¾åƒÂ </p> 
<pre><code class="language-python"># æ–¹æ³• 1ï¼šå»ºç«‹ä¸€ä¸ªç®¡é“ï¼š
from diffusers import DDPMPipeline

image_pipe = DDPMPipeline(unet=model, scheduler=noise_scheduler)
pipeline_output = image_pipe()
pipeline_output.images[0]</code></pre> 
<p><img alt="" height="100" src="https://images2.imgbox.com/2b/62/xOlY037D_o.png" width="774"></p> 
<pre><code class="language-python"># æˆ‘ä»¬å¯ä»¥åœ¨æœ¬åœ°æ–‡ä»¶å¤¹è¿™æ ·ä¿å­˜ä¸€ä¸ªç®¡é“ï¼š
image_pipe.save_pretrained("my_pipeline")

# æ£€æŸ¥æ–‡ä»¶å¤¹çš„å†…å®¹ï¼š
!ls my_pipeline/</code></pre> 
<p>Â è¿™é‡Œschedulerä¸unetå­æ–‡ä»¶å¤¹ä¸­åŒ…å«äº†ç”Ÿæˆå›¾åƒæ‰€éœ€çš„å…¨éƒ¨ç»„ä»¶ã€‚æ¯”å¦‚ï¼Œåœ¨unetæ–‡ä»¶ä¸­èƒ½çœ‹åˆ°æ¨¡å‹å‚æ•° (diffusion_pytorch_model.bin) ä¸æè¿°æ¨¡å‹ç»“æ„çš„é…ç½®æ–‡ä»¶ã€‚</p> 
<p><img alt="" height="101" src="https://images2.imgbox.com/83/d1/gcvlUawN_o.png" width="666"></p> 
<pre><code class="language-python"># æ–¹æ³• 2ï¼šå†™ä¸€ä¸ªå–æ ·å¾ªç¯

# ä»éšæœºå™ªå£°å¼€å§‹ï¼Œéå†ç®¡ç†å™¨çš„è¿­ä»£å‘¨æœŸæ¥çœ‹ä»æœ€å˜ˆæ‚ç›´åˆ°æœ€å¾®å°çš„å™ªå£°å˜åŒ–ï¼ŒåŸºäºæ¨¡å‹çš„é¢„æµ‹ä¸€æ­¥æ­¥å‡å°‘ä¸€äº›å™ªå£°ï¼š

# Random starting point (8 random images):
sample = torch.randn(8, 3, 32, 32).to(device)

for i, t in enumerate(noise_scheduler.timesteps):

    # Get model pred
    with torch.no_grad():
        residual = model(sample, t).sample

    # Update sample with step
    sample = noise_scheduler.step(residual, t, sample).prev_sample
    
# noise_scheduler.step () å‡½æ•°ç›¸åº”åšäº† sampleï¼ˆå–æ ·ï¼‰æ—¶çš„æ•°å­¦è¿ç®—ã€‚
show_images(sample)</code></pre> 
<p><img alt="" height="77" src="https://images2.imgbox.com/64/d7/93Nh2dGf_o.png" width="554"></p> 
<h2>Â å…­ã€å°†æ¨¡å‹ä¸Šä¼ åˆ°Hugging Face</h2> 
<p>åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­æˆ‘ä»¬æŠŠç®¡é“ä¿å­˜åœ¨äº†æœ¬åœ°ã€‚ æŠŠæ¨¡å‹ push åˆ° hub ä¸Šï¼Œæˆ‘ä»¬ä¼šéœ€è¦å»ºç«‹æ¨¡å‹å’Œç›¸åº”æ–‡ä»¶çš„ä»“åº“åã€‚ æˆ‘ä»¬æ ¹æ®ä½ çš„é€‰æ‹©ï¼ˆæ¨¡å‹ IDï¼‰æ¥å†³å®šä»“åº“çš„åå­—ï¼ˆå¤§èƒ†çš„å»æ›¿æ¢æ‰model_nameå§ï¼›éœ€è¦åŒ…å«ä½ çš„ç”¨æˆ·åï¼Œget_full_repo_name ()ä¼šå¸®ä½ åšåˆ°ï¼‰ï¼š</p> 
<pre><code class="language-python">from huggingface_hub import get_full_repo_name

model_name = "sd-class-butterflies-32"
hub_model_id = get_full_repo_name(model_name)
hub_model_id</code></pre> 
<pre><code class="language-python"># ç„¶åï¼Œåœ¨ ğŸ¤— Hub ä¸Šåˆ›å»ºæ¨¡å‹ä»“åº“å¹¶ push å®ƒå§ï¼š

from huggingface_hub import HfApi, create_repo

create_repo(hub_model_id)
api = HfApi()
api.upload_folder(
    folder_path="my_pipeline/scheduler", path_in_repo="", repo_id=hub_model_id
)
api.upload_folder(folder_path="my_pipeline/unet", path_in_repo="", repo_id=hub_model_id)
api.upload_file(
    path_or_fileobj="my_pipeline/model_index.json",
    path_in_repo="model_index.json",
    repo_id=hub_model_id,
)</code></pre> 
<pre><code class="language-python"># æœ€åä¸€ä»¶äº‹æ˜¯åˆ›å»ºä¸€ä¸ªè¶…æ£’çš„æ¨¡å‹å¡ï¼Œå¦‚æ­¤ï¼Œæˆ‘ä»¬çš„è´è¶ç”Ÿæˆå™¨å¯ä»¥è½»æ¾çš„åœ¨ Hub ä¸Šè¢«æ‰¾åˆ°ï¼ˆè¯·åœ¨æè¿°ä¸­éšæ„å‘æŒ¥ï¼ï¼‰ï¼š

from huggingface_hub import ModelCard

content = f"""
---
license: mit
tags:
- pytorch
- diffusers
- unconditional-image-generation
- diffusion-models-class
---

# Model Card for Unit 1 of the [Diffusion Models Class ğŸ§¨](https://github.com/huggingface/diffusion-models-class)

This model is a diffusion model for unconditional image generation of cute ğŸ¦‹.

## Usage

```python
from diffusers import DDPMPipeline

pipeline = DDPMPipeline.from_pretrained('{hub_model_id}')
image = pipeline().images[0]
image
```
"""

card = ModelCard(content)
card.push_to_hub(hub_model_id)</code></pre> 
<pre><code class="language-python"># ç°åœ¨æ¨¡å‹å·²ç»åœ¨ Hub ä¸Šäº†ï¼Œä½ å¯ä»¥è¿™æ ·ä»ä»»ä½•åœ°æ–¹ä½¿ç”¨DDPMPipelineçš„from_pretrained ()æ–¹æ³•æ¥ä¸‹æ¥å®ƒï¼š
from diffusers import DDPMPipeline

image_pipe = DDPMPipeline.from_pretrained(hub_model_id)
pipeline_output = image_pipe()
pipeline_output.images[0]</code></pre> 
<p><img alt="" height="337" src="https://images2.imgbox.com/06/31/O3ZnxrMp_o.png" width="1106"></p> 
<p></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a5cb65fa80bbb247e7657a7c170851db/" rel="prev">
			<span class="pager__subtitle">Â«&thinsp;Previous</span>
			<p class="pager__title">ã€Linuxã€‘è¿›ç¨‹æ¦‚å¿µä¸è¿›ç¨‹çŠ¶æ€</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/9555d6a5c869007de76f086365282e02/" rel="next">
			<span class="pager__subtitle">Next&thinsp;Â»</span>
			<p class="pager__title">OpenCVæ¢ç´¢ä¹‹è·¯ï¼ˆäºŒåä¸‰ï¼‰ï¼šç‰¹å¾æ£€æµ‹å’Œç‰¹å¾åŒ¹é…æ–¹æ³•æ±‡æ€»</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 èœé¸Ÿç¨‹åºå‘˜åšå®¢.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>