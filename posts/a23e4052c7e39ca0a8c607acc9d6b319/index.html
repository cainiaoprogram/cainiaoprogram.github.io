<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>python爬虫（案例）——豆瓣读书爬虫 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="python爬虫（案例）——豆瓣读书爬虫" />
<meta property="og:description" content="文章目录 要爬取的内容一级页面（分类中图书的列表）二级页面（每本书的详情页） 本案例中的防封ip小技巧多用几个user-agent（随机抽取）设置间隔时间 完整代码 本篇文章为豆瓣读书爬虫的案例，采用了xpath解析式，比较基础，未涉及其他深入的爬虫知识 要爬取的内容 根据豆瓣图书中不同的分类爬取图书的相关信息 ( 每个分类豆瓣最多给50页数据 ）
一级页面（分类中图书的列表） 爬取 ： 书名（文本和url），作者，出版社，出版日期，评价数，缩略图链接，短简介
二级页面（每本书的详情页） 通过一级页面 书名的url链接进入二级页面
爬取 ： 评分，内容简介，作者简介，标签
本案例中的防封ip小技巧 多用几个user-agent（随机抽取） def header_x(): # 随机获取一个headers user_agents = [&#39;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36&#39;, &#39;Mozilla/5.0 (Windows NT 6.1; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0&#39;, &#39;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.57.2 (KHTML, like Gecko) Version/5.1.7 Safari/534.57.2&#39;, &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/a23e4052c7e39ca0a8c607acc9d6b319/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-06-29T10:54:08+08:00" />
<meta property="article:modified_time" content="2021-06-29T10:54:08+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">python爬虫（案例）——豆瓣读书爬虫</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#_3" rel="nofollow">要爬取的内容</a></li><li><ul><li><a href="#_6" rel="nofollow">一级页面（分类中图书的列表）</a></li><li><a href="#_13" rel="nofollow">二级页面（每本书的详情页）</a></li></ul> 
  </li><li><a href="#ip_20" rel="nofollow">本案例中的防封ip小技巧</a></li><li><ul><li><a href="#useragent_21" rel="nofollow">多用几个user-agent（随机抽取）</a></li><li><a href="#_41" rel="nofollow">设置间隔时间</a></li></ul> 
  </li><li><a href="#_56" rel="nofollow">完整代码</a></li></ul> 
</div> 
<br> 本篇文章为豆瓣读书爬虫的案例，采用了xpath解析式，比较基础，未涉及其他深入的爬虫知识 
<p></p> 
<h2><a id="_3"></a>要爬取的内容</h2> 
<p>根据豆瓣图书中不同的分类爬取图书的相关信息 ( 每个分类豆瓣最多给50页数据 ）</p> 
<h3><a id="_6"></a>一级页面（分类中图书的列表）</h3> 
<p><img src="https://images2.imgbox.com/fb/a8/H0H2AK0a_o.png" alt="在这里插入图片描述"></p> 
<p><strong>爬取 ： 书名（文本和url），作者，出版社，出版日期，评价数，缩略图链接，短简介</strong></p> 
<hr> 
<h3><a id="_13"></a>二级页面（每本书的详情页）</h3> 
<p><img src="https://images2.imgbox.com/e5/0f/DYnpjT4t_o.png" alt="在这里插入图片描述"><br> 通过一级页面 书名的url链接进入二级页面</p> 
<p><strong>爬取 ： 评分，内容简介，作者简介，标签</strong></p> 
<hr> 
<h2><a id="ip_20"></a>本案例中的防封ip小技巧</h2> 
<h3><a id="useragent_21"></a>多用几个user-agent（随机抽取）</h3> 
<pre><code>def header_x():
    # 随机获取一个headers
    user_agents = ['Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36',
                   'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0',
                   'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.57.2 (KHTML, like Gecko) Version/5.1.7 Safari/534.57.2',
                   'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36',
                   'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71 Safari/537.1 LBBROWSER',
                   'Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 SE 2.X MetaSr 1.0'
                   ]

    headers = {
        "User-Agent": random.choice(user_agents)
    }
    return headers
</code></pre> 
<p>常见user-agent大全：https://www.cnblogs.com/zrmw/p/9332801.html（转，大佬写的（非本人））</p> 
<h3><a id="_41"></a>设置间隔时间</h3> 
<p>设置每条数据的时间间隔</p> 
<pre><code>time.sleep(random.randint(5, 8))
</code></pre> 
<p>设置换页的时间等待时间</p> 
<pre><code>time.sleep(random.randint(3, 5))
</code></pre> 
<p><strong>注意：随机数千万不要从0开始，不要太小，容易被封ip</strong></p> 
<h2><a id="_56"></a>完整代码</h2> 
<pre><code>import requests
from lxml import etree
import csv
import re
import time
import random

def header_x():
    # 随机获取一个headers
    user_agents = ['Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36',
                   'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0',
                   'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.57.2 (KHTML, like Gecko) Version/5.1.7 Safari/534.57.2',
                   'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36',
                   'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71 Safari/537.1 LBBROWSER',
                   'Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 SE 2.X MetaSr 1.0'
                   ]

    headers = {
        "User-Agent": random.choice(user_agents)
    }
    return headers

# 要爬取的 豆瓣读书中的 分类名称
kinds = ['','']


for book_kind in kinds:

    # 为每个分类创建一个csv文件
    csvFile = open("{}.csv".format(book_kind), mode="w+", encoding="utf-8")

    for i in range(1, 51):
        print('{name}   开始爬取第 {index} 页'.format(name=book_kind,index=i))

        # 拼接url
        url = 'https://book.douban.com/tag/{name}?start={num}&amp;type=T'.format(name=book_kind,num=i*20-20)

        headers = header_x()

        resp = requests.get(url, headers=headers)
        html = etree.HTML(resp.text)

        lis = html.xpath("//div[@id='subject_list']/ul/li")

        for li in lis:
            try:
                name = li.xpath("./div[@class='info']/h2/a/@title")   # 书名
                img_url = li.xpath("./div[@class='pic']/a/@href")     # 缩略图链接
                author = li.xpath("./div[@class='info']/div[@class='pub']/text()")[0].strip().split('/')[0]  # 作者
                publisher = li.xpath("./div[@class='info']/div[@class='pub']/text()")[0].strip().split('/')[-3]  # 出版社
                publish_time = li.xpath("./div[@class='info']/div[@class='pub']/text()")[0].strip().split('/')[-2]  # 出版年
                # 判断出版社，出版年份是否在指定位置 如果不在 则跳过
                if(publish_time.find('-')==-1):
                    continue
                grade = li.xpath(".//span[@class='pl']/text()")[0].strip()   # 评价数

                # 处理grade 提取数字
                grade_num = []
                grade_num = re.findall("\d+\.?\d*", grade)

                intro_1 = li.xpath("./div[@class='info']/p/text()")[0].strip()   # 小简介

                # 子链接
                son_url = li.xpath("./div[@class='info']/h2/a/@href")[0]   #子链接


                resp_son = requests.get(son_url, headers=headers)
                html_son = etree.HTML(resp_son.text)

                # 评分
                score = html_son.xpath("//strong[@class='ll rating_num ']/text()")[0].strip()
                # 简介
                intro = ''.join(html_son.xpath("//div[@id='link-report']//div[@class='intro']/p/text()"))
                # 作者简介
                author_intro = ''.join(html_son.xpath("//div[@class='indent ']//div[@class='']/div/p/text()"))
                # 标签
                label = html_son.xpath("//div[@id='db-tags-section']//a/text()")
                label = ' '.join(label)
            except IndexError:
                continue

            # 把字符串转换成列表
            author = list(author.split('&amp;&amp;'))
            publisher = list(publisher.split('&amp;&amp;'))
            publish_time = list(publish_time.split('&amp;&amp;'))
            intro_1 = list(intro_1.split('&amp;&amp;'))

            score = list(score.split('&amp;&amp;'))
            intro = list(intro.split('&amp;&amp;'))
            author_intro = list(author_intro.split('&amp;&amp;'))
            label = list(label.split('&amp;&amp;'))

            # 把数据放入列表
            result = []
            result.extend(name)
            result.extend(author)
            result.extend(img_url)
            result.extend(publisher)
            result.extend(publish_time)
            result.extend(grade_num)
            result.extend(intro_1)

            result.extend(score)
            result.extend(intro)
            result.extend(author_intro)
            result.extend(label)

            # 将列表写入 csv文件
            write = csv.writer(csvFile)
            write.writerow(result)

            # 设置每条数据的时间间隔
            time.sleep(random.randint(5, 8))

        print("{name}    第 {index} 页爬取完成！！！".format(name=book_kind,index=i))

        # 设置换页的时间等待时间
        time.sleep(random.randint(3, 5))


</code></pre> 
<p><strong>注：</strong> 修改kinds列表，就可以直接开始爬取数据了，最长2.5个小时爬取完一个分类（50页）</p> 
<p><strong>注：</strong> 因为豆瓣中每个详情页的结构可能不同（我也很是费解），所以有些值爬出来的可能为空，导致处理时程序报错，小编为了方便，遇到为空的就 try…except 直接跳过当前详情页了</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/870a51ba2a9edfadc62ce99af52cabd1/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">函数</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0f2a2a268d7e1616581290994b5aead0/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Elasticsearch指定以逗号分词查询</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>