<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>首篇综述！Open Vocabulary学习综述：全面调研 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="首篇综述！Open Vocabulary学习综述：全面调研" />
<meta property="og:description" content="点击下方卡片，关注“CVer”公众号
AI/CV重磅干货，第一时间送达
点击进入—&gt;【目标检测和Transformer】交流群
前言：
最近大火的Open Vocabulary 方向第一篇综述！北京大学、南洋理工大学等联合发表最新的Open Vocabulary 综述。综述涵盖多种任务（检测、分割、视频、3D）的百多种不同的Open Vocabulary方法，最新模型截止至今年6月！同时，综述还首次在统一数据集上公平对比了目前的代表性方法，并讨论了几个具有广阔前景的未来研究方向！
在CVer公众号后台回复：开放词汇综述，即可下载本文综述PDF和项目
摘要：
在视觉场景理解领域，深度神经网络在分割、跟踪和检测等各种核心任务中取得了令人印象深刻的进步。然而，大多数方法都基于闭集假设，这意味着模型只能识别训练集中存在的预定义类别。最近，由于视觉语言预训练（VLM）的快速进展，开放词汇（Open Vocabulary）设置被提出。这些新方法寻求识别和定位带注释的标签之外的类别。与弱监督（Weak Supervision）和零样本（Zero-Shot）设置相比，开放词汇方法更通用、更实用、更有效。作者对开放词汇学习进行了全面回顾，总结和分析了该领域的最新发展。作者首先将其与零样本学习、开集识别和分布外检测等相关概念进行比较。然后回顾了分割和检测中几个密切相关的任务，包括长尾问题、少样本和零样本设置。对于方法综述，作者首先介绍检测和分割的基本知识作为初步知识。接下来，作者划定了使用开放词汇学习的各种场景，然后比较常用数据集和基准（Benchmark）中最新的检测和分割方法。最后，作者总结了关于未来研究方向的见解、问题和讨论。
论文题目：Towards Open Vocabulary Learning: A Survey
发表单位：北京大学，南洋理工大学等
论文地址：https://arxiv.org/abs/2306.15880
项目地址：
https://github.com/jianzongwu/Awesome-Open-Vocabulary
提交时间：2023年6月28日
在CVer公众号后台回复：开放词汇综述，即可下载本文综述PDF和项目
1，研究动机：
新热点，大众瞩目的方向：
图 1 Open Vocabulary近期代表性工作
如图所示，从2021年第一篇提出Open Vocabulary Object Detection的工作开始，Open Vocabulary的工作数量逐年增加，逐渐成为计算机视觉&#43;自然语言处理，多模态领域的新热点。在过去的两中，针对不同任务的Open Vocabulary工作提出了总计有一百多种方法。
2，这篇综述的特色，以及和相关领域的综述有什么区别？
图 2 Open Vocabulary和其他setting的区别
本文是聚焦于Open Vocabulary领域的第一篇综述。
1，为了明确定义，作者对Open Vocabulary和其他setting做了详细的区分和定义。具体来说，Open-Set/Open World/OOD 不对novel类别进行分类，Zero-Shot对novel类别进行分类，Open Vocabulary不仅对novel类别进行分类，它还可以使用和图像相关的文本数据进行弱监督训练，而Zero-Shot中，训练数据是严格不能和novel类别重合的。
2，本综述也会系统地回顾下近些年来在闭集的一些分割检测方法的进展，作为预备的知识，方便新人也能很快了解闭集以及open-vocabulary之间的关系。同时综述还回顾了几个相关领域，比如zero-shot segmentation/detection，long-tail segmentation/detection。
3，本综述是从具体的技术细节对现有的分割检测以及3D任务进行细粒度的一个分类，确保读者对整个领域方法有个大致系统性的认知。
4，本综述详细地对比了多个不同open vocabulary setting下的方法性能，确保
3，Open Vocabulary典型算法框架总结：
图 3 Open Vocabulary典型算法框架
本文以一张图总结了Open Vocabulary领域检测/分割的一个典型通用算法框架。图片首先输入一个Image Encoder，再将Encoder得出的特征输入到检测/分割头中，得到bounding box/object mask，以及每个物体的visual embedding。和传统分割算法不同的是，open vocabulary领域将物体的分类器权重换成了由VLM-text生成的base和novel类别的text embedding。通过计算visual embedding和text embedding的相似度，最终可以得出每个物体的类别。目前使用最多的VLM-text模型是CLIP。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/ca0a78794889a5c6b90eceda37ef2c3f/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-07T23:59:33+08:00" />
<meta property="article:modified_time" content="2023-07-07T23:59:33+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">首篇综述！Open Vocabulary学习综述：全面调研</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <h4></h4> 
 <p style="text-align:center;">点击下方<strong>卡片</strong>，关注“<strong>CVer</strong>”公众号</p> 
 <h4></h4> 
 <p style="text-align:center;">AI/CV重磅干货，第一时间送达</p> 
 <p><a href="" rel="nofollow"><strong><strong><strong>点击进入—&gt;</strong>【目标检测和Transformer】交流群</strong></strong></a></p> 
 <p><strong>前言：</strong></p> 
 <p>最近大火的Open Vocabulary 方向第一篇综述！北京大学、南洋理工大学等联合发表最新的Open Vocabulary 综述。综述涵盖多种任务（检测、分割、视频、3D）的百多种不同的Open Vocabulary方法，最新模型截止至今年6月！同时，综述还首次在统一数据集上公平对比了目前的代表性方法，并讨论了几个具有广阔前景的未来研究方向！</p> 
 <p><strong>在CVer公众号后台回复：</strong><strong>开放词汇综述</strong><strong>，即可下载本文综述PDF和项目</strong></p> 
 <p><strong>摘要：</strong></p> 
 <p>在视觉场景理解领域，深度神经网络在分割、跟踪和检测等各种核心任务中取得了令人印象深刻的进步。然而，大多数方法都基于闭集假设，这意味着模型只能识别训练集中存在的预定义类别。最近，由于视觉语言预训练（VLM）的快速进展，开放词汇（Open Vocabulary）设置被提出。这些新方法寻求识别和定位带注释的标签之外的类别。与弱监督（Weak Supervision）和零样本（Zero-Shot）设置相比，开放词汇方法更通用、更实用、更有效。<strong>作者对开放词汇学习进行了全面回顾，总结和分析了该领域的最新发展</strong>。作者首先将其与零样本学习、开集识别和分布外检测等相关概念进行比较。然后回顾了分割和检测中几个密切相关的任务，包括长尾问题、少样本和零样本设置。<strong>对于方法综述，作者首先介绍检测和分割的基本知识作为初步知识。接下来，作者划定了使用开放词汇学习的各种场景，然后比较常用数据集和基准（Benchmark）中最新的检测和分割方法。最后，作者总结了关于未来研究方向的见解、问题和讨论</strong>。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/ae/ee/S2XF57Fa_o.png" alt="d0504fe6677a3f732d1f454b1c52fa93.png"></p> 
 <p>论文题目：Towards Open Vocabulary Learning: A Survey</p> 
 <p>发表单位：北京大学，南洋理工大学等</p> 
 <p>论文地址：https://arxiv.org/abs/2306.15880</p> 
 <p>项目地址：</p> 
 <p><strong>https://github.com/jianzongwu/Awesome-Open-Vocabulary</strong></p> 
 <p>提交时间：2023年6月28日</p> 
 <p><strong>在CVer公众号后台回复：</strong><strong>开放词汇综述</strong><strong>，即可下载本文综述PDF和项目</strong></p> 
 <p><strong>1，研究动机：</strong></p> 
 <p>新热点，大众瞩目的方向：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/58/75/C2gFZioc_o.png" alt="18270acd039e89759d9d55d26b9475bc.png"></p> 
 <p>图 1 Open Vocabulary近期代表性工作</p> 
 <p>如图所示，从2021年第一篇提出Open Vocabulary Object Detection的工作开始，Open Vocabulary的工作数量逐年增加，逐渐成为计算机视觉+自然语言处理，多模态领域的新热点。在过去的两中，针对不同任务的Open Vocabulary工作提出了总计有一百多种方法。</p> 
 <p><strong>2，这篇综述的特色，以及和相关领域的综述有什么区别？</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/51/aa/1xZqc8Ma_o.png" alt="3ebef55169262eb7eeb432e1fa939601.png"></p> 
 <p>图 2 Open Vocabulary和其他setting的区别</p> 
 <p><strong>本文是聚焦于Open Vocabulary领域的第一篇综述。</strong></p> 
 <p>1，为了明确定义，作者对Open Vocabulary和其他setting做了详细的区分和定义。具体来说，Open-Set/Open World/OOD 不对novel类别进行分类，Zero-Shot对novel类别进行分类，Open Vocabulary不仅对novel类别进行分类，它还可以使用和图像相关的文本数据进行弱监督训练，而Zero-Shot中，训练数据是严格不能和novel类别重合的。</p> 
 <p>2，本综述也会系统地回顾下近些年来在闭集的一些分割检测方法的进展，作为预备的知识，方便新人也能很快了解闭集以及open-vocabulary之间的关系。同时综述还回顾了几个相关领域，比如zero-shot segmentation/detection，long-tail segmentation/detection。</p> 
 <p>3，本综述是从具体的技术细节对现有的分割检测以及3D任务进行细粒度的一个分类，确保读者对整个领域方法有个大致系统性的认知。</p> 
 <p>4，本综述详细地对比了多个不同open vocabulary setting下的方法性能，确保</p> 
 <p><strong>3，Open Vocabulary典型算法框架总结：</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/42/0c/S9zfczIt_o.png" alt="e1e0379a73067f166d987517514755dd.png"></p> 
 <p>图 3 Open Vocabulary典型算法框架</p> 
 <p>本文以一张图总结了Open Vocabulary领域检测/分割的一个典型通用算法框架。图片首先输入一个Image Encoder，再将Encoder得出的特征输入到检测/分割头中，得到bounding box/object mask，以及每个物体的visual embedding。和传统分割算法不同的是，open vocabulary领域将物体的分类器权重换成了由VLM-text生成的base和novel类别的text embedding。通过计算visual embedding和text embedding的相似度，最终可以得出每个物体的类别。目前使用最多的VLM-text模型是CLIP。</p> 
 <p><strong>4. Open Vocabulary Object Detection 代表性方法梳理</strong></p> 
 <p><strong>4.1  知识蒸馏方法。</strong></p> 
 <p>这些技术的目标是将视觉语言模型（VLMs）的知识提炼到封闭集检测器中。由于VLMs的知识要比封闭集检测器的知识更为丰富，将新类别的知识提炼到基于已训练检测器的类别中是一个直观的想法，为此最早的工作都是在探索如何更好地去做知识蒸馏。</p> 
 <p><strong>4.2 联合视觉特征与文本的预训练策略。</strong></p> 
 <p>Open Vocabulary学习的另一个假设是大规模图像文本对数据的可用性以及可获取性质。由于这些对包含足够丰富的知识，可以覆盖检测和分割中最新颖或未见过的数据集。区域文本对齐的学习将视觉特征和文本特征的新类别映射到一个对齐的特征空间中。</p> 
 <p><strong>4.3 使用更平衡的数据进行训练。</strong></p> 
 <p>图像分类数据集中常常存在罕见和未知的数据，为此可以使用这些数据集和Open-Vocabulary数据集联合训练来解决novel class缺失问题。这些方法的核心思想是通过利用更平衡的数据，包括图像分类数据集、图像-文本数据的伪标签或额外相关的检测数据来解决这个问题。</p> 
 <p><strong>4.4  视觉区域特征与文本对齐的策略。</strong></p> 
 <p>一些方法主要是设计更好的对齐策略，来提升VLMs蒸馏效果。这些方法主要是探究如何把box对应的区域性质的VLMs视觉特征与VLMs的语言特征进行对齐。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/5d/14/hRKzfJgM_o.png" alt="e95c01f2e6ce3744b789a44429ab0c3c.png"></p> 
 <p>图 4 Open Vocabulary检测和分割代表性方法梳理</p> 
 <p>此外，本文对两年来open vocabulary在检测和分割领域的代表性方法用一张表格做了梳理。表格中包含方法的简称，针对的任务，文本训练数据，视觉训练数据，使用的文本模型，视觉模型，以及一句话概括式描述，如图4所示。</p> 
 <p><strong>5. Open Vocabulary 分割方法总结</strong></p> 
 <p><strong>5.1 使用VLM的大规模语料分类性能</strong></p> 
 <p>视觉语言模型（VLM）通常在大规模图片-文本对上进行训练，比如LAION-5B数据集，在这些数据集上训练出的VLM本质上见过包括base类和novel类在内的各种类别，天然具备识别novel类的能力。然而VLM通常是用图像级别的对比学习进行预训练的，没有处理像素级别任务的能力，例如语义分割。在open vocabulary semantic segmentation领域，一种自然的想法就是提取利用VLM的知识，用VLM的文本特征代替原本的闭集分类器，让分割模型能够识别出novel类别。</p> 
 <p><strong>5.2 从图像标题数据中学习</strong></p> 
 <p>除了利用VLM在大规模数据上训练得出的分类性能之外，还有一种广泛存在且易获得的数据类型，即图像标题（image captions）。和预定义好的类别不同，标题中描述的物体可能会形容一些novel类，这就给了模型在训练过程中接触novel类弱标注的机会，这也是open vocabulary区别于zero-shot的核心不同点。数篇文章提出不同的方法，以更好的利用caption data来提取其中的novel类别，帮助扩展模型在novel类上的识别能力。</p> 
 <p><strong>5.3 不使用像素级别的数据进行训练</strong></p> 
 <p>在大多数open vocabulary工作中，尽管模型不需要novel类别的像素级别标注进行训练（如mask，bounding box），但仍需要base类别的像素级别标注，这仍然导致了需要人工标注，增加数据困难的问题。为了彻底解决这一问题，有研究提出了仅使用image caption进行训练的方法，只需要caption这一弱标注，就可以训练出能够不限语义空间的检测器/分类器。例如GroupViT提出了基于group机制的分割方法，通过图像-标题的对比损失函数进行训练，摆脱了分割模型对于mask标注的依赖。</p> 
 <p><strong>5.4 同时学习多个任务</strong></p> 
 <p>图像分割的任务主要包括语义分割，实例分割，以及全景分割，在open vocabulary segmentation领域，这些任务之间能否彼此促进，以及来自不同任务的数据集能否通过共同训练促进彼此任务的性能提升，是一个值得探究的问题。数个工作提出了universal的模型，它们通常以一个Transformer架构产生隶属于不同任务的输出，可以同时在多个任务的数据集上进行训练。OpenSeed构建了一个可以同时在open vocabulary detection和segmentation数据集上训练的框架，他们发现将隶属于两个任务的数据联合起来训练，可以分别提升两方面数据单独训练时的detection/segmentation性能。</p> 
 <p><strong>5.5 使用生成式扩散模型</strong></p> 
 <p>近期，生成式扩散模型（Generative Diffusion Models, DM）在图像生成领域取得了巨大的成就。两篇文章就如何利用DM来帮助提升open vocabulary任务的性能提出了各自的方案。其一是利用DM能够生成逼真且高度差异化的图像的能力，来生成大量属于novel类别的伪图片，并将这些伪图片作为样本（prototypes）保存。在inference时，输入图片和各类别的prototypes进行比对，相似度最高的类别即为预测类别。另一篇文章指出，DM能够生成高质量图片，说明其模型中间层的特征已经建立了丰富的文本-语义联系的知识，如果能把这种知识提取出来，用为分割模型的分类器，将能够达到VLM一样甚至更好的效果。实验结果证明了他们结论的正确性。</p> 
 <p><strong>6，不同方法的实验结果对比</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/83/3c/DpLyVrEx_o.png" alt="7d5b4baf08b13e4220f20117ee6a6977.png"></p> 
 <p>表 1 OVOD在COCO上的结果</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/53/40/YfuLOhNL_o.png" alt="6acac5f495be31dca4ae082b6ff0f62a.png"></p> 
 <p>表 2 OVOD在LVIS上的结果</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/e1/df/JgeKi8zd_o.png" alt="06d3d437ca86be2d40dfe4453a5bcf28.png"></p> 
 <p>表 3 OVSS在self-evaluation setting上的结果</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/df/d2/qXyTfaLe_o.png" alt="9a9d256f1e8d23317cdae6c867ea63be.png"></p> 
 <p>表 4 OVSS在cross-evaluation上的结果</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/13/3a/X0GSlsS0_o.png" alt="38fe1828f9695ab54aa2351666aa68bd.png"></p> 
 <p>表 5 OVIS的结果</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/86/1d/AXe1BE9P_o.png" alt="1406edbaf0512af2e18f10be7b3d6ba8.png"></p> 
 <p>表 6 OVPS在ADE20K上的结果</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/a1/dd/UVbg660v_o.png" alt="5b6a90923e633361cdbda63e8a5ca5ae.png"></p> 
 <p>表 7 OVPS在COCO上的结果</p> 
 <p>本文首次对open vocabulary各任务，各方法做了一个公平与系统性的比较，open vocabulary任务包括在目标检测，语义分割，实例分割，全景分割。验证数据集包括COCO，ADE20K，LVIS，PASCAL VOC，PASCAL Context。</p> 
 <p><strong>5，未来可以进行的方向：</strong></p> 
 <p><strong>1) 探索时间维度的信息</strong></p> 
 <p>在实际应用中，视频数据容易获得并且使用更加频繁。准确地分割和跟踪novel类别的对象值得关注，这对于短视频剪辑和自动驾驶汽车等广泛的现实场景十分必要。然而，只有少数工作探索视频中检测和跟踪的开放词汇学习。因此，需要一个更加动态、更具挑战性的视频数据集来充分探索视觉语言模型在开放词汇学习中的潜力。</p> 
 <p><strong>2) 3D open vocabulary场景理解</strong></p> 
 <p>与图像和视频相比，点云数据的标注成本更高，特别是对于密集的预测任务。因此，3D open vocabulary场景理解的研究需求更加迫切。当前的 3D open vocabulary场景理解解决方案侧重于设计投影功能，以更好地使用2D VLM。将 2D VLM的知识整合到 3D数据将是未来的方向。</p> 
 <p><strong>3）探索针对特定任务的视觉基础模型的适配模块（adapter）</strong></p> 
 <p>视觉基础模型（Vision foundation models）可以在多个标准分类和分割数据集上实现良好的zero-shot性能。然而，对于一些特定的任务，例如医学图像分析和航空图像，仍然存在许多极端情况。因此，需要为这些特定任务设计特定于任务的适配器（adapters）。此类适配器可以充分利用预先训练的基础模型的知识，达到提升下游任务性能的效果。、</p> 
 <p><strong>4）在目标数据集上的有效训练</strong></p> 
 <p>如表4所示，大多数最先进的方法需要大量数据进行预训练才能获得良好的性能。然而，这样造成的结果是成本昂贵，大多数研究小组无法效仿。因此，借助VLM，设计更高效的数据学习pipeline或学习方法更加实用且经济。一种可能的解决方案是采用in-context learninig来充分探索或连接VLM和LLM的知识</p> 
 <p><strong>5）base类别过拟合问题</strong></p> 
 <p>大多数方法通过从base类的标注中学习检测和分割novel类别的对象。因此，novel类别的对象和base类的对象之间的形状和语义信息曝光程度存在天然差距。VLM 模型可以通过预先训练的视觉文本知识来弥补这种差距。然而，当两个novel类具有相似的形状和语义时，大多数检测器仍然很容易过度拟合到base类，因为这些类是以更高的置信度分数进行训练的。将来的工作需要更细粒度的特征判别建模（包括细节或属性）来处理这些问题。</p> 
 <p><strong>6）和持续学习（incremental learning）相结合    </strong></p> 
 <p>在实际场景中，数据标注通常是开放而容易改变的，其中新的类可能会持续地增加。然而，直接把现有的open vocabulary方法应用到incremental learning上可能会导致灾难性遗忘的问题。如何在一个框架中同时处理灾难性遗忘问题和新类别检测是未来值得探索的。</p> 
 <p>这个项目的开源地址在https://github.com/jianzongwu/Awesome-Open-Vocabulary，如果有缺失的paper，欢迎相关同行提PR，本项目会持续更新。</p> 
 <p><strong>在CVer公众号后台回复：</strong><strong>开放词汇综述</strong><strong>，即可下载本文综述PDF和项目</strong></p> 
 <h4></h4> 
 <p style="text-align:left;"><em><em><strong><strong><a href="" rel="nofollow"><strong><strong><strong><strong>点击进入—&gt;</strong>【目标检测和Transformer】交流群</strong></strong></strong></a></strong></strong></em></em></p> 
 <p style="text-align:left;"><strong>最新CVPR 2023论文和代码下载</strong><br></p> 
 <pre></pre> 
 <p style="text-align:left;">后台回复：<strong>CVPR2023，</strong>即可下载CVPR 2023论文和代码开源的论文合集</p> 
 <p style="text-align:left;">后台回复：<strong>Transformer综述，</strong>即可下载最新的3篇Transformer综述PDF</p> 
 <pre class="has"><code class="language-go">目标检测和Transformer交流群成立
扫描下方二维码，或者添加微信：CVer333，即可添加CVer小助手微信，便可申请加入CVer-目标检测或者Transformer 微信交流群。另外其他垂直方向已涵盖：目标检测、图像分割、目标跟踪、人脸检测&amp;识别、OCR、姿态估计、超分辨率、SLAM、医疗影像、Re-ID、GAN、NAS、深度估计、自动驾驶、强化学习、车道线检测、模型剪枝&amp;压缩、去噪、去雾、去雨、风格迁移、遥感图像、行为识别、视频理解、图像融合、图像检索、论文投稿&amp;交流、PyTorch、TensorFlow和Transformer等。
一定要备注：研究方向+地点+学校/公司+昵称（如目标检测或者Transformer+上海+上交+卡卡），根据格式备注，可更快被通过且邀请进群

▲扫码或加微信号: CVer333，进交流群
CVer计算机视觉（知识星球）来了！想要了解最新最快最好的CV/DL/AI论文速递、优质实战项目、AI行业前沿、从入门到精通学习教程等资料，欢迎扫描下方二维码，加入CVer计算机视觉，已汇集数千人！

▲扫码进星球
▲点击上方卡片，关注CVer公众号</code></pre> 
 <p style="text-align:right;"><strong><strong><strong><strong><strong><strong><strong><strong>整理不易，请点赞和在看<strong><img width="30" src="https://images2.imgbox.com/47/f4/tArWTTyL_o.gif" alt="0dc12f1b2c94a70f2f16cc76ab206e50.gif"></strong></strong></strong></strong></strong></strong></strong></strong></strong></p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/3c3bafeb7394e47716ee83a034f4f502/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Maven高级（四）--私服</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/900c6b23a0c511620984a774f8826eeb/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">jar包运行</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>