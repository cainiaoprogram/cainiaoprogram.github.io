<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ICCV 2023 | 论文及代码合集 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="ICCV 2023 | 论文及代码合集" />
<meta property="og:description" content="近日，世界三大顶级视觉会议之一ICCV公开了最新录用结果。
根据文件里给出的ID，总共有2160篇论文入选。
我们整理了部分录用论文及其代码合集（持续更新…）
[1] Rethinking Mobile Block for Efficient Attention-based Models
[Code]GitHub - zhangzjn/EMO: [ICCV 2023] Official PyTorch implementation of &#34;Rethinking Mobile Block for Efficient Attention-based Models&#34;
[Area]backbone
[2] IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel View Synthesis
[Code]https://zju3dv.github.io/intrinsic_nerf/
[Area]NeRF
[3] PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment
[Code]PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment
[Area]Diffusion Models
[4] FreeDoM: Training-Free Energy-Guided Conditional Diffusion Model
[Code]GitHub - vvictoryuki/FreeDoM: [ICCV 2023] Official PyTorch implementation for the paper &#34;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/4013d3cf24eea8f7e0420c081b0c4151/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-18T11:34:33+08:00" />
<meta property="article:modified_time" content="2023-07-18T11:34:33+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ICCV 2023 | 论文及代码合集</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>近日，世界三大顶级视觉会议之一ICCV公开了最新录用结果。</p> 
<p></p> 
<p class="img-center"><img alt="" height="724" src="https://images2.imgbox.com/27/4f/jfVUd3bI_o.png" width="1200"></p> 
<p>根据文件里给出的ID，总共有2160篇论文入选。</p> 
<p></p> 
<p>我们整理了部分录用论文及其代码合集（持续更新…）</p> 
<p></p> 
<p>[1] Rethinking Mobile Block for Efficient Attention-based Models</p> 
<p>[Code]<a href="https://github.com/zhangzjn/EMO" title='GitHub - zhangzjn/EMO: [ICCV 2023] Official PyTorch implementation of "Rethinking Mobile Block for Efficient Attention-based Models"'>GitHub - zhangzjn/EMO: [ICCV 2023] Official PyTorch implementation of "Rethinking Mobile Block for Efficient Attention-based Models"</a></p> 
<p>[Area]<strong>backbone</strong></p> 
<p></p> 
<p>[2] IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel View Synthesis</p> 
<p>[Code]<a href="https://zju3dv.github.io/intrinsic_nerf" rel="nofollow" title="https://zju3dv.github.io/intrinsic_nerf/">https://zju3dv.github.io/intrinsic_nerf/</a></p> 
<p>[Area]<strong>NeRF</strong></p> 
<p></p> 
<p>[3] PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment</p> 
<p>[Code]<a href="https://posediffusion.github.io/" rel="nofollow" title="PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment">PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment</a></p> 
<p>[Area]<strong>Diffusion Models</strong></p> 
<p></p> 
<p>[4] FreeDoM: Training-Free Energy-Guided Conditional Diffusion Model</p> 
<p>[Code]<a href="https://github.com/vvictoryuki/FreeDoM" title='GitHub - vvictoryuki/FreeDoM: [ICCV 2023] Official PyTorch implementation for the paper "FreeDoM: Training-Free Energy-Guided Conditional Diffusion Model"'>GitHub - vvictoryuki/FreeDoM: [ICCV 2023] Official PyTorch implementation for the paper "FreeDoM: Training-Free Energy-Guided Conditional Diffusion Model"</a></p> 
<p>[Area]<strong>Diffusion Models</strong></p> 
<p></p> 
<p>[5] Femtodet: an object detection baseline for energy versus performance tradeoffs</p> 
<p>[Code]<a href="https://github.com/yh-pengtu/FemtoDet" title="https://github.com/yh-pengtu/FemtoDet">https://github.com/yh-pengtu/FemtoDet</a></p> 
<p>[Area]<strong>目标检测</strong></p> 
<p></p> 
<p>[6] Segment Anything</p> 
<p>[Code]<a href="https://github.com/facebookresearch/segment-anything" title="GitHub - facebookresearch/segment-anything: The repository provides code for running inference with the SegmentAnything Model (SAM), links for downloading the trained model checkpoints, and example notebooks that show how to use the model.">GitHub - facebookresearch/segment-anything: The repository provides code for running inference with the SegmentAnything Model (SAM), links for downloading the trained model checkpoints, and example notebooks that show how to use the model.</a></p> 
<p>[Area]<strong>语义分割</strong></p> 
<p></p> 
<p>[7] MARS: Model-agnostic Biased Object Removal without Additional Supervision for Weakly-Supervised Semantic Segmentation</p> 
<p>[Code]<a href="https://github.com/shjo-april/MARS" title="GitHub - shjo-april/MARS: [ICCV2023] MARS: Model-agnostic Biased Object Removal without Additional Supervision for Weakly-Supervised Semantic Segmentation">GitHub - shjo-april/MARS: [ICCV2023] MARS: Model-agnostic Biased Object Removal without Additional Supervision for Weakly-Supervised Semantic Segmentation</a></p> 
<p>[Area]<strong>语义分割</strong></p> 
<p></p> 
<p>[8] DVIS: Decoupled Video Instance Segmentation Framework</p> 
<p>[Code]<a href="https://github.com/zhang-tao-whu/DVIS" title="GitHub - zhang-tao-whu/DVIS: DVIS: Decoupled Video Instance Segmentation Framework">GitHub - zhang-tao-whu/DVIS: DVIS: Decoupled Video Instance Segmentation Framework</a></p> 
<p>[Area]<strong>视频实例分割</strong></p> 
<p></p> 
<p>[9] Robo3D: Towards Robust and Reliable 3D Perception against Corruptions</p> 
<p>[Code]<a href="https://github.com/ldkong1205/Robo3D" title="GitHub - ldkong1205/Robo3D: [ICCV'23] Robo3D: Towards Robust and Reliable 3D Perception against Corruptions">GitHub - ldkong1205/Robo3D: [ICCV'23] Robo3D: Towards Robust and Reliable 3D Perception against Corruptions</a></p> 
<p>[Area]<strong>3D点云</strong></p> 
<p></p> 
<p>[10] PETRv2: A Unified Framework for 3D Perception from Multi-Camera Images</p> 
<p>[Code]<a href="https://github.com/megvii-research/PETR" title="GitHub - megvii-research/PETR: [ECCV2022] PETR: Position Embedding Transformation for Multi-View 3D Object Detection &amp; [ICCV2023] PETRv2: A Unified Framework for 3D Perception from Multi-Camera Images">GitHub - megvii-research/PETR: [ECCV2022] PETR: Position Embedding Transformation for Multi-View 3D Object Detection &amp; [ICCV2023] PETRv2: A Unified Framework for 3D Perception from Multi-Camera Images</a></p> 
<p>[Area]<strong>3D目标检测</strong></p> 
<p></p> 
<p>[11] DQS3D: Densely-matched Quantization-aware Semi-supervised 3D Detection</p> 
<p>[Code]<a href="https://github.com/AIR-DISCOVER/DQS3D" title="GitHub - AIR-DISCOVER/DQS3D: DQS3D: Densely-matched Quantization-aware Semi-supervised 3D Detection">GitHub - AIR-DISCOVER/DQS3D: DQS3D: Densely-matched Quantization-aware Semi-supervised 3D Detection</a></p> 
<p>[Area]<strong>3D目标检测</strong></p> 
<p></p> 
<p>[12] SparseFusion: Fusing Multi-Modal Sparse Representations for Multi-Sensor 3D Object Detection</p> 
<p>[Code]<a href="https://github.com/yichen928/SparseFusion" title="GitHub - yichen928/SparseFusion: [ICCV 2023] SparseFusion: Fusing Multi-Modal Sparse Representations for Multi-Sensor 3D Object Detection">GitHub - yichen928/SparseFusion: [ICCV 2023] SparseFusion: Fusing Multi-Modal Sparse Representations for Multi-Sensor 3D Object Detection</a></p> 
<p>[Area]<strong>3D目标检测</strong></p> 
<p></p> 
<p>[13] StreamPETR: Exploring Object-Centric Temporal Modeling for Efficient Multi-View 3D Object Detection</p> 
<p>[Code]<a href="https://github.com/exiawsh/StreamPETR.git" title="GitHub - exiawsh/StreamPETR: [ICCV 2023] StreamPETR: Exploring Object-Centric Temporal Modeling for Efficient Multi-View 3D Object Detection">GitHub - exiawsh/StreamPETR: [ICCV 2023] StreamPETR: Exploring Object-Centric Temporal Modeling for Efficient Multi-View 3D Object Detection</a></p> 
<p>[Area]<strong>3D目标检测</strong></p> 
<p></p> 
<p>[14] Cross Modal Transformer: Towards Fast and Robust 3D Object Detection</p> 
<p>[Code]<a href="https://github.com/junjie18/CMT" title="https://github.com/junjie18/CMT">https://github.com/junjie18/CMT</a></p> 
<p>[Area]<strong>3D目标检测</strong></p> 
<p></p> 
<p>[15] Rethinking Range View Representation for LiDAR Segmentation</p> 
<p>[Code]None</p> 
<p>[Area]<strong>3D语义分割</strong></p> 
<p></p> 
<p>[16] Unmasked Teacher: Towards Training-Efficient Video Foundation Models</p> 
<p>[Code]<a href="https://github.com/OpenGVLab/unmasked_teacher" title="GitHub - OpenGVLab/unmasked_teacher: [ICCV2023] Unmasked Teacher: Towards Training-Efficient Video Foundation Models">GitHub - OpenGVLab/unmasked_teacher: [ICCV2023] Unmasked Teacher: Towards Training-Efficient Video Foundation Models</a></p> 
<p>[Area]<strong>视频理解</strong></p> 
<p></p> 
<p>[17] FreeDoM: Training-Free Energy-Guided Conditional Diffusion Model</p> 
<p>[Code]<a href="https://github.com/vvictoryuki/FreeDoM" title='GitHub - vvictoryuki/FreeDoM: [ICCV 2023] Official PyTorch implementation for the paper "FreeDoM: Training-Free Energy-Guided Conditional Diffusion Model"'>GitHub - vvictoryuki/FreeDoM: [ICCV 2023] Official PyTorch implementation for the paper "FreeDoM: Training-Free Energy-Guided Conditional Diffusion Model"</a></p> 
<p>[Area]<strong>图像生成</strong></p> 
<p></p> 
<p>[18] Simulating Fluids in Real-World Still Images</p> 
<p>[Code]<a href="https://github.com/simon3dv/SLR-SFS" title="https://github.com/simon3dv/SLR-SFS">https://github.com/simon3dv/SLR-SFS</a></p> 
<p>[Area]<strong>视频生成</strong></p> 
<p></p> 
<p>[19] FateZero: Fusing Attentions for Zero-shot Text-based Video Editing</p> 
<p>[Code]<a href="https://fate-zero-edit.github.io/" rel="nofollow" title="Fate/Zero">Fate/Zero</a></p> 
<p>[Area]<strong>视频编辑</strong></p> 
<p></p> 
<p>[20] Implicit Neural Representation for Cooperative Low-light Image Enhancement</p> 
<p>[Code]<a href="https://github.com/Ysz2022/NeRCo" title="https://github.com/Ysz2022/NeRCo">https://github.com/Ysz2022/NeRCo</a></p> 
<p>[Area]<strong>低光照图像增强</strong></p> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ffc81ebc7ccec077031671169cba8704/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">CSS中的BFC，是什么？</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/72b64f80d45524612a0bfaac2d98739b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">尚硅谷学习笔记-Kubeadm安装K8S集群</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>