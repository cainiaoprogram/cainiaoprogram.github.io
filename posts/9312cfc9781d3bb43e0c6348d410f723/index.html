<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>BERT系列算法解读:（RoBERTa/ALBERT/DistilBERT/Transformer/Hugging Face/NLP/预训练模型/模型蒸馏） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="BERT系列算法解读:（RoBERTa/ALBERT/DistilBERT/Transformer/Hugging Face/NLP/预训练模型/模型蒸馏）" />
<meta property="og:description" content="BERT（Bidirectional Encoder Representations from Transformers，基于Transformers的双向编码器表示）系列算法在自然语言处理任务中是必不可少的经典模型，当初第一代GPT模型发布的时候，坐了冷板凳，罪魁祸首就是BERT。
有任何问题欢迎在下面留言
本篇文章配套的PPT资源已经上传
目录
1、如何训练BERT
1.1相关背景
1.2 方法1随机遮挡
1.3 方法2连接预测
2 ALBERT
2.1 什么是ALBERT
2.2 第一点大矩阵拆分
2.3 第二点跨层参数共享
2.4 实验中还告诉我们的故事
3 RoBERTa
3.1 如何训练RoBERTa
3.2RoBERTa-wwm
4 DistilBERT
1、如何训练BERT 在图像任务中，针对不同的任务需要设计不同的策略以及不同的网络设计，但是NLP任务真没有五花八门的操作，太多NLP的基础基本都是不需要的。基于2023的今天，大多数的NLP我觉得都比较啰嗦，后续也不可能用到，硕博学位论文那肯定还是会用到的。
1.1相关背景 语言模型，BERT只是其中的一种，后续本人会在CSDN上分析多个语言模型。
17年，Transformer提出来，给了NLP一个新的方法新的架构
18年，谷歌带头做出BERT，他说他有数据，他有算力，用Transformer训练一个大的语言模型出来即BERT模型
19年-20年，出现了很多对于BERT的改进（改进不是说把模型结构改变，把模型做的更大，是想办法把训练效率做的更高一些）
这么说吧，整个NLP领域都是基于Transformer去做的，只不过做的更大而已，这么多年过去了，还是用的17年这个结构，只不过做的更大。
打开Hugging Face的模型页面，按销量排：
（截止2023年7月13日）排前18的模型有10个都是BERT相关的，其中标红的第一个bert-base-uncased是原装的bert。
关于这方面已经不推荐大家去看“很多论文”了，因为论文很多告诉你的都是一个训练过程。
1.2 方法1随机遮挡 方法1：句子中有15%的词汇被随机mask掉交给模型去预测被mask的家伙到底是什么词语的可能性太多了，中文一般是字如果BERT训练的向量好，那分类自然OK 比如我现在有很多文本数据，但是其实并没有一个实际要做的目的。语言模型并不是一个固定的任务，不是做什么分类和回归，也不是做什么NER。就是让模型理解人类的文字，理解人说话的逻辑，这才叫语言模型，并不是一个具体的任务。
现在需要的是培养模型的语言能力，所以需要标签吗？不需要标签，在互联网中有海量的数据，随便去取一个句子，这句话有四个词，我随机mask掉一个词，这是随机选择的，然后让模型去猜这个被mask的词是哪个。
想一想英语考试，考试考的是什么？是你英文的一个学习能力，怎么考察呢？有一些完形填空的任务，有一些选择题，有一些阅读理解题，让你理解这些题在什么前提下？在你英语水平比较高的前提下，你就能做的比较好了
1.3 方法2连接预测 方法2：预测两个句子是否应该连在一起[seq]：两个句子之前的连接符[cls]：表示要做分类的向量 第二种方法没有mask，将两句完整句子用[SEP]连接起来，判断两个句子有没有相关性，做一个二分类
如果模型能够理解语言的含义，自然而然能够预测准，看第二张图：
这两句话就没有连接性，老师上召唤师峡谷给你点名吗？
两种方法都是为了让模型理解我们文字的含义，想想你在英语考试中什么前提能让你在考试中做的好，就是语言能力。
有了语言模型，有一个能理解语言含义的模型之后，就能做一下下游任务，比如：
情感分类NER（Named Entity Recognition，命名实体识别）关系抽取语法检查纠错 仅仅列出一小部分，这些都是一个下游任务，怎么把这些下游任务做好，归根到底就是模型的语言能力非常强，能够很好的理解上下文的含义，拼的就是一个把语言模型做好的前提。
你让一个清华的状元和一个街溜子去做同一件事，清华状元的学习能力强可能就能做得好。
2 ALBERT 2.1 什么是ALBERT A Lite BERT，轻量级的BERT" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/9312cfc9781d3bb43e0c6348d410f723/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-17T17:57:55+08:00" />
<meta property="article:modified_time" content="2023-07-17T17:57:55+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">BERT系列算法解读:（RoBERTa/ALBERT/DistilBERT/Transformer/Hugging Face/NLP/预训练模型/模型蒸馏）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>BERT（Bidirectional Encoder Representations from Transformers，基于Transformers的双向编码器表示）系列算法在自然语言处理任务中是必不可少的经典模型，当初第一代GPT模型发布的时候，坐了冷板凳，罪魁祸首就是BERT。</p> 
<blockquote> 
 <p><span style="color:#fe2c24;"><strong>有任何问题欢迎在下面留言</strong></span></p> 
 <p><span style="color:#fe2c24;"><strong>本篇文章配套的PPT资源已经上传</strong></span></p> 
</blockquote> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="1%E3%80%81%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83BERT-toc" style="margin-left:0px;"><a href="#1%E3%80%81%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83BERT" rel="nofollow">1、如何训练BERT</a></p> 
<p id="1.1%E7%9B%B8%E5%85%B3%E8%83%8C%E6%99%AF-toc" style="margin-left:40px;"><a href="#1.1%E7%9B%B8%E5%85%B3%E8%83%8C%E6%99%AF" rel="nofollow">1.1相关背景</a></p> 
<p id="1.2%20%E6%96%B9%E6%B3%951%E9%9A%8F%E6%9C%BA%E9%81%AE%E6%8C%A1-toc" style="margin-left:40px;"><a href="#1.2%20%E6%96%B9%E6%B3%951%E9%9A%8F%E6%9C%BA%E9%81%AE%E6%8C%A1" rel="nofollow">1.2 方法1随机遮挡</a></p> 
<p id="1.3%20%E6%96%B9%E6%B3%952%E8%BF%9E%E6%8E%A5%E9%A2%84%E6%B5%8B-toc" style="margin-left:40px;"><a href="#1.3%20%E6%96%B9%E6%B3%952%E8%BF%9E%E6%8E%A5%E9%A2%84%E6%B5%8B" rel="nofollow">1.3 方法2连接预测</a></p> 
<p id="2%20ALBERT-toc" style="margin-left:0px;"><a href="#2%20ALBERT" rel="nofollow">2 ALBERT</a></p> 
<p id="2.1%20%E4%BB%80%E4%B9%88%E6%98%AFALBERT-toc" style="margin-left:40px;"><a href="#2.1%20%E4%BB%80%E4%B9%88%E6%98%AFALBERT" rel="nofollow">2.1 什么是ALBERT</a></p> 
<p id="2.2%20%E7%AC%AC%E4%B8%80%E7%82%B9%E5%A4%A7%E7%9F%A9%E9%98%B5%E6%8B%86%E5%88%86-toc" style="margin-left:40px;"><a href="#2.2%20%E7%AC%AC%E4%B8%80%E7%82%B9%E5%A4%A7%E7%9F%A9%E9%98%B5%E6%8B%86%E5%88%86" rel="nofollow">2.2 第一点大矩阵拆分</a></p> 
<p id="2.3%20%E7%AC%AC%E4%BA%8C%E7%82%B9%E8%B7%A8%E5%B1%82%E5%8F%82%E6%95%B0%E5%85%B1%E4%BA%AB-toc" style="margin-left:40px;"><a href="#2.3%20%E7%AC%AC%E4%BA%8C%E7%82%B9%E8%B7%A8%E5%B1%82%E5%8F%82%E6%95%B0%E5%85%B1%E4%BA%AB" rel="nofollow">2.3 第二点跨层参数共享</a></p> 
<p id="2.4%20%E5%AE%9E%E9%AA%8C%E4%B8%AD%E8%BF%98%E5%91%8A%E8%AF%89%E6%88%91%E4%BB%AC%E7%9A%84%E6%95%85%E4%BA%8B-toc" style="margin-left:40px;"><a href="#2.4%20%E5%AE%9E%E9%AA%8C%E4%B8%AD%E8%BF%98%E5%91%8A%E8%AF%89%E6%88%91%E4%BB%AC%E7%9A%84%E6%95%85%E4%BA%8B" rel="nofollow">2.4 实验中还告诉我们的故事</a></p> 
<p id="3%20RoBERTa-toc" style="margin-left:0px;"><a href="#3%20RoBERTa" rel="nofollow">3 RoBERTa</a></p> 
<p id="3.1%20%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83RoBERTa-toc" style="margin-left:40px;"><a href="#3.1%20%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83RoBERTa" rel="nofollow">3.1 如何训练RoBERTa</a></p> 
<p id="3.2RoBERTa-wwm-toc" style="margin-left:40px;"><a href="#3.2RoBERTa-wwm" rel="nofollow">3.2RoBERTa-wwm</a></p> 
<p id="4%C2%A0DistilBERT-toc" style="margin-left:0px;"><a href="#4%C2%A0DistilBERT" rel="nofollow">4 DistilBERT</a></p> 
<hr id="hr-toc"> 
<h2 id="1%E3%80%81%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83BERT">1、如何训练BERT</h2> 
<p>在图像任务中，针对不同的任务需要设计不同的策略以及不同的网络设计，但是NLP任务真没有五花八门的操作，太多NLP的基础基本都是不需要的。基于2023的今天，大多数的NLP我觉得都比较啰嗦，后续也不可能用到，硕博学位论文那肯定还是会用到的。</p> 
<h3 id="1.1%E7%9B%B8%E5%85%B3%E8%83%8C%E6%99%AF">1.1相关背景</h3> 
<p>语言模型，BERT只是其中的一种，后续本人会在CSDN上分析多个语言模型。</p> 
<p>17年，Transformer提出来，给了NLP一个新的方法新的架构</p> 
<p>18年，谷歌带头做出BERT，他说他有数据，他有算力，用Transformer训练一个大的语言模型出来即BERT模型</p> 
<p>19年-20年，出现了很多对于BERT的改进（改进不是说把模型结构改变，把模型做的更大，是想办法把训练效率做的更高一些）</p> 
<p>这么说吧，整个NLP领域都是基于Transformer去做的，只不过做的更大而已，这么多年过去了，还是用的17年这个结构，只不过做的更大。</p> 
<p>打开<a class="link-info" href="https://huggingface.co/models" rel="nofollow" title="Hugging Face的模型页面">Hugging Face的模型页面</a>，按销量排：</p> 
<p><img alt="" height="296" src="https://images2.imgbox.com/8b/cc/aRxTo7AC_o.png" width="403"></p> 
<p>（截止2023年7月13日）排前18的模型有10个都是BERT相关的，其中标红的第一个bert-base-uncased是原装的bert。</p> 
<p>关于这方面已经不推荐大家去看“很多论文”了，因为论文很多告诉你的都是一个训练过程。</p> 
<h3 id="1.2%20%E6%96%B9%E6%B3%951%E9%9A%8F%E6%9C%BA%E9%81%AE%E6%8C%A1">1.2 方法1随机遮挡</h3> 
<ul><li>方法1：句子中有15%的词汇被随机mask掉</li><li>交给模型去预测被mask的家伙到底是什么</li><li>词语的可能性太多了，中文一般是字</li><li>如果BERT训练的向量好，那分类自然OK</li></ul> 
<p>比如我现在有很多文本数据，但是其实并没有一个实际要做的目的。语言模型并不是一个固定的任务，不是做什么分类和回归，也不是做什么NER。就是让模型理解人类的文字，理解人说话的逻辑，这才叫语言模型，并不是一个具体的任务。</p> 
<p>现在需要的是培养模型的语言能力，所以需要标签吗？不需要标签，在互联网中有海量的数据，随便去取一个句子，这句话有四个词，我随机mask掉一个词，这是随机选择的，然后让模型去猜这个被mask的词是哪个。</p> 
<p><img alt="" height="354" src="https://images2.imgbox.com/f1/e0/pfrkL0qd_o.png" width="306"></p> 
<p> 想一想英语考试，考试考的是什么？是你英文的一个学习能力，怎么考察呢？有一些完形填空的任务，有一些选择题，有一些阅读理解题，让你理解这些题在什么前提下？在你英语水平比较高的前提下，你就能做的比较好了</p> 
<h3 id="1.3%20%E6%96%B9%E6%B3%952%E8%BF%9E%E6%8E%A5%E9%A2%84%E6%B5%8B">1.3 方法2连接预测</h3> 
<ul><li>方法2：预测两个句子是否应该连在一起</li><li>[seq]：两个句子之前的连接符</li><li>[cls]：表示要做分类的向量</li></ul> 
<p>第二种方法没有mask，将两句完整句子用[SEP]连接起来，判断两个句子有没有相关性，做一个二分类</p> 
<p><img alt="" height="238" src="https://images2.imgbox.com/fd/55/WXo2fbfH_o.png" width="401"></p> 
<p> 如果模型能够理解语言的含义，自然而然能够预测准，看第二张图：</p> 
<p><img alt="" height="234" src="https://images2.imgbox.com/06/c2/jHpSEb1G_o.png" width="394"></p> 
<p>这两句话就没有连接性，老师上召唤师峡谷给你点名吗？</p> 
<p>两种方法都是为了让模型理解我们文字的含义，想想你在英语考试中什么前提能让你在考试中做的好，就是语言能力。</p> 
<p>有了语言模型，有一个能理解语言含义的模型之后，就能做一下下游任务，比如：</p> 
<ul><li>情感分类</li><li>NER（Named Entity Recognition，命名实体识别）</li><li>关系抽取</li><li>语法检查纠错</li></ul> 
<p>仅仅列出一小部分，这些都是一个下游任务，怎么把这些下游任务做好，归根到底就是模型的语言能力非常强，能够很好的理解上下文的含义，拼的就是一个把语言模型做好的前提。</p> 
<p>你让一个清华的状元和一个街溜子去做同一件事，清华状元的学习能力强可能就能做得好。</p> 
<h2 id="2%20ALBERT">2 ALBERT</h2> 
<h3 id="2.1%20%E4%BB%80%E4%B9%88%E6%98%AFALBERT">2.1 什么是ALBERT</h3> 
<p>A Lite BERT，轻量级的BERT</p> 
<ul><li>从BERT开始NLP就一直强调一件事，要想效果好，模型就一定得大</li><li>但是如果模型很大，权重参数就会非常多，训练是一个大问题(显存都装不下)</li><li>训练速度也是一个事，现在大厂模型都要以月为单位，速度巨慢</li><li>能不能简化下BERT，让他训练的更快更容易一些呢？（Transformer中Embedding占20%参数，Attetntion占80%）</li></ul> 
<p>之前训练CV任务的时候，都是以小时为单位的，可能训练三五个小时就出效果了，但是在自然语言处理任务中，三五个小时那可能还没训练完百分之一呢。</p> 
<p>给大家举个例子，<a class="link-info" href="https://huggingface.co/bigscience" rel="nofollow" title="看这个模型BLOOM">看这个模型BLOOM</a>，176B（1760亿）个参数，开发人员和研究人员超过1000个，BLOOM能够以46种自然语言和13种编程语言生成文本。</p> 
<p><img alt="" height="239" src="https://images2.imgbox.com/78/38/vtZLDhqQ_o.png" width="481"></p> 
<p></p> 
<p>他们每天都在推特发布训练进度：</p> 
<p><img alt="" height="341" src="https://images2.imgbox.com/2b/53/5nyJ4hmu_o.png" width="277"></p> 
<p>每天只能训练1%，现在已经训练完成，那他是不是单CPU去训练的啊？要不然怎么会这么慢呢。到底是不是单卡，看看这台超级计算机的配置： </p> 
<blockquote> 
 <p>这是来自70多个国家和250多个机构的1000多名研究人员一年工作的成果，最终在法国巴黎南部的Jean Zay超级计算机上训练了117天(3月11日至7月6日)的BLOOM模型，这要归功于法国国家科学研究中心(CNRS)和法国科学研究中心(CNRS)估计价值300万欧元的计算拨款。</p> 
 <p>训练硬件:</p> 
 <ul><li> <p>GPU: 384 张 NVIDIA A100 80GB GPU (48 个节点) + 32 张备用 GPU</p> </li><li> <p>每个节点 8 张 GPU，4 条 NVLink 卡间互联，4 条 OmniPath 链路</p> </li><li> <p>CPU: AMD EPYC 7543 32 核处理器</p> </li><li> <p>CPU 内存: 每个节点 512GB</p> </li><li> <p>GPU 显存: 每个节点 640GB</p> </li></ul> 
 <p>Checkpoints:</p> 
 <ul><li> <p>每个 checkpoint 含精度为 fp32 的优化器状态和精度为 bf16+fp32 的权重，占用存储空间为 2.3TB。如只保存 bf16 的权重，则仅占用 329GB 的存储空间。</p> </li></ul> 
 <p>数据集:</p> 
 <ul><li> <p>1.5TB 经过大量去重和清洗的文本，包含 46 种语言，最终转换为 350B 个词元</p> </li><li> <p>模型的词汇表含 250,680 个词元</p> </li></ul> 
 <p>176B BLOOM 模型的训练于 2022 年 3 月至 7 月期间，耗时约 3.5 个月完成 (约 100 万计算时)。</p> 
</blockquote> 
<p>NLP的模型量级怎么增长的，是指数级增长啊，真的恐怖如斯</p> 
<p>ALBERT从轻量级作为一个切入点，ALBERT的研究人员发现Transformer中Embedding占20%参数，Attetntion占80%</p> 
<p>先熟悉几个变量：</p> 
<ul><li>E：词嵌入大小</li><li>H：隐藏层大小</li><li>V：语料库中词的个数</li></ul> 
<p>E就是第一层Embedding后得到向量的维度。比如“今天”这个词，经过词嵌入后映射成了一个768维的向量，这个E就等于768了。</p> 
<p>H就是词经过词嵌入后，再经过self-Attention，还要经过一些全连接层，之后得到的向量维度，一般都会等于E。</p> 
<p>V是语料库中词的个数，比如字典中一共有两万个词，在前面的mask任务中，求解mask就要从这个两万个词中选择，这个任务就是一个两万分类的任务了。</p> 
<p>正常情况下，在embbeding层的参数量，需要把所有词都映射成向量，一共多少词呢？这里一定要理解，就是两万个词。每个词都映射成768维的向量，这中间需要构建的权重矩阵就是768*20000。这个矩阵很大，两万这个数字还只是一个保守的说法，在实际中可能要再翻个十倍。</p> 
<p><span style="color:#fe2c24;"><strong>那有没有办法给这个参数量降低一下呢？</strong></span></p> 
<h3 id="2.2%20%E7%AC%AC%E4%B8%80%E7%82%B9%E5%A4%A7%E7%9F%A9%E9%98%B5%E6%8B%86%E5%88%86"><span style="color:#0d0016;">2.2 第一点大矩阵拆分</span></h3> 
<ul><li>通过一个中介，将一层转换为两层，但是参数量可以大幅降低</li><li>参数量：（V×H）降低到( V ×E + E ×H )</li><li>此时如果H&gt;&gt;E，就达到了咱们的目的（E越小可能会效果越差）</li><li>但是Embedding层只是第一步，Attention如何简化才是重头戏</li></ul> 
<p></p> 
<p>我将768和10000二者之间引入一个中介，将这个768*20000的大矩阵拆分成两个小矩阵。所以ALBERT的做法就是，embbeding层做成两层。由768*20000变成768*100+100*20000，这个100是自己设计的，这样参数量就大大降低了。</p> 
<p>不同E值会产生影响，E小一些影响结果，但是不大，这是一个E值的影响的表格：</p> 
<table align="center" border="1" cellpadding="1" cellspacing="1" style="width:666px;"><tbody><tr><td style="width:80px;">Model</td><td style="width:52px;">E</td><td style="width:91px;">Parameters</td><td style="width:95px;">SQuAD1.1</td><td style="width:51px;">SQuAD2.0</td><td style="width:61px;">MNLI</td><td style="width:60px;">SST-2</td><td style="width:84px;">RACE</td><td style="width:65px;">Avg</td></tr><tr><td colspan="1" rowspan="4" style="width:80px;"> <p>ALBERT</p> <p>   base</p> <p>not-hared</p> </td><td style="width:52px;"> <p>64</p> </td><td style="width:91px;"> <p>    87M</p> </td><td style="width:95px;">89.9/82.9</td><td style="width:51px;">80.1/77.8</td><td style="width:61px;">82.9</td><td style="width:60px;">91.5</td><td style="width:84px;">66.7</td><td style="width:65px;">81.3</td></tr><tr><td style="width:52px;"> <p>128</p> </td><td style="width:91px;"> <p>    89M</p> </td><td style="width:95px;">89.9/82.8</td><td style="width:51px;">80.377.3</td><td style="width:61px;">83.7</td><td style="width:60px;">91.5</td><td style="width:84px;">67.9</td><td style="width:65px;">81.7</td></tr><tr><td style="width:52px;"> <p>256</p> </td><td style="width:91px;"> <p>    93M</p> </td><td style="width:95px;">90.2/83.2</td><td style="width:51px;">80.3/77.4</td><td style="width:61px;">84.1</td><td style="width:60px;">91.9</td><td style="width:84px;">67.3</td><td style="width:65px;">81.8</td></tr><tr><td style="width:52px;">768</td><td style="width:91px;">    108M</td><td style="width:95px;">90.4/83.2</td><td style="width:51px;">80.4/77.6</td><td style="width:61px;">84.5</td><td style="width:60px;">92.8</td><td style="width:84px;">68.2</td><td style="width:65px;">82.3</td></tr><tr><td colspan="1" rowspan="4" style="width:80px;"> <p>ALBERT</p> <p>   base</p> <p>all-hared</p> </td><td style="width:52px;"> <p>64</p> </td><td style="width:91px;">    10M</td><td style="width:95px;">88.7/81.4 </td><td style="width:51px;">77.5/74.8</td><td style="width:61px;">80.8</td><td style="width:60px;">89.4</td><td style="width:84px;">63.5</td><td style="width:65px;">79.0</td></tr><tr><td style="width:52px;"> <p>128</p> </td><td style="width:91px;">   12M        </td><td style="width:95px;">89.3/82.3</td><td style="width:51px;">80.0/77.1</td><td style="width:61px;">81.6</td><td style="width:60px;">90.3</td><td style="width:84px;">64.0</td><td style="width:65px;">80.1</td></tr><tr><td style="width:52px;">256</td><td style="width:91px;">   16M</td><td style="width:95px;">88.8/81.5</td><td style="width:51px;">79.1/76.3</td><td style="width:61px;">81.5</td><td style="width:60px;">90.3</td><td style="width:84px;">63.4</td><td style="width:65px;">79.6</td></tr><tr><td style="width:52px;">768</td><td style="width:91px;">   31M</td><td style="width:95px;">88.6/81.5</td><td style="width:51px;">79.2/76.6</td><td style="width:61px;">82.0</td><td style="width:60px;">90.6</td><td style="width:84px;">63.3</td><td style="width:65px;">79.8</td></tr></tbody></table> 
<p> 首先可以看E越大特征越多，后面6列都是不同对比实验也有不同评分标准。从表格中可以看出，虽然这个策略有一定效果，但是并没有一个本质上的改进。这个结果有一定了解就行了，因为你几乎不可能去复现这个结果的。（别人的数据是无限的，计算资源也很强，论文你看看得了，这种实验就是一个可远观不可亵玩焉）</p> 
<h3 id="2.3%20%E7%AC%AC%E4%BA%8C%E7%82%B9%E8%B7%A8%E5%B1%82%E5%8F%82%E6%95%B0%E5%85%B1%E4%BA%AB"><span style="color:#0d0016;">2.3 第二点跨层参数共享</span></h3> 
<p><span style="color:#0d0016;">共享的方法有很多，ALBERT选择了全部共享，FFN和ATTENTION的都共享。</span></p> 
<p><span style="color:#0d0016;">之前我们的文章有讲过卷积神经网络，卷积层和全连接对比一下。全连接非常费参数，因为全连接没有任何共享的地方。卷积呢？为什么能省参数，同一个卷积核，在图像中的各个位置的权重完全一样，所以它参数少，好训练，因为CNN的效果比FC强。</span></p> 
<p style="text-align:center;"><img alt="" height="245" src="https://images2.imgbox.com/a0/5d/wHXAHBwo_o.png" width="326"></p> 
<p><span style="color:#0d0016;">在Transformer的堆叠中，一层self-Attention接着一层FC，上一层self-Attention和下一层self-Attention的权重参数能共享吗？当时19年读这篇论文的时候，我就在想，啊？这还能共享啊？self-Attention一层一层的堆叠，参数还共享，那还是Transformer吗，这个Transformer不是没有意义了么？但是经过实验证明，还真能共享。 </span></p> 
<p><span style="color:#0d0016;">来看一下结果：</span></p> 
<p><img alt="" height="267" src="https://images2.imgbox.com/00/05/fiGWol54_o.png" width="1034"></p> 
<p>先看这个E=768的，就是先不考虑E，all-shared全部共享参数有31M，结果是79.8，not-shared不共享，参数有108M，结果是82.3。确实把模型变小了很多，但是结果下降了，要是没有下降，那真是见鬼了。但是令人惊叹的是，有下降，但是下降的却很少。</p> 
<p>然后看E=128，也就是说E也减少了，然后参数又进一步大大减小了，但是结果仍然是有下降，但是下降的不多。</p> 
<p>在深度学习的很多结果，都是一个没有办法证明的事，只能是通过实验看到这样一个结论。</p> 
<p>这就使得ALBERT在实际中被使用的更多，在我们的实际项目中，我们很少使用BERT这样一个模型，因为太庞大了。在训练与部署的过程中，需要考虑成本的，什么成本，时间成本。</p> 
<p>你想想，用户给你一个反馈，点了鼠标，半天没有反应，人家气的肯定直接关了网页。</p> 
<h3 id="2.4%20%E5%AE%9E%E9%AA%8C%E4%B8%AD%E8%BF%98%E5%91%8A%E8%AF%89%E6%88%91%E4%BB%AC%E7%9A%84%E6%95%85%E4%BA%8B">2.4 实验中还告诉我们的故事</h3> 
<p>层数是不是越多越好？</p> 
<p><img alt="" height="161" src="https://images2.imgbox.com/af/74/0lElzUVx_o.png" width="793"></p> 
<p>目前来看确实就是的。</p> 
<p>隐层的特征是不是越多越好呢？</p> 
<p><img alt="" height="123" src="https://images2.imgbox.com/c7/c8/tEjG6Da6_o.png" width="740"></p> 
<p>目前来看确实也是的，但是也不能一直大下去啊，一般来说隐层特征768就可以了。大到6144就开始下降了，实际上是因为数据跟不上了，虽然是NLP的数据是无限的，但是毕竟还是有限的。太大了也确实没意义。</p> 
<p>看一下此时此刻，ALBERT在Hugging Face能排到第几名：</p> 
<p><img alt="" height="294" src="https://images2.imgbox.com/79/67/LObwANKR_o.png" width="417"></p> 
<p>第16名，六百多万的下载量，也很不错了。</p> 
<h2 id="3%20RoBERTa">3 RoBERTa</h2> 
<p>RoBERTa，Robustly optimized BERT approach</p> 
<p>我们的任务不应该仅仅是ALBERT，还有很多其他BERT的变形体。RoBERTa和ALBERT实际上是两条路，ALBERT实际上是对网络结构进行改变，RoBERTa是对训练过程进行优化。</p> 
<p><img alt="" height="323" src="https://images2.imgbox.com/7a/98/Xq2Gu2SN_o.png" width="443"></p> 
<p>这个排名也很高，第九名，接近一千万的下载量。</p> 
<h3 id="3.1%20%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83RoBERTa">3.1 如何训练RoBERTa</h3> 
<ul><li>基本就是说训练过程可以再优化优化</li><li>最核心的就是如何在语言模型中设计mask:</li><li>动态mask光听感觉肯定都比静态的要强，也就是这篇论文的核心</li><li>取消NSP任务（Next Sentence Prediction）后效果反而好</li></ul> 
<p>先想想BERT是怎么做的？</p> 
<p>拿到一个数据集，有n条数据，BERT会将第1条一直到第n条数据，全部随机选中一个位置mask掉。但是哪个地方应该被mask掉，这个事情是一个确定的事情，后续无法改变的，这个是原始的BERT的方法。在深度学习中，一定会强调一个叫做epoch的东西，深度学习会将数据不断的进行迭代，迭代epochs次。在原始BERT中，对于n个句子中，在每一个epoch的迭代中，被mask的位置都是固定的。</p> 
<p>比如：“珠穆朗玛峰是世界第一高峰”，第一次把“珠穆朗玛峰”给mask掉，让BERT猜谁是世界第一高峰，往后的第二次、第三次都是这么做。</p> 
<p>但是我们能不能不做这么做呢？第一次我把“珠穆朗玛峰”给mask掉，第二次我把第一的一给mask掉可以吗？这样模型学到的东西是不是就更多了。</p> 
<p>这这种做法，就是叫做动态的mask。</p> 
<p><img alt="" height="195" src="https://images2.imgbox.com/d5/fe/kZVAoh0C_o.png" width="462"></p> 
<p>在回答深度学习训练方面的问题的时候，我经常回答把batch_size改成1试试。为什么呢，因为普遍大家的机器都没有几个说很舍得买3090,4090的，毕竟花一两万就买个显卡，确实还是没那么舍得。</p> 
<p>所以batch_size肯定是越大越好，因为这样能够更快训练完一个数据集，更好的拟合到数据的概率分布。</p> 
<p>所以提出RoBERTa的论文实际上就做了一个动态mask，然后做了一些实验。</p> 
<p>看一看RoBERTa优化的那些地方：</p> 
<ul><li>BatchSize基本也是大家公认的：</li><li>用了更多的数据集，训练了更久，提升了一点效果</li><li>分词方式做了一点改进，让英文拆的更细致（与中文无关）</li></ul> 
<p><img alt="" height="169" src="https://images2.imgbox.com/0e/af/rI2HZWOK_o.png" width="469"></p> 
<p>看看人家的batch_size吧，我们自己玩不了这个东西。后面再看看gpt大模型，何止是以k为单位，简直是以w和m为单位。 </p> 
<h3 id="3.2RoBERTa-wwm">3.2RoBERTa-wwm</h3> 
<p>wwm就是 word whole mask，全词掩码，这里用中文版本的介绍一下（ 就是哈工大和科大讯飞的<a class="link-info" href="https://huggingface.co/hfl/chinese-bert-wwm-ext" rel="nofollow" title="中文分词模型">中文分词模型</a>）</p> 
<p>比如这句话：我喜欢吃哈X滨正宗烤冷面，最早来说就是把中文的一个字给mask掉，但是这个时候模型学的是什么哈什么滨，跟正宗烤冷面是没有关系的，不仅跟烤冷面没关系，跟其他任务都没有关系，因为哈什么滨中国就一个哈尔滨。</p> 
<p>所以说以前用字来mask带了一些问题，全词掩码，那就应该把这一整个词全遮挡住让BERT来猜：</p> 
<p>我喜欢吃XXX正宗烤冷面</p> 
<p>所以这个时候，模型思考的时候，正宗烤冷面在全国哪里最多呢？然后分析得出了哈尔滨。</p> 
<p>这就是中文版的RoBERTa-wwm分词方式</p> 
<p><img alt="" height="230" src="https://images2.imgbox.com/e2/3c/VzeLZbPm_o.png" width="1033"></p> 
<h2 id="4%C2%A0DistilBERT">4 DistilBERT</h2> 
<p><img alt="" height="230" src="https://images2.imgbox.com/9b/14/jSxJoZjs_o.png" width="504"></p> 
<p> 就叫它老四吧，老四觉得BERT模型太大了，在训练的时候不去改BERT的网络结构，但是训练完了之后，能不能给它拆了，让它变得更小呢？这件事情就叫做蒸馏。</p> 
<p>什么叫做蒸馏？模型实现效果会有一些核心支撑点，但是模型会在核心支撑额外加一些东西，这些额外的东西在训练的时候需要，在部署和测试的时候可能并不需要。所以蒸馏就是在一个大模型的基础上进行拆分，把一些不重要没必要的东西拆除的过程，就是蒸馏。</p> 
<p>为什么一定要做这个？NLP的参数是呈指数上升的趋势，是很恐怖的，CV就没有这么恐怖。因为NLP效果提升都不是在改网络结构，是把模型做的更大。因为都是基于Transformer去做的。所以学术上就一点，大力出奇迹。</p> 
<p>但是工程上怎么办？还是得面向实际落地啊！所以还是需要想办法做的小一些。</p> 
<p>DistilBERT的实验中去掉了40%的参数，但是效果依然能保持在97% ：</p> 
<p><img alt="" height="118" src="https://images2.imgbox.com/62/b7/iP1KwLkR_o.png" width="497"></p> 
<p>  distil蒸馏，在Hugging Face上很多模型都出现了蒸馏版本：</p> 
<p><img alt="" height="301" src="https://images2.imgbox.com/c3/d4/s9feiMuM_o.png" width="428"></p> 
<p>到今天为止的NLP任务中，主流的模型就是BERT和GPT两个方向。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/872a8e8663d9acfd7ea125ee46e96c35/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">docker中配置mysql主从分离</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a63b48323e670721360eecc092e744a4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">简单介绍GMP调度器模型</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>