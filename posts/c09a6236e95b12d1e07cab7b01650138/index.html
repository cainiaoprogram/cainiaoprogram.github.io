<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Machine Learning（study notes） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Machine Learning（study notes）" />
<meta property="og:description" content="There is no studying without going crazy
Studying alwats drives us crazy
course from 吴恩达机器学习系列课程
文章目录 DefineMachine LearningSupervised Learning（监督学习）Regression problemClassidication Unspervised LearningClustering StudyModel representation（模型概述）const functionHow to use and tsolve problem gradient descentThe summary to gradient descentGradient descent for linear regression Matrices and vectors（basic knowledge）Addition and scalar multiplicationMatrix-vector multiplicationMatrix-matrix multiplicationMatrix multiplication propertiesIdentity Matrix Inverse and transpose Multiple featuresGradient descent for multiple variablesGradient descent in practice（多元梯度下降法）I : Feature Scaling(1：特征缩放)II : Learning rate Features and polynomial regressionNormal equation Define Machine Learning A computer program is said to learn from experience E with respect to some task T and some performance measure P , if its performance on T, as measued" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/c09a6236e95b12d1e07cab7b01650138/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-20T16:48:23+08:00" />
<meta property="article:modified_time" content="2023-12-20T16:48:23+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Machine Learning（study notes）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><strong>There is no studying without going crazy</strong></p> 
<p><strong>Studying alwats drives us crazy</strong></p> 
<p>course from <a rel="nofollow">吴恩达机器学习系列课程</a><br> </p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#Define_6" rel="nofollow">Define</a></li><li><ul><li><a href="#Machine_Learning_7" rel="nofollow">Machine Learning</a></li><li><a href="#Supervised_Learning_14" rel="nofollow">Supervised Learning（监督学习）</a></li><li><ul><li><a href="#Regression_problem_16" rel="nofollow">Regression problem</a></li><li><a href="#Classidication_24" rel="nofollow">Classidication</a></li></ul> 
   </li><li><a href="#Unspervised_Learning_32" rel="nofollow">Unspervised Learning</a></li><li><ul><li><a href="#Clustering_34" rel="nofollow">Clustering</a></li></ul> 
  </li></ul> 
  </li><li><a href="#Study_45" rel="nofollow">Study</a></li><li><ul><li><a href="#Model_representation_47" rel="nofollow">Model representation（模型概述）</a></li><li><ul><li><a href="#const_function_63" rel="nofollow">const function</a></li><li><a href="#How_to_use_and_t_72" rel="nofollow">How to use and t</a></li><li><a href="#solve_problem_80" rel="nofollow">solve problem</a></li></ul> 
   </li><li><a href="#gradient_descent_93" rel="nofollow">gradient descent</a></li><li><ul><li><a href="#The_summary_to_gradient_descent_108" rel="nofollow">The summary to gradient descent</a></li><li><a href="#Gradient_descent_for_linear_regression_130" rel="nofollow">Gradient descent for linear regression</a></li></ul> 
   </li><li><a href="#Matrices_and_vectorsbasic_knowledge_140" rel="nofollow">Matrices and vectors（basic knowledge）</a></li><li><ul><li><a href="#Addition_and_scalar_multiplication_154" rel="nofollow">Addition and scalar multiplication</a></li><li><a href="#Matrixvector_multiplication_162" rel="nofollow">Matrix-vector multiplication</a></li><li><a href="#Matrixmatrix_multiplication_166" rel="nofollow">Matrix-matrix multiplication</a></li><li><a href="#Matrix_multiplication_properties_169" rel="nofollow">Matrix multiplication properties</a></li><li><ul><li><a href="#Identity_Matrix_173" rel="nofollow">Identity Matrix</a></li></ul> 
    </li><li><a href="#Inverse_and_transpose_178" rel="nofollow">Inverse and transpose</a></li></ul> 
   </li><li><a href="#Multiple_features_184" rel="nofollow">Multiple features</a></li><li><ul><li><a href="#Gradient_descent_for_multiple_variables_194" rel="nofollow">Gradient descent for multiple variables</a></li><li><a href="#Gradient_descent_in_practice_198" rel="nofollow">Gradient descent in practice（多元梯度下降法）</a></li><li><ul><li><a href="#I__Feature_Scaling1_200" rel="nofollow">I : Feature Scaling(1：特征缩放)</a></li><li><a href="#II__Learning_rate_210" rel="nofollow">II : Learning rate</a></li></ul> 
    </li><li><a href="#Features_and_polynomial_regression_221" rel="nofollow">Features and polynomial regression</a></li><li><a href="#Normal_equation_230" rel="nofollow">Normal equation</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="Define_6"></a>Define</h2> 
<h3><a id="Machine_Learning_7"></a>Machine Learning</h3> 
<p>A computer program is said to learn from experience E with respect to some task T and some performance measure P , if its performance on T, as measued</p> 
<blockquote> 
 <p>计算机程序从经验E中学习，解决某一任务T进行某一性能度量P，通过P测定在T上的表现因经验E而提高<br> realy rhyme</p> 
</blockquote> 
<h3><a id="Supervised_Learning_14"></a>Supervised Learning（监督学习）</h3> 
<p>right answers given</p> 
<h4><a id="Regression_problem_16"></a>Regression problem</h4> 
<p>tring to predict continuios valued ouput</p> 
<blockquote> 
 <p>需要预测连续的数值输出</p> 
</blockquote> 
<p>in that problem , you should give its some right valued with different classic and machine learning will learn to predict it</p> 
<p><img src="https://images2.imgbox.com/b8/55/Tc1Up0Rq_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="Classidication_24"></a>Classidication</h4> 
<p>discrete valued output (zero or one)</p> 
<blockquote> 
 <p>离散取值输出</p> 
</blockquote> 
<p>in that problem, you should give some valued . Different with regression , maybe the type of data<br> <img src="https://images2.imgbox.com/5c/9e/UuTJVz5j_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="Unspervised_Learning_32"></a>Unspervised Learning</h3> 
<h4><a id="Clustering_34"></a>Clustering</h4> 
<p>maybe using clustring algorithm to break that data into two separate clusters</p> 
<blockquote> 
 <p>使用聚类算法将数据分为两簇</p> 
</blockquote> 
<p>do not know what data mean and data features and so on(just about data information),and you know ,machine learning should classification those data into different clusters</p> 
<p>the classic problem of that maybe Cocktail party problem algorithm</p> 
<blockquote> 
 <p>经典问题就是鸡尾酒派对算法，就是有背景音乐以及人声，能分理分离出两者的声音</p> 
</blockquote> 
<h2><a id="Study_45"></a>Study</h2> 
<h3><a id="Model_representation_47"></a>Model representation（模型概述）</h3> 
<p>using this example<br> <img src="https://images2.imgbox.com/f7/75/DjQdUxJV_o.png" alt="在这里插入图片描述"></p> 
<p>And we will give some training set<br> <img src="https://images2.imgbox.com/fd/19/7RT4bfuc_o.png" alt="在这里插入图片描述"><br> As you can see , we put<br> m as number of training examples,<br> x’s as “input” variable / features ,<br> y’s as “output” variable / “target” variable ,<br> (x,y) as one training example ,<br> (x<sup>(i)</sup>,y<sup>(i)</sup>) refer to the i<sub>th</sub> training example.(this superscript i over here , this is not exponentiation.The superscript i in parenthess that’s just an index into my training set)</p> 
<p><img src="https://images2.imgbox.com/63/cf/0YiCkpAz_o.png" alt="在这里插入图片描述"><br> We saw that with the training set like our training set of housing prices and we feed that to our learning algorithm.Is the job of a learning algorithm to then output a function which by convention is usually denoted lowercase h</p> 
<h4><a id="const_function_63"></a>const function</h4> 
<p><img src="https://images2.imgbox.com/64/44/L1b3rhwQ_o.png" alt="在这里插入图片描述"><br> In this chart , we want the difference between h(x) and y to be small .And one thing I’m gonna do is try to minimize the square difference between the output of the hypothesis and the actual price of the house.</p> 
<p><img src="https://images2.imgbox.com/b7/35/rrWf89uJ_o.png" alt="在这里插入图片描述"><br> What we want to do is minimize over theta zero and theta one my function J of theta zero comma theta one</p> 
<p>error cost function is probably the most commonly used one for regeression problem</p> 
<h4><a id="How_to_use_and_t_72"></a>How to use and t</h4> 
<p><img src="https://images2.imgbox.com/d0/c5/tcaTkKhC_o.png" alt="在这里插入图片描述"><br> Here is something we will use.</p> 
<p>In order to figure out how it use, we think of theta zero as setting the parameter theta zero equal to 0.So we have only one parameter theta one.</p> 
<p><img src="https://images2.imgbox.com/98/ee/POzo3r5r_o.png" alt="在这里插入图片描述"><br> In the left , the line we fit and the theta we chose will mapping to the chart in right</p> 
<h4><a id="solve_problem_80"></a>solve problem</h4> 
<p><img src="https://images2.imgbox.com/84/7f/ofV4mdN8_o.png" alt="在这里插入图片描述"><br> Here is our problem formulation as usual with the hypothesis ,parameters, cost function ,and our optimization objective</p> 
<p><img src="https://images2.imgbox.com/d6/e0/o5WaGSbu_o.png" alt="在这里插入图片描述"><br> Using that function, we will finally get that plot</p> 
<p><img src="https://images2.imgbox.com/39/b3/OvXoz2sz_o.png" alt="在这里插入图片描述"><br> Here is an example of a contour figure</p> 
<h3><a id="gradient_descent_93"></a>gradient descent</h3> 
<p>It turns out gradient descent is a more general algorithm, and is used not only in linear regression. It’s actually used all over the place in machine learning.</p> 
<p><img src="https://images2.imgbox.com/90/93/oTKUv6Jr_o.png" alt="在这里插入图片描述"><br> Here is the problem setup.<br> We are going to see that we have some function J of (θ<sub>0</sub>,θ<sub>1</sub>). Maybe it is a cost function from linear regression.And we want to come up with an algorithm for minimizing that as a function of J of (θ<sub>0</sub>,θ<sub>1</sub>).</p> 
<p>For the sake of brevity , for the sake of your succinctness of notation , so we just goingn to pretend that have only two parameters through the rest of this video.</p> 
<hr> 
<p><strong>The idea for gradient descent :</strong></p> 
<p>What we’re going to do is we are going to strat off with some initial guesses for θ<sub>0</sub> and θ<sub>1</sub>.<br> What we are going to do in gradient descent is we’ll keep changing θ<sub>0</sub> and θ<sub>1</sub> a little bit to try to reduce J of (θ<sub>0</sub>,θ<sub>1</sub>)</p> 
<h4><a id="The_summary_to_gradient_descent_108"></a>The summary to gradient descent</h4> 
<p><img src="https://images2.imgbox.com/c7/e2/RGbw066w_o.png" alt="在这里插入图片描述"><br> Here is the gradient descent algorithm that we saw last time.</p> 
<p>In order to convey these intutions, we want to do is use a slightly simpler example where we want to minimize the function of just one parameter.<br> So we have a cost function J of just one parameter ,theta one.</p> 
<p><img src="https://images2.imgbox.com/45/77/wdYQWxCz_o.png" alt="在这里插入图片描述"></p> 
<p>As we choose the red point ,we will through the funtion((d/dθ<sub>1</sub>)J(θ<sub>1</sub>)).<br> Through the function , we can get the tangent of it .<br> And it is absoultly a postive number . So we will get θ<sub>1</sub> = θ<sub>1</sub> - α*(positive number).α , the learning rate is always a positive number. So θ<sub>1</sub> is decrease.<br> And it is actually right, the direction of theta move get me closer to the minimum</p> 
<p>Similarly, when a point is selected on the left, it will eventually move to the right</p> 
<p><img src="https://images2.imgbox.com/92/f5/wDLdqgRQ_o.png" alt="在这里插入图片描述"><br> let’s suppose you initialize theta one at a local minimum.And it is already at a local optimum or the local minimum.It turns out that at local optimum your derivative would be equal to zero.So, in your gradient descent update, you have theta one , gives update that theta one ,minus alpha times zero.<br> what this means is that ,if you are already at a local optimum , it leaves theta one unchanged cause this.</p> 
<p><img src="https://images2.imgbox.com/09/e9/X1jYSdCh_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="Gradient_descent_for_linear_regression_130"></a>Gradient descent for linear regression</h4> 
<p><img src="https://images2.imgbox.com/60/93/fGxkOdmI_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/ae/27/DPgs7qr8_o.png" alt="在这里插入图片描述"><br> Here is gradient descent for the regression ,which is going to repeat until convergence<br> <img src="https://images2.imgbox.com/df/77/xzqv1zNP_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/42/4d/BFZYxQQL_o.png" alt="在这里插入图片描述"><br> This kind of algorithm is sometimes called batch gradient descent</p> 
<h3><a id="Matrices_and_vectorsbasic_knowledge_140"></a>Matrices and vectors（basic knowledge）</h3> 
<p>Firstly, we learn what is matrices<br> <img src="https://images2.imgbox.com/b0/a1/sVtCvkJm_o.png" alt="在这里插入图片描述"><br> Next , let us talk about how to refer to sppecific elements of the matrix</p> 
<p><img src="https://images2.imgbox.com/0b/29/Noksn64u_o.png" alt="在这里插入图片描述"><br> What is vector?<br> A vector turns out to be a special case of a matrix.<br> A vector is a matrix that has only 1 column</p> 
<p><img src="https://images2.imgbox.com/c2/52/sqFlgPrI_o.png" alt="在这里插入图片描述"><br> about index:<br> in the matchine, the index is from zero .So ,while we use, zero_index vector is a more convenient notation.</p> 
<h4><a id="Addition_and_scalar_multiplication_154"></a>Addition and scalar multiplication</h4> 
<p><img src="https://images2.imgbox.com/3f/88/pFgiQHzs_o.png" alt="在这里插入图片描述"><br> It turns out you can add only two matrices that are of the same dimensions</p> 
<p><img src="https://images2.imgbox.com/1c/d9/e4qprmBE_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/e6/2f/N6tInZu0_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="Matrixvector_multiplication_162"></a>Matrix-vector multiplication</h4> 
<p><img src="https://images2.imgbox.com/f5/ef/iFqOlRBV_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/0a/23/gE95vEqQ_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="Matrixmatrix_multiplication_166"></a>Matrix-matrix multiplication</h4> 
<p><img src="https://images2.imgbox.com/4b/3d/w6jKOpZG_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/e6/17/47Zt17NG_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="Matrix_multiplication_properties_169"></a>Matrix multiplication properties</h4> 
<p><img src="https://images2.imgbox.com/86/f9/1P5x3XKs_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/3c/26/DStICth6_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="Identity_Matrix_173"></a>Identity Matrix</h5> 
<p>The indentity matrix has the property that it has ones along the diagonals and is zero everywhere else.</p> 
<p><img src="https://images2.imgbox.com/af/7d/5LGhQYJr_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="Inverse_and_transpose_178"></a>Inverse and transpose</h4> 
<p><img src="https://images2.imgbox.com/5a/a8/HQEWz8ep_o.png" alt="在这里插入图片描述"><br> It turns out only square matrices have inverses<br> Matrices that don’t have an inverse are “singular” or “degenerate”</p> 
<p><img src="https://images2.imgbox.com/56/7f/8vO9mfPw_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="Multiple_features_184"></a>Multiple features</h3> 
<p><img src="https://images2.imgbox.com/e1/72/wtT6XiWQ_o.png" alt="在这里插入图片描述"><br> We have a single feature x, the size of the hourse , and we wanted to use that to predict y the price of the house and the function h<sub>θ</sub> was our form of our hypothesis.<br> But now imagine , what if we had not only the size of the house as a feature or as a variable with which to try to predict the price, but that we also knew the number of bedrooms, the number of floors, and the age of home in years.It seems like this would give us a lot of information with which to predict the price.</p> 
<p><img src="https://images2.imgbox.com/12/78/ZLA7ubJa_o.png" alt="在这里插入图片描述"><br> if we have N features then rather than summing up over our four features, we would have a sum over our N features.<br> <img src="https://images2.imgbox.com/8b/e4/tPwjnWVm_o.png" alt="在这里插入图片描述"><br> In order to simplify the function, we add x<sub>0</sub> = 1,and the finaly function express like that<br> <img src="https://images2.imgbox.com/65/57/yTiAoAtm_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="Gradient_descent_for_multiple_variables_194"></a>Gradient descent for multiple variables</h4> 
<p><img src="https://images2.imgbox.com/9f/07/uJsVow4R_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ee/ed/zn0RzTTS_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="Gradient_descent_in_practice_198"></a>Gradient descent in practice（多元梯度下降法）</h4> 
<h5><a id="I__Feature_Scaling1_200"></a>I : Feature Scaling(1：特征缩放)</h5> 
<p>We are going to repeatedly update each parameter θ<sub>j</sub> according to θ<sub>j</sub> minus α times this derivative tern.<br> A useful thing to do is to scale the features. Concretely, if you instead define the feature X<sub>1</sub> to be the size of the house divided by 2000, and define X<sub>2</sub> to be maybe the number of bedrooms dividied by five, then thw contours of the cost function J can become much less skewed, so the contours may look more like circles.And if you turn gradient descent on a cost function like this, then gradient descent, you can show mathematically, can find a much more direct path to the global minimum, rather than taking a much more convoluted path.</p> 
<p><img src="https://images2.imgbox.com/c1/96/GPGdgm4l_o.png" alt="在这里插入图片描述"><br> And the feature range should in -3 to 3,too large and too small is not allowed<br> <img src="https://images2.imgbox.com/12/86/hNo9j97O_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/66/5a/SEfzXz5R_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="II__Learning_rate_210"></a>II : Learning rate</h5> 
<p>The target to learn learning rate is make sure that gradient descent is working correctly.<br> <img src="https://images2.imgbox.com/2e/5c/DKwa5mrq_o.png" alt="在这里插入图片描述"><br> What this plot is showing , is it’s showing the value of your cost function after each iteration of gradient descent.And ,if gradient descent is working properly, then J of theta should decrease after every iteration.</p> 
<p><img src="https://images2.imgbox.com/3a/0d/HOjaliFB_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/0e/27/WIpHCtza_o.png" alt="在这里插入图片描述"><br> And the summary of alpha choose is that:</p> 
<p><img src="https://images2.imgbox.com/fd/ed/o0Pw8n5V_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="Features_and_polynomial_regression_221"></a>Features and polynomial regression</h4> 
<p>Using sold house example</p> 
<p>We have twi features called frontage and depth.You might build a linear regression model like this<br> <img src="https://images2.imgbox.com/f9/42/XLWgW7VA_o.png" alt="在这里插入图片描述"><br> where frontage is your first feature x_1 and depth is your second feature x_2, but when you are applying linear regression, you do not necessarily have to use just the features x_1 and x_2 that you are given. What you can do is actually create new features by yourself. So ,if I want to predict the price of a house, what I might do instead is decide that what really determines the size of the house is the area or the land area that I own.So, I might create a new feature.I’m just gonna call this feature x, which is frontage, times depth.This is a multiplication symbol.It’s a frontage times depth because this is the land area that I own and I might then select my hypothesis as that usingjust one feature which is my land area. Because the area of a rectangle is the product of the lengths of the sides. So, depending on what insight you might have into a particular problem ,rather than just taking the features frontage and depth taht we happen to have started off with, sometimes by defining new features you might actually get a better model.</p> 
<p>Closely related to the idea of choosing your features is this idea called polynomial regression.</p> 
<h4><a id="Normal_equation_230"></a>Normal equation</h4> 
<p>Its essence is actually to take the partial derivative and make it zero, so that its minimum value can be determined。What‘s more ,it just went from univariate to multivariate evidence</p> 
<p><img src="https://images2.imgbox.com/6f/6c/AaSEwCiE_o.png" alt="在这里插入图片描述"><br> why should we set x_0, it just a const ,like ax+b=0,and x_0 is the b.<br> <img src="https://images2.imgbox.com/a8/0f/T9CWLS7k_o.png" alt="在这里插入图片描述"><br> There is the change of feature x.Its form becomes a matrix。<br> <img src="https://images2.imgbox.com/9b/d5/yCph64qR_o.png" alt="在这里插入图片描述"><br> And this equation will calculate the minimum value we want.(Although I do not know this implementation principle)</p> 
<p>what the advantage of this normal equation is you can take any value you want .You do not think about the range of value.(can do not use feature scaling）<br> <img src="https://images2.imgbox.com/c8/1b/Pm88OJnX_o.png" alt="在这里插入图片描述"><br> Then ， when should you choose gradient descent and when should you choose normal equation , here is some advice.<br> <img src="https://images2.imgbox.com/4d/6a/QD6HTbdV_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/0b6f5f91389148b522951d527c9adda6/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">LLaVA和LLaVA-Plus视觉指令微调及工具使用构建多模态智能体</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6b5e2f1db0ca7c0712a142c83fb1cba1/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【大数据实训】python石油大数据可视化(八)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>