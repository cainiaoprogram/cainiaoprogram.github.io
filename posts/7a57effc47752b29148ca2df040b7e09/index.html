<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>siamfc代码解读_SiamFC用于目标跟踪的全卷积孪生网络 fully-convolutional siame - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="siamfc代码解读_SiamFC用于目标跟踪的全卷积孪生网络 fully-convolutional siame" />
<meta property="og:description" content="SiamFC用于目标跟踪的全卷积孪生网络 fully-convolutional siame
SiamFC：用于目标跟踪的全卷积孪生网络 fully-convolutional siamese networks for object tracking
SiamFC网络
图中z代表的是模板图像，算法中使用的是第一帧的ground truth；x代表的是search region，代表在后面的待跟踪帧中的候选框搜索区域；?代表的是一种特征映射操作，将原始图像映射到特定的特征空间，文中采用的是CNN中的卷积层和pooling层；6×6×128代表z经过?后得到的特征，是一个128通道6×6大小feature，同理，22×22×128是x经过?后的特征；后面的×代表卷积操作，让22×22×128的feature被6×6×128的卷积核卷积，得到一个17×17的score map，代表着搜索区域中各个位置与模板相似度值。
算法本身是比较搜索区域与目标模板的相似度，最后得到搜索去区域的score map。其实从原理上来说，这种方法和相关性滤波的方法很相似。其在搜索区域中逐点的目标模板进行匹配，将这种逐点平移匹配计算相似度的方法看成是一种卷积，然后在卷积结果中找到相似度值最大的点，作为新的目标的中心。
上图所画的?其实是CNN中的一部分，并且两个?的网络结构是一样的，这是一种典型的孪生神经网络，并且在整个模型中只有conv层和pooling层，因此这也是一种典型的全卷积(fully-convolutional)神经网络。
在训练模型的时肯定需要损失函数，并通过最小化损失函数来获取最优模型。本文算法为了构造有效的损失函数，对搜索区域的位置点进行了正负样本的区分，即目标一定范围内的点作为正样本，这个范围外的点作为负样本，例如图1中最右侧生成的score map中，红色点即正样本，蓝色点为负样本，他们都对应于search region中的红色矩形区域和蓝色矩形区域。文章采用的是logistic loss，具体的损失函数形式如下：
对于score map中了每个点的损失：
其中v是score map中每个点真实值，y∈{&#43;1,?1}是这个点所对应的标签。
上面的是score map中每个点的loss值，而对于score map整体的loss，则采用的是全部点的loss的均值。即：
这里的u∈D代表score map中的位置。
整个网络结构类似与AlexNet，但是没有最后的全连接层，只有前面的卷积层和pooling层。
整个网络结构入上表，其中pooling层采用的是max-pooling，每个卷积层后面都有一个ReLU非线性激活层，但是第五层没有。另外，在训练的时候，每个ReLU层前都使用了batch normalization(批规范化是深度学习中经常见到的一种训练方法，指在采用梯度下降法训练DNN时，对网络层中每个mini-batch的数据进行归一化，使其均值变为0，方差变为1，其主要作用是缓解DNN训练中的梯度消失/爆炸现象，加快模型的训练速度)，用于降低过拟合的风险。
AlexNet:
AlexNet为8层结构，其中前5层为卷积层，后面3层为全连接层；学习参数有6千万个，神经元有650,000个。AlexNet在两个GPU上运行；AlexNet在第2,4,5层均是前一层自己GPU内连接，第3层是与前面两层全连接，全连接是2个GPU全连接；
RPN层第1,2个卷积层后；Max pooling层在RPN层以及第5个卷积层后。ReLU在每个卷积层以及全连接层后。
卷积核大小数量：
conv1:96 11×11×3(个数/长/宽/深度)
conv2:256 5×5×48
conv3:384 3×3×256
conv4: 384 3×3×192
conv5: 256 3×3×192
ReLU、双GPU运算：提高训练速度。(应用于所有卷积层和全连接层)
重叠pool池化层：提高精度，不容易产生过度拟合。(应用在第一层，第二层，第五层后面)
局部响应归一化层(LRN)：提高精度。(应用在第一层和第二层后面)
Dropout：减少过度拟合。(应用在前两个全连接层)
微调(fine-tune)
看到别人一个很好的模型，虽然针对的具体问题不一样，但是也想试试看，看能不能得到很好的效果，而且自己的数据也不多，怎么办？没关系，把别人现成的训练好了的模型拿过来，换成自己的数据，调整一下参数，在训练一遍，这就是微调(fine-tune)。
冻结预训练模型的部分卷积层(通常是靠近输入的多数卷积层)，训练剩下的卷积层(通常是靠近输出的部分卷积层)和全连接层。从某意义上来说，微调应该是迁移学习中的一部分。
感知机：PLA
多层感知机是由感知机推广而来，感知机学习算法(PLA: Perceptron Learning Algorithm)用神经元的结构进行描述的话就是一个单独的。
感知机的神经网络表示如下：
多层感知机：MLP
多层感知机的一个重要特点就是多层，我们将第一层称之为输入层，最后一层称之有输出层，中间的层称之为隐层。MLP并没有规定隐层的数量，因此可以根据各自的需求选择合适的隐层层数。且对于输出层神经元的个数也没有限制。
MLP神经网络结构模型如下,本文中只涉及了一个隐层，输入只有三个变量[x1,x2,x3]和一个偏置量b，输出层有三个神经元。相比于感知机算法中的神经元模型对其进行了集成。
ReLU函数：
sigmod函数：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/7a57effc47752b29148ca2df040b7e09/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-12-21T17:47:04+08:00" />
<meta property="article:modified_time" content="2020-12-21T17:47:04+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">siamfc代码解读_SiamFC用于目标跟踪的全卷积孪生网络 fully-convolutional siame</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div style="font-size:16px;"> 
 <p>SiamFC用于目标跟踪的全卷积孪生网络 fully-convolutional siame</p> 
 <p>SiamFC：用于目标跟踪的全卷积孪生网络 fully-convolutional siamese networks for object tracking</p> 
 <p>SiamFC网络</p> 
 <p>图中z代表的是模板图像，算法中使用的是第一帧的ground truth；x代表的是search region，代表在后面的待跟踪帧中的候选框搜索区域；?代表的是一种特征映射操作，将原始图像映射到特定的特征空间，文中采用的是CNN中的卷积层和pooling层；6×6×128代表z经过?后得到的特征，是一个128通道6×6大小feature，同理，22×22×128是x经过?后的特征；后面的×代表卷积操作，让22×22×128的feature被6×6×128的卷积核卷积，得到一个17×17的score map，代表着搜索区域中各个位置与模板相似度值。</p> 
 <p>算法本身是比较搜索区域与目标模板的相似度，最后得到搜索去区域的score map。其实从原理上来说，这种方法和相关性滤波的方法很相似。其在搜索区域中逐点的目标模板进行匹配，将这种逐点平移匹配计算相似度的方法看成是一种卷积，然后在卷积结果中找到相似度值最大的点，作为新的目标的中心。</p> 
 <p>上图所画的?其实是CNN中的一部分，并且两个?的网络结构是一样的，这是一种典型的孪生神经网络，并且在整个模型中只有conv层和pooling层，因此这也是一种典型的全卷积(fully-convolutional)神经网络。</p> 
 <p>在训练模型的时肯定需要损失函数，并通过最小化损失函数来获取最优模型。本文算法为了构造有效的损失函数，对搜索区域的位置点进行了正负样本的区分，即目标一定范围内的点作为正样本，这个范围外的点作为负样本，例如图1中最右侧生成的score map中，红色点即正样本，蓝色点为负样本，他们都对应于search region中的红色矩形区域和蓝色矩形区域。文章采用的是logistic loss，具体的损失函数形式如下：</p> 
 <p>对于score map中了每个点的损失：</p> 
 <p>其中v是score map中每个点真实值，y∈{+1,?1}是这个点所对应的标签。</p> 
 <p>上面的是score map中每个点的loss值，而对于score map整体的loss，则采用的是全部点的loss的均值。即：</p> 
 <p>这里的u∈D代表score map中的位置。</p> 
 <p>整个网络结构类似与AlexNet，但是没有最后的全连接层，只有前面的卷积层和pooling层。</p> 
 <p>整个网络结构入上表，其中pooling层采用的是max-pooling，每个卷积层后面都有一个ReLU非线性激活层，但是第五层没有。另外，在训练的时候，每个ReLU层前都使用了batch normalization(批规范化是深度学习中经常见到的一种训练方法，指在采用梯度下降法训练DNN时，对网络层中每个mini-batch的数据进行归一化，使其均值变为0，方差变为1，其主要作用是缓解DNN训练中的梯度消失/爆炸现象，加快模型的训练速度)，用于降低过拟合的风险。</p> 
 <p>AlexNet:</p> 
 <p>AlexNet为8层结构，其中前5层为卷积层，后面3层为全连接层；学习参数有6千万个，神经元有650,000个。AlexNet在两个GPU上运行；AlexNet在第2,4,5层均是前一层自己GPU内连接，第3层是与前面两层全连接，全连接是2个GPU全连接；</p> 
 <p>RPN层第1,2个卷积层后；Max pooling层在RPN层以及第5个卷积层后。ReLU在每个卷积层以及全连接层后。</p> 
 <p>卷积核大小数量：</p> 
 <p>conv1:96 11×11×3(个数/长/宽/深度)</p> 
 <p>conv2:256 5×5×48</p> 
 <p>conv3:384 3×3×256</p> 
 <p>conv4: 384 3×3×192</p> 
 <p>conv5: 256 3×3×192</p> 
 <p>ReLU、双GPU运算：提高训练速度。(应用于所有卷积层和全连接层)</p> 
 <p>重叠pool池化层：提高精度，不容易产生过度拟合。(应用在第一层，第二层，第五层后面)</p> 
 <p>局部响应归一化层(LRN)：提高精度。(应用在第一层和第二层后面)</p> 
 <p>Dropout：减少过度拟合。(应用在前两个全连接层)</p> 
 <p>微调(fine-tune)</p> 
 <p>看到别人一个很好的模型，虽然针对的具体问题不一样，但是也想试试看，看能不能得到很好的效果，而且自己的数据也不多，怎么办？没关系，把别人现成的训练好了的模型拿过来，换成自己的数据，调整一下参数，在训练一遍，这就是微调(fine-tune)。</p> 
 <p>冻结预训练模型的部分卷积层(通常是靠近输入的多数卷积层)，训练剩下的卷积层(通常是靠近输出的部分卷积层)和全连接层。从某意义上来说，微调应该是迁移学习中的一部分。</p> 
 <p>感知机：PLA</p> 
 <p>多层感知机是由感知机推广而来，感知机学习算法(PLA: Perceptron Learning Algorithm)用神经元的结构进行描述的话就是一个单独的。</p> 
 <p>感知机的神经网络表示如下：</p> 
 <p>多层感知机：MLP</p> 
 <p>多层感知机的一个重要特点就是多层，我们将第一层称之为输入层，最后一层称之有输出层，中间的层称之为隐层。MLP并没有规定隐层的数量，因此可以根据各自的需求选择合适的隐层层数。且对于输出层神经元的个数也没有限制。</p> 
 <p>MLP神经网络结构模型如下,本文中只涉及了一个隐层，输入只有三个变量[x1,x2,x3]和一个偏置量b，输出层有三个神经元。相比于感知机算法中的神经元模型对其进行了集成。</p> 
 <p>ReLU函数：</p> 
 <p>sigmod函数：</p> 
 <p align="center"><img src="" alt=""></p> 
 <p>SiamFC用于目标跟踪的全卷积孪生网络 fully-convolutional siame相关教程</p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/55fcb47ed351ab40c45edbe917d27917/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Clipper库中文文档详解</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/67bb575160e29962dec1897335cf608c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">大小写 数据库 达梦_达梦常用语句</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>