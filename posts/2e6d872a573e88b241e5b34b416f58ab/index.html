<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>人脸识别损失函数之Center Loss - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="人脸识别损失函数之Center Loss" />
<meta property="og:description" content="人脸识别的难点在于:
1、不同类人脸类别之间的分类界限不明显；
2、人脸之间的相似度很高，人类也难以区分；
3、人脸的特征分类困难。
解决途径：
1、网络模型：
残差 深度可分类 稠密网络 densenet alexet inception net 等等。。
2、损失函数：
对于回归问题，常用的损失函数是均方误差（MSE，Mean Squared Error）。
对于分类问题，常用的损失函数为交叉熵（CE，Cross Entropy）。交叉熵一般与one-hot和softmax在一起使用。
改进多分类损失函数，对细微特征的能达到好的分类效果。
一、One-Hot
在分类问题中，one-hot编码是目标类别的表达方式。目标类别需要由文字标签，转换为one-hot编码的标签。one-hot向量，在目标类别的索引位置是1，在其他位置是0。类别的数量就是one-hot向量的维度。在one-hot编码中，假设类别变量之间相互独立。同时，在多分类问题中，one-hot与softmax组合使用。
import numpy as np def one_hot(arr): &#34;&#34;&#34; 概率矩阵转换为One-Hot矩阵 arr = np.array([[0.1, 0.5, 0.4], [0.2, 0.1, 0.6]]) :param arr: 概率矩阵 :return: One-Hot矩阵 &#34;&#34;&#34; arr_size = arr.shape[1] # 类别数 arr_max = np.argmax(arr, axis=1) # 最大值位置 oh_arr = np.eye(arr_size)[arr_max] # One-Hot矩阵 return oh_arr 二、Softmax
softmax使得神经网络的多个输出值的总和为1，softmax的输出值就是概率分布，应用于多分类问题。softmax也属于激活函数。softmax、one-hot和cross-entropy，一般组合使用。
import numpy as np def softmax(x): orig_shape=x." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/2e6d872a573e88b241e5b34b416f58ab/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-05-28T11:01:31+08:00" />
<meta property="article:modified_time" content="2020-05-28T11:01:31+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">人脸识别损失函数之Center Loss</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-github-gist">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>人脸识别的难点在于:<br> <strong>1、不同类人脸类别之间的分类界限不明显；<br> 2、人脸之间的相似度很高，人类也难以区分；<br> 3、人脸的特征分类困难。</strong><br> 解决途径：<br> 1、网络模型：<br> 残差 深度可分类 稠密网络 densenet alexet inception net 等等。。<br> 2、损失函数：<br> 对于回归问题，常用的损失函数是均方误差（MSE，Mean Squared Error）。<br> 对于分类问题，常用的损失函数为交叉熵（CE，Cross Entropy）。交叉熵一般与one-hot和softmax在一起使用。<br> 改进多分类损失函数，对细微特征的能达到好的分类效果。</p> 
<p><strong>一、One-Hot</strong></p> 
<p>在分类问题中，one-hot编码是目标类别的表达方式。目标类别需要由文字标签，转换为one-hot编码的标签。one-hot向量，在目标类别的索引位置是1，在其他位置是0。类别的数量就是one-hot向量的维度。在one-hot编码中，假设类别变量之间相互独立。同时，在多分类问题中，one-hot与softmax组合使用。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token keyword">def</span> <span class="token function">one_hot</span><span class="token punctuation">(</span>arr<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    概率矩阵转换为One-Hot矩阵
    arr = np.array([[0.1, 0.5, 0.4], [0.2, 0.1, 0.6]])
    :param arr: 概率矩阵
    :return: One-Hot矩阵
    """</span>
    arr_size <span class="token operator">=</span> arr<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>  <span class="token comment"># 类别数</span>
    arr_max <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>arr<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 最大值位置</span>
    oh_arr <span class="token operator">=</span> np<span class="token punctuation">.</span>eye<span class="token punctuation">(</span>arr_size<span class="token punctuation">)</span><span class="token punctuation">[</span>arr_max<span class="token punctuation">]</span>  <span class="token comment"># One-Hot矩阵</span>
    <span class="token keyword">return</span> oh_arr
</code></pre> 
<p><strong>二、Softmax</strong></p> 
<p>softmax使得神经网络的多个输出值的总和为1，softmax的输出值就是概率分布，应用于多分类问题。softmax也属于激活函数。softmax、one-hot和cross-entropy，一般组合使用。<br> <img src="https://images2.imgbox.com/3e/69/0TrRtVNz_o.png" alt="softmax"></p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">def</span> <span class="token function">softmax</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    orig_shape<span class="token operator">=</span>x<span class="token punctuation">.</span>shape
    <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token operator">&gt;</span><span class="token number">1</span><span class="token punctuation">:</span>
        <span class="token comment">#矩阵</span>
        tmp<span class="token operator">=</span>np<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span>axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 取最大值</span>
        x<span class="token operator">-=</span>tmp<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 减等最大值，防止指数爆炸</span>
        x<span class="token operator">=</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        tmp<span class="token operator">=</span>np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span>axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 重定义tmp为公式分母</span>
        x<span class="token operator">/=</span>tmp<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"matrix"</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token comment">#向量</span>
        tmp<span class="token operator">=</span>np<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x<span class="token operator">-=</span>tmp
        x<span class="token operator">=</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        tmp<span class="token operator">=</span>np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x<span class="token operator">/=</span>tmp
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"vector"</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> x

</code></pre> 
<p><strong>三、Cross-entropy</strong></p> 
<p>熵，热力学中表征物质状态的参量之一，用符号S表示，其物理意义是体系混乱程度的度量。香农提出了“信息熵”的概念，才解决了对信息的量化度量问题。<br> 给定两个概率分布：p（理想结果）和q（随机分布），则通过q来表示p的交叉熵为：<br> <img src="https://images2.imgbox.com/08/3a/iiGxYSMz_o.png" alt="交叉熵"><br> 交叉熵刻画的是通过概率分布q来表达概率分布p的困难程度，其中p是正确答案，q是预测值，也就是交叉熵值越小，两个概率分布越接近。这样我们可以用交叉熵来比较经过softmax输出和one-hot编码（标签）之间的距离，即模型的输出和真值，再对得到的损失进行优化。</p> 
<p><strong>四、Softmax Loss</strong></p> 
<p>多分类激活函数 softmax：将输入范围正负无穷，输出为：0~1的概率值，可以扩大相邻数值间的差距，所有概率的和为一。对softmax激活后的结果再使用交叉熵就是Softmax loss，使用Softmax loss可以刚好把人脸分开，但不能进行很好的应用。因为softmax经过了独热编码，标签为正时，yj=1，负yj=0。公式简化为：<br> <img src="https://images2.imgbox.com/bc/1a/aaTlQhah_o.png" alt="softmax loss"><br> <strong>softmax输出为0~1之间的值，求log之后：值域为负无穷到0，求负后为0到正无穷。当损失为0时，softmax输出值为1，刚好满足交叉熵的定义。</strong><br> 下图中，当<strong>Pj为softmax输出的（0~1）概率时，两者等价</strong>。<br> <img src="https://images2.imgbox.com/59/a6/7bGHLRls_o.png" alt="CE and softmax loss"><br> <img src="https://images2.imgbox.com/26/ba/OHfeBAeg_o.png" alt="在这里插入图片描述"><br> 在使用Softmax Loss对手写数字识别的分类时，可视化效果如下图。贴个<a href="http://proceedings.mlr.press/v48/liud16.pdf" rel="nofollow">论文地址。</a><br> <img src="https://images2.imgbox.com/0c/36/whlbofJr_o.png" alt="Mnist"><br> 对于可以看出的10个分类而言，我们希望分类效果好意味着，让每个类之间的距离变得更大，分的更开。而且：类的中心处没有分开，我们要在<strong>增加类间距的同时，还要减小类内距离</strong>，这里引入Center loss来配合原损失函数达到给每个类规划一个中心并使其在分类的同时，类内距离减小。注意：<strong>Center loss不可以单独去使用，要配合Softmax loss。</strong><br> 在这段发展过程中还有两个插曲：Siamese Network 、Triplet Loss。这里不展开讨论了。</p> 
<p><strong>五、Center Loss</strong><br> <a href="https://ydwen.github.io/papers/WenECCV16.pdf" rel="nofollow">论文链接：https://ydwen.github.io/papers/WenECCV16.pdf</a><br> 原文片段：<strong>增加类间距的同时，还要减小类内距离。</strong><br> <img src="https://images2.imgbox.com/7b/ba/DHkP7s3M_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/51/88/kLFWY2PK_o.png" alt="在这里插入图片描述"><br> <strong>类中心c:</strong></p> 
<p>每一个样本的特征需要通过一个好的网络到达特征层获得，这样计算完后所有样本的特征的平均值为类中心c，而好的网络需要是在有类中心加入的情况下才能得到。</p> 
<p><strong>优化过程：</strong></p> 
<p>每个类别的中心c无法直接获得，我们将其放到网络里根据输入的feature和label的形状自己生成，在每一个batch里更新center.即随机初始化center，而后每一个batch里计算当前数据与center的距离，而后将这个梯度形式的距离加到center上。类似于参数修正。同样的类似于梯度下降法,这里再增加一个<strong>scale度量α</strong>，使得center不会抖动。一般设置为0.5。可以视其为步长或者学习率。<br> <img src="https://images2.imgbox.com/ec/28/OUZVhIgF_o.png" alt="optimal"><br> 如何配合使用呢？看以下公式：<br> <img src="https://images2.imgbox.com/f1/e6/dTWeKToN_o.png" alt="loss"><br> 红色框是Softmax loss，蓝色框是Center loss，其中的<strong>λ</strong>是人为给定的平衡两个损失的权重，见下图中不同<strong>λ</strong>对应的分类效果有所不同。看在训练时更加侧重于哪个损失。在训练人脸数据集时，一般取值0.001，在训练数字10分类时一般给1或者2都行。<br> <img src="https://images2.imgbox.com/c2/66/RxYqS2P6_o.png" alt="Center loss"><br> 如何将Center loss应用到实际项目中呢？见下图中，我们需要将<strong>倒数第二层经过最大值池化的输出通过全连接层输出</strong>，这部分是用来做<strong>Center loss</strong>的，另外一部分，经过<strong>最后一层卷积输出的再经过全连接层输出</strong>，用来做<strong>Softmax loss</strong>。在提取人脸特征时，需要经过池化后输出的<strong>维度N大于128，因为这里维度越高，学习的信息也就越全面</strong>，在进行手写数字分类任务中，N=2，这里是为了可视化我们的分类效果。<br> <img src="https://images2.imgbox.com/36/9c/Lt6nHFbH_o.png" alt="use"><br> 综上所述，我们使用<strong>Softmax loss 来达到增大类间距离的目的，使用Center loss 来达到使类内距离减小的目的</strong>。下篇讲一下ArcSoftmax。<br> Center loss的缺点：<br> 1、因为他在使用时为每个类都要确定一个中心，相当于在正常分类的同时，增加了类别中心的计算和更新，对我们的<strong>硬件要求较高</strong>；<br> 2、因为正则化方式L2范数的平方问题，而Center loss又是用所有点对中心点进行计算后取均值，其中的离群点会对整体结果有较大影响，导致<strong>离群点不容易回归</strong>；<br> 3、Center loss不适合用于对类别差异较大数据分类，而用于<strong>单类多目标（同一类，类间相似，类外不同）的数据</strong>适用于人脸，手写数字的分类。<br> 手写数字10分类Center loss效果代码，目前Pytorch没有封装Center loss的函数需要自己写。第一部分代码是单独Center loss损失的设计测试，第二部分是使用两个优化器单独优化softmax loss和center loss，第三部分是训练。<br> 在这里补上center loss求解形状变换图：<br> <img src="https://images2.imgbox.com/f8/0f/z74YewWU_o.png" alt="形状图"></p> 
<p><strong>Part 1</strong></p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token keyword">def</span> <span class="token function">center_loss</span><span class="token punctuation">(</span>output<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>feature<span class="token punctuation">,</span> label<span class="token punctuation">,</span> lambdas<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    :param output: shape[N,10] 网络输出，用于做BCEloss 这里用来生成center的shape[0],如手写数字为10个类
    :param feature: shape[N,2] 特征层数据，用于做损失
    :param label: shape[N] 标签
    :param lambdas: λ超参数(学习率、步长)
    :return: loss
    """</span>
    label <span class="token operator">=</span> label<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
    center <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>output<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> feature<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment">#center: torch.Size([10, 2]) 其中10为类别数，2为center的维度，可视化过程中为2用于绘图。</span>
    <span class="token comment">#注意：feature与label对应的为一类，例如 feature的第4个[N,2]对应的类别为label中的[N],这样我们就要为每一个feature的相同的类规定一个中心，使用centreloss把这些相同类的拉到一堆。</span>
    <span class="token comment">#首先随机给定中心，但是要确定center的shape，因为特征点要与中心点相减，但是现在维度形状不一样</span>
    <span class="token comment">#使用index_select让center按照标签的形式扩张</span>
    <span class="token comment">#torch.index_select()函数：</span>
    label <span class="token operator">=</span> label<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span>
    center_exp <span class="token operator">=</span> center<span class="token punctuation">.</span>index_select<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> index<span class="token operator">=</span>label<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment">#center_exp: torch.Size([N, 2]) 原来center为[10,2]: 10为总的类别数，2为每个类的中心点，center_exp为根据label索引排列为[N,2]，这样相当于为N个数据都对应了一个中心点。</span>
    <span class="token comment">#bins=int(max(label).item() + 1)：当前批次label中包含的类别，不能用长度</span>
    <span class="token comment">#max=int(max(label).item() </span>
    count <span class="token operator">=</span> torch<span class="token punctuation">.</span>histc<span class="token punctuation">(</span>label<span class="token punctuation">,</span> bins<span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token builtin">max</span><span class="token punctuation">(</span>label<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">min</span><span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">max</span><span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token builtin">max</span><span class="token punctuation">(</span>label<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment">#做统计直方图，统计每个类别出现了几次</span>
    <span class="token comment">#count: torch.size([10])：10为这批次里有10个类，其中每个对应的数字就是该类别出现的次数</span>
    count_exp <span class="token operator">=</span> count<span class="token punctuation">.</span>index_select<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> index<span class="token operator">=</span>label<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment">#按照label去扩张，同样得到的是每个label的元素按照N个去对应的该类别所对应的出现的次数。</span>
    <span class="token comment">#下面做个图</span>
    <span class="token comment">#count_exp: torch.size([N])</span>
    loss <span class="token operator">=</span> lambdas<span class="token operator">/</span><span class="token number">2</span><span class="token operator">*</span>torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>div<span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>feature <span class="token operator">-</span> center_exp<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> count_exp<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> loss
</code></pre> 
<p><strong>Part 2 网络及损失</strong></p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

<span class="token keyword">class</span> <span class="token class-name">CenterLoss</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> cls_num<span class="token punctuation">,</span> feature_num<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>cls_num <span class="token operator">=</span> cls_num
        self<span class="token punctuation">.</span>center <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>cls_num<span class="token punctuation">,</span> feature_num<span class="token punctuation">)</span><span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> xs<span class="token punctuation">,</span> ys<span class="token punctuation">)</span><span class="token punctuation">:</span>
        center_exp <span class="token operator">=</span> self<span class="token punctuation">.</span>center<span class="token punctuation">.</span>index_select<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> index<span class="token operator">=</span>ys<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        count <span class="token operator">=</span> torch<span class="token punctuation">.</span>histc<span class="token punctuation">(</span>ys<span class="token punctuation">,</span> bins<span class="token operator">=</span>self<span class="token punctuation">.</span>cls_num<span class="token punctuation">,</span> <span class="token builtin">min</span><span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">max</span><span class="token operator">=</span>self<span class="token punctuation">.</span>cls_num <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span>
        count_dis <span class="token operator">=</span> count<span class="token punctuation">.</span>index_select<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> index<span class="token operator">=</span>ys<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>div<span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>xs <span class="token operator">-</span> center_exp<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> count_dis<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">class</span> <span class="token class-name">Net2</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># 28*28</span>
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>PReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># 28*28</span>
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>PReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            
            nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># 14*14</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># 14*14</span>
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>PReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># 14*14</span>
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>PReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            
            nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># 7*7</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># 7*7</span>
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>PReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># 7*7</span>
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>PReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># 3*3</span>
        <span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>feature <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">128</span> <span class="token operator">*</span> <span class="token number">3</span> <span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># 2:(x,y)</span>
        self<span class="token punctuation">.</span>output_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>center_loss_layer <span class="token operator">=</span> CenterLoss<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        <span class="token comment"># self.center_loss = self.center_loss_layer()</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> xs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        _feature <span class="token operator">=</span> self<span class="token punctuation">.</span>conv_layer<span class="token punctuation">(</span>xs<span class="token punctuation">)</span>
        y_conv <span class="token operator">=</span> torch<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>_feature<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">128</span> <span class="token operator">*</span> <span class="token number">3</span> <span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        y_feature <span class="token operator">=</span> self<span class="token punctuation">.</span>feature<span class="token punctuation">(</span>y_conv<span class="token punctuation">)</span>  <span class="token comment"># [N 2]</span>
        y_output <span class="token operator">=</span> torch<span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>self<span class="token punctuation">.</span>output_layer<span class="token punctuation">(</span>y_feature<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># print(y_feature.shape, y_output.shape)</span>
        <span class="token keyword">return</span> y_feature<span class="token punctuation">,</span> y_output
    
    <span class="token keyword">def</span> <span class="token function">get_loss</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> features<span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token punctuation">:</span>
        loss_center <span class="token operator">=</span> self<span class="token punctuation">.</span>center_loss_layer<span class="token punctuation">(</span>features<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>
        <span class="token keyword">return</span> loss_center

    <span class="token keyword">def</span> <span class="token function">visualize</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> feat<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> epoch<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># plt.ion()</span>
        color <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'#ff0000'</span><span class="token punctuation">,</span> <span class="token string">'#ffff00'</span><span class="token punctuation">,</span> <span class="token string">'#00ff00'</span><span class="token punctuation">,</span> <span class="token string">'#00ffff'</span><span class="token punctuation">,</span> <span class="token string">'#0000ff'</span><span class="token punctuation">,</span>
                 <span class="token string">'#ff00ff'</span><span class="token punctuation">,</span> <span class="token string">'#990000'</span><span class="token punctuation">,</span> <span class="token string">'#999900'</span><span class="token punctuation">,</span> <span class="token string">'#009900'</span><span class="token punctuation">,</span> <span class="token string">'#009999'</span><span class="token punctuation">]</span>
        plt<span class="token punctuation">.</span>clf<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>feat<span class="token punctuation">[</span>labels <span class="token operator">==</span> i<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> feat<span class="token punctuation">[</span>labels <span class="token operator">==</span> i<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'.'</span><span class="token punctuation">,</span> c<span class="token operator">=</span>color<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>
        plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'0'</span><span class="token punctuation">,</span> <span class="token string">'1'</span><span class="token punctuation">,</span> <span class="token string">'2'</span><span class="token punctuation">,</span> <span class="token string">'3'</span><span class="token punctuation">,</span> <span class="token string">'4'</span><span class="token punctuation">,</span> <span class="token string">'5'</span><span class="token punctuation">,</span> <span class="token string">'6'</span><span class="token punctuation">,</span> <span class="token string">'7'</span><span class="token punctuation">,</span> <span class="token string">'8'</span><span class="token punctuation">,</span> <span class="token string">'9'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> loc<span class="token operator">=</span><span class="token string">'upper right'</span><span class="token punctuation">)</span>
        <span class="token comment"># plt.xlim(xmin=-5, xmax=5)</span>
        <span class="token comment"># plt.ylim(ymin=-5, ymax=5)</span>
        plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"epoch=%d"</span> <span class="token operator">%</span> epoch<span class="token punctuation">)</span>
        plt<span class="token punctuation">.</span>savefig<span class="token punctuation">(</span><span class="token string">'./images/epoch=%d.jpg'</span> <span class="token operator">%</span> epoch<span class="token punctuation">)</span>
        <span class="token comment"># plt.draw()</span>
        <span class="token comment"># plt.pause(0.001)</span>
</code></pre> 
<p><strong>Part 3 训练</strong></p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">as</span> data
<span class="token keyword">import</span> torchvision
<span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">as</span> transforms
<span class="token keyword">from</span> center<span class="token punctuation">.</span>Center_loss_Net <span class="token keyword">import</span> CenterLoss<span class="token punctuation">,</span> Net2
<span class="token keyword">import</span> os
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    
    save_path1 <span class="token operator">=</span> <span class="token string">"./params3/net_center1.pth"</span>
    train_data <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span>root<span class="token operator">=</span>r<span class="token string">"C:\Projects"</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                            transform<span class="token operator">=</span>transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                                                          transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span>mean<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token punctuation">]</span><span class="token punctuation">,</span>
                                                                                               std<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    train_loader <span class="token operator">=</span> data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token operator">=</span>train_data<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span>
    device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span>
    net <span class="token operator">=</span> Net2<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    <span class="token keyword">if</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>save_path1<span class="token punctuation">)</span><span class="token punctuation">:</span>
        net<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>save_path1<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"NO Param"</span><span class="token punctuation">)</span>
    lossfn_cls <span class="token operator">=</span> nn<span class="token punctuation">.</span>NLLLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimzer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    optimzer2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>center_loss_layer<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>
    epoch <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
        feat_loader <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        label_loader <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
            y <span class="token operator">=</span> y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
            <span class="token comment"># x = torch.reshape(x, [-1, 28*28])</span>
            feature<span class="token punctuation">,</span> output <span class="token operator">=</span> net<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            <span class="token comment"># feature2, output2 = net2.forward(x)</span>
            <span class="token comment"># print(feature.shape)  # [N,2]</span>
            <span class="token comment"># print(feature.shape)#[N,2]</span>
            <span class="token comment"># print(output.shape)#[N,10]</span>
            <span class="token comment"># center = nn.Parameter(torch.randn(output.shape[1], feature.shape[1]))</span>
            <span class="token comment"># print(center.shape)#[10,2]</span>
            loss_cls <span class="token operator">=</span> lossfn_cls<span class="token punctuation">(</span>output<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
            <span class="token comment"># y = y.float()</span>
            <span class="token comment"># loss_cls, loss_center = net.get_loss(output, feature)</span>
            <span class="token comment"># loss_center = lossfn_2(feature2, feature2, y)</span>
            <span class="token comment"># print(feature2, y)</span>
            loss_center <span class="token operator">=</span> net<span class="token punctuation">.</span>get_loss<span class="token punctuation">(</span>features<span class="token operator">=</span>feature<span class="token punctuation">,</span> labels<span class="token operator">=</span>y<span class="token punctuation">)</span>
            
            loss <span class="token operator">=</span> loss_cls <span class="token operator">+</span> loss_center
            optimzer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            optimzer2<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token comment"># loss_cls.backward(retain_graph)</span>
            <span class="token comment"># loss_center.backward()</span>
            loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            optimzer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
            optimzer2<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token comment"># feature.shape=[100,2]</span>
            <span class="token comment"># y.shape=[100]</span>
            feat_loader<span class="token punctuation">.</span>append<span class="token punctuation">(</span>feature<span class="token punctuation">)</span>
            label_loader<span class="token punctuation">.</span>append<span class="token punctuation">(</span>y<span class="token punctuation">)</span>
            <span class="token keyword">if</span> i <span class="token operator">%</span> <span class="token number">20</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
                <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"epoch:"</span><span class="token punctuation">,</span> epoch<span class="token punctuation">,</span> <span class="token string">"i:"</span><span class="token punctuation">,</span> i<span class="token punctuation">,</span> <span class="token string">"total:"</span><span class="token punctuation">,</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"softmax_loss:"</span><span class="token punctuation">,</span> loss_cls<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"center_loss:"</span><span class="token punctuation">,</span>
                      loss_center<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        feat <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>feat_loader<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
        labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>label_loader<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token string">'---------------'</span>
        <span class="token comment"># print(feat.shape)#feat.shape=[60000,2]</span>
        <span class="token comment"># print(labels.shape)#feat.shape=[60000]</span>
        <span class="token string">'-------------------'</span>
        net<span class="token punctuation">.</span>visualize<span class="token punctuation">(</span>feat<span class="token punctuation">.</span>data<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> labels<span class="token punctuation">.</span>data<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> epoch<span class="token punctuation">)</span>
        epoch <span class="token operator">+=</span> <span class="token number">1</span>
        torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>net<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> save_path1<span class="token punctuation">)</span>
        <span class="token comment"># torch.save(net2.state_dict(), save_path2)</span>
        <span class="token keyword">if</span> epoch <span class="token operator">==</span> <span class="token number">150</span><span class="token punctuation">:</span>
            <span class="token keyword">break</span>

</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/bc622705325fc014571651ace4d43c33/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">删除数组中的指定元素——C&#43;&#43;实现</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/2fe62b489075f6406b5126a2795f7c4c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">（4.6.12）Android  Resource详解</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>