<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【神经网络】神经元模型和感知器 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【神经网络】神经元模型和感知器" />
<meta property="og:description" content=" 1.神经元模型 在生物神经网络中，每个神经元与其他神经元相连，当它“兴奋”时，就会向相连的神经元发送化学物质，从而改变这些神经元内的电位。如果某神经元的电位超过了一个“阈值”，那么它就会被激活，即“兴奋”起来。
1943年，心理学家McCulloch和数学家Pitts提出了第一个抽象的神经元模型，称为MP模型。MP模型是人工神经网络的基本单元，如下图所示：
其中，向量是神经元模型的输入，为第i个神经元到第n个神经元的输出。是与输入相对应权值。在输入和神经元之间通常还有一个偏置（图上未画），这里记为b，神经元的输出为y。
整个过程为：神经元接收来自 n个其他神经元传递过来的输入信号x，将输入 x 和所对应的权值W 相乘后再求和，得到神经元接收到的总输入值，然后通过激活函数f处理产生神经元最终的输出y，如下式所示：
激活函数f用于模拟人脑部神经元对输入的信息进行处理并判断是否产生对应的响应，即是否“兴奋”。常见的激活函数有：
阶跃函数 其中z表示阈值，即当输入x超过阈值z时（一般情况下z=0），激活函数的输出为1，表示神经元被激活，否则被抑制。但是可以看出，其函数输出是不连续的，为此更多地采用sigmoid函数。
Sigmoid函数 Sigmoid函数是在定义域内连续递增的函数，把输入映射到0到1之间，其中 被映射到1， 被映射到0。同样会设置一个阈值，当sigmoid函数的输出大于这个阈值时，神经元模型被激活。 它们的图像如下：
MP模型虽然简单，却建立了神经网络大厦的地基。但是MP模型中，权重的值都是预先设置的，因此不能学习 2.感知器 2.1 基本结构 感知器模型（Perceptron）是在1958年由计算科学家Rosenblatt首先提出的。感知器由两层神经元组成，分别为输入层和输出层构成。输入层接收外界输入信号后传递给输出层，输出层是 M-P 神经元，也就是说只有输出层神经元进行激活函数处理。如下图所示：
感知器的计算公式如下所示：
所采用的激活函数f为sign函数：
可以把感知机看作是n维实例空间中的超平面决策面，对于超平面一侧的实例，感知器输出1，即正例；对于另一侧的实例输出 -1，即负例。这个决策超平面方程就是（n=2的话，即二维实例空间，此时感知机退化成一条直线）。那些可以被某一个超平面分割的正反样例集合称为线性可分样例集合，换句话说，感知机是一种线性的分类模型，不能完成非线性的分类。
从计算表达式上来看，感知器模型与神经元模型并没有明显区别，但是实际上主要的区别有三点：
（1）感知器的输入变成了一层神经元，而神经元模型的输入为其他神经元的输出信号；
（2）感知器的激活函数为sign函数，而神经元模型的激活函数可以是sign函数，也可以是sigmoid函数等；
（3）感知器可以采用有监督的方式学习到参数值，而神经元模型的参数是不能被学习的。
2.2 损失函数 感知器最大的亮点就是在于可以利用梯度下降算法来自主更新参数，为此必须要建立起损失函数。定义感知机的损失函数之前需要明确以下几点：
（1）当, ，即数据被预测为正例；当, ，数据被预测为负例，因此对于正确分类的数据都满足，分类错误的数据。
（2）点到平面的距离计算公式为：
所以定义感知机的损失函数可以让误分类的数据到直线的距离和最小，此时误分类点到超平面的距离是：
这样，假设超平面S的误分类点集合为M，那么所有误分类点到超平面的总距离为:
不考虑，就得到了感知机学习的损失函数：
显然，损失函数是非负的。如果没有误分类点，损失函数的值为0。而且误分类点越少，误分类点离超平面越近，损失函数的值就越小。
为什么损失函数可以不考虑1/||𝑤||？
1/||𝑤||不影响𝑦𝑖(𝑤⋅𝑥𝑖&#43;𝑏)正负的判断，即不影响学习算法的中间过程。因为感知机学习算法是误分类驱动的，这里需要注意的是所谓的“误分类驱动”指的是我们只需要判断−𝑦𝑖(𝑤⋅𝑥𝑖&#43;𝑏)的正负来判断分类的正确与否，而1/||𝑤||并不影响正负值的判断。所以1||𝑤||对感知机学习算法的中间过程可以不考虑。1||𝑤||不影响感知机学习算法的最终结果。因为感知机学习算法最终的终止条件是所有的输入都被正确分类，即不存在误分类的点。则此时损失函数为0. 对应于−1/||𝑤||∑𝑖∈𝑀𝑦𝑖(𝑤⋅𝑥𝑖&#43;𝑏)，即分子为0.则可以看出1/||𝑤||对最终结果也无影响。 综上所述，即使忽略1/||𝑤||，也不会对感知机学习算法的执行过程产生任何影响。反而还能简化运算，提高算法执行效率。
2.3 梯度下降更新 梯度下降首先要求得损失函数对于参数的梯度，即导数。通过连式法则，很容易求出损失函数对参数W和b的导数：
梯度下降进行更新：
2.4 感知机和SVM的异同。 （1）相同点：
都可以求取一个超平面来完成线性的二分类问题均属于监督学习的分类器 （2）不同点
SVM可以通过改变核函数来解决非线性问题，而感知机不行感知机的损失函数为所有误分类点的几何距离，使其最小，SVM的损失函数是参数的L2范数，也就是结构风险最小化的。或者说感知机的损失函数是函数距离的和，而支持向量机是几何距离的和，感知机是去掉距离限制的SVM感知机追求最大程度正确划分，最小化所有错误点到超平面的距离，很容易造成过拟合。而支持向量机最求两种数据最大程度的分开，即最大化支持向量点之间的间隔，一定程度上避免了过拟合， " />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/41fc3b23a132c11abb450f738ee55501/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-07-22T17:17:04+08:00" />
<meta property="article:modified_time" content="2020-07-22T17:17:04+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【神经网络】神经元模型和感知器</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>1.神经元模型</h2> 
<p>在生物神经网络中，每个神经元与其他神经元相连，当它“兴奋”时，就会向相连的神经元发送化学物质，从而改变这些神经元内的电位。如果某神经元的电位超过了一个“阈值”，那么它就会被激活，即“兴奋”起来。</p> 
<p>1943年，心理学家McCulloch和数学家Pitts提出了第一个抽象的神经元模型，称为<strong>MP模型</strong>。MP模型是人工神经网络的基本单元，如下图所示：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/03/1d/ZhjvYJnl_o.png"></p> 
<p style="margin-left:0cm;">其中，向量<img alt="x = (x_{1},x_{2},...,x_{n})^{T}" class="mathcode" src="https://images2.imgbox.com/a3/54/3ey0X5T0_o.gif">是神经元模型的输入，为<strong>第i个神经元到第n个神经元的输出。</strong><img alt="w = (w_{1},w_{2},...,w_{n})^{T}" class="mathcode" src="https://images2.imgbox.com/9b/dd/MnXAIs6S_o.gif">是与输入相对应权值。在输入和神经元之间通常还有一个偏置（图上未画），这里记为b，神经元的输出为y。</p> 
<p style="margin-left:0cm;">整个过程为：神经元接收来自 n个其他神经元传递过来的输入信号x，将输入 x 和所对应的权值W 相乘后再求和，得到神经元接收到的总输入值<img alt="\sum x_{i}w_{i}" class="mathcode" src="https://images2.imgbox.com/3d/73/JQnVMSA3_o.gif">，然后通过激活函数f处理产生神经元最终的输出y，如下式所示：</p> 
<p style="margin-left:0cm;">                                                                                                    <img alt="y = f(\sum_{i = 0}^{n} w_{i}x_{i}+b)" class="mathcode" src="https://images2.imgbox.com/89/b5/IwAPWX2h_o.gif"></p> 
<p style="margin-left:0cm;">激活函数f用于模拟人脑部神经元对输入的信息进行处理并判断是否产生对应的响应，即是否“兴奋”。常见的激活函数有：</p> 
<ul><li style="margin-left:0cm;">阶跃函数</li></ul> 
<p style="text-align:center;"><img alt="" height="69" src="https://images2.imgbox.com/47/97/FsZMhWBs_o.png" width="153"></p> 
<p> 其中z表示阈值，即当输入x超过阈值z时（一般情况下z=0），激活函数的输出为1，表示神经元被激活，否则被抑制。但是可以看出，其函数输出是<strong>不连续</strong>的，为此更多地采用sigmoid函数。</p> 
<ul><li style="margin-left:0cm;">Sigmoid函数</li></ul> 
<p style="text-align:center;"><img alt="" height="65" src="https://images2.imgbox.com/7b/a9/VlEoIAL9_o.png" width="134"></p> 
<p>Sigmoid函数是在定义域内连续递增的函数，把输入映射到0到1之间，其中 被映射到1， 被映射到0。同样会设置一个阈值，当sigmoid函数的输出大于这个阈值时，神经元模型被激活。 </p> 
<p>它们的图像如下：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/37/95/kePNNiPg_o.png"></p> 
<blockquote> 
 <p>MP模型虽然简单，却建立了神经网络大厦的地基。但是MP模型中，权重的值都是<span style="color:#f33b45;"><strong>预先设置的</strong></span>，因此<span style="color:#f33b45;"><strong>不能学习</strong></span> </p> 
</blockquote> 
<h2>2.感知器</h2> 
<h4 style="margin-left:0cm;"><strong>2.1 基本结构</strong></h4> 
<p style="margin-left:0cm;">感知器模型（Perceptron）是在1958年由计算科学家Rosenblatt首先提出的。感知器由<strong>两层神经元</strong>组成，分别为输入层和输出层构成。输入层接收外界输入信号后传递给输出层，<strong>输出层是 M-P 神经元，<span style="color:#f33b45;">也就是说只有输出层神经元进行激活函数处理</span></strong>。如下图所示：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/f0/3d/2a4n0hTy_o.png"></p> 
<p> </p> 
<p>感知器的计算公式如下所示：</p> 
<p>                                                                                          <img alt="y = f(\sum_{i = 0}^{n} w_{i}x_{i}+b)" class="mathcode" src="https://images2.imgbox.com/6a/b9/Fk20SBOS_o.gif"></p> 
<p>所采用的激活函数f为sign函数：</p> 
<p>                                                                                        <img alt="sign(x) = \left\{\begin{matrix} 1, x \geqslant 0 &amp; \\ -1,x &lt;0&amp; \end{matrix}\right." class="mathcode" src="https://images2.imgbox.com/25/f9/pzIZr83U_o.gif"></p> 
<p>可以把感知机看作是n维实例空间中的超平面决策面，对于超平面一侧的实例，感知器输出1，即正例；对于另一侧的实例输出 -1，即负例。这个决策超平面方程就是<img alt="wx+b = 0" class="mathcode" src="https://images2.imgbox.com/22/fc/JvRbGjiW_o.gif">（n=2的话，即二维实例空间，此时感知机退化成一条直线）。那些可以被某一个超平面分割的正反样例集合称为<strong>线性可分</strong>样例集合，换句话说，感知机是一种<span style="color:#f33b45;"><strong>线性的分类模型</strong></span>，<span style="color:#f33b45;"><strong>不能完成非线性的分类</strong></span>。</p> 
<blockquote> 
 <p>从计算表达式上来看，感知器模型与神经元模型并没有明显区别，但是实际上主要的区别有三点：<br> （1）感知器的输入变成了<strong>一层神经元</strong>，而神经元模型的输入为其他神经元的输出信号；<br> （2）感知器的激活函数为<strong>sign函数</strong>，而神经元模型的激活函数可以是sign函数，也可以是sigmoid函数等；<br> （3）感知器可以采用<strong>有监督的方式</strong>学习到参数值，而神经元模型的参数是<strong>不能被学习的</strong>。</p> 
</blockquote> 
<h4><strong>2.2 损失函数</strong> </h4> 
<p>感知器最大的亮点就是在于可以利用梯度下降算法来自主更新参数，为此必须要建立起损失函数。定义感知机的损失函数之前需要明确以下几点：</p> 
<p>（1）当<img alt="wx+b &gt; 0" class="mathcode" src="https://images2.imgbox.com/61/40/yKJAS8X1_o.gif">, <img alt="y = +1" class="mathcode" src="https://images2.imgbox.com/41/c0/cHJnVeCB_o.gif">，即数据被预测为正例；当<img alt="wx+b &lt; 0" class="mathcode" src="https://images2.imgbox.com/11/78/9lC89MsK_o.gif">, <img alt="y = -1" class="mathcode" src="https://images2.imgbox.com/0a/01/GnPTYTFo_o.gif">，数据被预测为负例，因此对于正确分类的数据都满足<img alt="y(wx+b) &gt; 0" class="mathcode" src="https://images2.imgbox.com/8d/c8/yWycMWpR_o.gif">，分类错误的数据<img alt="y(wx+b) &lt; 0" class="mathcode" src="https://images2.imgbox.com/f1/a8/Ca3S3AZV_o.gif">。</p> 
<p>（2）点到平面的距离计算公式为：<img alt="d = \frac{|wx + b|}{||w||}" class="mathcode" src="https://images2.imgbox.com/9a/d5/0crKYTdt_o.gif"></p> 
<p>所以定义感知机的损失函数可以<span style="color:#f33b45;"><strong>让误分类的数据到直线的距离和最小</strong></span>，此时误分类点到超平面的距离是：</p> 
<p>                                                                                      <img alt="d_{i} = -\frac{y_{i}(wx_{i} + b)}{||w||}" class="mathcode" src="https://images2.imgbox.com/31/d8/jHditWJu_o.gif"></p> 
<p>这样，假设超平面S的误分类点集合为M，那么所有误分类点到超平面的总距离为:<br>                                                                                      <img alt="\frac{1}{||w||} \sum_{i = 0}^{M} y_{i}(wx_{i} + b)" class="mathcode" src="https://images2.imgbox.com/98/5e/JZAZlBmH_o.gif"></p> 
<p>不考虑<img alt="\frac{1}{||w||}" class="mathcode" src="https://images2.imgbox.com/01/c3/Lju5YBpH_o.gif">，就得到了感知机学习的<span style="color:#f33b45;"><strong>损失函数</strong></span>：</p> 
<p>                                                                                         <img alt="L(w,b) =-\sum_{i = 0}^{M} y_{i}(wx_{i} + b)" class="mathcode" src="https://images2.imgbox.com/af/4f/eGY1h4Zx_o.gif"></p> 
<p>显然，损失函数是非负的。如果没有误分类点，损失函数的值为0。而且误分类点越少，误分类点离超平面越近，损失函数的值就越小。</p> 
<blockquote> 
 <p id="为什么可以不考虑frac1w"><strong>为什么损失函数可以不考虑1/||𝑤||？</strong></p> 
 <ul><li>1/||𝑤||不影响𝑦𝑖(𝑤⋅𝑥𝑖+𝑏)正负的判断，即不影响学习算法的中间过程。因为感知机学习算法是<strong>误分类驱动</strong>的，这里需要注意的是所谓的“误分类驱动”指的是我们只需要判断−𝑦𝑖(𝑤⋅𝑥𝑖+𝑏)的正负来判断分类的正确与否，而1/||𝑤||并不影响正负值的判断。所以1||𝑤||对感知机学习算法的中间过程可以不考虑。</li><li>1||𝑤||不影响感知机学习算法的最终结果。因为感知机学习算法最终的终止条件是所有的输入都被正确分类，即不存在误分类的点。则此时损失函数为0. 对应于−1/||𝑤||∑𝑖∈𝑀𝑦𝑖(𝑤⋅𝑥𝑖+𝑏)，即分子为0.则可以看出1/||𝑤||对最终结果也无影响。</li></ul> 
 <p>综上所述，即使忽略1/||𝑤||，也不会对感知机学习算法的执行过程产生任何影响。反而还能简化运算，提高算法执行效率。</p> 
</blockquote> 
<h4><strong>2.3 梯度下降更新</strong></h4> 
<p>梯度下降首先要求得损失函数对于参数的梯度，即导数。通过连式法则，很容易求出损失函数对参数W和b的导数：</p> 
<p>                                                                                     <img alt="dw = -\sum_{i = 0}^{M}y_{i}x_{i}" class="mathcode" src="https://images2.imgbox.com/f0/7c/qA9xlhwg_o.gif"></p> 
<p>                                                                                       <img alt="db = -\sum_{i = 0}^{M}y_{i}" class="mathcode" src="https://images2.imgbox.com/39/b9/RLDCUeaX_o.gif"></p> 
<p>梯度下降进行更新：</p> 
<p>                                                                                    <img alt="w = w -\eta dw = w + \eta y_{i}x_{i}" class="mathcode" src="https://images2.imgbox.com/fd/03/Hd0IFcog_o.gif"></p> 
<p>                                                                                       <img alt="b = b - \eta db = b + \eta y_{i}" class="mathcode" src="https://images2.imgbox.com/60/73/UISVNEvA_o.gif"></p> 
<h4>2.4 感知机和SVM的异同。</h4> 
<p>（1）相同点：</p> 
<ul><li>都可以求取一个超平面来完成线性的二分类问题</li><li>均属于监督学习的分类器</li></ul> 
<p>（2）不同点</p> 
<ul><li>SVM可以通过改变核函数来解决非线性问题，而感知机不行</li><li>感知机的损失函数为所有误分类点的几何距离，使其最小，SVM的损失函数是参数的L2范数，也就是结构风险最小化的。或者说感知机的损失函数是函数距离的和，而支持向量机是几何距离的和，感知机是去掉距离限制的SVM</li><li>感知机追求最大程度正确划分，最小化所有错误点到超平面的距离，很容易造成过拟合。而支持向量机最求两种数据最大程度的分开，即最大化支持向量点之间的间隔，一定程度上避免了过拟合，</li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/11bd4499d09a29e114be9806086023a0/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">drools学习笔记 workbench tomcat下部署</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6a2153c2cf4c1a3534748e7e197610b1/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">倒数第二个元素css选择器_创建一个CSS / JS倒数计时器</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>