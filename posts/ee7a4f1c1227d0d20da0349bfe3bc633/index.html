<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>零基础读懂Stable Diffusion（II）：怎么训练 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="零基础读懂Stable Diffusion（II）：怎么训练" />
<meta property="og:description" content="原文：零基础读懂Stable Diffusion（II）：怎么训练 - 知乎
前几个月AIGC可谓是大热了一把，各种高质量的生成图片层出不穷，而其中最重要的开源模型Stable Diffusion也受到了各种技术商业上的热捧，以很快的速度不断的向前迭代着。之前作为一个没有相关知识基础的小白，为了了解相关的技术知识，找了很多文章看，最后还是发现Jay Alammar的这篇文章讲的最为通俗易懂，于是决定简单翻译一下，方便更多人从零开始了解这项强大的技术。
由于原文篇幅较长，所以这里分为三篇文章进行讲解：
第一篇，主要讲“是什么”的问题，包括Stable Diffusion是什么，里面的各个模块是什么，上一篇的链接在这里。第二篇，也就是本篇，主要讲“怎么办”的问题，也就是Diffusion怎么训练以及怎么使用的问题。第三篇，主要讲“如何控制”的问题，具体阐述语义信息到底是怎么影响生成图片的过程的。 其中，第一篇文章也可以点击这张卡片阅读：
曾飞飞：零基础读懂Stable Diffusion（I）：怎么组成308 赞同 · 5 评论文章正在上传…重新上传取消
接下来正式进入第二篇的介绍，谈谈训练Diffusion该怎么去训练和怎么去使用的问题。
原文链接： The Illustrated Stable Diffusion 有能力和时间的小伙伴还是更推荐阅读原文噢 作者： Jay Alammar 译者：曾飞飞（知乎） 上文讲到，Stable Diffusion中有着三个主要的模块，包括一个Text Understander处理语义信息，一个Image Information Creator生成图片的隐变量，一个Image Decoder利用隐变量生成真正的图片。同时，对于整个图片生成的过程，我们也有了更加深入的了解。我们不仅仅知道了向量通过各个阶段时的形状变化，还可视化了过程中噪声变为图片的全过程。因此，在大概了解Stable Diffusion的工作流程之后，我们接下来要开始学会训练这个模型了。
一，Diffusion怎么训练 Diffusion模型能够生成高质量图片，其核心原因在于我们现在有着极其强大的计算机视觉模型。只要数据集够大，我们强大的模型就能学习到任何复杂的操作。那具体diffusion里面让unet学习了怎样一个操作呢？简单来说，就是“去噪”。
那如何为去噪的任务设计数据集呢？很简单，我们只要向普通的照片里添加噪声，不就有了加噪的图片了嘛。假定我们现在有一张金字塔的图片，我们用random函数生成从强到弱各个强度的噪声，比如下图中0~3共计4个强度的噪声。现在我们选定个某个强度的噪声，比如下图中选了噪声1，并且把这个噪声添加到图片里：
训练集如何制作：1，选张图片 2，生成从强到弱各个强度的噪声 3，从中选个噪声（比如强度1） 4，加到图片里
现在，我们就制作完成了训练集里面的一张图片。按照这样的操作，选一张图片，再选一种强度的噪声混合，我们还可以制作很多训练集。比如下面就选了图书馆的一张照片，混合了强度为2的噪声，创造了一个更模糊一点的训练样本：
上面仅仅作为一个简单的例子，所以噪声只设置了四个档位。实际上我们可以更细腻地划分噪声的等级，将其分为几十个甚至上百个档位，这样就可以创建出成千上万个训练集。比如我们现在噪声设置成100个档位，下面就展示了利用不同的档位结合不同的图片创建6张训练集的过程：
这样的话，一组训练集包括了三样东西：噪声强度(上图数字)，加噪后的图片(上图左列图片)，以及噪声图（上图右列图片）就可以了。训练的时候我们的unet只要在已知噪声强度的条件下，学习如何从加噪后的图片中计算出噪声图就可以了。注意，我们并不直接输出无噪声的原图，而是让unet去预测原图上所加过的噪声。当需要生成图片的时候，我们用加噪图减掉噪声就能恢复出原图了。
具体的一个训练过程就如下图所示，一共分四步走：
从训练集中选取一张加噪过的图片和噪声强度，比如下面的加噪街道图和噪声强度3。输入unet，让unet预测噪声图，比如下图的unet prediction。计算和真正的噪声图之间的误差通过反向传播更新unet的参数。 那完成训练后，我们该如何生成图片呢？
二，Diffusion怎么生成图片 假设我们现在已经按照上面的步骤训练好了一个unet，这就意味着它就可以成功从一个加噪的图片中推断出噪声了。如下图中，知道噪声强度的情况下，给unet输入一张有噪图，unet就输出有噪图上面加过的噪声：
只要知道噪声强度，训练好的unet就可以成功推断出噪声
既然现在噪声图能够被推断出来，我们只要把加噪后的图片减去这个噪声图，就可以轻松得到一张略微去噪的图片了：
重复这个过程，预测噪声图，再减去噪声图，进行第二步去噪：
不断地重复这个过程，不断的去除一张噪声图片的噪声，最终我们就可以得到一张很棒的图片。这个图片是接近训练集分布的，它和训练集保有相同的像素规律。比如你用一个艺术家数据集去训练，它就会遵循美学的颜色分布，你用真实世界的训练集去训练，它的结果就会尽量遵循真实世界的规律。现在，你已经了解了Diffusion模型的基本规律了，这不仅仅适用于Stable Diffusion，也适用于OpenAI的Dall-E 2和Google的Imagen。
注意到上面这个过程中我们暂时还没有引入文字和语义向量的控制。也就是说，如果单纯的按照上面的流程走，我们可能能得到一些很炫酷的图片，但我们没有办法去控制最后的结果到底是什么。那如何引入文字控制呢？这就要使用语言模型和Attention机制来引入语义啦，这一部分内容我们放到下篇中讲解。
三、总结 在上一篇我们介绍完Stable Diffusion的各个模块和工作流程后，这一篇我们着重讲述了Diffusion是如何训练和推理的。文章的结尾，这里简单总结一下这篇的内容：
Diffusion&#39;s Training: 利用 “噪声强度、噪声图、加噪后图片”组成训练集，训练unet，使其学习如何从加噪后的图片推断出所加的噪声。Diffusion&#39;s Inference: 利用训练好的unet，从纯噪声中一步一步去噪，得到合理正常的图片。 至于语义信息是如何控制生成图片的过程的，我们就留到下篇中再做叙述啦。下篇文章可以点击这里阅读：
曾飞飞：零基础读懂Stable Diffusion（III）：怎么控制252 赞同 · 38 评论文章正在上传…重新上传取消" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/ee7a4f1c1227d0d20da0349bfe3bc633/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-18T10:33:27+08:00" />
<meta property="article:modified_time" content="2023-04-18T10:33:27+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">零基础读懂Stable Diffusion（II）：怎么训练</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>原文：<a href="https://zhuanlan.zhihu.com/p/597732415" rel="nofollow" title="零基础读懂Stable Diffusion（II）：怎么训练 - 知乎">零基础读懂Stable Diffusion（II）：怎么训练 - 知乎</a></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/17/6a/Clx4VnRD_o.jpg"></p> 
<p>前几个月AIGC可谓是大热了一把，各种高质量的生成图片层出不穷，而其中最重要的开源模型Stable Diffusion也受到了各种技术商业上的热捧，以很快的速度不断的向前迭代着。之前作为一个没有相关知识基础的小白，为了了解相关的技术知识，找了很多文章看，最后还是发现<a href="https://link.zhihu.com/?target=https%3A//jalammar.github.io/" rel="nofollow" title="Jay Alammar">Jay Alammar</a>的这篇文章讲的最为通俗易懂，于是决定简单翻译一下，方便更多人从零开始了解这项强大的技术。</p> 
<p>由于原文篇幅较长，所以这里分为三篇文章进行讲解：</p> 
<ol><li><a href="https://zhuanlan.zhihu.com/p/597247221" rel="nofollow" title="第一篇">第一篇</a>，主要讲“<strong>是什么</strong>”的问题，包括Stable Diffusion是什么，里面的各个模块是什么，上一篇的链接在这里。</li><li>第二篇，也就是本篇，主要讲“<strong>怎么办</strong>”的问题，也就是Diffusion怎么训练以及怎么使用的问题。</li><li><a href="https://zhuanlan.zhihu.com/p/598070109" rel="nofollow" title="第三篇">第三篇</a>，主要讲“<strong>如何控制</strong>”的问题，具体阐述语义信息到底是怎么影响生成图片的过程的。</li></ol> 
<p>其中，第一篇文章也可以点击这张卡片阅读：</p> 
<p><a href="https://zhuanlan.zhihu.com/p/597247221" rel="nofollow" title="曾飞飞：零基础读懂Stable Diffusion（I）：怎么组成308 赞同 · 5 评论文章正在上传…重新上传取消">曾飞飞：零基础读懂Stable Diffusion（I）：怎么组成308 赞同 · 5 评论文章正在上传…重新上传取消</a></p> 
<p>接下来正式进入第二篇的介绍，谈谈训练Diffusion该怎么去训练和怎么去使用的问题。</p> 
<blockquote>
  原文链接： 
 <a href="https://link.zhihu.com/?target=https%3A//jalammar.github.io/illustrated-stable-diffusion/" rel="nofollow" title="The Illustrated Stable Diffusion">The Illustrated Stable Diffusion</a> 有能力和时间的小伙伴还是更推荐阅读原文噢 
 <br> 作者： 
 <a href="https://link.zhihu.com/?target=https%3A//jalammar.github.io/" rel="nofollow" title="Jay Alammar">Jay Alammar</a> 
 <br> 译者：曾飞飞（知乎） 
</blockquote> 
<p>上文讲到，Stable Diffusion中有着三个主要的模块，包括一个<strong>Text Understander</strong>处理语义信息，一个<strong>Image Information Creator</strong>生成图片的隐变量，一个<strong>Image Decoder</strong>利用隐变量生成真正的图片。同时，对于整个图片生成的过程，我们也有了更加深入的了解。我们不仅仅知道了向量通过各个阶段时的<strong>形状变化</strong>，还<strong>可视化</strong>了过程中噪声变为图片的全过程。因此，在大概了解Stable Diffusion的工作流程之后，我们接下来要开始学会<strong>训练这个模型</strong>了。</p> 
<p></p> 
<h3>一，Diffusion怎么训练</h3> 
<p>Diffusion模型能够生成高质量图片，其核心原因在于我们现在有着极其强大的计算机视觉模型。只要数据集够大，我们强大的模型就能学习到任何复杂的操作。那具体diffusion里面让unet学习了怎样一个操作呢？简单来说，就是“<strong>去噪</strong>”。</p> 
<p>那如何为去噪的任务设计数据集呢？很简单，我们只要<strong>向普通的照片里添加噪声</strong>，不就有了加噪的图片了嘛。假定我们现在有一张金字塔的图片，我们用random函数生成从强到弱各个强度的噪声，比如下图中0~3共计4个强度的噪声。现在我们<strong>选定个某个强度</strong>的噪声，比如下图中选了噪声1，并且把这个噪声添加到图片里：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/2c/33/990nCazs_o.png"></p> 
<p>训练集如何制作：1，选张图片 2，生成从强到弱各个强度的噪声 3，从中选个噪声（比如强度1） 4，加到图片里</p> 
<p>现在，我们就制作完成了训练集里面的一张图片。按照这样的操作，选一张图片，再选一种强度的噪声混合，我们还可以制作很多训练集。比如下面就选了图书馆的一张照片，混合了强度为2的噪声，创造了一个<strong>更模糊一点</strong>的训练样本：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/d6/c4/z0B5dg0I_o.png"></p> 
<p>上面仅仅作为一个简单的例子，所以噪声只设置了四个档位。实际上我们可以更细腻地划分噪声的等级，将其分为几十个甚至上百个档位，这样就可以创建出成千上万个训练集。比如我们现在噪声设置成100个档位，下面就展示了利用不同的档位结合不同的图片创建6张训练集的过程：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/ec/f9/O4Tf08VV_o.png"></p> 
<p>这样的话，一组训练集包括了三样东西：<strong>噪声强度</strong>(上图数字)，<strong>加噪后的图片</strong>(上图左列图片)，以及<strong>噪声图</strong>（上图右列图片）就可以了。训练的时候我们的unet只要在已知噪声强度的条件下，学习如何从加噪后的图片中计算出<strong>噪声图</strong>就可以了。注意，我们并不直接输出无噪声的原图，而是让unet去<strong>预测原图上所加过的噪声</strong>。当需要生成图片的时候，我们用加噪图<strong>减掉</strong>噪声就能恢复出原图了。</p> 
<p>具体的一个训练过程就如下图所示，一共分四步走：</p> 
<ol><li>从训练集中选取一张加噪过的图片和噪声强度，比如下面的加噪街道图和噪声强度3。</li><li>输入unet，让unet预测<strong>噪声图</strong>，比如下图的unet prediction。</li><li>计算和真正的噪声图之间的误差</li><li>通过反向传播更新unet的参数。</li></ol> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/f7/0c/1efB5JGs_o.png"></p> 
<p>那完成训练后，我们该如何生成图片呢？</p> 
<p></p> 
<h3>二，Diffusion怎么生成图片</h3> 
<p>假设我们现在已经按照上面的步骤训练好了一个unet，这就意味着它就<strong>可以成功从一个加噪的图片中推断出噪声了</strong>。如下图中，知道噪声强度的情况下，给unet输入一张有噪图，unet就输出有噪图上面加过的噪声：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/80/4f/7iH04qOg_o.png"></p> 
<p>只要知道噪声强度，训练好的unet就可以成功推断出噪声</p> 
<p>既然现在噪声图能够被推断出来，我们只要把加噪后的图片减去这个噪声图，就可以轻松得到一张略微去噪的图片了：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/33/d5/KnZiC3Me_o.png"></p> 
<p>重复这个过程，预测噪声图，再减去噪声图，进行第二步去噪：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/7e/d6/4yqN0qDM_o.png"></p> 
<p>不断地重复这个过程，不断的去除一张噪声图片的噪声，最终我们就可以得到一张很棒的图片。这个图片是接近训练集分布的，它和训练集保有相同的像素规律。比如你用一个艺术家数据集去训练，它就会遵循美学的颜色分布，你用真实世界的训练集去训练，它的结果就会尽量遵循真实世界的规律。现在，你已经了解了Diffusion模型的基本规律了，这不仅仅适用于Stable Diffusion，也适用于OpenAI的Dall-E 2和Google的Imagen。</p> 
<p>注意到上面这个过程中我们暂时还没有引入文字和语义向量的控制。也就是说，如果单纯的按照上面的流程走，我们可能能得到一些很炫酷的图片，但我们<strong>没有办法去控制最后的结果到底是什么</strong>。那如何引入文字控制呢？这就要使用<strong>语言模型</strong>和<strong>Attention机制</strong>来引入语义啦，这一部分内容我们放到下篇中讲解。</p> 
<p></p> 
<h4>三、总结</h4> 
<p>在上一篇我们介绍完Stable Diffusion的各个模块和工作流程后，这一篇我们着重讲述了Diffusion是如何训练和推理的。文章的结尾，这里简单总结一下这篇的内容：</p> 
<ol><li>Diffusion's Training: 利用 <strong>“噪声强度、噪声图、加噪后图片”</strong>组成训练集，训练unet，使其学习如何从加噪后的图片推断出所加的噪声。</li><li>Diffusion's Inference: 利用训练好的unet，<strong>从纯噪声中一步一步去噪</strong>，得到合理正常的图片。</li></ol> 
<p>至于语义信息是如何控制生成图片的过程的，我们就留到下篇中再做叙述啦。下篇文章可以点击这里阅读：</p> 
<p><a href="https://zhuanlan.zhihu.com/p/598070109" rel="nofollow" title="曾飞飞：零基础读懂Stable Diffusion（III）：怎么控制252 赞同 · 38 评论文章正在上传…重新上传取消">曾飞飞：零基础读懂Stable Diffusion（III）：怎么控制252 赞同 · 38 评论文章正在上传…重新上传取消</a></p> 
<p>最后的最后，码字不易，喜欢的话可以点个赞或者收藏，作者会很开心的。后续作者也会更新更多关于深度学习领域的内容，感兴趣的话也可以关注一下哦～</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/5d2154da6a73dfa7076c6a565a5b4a9b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">PyTorch学习笔记07——模型的保存和加载</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7165759818cd35506f06be30e1095191/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">人工智能中的顶级期刊</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>