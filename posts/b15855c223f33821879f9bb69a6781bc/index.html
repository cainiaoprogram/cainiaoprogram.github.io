<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>PyTorch教程（5）激活函数(后记） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="PyTorch教程（5）激活函数(后记）" />
<meta property="og:description" content="之前的文章已经讲了很多，下面我们来深入讲解激活函数：
放大看一下：
相关激活函数的导数：
激活函数案例 假设你想尝试各种激活函数，来找出哪个激活函数是最好的。会怎么做呢?通常我们执行超参数优化——这可以使用scikit-learn的GridSearchCV函数来完成。但是我们想要比较，所以我们选择一些超参数并保持它们不变，同时改变激活函数。
让我给你们简单介绍一下，我在这里要做的：
使用不同上网激活函数训练相同的神经网络神经模型利用每个激活函数的结果，绘制一个损失和准确性图。 我们从导入我们所需要的一切开始。注意这里使用了4个库;Tensorflow, numpy, matplotlib和keras。
导入相关库 import tensorflow as tf import numpy as np import matplotlib.pyplot as plt from keras.datasets import mnist from keras.utils.np_utils import to_categorical from keras.models import Sequential from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Activation, LeakyReLU from keras.layers.noise import AlphaDropout from keras.utils.generic_utils import get_custom_objects from keras import backend as K from keras.optimizers import Adam 导入数据 从这里开始，我们想要加载一个数据集来运行这个实验;让我们选择MNIST数据集。我们可以直接从Keras导入。
(x_train,y_train),(x_test,y_test)=mnist.load_data() 数据预处理 这很好，但我们想对数据进行预处理，使其标准化。我们通过使用许多函数来实现这一点，主要是.reshape图像并除以/= 255，即最大RGB值。最后，我们用to_categorical()对数据进行one-hot编码。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/b15855c223f33821879f9bb69a6781bc/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-08-17T14:03:53+08:00" />
<meta property="article:modified_time" content="2021-08-17T14:03:53+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">PyTorch教程（5）激活函数(后记）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>之前的<a href="https://blog.csdn.net/weixin_43229348/article/details/119353266?spm=1001.2014.3001.5501">文章</a>已经讲了很多，下面我们来深入讲解激活函数：<br> <img src="https://images2.imgbox.com/bb/c4/mhMadQri_o.gif" alt="请添加图片描述"><br> 放大看一下：<br> <img src="https://images2.imgbox.com/39/9d/VoVUsrQc_o.gif" alt="请添加图片描述"><br> 相关激活函数的导数：<br> <img src="https://images2.imgbox.com/89/9c/bk9uUlOB_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_7"></a>激活函数案例</h3> 
<p>假设你想尝试各种激活函数，来找出哪个激活函数是最好的。会怎么做呢?通常我们执行超参数优化——这可以使用scikit-learn的GridSearchCV函数来完成。但是我们想要比较，所以我们选择一些超参数并保持它们不变，同时改变激活函数。</p> 
<p>让我给你们简单介绍一下，我在这里要做的：</p> 
<ol><li>使用不同上网激活函数训练相同的神经网络神经模型</li><li>利用每个激活函数的结果，绘制一个损失和准确性图。</li></ol> 
<p>我们从导入我们所需要的一切开始。注意这里使用了4个库;Tensorflow, numpy, matplotlib和keras。</p> 
<ol><li>导入相关库</li></ol> 
<pre><code class="prism language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">from</span> keras<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> mnist
<span class="token keyword">from</span> keras<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>np_utils <span class="token keyword">import</span> to_categorical
<span class="token keyword">from</span> keras<span class="token punctuation">.</span>models <span class="token keyword">import</span> Sequential
<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Dense<span class="token punctuation">,</span> Dropout<span class="token punctuation">,</span> Flatten<span class="token punctuation">,</span> Conv2D<span class="token punctuation">,</span> MaxPooling2D<span class="token punctuation">,</span> Activation<span class="token punctuation">,</span> LeakyReLU
<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>noise <span class="token keyword">import</span> AlphaDropout
<span class="token keyword">from</span> keras<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>generic_utils <span class="token keyword">import</span> get_custom_objects
<span class="token keyword">from</span> keras <span class="token keyword">import</span> backend <span class="token keyword">as</span> K
<span class="token keyword">from</span> keras<span class="token punctuation">.</span>optimizers <span class="token keyword">import</span> Adam
</code></pre> 
<ol start="2"><li>导入数据</li></ol> 
<p>从这里开始，我们想要加载一个数据集来运行这个实验;让我们选择MNIST数据集。我们可以直接从Keras导入。</p> 
<pre><code class="prism language-python"><span class="token punctuation">(</span>x_train<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span>x_test<span class="token punctuation">,</span>y_test<span class="token punctuation">)</span><span class="token operator">=</span>mnist<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<ol start="3"><li>数据预处理</li></ol> 
<p>这很好，但我们想对数据进行预处理，使其标准化。我们通过使用许多函数来实现这一点，主要是.reshape图像并除以/= 255，即最大RGB值。最后，我们用to_categorical()对数据进行one-hot编码。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">preprocess_mnist</span><span class="token punctuation">(</span>x_train<span class="token punctuation">,</span>y_train<span class="token punctuation">,</span>x_test<span class="token punctuation">,</span>y_test<span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token comment"># 将所有图像reshape为28*28</span>
	x_train<span class="token operator">=</span>x_train<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>x_train<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">28</span><span class="token punctuation">,</span><span class="token number">28</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
	x_test<span class="token operator">=</span>x_test<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>x_test<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">28</span><span class="token punctuation">,</span><span class="token number">28</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
	input_shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">28</span><span class="token punctuation">,</span><span class="token number">28</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
	
	<span class="token comment"># 将数据转为float类型</span>
	x_train<span class="token operator">=</span>x_train<span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">"float32"</span><span class="token punctuation">)</span>
	x_test<span class="token operator">=</span>x_test<span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">"float32"</span><span class="token punctuation">)</span>
	
	<span class="token comment"># 归一化</span>
	x_train<span class="token operator">/=</span><span class="token number">255</span><span class="token punctuation">.</span>
	x_test<span class="token operator">/=</span><span class="token number">255</span><span class="token punctuation">.</span>
	<span class="token comment"># one-hot编码</span>
	y_train<span class="token operator">=</span>to_categorical<span class="token punctuation">(</span>y_train<span class="token punctuation">)</span>
	y_test<span class="token operator">=</span>to_categorical<span class="token punctuation">(</span>y_test<span class="token punctuation">)</span>
	<span class="token keyword">return</span> x_train<span class="token punctuation">,</span>y_train<span class="token punctuation">,</span>x_test<span class="token punctuation">,</span>y_test<span class="token punctuation">,</span>input_shape

x_train<span class="token punctuation">,</span>y_train<span class="token punctuation">,</span>x_test<span class="token punctuation">,</span>y_test<span class="token punctuation">,</span>input_shape<span class="token operator">=</span>preprocess_mnist<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span>y_train<span class="token punctuation">,</span>x_test<span class="token punctuation">,</span>y_test<span class="token punctuation">)</span>
</code></pre> 
<ol start="4"><li>构建模型</li></ol> 
<p>现在我们已经对数据进行了预处理，现在可以构建模型并定义Keras要运行的一些东西了。让我们从卷积神经网络模型本身开始。对于SELU激活函数，我们有一个特殊的情况，我们需要使用kernel初始化器lecun_normal和dropout的特殊形式AlphaDropout()。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">build_cnn</span><span class="token punctuation">(</span>activation<span class="token punctuation">,</span> dropout_rate<span class="token punctuation">,</span> optimizer<span class="token punctuation">)</span><span class="token punctuation">:</span>
    model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>activation <span class="token operator">==</span> <span class="token string">"selu"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> activation<span class="token operator">=</span>activation<span class="token punctuation">,</span> input_shape<span class="token operator">=</span>input_shape<span class="token punctuation">,</span>
                         kernel_initializer<span class="token operator">=</span><span class="token string">"lecun_normal"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> activation<span class="token operator">=</span>activation<span class="token punctuation">,</span> kernel_initializer<span class="token operator">=</span><span class="token string">"lecun_normal"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>MaxPooling2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>AlphaDropout<span class="token punctuation">(</span><span class="token number">0.25</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> activation<span class="token operator">=</span>activation<span class="token punctuation">,</span> kernel_initializer<span class="token operator">=</span><span class="token string">"lecun_normal"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>AlphaDropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">"softmax"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> activation<span class="token operator">=</span>activation<span class="token punctuation">,</span> input_shape<span class="token operator">=</span>input_shape<span class="token punctuation">)</span><span class="token punctuation">)</span>
        model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> activation<span class="token operator">=</span>activation<span class="token punctuation">)</span><span class="token punctuation">)</span>
        model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>MaxPooling2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>AlphaDropout<span class="token punctuation">(</span><span class="token number">0.25</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> activation<span class="token operator">=</span>activation<span class="token punctuation">)</span><span class="token punctuation">)</span>
        model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>AlphaDropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">"softmax"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span><span class="token string">"binary_crossentropy"</span><span class="token punctuation">,</span> optimizer<span class="token operator">=</span>optimizer<span class="token punctuation">,</span> metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"accuracy"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> model	
</code></pre> 
<ol start="5"><li>构建GELU激活函数</li></ol> 
<p>GELU 函数在 Keras 中尚不存在。但是向 Keras 添加新的激活函数非常容易。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">gelu</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token number">0.5</span> <span class="token operator">*</span> x <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> tf<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>pi<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>x <span class="token operator">+</span> <span class="token number">0.044715</span> <span class="token operator">*</span> tf<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 添加gelu，这样我们就可以将其作为字符串使用</span>
get_custom_objects<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token string">'gelu'</span><span class="token punctuation">:</span> Activation<span class="token punctuation">(</span>gelu<span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token punctuation">)</span>

<span class="token comment"># 添加leaky-relu，这样我们就可以将其作为字符串使用</span>
get_custom_objects<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token string">"leaky-relu"</span><span class="token punctuation">:</span> Activation<span class="token punctuation">(</span>LeakyReLU<span class="token punctuation">(</span>alpha<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token punctuation">)</span>

act_func <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"sigmoid"</span><span class="token punctuation">,</span> <span class="token string">"relu"</span><span class="token punctuation">,</span> <span class="token string">"elu"</span><span class="token punctuation">,</span> <span class="token string">"leaky-relu"</span><span class="token punctuation">,</span> <span class="token string">"selu"</span><span class="token punctuation">,</span> <span class="token string">"gelu"</span><span class="token punctuation">]</span>
</code></pre> 
<ol start="6"><li>训练</li></ol> 
<p>现在，我们准备使用在act_func数组中定义的不同激活函数来训练模型。我们对每个激活函数运行一个简单的for循环，并将其结果添加到一个数组中。</p> 
<pre><code class="prism language-python">result<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> activation <span class="token keyword">in</span> act_func<span class="token punctuation">:</span>
	<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\nTraining with --&gt;{0}&lt;-- activation function\n"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>activation<span class="token punctuation">)</span><span class="token punctuation">)</span>
	model<span class="token operator">=</span>build_cnn<span class="token punctuation">(</span>activation<span class="token operator">=</span>activation<span class="token punctuation">,</span>dropout_rate<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span>optimizer<span class="token operator">=</span>Adam<span class="token punctuation">(</span>clipvalue<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
	history<span class="token operator">=</span>model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> validation_split<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span>epochs<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span>verbose<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>validation_data<span class="token operator">=</span><span class="token punctuation">(</span>x_test<span class="token punctuation">,</span>y_test<span class="token punctuation">)</span><span class="token punctuation">)</span>
	result<span class="token punctuation">.</span>append<span class="token punctuation">(</span>history<span class="token punctuation">)</span>
	K<span class="token punctuation">.</span>clear_session<span class="token punctuation">(</span><span class="token punctuation">)</span>
	<span class="token keyword">del</span> model
<span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span>
</code></pre> 
<ol start="7"><li>绘制结果</li></ol> 
<p>由此，我们可以绘制从model.fit()中获得的每个激活函数的结果。<br> 现在我们已经准备好绘制数据了，我使用matplotlib编写了一些简短的代码：</p> 
<pre><code class="prism language-python">new_act_arr<span class="token operator">=</span>act_func<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
new_results<span class="token operator">=</span>result<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
<span class="token keyword">def</span> <span class="token function">plot_act_func_results</span><span class="token punctuation">(</span>results<span class="token punctuation">,</span>activation_functions<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
	plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
	<span class="token comment"># 绘制验证准确率</span>
	<span class="token keyword">for</span> act_func <span class="token keyword">in</span> results<span class="token punctuation">:</span>
		plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>act_func<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">"val_acc"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
	plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"Model Accuracy"</span><span class="token punctuation">)</span>
	plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"Test Accuracy"</span><span class="token punctuation">)</span>
	plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">"Epoch"</span><span class="token punctuation">)</span>
	plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>activation_functions<span class="token punctuation">)</span>
	plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
	<span class="token comment"># 绘制验证集损失值</span>
	plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
	<span class="token keyword">for</span> act_func <span class="token keyword">in</span> results<span class="token punctuation">:</span>
		plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>act_func<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">"val_loss"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
	plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"Model Loss"</span><span class="token punctuation">)</span>
	plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"Test Loss"</span><span class="token punctuation">)</span>
	plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">"Epoch"</span><span class="token punctuation">)</span>
	plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>activation_functions<span class="token punctuation">)</span>
	plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
plot_act_func_results<span class="token punctuation">(</span>new_results<span class="token punctuation">,</span>new_act_arr<span class="token punctuation">)</span>
	
</code></pre> 
<p><img src="https://images2.imgbox.com/d1/3d/azE6knfC_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/f4/fb/qiQ2iLdG_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_156"></a>激活函数解析</h3> 
<ol><li>sigmoid激活函数<br> <img src="https://images2.imgbox.com/4b/99/5gQuV2eM_o.png" alt="在这里插入图片描述"></li></ol> 
<p><img src="https://images2.imgbox.com/08/27/ifmqWVD9_o.png" alt="在这里插入图片描述"><br> 导数为：<br> <img src="https://images2.imgbox.com/0a/de/ZPYVuvTT_o.png" alt="在这里插入图片描述"><br> 优点：<br> （1）便于求导的平滑函数；<br> （2）能压缩数据，保证数据幅度不会有问题；<br> 缺点：<br> （1）容易出现梯度消失（gradient vanishing）的现象：当激活函数接近饱和区时，变化太缓慢，导数接近0，根据后向传递的数学依据是微积分求导的链式法则，当前导数需要之前各层导数的乘积，几个比较小的数相乘，导数结果很接近0，从而无法完成深层网络的训练。<br> （2）Sigmoid的输出不是0均值（zero-centered）的：这会导致后层的神经元的输入是非0均值的信号，这会对梯度产生影响。以 f=sigmoid(wx+b)为例， 假设输入均为正数（或负数），那么对w的导数总是正数（或负数），这样在反向传播过程中要么都往正方向更新，要么都往负方向更新，导致有一种捆绑效果，使得收敛缓慢。<br> （3）幂运算相对耗时</p> 
<ol start="2"><li> <p>tanh激活函数<br> <img src="https://images2.imgbox.com/00/b7/ez25Zh3k_o.png" alt="在这里插入图片描述"><br> tanh 是对 sigmoid 的平移和收缩: tanh(x)=2⋅σ(2x)−1<br> <img src="https://images2.imgbox.com/41/97/5FzCLikX_o.png" alt="在这里插入图片描述"><br> tanh 作为激活函数的特点：<br> 相比 Sigmoid 函数，<br> （1） tanh 的输出范围时(-1, 1)， 解决了 Sigmoid 函数的不是 zero-centered 输出问题；<br> （2） 幂运算的问题仍然存在；<br> （3） tanh 导数范围在(0, 1)之间， 相比 sigmoid 的(0, 0.25)， 梯度消失（ gradient vanishing） 问题会得到缓解， 但仍然还会存在。<br> （4）以零为中心的影响：如果当前参数(w0,w1)的最佳优化方向是(+d0, -d1),则根据反向传播计算公式,我们希望 x0 和 x1 符号相反。但是如果上一级神经元采用 Sigmoid 函数作为激活函数，sigmoid不以0为中心，输出值恒为正，那么我们无法进行最快的参数更新，而是走 Z 字形逼近最优解。</p> </li><li> <p>relu激活函数<br> <img src="https://images2.imgbox.com/5a/7b/3jhH4HY8_o.png" alt="在这里插入图片描述"></p> </li></ol> 
<p><img src="https://images2.imgbox.com/6f/46/meIN4pbC_o.png" alt="ReLU激活函数"><br> <img src="https://images2.imgbox.com/ca/4a/0R9mZ037_o.png" alt="ReLU求导"><br> 优点： 相比于 sigmoid， 由于稀疏性， 时间和空间复杂度更低； 不涉及成本更高的指数运算； 能避免梯度消失问题。<br> 缺点： 引入了死亡 ReLU 问题， 即网络的大部分分量都永远不会更新。 但这有时候也是一个优势； ReLU 不能避免梯度爆炸问题。<br> 如果在计算梯度时有太多值都低于 0 会怎样呢？ 我们会得到相当多不会更新的权重和偏置， 因为其更新的量为 0。</p> 
<ol start="4"><li> <p>elu激活函数<br> <img src="https://images2.imgbox.com/0e/d4/wsCY0hfz_o.png" alt="在这里插入图片描述"><br> ELU函数如下图所示<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           α 
          
         
        
          \alpha 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span></span></span></span></span>值为0.2。<br> <img src="https://images2.imgbox.com/83/e4/RwWw7F4f_o.png" alt="ELU激活函数"><br> ELU的导数为：<br> <img src="https://images2.imgbox.com/8d/68/rbQsVT8M_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/65/88/o7Lv99Q7_o.png" alt="ELU求导"><br> 优点： 能避免死亡 ReLU 问题； 能得到负值输出， 这能帮助网络向正确的方向推动权重和偏置变化； 在计算梯度时能得到激活， 而不是让它们等于 0。<br> 缺点： 由于包含指数运算， 所以计算时间更长； 无法避免梯度爆炸问题； 神经网络不学习α值。</p> </li><li> <p>leaky-relu激活函数<br> <img src="https://images2.imgbox.com/e0/84/zQDvwGet_o.png" alt="在这里插入图片描述"><br> Leaky ReLU画在这里，假设<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           α 
          
         
        
          \alpha 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span></span></span></span></span>是0.2:<br> <img src="https://images2.imgbox.com/ef/db/fqmiRuYS_o.png" alt="Leaky ReLU"><br> LReLU的导数为：<br> <img src="https://images2.imgbox.com/44/50/tTQcEwEV_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/d7/05/AKvhDZeC_o.png" alt="Leaky ReLU求导"><br> 优点： 类似 ELU， Leaky ReLU 也能避免死亡 ReLU 问题， 因为其在计算导数时允许较小的梯度； 由于不包含指数运算， 所以计算速度比 ELU 快。<br> 缺点： 无法避免梯度爆炸问题； 神经网络不学习 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           α 
          
         
        
          \alpha 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span></span></span></span></span> 值； 在微分时， 两部分都是线性的； 而 ELU 的一部分是线性的， 一部分是非线性的。</p> </li><li> <p>selu激活函数<br> <img src="https://images2.imgbox.com/03/72/2nsYwm6u_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/14/95/p6KfWOP1_o.png" alt="在这里插入图片描述"><br> 如果输入值 x 大于 0， 则输出值为 x 乘以 λ； 如果输入值 x 小于 0， 则会得到一个奇异函数—<br> —它随 x 增大而增大并趋近于 x 为 0 时的值 0.0848。 本质上看， 当 x 小于 0 时， 先用<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           α 
          
         
        
          \alpha 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span></span></span></span></span>乘以 x 值的指数， 再减去 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           α 
          
         
        
          \alpha 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span></span></span></span></span>， 然后乘以 λ 值。<br> <img src="https://images2.imgbox.com/9a/08/HRT2iHft_o.png" alt="SELU"><br> SELU的导数为：<br> <img src="https://images2.imgbox.com/d4/b3/qQnhrlow_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ad/09/3JODgnFd_o.png" alt="SELU求导"></p> </li></ol> 
<p>SELU 激活能够对神经网络进行自归一化（self-normalizing） 。 这是什么意思？<br> 首先， 我们先看看什么是归一化（ normalization） 。 简单来说， 归一化首先是减去均值， 然后除以标准差。 因此， 经过归一化之后， 网络的组（权重、 偏置和激活） 的均值为 0， 标准差为 1。 而这正是 SELU 激活函数的输出值。均值为 0 且标准差为 1 又如何呢？ 在初始化函数为 lecun_normal 的假设下， 网络参数会被初始化一个正态分布（或高斯分布） ， 然后在SELU 的情况下， 网络会在论文中描述的范围内完全地归一化。 本质上看，当乘或加这样的网络分量时， 网络仍被视为符合高斯分布。 我们就称之为归一化。 反过来， 这又意味着整个网络及其最后一层的输出也是归一化的。</p> 
<p>注意实际应用这个激活函数时， 必须使用 lecun_normal 进行权重初始化。 如果希望应用 dropout， 则应当使用AlphaDropout。<br> 优点： 内部归一化的速度比外部归一化快， 这意味着网络能更快收敛； 不可能出现梯度消失或爆炸问题。<br> 缺点： 这个激活函数相对较新——需要更多论文比较性地探索其在 CNN 和 RNN 等架构中应用。</p> 
<ol start="7"><li>gelu激活函数<br> <img src="https://images2.imgbox.com/b2/e2/C82bYPOM_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/72/6e/DhkBgQIt_o.png" alt="GELU"><br> <img src="https://images2.imgbox.com/b6/53/aDJt3hUn_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/aa/ec/T9gboNRF_o.png" alt="GELU求导"><br> 优点： 似乎是 NLP 领域的当前最佳； 尤其在 Transformer 模型中表现最好；能避免梯度消失问题。<br> 缺点： 尽管是 2016 年提出的， 但在实际应用中还是一个相当新颖的激活函数。</li></ol> 
<h3><a id="_236"></a>为什么深度神经网络很难训练</h3> 
<p>在训练深度神经网络时，你可能会遇到两个挑战。</p> 
<ol><li> <p>梯度消失<br> 与sigmoid函数类似，某些激活函数将输入空间压缩到0到1之间的小输出空间。sigmoid函数的输入发生很大的变化，输出仅发生很小的变化。因此，导数变得很小。对于只有几层使用这些激活的浅层网络来说，这不是一个大问题。然而，当使用更多层时，它可能会导致梯度太小，训练无法有效地工作。</p> </li><li> <p>梯度爆炸<br> 梯度爆炸是指在训练过程中显著的误差梯度累积并导致神经网络模型权值进行非常大的更新的问题。<br> 当梯度呈爆炸式增长，学习无法完成时，网络就会变得不稳定。<br> 权值也可能变得非常大，以至于溢出并产生NaN值。<br> 梯度爆炸需要采用梯度裁剪、BN、设置较小学习率等方式解决。</p> </li><li> <p>解决方法<br> a. 解决梯度消失需要考虑几个方面:<br> 1）权重初始化<br> 使用合适的方式初始化权重, 如ReLU使用MSRA的初始化方式, tanh使用xavier初始化方式.<br> 2） 激活函数选择<br> 激活函数要选择ReLU等梯度累乘稳定的.<br> 3）学习率<br> 一种训练优化方式是对输入做白化操作(包括正规化和去相关), 目的是可以选择更大的学习率。现代深度学习网络中常使用Batch Normalization(包括正规化步骤,但不含去相关)。<br> b. 梯度爆炸需要采用梯度裁剪、BN、设置较小学习率等方式解决。</p> </li></ol> 
<h3><a id="_257"></a>梯度函数的选择</h3> 
<p>您需要根据您正在解决的预测问题的类型(具体地说，预测变量的类型)匹配输出层的激活函数。<br> 根据经验，您可以从使用ReLU激活函数开始，如果ReLU不能提供最佳结果，则可以转移到其他激活函数。<br> 这里有一些其他的指导方针来帮助你。<br> 1）ReLU激活函数只能在隐藏层中使用。<br> 2）Sigmoid/Logistic和Tanh激活函数不应该在隐藏层中使用，因为它们会使模型在训练过程中更容易出现问题(由于梯度消失)。<br> 3）Swish函数用于深度大于40层的神经网络。</p> 
<p>最后，根据你要解决的预测问题的类型，为你的输出层选择激活函数的一些规则:<br> 1）回归问题：线性激活函数<br> 2）二分类：Sigmoid/Logistic激活函数<br> 3）多分类：Softmax激活函数<br> 4）多标签分类：Sigmoid激活函数<br> 隐层中使用的激活函数通常是根据神经网络结构的类型来选择的。<br> 5）卷积神经网络：ReLU激活函数<br> 6）循环神经网络：Tanh或者Sigmoid激活函数</p> 
<h3><a id="_273"></a>参考目录</h3> 
<p><a href="https://mlfromscratch.com/activation-functions-explained/#the-good-and-the-bad-properties-of-dead-relus" rel="nofollow">https://mlfromscratch.com/activation-functions-explained/#the-good-and-the-bad-properties-of-dead-relus</a><br> <a href="https://www.cnblogs.com/makefile/p/activation-function.html" rel="nofollow">https://www.cnblogs.com/makefile/p/activation-function.html</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/3849a3ac317a974762d927c858017b63/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">各厂商防火墙初始登录IP及密码信息</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/036dc8f024aa07a511a88814c50f0d3f/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">信号完整性之S参数（八）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>