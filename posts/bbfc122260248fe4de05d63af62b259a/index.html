<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>基于diffusion扩散模型/GAN生成对抗方法 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="基于diffusion扩散模型/GAN生成对抗方法" />
<meta property="og:description" content="1、DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation 大型文本生成图像模型已取得显著进展，有能力从给定的文本提示中生成高质量和多样化的图像。然而，给定目标个体的一些参考图片（这里不妨称之为“主题”），这些模型还无法做到的是，在不同的上下文环境里去生成关于它们不同图片的能力。
DreamBooth是一种新的文本到图像扩散模型的“个性化”方法。给定一个主题的几张图像作为输入，对预训练的文本到图像模型进行微调，使其学会将一个唯一标识符（identifier）绑定到特定的主题。一旦主题被嵌入到模型的输出域中，唯一的标识符就可以用来生成不同场景下关于主题的新颖逼真图像。通过利用模型中嵌入的语义先验和一种类特定先验保留损失，能够在参考图像中没有出现的不同场景、姿势、视图和光照条件下合成主题。
在保留主题关键特征的同时，应用于主题重新背景化、文本引导的视图合成和艺术渲染等任务。此外，研究者还为这个新的主题驱动生成任务提供了一个新的数据集和评估协议。项目页面:https://dreambooth.github.io/
2、Ablating Concepts in Text-to-Image Diffusion Models 大规模的文本到图像扩散模型可以生成高保真图像。模型通常是根据大量互联网数据进行训练的，这些数据通常包含受版权保护的材料、许可图像和个人照片。此外，它们被发现可以复制各种艺术家的风格或记住准确的训练样本。如何在不从头开始重新训练模型的情况下删除此类受版权保护的概念或图像？
为了实现这一目标，提出了一种在预训练模型中消除概念的有效方法，即防止目标概念的生成。算法将希望消融的目标样式、实例或文本提示的图像分布与锚概念对应的分布相匹配。这可以防止模型在给定文本条件的情况下生成目标概念。大量实验表明，方法可以成功地防止消除概念的产生，同时在模型中保留密切相关的概念。
3、Multi-Concept Customization of Text-to-Image Diffusion 生成模型从大型数据库中学习到概念，但用户通常希望合成他们自己的概念（例如，他们的家人、宠物或物品）的实例。可以教一个模型快速获得一个新概念吗？可以将多个新概念组合在一起吗？
本文提出了 Custom Diffusion，一种增强现有文本到图像模型的有效方法。仅优化文本到图像调节机制中的几个参数就足以代表新概念，同时实现快速调整（∼ 6 分钟）。
此外，可以联合训练多个概念，或通过闭式约束优化将多个微调模型组合成一个。经过微调的模型生成多个新概念的变体，并将它们与新设置中的现有概念无缝组合。方法在定性和定量评估方面优于多个基线和并行工作，同时具有内存和计算效率。
4、Imagic: Text-Based Real Image Editing with Diffusion Models 以文本为条件的图像编辑最近引起了相当大的兴趣。然而，目前大多数方法要么仅限于特定的编辑类型（例如，目标融合叠加、风格迁移），要么适用于合成生成的图像，或者需要一个公共对象的多个输入图像。
本文首次展示了将复杂（例如，非刚性）文本引导语义编辑应用于单个真实图像的能力。例如，可以改变图像中一个或多个对象的姿势和构图，同时保留其原始特征。方法可以让站立的狗坐下或跳跃，让鸟张开翅膀等等——每一个都在用户提供的单个高分辨率自然图像中。
与以前的工作相反，提出的方法只需要一个输入图像和一个目标文本（所需的编辑）。使用真实图像，不需要任何额外的输入（例如图像掩码或对象的额外视图）。方法称之为“Imagic”，利用预训练的文本到图像扩散模型来完成这项任务。它生成与输入图像和目标文本对齐的文本嵌入，同时微调扩散模型以捕获特定于图像的外观。
在来自不同领域的大量输入上展示了方法的质量和多功能性，展示了大量高质量的复杂语义图像编辑。
https://imagic-editing.github.io/
5、Shifted Diffusion for Text-to-image Generation 本文提出了一种新的文本到图像生成方法Corgi。Corgi基于本文出的shifted扩散模型，从输入文本中实现了更好的图像特征嵌入生成。与在DALL-E 2中使用的基线扩散模型不同，方法通过设计新的初始化分布和新的扩散步骤，无缝地编码预训练的CLIP模型在扩散过程中的先验知识。
与强DALL-E 2基线相比，方法在从文本生成图像嵌入方面的效率和有效性都更好，从而获得更好的文本到图像生成。进行了大量的大规模实验，从定量测量和人工评价两方面进行了评价，表明方法比现有方法具有更强的生成能力。
此外，模型支持半监督和无语言的文本到图像生成训练，其中训练数据集中只有部分或没有图像具有相关的文本描述。半监督模型在只有1.7%的图像被配上文本的情况下进行训练，在MS-COCO上评估零镜头文本到图像生成时，得到的FID结果与DALL-E 2相当。Corgi还在下游无语言文本到图像生成任务的不同数据集上获得了最新的结果，大大超过了之前的Lafite方法。
6、SpaText: Spatio-Textual Representation for Controllable Image Generation 最近的文本到图像扩散模型能够以前所未有的质量生成令人信服的结果。然而，当前方法无法以精细控制不同区域/对象的形状或它们的布局。以前提供此类的尝试，却因依赖标签而有所受限制。
为此，本文提出了 SpaText，一种使用开放式词汇场景控制、进行文本到图像生成的新方法。除了描述整个场景的全局文本外，用户还提供了一个分割图，其中每个感兴趣的区域都用自由形式的自然语言描述进行了注释。由于缺乏对图像中每个区域进行详细文本描述的大规模数据集，选择利用当前的大规模文本到图像数据集，并将方法基于一种新的基于 CLIP 的空间文本表示，并展示其在两种最先进的扩散模型上的有效性：基于像素和基于潜在。
此外，展示了如何将扩散模型中的无分类器指导方法扩展到多条件情况，并提出了一种替代加速推理算法。最后，除了 FID 分数和用户研究之外，还提供了几个自动评估指标并评估方法。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/bbfc122260248fe4de05d63af62b259a/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-25T16:19:50+08:00" />
<meta property="article:modified_time" content="2023-04-25T16:19:50+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">基于diffusion扩散模型/GAN生成对抗方法</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h4>1、DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</h4> 
<ul><li> <p>大型文本生成图像模型已取得显著进展，有能力从给定的文本提示中生成高质量和多样化的图像。然而，给定目标个体的一些参考图片（这里不妨称之为“主题”），这些模型还无法做到的是，在不同的上下文环境里去生成关于它们不同图片的能力。</p> </li><li> <p>DreamBooth是一种新的文本到图像扩散模型的“个性化”方法。给定一个主题的几张图像作为输入，对预训练的文本到图像模型进行微调，使其学会将一个唯一标识符（identifier）绑定到特定的主题。一旦主题被嵌入到模型的输出域中，唯一的标识符就可以用来生成不同场景下关于主题的新颖逼真图像。通过利用模型中嵌入的语义先验和一种类特定先验保留损失，能够在参考图像中没有出现的不同场景、姿势、视图和光照条件下合成主题。</p> </li><li> <p>在保留主题关键特征的同时，应用于主题重新背景化、文本引导的视图合成和艺术渲染等任务。此外，研究者还为这个新的主题驱动生成任务提供了一个新的数据集和评估协议。项目页面:https://dreambooth.github.io/</p> </li></ul> 
<h4><img alt="" height="303" src="https://images2.imgbox.com/da/02/avAqto9C_o.png" width="826"> 2、Ablating Concepts in Text-to-Image Diffusion Models</h4> 
<ul><li> <p>大规模的文本到图像扩散模型可以生成高保真图像。模型通常是根据大量互联网数据进行训练的，这些数据通常包含受版权保护的材料、许可图像和个人照片。此外，它们被发现可以复制各种艺术家的风格或记住准确的训练样本。如何在不从头开始重新训练模型的情况下删除此类受版权保护的概念或图像？</p> </li><li> <p>为了实现这一目标，提出了一种在预训练模型中消除概念的有效方法，即防止目标概念的生成。算法将希望消融的目标样式、实例或文本提示的图像分布与锚概念对应的分布相匹配。这可以防止模型在给定文本条件的情况下生成目标概念。大量实验表明，方法可以成功地防止消除概念的产生，同时在模型中保留密切相关的概念。</p> </li></ul> 
<p><img alt="" height="403" src="https://images2.imgbox.com/f4/a8/9VqxW0Xn_o.png" width="823"></p> 
<h4>3、Multi-Concept Customization of Text-to-Image Diffusion</h4> 
<ul><li> <p>生成模型从大型数据库中学习到概念，但用户通常希望合成他们自己的概念（例如，他们的家人、宠物或物品）的实例。可以教一个模型快速获得一个新概念吗？可以将多个新概念组合在一起吗？</p> </li><li> <p>本文提出了 Custom Diffusion，一种增强现有文本到图像模型的有效方法。仅优化文本到图像调节机制中的几个参数就足以代表新概念，同时实现快速调整（∼ 6 分钟）。</p> </li><li> <p>此外，可以联合训练多个概念，或通过闭式约束优化将多个微调模型组合成一个。经过微调的模型生成多个新概念的变体，并将它们与新设置中的现有概念无缝组合。方法在定性和定量评估方面优于多个基线和并行工作，同时具有内存和计算效率。</p> </li></ul> 
<p><img alt="" height="395" src="https://images2.imgbox.com/8c/b4/zktIxtbw_o.png" width="831"></p> 
<h4>4、Imagic: Text-Based Real Image Editing with Diffusion Models</h4> 
<ul><li> <p>以文本为条件的图像编辑最近引起了相当大的兴趣。然而，目前大多数方法要么仅限于特定的编辑类型（例如，目标融合叠加、风格迁移），要么适用于合成生成的图像，或者需要一个公共对象的多个输入图像。</p> </li><li> <p>本文首次展示了将复杂（例如，非刚性）文本引导语义编辑应用于单个真实图像的能力。例如，可以改变图像中一个或多个对象的姿势和构图，同时保留其原始特征。方法可以让站立的狗坐下或跳跃，让鸟张开翅膀等等——每一个都在用户提供的单个高分辨率自然图像中。</p> </li><li> <p>与以前的工作相反，提出的方法只需要一个输入图像和一个目标文本（所需的编辑）。使用真实图像，不需要任何额外的输入（例如图像掩码或对象的额外视图）。方法称之为“Imagic”，利用预训练的文本到图像扩散模型来完成这项任务。它生成与输入图像和目标文本对齐的文本嵌入，同时微调扩散模型以捕获特定于图像的外观。</p> </li><li> <p>在来自不同领域的大量输入上展示了方法的质量和多功能性，展示了大量高质量的复杂语义图像编辑。</p> </li><li> <p>https://imagic-editing.github.io/</p> </li></ul> 
<p><img alt="" height="411" src="https://images2.imgbox.com/38/29/wUlyEetZ_o.png" width="820"></p> 
<h4>5、Shifted Diffusion for Text-to-image Generation</h4> 
<ul><li> <p>本文提出了一种新的文本到图像生成方法Corgi。Corgi基于本文出的shifted扩散模型，从输入文本中实现了更好的图像特征嵌入生成。与在DALL-E 2中使用的基线扩散模型不同，方法通过设计新的初始化分布和新的扩散步骤，无缝地编码预训练的CLIP模型在扩散过程中的先验知识。</p> </li><li> <p>与强DALL-E 2基线相比，方法在从文本生成图像嵌入方面的效率和有效性都更好，从而获得更好的文本到图像生成。进行了大量的大规模实验，从定量测量和人工评价两方面进行了评价，表明方法比现有方法具有更强的生成能力。</p> </li><li> <p>此外，模型支持半监督和无语言的文本到图像生成训练，其中训练数据集中只有部分或没有图像具有相关的文本描述。半监督模型在只有1.7%的图像被配上文本的情况下进行训练，在MS-COCO上评估零镜头文本到图像生成时，得到的FID结果与DALL-E 2相当。Corgi还在下游无语言文本到图像生成任务的不同数据集上获得了最新的结果，大大超过了之前的Lafite方法。</p> </li></ul> 
<p><img alt="" height="611" src="https://images2.imgbox.com/15/46/VOVnfE0v_o.png" width="644"> </p> 
<h4>6、SpaText: Spatio-Textual Representation for Controllable Image Generation</h4> 
<ul><li> <p>最近的文本到图像扩散模型能够以前所未有的质量生成令人信服的结果。然而，当前方法无法以精细控制不同区域/对象的形状或它们的布局。以前提供此类的尝试，却因依赖标签而有所受限制。</p> </li><li> <p>为此，本文提出了 SpaText，一种使用开放式词汇场景控制、进行文本到图像生成的新方法。除了描述整个场景的全局文本外，用户还提供了一个分割图，其中每个感兴趣的区域都用自由形式的自然语言描述进行了注释。由于缺乏对图像中每个区域进行详细文本描述的大规模数据集，选择利用当前的大规模文本到图像数据集，并将方法基于一种新的基于 CLIP 的空间文本表示，并展示其在两种最先进的扩散模型上的有效性：基于像素和基于潜在。</p> </li><li> <p>此外，展示了如何将扩散模型中的无分类器指导方法扩展到多条件情况，并提出了一种替代加速推理算法。最后，除了 FID 分数和用户研究之外，还提供了几个自动评估指标并评估方法。</p> </li></ul> 
<p>https://omriavrahami.com/spatext/</p> 
<p><img alt="" height="446" src="https://images2.imgbox.com/af/5c/YRcKL3kN_o.png" width="809"></p> 
<h4>7、Scaling up GANs for Text-to-Image Synthesis</h4> 
<ul><li> <p>最近，文字-图像生成技术的成功已经席卷全球，激发了大众的想象力。从技术的角度来看，它也标志着设计生成图像模型所青睐的架构的巨大变化。GANs曾经是事实上的选择，有StyleGAN这样的优秀技术。随着DALL·e2的出现，自回归和扩散模型似乎一夜之间成为大规模生成模型的新标准。</p> </li><li> <p>这种快速的转变提出了一个基本问题：能否扩大GANs的规模，从像LAION这样的大型数据集中受益?本文发现，随意增加StyleGAN架构的容量很快就会变得不稳定。因此提出了一种新的GAN架构GigaGAN，它远远超过了这一限制，证明了GAN是文本到图像生成的可行选择。</p> </li><li> <p>GigaGAN有三大优势。首先，它的推理速度快了几个数量级，合成一张512px的图像只需要0.13秒。其次，它可以在3.66秒内合成高分辨率图像，例如1600万像素的图像。最后，GigaGAN支持各种潜在空间编辑应用程序，如潜在插值、风格混合和其它编辑操作。</p> </li></ul> 
<p><img alt="" height="396" src="https://images2.imgbox.com/b2/33/4Z9ryW5T_o.png" width="824"></p> 
<h4>8、GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis</h4> 
<ul><li> <p>为了同时实现生成质量高、训练高效、生成速度快，以及内容更可控的文本到图像生成模型，作者提出了 Generative Adversarial CLIPs，即 GALIP。</p> </li><li> <p>GALIP首先提出了一个基于 CLIP 的判别器。CLIP的复杂场景理解能力使判别器能够更加准确地评估复杂图像的质量。此外，还提出了一个 CLIP增强的生成器，它通过Bridge Feature和Prompts从 CLIP 中抽取有用的视觉概念。集成 CLIP 的生成器和判别器提高了对抗学习效率，使得GALIP只需要大约 3% 的训练数据和 6% 的可学习参数，仅用8张3090显卡训练3天时间，取得了与大规模预训练的自回归和扩散模型相当的结果。同时，GALIP的生成速度也快了120倍，且继承了GAN更加可控的平滑隐空间。实验结果证明GALIP的卓越性能。（https://github.com/tobran/GALIP）</p> </li></ul> 
<p> <img alt="" height="427" src="https://images2.imgbox.com/5b/40/X6a6QyVs_o.png" width="640"></p> 
<h4>9、Variational Distribution Learning for Unsupervised Text-to-Image Generation</h4> 
<ul><li> <p>当训练期间图像的文本不可用时，本文提出了一种基于深度神经网络的文本到图像生成算法。这项工作不是简单地使用现有的图像文本描述生成方法生成训练图像的伪句子，而是使用预训练的 CLIP 模型，该模型能够在联合空间中正确对齐图像和相应文本的嵌入，因此， 在零样本识别任务上效果很好。        whaosoft aiot <a href="http://143ai.com/" rel="nofollow" title="http://143ai.com">http://143ai.com</a>  </p> </li><li> <p>通过最大化以图像-文本 CLIP 嵌入对为条件的数据对数似然来优化文本到图像生成模型。为了更好地对齐两个域中的数据，采用了一种基于变分推理方法，可以有效地估计给定图像及其 CLIP 特征的隐藏文本嵌入的近似后验。实验结果证实，在无监督和半监督的文本到图像生成设置下，所提出的框架大大优于现有方法。</p> </li></ul> 
<p><img alt="" height="278" src="https://images2.imgbox.com/66/49/nrttzz7w_o.png" width="818"></p> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/57caa4649209118cbeef0f1933396fb1/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">elementUI中使用tooltip遇到的坑</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f070beaa98ea723d01a2f2f37c0f234f/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">计算机的运算方法--乘法运算</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>