<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>yolov5 提速多GPU训练显存低的问题 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="yolov5 提速多GPU训练显存低的问题" />
<meta property="og:description" content="yolov5多GPU训练显存低
修改前： 按照配置，在train.py配置如下：
运行 python train.py 后nvidia-smi 显示显存占用如下：
修改后 参考yolov5 官方中的issue中，有人提到的分布式多进程的方法：
在yolov5运行的虚拟环境下，找到torch的distributed 的环境：比如我的在conda3/envs/rcnn/lib/python3.6/site-packages/torch/distributed/；
在distributed文件下，新建多进程的脚本，命名为yolov5_launch.py：
import sys import subprocess import os from argparse import ArgumentParser, REMAINDER def parse_args(): &#34;&#34;&#34; Helper function parsing the command line options @retval ArgumentParser &#34;&#34;&#34; parser = ArgumentParser(description=&#34;PyTorch distributed training launch &#34; &#34;helper utility that will spawn up &#34; &#34;multiple distributed processes&#34;) # Optional arguments for the launch helper parser.add_argument(&#34;--nnodes&#34;, type=int, default=1, help=&#34;The number of nodes to use for distributed &#34;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/2e3ee4f5ba219f4fcf9f248dcf6b7f15/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-11-11T14:33:57+08:00" />
<meta property="article:modified_time" content="2021-11-11T14:33:57+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">yolov5 提速多GPU训练显存低的问题</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>yolov5多GPU训练显存低</p> 
<h2><a id="_1"></a>修改前：</h2> 
<p>按照配置，在train.py配置如下：<br> <img src="https://images2.imgbox.com/11/b0/JTq1CA3d_o.png" alt="gpu配置如上图"><br> 运行 python train.py 后nvidia-smi 显示显存占用如下：<br> <img src="https://images2.imgbox.com/7d/a1/C59v6Bku_o.png" alt=""></p> 
<h3><a id="_6"></a>修改后</h3> 
<p>参考yolov5 官方中的issue中，有人提到的分布式多进程的方法：</p> 
<p>在yolov5运行的虚拟环境下，找到torch的distributed 的环境：比如我的在conda3/envs/rcnn/lib/python3.6/site-packages/torch/distributed/；<br> 在distributed文件下，新建多进程的脚本，命名为yolov5_launch.py：</p> 
<pre><code class="prism language-python">
<span class="token keyword">import</span> sys
<span class="token keyword">import</span> subprocess
<span class="token keyword">import</span> os
<span class="token keyword">from</span> argparse <span class="token keyword">import</span> ArgumentParser<span class="token punctuation">,</span> REMAINDER


<span class="token keyword">def</span> <span class="token function">parse_args</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Helper function parsing the command line options
    @retval ArgumentParser
    """</span>
    parser <span class="token operator">=</span> ArgumentParser<span class="token punctuation">(</span>description<span class="token operator">=</span><span class="token string">"PyTorch distributed training launch "</span>
                                        <span class="token string">"helper utility that will spawn up "</span>
                                        <span class="token string">"multiple distributed processes"</span><span class="token punctuation">)</span>

    <span class="token comment"># Optional arguments for the launch helper</span>
    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"--nnodes"</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
                        <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">"The number of nodes to use for distributed "</span>
                             <span class="token string">"training"</span><span class="token punctuation">)</span>
    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"--node_rank"</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
                        <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">"The rank of the node for multi-node distributed "</span>
                             <span class="token string">"training"</span><span class="token punctuation">)</span>
    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"--nproc_per_node"</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
                        <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">"The number of processes to launch on each node, "</span>
                             <span class="token string">"for GPU training, this is recommended to be set "</span>
                             <span class="token string">"to the number of GPUs in your system so that "</span>
                             <span class="token string">"each process can be bound to a single GPU."</span><span class="token punctuation">)</span><span class="token comment">#修改成你对应GPU的个数</span>
    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"--master_addr"</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token string">"127.0.0.1"</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">str</span><span class="token punctuation">,</span>
                        <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">"Master node (rank 0)'s address, should be either "</span>
                             <span class="token string">"the IP address or the hostname of node 0, for "</span>
                             <span class="token string">"single node multi-proc training, the "</span>
                             <span class="token string">"--master_addr can simply be 127.0.0.1"</span><span class="token punctuation">)</span>
    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"--master_port"</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token number">29528</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">,</span>
                        <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">"Master node (rank 0)'s free port that needs to "</span>
                             <span class="token string">"be used for communication during distributed "</span>
                             <span class="token string">"training"</span><span class="token punctuation">)</span>
    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"--use_env"</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> action<span class="token operator">=</span><span class="token string">"store_true"</span><span class="token punctuation">,</span>
                        <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">"Use environment variable to pass "</span>
                             <span class="token string">"'local rank'. For legacy reasons, the default value is False. "</span>
                             <span class="token string">"If set to True, the script will not pass "</span>
                             <span class="token string">"--local_rank as argument, and will instead set LOCAL_RANK."</span><span class="token punctuation">)</span>
    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"-m"</span><span class="token punctuation">,</span> <span class="token string">"--module"</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> action<span class="token operator">=</span><span class="token string">"store_true"</span><span class="token punctuation">,</span>
                        <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">"Changes each process to interpret the launch script "</span>
                             <span class="token string">"as a python module, executing with the same behavior as"</span>
                             <span class="token string">"'python -m'."</span><span class="token punctuation">)</span>
    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"--no_python"</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> action<span class="token operator">=</span><span class="token string">"store_true"</span><span class="token punctuation">,</span>
                        <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">"Do not prepend the training script with \"python\" - just exec "</span>
                             <span class="token string">"it directly. Useful when the script is not a Python script."</span><span class="token punctuation">)</span>

    <span class="token comment"># # positional</span>
    <span class="token comment"># parser.add_argument("training_script", type=str,default=r"train,py"</span>
    <span class="token comment">#                     help="The full path to the single GPU training "</span>
    <span class="token comment">#                          "program/script to be launched in parallel, "</span>
    <span class="token comment">#                          "followed by all the arguments for the "</span>
    <span class="token comment">#                          "training script")</span>

    <span class="token comment"># # rest from the training program</span>
    <span class="token comment"># parser.add_argument('training_script_args', nargs=REMAINDER)</span>
    <span class="token keyword">return</span> parser<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    args <span class="token operator">=</span> parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>training_script <span class="token operator">=</span> <span class="token string">r"yolov5-master/train.py"</span><span class="token comment">#修改成你要训练的train.py的绝对路径</span>
    <span class="token comment"># world size in terms of number of processes</span>
    dist_world_size <span class="token operator">=</span> args<span class="token punctuation">.</span>nproc_per_node <span class="token operator">*</span> args<span class="token punctuation">.</span>nnodes

    <span class="token comment"># set PyTorch distributed related environmental variables</span>
    current_env <span class="token operator">=</span> os<span class="token punctuation">.</span>environ<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>
    current_env<span class="token punctuation">[</span><span class="token string">"MASTER_ADDR"</span><span class="token punctuation">]</span> <span class="token operator">=</span> args<span class="token punctuation">.</span>master_addr
    current_env<span class="token punctuation">[</span><span class="token string">"MASTER_PORT"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">str</span><span class="token punctuation">(</span>args<span class="token punctuation">.</span>master_port<span class="token punctuation">)</span>
    current_env<span class="token punctuation">[</span><span class="token string">"WORLD_SIZE"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">str</span><span class="token punctuation">(</span>dist_world_size<span class="token punctuation">)</span>

    processes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

    <span class="token keyword">if</span> <span class="token string">'OMP_NUM_THREADS'</span> <span class="token keyword">not</span> <span class="token keyword">in</span> os<span class="token punctuation">.</span>environ <span class="token keyword">and</span> args<span class="token punctuation">.</span>nproc_per_node <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
        current_env<span class="token punctuation">[</span><span class="token string">"OMP_NUM_THREADS"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">str</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"*****************************************\n"</span>
              <span class="token string">"Setting OMP_NUM_THREADS environment variable for each process "</span>
              <span class="token string">"to be {} in default, to avoid your system being overloaded, "</span>
              <span class="token string">"please further tune the variable for optimal performance in "</span>
              <span class="token string">"your application as needed. \n"</span>
              <span class="token string">"*****************************************"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>current_env<span class="token punctuation">[</span><span class="token string">"OMP_NUM_THREADS"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">for</span> local_rank <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> args<span class="token punctuation">.</span>nproc_per_node<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># each process's rank</span>
        dist_rank <span class="token operator">=</span> args<span class="token punctuation">.</span>nproc_per_node <span class="token operator">*</span> args<span class="token punctuation">.</span>node_rank <span class="token operator">+</span> local_rank
        current_env<span class="token punctuation">[</span><span class="token string">"RANK"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">str</span><span class="token punctuation">(</span>dist_rank<span class="token punctuation">)</span>
        current_env<span class="token punctuation">[</span><span class="token string">"LOCAL_RANK"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">str</span><span class="token punctuation">(</span>local_rank<span class="token punctuation">)</span>

        <span class="token comment"># spawn the processes</span>
        with_python <span class="token operator">=</span> <span class="token keyword">not</span> args<span class="token punctuation">.</span>no_python
        cmd <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">if</span> with_python<span class="token punctuation">:</span>
            cmd <span class="token operator">=</span> <span class="token punctuation">[</span>sys<span class="token punctuation">.</span>executable<span class="token punctuation">,</span> <span class="token string">"-u"</span><span class="token punctuation">]</span>
            <span class="token keyword">if</span> args<span class="token punctuation">.</span>module<span class="token punctuation">:</span>
                cmd<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">"-m"</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> <span class="token keyword">not</span> args<span class="token punctuation">.</span>use_env<span class="token punctuation">:</span>
                <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"When using the '--no_python' flag, you must also set the '--use_env' flag."</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> args<span class="token punctuation">.</span>module<span class="token punctuation">:</span>
                <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Don't use both the '--no_python' flag and the '--module' flag at the same time."</span><span class="token punctuation">)</span>

        cmd<span class="token punctuation">.</span>append<span class="token punctuation">(</span>args<span class="token punctuation">.</span>training_script<span class="token punctuation">)</span>

        <span class="token keyword">if</span> <span class="token keyword">not</span> args<span class="token punctuation">.</span>use_env<span class="token punctuation">:</span>
            cmd<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">"--local_rank={}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>local_rank<span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token comment"># cmd.extend(args.training_script_args)</span>

        process <span class="token operator">=</span> subprocess<span class="token punctuation">.</span>Popen<span class="token punctuation">(</span>cmd<span class="token punctuation">,</span> env<span class="token operator">=</span>current_env<span class="token punctuation">)</span>
        processes<span class="token punctuation">.</span>append<span class="token punctuation">(</span>process<span class="token punctuation">)</span>

    <span class="token keyword">for</span> process <span class="token keyword">in</span> processes<span class="token punctuation">:</span>
        process<span class="token punctuation">.</span>wait<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> process<span class="token punctuation">.</span>returncode <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> subprocess<span class="token punctuation">.</span>CalledProcessError<span class="token punctuation">(</span>returncode<span class="token operator">=</span>process<span class="token punctuation">.</span>returncode<span class="token punctuation">,</span>
                                                cmd<span class="token operator">=</span>cmd<span class="token punctuation">)</span>


<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>
    <span class="token comment"># import os </span>
    <span class="token comment"># os.environ['CUDA_VISIBLE_DEVICES'] = "0,1"</span>
    main<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>运行上述脚本： python yolov5_launch.py</p> 
<p><img src="https://images2.imgbox.com/23/45/EMvx86K2_o.png" alt="在这里插入图片描述"><br> 显存占用超过80%，注意这里可以将train.py 配置里面的batch_size 调大；</p> 
<h3><a id="_142"></a>另外一种方法</h3> 
<p>在网上看到另外一种方法，是不用在distributed文件夹下面新建文件这样麻烦，在</p> 
<pre><code>python -m torch.distributed.launch --nproc_per_node 2 train.py --batch-size 64 --data data/Allcls_one.yaml --weights weights/yolov5l.pt --cfg models/yolov5l_1cls.yaml --epochs 1 --device 0,1
</code></pre> 
<p>训练时，在python后面加上-m torch.distributed.launch --nproc_per_node (修改成你的gpu的个数)再运行train.py 再后面加上各种配置文件</p> 
<p>这个方法亲测可行，比第一种方法简单有效！</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/1b1f308fac203d040155b04d9bb9dc28/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Ceph 存储介绍</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7d7c3ab4f6ca8933bb778f1022240e75/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Axure授权码，2021年11月11日亲测有效</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>