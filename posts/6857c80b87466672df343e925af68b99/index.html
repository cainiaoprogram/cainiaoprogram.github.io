<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>强化学习Reinforcement Learning概念理解篇(一) - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="强化学习Reinforcement Learning概念理解篇(一)" />
<meta property="og:description" content="在学习强化学习之前，应该对强化学习有一个大致的了解，即去分析一下强化学习的结构或者组成元素：
什么是强化学习？所谓强化学习，就是在与环境的互动当中，为了达到某一个目标而精心的学习过程，因此称之为Goal-directed learning from interaction with the environment，这其实就是强化学习的第1层结构。可将其称之为强化学习的基本元素，包括：第一Agent，第二Environment，第三Goal。其中Environment是环境,Goal是目标，与环境进行互动的主体称为Agent玩家。
这个Agent的中文翻译很成问题，有人翻译为代理，比较接近于它字面上的意思，有人翻译为主体，也就比较译义。还有得翻译为智能体，就是比较贴切与人工智能了。那么，我实际上不会去翻译这个字，很多时候就直接称为Agent。不过从中文的理解上来说，我们知道Agent这个词，它有特工的含义。这样理解其实比较有意思。另外，我更倾向于玩家，强化学习的过程，在很大程度上就像是一场游戏。
强化学习现在最为领先的应用于领域也就是棋牌、游戏，包括最开始的Atari-雅达利游戏以及强化学习的封神之战围棋，到之后Dota、星际争霸等等。
那么，从学习的角度上来说，我觉得就把强化学习看成一场游戏，更有意思一些，Agent你就理解为玩家，当然这不是很重要了，重要的是理解强化学习的第1层结构，也就是基本元素Agent玩家，Environment环境以及Goal目标。
强化学习是玩家在与环境的互动当中，为了达成一个目标而进行的学习过程。
有了玩家和环境，环境可以说就是这个游戏本身，那么这个游戏的玩法是什么，玩家的目标又是什么呢？这就是强化学习的第2层结构，称之为主要元素，包括第一State状态，第二Action行动，第三Reward奖励。
为什么称之为主要元素呢？因为整个强化学习的过程就是围绕着这三个元素展开的，具体来说，首先，玩家和环境会处于某种状态State，这个状态的含义很广泛，可以说包括了所有的相关信息。
如果说这个游戏是英雄联盟，那么，状态就应该包括敌方和队友的位置、等级、技能、双方的经济，野区的情况以及玩家自己的情况等等等等。如果说这个游戏是一场篮球，那么状态就应该包括所没有球员的位置，速度，球在谁手上，地板滑不滑等等。也就是说，状态是可以很复杂的。
举一个简单的例子-围棋,围棋的状态非常简单，也就是棋盘上361个落子点的状态的整体，对于每一个落子点来说，可以有黑棋、白棋、空三种状态，那么，整个围棋的状态，在理论上只有3的361次方。虽然这是一个巨大的天文数字，但是总的来说，围棋的状态是怎么样的，是很容易去分析的。
那么，继续以围棋为例，在一个状态之下，玩家需要做出某种行动，也就是Action。比如黑棋先手，当前的状态是棋盘上没有落子，黑棋则可以采取361种可能的行动，可以在任何一个位置落子，当黑棋采取了某一行动之后，比如黑棋走了星位，状态将会发生变化。比如白棋同样走星位，那么这就进入了下一个状态，星位再次做出行动。State和Action，状态和行动的往复就构成了强化学习的主体部分。
什么是Reward奖励呢?Reward是指Agent在一个状态之下采取了特定的行动之后，所得到的及时的反馈。在强化学习中，Reward通常是一个实数，并且可能是0。比如在围棋中玩家的目标是赢得棋局，那么只有在达到赢棋的状态时，才会有一个大于0的奖励，我们可以规定赢棋的奖励为1。输棋或者和棋的奖励为0，而在棋局结束之前，任何一次行动实际上得到的奖励都为0，如果是在一场篮球比赛中，当然玩家的目标仍然是赢得比赛，不过获胜的条件变成了得分数大于对手。那么我们可以规定奖励为己方投篮得分数以及对方投篮得分的相反数。
总的来说，奖励应该是由最终的目标所决定的。如果在围棋中对吃掉对方的子进行奖励，那么强化学习的结果就会倾向于吃掉对方的子，而围棋获胜的条件是围地，而不是吃子，一味的吃子可能适得其反，所以根据最终的目标，合理的设置奖励对于强化学习来说是很重要的。
反正强化学习的目的则是最大化总的奖励，也就是整个游戏过程中所获得的奖励之和。奖励是一个即时的反馈，而目标是一个长远的结果。这两者之间的关系是需要理解清楚的，那么以上就是强迫学习的第2层结构，我称之为主要元素，包括第一State状态，第二Action行动,第三Reword奖励。
最后则是强化学习的第3层结构，我称之为核心元素，一共有两个：policy策略以及Value价值。策略很好理解，就是指在某一个状态下应该采取什么样的行动？那么简单的说，在数学上策略，其实就是一个函数，他的自变量，或者说输入是一个状态，而因变量或者说输出则是一个行动，在围棋中将当前棋盘的状态告诉这个策略函数，它则会告诉你下一步应该在哪里落子。很显然，强化学习想要达到的最终效果就是一个好的策略。
所以说Policy策略是强化学习的核心元素之一。那么，什么是Value价值呢？价值同样是一个函数，并且策略函数就取决于价值函数，所以毫无疑问，价值也是强化学习的核心元素。价值函数通常有两种，第一种称为State Value状态价值函数。顾名思义，它的输入是一个状态，而输出则是一个实数，这个实数就称为这个状态的价值。价值的含义很关键，它指的是预期将来会得到的所有奖励之和，也就是说处于当前这一状态的情况下，玩家在将来能够得到的所有奖励的一个期望值。注意玩家的目标就是得到了奖励之和尽可能大，因此，通过状态价值函数，玩家应该选择进入价值尽可能大的状态，而这是通过特定的行动来实现的，这就是状态价值函数决定了玩家的策略，另一种价值函数称为State-Action Value状态行动价值函数。顾名思义，它指的不单单是一个状态所对应的价值，而是在特定状态下采取某种行动所具有的价值，同样，价值指的是在将来能够得到的所有奖励的一个期望值，那么，显然在一个特定的状态下，根据状态行动价值函数，玩家应该选择价值最大的那一个行动，这就是状态行动价值函数决定了玩家的策略。
综上，大家应该理解了，为什么Policy策略和Value价值是强化学习的核心元素。强化学习所要学习的东西，实际上就是一个好的价值函数，而一个好的价值函数决定一个好的策略。当然有一部分算法可以不依赖于价值直接学习策略，不过主流的，或者说，核心的强化学习算法通常都是基于价值的。
本节主要分析了强化学习的结构，首先，什么是强化学习？强化学习是Agent在于环境的互动当中，为了达成一个目标而进行的学习过程，我把这称之为强化学习的第1层结构，也就是基本元素，包括第一Agent玩家、第二environment环境、第三Goal目标。强化学习的第2层结构称之为主要元素，包括第一state状态、第二Action行动、第三Reword奖励。最后则是强化学习的第3层结构，称之为核心元素policy策略和Value价值。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/6857c80b87466672df343e925af68b99/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-01-11T13:59:54+08:00" />
<meta property="article:modified_time" content="2022-01-11T13:59:54+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">强化学习Reinforcement Learning概念理解篇(一)</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><em><strong>在学习强化学习之前，应该对强化学习有一个大致的了解，即去分析一下强化学习的结构或者组成元素：</strong></em></p> 
<p><strong>什么是强化学习？所谓强化学习，就是在与环境的互动当中，为了达到某一个目标而精心的学习过程，因此称之为Goal-directed learning from interaction with the environment，这其实就是强化学习的第1层结构。可将其称之为强化学习的基本元素，包括：第一Agent，第二Environment，第三Goal。其中Environment是环境,Goal是目标，与环境进行互动的主体称为Agent玩家。</strong></p> 
<p>这个Agent的中文翻译很成问题，有人翻译为代理，比较接近于它字面上的意思，有人翻译为主体，也就比较译义。还有得翻译为智能体，就是比较贴切与人工智能了。那么，我实际上不会去翻译这个字，很多时候就直接称为Agent。不过从中文的理解上来说，我们知道Agent这个词，它有特工的含义。这样理解其实比较有意思。另外，我更倾向于玩家，强化学习的过程，在很大程度上就像是一场游戏。</p> 
<p>强化学习现在最为领先的应用于领域也就是棋牌、游戏，包括最开始的Atari-雅达利游戏以及强化学习的封神之战围棋，到之后Dota、星际争霸等等。</p> 
<p>那么，从学习的角度上来说，我觉得就把强化学习看成一场游戏，更有意思一些，Agent你就理解为玩家，当然这不是很重要了，重要的是理解强化学习的第1层结构，也就是基本元素Agent玩家，Environment环境以及Goal目标。</p> 
<p><strong>强化学习是玩家在与环境的互动当中，为了达成一个目标而进行的学习过程。<br> 有了玩家和环境，环境可以说就是这个游戏本身，那么这个游戏的玩法是什么，玩家的目标又是什么呢？这就是强化学习的第2层结构，称之为主要元素，包括第一State状态，第二Action行动，第三Reward奖励。</strong></p> 
<p>为什么称之为主要元素呢？因为整个强化学习的过程就是围绕着这三个元素展开的，具体来说，首先，玩家和环境会处于某种状态State，这个状态的含义很广泛，可以说包括了所有的相关信息。<br> 如果说这个游戏是英雄联盟，那么，状态就应该包括敌方和队友的位置、等级、技能、双方的经济，野区的情况以及玩家自己的情况等等等等。如果说这个游戏是一场篮球，那么状态就应该包括所没有球员的位置，速度，球在谁手上，地板滑不滑等等。也就是说，状态是可以很复杂的。</p> 
<p>举一个简单的例子-围棋,围棋的状态非常简单，也就是棋盘上361个落子点的状态的整体，对于每一个落子点来说，可以有黑棋、白棋、空三种状态，那么，整个围棋的状态，在理论上只有3的361次方。虽然这是一个巨大的天文数字，但是总的来说，围棋的状态是怎么样的，是很容易去分析的。<br> 那么，继续以围棋为例，在一个状态之下，玩家需要做出某种行动，也就是Action。比如黑棋先手，当前的状态是棋盘上没有落子，黑棋则可以采取361种可能的行动，可以在任何一个位置落子，当黑棋采取了某一行动之后，比如黑棋走了星位，状态将会发生变化。比如白棋同样走星位，那么这就进入了下一个状态，星位再次做出行动。State和Action，状态和行动的往复就构成了强化学习的主体部分。</p> 
<p>什么是Reward奖励呢?Reward是指Agent在一个状态之下采取了特定的行动之后，所得到的及时的反馈。在强化学习中，Reward通常是一个实数，并且可能是0。比如在围棋中玩家的目标是赢得棋局，那么只有在达到赢棋的状态时，才会有一个大于0的奖励，我们可以规定赢棋的奖励为1。输棋或者和棋的奖励为0，而在棋局结束之前，任何一次行动实际上得到的奖励都为0，如果是在一场篮球比赛中，当然玩家的目标仍然是赢得比赛，不过获胜的条件变成了得分数大于对手。那么我们可以规定奖励为己方投篮得分数以及对方投篮得分的相反数。</p> 
<p>总的来说，奖励应该是由最终的目标所决定的。如果在围棋中对吃掉对方的子进行奖励，那么强化学习的结果就会倾向于吃掉对方的子，而围棋获胜的条件是围地，而不是吃子，一味的吃子可能适得其反，所以根据最终的目标，合理的设置奖励对于强化学习来说是很重要的。</p> 
<p>反正强化学习的目的则是最大化总的奖励，也就是整个游戏过程中所获得的奖励之和。奖励是一个即时的反馈，而目标是一个长远的结果。这两者之间的关系是需要理解清楚的，那么以上就是强迫学习的第2层结构，我称之为主要元素，包括第一State状态，第二Action行动,第三Reword奖励。</p> 
<p><strong>最后则是强化学习的第3层结构，我称之为核心元素，一共有两个：policy策略以及Value价值。策略很好理解，就是指在某一个状态下应该采取什么样的行动？那么简单的说，在数学上策略，其实就是一个函数，他的自变量，或者说输入是一个状态，而因变量或者说输出则是一个行动，在围棋中将当前棋盘的状态告诉这个策略函数，它则会告诉你下一步应该在哪里落子。很显然，强化学习想要达到的最终效果就是一个好的策略。</strong></p> 
<p>所以说Policy策略是强化学习的核心元素之一。那么，什么是Value价值呢？价值同样是一个函数，并且策略函数就取决于价值函数，所以毫无疑问，价值也是强化学习的核心元素。价值函数通常有两种，第一种称为State Value状态价值函数。顾名思义，它的输入是一个状态，而输出则是一个实数，这个实数就称为这个状态的价值。价值的含义很关键，它指的是预期将来会得到的所有奖励之和，也就是说处于当前这一状态的情况下，玩家在将来能够得到的所有奖励的一个期望值。注意玩家的目标就是得到了奖励之和尽可能大，因此，通过状态价值函数，玩家应该选择进入价值尽可能大的状态，而这是通过特定的行动来实现的，这就是状态价值函数决定了玩家的策略，另一种价值函数称为State-Action Value状态行动价值函数。顾名思义，它指的不单单是一个状态所对应的价值，而是在特定状态下采取某种行动所具有的价值，同样，价值指的是在将来能够得到的所有奖励的一个期望值，那么，显然在一个特定的状态下，根据状态行动价值函数，玩家应该选择价值最大的那一个行动，这就是状态行动价值函数决定了玩家的策略。</p> 
<p>综上，大家应该理解了，为什么Policy策略和Value价值是强化学习的核心元素。强化学习所要学习的东西，实际上就是一个好的价值函数，而一个好的价值函数决定一个好的策略。当然有一部分算法可以不依赖于价值直接学习策略，不过主流的，或者说，核心的强化学习算法通常都是基于价值的。</p> 
<p>本节主要分析了强化学习的结构，首先，什么是强化学习？强化学习是Agent在于环境的互动当中，为了达成一个目标而进行的学习过程，我把这称之为强化学习的第1层结构，也就是基本元素，包括第一Agent玩家、第二environment环境、第三Goal目标。强化学习的第2层结构称之为主要元素，包括第一state状态、第二Action行动、第三Reword奖励。最后则是强化学习的第3层结构，称之为核心元素policy策略和Value价值。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/60bd2bd638e067c7e992f47359c4b632/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">离散系统的稳定性分析</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7e512a7c0e4e64dcc0b7363458dc883e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Intellij IDEA手动添加Libraries（非pom依赖的情况下）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>