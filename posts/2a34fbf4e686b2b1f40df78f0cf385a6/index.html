<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>轻量级网络——MobileNetV2 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="轻量级网络——MobileNetV2" />
<meta property="og:description" content="文章目录 1.MobileNetV2的介绍2.MobileNetV2的结构1）Inverted Residuals2）Linear Bottlenecks 3.MobileNetV2的性能统计4.MobileNetV2的pytorch实现 1.MobileNetV2的介绍 MobileNet v2网络是由google团队在2018年提出的，相比MobileNet V1网络，准确率更高，模型更小。
网络中的亮点 ：
Inverted Residuals （倒残差结构 ）Linear Bottlenecks（结构的最后一层采用线性层） 2.MobileNetV2的结构 1）Inverted Residuals 在之前的ResNet残差结构是先用1x1的卷积降维，再升维的操作。而在MobileNetV2中，是先升维，在降维的操作。
所以对于ResNet残差结构是两头大，中间小。而对于MobileNetV2结构是中间大，两头小的结构。
其中，在MobileNet结构中，采用了新的激活函数：ReLU6
2）Linear Bottlenecks 针对倒残差结构中，最后一层的卷积层，采用了线性的激活函数，而不是ReLU激活函数。
一个解释是，ReLU激活函数对于低维的信息可能会造成比较大的瞬损失，而对于高维的特征信息造成的损失很小。而且由于倒残差结构是两头小中间大，所以输出的是一个低维的特征信息。所以使用一个线性的激活函数避免特征损失。
结构如下所示：
ps：当stride=1且 输入特征矩阵与输出特征矩阵shape 相同时才有shortcut连接
shape的变化：其中的k是扩充因子
3.MobileNetV2的性能统计 Classification分类任务
其中MobileNetV2（1.4）中的1.4代表的是倍率因子也就是α，其中α是控制卷积层卷积核个数的超参数，β是控制输入图像的大小
可以看见，在CPU上分类一张图片主需要花75ms，基本上达到了实时性的要求。
Object Detection目标检测任务
可以看见，MobileNetV2的提出，已经基本上可以实现在移动设备或者是嵌入式设备来跑深度学习的模型了。将研究与日常生活结合了起来。
4.MobileNetV2的pytorch实现 MobileNetV2的网络结构
其中：
t是扩展因子，第一层1x1卷积层中卷积核的扩展倍率c是输出特征矩阵深度channeln是bottleneck的重复次数s是步距（ 针对第一层，其他为1 ，与ResNet的类似，通过第一层的步长改变尺寸变化） 参考代码
import torch import torch.nn as nn import torchvision # 分类个数 num_class = 5 # DW卷积 def Conv3x3BNReLU(in_channels,out_channels,stride,groups): return nn.Sequential( # stride=2 wh减半，stride=1 wh不变 nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=stride, padding=1, groups=groups), nn." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/2a34fbf4e686b2b1f40df78f0cf385a6/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-05-16T16:45:01+08:00" />
<meta property="article:modified_time" content="2021-05-16T16:45:01+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">轻量级网络——MobileNetV2</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night-eighties">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><ul><li><ul><li><a href="#1MobileNetV2_1" rel="nofollow">1.MobileNetV2的介绍</a></li><li><a href="#2MobileNetV2_8" rel="nofollow">2.MobileNetV2的结构</a></li><li><ul><li><ul><li><a href="#1Inverted_Residuals_9" rel="nofollow">1）Inverted Residuals</a></li><li><a href="#2Linear_Bottlenecks_18" rel="nofollow">2）Linear Bottlenecks</a></li></ul> 
    </li></ul> 
    </li><li><a href="#3MobileNetV2_30" rel="nofollow">3.MobileNetV2的性能统计</a></li><li><a href="#4MobileNetV2pytorch_43" rel="nofollow">4.MobileNetV2的pytorch实现</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h4><a id="1MobileNetV2_1"></a>1.MobileNetV2的介绍</h4> 
<p>MobileNet v2网络是由google团队在2018年提出的，相比MobileNet V1网络，准确率更高，模型更小。</p> 
<p>网络中的亮点 ：</p> 
<ul><li>Inverted Residuals （倒残差结构 ）</li><li>Linear Bottlenecks（结构的最后一层采用线性层）</li></ul> 
<h4><a id="2MobileNetV2_8"></a>2.MobileNetV2的结构</h4> 
<h6><a id="1Inverted_Residuals_9"></a>1）Inverted Residuals</h6> 
<p><img src="https://images2.imgbox.com/8e/fb/V9ZDCD1J_o.png" alt="在这里插入图片描述"><br> 在之前的ResNet残差结构是先用1x1的卷积降维，再升维的操作。而在MobileNetV2中，是先升维，在降维的操作。</p> 
<p>所以对于ResNet残差结构是两头大，中间小。而对于MobileNetV2结构是中间大，两头小的结构。</p> 
<p><strong>其中，在MobileNet结构中，采用了新的激活函数：ReLU6</strong><br> <img src="https://images2.imgbox.com/38/42/Eabed8XT_o.png" alt="在这里插入图片描述"></p> 
<h6><a id="2Linear_Bottlenecks_18"></a>2）Linear Bottlenecks</h6> 
<p>针对倒残差结构中，最后一层的卷积层，采用了线性的激活函数，而不是ReLU激活函数。<br> <img src="https://images2.imgbox.com/f2/31/hOhYKFID_o.png" alt="在这里插入图片描述"><br> 一个解释是，ReLU激活函数对于低维的信息可能会造成比较大的瞬损失，而对于高维的特征信息造成的损失很小。而且由于倒残差结构是两头小中间大，所以输出的是一个低维的特征信息。所以使用一个线性的激活函数避免特征损失。</p> 
<p>结构如下所示：<br> <img src="https://images2.imgbox.com/3a/b7/JfRfq1vd_o.png" alt="在这里插入图片描述"><br> ps：当stride=1且 输入特征矩阵与输出特征矩阵shape 相同时才有shortcut连接</p> 
<p>shape的变化：其中的k是扩充因子<br> <img src="https://images2.imgbox.com/02/c1/2nL8Rjdd_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="3MobileNetV2_30"></a>3.MobileNetV2的性能统计</h4> 
<ul><li>Classification分类任务<br> <img src="https://images2.imgbox.com/59/95/zKebe3Lq_o.png" alt="在这里插入图片描述"></li></ul> 
<p>其中MobileNetV2（1.4）中的1.4代表的是倍率因子也就是α，其中α是控制卷积层卷积核个数的超参数，β是控制输入图像的大小</p> 
<p>可以看见，在CPU上分类一张图片主需要花75ms，基本上达到了实时性的要求。</p> 
<ul><li>Object Detection目标检测任务<br> <img src="https://images2.imgbox.com/ba/98/VCsUaiGE_o.png" alt="在这里插入图片描述"></li></ul> 
<p>可以看见，MobileNetV2的提出，已经基本上可以实现在移动设备或者是嵌入式设备来跑深度学习的模型了。将研究与日常生活结合了起来。</p> 
<h4><a id="4MobileNetV2pytorch_43"></a>4.MobileNetV2的pytorch实现</h4> 
<p><strong>MobileNetV2的网络结构</strong><br> <img src="https://images2.imgbox.com/6d/0b/B1t5XQ4s_o.png" alt="在这里插入图片描述"><br> 其中：</p> 
<ul><li>t是扩展因子，第一层1x1卷积层中卷积核的扩展倍率</li><li>c是输出特征矩阵深度channel</li><li>n是bottleneck的重复次数</li><li>s是步距（ 针对第一层，其他为1 ，与ResNet的类似，通过第一层的步长改变尺寸变化）</li></ul> 
<p><strong>参考代码</strong></p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torchvision

<span class="token comment"># 分类个数</span>
num_class <span class="token operator">=</span> <span class="token number">5</span>

<span class="token comment"># DW卷积</span>
<span class="token keyword">def</span> <span class="token function">Conv3x3BNReLU</span><span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span>out_channels<span class="token punctuation">,</span>stride<span class="token punctuation">,</span>groups<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            <span class="token comment"># stride=2 wh减半，stride=1 wh不变</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span>stride<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> groups<span class="token operator">=</span>groups<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU6<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>

<span class="token comment"># PW卷积</span>
<span class="token keyword">def</span> <span class="token function">Conv1x1BNReLU</span><span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span>out_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU6<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>

<span class="token comment"># # PW卷积(Linear) 没有使用激活函数</span>
<span class="token keyword">def</span> <span class="token function">Conv1x1BN</span><span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span>out_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">)</span>
        <span class="token punctuation">)</span>

<span class="token keyword">class</span> <span class="token class-name">InvertedResidual</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># t = expansion_factor,也就是扩展因子，文章中取6</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> expansion_factor<span class="token punctuation">,</span> stride<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>InvertedResidual<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>stride <span class="token operator">=</span> stride
        self<span class="token punctuation">.</span>in_channels <span class="token operator">=</span> in_channels
        self<span class="token punctuation">.</span>out_channels <span class="token operator">=</span> out_channels
        mid_channels <span class="token operator">=</span> <span class="token punctuation">(</span>in_channels <span class="token operator">*</span> expansion_factor<span class="token punctuation">)</span>
        <span class="token comment"># print("expansion_factor:", expansion_factor)</span>
        <span class="token comment"># print("mid_channels:",mid_channels)</span>

        <span class="token comment"># 先1x1卷积升维，再1x1卷积降维</span>
        self<span class="token punctuation">.</span>bottleneck <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            <span class="token comment"># 升维操作: 扩充维度是 in_channels * expansion_factor (6倍)</span>
            Conv1x1BNReLU<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> mid_channels<span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token comment"># DW卷积,降低参数量</span>
            Conv3x3BNReLU<span class="token punctuation">(</span>mid_channels<span class="token punctuation">,</span> mid_channels<span class="token punctuation">,</span> stride<span class="token punctuation">,</span> groups<span class="token operator">=</span>mid_channels<span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token comment"># 降维操作: 降维度 in_channels * expansion_factor(6倍) 降维到指定 out_channels 维度</span>
            Conv1x1BN<span class="token punctuation">(</span>mid_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span>
        <span class="token punctuation">)</span>

        <span class="token comment"># 第一种: stride=1 才有shortcut 此方法让原本不相同的channels相同</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>stride <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>shortcut <span class="token operator">=</span> Conv1x1BN<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span>

        <span class="token comment"># 第二种: stride=1 切 in_channels=out_channels 才有 shortcut</span>
        <span class="token comment"># if self.stride == 1 and in_channels == out_channels:</span>
        <span class="token comment">#     self.shortcut = ()</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>bottleneck<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token comment"># 第一种:</span>
        out <span class="token operator">=</span> <span class="token punctuation">(</span>out<span class="token operator">+</span>self<span class="token punctuation">.</span>shortcut<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">if</span> self<span class="token punctuation">.</span>stride<span class="token operator">==</span><span class="token number">1</span> <span class="token keyword">else</span> out
        <span class="token comment"># 第二种:</span>
        <span class="token comment"># out = (out + x) if self.stride == 1 and self.in_channels == self.out_channels else out</span>
        <span class="token keyword">return</span> out

<span class="token keyword">class</span> <span class="token class-name">MobileNetV2</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># num_class为分类个数, t为扩充因子</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_classes<span class="token operator">=</span>num_class<span class="token punctuation">,</span> t<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>MobileNetV2<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment"># 3 -&gt; 32 groups=1 不是组卷积 单纯的卷积操作</span>
        self<span class="token punctuation">.</span>first_conv <span class="token operator">=</span> Conv3x3BNReLU<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span>groups<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token comment"># 32 -&gt; 16 stride=1 wh不变</span>
        self<span class="token punctuation">.</span>layer1 <span class="token operator">=</span> self<span class="token punctuation">.</span>make_layer<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> factor<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> block_num<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 16 -&gt; 24 stride=2 wh减半</span>
        self<span class="token punctuation">.</span>layer2 <span class="token operator">=</span> self<span class="token punctuation">.</span>make_layer<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">24</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> factor<span class="token operator">=</span>t<span class="token punctuation">,</span> block_num<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        <span class="token comment"># 24 -&gt; 32 stride=2 wh减半</span>
        self<span class="token punctuation">.</span>layer3 <span class="token operator">=</span> self<span class="token punctuation">.</span>make_layer<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">24</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> factor<span class="token operator">=</span>t<span class="token punctuation">,</span> block_num<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>
        <span class="token comment"># 32 -&gt; 64 stride=2 wh减半</span>
        self<span class="token punctuation">.</span>layer4 <span class="token operator">=</span> self<span class="token punctuation">.</span>make_layer<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> factor<span class="token operator">=</span>t<span class="token punctuation">,</span> block_num<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span>
        <span class="token comment"># 64 -&gt; 96 stride=1 wh不变</span>
        self<span class="token punctuation">.</span>layer5 <span class="token operator">=</span> self<span class="token punctuation">.</span>make_layer<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">96</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> factor<span class="token operator">=</span>t<span class="token punctuation">,</span> block_num<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>
        <span class="token comment"># 96 -&gt; 160 stride=2 wh减半</span>
        self<span class="token punctuation">.</span>layer6 <span class="token operator">=</span> self<span class="token punctuation">.</span>make_layer<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">96</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">160</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> factor<span class="token operator">=</span>t<span class="token punctuation">,</span> block_num<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>
        <span class="token comment"># 160 -&gt; 320 stride=1 wh不变</span>
        self<span class="token punctuation">.</span>layer7 <span class="token operator">=</span> self<span class="token punctuation">.</span>make_layer<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">160</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">320</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> factor<span class="token operator">=</span>t<span class="token punctuation">,</span> block_num<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 320 -&gt; 1280 单纯的升维操作</span>
        self<span class="token punctuation">.</span>last_conv <span class="token operator">=</span> Conv1x1BNReLU<span class="token punctuation">(</span><span class="token number">320</span><span class="token punctuation">,</span><span class="token number">1280</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>avgpool <span class="token operator">=</span> nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span>stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">1280</span><span class="token punctuation">,</span>out_features<span class="token operator">=</span>num_classes<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>init_params<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">make_layer</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> stride<span class="token punctuation">,</span> factor<span class="token punctuation">,</span> block_num<span class="token punctuation">)</span><span class="token punctuation">:</span>
        layers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token comment"># 与ResNet类似，每层Bottleneck单独处理，指定stride。此层外的stride均为1</span>
        layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>InvertedResidual<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> factor<span class="token punctuation">,</span> stride<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 这些叠加层stride均为1，in_channels = out_channels, 其中 block_num-1 为重复次数</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> block_num<span class="token punctuation">)</span><span class="token punctuation">:</span>
            layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>InvertedResidual<span class="token punctuation">(</span>out_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> factor<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>layers<span class="token punctuation">)</span>

    <span class="token comment"># 初始化权重操作</span>
    <span class="token keyword">def</span> <span class="token function">init_params</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> m <span class="token keyword">in</span> self<span class="token punctuation">.</span>modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
                nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>kaiming_normal_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
                nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
            <span class="token keyword">elif</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">)</span> <span class="token operator">or</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
                nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
                nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>first_conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># torch.Size([1, 32, 112, 112])</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>      <span class="token comment"># torch.Size([1, 16, 112, 112])</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>      <span class="token comment"># torch.Size([1, 24, 56, 56])</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>      <span class="token comment"># torch.Size([1, 32, 28, 28])</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer4<span class="token punctuation">(</span>x<span class="token punctuation">)</span>      <span class="token comment"># torch.Size([1, 64, 14, 14])</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer5<span class="token punctuation">(</span>x<span class="token punctuation">)</span>      <span class="token comment"># torch.Size([1, 96, 14, 14])</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer6<span class="token punctuation">(</span>x<span class="token punctuation">)</span>      <span class="token comment"># torch.Size([1, 160, 7, 7])</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer7<span class="token punctuation">(</span>x<span class="token punctuation">)</span>      <span class="token comment"># torch.Size([1, 320, 7, 7])</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>last_conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span>   <span class="token comment"># torch.Size([1, 1280, 7, 7])</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>avgpool<span class="token punctuation">(</span>x<span class="token punctuation">)</span>     <span class="token comment"># torch.Size([1, 1280, 1, 1])</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment"># torch.Size([1, 1280])</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">)</span>      <span class="token comment"># torch.Size([1, 5])</span>
        <span class="token keyword">return</span> x


<span class="token keyword">if</span> __name__<span class="token operator">==</span><span class="token string">'__main__'</span><span class="token punctuation">:</span>
    model <span class="token operator">=</span> MobileNetV2<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># model = torchvision.models.MobileNetV2()</span>
    <span class="token comment"># print(model)</span>

    <span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">)</span>
    out <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>out<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>

</code></pre> 
<p>参考：<br> https://www.bilibili.com/video/BV1yE411p7L7</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b4d1fbf003ad55c9174d22c9d4a9f8c1/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">JS完成QQ空间</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/52ee72385ac121bbbc8d4addde928ac9/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【机器学习基础】集成模型</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>