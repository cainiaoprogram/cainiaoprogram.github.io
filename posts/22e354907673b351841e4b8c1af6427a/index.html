<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>深入torch框架内部 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="深入torch框架内部" />
<meta property="og:description" content="torch框架下的函数都进行了封装，使初学者很难清楚内部的数据形式到底是什么，所以，经过了torch框架的练习，这篇文章来解析一下torch框架下的封装函数。
我们打印
print(torch.nn)
print(torch.optim)
print(torch.cuda)
输出结果：
&lt;module &#39;torch.nn&#39; from &#39;D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\__init__.py&#39;
&lt;module&#39;torch.optim&#39;from&#39;D:\\Anaconda\\envs\\pytorch\\lib\\sitepackages\\torch\\optim\\__init__.py
&lt;module&#39;torch.cuda&#39;from&#39;D:\\Anaconda\\envs\\pytorch\\lib\\sitepackages\\torch\\cuda\\__init__.py&#39;
nn,optim,cuda均在torch框架下的目录中，函数或类就在目录下的文件中。
1，学习率衰减函数：
#学习率衰减 scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5,last_epoch=-1) 参数含义：
step_size:每经过多少个epoch对学习率进行调整，此时的scheduler.step()放在epoch循环下。
gamma:经过step_size个epoch后，学习率变成:lr * gamma。
last_epoch:last_epoch之后恢复lr为initial_lr(如果是训练了很多个epoch后中断了 继续训练 这个值就等于加载的模型的epoch 默认为-1表示从头开始训练，即从epoch=1开始。
scheduler 对象常用的几个属性：
scheduler.step():对学习率进行更新，放在epoch下，优化器的更新之后。
scheduler.dict():状态信息 ，字典格式
print(scheduler.state_dict()) # {&#39;step_size&#39;: 20, &#39;gamma&#39;: 0.5, &#39;base_lrs&#39;: [0.01], &#39;last_epoch&#39;: 0, &#39;_step_count&#39;: 1, &#39;verbose&#39;: False, # &#39;_get_lr_called_within_step&#39;: False, &#39;_last_lr&#39;: [0.01]} 查看打印每个epoch的学习率：
可用优化器的属性state_dict():optimizer.state_dict()[&#39;param_groups&#39;][0][&#39;lr&#39;]
state_dict()下的内容：
for k,v in state_dict.items(): print(k) #state #param_groups &#39;param_groups&#39;: [{&#39;lr&#39;: 0.01, &#39;momentum&#39;: 0.9, &#39;dampening&#39;: 0, &#39;weight_decay&#39;: 0." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/22e354907673b351841e4b8c1af6427a/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-05-03T17:42:24+08:00" />
<meta property="article:modified_time" content="2022-05-03T17:42:24+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深入torch框架内部</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p></p> 
<p>torch框架下的函数都进行了封装，使初学者很难清楚内部的数据形式到底是什么，所以，经过了torch框架的练习，这篇文章来解析一下torch框架下的封装函数。</p> 
<h4></h4> 
<p>我们打印</p> 
<p>print(torch.nn)</p> 
<p>print(torch.optim)</p> 
<p>print(torch.cuda)</p> 
<p>输出结果：</p> 
<p>&lt;module 'torch.nn' from 'D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\__init__.py'<br> &lt;module'torch.optim'from'D:\\Anaconda\\envs\\pytorch\\lib\\sitepackages\\torch\\optim\\__init__.py<br> &lt;module'torch.cuda'from'D:\\Anaconda\\envs\\pytorch\\lib\\sitepackages\\torch\\cuda\\__init__.py'</p> 
<p>nn,optim,cuda均在torch框架下的目录中，函数或类就在目录下的文件中。</p> 
<p>1，学习率衰减函数：</p> 
<pre><code class="language-python">  #学习率衰减
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5,last_epoch=-1)</code></pre> 
<p>参数含义：</p> 
<p> step_size:每经过多少个epoch对学习率进行调整，此时的scheduler.step()放在epoch循环下。</p> 
<p>gamma:经过step_size个epoch后，学习率变成:lr * gamma。</p> 
<p>last_epoch:last_epoch之后恢复lr为initial_lr(如果是训练了很多个epoch后中断了 继续训练 这个值就等于加载的模型的epoch 默认为-1表示从头开始训练，即从epoch=1开始。</p> 
<p></p> 
<p>scheduler 对象常用的几个属性：</p> 
<p>scheduler.step():对学习率进行更新，放在epoch下，优化器的更新之后。</p> 
<p>scheduler.dict():状态信息 ，字典格式</p> 
<pre><code class="language-python">    print(scheduler.state_dict())
    # {'step_size': 20, 'gamma': 0.5, 'base_lrs': [0.01], 'last_epoch': 0, '_step_count': 1, 'verbose': False,
    #  '_get_lr_called_within_step': False, '_last_lr': [0.01]}</code></pre> 
<p>查看打印每个epoch的学习率：</p> 
<p>可用优化器的属性state_dict():optimizer.state_dict()['param_groups'][0]['lr']</p> 
<p>state_dict()下的内容：</p> 
<pre><code class="language-python">  for k,v in state_dict.items():
                print(k)
                #state
                #param_groups</code></pre> 
<pre><code class="language-python">'param_groups': [{'lr': 0.01, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.0002, 'nesterov': False, 'initial_lr': 0.01, 'params': [0, 1,</code></pre> 
<p> 2，DataLoader：</p> 
<p>数据加载器，当训练时，每一个epoch,就是从DataLoader中获取一个batch_size大小的数据。</p> 
<pre><code class="language-python"> for index, (input, target) in enumerate(train_loader,0):

            model.train()

            input = Variable(input).cuda()

            target = Variable(target).cuda()

            output = model(input)

            loss = criterion(output, target)

            optimizer.zero_grad()

            loss.backward()

            optimizer.step()</code></pre> 
<p>model.train():开始启用batch_normalization  和 drop_out</p> 
<p>model.eval():会使用batch_normalization  不只用drop_out</p> 
<p>.cuda() :模型和相应的数据进行 .cuda()处理，可以将内存中的数据复制（迁移）到GPU的显存中。从而通过GPU来进行运算。</p> 
<pre><code class="language-python">    model = Model.get_net()
    if torch.cuda.is_available():
        model = model.cuda()</code></pre> 
<p> 对数据的迁移：</p> 
<p>数据方面常用的有两种：Tensor  和  Variable 。实际中这两种类型是同一个东西，因为Variable实际上只是一个容器。</p> 
<p>一，将Tensor迁移到显存中：</p> 
<pre><code class="language-python">import torch
a = torch.FloatTensor(2)
print(a)
b = a.cuda()
print(b)
c = b.cpu()
print(c)


#tensor([0., 0.])
#tensor([0., 0.], device='cuda:0')
#tensor([0., 0.])</code></pre> 
<p> 如果要将显存中的数据复制到内存中，则对cuda数据类型使用.cpu()方法即可。</p> 
<p>二，将Variable迁移到显存中</p> 
<p>常用Variable这个容器来装载数据。主要是Variable可以进行反向传播进行自动求导。</p> 
<p>同样的，要将Variable迁移到显存中，只需要使用.cuda()即可实现。</p> 
<p>对Variable直接使用.cuda() 和对Tenso先r进行.cuda()然后再放置到Variable中的结果一致。</p> 
<pre><code class="language-python">import torch
from torch.autograd import Variable
a = torch.FloatTensor(2)
b = Variable(a).cuda()
print(b)
c = a.cuda()
d = Variable(c)
print(d)

#tensor([0., 0.], device='cuda:0')
#tensor([0., 0.], device='cuda:0')</code></pre> 
<p>.cuda()操作默认使用GPU 0也就是第一张显卡来进行操作，当要存储到其他的显卡时可以使用</p> 
<p>.cuda(显卡卡号)来将数据存储到指定的显卡中。</p> 
<p>注意：</p> 
<p>对于不同存储位置的变量，不可以对他们进行计算。</p> 
<p></p> 
<p>不断的更新完善。</p> 
<p></p> 
<p style="text-align:center;"></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/3e1ff6051671e274f7d30a9c8abac4e6/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">docker高级</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/da1a992c024e1be22b041bf781e371e8/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">torch模型的保存和加载</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>