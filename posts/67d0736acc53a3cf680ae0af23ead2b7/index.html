<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>cuda编程学习——第二个cuda程序（官方案例分析）！干货向（二） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="cuda编程学习——第二个cuda程序（官方案例分析）！干货向（二）" />
<meta property="og:description" content="前言： 最近在做三维重建，尤其是Nerf方面多视角合成工作的时候，意识到了cuda的编程计算可以大大提高其中渲染的计算，最明显的例子是Instant-ngp，Plenoxels等文章，因此后面会学Cuda一段时间，同时也就开了这个新坑。
因为笔者也是cuda新手，所以大家有问题的话可以评论区指出，一起学习进步！
我已经更新了我的Github仓库，大家可以前往仓库下载代码
我的CUDA学习仓库
Nvidia Cuda官方入门资料
运行环境： Windows10，Visual Studio2019,显卡3050Ti
（大家自行根据自己笔记本情况去配环境，或者没有gpu，租借云服务器去学习也是可以的）
1：C&#43;&#43;模板 首先从下面的C&#43;&#43;代码开始
其作用为把2个数组元素相加，代码里设为1&lt;&lt;20，也就是在二进制下把00001左移20位，最终值为2^20，为1048576，大约位1million（百万），相加的值最终减去3.0，判断有没有误差。
#include &lt;iostream&gt; #include &lt;math.h&gt; // function to add the elements of two arrays void add(int n, float *x, float *y) { for (int i = 0; i &lt; n; i&#43;&#43;) y[i] = x[i] &#43; y[i]; } int main(void) { int N = 1&lt;&lt;20; // 1M elements float *x = new float[N]; float *y = new float[N]; // initialize x and y arrays on the host for (int i = 0; i &lt; N; i&#43;&#43;) { x[i] = 1." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/67d0736acc53a3cf680ae0af23ead2b7/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-09-27T20:58:48+08:00" />
<meta property="article:modified_time" content="2023-09-27T20:58:48+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">cuda编程学习——第二个cuda程序（官方案例分析）！干货向（二）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_0"></a>前言：</h2> 
<p>最近在做三维重建，尤其是Nerf方面多视角合成工作的时候，意识到了cuda的编程计算可以大大提高其中渲染的计算，最明显的例子是Instant-ngp，Plenoxels等文章，因此后面会学Cuda一段时间，同时也就开了这个新坑。<br> 因为笔者也是cuda新手，所以大家有问题的话可以评论区指出，一起学习进步！</p> 
<p>我已经更新了我的Github仓库，大家可以前往仓库下载代码<br> <a href="https://github.com/QinghongShao-sqh/CUDA_Study">我的CUDA学习仓库</a><br> <img src="https://images2.imgbox.com/bd/da/MxMHYzp5_o.png" alt="在这里插入图片描述"></p> 
<p><a href="https://developer.nvidia.com/blog/even-easier-introduction-cuda/" rel="nofollow">Nvidia Cuda官方入门资料</a></p> 
<h2><a id="_10"></a>运行环境：</h2> 
<p>Windows10，Visual Studio2019,显卡3050Ti<br> （大家自行根据自己笔记本情况去配环境，或者没有gpu，租借云服务器去学习也是可以的）</p> 
<h2><a id="1C_13"></a>1：C++模板</h2> 
<p>首先从下面的C++代码开始<br> 其作用为把2个数组元素相加，代码里设为1&lt;&lt;20，也就是在二进制下把00001左移20位，最终值为2^20，为1048576，大约位1million（百万），相加的值最终减去3.0，判断有没有误差。</p> 
<pre><code class="prism language-cpp"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;iostream&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;math.h&gt;</span></span>

<span class="token comment">// function to add the elements of two arrays</span>
<span class="token keyword">void</span> <span class="token function">add</span><span class="token punctuation">(</span><span class="token keyword">int</span> n<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>x<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>y<span class="token punctuation">)</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span>
      y<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> x<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+</span> y<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>

<span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token punctuation">)</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token keyword">int</span> N <span class="token operator">=</span> <span class="token number">1</span><span class="token operator">&lt;&lt;</span><span class="token number">20</span><span class="token punctuation">;</span> <span class="token comment">// 1M elements</span>

  <span class="token keyword">float</span> <span class="token operator">*</span>x <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token keyword">float</span><span class="token punctuation">[</span>N<span class="token punctuation">]</span><span class="token punctuation">;</span>
  <span class="token keyword">float</span> <span class="token operator">*</span>y <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token keyword">float</span><span class="token punctuation">[</span>N<span class="token punctuation">]</span><span class="token punctuation">;</span>

  <span class="token comment">// initialize x and y arrays on the host</span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    x<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1.0f</span><span class="token punctuation">;</span>
    y<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">2.0f</span><span class="token punctuation">;</span>
  <span class="token punctuation">}</span>

  <span class="token comment">// Run kernel on 1M elements on the CPU</span>
  <span class="token function">add</span><span class="token punctuation">(</span>N<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">;</span>

  <span class="token comment">// Check for errors (all values should be 3.0f)</span>
  <span class="token keyword">float</span> maxError <span class="token operator">=</span> <span class="token number">0.0f</span><span class="token punctuation">;</span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span>
    maxError <span class="token operator">=</span> <span class="token function">fmax</span><span class="token punctuation">(</span>maxError<span class="token punctuation">,</span> <span class="token function">fabs</span><span class="token punctuation">(</span>y<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">-</span><span class="token number">3.0f</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">//fabs（） 求浮点数的绝对值 fmax()返回2个参数最大的1个</span>
  std<span class="token double-colon punctuation">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"Max error: "</span> <span class="token operator">&lt;&lt;</span> maxError <span class="token operator">&lt;&lt;</span> std<span class="token double-colon punctuation">::</span>endl<span class="token punctuation">;</span>

  <span class="token comment">// Free memory</span>
  
</code></pre> 
<p>输出如下<br> <img src="https://images2.imgbox.com/25/9b/ni4byi47_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="2cuda_55"></a>2：编写核函数cuda</h2> 
<p>（1）__global__可以理解为一个关键词，其告诉编译器该函数可以在GPU上运行，并且可以从CPU代码上调用<br> （2）这些__global__函数被称为内核，在GPU上运行的代码通常被称为device code，而在CPU上运行的代码被称为host code</p> 
<pre><code class="prism language-cpp"><span class="token comment">// CUDA Kernel function to add the elements of two arrays on the GPU</span>
__global__
<span class="token keyword">void</span> <span class="token function">add</span><span class="token punctuation">(</span><span class="token keyword">int</span> n<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>x<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>y<span class="token punctuation">)</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span>
      y<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> x<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+</span> y<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<h2><a id="3CudaMemory_Allocation_68"></a>3：Cuda下的内存分配（Memory Allocation）</h2> 
<p>（1）为了在GPU上进行计算，我需要分配GPU可访问的内存。CUDA中的统一内存通过提供系统中所有gpu和cpu可访问的单一内存空间使这一点变得容易。<br> （2）要在统一内存中分配数据，调用cudaMallocManaged()，它返回一个指针，您可以从host(CPU)code或device(GPU)code访问。要释放数据，只需将指针传递给cudaFree()。<br> （3）我只需要用对cudaMallocManaged()的调用替换上面代码中的new调用，并用对cudaFree（）来执行释放数值操作</p> 
<pre><code class="prism language-cpp">  <span class="token comment">// Allocate Unified Memory -- accessible from CPU or GPU</span>
  <span class="token keyword">float</span> <span class="token operator">*</span>x<span class="token punctuation">,</span> <span class="token operator">*</span>y<span class="token punctuation">;</span>
  <span class="token function">cudaMallocManaged</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>x<span class="token punctuation">,</span> N<span class="token operator">*</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">cudaMallocManaged</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>y<span class="token punctuation">,</span> N<span class="token operator">*</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment">//理解为，分配空间指向 指针x指向的空间地址，大小为 N* x  x表示float的空间大小，因为有N个浮点数据，大约为1 million</span>

  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>

  <span class="token comment">// Free memory</span>
  <span class="token function">cudaFree</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">cudaFree</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<p>（4）在GPU上调用核函数需要在参数列表前添加符号&lt;&lt;&lt; &gt;&gt;&gt;<br> 简单理解，启动1个GPU线程来运行add（）函数</p> 
<pre><code class="prism language-cpp">add<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>N<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<p>（5）目前为止，完整代码如下</p> 
<pre><code class="prism language-cpp"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;iostream&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;math.h&gt;</span></span>
<span class="token comment">// Kernel function to add the elements of two arrays</span>
__global__
<span class="token keyword">void</span> <span class="token function">add</span><span class="token punctuation">(</span><span class="token keyword">int</span> n<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>x<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>y<span class="token punctuation">)</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span>
    y<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> x<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+</span> y<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>

<span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token punctuation">)</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token keyword">int</span> N <span class="token operator">=</span> <span class="token number">1</span><span class="token operator">&lt;&lt;</span><span class="token number">20</span><span class="token punctuation">;</span>
  <span class="token keyword">float</span> <span class="token operator">*</span>x<span class="token punctuation">,</span> <span class="token operator">*</span>y<span class="token punctuation">;</span>

  <span class="token comment">// Allocate Unified Memory – accessible from CPU or GPU</span>
  <span class="token function">cudaMallocManaged</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>x<span class="token punctuation">,</span> N<span class="token operator">*</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">cudaMallocManaged</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>y<span class="token punctuation">,</span> N<span class="token operator">*</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

  <span class="token comment">// initialize x and y arrays on the host</span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    x<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1.0f</span><span class="token punctuation">;</span>
    y<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">2.0f</span><span class="token punctuation">;</span>
  <span class="token punctuation">}</span>

  <span class="token comment">// Run kernel on 1M elements on the GPU</span>
  add<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>N<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">;</span>

  <span class="token comment">// Wait for GPU to finish before accessing on host</span>
  <span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

  <span class="token comment">// Check for errors (all values should be 3.0f)</span>
  <span class="token keyword">float</span> maxError <span class="token operator">=</span> <span class="token number">0.0f</span><span class="token punctuation">;</span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span>
    maxError <span class="token operator">=</span> <span class="token function">fmax</span><span class="token punctuation">(</span>maxError<span class="token punctuation">,</span> <span class="token function">fabs</span><span class="token punctuation">(</span>y<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">-</span><span class="token number">3.0f</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  std<span class="token double-colon punctuation">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"Max error: "</span> <span class="token operator">&lt;&lt;</span> maxError <span class="token operator">&lt;&lt;</span> std<span class="token double-colon punctuation">::</span>endl<span class="token punctuation">;</span>

  <span class="token comment">// Free memory</span>
  <span class="token function">cudaFree</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">cudaFree</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">;</span>
  
  <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>这只是第一步，因为正如所写的那样，这个内核只适用于单个线程，因为运行它的每个线程都会对整个数组执行add操作。此外，由于多个并行线程会读取和写入相同的位置，因此存在竞争条件。</p> 
<h2><a id="4_140"></a>4：并行线程计算</h2> 
<p>（1）找出内核运行时间的最简单方法是使用nvprof来运行它，nvprof是CUDA工具包附带的命令行GPU分析器。<br> （2）上述代码我们体验了利用GPU上一个线程来进行计算。那如何并行呢！<br> 下面代码表示CUDA运行时在GPU上启动要使用多少并行线程，这里选取256个线程</p> 
<pre><code class="prism language-cpp">add<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>N<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<p>（3）如果我在运行代码时只做了这样的修改，它将对每个线程执行一次计算，而不是将计算分散到并行线程上。为了正确地做到这一点，我需要修改内核，引入了两个量：<br> <strong>threadIdx.x</strong>：包含了the index of the thread within the block，块中的线程索引。此例中，index范围为0~255。<br> <strong>blockDim.x</strong>：包含了the size of thread block（number of threads in the thread block）线程块的大小，等于线程块中的线程数量。此例中，该值为256<br> <img src="https://images2.imgbox.com/80/1d/c8hN9oAJ_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-cpp">__global__
<span class="token keyword">void</span> <span class="token function">add</span><span class="token punctuation">(</span><span class="token keyword">int</span> n<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>x<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>y<span class="token punctuation">)</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token keyword">int</span> index <span class="token operator">=</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
  <span class="token keyword">int</span> stride <span class="token operator">=</span> blockDim<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> index<span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> i <span class="token operator">+=</span> stride<span class="token punctuation">)</span>
      y<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> x<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+</span> y<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>下面是运行结果（这里其实计算给出结果相较之前会更快）<br> <img src="https://images2.imgbox.com/c4/5d/z1veRXQm_o.png" alt="在这里插入图片描述"><br> 官网给出的比较：<br> 这是一个很大的加速(从463ms减少到2.7ms)。<br> <img src="https://images2.imgbox.com/8d/79/W7ziKv12_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="5Out_of_the_Blocks_169"></a>5：Out of the Blocks</h2> 
<p>（1）CUDA gpu有许多并行处理器，它们被分组为流多处理器(SMs)。每个SM可以运行多个并发线程块。为了充分利用CUDA GPU，kernel应启动多个thread blocks。</p> 
<p>（2）到目前为止，我们可以想到执行配置的第一个参数指定了<strong>线程块的数量</strong>。这些并行线程块一起构成了所谓的网格grid。因为我有N个元素要处理，每个块有256个线程，所以我只需要计算块的数量来获得至少N个线程。我只是用N除以块大小(注意，如果N不是blockSize的倍数，就要四舍五入)。</p> 
<pre><code class="prism language-cpp"><span class="token keyword">int</span> blockSize <span class="token operator">=</span> <span class="token number">256</span><span class="token punctuation">;</span><span class="token comment">//并行线程数量</span>
<span class="token comment">//使用N 除以块大小，表示 线程块数量</span>
<span class="token keyword">int</span> numBlocks <span class="token operator">=</span> <span class="token punctuation">(</span>N <span class="token operator">+</span> blockSize <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> blockSize<span class="token punctuation">;</span>
add<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>numBlocks<span class="token punctuation">,</span> blockSize<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>N<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<p>（3）我还需要更新内核代码，以考虑到线程块的整个网格。CUDA提供gridDim.x，其中包含网格中的块数量，以及blockIdx.x ,它包含网格中当前线程块的索引。<br> 再引入两个量：<br> <strong>blockIdx.x</strong>：包含the index of the block with in the grid。网格中线程块的索引<br> <strong>gridDim.x</strong>：包含the size of the grid。网格大小，可以理解为网格中块的数量</p> 
<p>图1说明了在CUDA中使用blockDim.x , gridDim.x 和 threadIdx.x对数组(一维)进行索引的方法。其思想是，每个线程通过计算其块开始的偏移量(块索引乘以块大小:blockIdx)来获得其索引。<br> 这个想法是，每个线程通过计算其块开始的偏移量(块索引乘以块大小)来获得其索引:blockIdx.x* blockDim.x)。并在块中添加线程的索引threadIdx.x。<br> <img src="https://images2.imgbox.com/1f/8b/SS8hCcmp_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-cpp">__global__
<span class="token keyword">void</span> <span class="token function">add</span><span class="token punctuation">(</span><span class="token keyword">int</span> n<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>x<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>y<span class="token punctuation">)</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token keyword">int</span> index <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
  <span class="token keyword">int</span> stride <span class="token operator">=</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">*</span> gridDim<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> index<span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> i <span class="token operator">+=</span> stride<span class="token punctuation">)</span>
    y<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> x<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+</span> y<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>更新后的kernel还将stride设置为网格中（blockDim.x*gridDim.x）的线程总数。在CUDA内核中，这种类型的循环通常被称为网格跨步循环。</p> 
<p>最终代码如下</p> 
<pre><code class="prism language-cpp"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;iostream&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;math.h&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span><span class="token string">&lt;stdint.h&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span><span class="token string">&lt;cuda.h&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">"cuda_runtime.h"</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">"device_launch_parameters.h"</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h&gt;</span></span>
<span class="token comment">// Kernel function to add the elements of two arrays</span>
__global__
<span class="token keyword">void</span> <span class="token function">add</span><span class="token punctuation">(</span><span class="token keyword">int</span> n<span class="token punctuation">,</span> <span class="token keyword">float</span><span class="token operator">*</span> x<span class="token punctuation">,</span> <span class="token keyword">float</span><span class="token operator">*</span> y<span class="token punctuation">)</span>
<span class="token punctuation">{<!-- --></span>
    <span class="token keyword">int</span> index <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
    <span class="token keyword">int</span> stride <span class="token operator">=</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">*</span> gridDim<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> index<span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> i <span class="token operator">+=</span> stride<span class="token punctuation">)</span>
        y<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> x<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+</span> y<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>

<span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token punctuation">)</span>
<span class="token punctuation">{<!-- --></span>
    <span class="token keyword">int</span> N <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">&lt;&lt;</span> <span class="token number">20</span><span class="token punctuation">;</span>
    <span class="token keyword">float</span><span class="token operator">*</span> x<span class="token punctuation">,</span> <span class="token operator">*</span> y<span class="token punctuation">;</span>

    <span class="token comment">// Allocate Unified Memory – accessible from CPU or GPU</span>
    <span class="token function">cudaMallocManaged</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>x<span class="token punctuation">,</span> N <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaMallocManaged</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>y<span class="token punctuation">,</span> N <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token comment">// initialize x and y arrays on the host</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        x<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1.0f</span><span class="token punctuation">;</span>
        y<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">2.0f</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>

    <span class="token comment">// Run kernel on 1M elements on the GPU</span>
  <span class="token comment">//  add &lt;&lt;&lt;1, 256 &gt;&gt;&gt; (N, x, y);</span>


    <span class="token keyword">int</span> blockSize <span class="token operator">=</span> <span class="token number">256</span><span class="token punctuation">;</span><span class="token comment">//并行线程数量</span>
<span class="token comment">//使用N 除以块大小，表示 线程块数量</span>
    <span class="token keyword">int</span> numBlocks <span class="token operator">=</span> <span class="token punctuation">(</span>N <span class="token operator">+</span> blockSize <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> blockSize<span class="token punctuation">;</span>
    add <span class="token operator">&lt;&lt;</span> <span class="token operator">&lt;</span>numBlocks<span class="token punctuation">,</span> blockSize <span class="token operator">&gt;&gt;</span> <span class="token operator">&gt;</span> <span class="token punctuation">(</span>N<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">;</span>



    <span class="token comment">// Wait for GPU to finish before accessing on host</span>
    <span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token comment">// Check for errors (all values should be 3.0f)</span>
    <span class="token keyword">float</span> maxError <span class="token operator">=</span> <span class="token number">0.0f</span><span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span>
        maxError <span class="token operator">=</span> <span class="token function">fmax</span><span class="token punctuation">(</span>maxError<span class="token punctuation">,</span> <span class="token function">fabs</span><span class="token punctuation">(</span>y<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">3.0f</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    std<span class="token double-colon punctuation">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"Max error: "</span> <span class="token operator">&lt;&lt;</span> maxError <span class="token operator">&lt;&lt;</span> std<span class="token double-colon punctuation">::</span>endl<span class="token punctuation">;</span>

    <span class="token comment">// Free memory</span>
    <span class="token function">cudaFree</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaFree</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<h2><a id="6_264"></a>6：结果比较（性能）</h2> 
<p><img src="https://images2.imgbox.com/a3/ab/wUeIdvoh_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_267"></a>额外知识补充</h2> 
<p>代码被分成两部分，一部分是在CPU上，也称之为在Host上，另一部分是在GPU上，也称之为在device上。他们两者的关系如下图所示<br> <img src="https://images2.imgbox.com/62/e0/w14L1YAu_o.png" alt="在这里插入图片描述"></p> 
<p><strong>网格grids和线程块blocks</strong></p> 
<p>网格grids，在上层，至多可以分成三维的blocks，在不同block当中的线程是不能通信的；线程块blocks在相对较低的层级，同样可以将线程分成三维，而在同一个块中的线程是可以通信的。</p> 
<p>对于一个核函数，只能有一个grid，但是可以有多个block，之所以将线程划分为grid和block是为了使得结构更清晰，便于线程管理，灵活运用。</p> 
<p><img src="https://images2.imgbox.com/95/5e/bbAlG3TR_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/5781b4cc7d637b8afa729d91a23a5345/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">图像细化和骨架提取</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ac69ebd844acea2193d9420da07f77e0/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Cannot resolve MVC view ‘xxx‘</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>