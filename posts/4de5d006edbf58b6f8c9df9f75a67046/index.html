<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>利用word2vec对关键词进行聚类 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="利用word2vec对关键词进行聚类" />
<meta property="og:description" content="继上次提取关键词之后，项目组长又要求我对关键词进行聚类。说实话，我不太明白对关键词聚类跟新闻推荐有什么联系，不过他说什么我照做就是了。
按照一般的思路，可以用新闻ID向量来表示某个关键词，这就像广告推荐系统里面用用户访问类别向量来表示用户一样，然后就可以用kmeans的方法进行聚类了。不过对于新闻来说存在一个问题，那就量太大，如果给你十万篇新闻，那每一个关键词将需要十万维的向量表示，随着新闻数迅速增加，那维度就更大了，这计算起来难度太大。于是，这个方法思路简单但是不可行。
好在我们有word2vec这个工具，这是google的一个开源工具，能够仅仅根据输入的词的集合计算出词与词直接的距离，既然距离知道了自然也就能聚类了，而且这个工具本身就自带了聚类功能，很是强大。下面正式介绍如何使用该工具进行词的分析，关键词分析和聚类自然也就包含其中了。word2vec官网地址看这里：https://code.google.com/p/word2vec/
1、寻找语料 要分析，第一步肯定是收集数据，这里不可能一下子就得到所有词的集合，最常见的方法是自己写个爬虫去收集网页上的数据。不过，如果不需要实时性，我们可以使用别人提供好的网页数据，例如搜狗2012年6月到7月的新闻数据：http://www.sogou.com/labs/dl/ca.html 直接下载完整版，注册一个帐号，然后用ftp下载，ubuntu下推荐用filezilla
下载得到的数据有1.5G
2、分词 我们得到的1.5的数据是包含一些html标签的，我们只需要新闻内容，也就是content其中的值。首先可以通过简单的命令把非content的标签干掉
cat news_tensite_xml.dat | iconv -f gbk -t utf-8 -c | grep &#34;&lt;content&gt;&#34; &gt; corpus.txt 得到了corpus.txt文件只含有content标签之间的内容，再对内容进行分词即可，这里推荐使用之前提到过的ANSJ，没听过的看这里： http://blog.csdn.net/zhaoxinfan/article/details/10403917 下面是调用ANSJ进行分词的程序：
import java.util.HashSet; import java.util.List; import java.util.Set; import java.io.BufferedReader; import java.io.BufferedWriter; import java.io.File; import java.io.FileInputStream; import java.io.FileReader; import java.io.FileWriter; import java.io.IOException; import java.io.InputStreamReader; import java.io.PrintWriter; import java.io.StringReader; import java.util.Iterator; import love.cq.util.IOUtil; import org.ansj.app.newWord.LearnTool; import org.ansj.domain.Term; import org.ansj.recognition.NatureRecognition; import org.ansj.splitWord.Analysis; import org.ansj.splitWord.analysis.NlpAnalysis; import org.ansj.splitWord.analysis.ToAnalysis; import org." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/4de5d006edbf58b6f8c9df9f75a67046/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2013-09-04T17:07:47+08:00" />
<meta property="article:modified_time" content="2013-09-04T17:07:47+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">利用word2vec对关键词进行聚类</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>继上次提取关键词之后，项目组长又要求我对关键词进行聚类。说实话，我不太明白对关键词聚类跟新闻推荐有什么联系，不过他说什么我照做就是了。</p> 
<p>按照一般的思路，可以用新闻ID向量来表示某个关键词，这就像广告推荐系统里面用用户访问类别向量来表示用户一样，然后就可以用kmeans的方法进行聚类了。不过对于新闻来说存在一个问题，那就量太大，如果给你十万篇新闻，那每一个关键词将需要十万维的向量表示，随着新闻数迅速增加，那维度就更大了，这计算起来难度太大。于是，这个方法思路简单但是不可行。</p> 
<p>好在我们有word2vec这个工具，这是google的一个开源工具，能够仅仅根据输入的词的集合计算出词与词直接的距离，既然距离知道了自然也就能聚类了，而且这个工具本身就自带了聚类功能，很是强大。下面正式介绍如何使用该工具进行词的分析，关键词分析和聚类自然也就包含其中了。word2vec官网地址看这里：<a target="_blank" href="https://code.google.com/p/word2vec/" rel="nofollow noopener noreferrer">https://code.google.com/p/word2vec/</a></p> 
<h3>1、寻找语料</h3> 
<p>要分析，第一步肯定是收集数据，这里不可能一下子就得到所有词的集合，最常见的方法是自己写个爬虫去收集网页上的数据。不过，如果不需要实时性，我们可以使用别人提供好的网页数据，例如搜狗2012年6月到7月的新闻数据：<a target="_blank" href="http://www.sogou.com/labs/dl/ca.html" rel="nofollow noopener noreferrer">http://www.sogou.com/labs/dl/ca.html</a> 直接下载完整版，注册一个帐号，然后用ftp下载，ubuntu下推荐用<a target="_blank" href="https://filezilla-project.org/" rel="nofollow noopener noreferrer">filezilla</a></p> 
<p><img src="https://images2.imgbox.com/5a/db/bl9BZD3p_o.png" alt=""><br> </p> 
<p><br> </p> 
<p><img src="https://images2.imgbox.com/45/8a/UMkwljIN_o.png" alt=""><br> </p> 
<p><br> </p> 
<p>下载得到的数据有1.5G</p> 
<p><img src="https://images2.imgbox.com/37/aa/ticmSyIK_o.png" alt=""><br> </p> 
<p><br> </p> 
<h3>2、分词</h3> 
<p>我们得到的1.5的数据是包含一些html标签的，我们只需要新闻内容，也就是content其中的值。首先可以通过简单的命令把非content的标签干掉</p> 
<p></p> 
<pre><code class="language-plain">cat news_tensite_xml.dat | iconv -f gbk -t utf-8 -c | grep "&lt;content&gt;"  &gt; corpus.txt</code></pre> 
<br> 得到了corpus.txt文件只含有content标签之间的内容，再对内容进行分词即可，这里推荐使用之前提到过的ANSJ，没听过的看这里： 
<a target="_blank" href="http://blog.csdn.net/zhaoxinfan/article/details/10403917" rel="noopener noreferrer">http://blog.csdn.net/zhaoxinfan/article/details/10403917</a> 
<p></p> 
<p>下面是调用ANSJ进行分词的程序：</p> 
<p></p> 
<pre><code class="language-java">import java.util.HashSet;
import java.util.List;
import java.util.Set;
import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.PrintWriter;
import java.io.StringReader;
import java.util.Iterator;

import love.cq.util.IOUtil;

import org.ansj.app.newWord.LearnTool;
import org.ansj.domain.Term;
import org.ansj.recognition.NatureRecognition;
import org.ansj.splitWord.Analysis;
import org.ansj.splitWord.analysis.NlpAnalysis;
import org.ansj.splitWord.analysis.ToAnalysis;
import org.ansj.util.*;
import org.ansj.recognition.*;


public class test {
	public static final String TAG_START_CONTENT = "&lt;content&gt;";
    public static final String TAG_END_CONTENT = "&lt;/content&gt;";
    
    public static void main(String[] args) {
        String temp = null ;
        
        BufferedReader reader = null;
        PrintWriter pw = null;
        try {
            reader = IOUtil.getReader("corpus.txt", "UTF-8") ;
            ToAnalysis.parse("test 123 孙") ;
            pw = new PrintWriter("resultbig.txt");
            long start = System.currentTimeMillis()  ;
            int allCount =0 ;
            int termcnt = 0;
            Set&lt;String&gt; set = new HashSet&lt;String&gt;();
            while((temp=reader.readLine())!=null){
                temp = temp.trim();
                if (temp.startsWith(TAG_START_CONTENT)) {
                    int end = temp.indexOf(TAG_END_CONTENT);
                    String content = temp.substring(TAG_START_CONTENT.length(), end);
                    //System.out.println(content);
                    if (content.length() &gt; 0) {
                        allCount += content.length() ;
                        List&lt;Term&gt; result = ToAnalysis.parse(content);
                        for (Term term: result) {
                            String item = term.getName().trim();
                            if (item.length() &gt; 0) {
                                termcnt++;
                                pw.print(item.trim() + " ");
                                set.add(item);
                            }
                        }
                        pw.println();
                    }
                }
            }
            long end = System.currentTimeMillis() ;
            System.out.println("共" + termcnt + "个term，" + set.size() + "个不同的词，共 "
                    +allCount+" 个字符，每秒处理了:"+(allCount*1000.0/(end-start)));
        } catch (IOException e) { 
            e.printStackTrace();
        } finally {
            if (null != reader) {
                try {
                    reader.close();
                } catch (IOException e) {
                    e.printStackTrace();
                }
            }
            if (null != pw) {
                pw.close();
            }
        }
    }
}</code></pre> 
<br> 经过对新闻内容分词之后，得到的输出文件resultbig.txt有2.2G，其中的格式如下： 
<p></p> 
<p><img src="https://images2.imgbox.com/50/5a/RL6D79Gf_o.png" alt=""><br> </p> 
<p>这个文件就是word2vec工具的输入文件</p> 
<p><br> </p> 
<h3>3、本地运行word2vec进行分析</h3> 
<p>首先要做的肯定是从官网上下载word2vec的源码：<a target="_blank" href="http://word2vec.googlecode.com/svn/trunk/" rel="nofollow noopener noreferrer">http://word2vec.googlecode.com/svn/trunk/</a> ，然后把其中makefile文件的.txt后缀去掉，在终端下执行make操作，这时能发现word2vec文件夹下多了好几个东西。接下来就是输入resultbig.txt进行分析了：</p> 
<p></p> 
<pre><code class="language-plain">./word2vec -train resultbig.txt -output vectors.bin -cbow 0 -size 200 -window 5 -negative 0 -hs 1 -sample 1e-3 -threads 12 -binary 1</code></pre> 
<br> 这里我们指定输出为vectors.bin文件，显然输出到文件便于以后重复利用，省得每次都要计算一遍，要知道处理这2.2G的词集合需要接近半个小时的时间： 
<p></p> 
<p><br> </p> 
<p><img src="https://images2.imgbox.com/56/21/LpOueDQE_o.png" alt=""><br> </p> 
<p><br> </p> 
<p>下面再输入计算距离的命令即可计算与每个词最接近的词了：</p> 
<p></p> 
<pre><code class="language-plain">./distance vectors.bin</code></pre> 
<br> 这里列出一些有意思的输出： 
<p></p> 
<p><img src="https://images2.imgbox.com/92/7e/EgvREw9W_o.png" alt=""><img src="https://images2.imgbox.com/6a/ff/05OJle0J_o.png" alt=""><img src="https://images2.imgbox.com/a1/d4/LS5PUq1N_o.png" alt=""><br> </p> 
<p><img src="https://images2.imgbox.com/d6/90/dt2niRmF_o.png" alt=""><img src="https://images2.imgbox.com/b6/1a/5LgzWzmm_o.png" alt=""><br> </p> 
<p><img src="https://images2.imgbox.com/ea/d3/On37cRlN_o.png" alt=""><img src="https://images2.imgbox.com/e3/ed/5dnkGs6T_o.png" alt=""><img src="https://images2.imgbox.com/a8/09/GZUyGJSg_o.png" alt=""></p> 
<p><img src="https://images2.imgbox.com/fc/81/IMIfZ0os_o.png" alt=""><br> </p> 
<p><br> </p> 
<p>怎么样，是不是觉得还挺靠谱的？补充一点，由于word2vec计算的是余弦值，距离范围为0-1之间，值越大代表这两个词关联度越高，所以越排在上面的词与输入的词越紧密。</p> 
<p>至于聚类，只需要另一个命令即可：</p> 
<p></p> 
<pre><code class="language-plain">./word2vec -train resultbig.txt -output classes.txt -cbow 0 -size 200 -window 5 -negative 0 -hs 1 -sample 1e-3 -threads 12 -classes 500</code></pre> 
<br> 
<p>按类别排序：</p> 
<p></p> 
<pre><code class="language-plain">sort classes.txt -k 2 -n &gt; classes.sorted.txt</code></pre> 
<br> 
<br> 
<p></p> 
<p></p> 
<p><br> </p> 
<p><span style="color:rgb(0,102,0)"><span style="font-size:18px">后记：如果想要了解word2vec的实现原理，应该读一读官网后面的三篇参考文献。显然，最主要的应该是这篇：</span><span style="font-family:arial,sans-serif"><span style="font-size:12.800000190734863px"> <span style="font-family:arial,sans-serif; font-size:13px"><a target="_blank" href="http://arxiv.org/pdf/1310.4546.pdf" rel="nofollow noopener noreferrer">Distributed Representations of Words and Phrases and their Compositionality  </a></span></span></span></span></p> 
<p><span style="font-size:18px"><span style="color:#006600">这篇文章的基础是 <a target="_blank" href="http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/35671.pdf" rel="nofollow noopener noreferrer">Natural Language Processing (almost) from Scratch</a> 其中第四部分提到了把deep learning用在NLP上。</span></span></p> 
<p><span style="font-size:18px"><span style="color:#006600">最后感谢晓阳童鞋向我提到这个工具，不愧是立志要成为NLP专家的人。</span></span></p> 
<p><span style="font-size:18px"><span style="color:#006600"><br> </span></span></p> 
<p><span style="font-size:18px"><span style="color:#006600">附：一个在线测试的网站，貌似是一位清华教授做的：<a target="_blank" href="http://cikuapi.com/index.php" rel="nofollow noopener noreferrer">http://cikuapi.com/index.php</a></span></span></p> 
<p><br> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/04fb6606b2744aeccbe75853ca7c53e1/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Geohash距离估算</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/cc981ecc65ecf63ad1673cbec9c64198/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">LTE</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>