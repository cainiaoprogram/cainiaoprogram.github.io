<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>模型压缩总结 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="模型压缩总结" />
<meta property="og:description" content="1.模型复杂度衡量 model sizeRuntime Memory Number of computing operations model size 就是模型的大小，我们一般使用参数量parameter来衡量，注意，它的单位是个。但是由于很多模型参数量太大，所以一般取一个更方便的单位：兆(M) 来衡量（M即为million，为10的6次方）。比如ResNet-152的参数量可以达到60 million = 0.00006M。
有些时候，model size在实际计算时除了包含参数量以外，还包括网络架构信息和优化器信息等。比如存储一个一般的CNN模型(ImageNet训练)需要大于300MB。
这里你可能会有疑问：刚才的单位是M，怎么这里出来了个MB？是不是写错了？
肯定没有，我们需要注意这里的M和MB的换算关系：
比如说我有一个模型参数量是1M，在一般的深度学习框架中(比如说PyTorch)，一般是32位存储。32位存储的意思就是1个参数用32个bit来存储。那么这个拥有1M参数量的模型所需要的存储空间的大小即为：1M * 32 bit = 32Mb = 4MB。因为1 Byte = 8 bit。
所以读到这里你应该明白说一个模型的model size，用M和MB其实是一样的意思。
那你可能还会有疑问：是不是一定要用32位存储？
这个问题很好，现在的quantization技术就是减少参数量所占的位数：比如我用8位存储，那么：
所需要的存储空间的大小即为：1M * 8 bit = 8Mb = 1MB。
更有甚者使用二值神经网络进一步减小参数量所占的位数(权值被限制为{-1, 1}或{-1, 0, 1})，后文有论文的链接，有空再专门介绍这个方法吧。下面简单介绍下参数量的计算方法：
卷积层参数量的计算方法：
Run time Memory 就是模型实际运行时所占的内存。注意这个指标与只存储模型参数所占的存储空间的大小是不一样的，这个指标更大。这对于GPU来讲不算是问题，但是对于硬件能力极为有限的端侧设备来说就显得无法承受了。它的单位是兆字节 (MB)。
Number of computing operations FLOPS MACs 就是模型的计算量，有FLOPs和MACs两种衡量的方式、简而言之，前者指的是乘加的数量，而后者指运算量。比如ResNet-152在前向传播一张256 * 256的图片的运算量可以达到20 GFLOPs。下面简单介绍下模型计算量的计算方法：
第1种：FLOPs：
卷积层FLOPs的计算方法：
只需在parameters的基础上再乘以feature map的大小即可，即对于某个卷积层，它的FLOPs数量为：
全连接层FLOPs的计算方法：
对于全连接层，由于不存在权值共享，它的FLOPs数目即是该层参数数目：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/b8bae77fdc6b6a4d0bc76092297012bb/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-11-15T10:43:40+08:00" />
<meta property="article:modified_time" content="2022-11-15T10:43:40+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">模型压缩总结</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3>1.模型复杂度衡量</h3> 
<ul><li>model size</li><li>Runtime Memory </li><li>Number of computing operations</li></ul> 
<p></p> 
<ul><li> <h4>model size </h4> </li></ul> 
<p>就是模型的大小，我们一般使用参数量parameter来衡量，注意，它的单位是<strong>个</strong>。但是由于很多模型参数量太大，所以一般取一个更方便的单位：<strong>兆(M)</strong> 来衡量（M即为million，为10的6次方）。比如ResNet-152的参数量可以达到60 million = 0.00006M。</p> 
<p>有些时候，model size在实际计算时除了包含参数量以外，还包括网络架构信息和优化器信息等。比如存储一个一般的CNN模型(ImageNet训练)需要大于300MB。</p> 
<p>这里你可能会有疑问：刚才的单位是M，怎么这里出来了个MB？是不是写错了？</p> 
<p>肯定没有，我们需要注意这里的M和MB的换算关系：</p> 
<blockquote> 
 <p>比如说我有一个模型参数量是1M，在一般的深度学习框架中(比如说PyTorch)，一般是32位存储。32位存储的意思就是1个参数用32个bit来存储。那么这个拥有1M参数量的模型所需要的存储空间的大小即为：1M * 32 bit = 32Mb = 4MB。因为1 Byte = 8 bit。</p> 
</blockquote> 
<p>所以读到这里你应该明白说一个模型的model size，用M和MB其实是一样的意思。</p> 
<p>那你可能还会有疑问：是不是一定要用32位存储？</p> 
<p>这个问题很好，现在的quantization技术就是减少参数量所占的位数：比如我用8位存储，那么：</p> 
<blockquote> 
 <p>所需要的存储空间的大小即为：1M * 8 bit = 8Mb = 1MB。</p> 
</blockquote> 
<p>更有甚者使用二值神经网络进一步减小参数量所占的位数(权值被限制为{-1, 1}或{-1, 0, 1})，后文有论文的链接，有空再专门介绍这个方法吧。下面简单介绍下<strong>参数量的计算方法：</strong></p> 
<p><strong>卷积层参数量的计算方法：</strong></p> 
<ul><li>Run time Memory</li></ul> 
<p>就是<strong>模型实际运行时所占的内存</strong>。注意这个指标与<strong>只存储模型参数所占的存储空间</strong>的大小是不一样的，这个指标更大。这对于GPU来讲不算是问题，但是对于硬件能力极为有限的端侧设备来说就显得无法承受了。它的单位是<strong>兆字节 (MB)</strong>。</p> 
<ul><li>Number of computing operations  FLOPS MACs</li></ul> 
<p>就是模型的计算量，有FLOPs和MACs两种衡量的方式、简而言之，前者指的是乘加的数量，而后者指运算量。比如ResNet-152在前向传播一张256 * 256的图片的运算量可以达到20 GFLOPs。下面简单介绍下<strong>模型计算量的计算方法：</strong></p> 
<p><strong>第1种：FLOPs：</strong></p> 
<p><strong>卷积层FLOPs的计算方法：</strong></p> 
<p>只需在parameters的基础上再乘以feature map的大小即可，即对于某个卷积层，它的FLOPs数量为：</p> 
<p><img alt="" height="71" src="https://images2.imgbox.com/bc/00/trp7cgfg_o.png" width="784"></p> 
<p> <img alt="" height="160" src="https://images2.imgbox.com/29/f7/vy4vWP4D_o.png" width="735"></p> 
<p></p> 
<p><strong>全连接层FLOPs的计算方法：</strong></p> 
<p>对于全连接层，由于不存在权值共享，它的FLOPs数目即是该层参数数目：</p> 
<p><img alt="" height="91" src="https://images2.imgbox.com/25/d3/v6YgKoRK_o.png" width="838"></p> 
<p></p> 
<p><strong>第2种：MACs：</strong></p> 
<p><strong>MACs与FLOPs的关系：</strong></p> 
<p>设有全连接层为：</p> 
<blockquote> 
 <p> y = w[0]*x[0] + w[1]*x[1] + w[2]*x[2] + ... + w[n8]*x[8]</p> 
</blockquote> 
<p>对于上式而言共有9次乘加，即9MACs（实际上，9次相乘、9-1次相加，但为了方便统计，将计算量近似记为9MACs。所以近似来看<img alt="1MACs\approx 2FLOPs" class="mathcode" src="https://images2.imgbox.com/a0/6d/drvsis41_o.png"> 。(需要指出的是，现有很多硬件都将乘加运算作为一个单独的指令)。</p> 
<p><strong>全连接层MACs的计算：</strong></p> 
<p><img alt="" height="50" src="https://images2.imgbox.com/e0/41/fQcJ5pg6_o.png" width="829"></p> 
<p><strong>激活层MACs的计算：</strong></p> 
<p>激活层不计算MAC，计算FLOPs。假设激活函数为：</p> 
<p><img alt="" height="82" src="https://images2.imgbox.com/e8/f3/U5FPlY8C_o.png" width="833"></p> 
<p></p> 
<p>则计算量为 <img alt="4N_{out}" class="mathcode" src="https://images2.imgbox.com/7d/28/5scDjvjQ_o.png">FLOPs(乘法，指数，加法，除法)。</p> 
<p>在计算FLOPS时，我们通常将加，减，乘，除，求幂，平方根等计为单个FLOP。</p> 
<p>但是，实际上，我们通常不计这些操作，因为它们只占总时间的一小部分。通常只计算矩阵乘法和点积(dot product)，<strong>忽略激活函数的计算量</strong>。</p> 
<p><strong>卷积层MACC的计算：</strong></p> 
<p><img alt="" height="41" src="https://images2.imgbox.com/a4/44/p4ezPy4Y_o.png" width="851"></p> 
<p> 关于这些指标，更详细的解读以及对应的代码实现可以参考：</p> 
<p>科技猛兽：PyTorch 63.Coding for FLOPs, Params and Latency</p> 
<p>coding实现 <a href="https://zhuanlan.zhihu.com/p/268816646" rel="nofollow" title=" PyTorch 63.Coding for FLOPs, Params and Latency - 知乎"> PyTorch 63.Coding for FLOPs, Params and Latency - 知乎</a></p> 
<p></p> 
<h3>2.常见模型压缩方法</h3> 
<p>剪枝，量化，蒸馏，轻量化模块设计，低秩分解，加法网络等</p> 
<ul><li><strong>剪枝就是通过去除网络中冗余的channels,filters, neurons, or layers以得到一个更轻量级的网络，同时不影响性能。</strong> 代表性的工作有：</li></ul> 
<blockquote> 
 <p><strong>奇异值分解SVD(NIPS 2014)：</strong> Exploiting linear structure within convolutional networks for efficient evaluation<br><strong>韩松(ICLR 2016)：</strong> Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding<br><strong>(NIPS 2015)：</strong> Learning both weights and connections for efficient neural network<br><strong>频域压缩(NIPS 2016)：</strong> Packing convolutional neural networks in the frequency domain<br><strong>剪Filter Reconstruction Error(ICCV 2017)：</strong> Thinet: A filter level pruning method for deep neural network compression<br><strong>LASSO regression(ICCV 2017)：</strong> Channel pruning for accelerating very deep neural networks<br><strong>Discriminative channels(NIPS 2018)：</strong> Discrimination-aware channel pruning for deep neural networks<br><strong>剪枝(ICCV 2017)：</strong> Channel pruning for accelerating very deep neural networks<br><strong>neuron level sparsity(ECCV 2017)：</strong> Less is more: Towards compact cnns<br><strong>Structured Sparsity Learning(NIPS 2016)：</strong> Learning structured sparsity in deep neural networks</p> 
 <p><strong>(ICCV 2017)：清华张长水，黄高团队：</strong> Learning Efficient Convolutional Networks through Network Slimming<br><strong>(ECCV 2020)：得克萨斯大学奥斯汀分校团队：</strong> GAN Slimming: All-in-One GAN Compression by A Unified Optimization Framework</p> 
</blockquote> 
<ul><li><strong>轻量化模块设计就是设计一些计算效率高，适合在端侧设备上部署的模块。</strong> 代表性的工作有：</li></ul> 
<blockquote> 
 <p><strong>Bottleneck(ICLR 2017)：</strong> Squeezenet: Alexnet-level accuracy with 50x fewer parameters and 0.5 mb model size<br><strong>MobileNet(CVPR 2017)：</strong> Mobilenets: Efficient convolutional neural networks for mobile vision applications<br><strong>ShuffleNet(CVPR 2018)：</strong> Shufflenet: An extremely efficient convolutional neural network for mobile devices<br><strong>SE模块(CVPR 2018)：</strong> Squeeze-and-excitation networks<br><strong>无参数的Shift操作(CVPR 2018)：</strong> Shift: A zero flop, zero parameter alternative to spatial convolutions<br><strong>Shift操作填坑(Arxiv)：</strong> Shift-based primitives for efficientconvolutional neural networks<br><strong>多用卷积核(NIPS 2018)：</strong> Learning versatile filters for efficient convolutional neural networks<br><strong>GhostNet(CVPR 2020)：</strong> GhostNet: More features from cheap operations</p> 
</blockquote> 
<ul><li><strong>蒸馏就是过模仿教师网络生成的软标签将知识从大的，预训练过的教师模型转移到轻量级的学生模型。</strong> 代表性的工作有：</li></ul> 
<blockquote> 
 <p><strong>Hinton(NIPS 2015)：</strong> Distilling the knowledge in a neural network<br><strong>中间层的特征作为提示(ICLR 2015)：</strong> Fitnets: Hints for thin deep nets<br><strong>多个Teacher(SIGKDD 2017)：</strong> Learning from multiple teacher networks<br><strong>两个特征得新知识再transfer(CVPR 2017)：</strong> A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</p> 
</blockquote> 
<ul><li><strong>量化就是减少权重等的表示的位数，比如原来网络权值用32 bit存储，现在我只用8 bit来存储，以减少模型的Memory为原来的</strong> <strong>；更有甚者使用二值神经网络。</strong> 代表性的工作有：</li></ul> 
<blockquote> 
 <p><strong>量化：</strong><br><strong>(ICML 2015)：</strong> Compressing neural networks with the hashing trick<br><strong>(NIPS 2015)：</strong> Learning both weights and connections for efficient neural network<br><strong>(CVPR 2018)：</strong> Quantization and training of neural networks for efficient integer-arithmetic-only inference<br><strong>(CVPR 2016)：</strong> Quantized convolutional neural networks for mobile devices<br><strong>(ICML 2018)：</strong> Deep k-means: Re-training and parameter sharing with harder cluster assignments for compressing deep convolutions<br><strong>(CVPR 2019)：</strong> Learning to quantize deep networks by optimizing quantization intervals with task loss<br><strong>(CVPR 2019)：</strong> HAQ: Hardware-Aware automated quantization with mixed precision.</p> 
 <p><strong>二值神经网络：</strong><br><strong>Binarized weights(NIPS 2015)：</strong> BinaryConnect: Training deep neural networks with binary weights during propagations<br><strong>Binarized activations(NIPS 2016)：</strong> Binarized neural networks<br><strong>XNOR(ECCV 2016)：</strong> Xnor-net: Imagenet classification using binary convolutional neural networks<br><strong>more weight and activation(NIPS 2017)：</strong> Towards accurate binary convolutional neural network<br><strong>(ECCV 2020)：</strong> Learning Architectures for Binary Networks<br><strong>(ECCV 2020)：</strong> BATS: Binary ArchitecTure Search</p> 
</blockquote> 
<ul><li><strong>低秩分解就是将原来大的权重矩阵分解成多个小的矩阵，而小矩阵的计算量都比原来大矩阵的计算量要小。</strong> 代表性的工作有：</li></ul> 
<blockquote> 
 <p><strong>低秩分解(ICCV 2017)：</strong> On compressing deep models by low rank and sparse decomposition<br><strong>乐高网络(ICML 2019)：</strong> Legonet: Efficient convolutional neural networks with lego filters<br><strong>奇异值分解(NIPS 2014)：</strong> Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation</p> 
</blockquote> 
<ul><li><strong>加法网络就是：利用卷积所计算的互相关性其实就是一种“相似性的度量方法”，所以在神经网络中用加法代替乘法，在减少运算量的同时获得相同的性能。</strong> 代表性的工作有：</li></ul> 
<blockquote> 
 <p><strong>(CVPR 20)：</strong> AdderNet: Do We Really Need Multiplications in Deep Learning?<br><strong>(NIPS 20)：</strong> Kernel Based Progressive Distillation for Adder Neural Networks<br><strong>(Arxiv)：</strong> AdderSR: Towards Energy Efficient Image Super-Resolution</p> 
</blockquote> 
<p></p> 
<p></p> 
<p></p> 
<p>参考链接：</p> 
<p><a href="https://mp.weixin.qq.com/s/YXR9yeWG_NlH92XBJ0fLVw" rel="nofollow" title="深入浅出的模型压缩：你一定从未见过如此通俗易懂的Slimming操作">深入浅出的模型压缩：你一定从未见过如此通俗易懂的Slimming操作</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a8cbecc7409690f1e8765dd6994fdae3/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">目前主流的跨平台技术、框架有那些？</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/9d4d0f076e05d97146fb874a1eaa7c83/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">ACL的原理及配置</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>