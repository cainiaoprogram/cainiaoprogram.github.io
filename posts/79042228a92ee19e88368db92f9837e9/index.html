<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>AI语音克隆 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="AI语音克隆" />
<meta property="og:description" content="安装 下载安装github代码库
git clone https://github.com/Plachtaa/VITS-fast-fine-tuning.git 安装文档
中日语言模型网站
目前支持的任务:
从 10条以上的短音频 克隆角色声音从 3分钟以上的长音频（单个音频只能包含单说话人） 克隆角色声音从 3分钟以上的视频（单个视频只能包含单说话人） 克隆角色声音通过输入 bilibili视频链接（单个视频只能包含单说话人） 克隆角色声音 本地运行和推理 python VC_inference.py --model_dir ./OUTPUT_MODEL/G_latest.pth --share True 这个时候在本地的浏览器打开网址
http://localhost:7860 就可以看到语音tts的使用界面，但这只能在本地电脑能看到，如果要在远程的电脑上访问，可以使用cpolar
cpolar http 7860 这个时候就会出现一个访问的网址链接。
本地训练 1.创建conda运行环境
conda create -n tts python=3.8 2.安装环境依赖
pip install -r requirements.txt 在这个过程中，有一部分安装包，比如OpenAI的whisper代码包，可能因为网络问题，而无法访问，无法使用pip进行网络安装。可以在其它地方，单独下载好代码包，然后使用pip单独安装本地包。
3.安装GPU版本的PyTorch
# CUDA 11.6 pip install torch==1.13.1&#43;cu116 torchvision==0.14.1&#43;cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116 # CUDA 11.7 pip install torch==1.13.1&#43;cu117 torchvision==0.14.1&#43;cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117 4.安装视频模块包
pip install imageio==2.4.1 pip install moviepy 5." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/79042228a92ee19e88368db92f9837e9/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-11-18T16:05:47+08:00" />
<meta property="article:modified_time" content="2023-11-18T16:05:47+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">AI语音克隆</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="_0"></a>安装</h3> 
<p>下载安装github代码库</p> 
<pre><code>git clone https://github.com/Plachtaa/VITS-fast-fine-tuning.git
</code></pre> 
<p><a href="https://github.com/Plachtaa/VITS-fast-fine-tuning/blob/main/README_ZH.md">安装文档</a><br> <a href="https://huggingface.co/spaces/zomehwh/vits-uma-genshin-honkai" rel="nofollow">中日语言模型网站</a><br> 目前支持的任务:</p> 
<ul><li>从 10条以上的短音频 克隆角色声音</li><li>从 3分钟以上的长音频（单个音频只能包含单说话人） 克隆角色声音</li><li>从 3分钟以上的视频（单个视频只能包含单说话人） 克隆角色声音</li><li>通过输入 bilibili视频链接（单个视频只能包含单说话人） 克隆角色声音</li></ul> 
<h3><a id="_12"></a>本地运行和推理</h3> 
<pre><code>python VC_inference.py --model_dir ./OUTPUT_MODEL/G_latest.pth --share True
</code></pre> 
<p>这个时候在本地的浏览器打开网址</p> 
<pre><code>http://localhost:7860
</code></pre> 
<p>就可以看到语音tts的使用界面，但这只能在本地电脑能看到，如果要在远程的电脑上访问，可以使用cpolar</p> 
<pre><code>cpolar http 7860
</code></pre> 
<p>这个时候就会出现一个访问的网址链接。</p> 
<h3><a id="_25"></a>本地训练</h3> 
<p>1.创建conda运行环境</p> 
<pre><code>conda create -n tts python=3.8
</code></pre> 
<p>2.安装环境依赖</p> 
<pre><code>pip install -r requirements.txt
</code></pre> 
<p>在这个过程中，有一部分安装包，比如OpenAI的whisper代码包，可能因为网络问题，而无法访问，无法使用pip进行网络安装。可以在其它地方，单独下载好代码包，然后使用pip单独安装本地包。<br> 3.安装GPU版本的PyTorch</p> 
<pre><code># CUDA 11.6
pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116
# CUDA 11.7
pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117
</code></pre> 
<p>4.安装视频模块包</p> 
<pre><code>pip install imageio==2.4.1
pip install moviepy
</code></pre> 
<p>5.构建预处理模块</p> 
<pre><code>cd monotonic_align
mkdir monotonic_align
python setup.py build_ext --inplace
cd ..
</code></pre> 
<p>6.下载辅助数据包</p> 
<pre><code>mkdir pretrained_models
# download data for fine-tuning
wget https://huggingface.co/datasets/Plachta/sampled_audio4ft/resolve/main/sampled_audio4ft_v2.zip
unzip sampled_audio4ft_v2.zip
# create necessary directories
mkdir video_data
mkdir raw_audio
mkdir denoised_audio
mkdir custom_character_voice
mkdir segmented_character_voice
</code></pre> 
<p>7.下载预训练模型</p> 
<pre><code>CJE: Trilingual (Chinese, Japanese, English)
CJ: Dualigual (Chinese, Japanese)
C: Chinese only
</code></pre> 
<pre><code>wget https://huggingface.co/spaces/Plachta/VITS-Umamusume-voice-synthesizer/resolve/main/pretrained_models/D_trilingual.pth -O ./pretrained_models/D_0.pth
wget https://huggingface.co/spaces/Plachta/VITS-Umamusume-voice-synthesizer/resolve/main/pretrained_models/G_trilingual.pth -O ./pretrained_models/G_0.pth
wget https://huggingface.co/spaces/Plachta/VITS-Umamusume-voice-synthesizer/resolve/main/configs/uma_trilingual.json -O ./configs/finetune_speaker.json
</code></pre> 
<pre><code>wget https://huggingface.co/spaces/sayashi/vits-uma-genshin-honkai/resolve/main/model/D_0-p.pth -O ./pretrained_models/D_0.pth
wget https://huggingface.co/spaces/sayashi/vits-uma-genshin-honkai/resolve/main/model/G_0-p.pth -O ./pretrained_models/G_0.pth
wget https://huggingface.co/spaces/sayashi/vits-uma-genshin-honkai/resolve/main/model/config.json -O ./configs/finetune_speaker.json
</code></pre> 
<pre><code>wget https://huggingface.co/datasets/Plachta/sampled_audio4ft/resolve/main/VITS-Chinese/D_0.pth -O ./pretrained_models/D_0.pth
wget https://huggingface.co/datasets/Plachta/sampled_audio4ft/resolve/main/VITS-Chinese/G_0.pth -O ./pretrained_models/G_0.pth
wget https://huggingface.co/datasets/Plachta/sampled_audio4ft/resolve/main/VITS-Chinese/config.json -O ./configs/finetune_speaker.json
</code></pre> 
<p>8.将语音数据放置在对应的文件目录</p> 
<ul><li>短语音<br> 将多段语音打包成zip文件，文件结构为</li></ul> 
<pre><code>Your-zip-file.zip
├───Character_name_1
├   ├───xxx.wav
├   ├───...
├   ├───yyy.mp3
├   └───zzz.wav
├───Character_name_2
├   ├───xxx.wav
├   ├───...
├   ├───yyy.mp3
├   └───zzz.wav
├───...
├
└───Character_name_n
    ├───xxx.wav
    ├───...
    ├───yyy.mp3
    └───zzz.wav
</code></pre> 
<p>将打包文件放置在<code>./custom_character_voice/</code><br> 运行</p> 
<pre><code>unzip ./custom_character_voice/custom_character_voice.zip -d ./custom_character_voice/
</code></pre> 
<ul><li>长语音<br> 将wav格式的语音命名为<code>Diana_234135.wav</code>，放置在<code>./raw_audio/</code></li><li>视频<br> 将视频命名为<code>Taffy_332452.mp4</code>，放置在<code>./video_data/</code></li></ul> 
<p>9.处理音频</p> 
<pre><code>python scripts/video2audio.py
python scripts/denoise_audio.py
python scripts/long_audio_transcribe.py --languages "{PRETRAINED_MODEL}" --whisper_size large
python scripts/short_audio_transcribe.py --languages "{PRETRAINED_MODEL}" --whisper_size large
python scripts/resample.py
</code></pre> 
<p>注意将<code>"{PRETRAINED_MODEL}"</code>替换为<code>"C"</code>，如果GPU内存没有12GB，将whisper_size替换为medium或small。</p> 
<p>10.处理文本数据<br> 选择对应的辅助数据包，运行</p> 
<pre><code>python preprocess_v2.py --add_auxiliary_data True --languages "C"
</code></pre> 
<p>如果不选择辅助数据包，运行</p> 
<pre><code>python preprocess_v2.py --languages "{PRETRAINED_MODEL}"
</code></pre> 
<p>11.开始训练<br> 运行命令，开始训练</p> 
<pre><code>python finetune_speaker_v2.py -m ./OUTPUT_MODEL --max_epochs "{Maximum_epochs}" --drop_speaker_embed True
</code></pre> 
<p>如果是从一个训练过的模型，开始继续训练</p> 
<pre><code>python finetune_speaker_v2.py -m ./OUTPUT_MODEL --max_epochs "{Maximum_epochs}" --drop_speaker_embed False --cont True
</code></pre> 
<p>12.清除语音数据</p> 
<pre><code>rm -rf ./custom_character_voice/* ./video_data/* ./raw_audio/* ./denoised_audio/* ./segmented_character_voice/* ./separated/* long_character_anno.txt short_character_anno.txt
</code></pre> 
<pre><code>del /Q /S .\custom_character_voice\* .\video_data\* .\raw_audio\* .\denoised_audio\* .\segmented_character_voice\* .\separated\* long_character_anno.txt short_character_anno.txt
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/bb53441ef28efff42e2a60f08e280672/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【尚跑】马拉松冬训指南</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c43536d57749e396790aa363d40ae01b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">QT专栏1 -Qt安装教程</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>