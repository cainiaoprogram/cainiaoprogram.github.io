<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Yolov8原理详细解析！一文看懂 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Yolov8原理详细解析！一文看懂" />
<meta property="og:description" content="引言 Yolo(You Only Look Once)是一种one-stage目标检测算法，即仅需要 “看” 一次就可以识别出图片中物体的class类别和边界框。Yolov8是Ultralytics公司最新推出的Yolo系列目标检测算法，可以用于图像分类、物体检测和实例分割等任务。
根据官方描述，Yolov8是一个SOTA模型，它建立在Yolo系列历史版本的基础上，并引入了新的功能和改进点，以进一步提升性能和灵活性，使其成为实现目标检测、图像分割、姿态估计等任务的最佳选择。其具体创新点包括一个新的骨干网络、一个新的Ancher-Free检测头和一个新的损失函数，可在CPU到GPU的多种硬件平台上运行。
此外，Yolov8还有一个特点就是可扩展性，ultralytics没有直接将开源库命名为Yolov8，而是直接使用&#34;ultralytics&#34;，将其定位为算法框架，而非某一个特定算法。这也使得Yolov8开源库不仅仅能够用于Yolo系列模型，而且能够支持非Yolo模型以及分类分割姿态估计等各类任务。
总而言之，Yolov8是Yolo系列模型的最新王者，各种指标全面超越现有对象检测与实例分割模型，借鉴了Yolov5、Yolov6、YoloX等模型的设计优点，在全面提升改进Yolov5模型结构的基础上实现，同时保持了Yolov5工程化简洁易用的优势。
如下表所示是基于COCO Val 2017数据集测试并对比Yolov8和Yolov5的mAP、参数量和FLOPs结果。由此可以看出，Yolov8相比Yolov5精度提升比较多，但是n/s/m模型参数量和flops增加不少，但是相比Yolov5大部分模型推理速度变慢了。
FlOPs(floating point operations)：浮点运算次数，用于衡量算法/模型的复杂度。
FLOPS（全部大写）(floating point operations per second)：每秒运算的浮点数，可以理解为计算速度，用于衡量硬件性能。
这里是衡量模型的复杂度，因此选择FlOPs。
模型Yolov5(300epoch)params(M)FLOPS@640(B)Yolov8(500epoch)params(M)FLOPS@640(B)n28.01.94.537.33.28.7s37.47.216.544.911.228.6m45.421.249.050.225.978.9l49.046.5109.152.943.7165.2x50.786.7205.753.968.2257.8 这里需要注意的是，目前各个Yolo系列算法都只是在COCO数据集上性能提升明显，然而在自定义数据集上的泛化性尚未得到充分验证。
Yolov8创新点 Yolov8主要借鉴了Yolov5、Yolov6、YoloX等模型的设计优点，其本身创新点不多，偏重在工程实践上，具体创新如下：
提供了一个全新的SOTA模型（包括P5 640和P6 1280分辨率的目标检测网络和基于YOLACT的实例分割模型）。并且，基于缩放系数提供了N/S/M/L/X不同尺度的模型，以满足不同部署平台和应用场景的需求。Backbone：同样借鉴了CSP模块思想，不过将Yolov5中的C3模块替换成了C2f模块，实现了进一步轻量化，同时沿用Yolov5中的SPPF模块，并对不同尺度的模型进行精心微调，不再是无脑式一套参数用于所有模型，大幅提升了模型性能。Neck：继续使用PAN的思想，但是通过对比YOLOv5与YOLOv8的结构图可以看到，YOLOv8移除了1*1降采样层。Head部分相比YOLOv5改动较大，Yolov8换成了目前主流的解耦头结构(Decoupled-Head)，将分类和检测头分离，同时也从Anchor-Based换成了Anchor-Free。Loss计算：使用VFL Loss作为分类损失(实际训练中使用BCE Loss)；使用DFL Loss&#43;CIOU Loss作为回归损失。标签分配：Yolov8抛弃了以往的IoU分配或者单边比例的分配方式，而是采用Task-Aligned Assigner正负样本分配策略。 Yolov8网络结构 Yolov8模型网络结构图如下图所示。
Backbone Yolov8的Backbone同样借鉴了CSPDarkNet结构网络结构，与Yolov5最大区别是，Yolov8使用C2f模块代替C3模块。具体改进如下：
第一个卷积层的Kernel size从6×6改为3x3。所有的C3模块改为C2f模块，如下图所示，多了更多的跳层连接和额外Split操作。Block数由C3模块3-6-9-3改为C2f模块的3-6-6-3。 C2f与C3对比
由上图可以看出，C2f中每个BottleNeck的输入Tensor的通道数channel都只是上一级的0.5倍，因此计算量明显降低。从另一方面讲，梯度流的增加，也能够明显提升收敛速度和收敛效果。
C2f模块首先以输入tensor(n,c,h,w)经过Conv1层进行split拆分，分成两部分(n,0.5c,h,w)，一部分直接经过n个Bottlenck，另一部分经过每一操作层后都会以(n,0.5c,h,w)的尺寸进行Shortcut，最后通过Conv2层卷积输出。也就是对应n&#43;2的Shortcut（第一层Conv1的分支tensor和split后的tensor为2&#43;n个bottenlenneck）。
Neck YOLOv8的Neck采用了PANet结构，如下图所示。
Backbone最后SPPF模块(Layer9)之后H、W经过32倍下采样，对应地Layer4经过8倍下采样，Layer6经过16倍下采样。输入图片分辨率为640*640，得到Layer4、Layer6、Layer9的分辨率分别为80*80、40*40和20*20。
Layer4、Layer6、Layer9作为PANet结构的输入，经过上采样，通道融合，最终将PANet的三个输出分支送入到Detect head中进行Loss的计算或结果解算。
与FPN(单向，自上而下)不同的是，PANet是一个双向通路网络，引入了自下向上的路径，使得底层信息更容易传递到顶层。
Head Head部分相比Yolov5改动较大，直接将耦合头改为类似Yolox的解耦头结构(Decoupled-Head)，将回归分支和预测分支分离，并针对回归分支使用了Distribution Focal Loss策略中提出的积分形式表示法。之前的目标检测网络将回归坐标作为一个确定性单值进行预测，DFL将坐标转变成一个分布。
yaml配置文件解析 参数部分 Yolov8采用Anchor-Free方式，因而在yaml文件中移除了anchors参数，并且将多个不同版本的模型参数写在一个yaml，同时在深度因子和宽度因子后面增加了 最大通道数 这一参数。
Backbone # [from, repeats, module, args] from：本层的来源，即就是输入。-1表示将上层的输出作为本层的输入。 repeats：本层重复次数。 module：本层名称。 args：本层参数。 第0层：[-1,1,Conv,[64, 3, 2]] #0-P1/2 -1表示将上层的输出作为本层的输入，第0层的输入是640*640*3的图像。Conv表示卷积层。[64, 3, 2]：输出通道数64，卷积核大小k为3，stride步长为2。由此计算padding为1。输出特征图大小(向下取整1)：f_out=((f_in - k &#43; 2*p ) / s )=((640 - 3 &#43; 2*1 ) / 2 )=320。所以本层输出特征图尺寸为320*320*64，长宽为原输入图片的1/2。 第1层：[-1,1,Conv,[128, 3, 2]] # 1-P2/4 [128, 3, 2]：输出通道数128，卷积核大小k为3，stride步长为2。输出特征图大小(向下取整1)：f_out=((f_in - k &#43; 2*p ) / s )=((320 - 3 &#43; 2*1 ) / 2 )=160。所以本层输出特征图尺寸为160*160*128，长宽为原输入图片的1/4。 第2层：[-1,3,C2f,[128, True]] [128, True]：128表示输出通道数，True表示Bottleneck有shortcut。本层输出特征图尺寸仍为160*160*128。 第3层：[-1,1,Conv,[256,3,2]] # 3-P3/8 [256,3,2]：输出通道数256，卷积核大小k为3，stride步长为2。输出特征图大小(向下取整1)：f_out=((f_in - k &#43; 2*p ) / s )=((160-3&#43; 2*1 )/2)=80。所以本层输出特征图尺寸为80*80*256，长宽为原输入图片的1/8。 …第9层：[-1,1,SPPF,[1024, 5]] [1024, 5]：1024表示输出通道数，5表示池化核大小k。输出特征图尺寸为20*20*1024。 Head 第10层：[-1,1,nn." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/5c4b011b25c2a9f8ac0b0bbac3e8699c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-08-30T15:12:36+08:00" />
<meta property="article:modified_time" content="2023-08-30T15:12:36+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Yolov8原理详细解析！一文看懂</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="_0"></a>引言</h3> 
<p>Yolo(You Only Look Once)是一种one-stage目标检测算法，即仅需要 <strong>“看”</strong> 一次就可以识别出图片中物体的class类别和边界框。<strong>Yolov8</strong>是Ultralytics公司最新推出的Yolo系列目标检测算法，可以用于图像分类、物体检测和实例分割等任务。<br> <img src="https://images2.imgbox.com/80/41/gbsIiLy5_o.png" alt="在这里插入图片描述"><br> 根据官方描述，Yolov8是一个SOTA模型，它建立在Yolo系列历史版本的基础上，并引入了新的功能和改进点，以进一步提升性能和灵活性，使其成为实现目标检测、图像分割、姿态估计等任务的最佳选择。其具体创新点包括一个新的骨干网络、一个新的Ancher-Free检测头和一个新的损失函数，可在CPU到GPU的多种硬件平台上运行。</p> 
<p>此外，Yolov8还有一个特点就是可扩展性，ultralytics没有直接将开源库命名为Yolov8，而是直接使用"ultralytics"，将其定位为算法框架，而非某一个特定算法。这也使得Yolov8开源库不仅仅能够用于Yolo系列模型，而且能够支持非Yolo模型以及分类分割姿态估计等各类任务。</p> 
<p>总而言之，Yolov8是Yolo系列模型的最新王者，各种指标全面超越现有对象检测与实例分割模型，借鉴了Yolov5、Yolov6、YoloX等模型的设计优点，<strong>在全面提升改进Yolov5模型结构的基础上实现，同时保持了Yolov5工程化简洁易用的优势。</strong><br> <img src="https://images2.imgbox.com/d5/cb/YTFvWyfY_o.png" alt="在这里插入图片描述"><br> 如下表所示是基于COCO Val 2017数据集测试并对比Yolov8和Yolov5的mAP、参数量和FLOPs结果。由此可以看出，Yolov8相比Yolov5精度提升比较多，但是n/s/m模型参数量和flops增加不少，但是相比Yolov5大部分模型推理速度变慢了。</p> 
<blockquote> 
 <p>FlOPs(floating point operations)：浮点运算次数，用于衡量算法/模型的复杂度。<br> FLOPS（全部大写）(floating point operations per second)：每秒运算的浮点数，可以理解为计算速度，用于衡量硬件性能。<br> 这里是衡量模型的复杂度，因此选择FlOPs。</p> 
</blockquote> 
<table><thead><tr><th>模型</th><th>Yolov5(300epoch)</th><th>params(M)</th><th>FLOPS@640(B)</th><th>Yolov8(500epoch)</th><th>params(M)</th><th>FLOPS@640(B)</th></tr></thead><tbody><tr><td>n</td><td>28.0</td><td>1.9</td><td>4.5</td><td>37.3</td><td>3.2</td><td>8.7</td></tr><tr><td>s</td><td>37.4</td><td>7.2</td><td>16.5</td><td>44.9</td><td>11.2</td><td>28.6</td></tr><tr><td>m</td><td>45.4</td><td>21.2</td><td>49.0</td><td>50.2</td><td>25.9</td><td>78.9</td></tr><tr><td>l</td><td>49.0</td><td>46.5</td><td>109.1</td><td>52.9</td><td>43.7</td><td>165.2</td></tr><tr><td>x</td><td>50.7</td><td>86.7</td><td>205.7</td><td>53.9</td><td>68.2</td><td>257.8</td></tr></tbody></table> 
<p><strong>这里需要注意的是，目前各个Yolo系列算法都只是在COCO数据集上性能提升明显，然而在自定义数据集上的泛化性尚未得到充分验证。</strong></p> 
<h3><a id="Yolov8_25"></a>Yolov8创新点</h3> 
<p>Yolov8主要借鉴了Yolov5、Yolov6、YoloX等模型的设计优点，其本身创新点不多，偏重在工程实践上，具体创新如下：</p> 
<ul><li>提供了一个全新的SOTA模型（包括P5 640和P6 1280分辨率的目标检测网络和基于YOLACT的实例分割模型）。并且，基于缩放系数提供了N/S/M/L/X不同尺度的模型，以满足不同部署平台和应用场景的需求。</li><li>Backbone：同样借鉴了CSP模块思想，不过将Yolov5中的C3模块替换成了C2f模块，实现了进一步轻量化，同时沿用Yolov5中的SPPF模块，并对不同尺度的模型进行精心微调，不再是无脑式一套参数用于所有模型，大幅提升了模型性能。</li><li>Neck：继续使用PAN的思想，但是通过对比YOLOv5与YOLOv8的结构图可以看到，YOLOv8移除了1*1降采样层。</li><li>Head部分相比YOLOv5改动较大，Yolov8换成了目前主流的解耦头结构(Decoupled-Head)，将分类和检测头分离，同时也从Anchor-Based换成了Anchor-Free。</li><li>Loss计算：使用VFL Loss作为分类损失(实际训练中使用BCE Loss)；使用DFL Loss+CIOU Loss作为回归损失。</li><li>标签分配：Yolov8抛弃了以往的IoU分配或者单边比例的分配方式，而是采用Task-Aligned Assigner正负样本分配策略。</li></ul> 
<h3><a id="Yolov8_35"></a>Yolov8网络结构</h3> 
<p>Yolov8模型网络结构图如下图所示。<br> <img src="https://images2.imgbox.com/bb/79/uSuL5P9l_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="Backbone_39"></a>Backbone</h4> 
<p>Yolov8的Backbone同样借鉴了CSPDarkNet结构网络结构，与Yolov5最大区别是，Yolov8使用C2f模块代替C3模块。具体改进如下：</p> 
<ul><li>第一个卷积层的Kernel size从6×6改为3x3。</li><li>所有的C3模块改为C2f模块，如下图所示，多了更多的跳层连接和额外Split操作。</li><li>Block数由C3模块3-6-9-3改为C2f模块的3-6-6-3。</li></ul> 
<p><strong>C2f与C3对比</strong></p> 
<p><img src="https://images2.imgbox.com/d4/d3/8DzQAEdw_o.png" alt="在这里插入图片描述"></p> 
<p>由上图可以看出，C2f中每个BottleNeck的输入Tensor的通道数channel都只是上一级的0.5倍，因此计算量明显降低。从另一方面讲，梯度流的增加，也能够明显提升收敛速度和收敛效果。</p> 
<p>C2f模块首先以输入tensor(n,c,h,w)经过Conv1层进行split拆分，分成两部分(n,0.5c,h,w)，一部分直接经过n个Bottlenck，另一部分经过每一操作层后都会以(n,0.5c,h,w)的尺寸进行Shortcut，最后通过Conv2层卷积输出。也就是对应n+2的Shortcut（第一层Conv1的分支tensor和split后的tensor为2+n个bottenlenneck）。</p> 
<h4><a id="Neck_55"></a>Neck</h4> 
<p>YOLOv8的Neck采用了PANet结构，如下图所示。</p> 
<p><img src="https://images2.imgbox.com/b1/a3/WAx1O6cj_o.png" alt="在这里插入图片描述"><br> Backbone最后SPPF模块(Layer9)之后H、W经过32倍下采样，对应地Layer4经过8倍下采样，Layer6经过16倍下采样。输入图片分辨率为640*640，得到Layer4、Layer6、Layer9的分辨率分别为80*80、40*40和20*20。</p> 
<p>Layer4、Layer6、Layer9作为PANet结构的输入，经过上采样，通道融合，最终将PANet的三个输出分支送入到Detect head中进行Loss的计算或结果解算。</p> 
<p>与FPN(单向，自上而下)不同的是，PANet是一个双向通路网络，引入了自下向上的路径，使得底层信息更容易传递到顶层。</p> 
<h4><a id="Head_65"></a>Head</h4> 
<p>Head部分相比Yolov5改动较大，直接将耦合头改为类似Yolox的解耦头结构(Decoupled-Head)，将回归分支和预测分支分离，并针对回归分支使用了Distribution Focal Loss策略中提出的积分形式表示法。之前的目标检测网络将回归坐标作为一个确定性单值进行预测，DFL将坐标转变成一个分布。<br> <img src="https://images2.imgbox.com/de/6d/Q6D6v04s_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="yaml_69"></a>yaml配置文件解析</h3> 
<h4><a id="_70"></a>参数部分</h4> 
<p><img src="https://images2.imgbox.com/5f/02/XfbFUNpW_o.png" alt="在这里插入图片描述"><br> Yolov8采用Anchor-Free方式，因而在yaml文件中移除了anchors参数，并且将多个不同版本的模型参数写在一个yaml，同时在深度因子和宽度因子后面增加了 <code>最大通道数</code> 这一参数。</p> 
<h4><a id="Backbone_74"></a>Backbone</h4> 
<p><img src="https://images2.imgbox.com/8a/97/5C75ITe1_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-BASH"># [from, repeats, module, args] 
from：本层的来源，即就是输入。-1表示将上层的输出作为本层的输入。
repeats：本层重复次数。
module：本层名称。
args：本层参数。
</code></pre> 
<ul><li>第0层：[-1,1,Conv,[64, 3, 2]] #0-P1/2 
  <ul><li>-1表示将上层的输出作为本层的输入，第0层的输入是640*640*3的图像。</li><li>Conv表示卷积层。</li><li>[64, 3, 2]：输出通道数64，卷积核大小k为3，stride步长为2。由此计算padding为1。</li><li>输出特征图大小(<strong>向下取整1</strong>)：<code>f_out=((f_in - k + 2*p ) / s )=((640 - 3 + 2*1 ) / 2 )=320</code>。</li><li>所以本层输出特征图尺寸为320*320*64，长宽为原输入图片的1/2。</li></ul> </li><li>第1层：[-1,1,Conv,[128, 3, 2]] # 1-P2/4 
  <ul><li>[128, 3, 2]：输出通道数128，卷积核大小k为3，stride步长为2。</li><li>输出特征图大小(<strong>向下取整1</strong>)：<code>f_out=((f_in - k + 2*p ) / s )=((320 - 3 + 2*1 ) / 2 )=160</code>。</li><li>所以本层输出特征图尺寸为160*160*128，长宽为原输入图片的1/4。</li></ul> </li><li>第2层：[-1,3,C2f,[128, True]] 
  <ul><li>[128, True]：128表示输出通道数，True表示Bottleneck有shortcut。</li><li>本层输出特征图尺寸仍为160*160*128。</li></ul> </li><li>第3层：[-1,1,Conv,[256,3,2]] # 3-P3/8 
  <ul><li>[256,3,2]：输出通道数256，卷积核大小k为3，stride步长为2。</li><li>输出特征图大小(<strong>向下取整1</strong>)：<code>f_out=((f_in - k + 2*p ) / s )=((160-3+ 2*1 )/2)=80</code>。</li><li>所以本层输出特征图尺寸为80*80*256，长宽为原输入图片的1/8。</li></ul> </li><li>…</li><li>第9层：[-1,1,SPPF,[1024, 5]] 
  <ul><li>[1024, 5]：1024表示输出通道数，5表示池化核大小k。</li><li>输出特征图尺寸为20*20*1024。</li></ul> </li></ul> 
<h4><a id="Head_108"></a>Head</h4> 
<p><img src="https://images2.imgbox.com/2b/f0/BIr1Mmin_o.png" alt="在这里插入图片描述"></p> 
<ul><li> <h3><a id="1011nnUpsampleNone2nearest_112"></a>第10层：[-1,1,nn.Upsample,[None,2,‘nearest’]]</h3> <pre><code class="prism language-BASH">  torch.nn.Upsample(size=None, scale_factor=None, mode='nearest', align_corners=None)
</code></pre> 
  <ul><li>本层为上采样层，-1表示将上层的输出作为本层的输入。None表示上采样的输出尺寸size不指定。2表示scale_factor=2，即输出尺寸是输入的2倍，nearest表示使用的上采样算法为最近邻插值算法。经过这层之后，特征图的长和宽变为原来的二倍，通道数不变，所以输出特征图尺寸为40*40*1024。</li></ul> </li><li>第11层：[[-1,6],1,Concat,[1]] #cat backbone P4 
  <ul><li>本层为cancat层，[-1,6]表示将上层和第6层的输出作为本层的输入，[1]表示concat拼接的维度为1。上层的输出尺寸为40*40*1024，第6层的输出尺寸为40*40*512，最终本层的输出尺寸为40*40*1536。</li></ul> </li><li>…</li><li>第21层：[-1,3,C2f,[1024]] # 21(P5/32-large) 
  <ul><li>本层是C2f模块，3表示本层重复3次。1024表示输出通道数。经过这层之后，特征图尺寸变为20*20*1024，特征图的长宽已经变成原输入图片的1/32。</li></ul> </li><li>第22层：[[15, 18, 21], 1, Detect, [nc]] 
  <ul><li>本层是Detect层，[15, 18, 21]表示将第15、18、21层的输出（分别是80*80*256、40*40*512、20*20*1024）作为本层的输入。nc是数据集的类别数。</li></ul> </li></ul> 
<h3><a id="Loss_125"></a>Loss计算</h3> 
<p>Loss计算过程包括两部分：<strong>正负样本分配策略和Loss计算</strong>。</p> 
<h4><a id="_128"></a>正负样本分配策略</h4> 
<p>在目标检测中，正负样本分配策略是指在训练期间为每个样本分配一个权重，以便模型更加关注困难的样本和重要的样本。</p> 
<p>常见的正负样本分配策略包括<strong>动态分配策略和静态分配策略</strong>两种。</p> 
<p><strong>静态分配策略</strong></p> 
<p>静态分配策略是指在训练开始之前，固定为一组预先定义的权重，这些权重不会在训练过程中改变。这种分配策略通常基于经验得出，可以根据数据集的特点进行调整，但是不够灵活，可能无法充分利用样本的信息，导致训练效果不佳。</p> 
<p><strong>动态分配策略</strong></p> 
<p>动态分配策略则可以根据训练的进展和样本的特点动态调整权重。在训练初期，模型可能会很难区分正负样本，因此应该更加关注那些容易被错分的样本。随着训练的进行，模型逐渐变得更加强大，可以更好地区分样本，因此应该逐渐减小困难样本的权重，同时增加易分样本的权重。动态分配策略可以根据训练损失或者其他指标来进行调整，可以更好地适应不同的数据集和模型。</p> 
<p>典型的动态分配策略如YOLOX的simOTA、TOOD的Task-Aligned Assigner和RTMDet的DynamicSoftLabelAssigner等。YOLOv5采用的依然是静态分配策略，考虑到动态分配策略的优异性，<strong>Yolov8算法中直接引用了TOOD中的Task-Aligned Assigner正负样本分配策略</strong>。</p> 
<p>Task-Aligned Assigner，顾名思义就是对齐分配器，即在训练过程中动态调整正负样本的分配比例。怎么个对齐方式呢？<strong>根据分类与回归的分数，作为加权分数，选择正样本</strong>。公式如下：<br> <img src="https://images2.imgbox.com/e3/af/lViqadjh_o.png" alt="在这里插入图片描述"><br> 其中，</p> 
<ul><li>s是标注类别对应的预测分值，u是预测框和GT Box的IoU，两者相乘即可衡量对齐程度(Task-Alignment)。α和β是权重超参数。</li><li>t可以同时控制分类得分和IoU的优化来实现Task-Alignment，从而引导网络动态关注高质量的anchor。当类别分值越高且IoU越高时，t的值就越接近于1。</li></ul> 
<p>具体执行步骤如下：</p> 
<ol><li>基于分类得分和预测框与GT的IoU，加权得到一个关联分类以及回归的对齐分数alignment_metrics。</li><li>计算anchor的中心点是否在当前GT框内，只有在当前GT内的anchor才能作为正样本。</li><li>满足<strong>2</strong>的前提下，基于alignment_metrics选取topK大的作为正样本，其余作为负样本进行训练。</li></ol> 
<p><strong>代码示例</strong></p> 
<pre><code class="prism language-Python"># 1. 计算对齐分数alignment_metrics
alignment_metrics = bbox_scores.pow(self.alpha)*overlaps.pow(self.beta)
# 2. 保证中心点在GT内部的mask
is_in_gts = select_candidates_in_gts(priors,gt_bboxes)
# 3. 选取TopK大的对齐分数的样本
topk_metric = self.select_topk_candidates(
            alignment_metrics * is_in_gts,
            topk_mask=pad_bbox_flag.repeat([1, 1, self.topk]).bool())

</code></pre> 
<h4><a id="Loss_170"></a>Loss计算</h4> 
<p>Loss计算包括2个分支：分类和回归分支，没有了之前的objectness(目标前景/背景)分支。</p> 
<h5><a id="_173"></a>分类损失</h5> 
<p>使用sigmoid函数来计算每个类别地概率，并采用VFL Loss或BCE Loss计算全局的类别损失。</p> 
<p><strong>VFL Loss</strong></p> 
<p>VFL Loss(Varifocal Loss)的灵感来自Focal Loss，这里也简要回顾一下Focal Loss。Focal Loss的设计是为了解决密集目标检测器训练中前景类和背景类之间极度不平衡的问题。Focal Loss定义为:</p> 
<p><img src="https://images2.imgbox.com/6c/cd/3qHh3Jzh_o.png" alt="在这里插入图片描述"></p> 
<p>其中，1为ground-truth类，表示前景类的预测概率。如公式所示，调制因子(γ为前景类和γ为背景类)可以减少简单样例的损失贡献，相对增加误分类样例的重要性。</p> 
<p>然而，不同的Focal Loss处理的正负样本是对称的，VFL Loss则是提出了非对称的加权操作，定义如下：</p> 
<p><img src="https://images2.imgbox.com/53/d2/w3GKBAVZ_o.png" alt="在这里插入图片描述"></p> 
<p>其中，预测值为目标分数。对于前景点将其ground truth类别分数设定为<strong>生成的边界框和它的ground truth(gt_IoU)之间的IoU</strong>，否则为0；而对于背景点，所有类的分数为0。</p> 
<p>如公式所示，通过利用γ的因子缩放损失，VFL Loss仅减少了负例(q=0)的损失贡献，而不以同样的方式降低正例(q&gt;0)的权重。这是因为正样本相对于负样本是非常罕见的，应该保留它们的学习信息。</p> 
<p><strong>BCE Loss</strong></p> 
<p>由源代码可以看出，Yolov8实际采用的是BCE Loss，如下图所示：</p> 
<p><img src="https://images2.imgbox.com/ac/19/JHj8qCIW_o.png" alt="在这里插入图片描述"></p> 
<p>这也说明，Yolov8团队应该是对VFL Loss和BCE Loss都尝试过，但最终发现使用VFL和使用普通的BCE效果相当，优势不明显，故采用了简单的BCE Loss。</p> 
<h5><a id="_203"></a>回归损失</h5> 
<p>Yolov8的回归损失计算分为<strong>CIou_Loss + Distribution Focal Loss</strong>两部分，其中，CIou_Loss用于计算预测框与目标框之间的IoU。</p> 
<p><strong>DFL Loss</strong></p> 
<p>常规的坐标点回归方式是一种狄拉克分布，即认为某一点概率无穷大，而其它点概率为0（概率密度是一条尖锐的竖线，如下图），这种方式认为标签是绝对正确的。</p> 
<p><img src="https://images2.imgbox.com/8f/59/jtDZ3rNF_o.png" alt="在这里插入图片描述"></p> 
<p>但是在实际应用中，对于遮挡、模糊场景下目标框的边界存在一定的不确定性，如下图中滑板左侧边界和大象右侧边界，常规的回归方式是不能解决这种不确定问题的，此时学习一个边界分布更为合理。</p> 
<p><img src="https://images2.imgbox.com/6a/d4/deenCqA6_o.png" alt="在这里插入图片描述"><br> 但是如果分布过于随意，网络学习的效率可能不会高，因为一个积分目标可能对应无穷多种分布模式。<br> <img src="https://images2.imgbox.com/28/58/AIqCvvRT_o.png" alt="在这里插入图片描述"><br> 考虑到真实的分布通常不会距离标注的位置太远，因此Distribution Focal Loss选择优化标签y附近左右两个位置（yi与yi+1）的概率，使得网络分布聚焦到标签值附近。<br> <img src="https://images2.imgbox.com/db/96/djQpY18d_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e864d531ff49cb019da5280419d0a746/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【原理/计算机网络】HTTPS中TLS握手过程（实验）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/608cce4398d70ab107872f8f576e471b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Centos7 百度网盘下载</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>