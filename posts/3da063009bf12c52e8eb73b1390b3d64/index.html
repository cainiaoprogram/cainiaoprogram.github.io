<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【论文阅读】《Distilling the Knowledge in a Neural Network》 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【论文阅读】《Distilling the Knowledge in a Neural Network》" />
<meta property="og:description" content="【论文阅读】《Distilling the Knowledge in a Neural Network》 推荐指数： 1. 动机 （1）虽然一个ensemble的模型可以提升模型的效果，但是在效率方面实在难以接受，尤其是在每个模型都是一个大型的网络模型的时候。
（2）前人的研究结果也已表明：模型参数有很多其实是冗余的。
2. 方法 distilling the knowledge in an ensemble of models into a single model.
作者们之所以这么做又是因为之前有篇文章得到的结论，这个结论【这是一个很重要的结论】是： it is possible to compress the knowledge in an ensemble into single model.
更加具体的就是：
raise the temperature of the final softmax until the cumbersome model produces a suitably soft set of targets.
3.具体实现 在谈具体实现之前，先把本文涉及到的一些专有术语解释一下：
distilled model : 小模型（学生模型） We have shown that distilling works very well for transferring knowledge from an ensemble or from a large highly regularized model into a smaller, distilled model." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/3da063009bf12c52e8eb73b1390b3d64/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-16T21:19:33+08:00" />
<meta property="article:modified_time" content="2023-07-16T21:19:33+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【论文阅读】《Distilling the Knowledge in a Neural Network》</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-dracula">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="Distilling_the_Knowledge_in_a_Neural_Network_0"></a>【论文阅读】《Distilling the Knowledge in a Neural Network》</h2> 
<ul><li>推荐指数：</li></ul> 
<h2><a id="1__2"></a>1. 动机</h2> 
<p>（1）虽然一个ensemble的模型可以提升模型的效果，但是在效率方面实在难以接受，尤其是在每个模型都是一个大型的网络模型的时候。<br> （2）前人的研究结果也已表明：模型参数有很多其实是冗余的。</p> 
<h2><a id="2__5"></a>2. 方法</h2> 
<ul><li>distilling the knowledge in an ensemble of models into a single model.<br> 作者们之所以这么做又是因为之前有篇文章得到的结论，这个结论【<strong>这是一个很重要的结论</strong>】是：</li></ul> 
<blockquote> 
 <p>it is possible to compress the knowledge in an ensemble into single model.</p> 
</blockquote> 
<p>更加具体的就是：</p> 
<blockquote> 
 <p>raise the temperature of the final softmax until the cumbersome model produces a suitably soft set of targets.</p> 
</blockquote> 
<h2><a id="3_12"></a>3.具体实现</h2> 
<p>在谈具体实现之前，先把本文涉及到的一些专有术语解释一下：</p> 
<ul><li><code>distilled model</code> : 小模型（学生模型）</li></ul> 
<blockquote> 
 <p>We have shown that distilling works very well for transferring knowledge from an ensemble or from a large highly regularized model into a smaller, distilled model.</p> 
</blockquote> 
<ul><li><code>cumbersome model</code>: 大模型（教师模型）</li></ul> 
<h3><a id="41__18"></a>4.1 训练教师模型</h3> 
<p>文中没提到如何训练教师模型，但我的理解是普通的那种训练方式即可。</p> 
<h3><a id="42__20"></a>4.2 训练学生模型</h3> 
<p>训练学生模型的过程：<br> <img src="https://images2.imgbox.com/84/32/QWkXNdDL_o.png" alt="在这里插入图片描述"><br> 第一项损失：与软目标的交叉熵损失；<br> 第二项损失：与正确目标的交叉熵损失；【权重较小】</p> 
<h2><a id="5_25"></a>5.效果</h2> 
<p>作者们提出了不同的压缩方法，并且在MNIST数据集上取得了惊人的成绩。同时在一个大量使用的商业系统的声学模型中，也有改善。</p> 
<p>不正确值的相对概率告诉我们许多（繁重的模型是如何倾向泛化的）。文中举例解释道：将BMW误认为垃圾车的概率很小，但是这个概率会比将BMW认为是胡萝卜大很多。</p> 
<p>作者们提出一种叫做“蒸馏”的通用解决方法，这种方法的做法是：提升最终的softmax中的温度系数直到复杂模型能够产生一个合适的软标签；然后在训练学生模型时照样使用高温度系数来匹配这些软标签。</p> 
<h2><a id="5_34"></a>5.数学知识</h2> 
<p>文中提到了一个数学知识，也就是下面这个：<br> <img src="https://images2.imgbox.com/b6/3d/icoGEo4O_o.png" alt="在这里插入图片描述">具体的推导我也不会，后面学习了再更。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/251c0e74b1ddf562f66444df9eab7467/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【Python笔记】多级雷达图绘制</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/19b061dd5a6b3ee17ddd20a8709165dd/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">PoolFormer实战：使用PoolFormer实现图像分类任务（二）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>