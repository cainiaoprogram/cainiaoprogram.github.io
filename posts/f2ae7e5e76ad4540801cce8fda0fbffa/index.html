<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Jetson嵌入式系列模型部署-1 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Jetson嵌入式系列模型部署-1" />
<meta property="og:description" content="目录 前言1. What、Why and How1.1 What1.2 Why1.3 How 2. tensorRT2.1 什么是tensorRT？2.2 tensorRT特性2.3 tensorRT工作流程 3. 驾驭tensorRT的几种方案3.1 repo1 https://github.com/wang-xinyu/tensorrtx3.2 repo2 https://github.com/NVIDIA-AI-IOT/torch2trt3.3 repo3 https://github.com/shouxieai/tensorRT_Pro 4. tensorRT_Pro4.1 Protobuf4.1.1 Protobuf简介4.1.2 基本使用流程 4.2 ONNX4.2.1 概念4.2.2 组成 5. Jetson nano6. 结语7. 下载链接8. 参考 前言 本文旨在为大家提供jetson嵌入式系列模型部署两个简单的技术路线，直白的说就是给大家安利两个仓库分别是tensorrtx和tensorRT_Pro。本文采用常见的yolov5(v6.0版本)目标检测算法实现在jetson nano上的模型部署工作(PS:手头只有nano，太穷了，😂)。假设各位看官的jetson nano环境配置已经完成，能够使用yolov5成功训练自己的数据集。我们重点关注jetson nano上的部署工作。有错误欢迎各位批评指正!!!
本次训练的模型使用yolov5s-6.0，类别数为2，为口罩识别😷。先看效果图，第一张图为tensorrtx在jetson nano上的推理效果图，可参考Jetson嵌入式系列模型部署-2查看详细流程；第二张图为tensorRT_Pro在jetson nano上的推理效果图，可参考Jetson嵌入式系列模型部署-3查看详细流程。
1. What、Why and How 问题: 什么是深度学习模型部署？为什么需要部署？如何去部署呢？
1.1 What 什么是深度学习模型部署？
简单来说就是将你训练好的深度学习模型应用在不同场景下的不同设备上(即特定环境下运行),这些设备可能是服务器、移动端、嵌入式…
1.2 Why 为什么要学习部署呢?
直接将模型放在不同的设备上跑不就完事了吗😕？其实不然，这样做存在两个问题：模型框架兼容性差以及模型运行速度慢。大家知道目前训练模型都基于深度学习框架如pytorch、tensorflow、paddle等，这些框架的兼容性差(直白点说就是环境配置麻烦，一想起深度学习环境配置就头痛🙃)，而且这些框架基于python语言其运行速度无法和C&#43;&#43;这类语言相比。
假设我们需要在jetson nano上去部署属于自己的yolov5模型，难道要求我们在jetson nano上配置pytorch等深度学习环境吗？那未免也太折磨人了，单纯基于pytorch等框架去进行模型的推理存在以下几个问题:
环境配置繁琐，且不说在arm架构的嵌入式上配置深度学习环境了，光在PC端都要折腾一阵携带的框架太过笨重，训练出的模型太冗余需要优化才能满足实际需求语言问题，框架大多基于python语言，运行速度慢移植问题，框架环境依赖性强，耦合性高，无法更加方便的移植参考自模型部署简介 基于以上问题，我们想能不能将框架隔离呢？即仅通过pytorch、tensorflow、paddle等框架训练模型，后续在不同场景下的部署实现仅需要训练好的模型即可，而不需要依赖框架推理。
1.3 How 如何去部署？
即解决方案。怎么利用训练好的模型不依赖框架推理呢？——通过模型推理部署框架，依旧是框架不过这次换成了模型推理框架😎。目前主流的模型推理部署框架有以下几种：
NVIDIA的TensorRT。首当其冲的肯定是tensorRT，NVIDIA通过其自家的GPU，CUDA、CUDNN等软件环境形成了一个强大的生态圈。该推理框架主要是针对NVIDIA的显卡和其推出的jetson系列嵌入式设备。Intel的OpenVINO。openvino是Intel开发的基于inter CPU计算设备的推理引擎。Tencent的NCNN。ncnn是腾讯基于移动端的推理引擎。Microsoft的ONNXRuntime。onnx是microsoft开发的一个中间格式，而ort(onnxruntime)是其为onnx开发的推理引擎Rockchip的RKNN。rknn是瑞芯微为其NPU设计的nn推理引擎。参考自业界主流模型推理部署框架，RKNN使用 具体使用那种推理框架呢？—看需求，部署的方式取决于需求。如果需要在jetson系列嵌入式平台上推理，那么选择tensorRT再合适不过了；如果需要在手机移动端推理，那么可以腾讯的ncnn推理框架；能做到见招拆招即可。参考自训练好的深度学习模型式怎么部署的?" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/f2ae7e5e76ad4540801cce8fda0fbffa/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-03-26T21:51:20+08:00" />
<meta property="article:modified_time" content="2023-03-26T21:51:20+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Jetson嵌入式系列模型部署-1</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>目录</h4> 
 <ul><li><ul><li><ul><li><a href="#_2" rel="nofollow">前言</a></li><li><a href="#1_WhatWhy_and_How_12" rel="nofollow">1. What、Why and How</a></li><li><ul><li><a href="#11_What_16" rel="nofollow">1.1 What</a></li><li><a href="#12_Why_22" rel="nofollow">1.2 Why</a></li><li><a href="#13_How_38" rel="nofollow">1.3 How</a></li></ul> 
    </li><li><a href="#2_tensorRT_53" rel="nofollow">2. tensorRT</a></li><li><ul><li><a href="#21_tensorRT_57" rel="nofollow">2.1 什么是tensorRT？</a></li><li><a href="#22_tensorRT_61" rel="nofollow">2.2 tensorRT特性</a></li><li><a href="#23_tensorRT_85" rel="nofollow">2.3 tensorRT工作流程</a></li></ul> 
    </li><li><a href="#3_tensorRT_105" rel="nofollow">3. 驾驭tensorRT的几种方案</a></li><li><ul><li><a href="#31_repo1_httpsgithubcomwangxinyutensorrtx_109" rel="nofollow">3.1 repo1 https://github.com/wang-xinyu/tensorrtx</a></li><li><a href="#32_repo2_httpsgithubcomNVIDIAAIIOTtorch2trt_129" rel="nofollow">3.2 repo2 https://github.com/NVIDIA-AI-IOT/torch2trt</a></li><li><a href="#33_repo3_httpsgithubcomshouxieaitensorRT_Pro_147" rel="nofollow">3.3 repo3 https://github.com/shouxieai/tensorRT_Pro</a></li></ul> 
    </li><li><a href="#4_tensorRT_Pro_166" rel="nofollow">4. tensorRT_Pro</a></li><li><ul><li><a href="#41_Protobuf_170" rel="nofollow">4.1 Protobuf</a></li><li><ul><li><a href="#411_Protobuf_174" rel="nofollow">4.1.1 Protobuf简介</a></li><li><a href="#412__196" rel="nofollow">4.1.2 基本使用流程</a></li></ul> 
     </li><li><a href="#42_ONNX_311" rel="nofollow">4.2 ONNX</a></li><li><ul><li><a href="#421__313" rel="nofollow">4.2.1 概念</a></li><li><a href="#422__324" rel="nofollow">4.2.2 组成</a></li></ul> 
    </li></ul> 
    </li><li><a href="#5_Jetson_nano_338" rel="nofollow">5. Jetson nano</a></li><li><a href="#6__344" rel="nofollow">6. 结语</a></li><li><a href="#7__348" rel="nofollow">7. 下载链接</a></li><li><a href="#8__357" rel="nofollow">8. 参考</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h4><a id="_2"></a>前言</h4> 
<blockquote> 
 <p>本文旨在为大家提供jetson嵌入式系列模型部署两个简单的技术路线，直白的说就是给大家安利两个仓库分别是<a href="https://github.com/wang-xinyu/tensorrtx">tensorrtx</a>和<a href="https://github.com/shouxieai/tensorRT_Pro">tensorRT_Pro</a>。本文采用常见的yolov5(v6.0版本)目标检测算法实现在jetson nano上的模型部署工作(PS:手头只有nano，太穷了，😂)。假设各位看官的jetson nano环境配置已经完成，能够使用yolov5成功训练自己的数据集。我们重点关注jetson nano上的部署工作。有错误欢迎各位批评指正!!!</p> 
</blockquote> 
<p>本次训练的模型使用yolov5s-6.0，类别数为2，为口罩识别😷。先看效果图，第一张图为<a href="https://github.com/wang-xinyu/tensorrtx">tensorrtx</a>在jetson nano上的推理效果图，可参考<a href="https://blog.csdn.net/qq_40672115/article/details/126368779?spm=1001.2014.3001.5502">Jetson嵌入式系列模型部署-2</a>查看详细流程；第二张图为<a href="https://github.com/shouxieai/tensorRT_Pro">tensorRT_Pro</a>在jetson nano上的推理效果图，可参考<a href="https://blog.csdn.net/qq_40672115/article/details/126369675?spm=1001.2014.3001.5502">Jetson嵌入式系列模型部署-3</a>查看详细流程。</p> 
<p><img src="https://images2.imgbox.com/12/4d/FN96c6CK_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/53/92/fNC5bKV5_o.jpg" alt="在这里插入图片描述"></p> 
<h4><a id="1_WhatWhy_and_How_12"></a>1. What、Why and How</h4> 
<blockquote> 
 <p><strong>问题</strong>: 什么是深度学习模型部署？为什么需要部署？如何去部署呢？</p> 
</blockquote> 
<h5><a id="11_What_16"></a>1.1 What</h5> 
<blockquote> 
 <p><strong>什么是深度学习模型部署？</strong></p> 
 <p>简单来说就是将你训练好的深度学习模型应用在不同场景下的不同设备上(即特定环境下运行),这些设备可能是服务器、移动端、嵌入式…</p> 
</blockquote> 
<h5><a id="12_Why_22"></a>1.2 Why</h5> 
<blockquote> 
 <p><strong>为什么要学习部署呢?</strong></p> 
 <p>直接将模型放在不同的设备上跑不就完事了吗😕？其实不然，这样做存在两个问题：<strong>模型框架兼容性差以及模型运行速度慢</strong>。大家知道目前训练模型都基于深度学习框架如<code>pytorch、tensorflow、paddle</code>等，这些框架的兼容性差(直白点说就是环境配置麻烦，一想起深度学习环境配置就头痛🙃)，而且这些框架基于python语言其运行速度无法和C++这类语言相比。</p> 
 <p>假设我们需要在jetson nano上去部署属于自己的yolov5模型，难道要求我们在jetson nano上配置pytorch等深度学习环境吗？那未免也太折磨人了，单纯基于<code>pytorch</code>等框架去进行模型的推理存在以下几个问题:</p> 
 <ul><li><strong>环境配置繁琐，且不说在arm架构的嵌入式上配置深度学习环境了，光在PC端都要折腾一阵</strong></li><li><strong>携带的框架太过笨重，训练出的模型太冗余需要优化才能满足实际需求</strong></li><li><strong>语言问题，框架大多基于python语言，运行速度慢</strong></li><li><strong>移植问题，框架环境依赖性强，耦合性高，无法更加方便的移植</strong></li><li><strong>参考自<a href="https://zhuanlan.zhihu.com/p/477743341" rel="nofollow">模型部署简介</a></strong></li></ul> 
 <p>基于以上问题，我们想能不能将框架隔离呢？即仅通过<code>pytorch、tensorflow、paddle</code>等框架训练模型，后续在不同场景下的部署实现仅需要训练好的模型即可，而不需要依赖框架推理。</p> 
</blockquote> 
<h5><a id="13_How_38"></a>1.3 How</h5> 
<blockquote> 
 <p><strong>如何去部署？</strong></p> 
 <p>即解决方案。怎么利用训练好的模型不依赖框架推理呢？——通过模型推理部署框架，依旧是框架不过这次换成了模型推理框架😎。目前主流的模型推理部署框架有以下几种：</p> 
 <ul><li><strong>NVIDIA的TensorRT</strong>。首当其冲的肯定是tensorRT，NVIDIA通过其自家的GPU，CUDA、CUDNN等软件环境形成了一个强大的生态圈。该推理框架主要是针对NVIDIA的显卡和其推出的jetson系列嵌入式设备。</li><li><strong>Intel的OpenVINO</strong>。openvino是Intel开发的基于inter CPU计算设备的推理引擎。</li><li><strong>Tencent的NCNN</strong>。ncnn是腾讯基于移动端的推理引擎。</li><li><strong>Microsoft的ONNXRuntime</strong>。onnx是microsoft开发的一个中间格式，而ort(onnxruntime)是其为onnx开发的推理引擎</li><li><strong>Rockchip的RKNN</strong>。rknn是瑞芯微为其NPU设计的nn推理引擎。</li><li>参考自<a href="https://bbs.huaweicloud.com/blogs/332726" rel="nofollow">业界主流模型推理部署框架</a>，<a href="https://wiki.t-firefly.com/zh_CN/CORE-1126-JD4/rknn.html" rel="nofollow">RKNN使用</a></li></ul> 
 <p>具体使用那种推理框架呢？—<strong>看需求，部署的方式取决于需求</strong>。如果需要在jetson系列嵌入式平台上推理，那么选择tensorRT再合适不过了；如果需要在手机移动端推理，那么可以腾讯的ncnn推理框架；能做到见招拆招即可。参考自<a href="https://www.zhihu.com/question/329372124/answer/743251971" rel="nofollow">训练好的深度学习模型式怎么部署的?</a></p> 
</blockquote> 
<h4><a id="2_tensorRT_53"></a>2. tensorRT</h4> 
<blockquote> 
 <p>分享jetson系列嵌入式设备的模型部署，那肯定需要聊聊tensorRT。</p> 
</blockquote> 
<h5><a id="21_tensorRT_57"></a>2.1 什么是tensorRT？</h5> 
<blockquote> 
 <p><code>tensorRT</code>是一个SDK(Software Development Kit)即软件开发工具包，用于优化经过训练的深度学习模型以实现高性能推理</p> 
</blockquote> 
<h5><a id="22_tensorRT_61"></a>2.2 tensorRT特性</h5> 
<blockquote> 
 <p>TensorRT为什么能加速推理过程，它是如何优化的？主要体现在以下几个方面：</p> 
</blockquote> 
<ul><li>算子融合 <code>Conv+Bias+ReLU -&gt; CBR</code></li><li>量化 
  <ul><li>INT8或FP16以及TF32</li><li>存储优势、计算优势、通信优势</li></ul> </li><li>内核自动调整 
  <ul><li>根据不同显卡架构、SM数量、内核频率等选择不同的优化策略以及计算方式，寻找最适合当前架构的计算方式</li><li>Kernel可以根据不同大小的batch和问题的复杂度去选择最合适的算法，TensorRT预先写了很多GPU实现，有一个自动选择的过程</li></ul> </li><li>动态张量线程WorkSpace</li><li>多流执行</li><li>参考自<a href="https://zhuanlan.zhihu.com/p/432215219" rel="nofollow">tensorRT如何进行推理加速?</a></li></ul> 
<blockquote> 
 <p>通过tensorRT能够在Nvidia系列GPU上发挥出最好的性能。值得注意的是，tensorRT的模型，需要在目标GPU上以<strong>实际运行</strong>的方式选择最优算法和配置，也因此tensorRT生成的模型是与其<strong>设备强绑定</strong>的，与其编译时的trt版本、cuda版本、GPU型号相关联。同时tensorRT支持<strong>FP32、FP16、INT8</strong>等多种精度，如何查看自身GPU是否支持FP16/INT8精度呢？主要分以下两步</p> 
 <ul><li><strong>1. 访问<a href="https://developer.nvidia.com/zh-cn/cuda-gpus#compute" rel="nofollow">https://developer.nvidia.com/zh-cn/cuda-gpus#compute</a>查看显卡对应的算力</strong></li><li><strong>2. 访问<a href="https://docs.nvidia.com/deeplearning/tensorrt/support-matrix/index.html#hardware-precision-matrix" rel="nofollow">https://docs.nvidia.com/deeplearning/tensorrt/support-matrix/index.html#hardware-precision-matrix</a>查看对应算力支持的精度</strong></li></ul> 
 <p>比如说jetson nano算力是5.3，只支持FP32不支持FP16、INT8。</p> 
 <p>如果想了解关于tensorRT更多细节请查看<a href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html" rel="nofollow">tensorRT官方文档</a></p> 
</blockquote> 
<h5><a id="23_tensorRT_85"></a>2.3 tensorRT工作流程</h5> 
<blockquote> 
 <p>tensorRT是如何构建模型呢？主要通过两种方式</p> 
</blockquote> 
<ul><li> <p><strong>1. 通过TRT API一层层搭建模型</strong></p> 
  <ul><li>tensorRT提供基于C++接口构建模型的方式，见下图，参考自<a href="https://github.com/NVIDIA/TensorRT/blob/main/samples/sampleMNISTAPI/sampleMNISTAPI.cpp">TensorRT/samples/sampleMNISTAPI/sampleMNISTAPI.cpp</a></li><li>tensorRT也提供基于Python接口构建模型的方式，见下图，参考自<a href="https://github.com/NVIDIA/TensorRT/blob/main/samples/python/engine_refit_mnist/sample.py">TensorRT/samples/python/engine_refit_mnist/sample.py</a></li></ul> </li></ul> 
<p><img src="https://images2.imgbox.com/b0/4e/pYz5oQA0_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/cd/84/XymNIdWi_o.png" alt="在这里插入图片描述"></p> 
<ul><li> <p><strong>2. NVIDIA官方也提供另外三种途径实现更加方便的封装</strong>，如下图所示</p> 
  <ul><li><code>UFF格式的文件</code>，通过<code>libnvparsers.so</code>可以调用TRT API去解析UFF文件从而构建模型(<strong>tensorflow采用的方案</strong>)</li><li><code>ONNX格式的文件</code>，通过<code>libnvonnxparser.so</code>可以调用TRT API去解析ONNX文件从而构建模型(<strong>pytorch采用的方案</strong>)</li><li><code>Caffe格式的文件</code>，通过<code>libnvcaffe_parser.so</code>可以调用TRT API去解析Caffe文件从而构建模型(使用较少)</li></ul> </li></ul> 
<p><img src="https://images2.imgbox.com/27/0e/Jf0LT963_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="3_tensorRT_105"></a>3. 驾驭tensorRT的几种方案</h4> 
<blockquote> 
 <p><strong>Copy自<a href="https://www.bilibili.com/video/BV1Xw411f7FW?p=2&amp;spm_id_from=pageDriver&amp;vd_source=2306b3f342c070c8a11931c94eafa8e3" rel="nofollow">详解TensorRT的C++/Python高性能部署</a></strong>，建议看<a href="https://www.bilibili.com/video/BV1Xw411f7FW?p=2&amp;spm_id_from=pageDriver&amp;vd_source=2306b3f342c070c8a11931c94eafa8e3" rel="nofollow">原视频</a>的详细讲解</p> 
</blockquote> 
<h5><a id="31_repo1_httpsgithubcomwangxinyutensorrtx_109"></a>3.1 repo1 https://github.com/wang-xinyu/tensorrtx</h5> 
<p><a href="https://github.com/wang-xinyu/tensorrtx">repo1</a>为每个模型写硬代码，流程如下所示</p> 
<p><img src="https://images2.imgbox.com/e9/1b/A1FyuVMe_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/74/a4/2RNfPMfy_o.png" alt="在这里插入图片描述"></p> 
<ul><li>使用作者自定义的<code>gen_wts.py</code>存储权重</li><li>使用C++硬代码调用TRT API构建模型结构，加载<code>gen_wts.py</code>产生的权重文件</li><li><strong>优点</strong> 
  <ul><li>可以控制每个layer的细节和权重，直接面对TRT API</li><li>在认为ONNX方案适配性差的前提下，这种方案不存在算子问题，如果存在不支持的算子，可以自行增加插件。灵活性最高</li><li>这种方案与官方的samples相似度高，有参照</li><li>作者提供了大量场景模型的硬代码，方便直接使用，受到Yolov5官方引用</li></ul> </li><li><strong>缺点</strong> 
  <ul><li>过于灵活，需要控制的细节太多，对技能要求较高</li><li>模型构建的方式采用的硬代码，灵活度差。新模型需要自己一个layer一个layer的写C++代码构建，不具有通用性</li><li>作者提供的推理代码是demo级，到使用阶段时，需要修改太多。可以看作官方的扩展</li><li>部署时无法查看网络结构进行分析和排查</li></ul> </li></ul> 
<h5><a id="32_repo2_httpsgithubcomNVIDIAAIIOTtorch2trt_129"></a>3.2 repo2 https://github.com/NVIDIA-AI-IOT/torch2trt</h5> 
<p><a href="https://github.com/NVIDIA-AI-IOT/torch2trt">repo2</a>为每个算子写Converter，反射Moule.forward捕获输入输出和图结构，流程如下所示</p> 
<p><img src="https://images2.imgbox.com/a3/86/5TNmxmm9_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/3b/b1/hFWcxDAb_o.png" alt="在这里插入图片描述"></p> 
<ul><li>作者为pytorch的每一个操作做了<code>Converter</code>，为每个操作的<code>forward</code>反射到自定义函数下</li><li>通过反射<code>torch的forward</code>操作捕获模块的权重，调用<code>Python API</code>接口实现模型构建</li><li><strong>优点</strong> 
  <ul><li>直接集成Python、Pytorch，可以实现pytorch模型到tensorRT模型的无缝无脑简单转换</li></ul> </li><li><strong>缺点</strong> 
  <ul><li>提供的是Python的方案，并没有C++的方案</li><li>新的算子需要自己实现Converter，需要维护新的算子库</li><li>直接用Pytorch转到tensorRT存储的模型是tensorRT模型，如果跨设备则必须在设备上安装pytorch，灵活度不利于部署</li><li>部署时无法查看网络结构进行分析和排查</li></ul> </li></ul> 
<h5><a id="33_repo3_httpsgithubcomshouxieaitensorRT_Pro_147"></a>3.3 repo3 https://github.com/shouxieai/tensorRT_Pro</h5> 
<p><a href="https://github.com/shouxieai/tensorRT_Pro">repo3</a>基于ONNX路线，提供C++、Python接口，深度定制ONNXParser，低耦合封装，实现常用模型Yolov7、YoloX、Yolov5、Yolov3、Unet、RetinaFace、Arcface、SCRFD、DeepSORT等等。算子由官方维护，模型直接导出，流程如下所示</p> 
<p><img src="https://images2.imgbox.com/29/a3/FkR0FhGP_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/45/5f/CDpIgnbS_o.png" alt="在这里插入图片描述"></p> 
<ul><li>对，就是<strong>he</strong>，知道别个的优缺点，就知道该怎么设计了</li><li><strong>优点</strong> 
  <ul><li>集成工业级推理方案，支持tensorRT从模型导出到应用到项目中的全部工作</li><li>案例有Yolov5、YoloX、AlphaPose、RetinaFace、SCRFD、Arcface、DeepSORT，每个应用均为高性能工业级拿来即可用，低耦合</li><li>具有简单的模型导出方法和onnx问题的解决方案</li><li>具有简单的模型推理接口，封装tensorRT细节。支持插件</li><li>支持python接口导出模型和推理接口</li><li>依赖onnx，pytorch方面有官方支持，tensorRT方面也有官方支持。咱们做的是桥梁。虽然onnx存在各种兼容性问题，搞清楚了，还是可以轻松驾驭它</li></ul> </li><li><strong>缺点</strong> 
  <ul><li>各位看官自己写吧(😄)</li></ul> </li></ul> 
<h4><a id="4_tensorRT_Pro_166"></a>4. tensorRT_Pro</h4> 
<blockquote> 
 <p>单独将这个<a href="https://github.com/shouxieai/tensorRT_Pro">repo</a>拿出来讲是因为里面有些内容值得深挖，该<a href="https://github.com/shouxieai/tensorRT_Pro">repo</a>基于ONNX路线完成tensorRT模型构建，需要编译protobuf，下面简单聊聊<strong>为什么需要编译protobuf、什么是protobuf以及onnx是什么等相关内容</strong></p> 
</blockquote> 
<h5><a id="41_Protobuf_170"></a>4.1 Protobuf</h5> 
<blockquote> 
 <p>关于protobuf的相关介绍<strong>Copy自<a href="https://www.bilibili.com/video/BV16U4y1U75F?p=22&amp;vd_source=2306b3f342c070c8a11931c94eafa8e3" rel="nofollow">赵老师的百度Apollo智能驾驶课程</a></strong>，建议看<a href="https://www.bilibili.com/video/BV16U4y1U75F?p=22&amp;vd_source=2306b3f342c070c8a11931c94eafa8e3" rel="nofollow">原视频</a>，关于protobuf的编译请参考<a href="https://blog.csdn.net/qq_40672115/article/details/126369675?spm=1001.2014.3001.5502">here</a></p> 
</blockquote> 
<h6><a id="411_Protobuf_174"></a>4.1.1 Protobuf简介</h6> 
<p><strong>概念</strong></p> 
<p><code>Protobuf</code>全称<code>Protocol buffers</code>，是<code>Google</code>研发的一种跨语言、跨平台的序列化结构的<strong>数据格式</strong>，是一个灵活的、高效的用于序列化数据的<strong>协议</strong></p> 
<p><strong>特点</strong></p> 
<p>在序列化数据时常用的数据格式还有<code>XML</code>、<code>JSON</code>等，相比较而言，<code>Protobuf</code>更小、效率更高且使用更为便捷，<code>Protobuf</code><strong>内置编译器</strong><code>protoc</code>，可以将<code>protobuf文件</code>编译成<code>C++</code>、<code>Python</code>、<code>Java</code>、<code>C#</code>、<code>Go</code>等多种语言对应的代码，然后可以直接被对应语言使用，轻松实现对数据流的读或写操作而不需要再做特殊解析。</p> 
<p><code>Protobuf</code>的优点如下：</p> 
<ul><li><strong>高效</strong>——序列化后字节占用空间少，序列化的时间效率高</li><li><strong>便捷</strong>——可以将结构化数据封装为类，使用方便</li><li><strong>跨语言</strong>——支持多种编程语言</li><li><strong>高兼容性</strong>——当数据交互的双方使用同一数据协议，如果一方修改了数据结构，不影响另一方的使用</li></ul> 
<p><code>Protobuf</code>也有缺点：</p> 
<ul><li>二进制格式易读性差</li><li>缺乏自描述</li></ul> 
<h6><a id="412__196"></a>4.1.2 基本使用流程</h6> 
<p>现有需求如下</p> 
<blockquote> 
 <p>创建一个protobuf文件，在该文件中声明学生的姓名、身高、年龄…等信息，然后分别使用C++和Python实现学生数据的读写操作。</p> 
</blockquote> 
<p>实现流程如下</p> 
<ul><li> <p><strong>1.编写proto文件</strong></p> </li><li> <p><strong>2.编译生成对应的C++或Python文件</strong></p> </li><li> <p><strong>3.在C++或Python中调用</strong></p> </li></ul> 
<p><strong>1.编写proto文件</strong>，如下所示</p> 
<pre><code class="prism language-cpp"><span class="token comment">// student.proto</span>

<span class="token comment">// 使用的 proto 版本</span>
syntax <span class="token operator">=</span> <span class="token string">"proto2"</span>

<span class="token comment">// 包</span>
package person<span class="token punctuation">;</span>

<span class="token comment">//消息 ---message 是关键字，Student 消息名称</span>
message Student<span class="token punctuation">{<!-- --></span>
    <span class="token comment">//字段</span>
    <span class="token comment">//字段格式：字段规则 数据类型 字段名称 字段编号</span>
    required string name   <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span>
    optional unit64 age    <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">;</span>
    optional <span class="token keyword">double</span> height <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">;</span>
    repeated string books  <span class="token operator">=</span> <span class="token number">4</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<p><strong>2.编译</strong>，指令如下</p> 
<pre><code class="prism language-shell">$ protoc student.proto --cpp_out<span class="token operator">=</span>./
</code></pre> 
<p>执行完成后，在当前目录下<code>student.proto</code>文件会生成<code>student.pb.h</code>和<code>student.pb.cc</code>，将<code>.cc</code>后缀修改为<code>.cpp</code>可供C++调用</p> 
<p><strong>3.C++调用</strong>，调用demo如下</p> 
<pre><code class="prism language-cpp"><span class="token comment">// test.cpp</span>

<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;student.pb.h&gt;</span></span>

<span class="token keyword">using</span> <span class="token keyword">namespace</span> std<span class="token punctuation">;</span>
<span class="token keyword">using</span> <span class="token keyword">namespace</span> person<span class="token punctuation">;</span>

<span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token keyword">int</span> argc<span class="token punctuation">,</span> <span class="token keyword">char</span> <span class="token keyword">const</span> <span class="token operator">*</span>argv<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token punctuation">{<!-- --></span>
    <span class="token comment">// 1. create object</span>
    person<span class="token double-colon punctuation">::</span>Student stu<span class="token punctuation">;</span>
    
    <span class="token comment">// 2. wirte data</span>
    stu<span class="token punctuation">.</span><span class="token function">set_name</span><span class="token punctuation">(</span><span class="token string">"zhangsan"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    stu<span class="token punctuation">.</span><span class="token function">set_age</span><span class="token punctuation">(</span><span class="token number">18</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    stu<span class="token punctuation">.</span><span class="token function">set_height</span><span class="token punctuation">(</span><span class="token number">1.75</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    stu<span class="token punctuation">.</span><span class="token function">add_books</span><span class="token punctuation">(</span><span class="token string">"c++"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    stu<span class="token punctuation">.</span><span class="token function">add_books</span><span class="token punctuation">(</span><span class="token string">"python"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token comment">// 3. read data</span>
    std<span class="token double-colon punctuation">::</span>string name <span class="token operator">=</span> stu<span class="token punctuation">.</span><span class="token function">name</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">uint64_t</span> age <span class="token operator">=</span> stu<span class="token punctuation">.</span><span class="token function">age</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">double</span> height <span class="token operator">=</span> stu<span class="token punctuation">.</span><span class="token function">height</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    std<span class="token double-colon punctuation">::</span>cout <span class="token operator">&lt;&lt;</span> name <span class="token operator">&lt;&lt;</span> <span class="token string">" == "</span> <span class="token operator">&lt;&lt;</span> age <span class="token operator">&lt;&lt;</span> <span class="token string">" == "</span> <span class="token operator">&lt;&lt;</span> height <span class="token operator">&lt;&lt;</span> std<span class="token double-colon punctuation">::</span>endl<span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> stu<span class="token punctuation">.</span><span class="token function">books_size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span>
    <span class="token punctuation">{<!-- --></span>
        std<span class="token double-colon punctuation">::</span>cout <span class="token operator">&lt;&lt;</span> stu<span class="token punctuation">.</span><span class="token function">books</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token operator">&lt;&lt;</span> <span class="token string">"-"</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    std<span class="token double-colon punctuation">::</span>cout <span class="token operator">&lt;&lt;</span> std<span class="token double-colon punctuation">::</span>endl<span class="token punctuation">;</span>

    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>CMakeLists.txt如下</p> 
<pre><code class="prism language-cpp"><span class="token function">cmake_minimum_required</span><span class="token punctuation">(</span>VERSION <span class="token number">3.0</span><span class="token punctuation">)</span>
<span class="token function">project</span><span class="token punctuation">(</span>test<span class="token punctuation">)</span>

<span class="token function">set</span><span class="token punctuation">(</span>CMAKE_CXX_FLAGS <span class="token string">"${CMAKE_CXX_FLAGS} -Wall -pthread -std=c++11"</span><span class="token punctuation">)</span>
<span class="token function">set</span><span class="token punctuation">(</span>CMAKE_BUILD_TYPE Debug<span class="token punctuation">)</span>
<span class="token function">set</span><span class="token punctuation">(</span>EXECUTABLE_OUTPUT_PATH $<span class="token punctuation">{<!-- --></span>PROJECT_SOURCE_DIR<span class="token punctuation">}</span><span class="token operator">/</span>workspace<span class="token punctuation">)</span>

<span class="token function">set</span><span class="token punctuation">(</span>PROTOBUF_DIR <span class="token string">"/home/zhlab/protobuf"</span><span class="token punctuation">)</span>

<span class="token function">include_directories</span><span class="token punctuation">(</span>
    $<span class="token punctuation">{<!-- --></span>PROTOBUF_DIR<span class="token punctuation">}</span><span class="token operator">/</span>include
    $<span class="token punctuation">{<!-- --></span>PROJECT_SOURCE_DIR<span class="token punctuation">}</span><span class="token operator">/</span>src
<span class="token punctuation">)</span>

<span class="token function">link_directories</span><span class="token punctuation">(</span>
    $<span class="token punctuation">{<!-- --></span>PROTOBUF_DIR<span class="token punctuation">}</span><span class="token operator">/</span>lib
<span class="token punctuation">)</span>

<span class="token function">add_executable</span><span class="token punctuation">(</span>main $<span class="token punctuation">{<!-- --></span>PROJECT_SOURCE_DIR<span class="token punctuation">}</span><span class="token operator">/</span>src<span class="token operator">/</span>test<span class="token punctuation">.</span>cpp $<span class="token punctuation">{<!-- --></span>PROJECT_SOURCE_DIR<span class="token punctuation">}</span><span class="token operator">/</span>src<span class="token operator">/</span>student<span class="token punctuation">.</span>pb<span class="token punctuation">.</span>cpp<span class="token punctuation">)</span>

<span class="token macro property"><span class="token directive-hash">#</span> <span class="token directive keyword">add</span> <span class="token expression">protobuf</span></span>
<span class="token function">target_link_libraries</span><span class="token punctuation">(</span>main protobuf<span class="token punctuation">)</span>
<span class="token function">target_link_libraries</span><span class="token punctuation">(</span>main pthread<span class="token punctuation">)</span>
</code></pre> 
<p>编译<code>test.cpp</code>文件，在<code>workspace/</code>文件夹下运行可执行文件，图解如下所示</p> 
<p><img src="https://images2.imgbox.com/26/81/8z6YcSVE_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/b5/c9/U9NpZD8S_o.png" alt="在这里插入图片描述"></p> 
<blockquote> 
 <p>给出protobuf的demo演示源码下载链接<a href="https://pan.baidu.com/s/1qS0iRhtqWJMhjZteR6B1Mw#list/path=%2F" rel="nofollow">Baidu Drive[password:yolo]</a></p> 
</blockquote> 
<h5><a id="42_ONNX_311"></a>4.2 ONNX</h5> 
<h6><a id="421__313"></a>4.2.1 概念</h6> 
<ul><li> <p>onnx可以理解为<strong>一种通用货币</strong>，开发者可以把自己开发训练好的模型保存为onnx文件，而部署工程师可以借助<strong>部署框架</strong>(如tensorRT、openvino、ncnn等)部署在不同的硬件平台上，而不必关系开发者使用的是哪一种框架</p> </li><li> <p>onnx的本质是一种<strong>protobuf格式文件</strong></p> </li><li> <p>protobuf通过编译<code>onnx-ml.proto</code>文件得到<code>onnx-ml.pb.h和onnx-ml.pb.cc</code>用于C++调用或<code>onnx_ml_pb2.py</code>用于python调用，如下图所示。如果本地python环境下安装了onnx第三方库，则在该库下可以找到<code>onnx_ml_pb2.py</code>文件<br> <img src="https://images2.imgbox.com/48/c5/ixDyaKXx_o.png" alt="在这里插入图片描述"></p> </li><li> <p>通过编译得到的<code>onnx-ml.pb.cc</code>和代码就可以<strong>操作onnx模型文件，实现对应的增删改</strong></p> </li><li> <p><code>onnx-ml.proto</code>用于<strong>描述onnx文件时如何组成的，具有什么结构</strong>，它是onnx经常参照的东西，如下是onnx-ml.proto部分内容，参考自<a href="https://github.com/shouxieai/tensorRT_Pro/blob/main/onnx/onnx-ml.proto">https://github.com/shouxieai/tensorRT_Pro/blob/main/onnx/onnx-ml.proto</a></p> </li></ul> 
<p><img src="https://images2.imgbox.com/60/ae/xCcqy2f8_o.png" alt="在这里插入图片描述"></p> 
<h6><a id="422__324"></a>4.2.2 组成</h6> 
<p>onnx文件组成如下图所示</p> 
<p><img src="https://images2.imgbox.com/3c/c8/k39z7OIg_o.png" alt="在这里插入图片描述"></p> 
<ul><li><strong>model</strong>：表示整个onnx模型，包括图结构和解析器版本、opset版本、导出程序类型 
  <ul><li>opset版本即operator版本号即pytorch得op(操作算子)版本</li></ul> </li><li><strong>model.graph</strong>：表示图结构，通常是<a href="https://netron.app/" rel="nofollow">Netron可视化工具</a>中看到的结构</li><li><strong>model.graph.node</strong>：表示图结构中所有节点如conv、bn、relu等</li><li><strong>model.graph.initializer</strong>：权重数据大都存储在这里</li><li><strong>model.graph.input</strong>：模型的输入</li><li><strong>model.graph.output</strong>：模型的输出</li></ul> 
<h4><a id="5_Jetson_nano_338"></a>5. Jetson nano</h4> 
<blockquote> 
 <p>关于jetson nano刷机就不再赘述了，需要各位看官自行配置好相关环境😄，外网访问较慢，这里提供Jetson nano的几个JetPack镜像下载链接<a href="https://pan.baidu.com/s/1RW8Z-EQK--NtPgIe6mmKIQ?_at_=1660636990470#list/path=%2F" rel="nofollow">Baidu Drive[password:nano]【更新完毕!!!】</a>(<strong>PS:提供4.6和4.6.1两个版本，注意4GB和2GB的区别，不要刷错了</strong>)，关于Jetson Nano 2GB和4GB的区别可参考链接<a href="https://zhuanlan.zhihu.com/p/339225116" rel="nofollow">Jetson NANO是什么？如何选？</a>。(吐槽下这玩意上传忒慢了，超级会员不顶用呀，终于上传完了，折磨!!!)，博主使用的jetpack版本为<strong>JetPack4.6.1</strong>，其详细信息如下所示</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/96/e6/nlycKrov_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="6__344"></a>6. 结语</h4> 
<blockquote> 
 <p>本篇博客简单介绍了模型部署的相关工作以及驾驭tensorRT的几种方案。后续通过<a href="https://github.com/wang-xinyu/tensorrtx">tesorrtx</a>和<a href="https://github.com/shouxieai/tensorRT_Pro">tensorRT_Pro</a>两个repo带大家实现在jetson nano上的yolov5模型部署。可参考<a href="https://blog.csdn.net/qq_40672115/article/details/126368779?spm=1001.2014.3001.5502">Jetson嵌入式系列模型部署-2</a>和<a href="https://blog.csdn.net/qq_40672115/article/details/126369675?spm=1001.2014.3001.5502">Jetson嵌入式系列模型部署-3</a></p> 
</blockquote> 
<h4><a id="7__348"></a>7. 下载链接</h4> 
<ul><li><a href="https://pan.baidu.com/s/1qS0iRhtqWJMhjZteR6B1Mw#list/path=%2F" rel="nofollow">protobuf的demo演示源码下载链接Biadu Drive[password:yolo]</a> 
  <ul><li>protoc_test文件夹下共包含<code>build src workspace CMakeLists.txt</code>四个文件</li><li><code>build</code>为博主在jetson nano上编译生成的中间文件(PS:可删除自行编译)</li><li><code>src</code>文件夹下包含<code>student.proto</code>及其生成的<code>student.pb.h</code>和<code>student.pb.cc</code>，<code>test.cpp</code>用于C++调用</li><li><code>workspace</code>下存放着可执行文件</li></ul> </li><li><a href="https://pan.baidu.com/s/1RW8Z-EQK--NtPgIe6mmKIQ?_at_=1660636990470#list/path=%2F" rel="nofollow">JetPack镜像下载链接Baidu Drive[password:nano]【更新完毕!!!】</a>，<strong>上传忒慢</strong></li></ul> 
<h4><a id="8__357"></a>8. 参考</h4> 
<ul><li><a href="https://github.com/wang-xinyu/tensorrtx">tensorrtx</a></li><li><a href="https://github.com/shouxieai/tensorRT_Pro">tensorRT_Pro</a></li><li><a href="https://github.com/NVIDIA-AI-IOT/torch2trt">torch2trt</a></li><li><a href="https://blog.csdn.net/qq_40672115/article/details/126368779?spm=1001.2014.3001.5502">Jetson嵌入式系列模型部署-2</a></li><li><a href="https://blog.csdn.net/qq_40672115/article/details/126369675?spm=1001.2014.3001.5502">Jetson嵌入式系列模型部署-3</a></li><li><a href="https://zhuanlan.zhihu.com/p/477743341" rel="nofollow">模型部署简介</a></li><li><a href="https://bbs.huaweicloud.com/blogs/332726" rel="nofollow">业界主流模型推理部署框架</a></li><li><a href="https://wiki.t-firefly.com/zh_CN/CORE-1126-JD4/rknn.html" rel="nofollow">RKNN使用</a></li><li><a href="https://www.zhihu.com/question/329372124/answer/743251971" rel="nofollow">训练好的深度学习模型式怎么部署的?</a></li><li><a href="https://zhuanlan.zhihu.com/p/432215219" rel="nofollow">tensorRT如何进行推理加速?</a></li><li><a href="https://developer.nvidia.com/zh-cn/cuda-gpus#compute" rel="nofollow">显卡算力查询</a></li><li><a href="https://docs.nvidia.com/deeplearning/tensorrt/support-matrix/index.html#hardware-precision-matrix" rel="nofollow">算力支持的精度查询</a></li><li><a href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html" rel="nofollow">tensorRT官方文档</a></li><li><a href="https://www.bilibili.com/video/BV1Xw411f7FW?p=2&amp;spm_id_from=pageDriver&amp;vd_source=2306b3f342c070c8a11931c94eafa8e3" rel="nofollow">详解TensorRT的C++/Python高性能部署</a> 博主强烈推荐!!!👍👍👍</li><li><a href="https://www.bilibili.com/video/BV16U4y1U75F?p=22&amp;vd_source=2306b3f342c070c8a11931c94eafa8e3" rel="nofollow">赵老师的百度Apollo智能驾驶课程</a></li><li><a href="https://blog.csdn.net/qq_40672115/article/details/126369675?spm=1001.2014.3001.5502">protobuf编译</a></li><li><a href="https://netron.app/" rel="nofollow">Netron可视化工具</a></li><li><a href="https://zhuanlan.zhihu.com/p/339225116" rel="nofollow">Jetson NANO是什么？如何选？</a></li></ul> 
<blockquote> 
 <p>感谢各位看到最后，创作不易，读后有收获的看官请帮忙点个👍⭐️</p> 
</blockquote>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/64dd3e44b7dc18c9727c9288a322fb2d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Jetson nano部署YOLOv7</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/739a9bf0e10c45d204b817e1bb3fab05/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">搭建网页vscode服务器Code-Server</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>