<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>transformer 学习 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="transformer 学习" />
<meta property="og:description" content="原理学习：
(3条消息) The Illustrated Transformer【译】_于建民的博客-CSDN博客
代码学习：
https://github.com/jadore801120/attention-is-all-you-need-pytorch/tree/master/transformer
mask学习：
(3条消息) NLP 中的Mask全解_mask在自然语言处理代表什么_郝伟博士的博客-CSDN博客
多头注意力机制学习：
【1】代码：
class MultiHeadAttention(nn.Module): &#39;&#39;&#39; Multi-Head Attention module &#39;&#39;&#39; def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1): super().__init__() self.n_head = n_head self.d_k = d_k self.d_v = d_v self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False) self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False) self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False) self.fc = nn.Linear(n_head * d_v, d_model, bias=False) self.attention = ScaledDotProductAttention(temperature=d_k ** 0." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/c607158fd56122090280f57654ed0a92/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-13T16:17:21+08:00" />
<meta property="article:modified_time" content="2023-07-13T16:17:21+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">transformer 学习</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><img alt="" height="401" src="https://images2.imgbox.com/f1/bb/1OWWlM4T_o.png" width="322"></p> 
<p> 原理学习：</p> 
<p><a href="https://blog.csdn.net/yujianmin1990/article/details/85221271" title="(3条消息) The Illustrated Transformer【译】_于建民的博客-CSDN博客">(3条消息) The Illustrated Transformer【译】_于建民的博客-CSDN博客</a></p> 
<p>代码学习：</p> 
<p>https://github.com/jadore801120/attention-is-all-you-need-pytorch/tree/master/transformer</p> 
<p> mask学习：</p> 
<p><a href="https://haolaoshi.blog.csdn.net/article/details/106630857" rel="nofollow" title="(3条消息) NLP 中的Mask全解_mask在自然语言处理代表什么_郝伟博士的博客-CSDN博客">(3条消息) NLP 中的Mask全解_mask在自然语言处理代表什么_郝伟博士的博客-CSDN博客</a></p> 
<p>多头注意力机制学习：</p> 
<p>【1】代码：</p> 
<pre><code class="language-python">class MultiHeadAttention(nn.Module):
    ''' Multi-Head Attention module '''

    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):
        super().__init__()

        self.n_head = n_head
        self.d_k = d_k
        self.d_v = d_v

        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)
        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)
        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)
        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)

        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)

        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)


    def forward(self, q, k, v, mask=None):

        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head
        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)

        residual = q

        # Pass through the pre-attention projection: b x lq x (n*dv)
        # Separate different heads: b x lq x n x dv
        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)
        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)
        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)

        # Transpose for attention dot product: b x n x lq x dv
        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)

        if mask is not None:
            mask = mask.unsqueeze(1)   # For head axis broadcasting.

        q, attn = self.attention(q, k, v, mask=mask)

        # Transpose to move the head dimension back: b x lq x n x dv
        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)
        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)
        q = self.dropout(self.fc(q))
        q += residual

        q = self.layer_norm(q)

        return q, attn</code></pre> 
<p> 【2】解读代码：</p> 
<blockquote> 
 <p>定义了一个名为 <code>MultiHeadAttention</code> 的类，继承自 <code>nn.Module</code>。该类实现了多头注意力模块。</p> 
 <p>在初始化方法 <code>__init__</code> 中，接受了一些参数：<code>n_head</code>（注意力头数）、<code>d_model</code>（模型维度）、<code>d_k</code>（查询和键的维度）、<code>d_v</code>（值的维度）和 <code>dropout</code>（dropout 比率）。</p> 
 <p>首先，这些参数被存储在对应的类属性中。</p> 
 <p>然后，定义了四个线性变换层 <code>w_qs</code>、<code>w_ks</code>、<code>w_vs</code> 和 <code>fc</code>。这些变换层将输入数据投影到不同的维度上，以用于多头注意力的计算。</p> 
 <p>接下来，创建了一个 <code>ScaledDotProductAttention</code> 类的实例 <code>self.attention</code>。这个类尚未提供代码，但根据命名可以猜测是实现了缩放点积注意力机制。</p> 
 <p>然后，创建了一个 <code>nn.Dropout</code> 类的实例 <code>self.dropout</code>，用于进行 dropout 操作。</p> 
 <p>最后，创建了一个 <code>nn.LayerNorm</code> 类的实例 <code>self.layer_norm</code>，用于进行层归一化操作。</p> 
 <p>在前向传播方法 <code>forward</code> 中，首先从类属性中获取各个维度的大小，并保存在局部变量中。</p> 
 <p>然后，将输入 <code>q</code> 复制给 <code>residual</code>，以便后面进行残差连接操作。</p> 
 <p>接下来，将输入 <code>q</code>、<code>k</code> 和 <code>v</code> 分别通过线性变换层 <code>w_qs</code>、<code>w_ks</code> 和 <code>w_vs</code> 进行线性变换，并调整维度以适应多头注意力的计算。</p> 
 <p>通过调用线性变换层的方法，将输入数据分别投影到多个注意力头的维度上，得到的结果维度为 <code>(sz_b, len_q, n_head, d_k)</code>、<code>(sz_b, len_k, n_head, d_k)</code> 和 <code>(sz_b, len_v, n_head, d_v)</code>。</p> 
 <p>对于进行注意力计算的维度调整，需要将 <code>q</code>、<code>k</code> 和 <code>v</code> 的维度进行转置，以便后续进行点积操作。</p> 
 <p>如果提供了掩码 <code>mask</code>，则将其增加一个维度使其与注意力张量的维度相匹配。</p> 
 <p>调用 <code>self.attention</code> 的前向传播函数，对输入的 <code>q</code>、<code>k</code> 和 <code>v</code> 进行多头注意力计算。</p> 
 <p>得到计算后的注意力张量 <code>q</code> 和注意力权重 <code>attn</code>。</p> 
 <p>根据多头注意力计算的结果，对 <code>q</code> 进行维度转置和重塑，然后通过线性变换层 <code>fc</code> 进行线性变换，并进行 dropout 操作。</p> 
 <p>接下来，将残差连接操作的结果与处理后的 <code>q</code> 相加，得到多头注意力模块的输出。</p> 
 <p>最后，对输出进行层归一化操作，并返回输出结果和注意力权重。</p> 
 <p>希望以上解释能够帮助到您理解这段代码的工作原理。如果还有其他问题，请随时提问。</p> 
</blockquote> 
<p> 【3】代码：</p> 
<pre><code class="language-python">class Transformer(nn.Module):
    def __init__(self, feature_dim, num_heads, num_layers,drop_p):
        super(Transformer, self).__init__()
        self.num_layers = num_layers
        self.self_attention_layers = nn.ModuleList([
            SelfAttention(feature_dim, num_heads)
            for _ in range(num_layers)
        ])
        self.feed_forward_layers = nn.ModuleList([
            nn.Sequential(
                nn.Dropout(p=drop_p),
                nn.Linear(feature_dim, feature_dim))
            for _ in range(num_layers)
        ])
        self.norm=nn.LayerNorm(feature_dim)
    def forward(self, x):
        for layer_idx in range(self.num_layers):
            attention_output = self.self_attention_layers[layer_idx](x)
            x = self.norm(x + attention_output)
            feed_forward_output = self.feed_forward_layers[layer_idx](x)
            x = self.norm(x + feed_forward_output)
        return x

class SelfAttention(nn.Module):
    def __init__(self, feature_dim, num_heads):
        super(SelfAttention, self).__init__()
        self.num_heads = num_heads
        self.feature_dim=feature_dim
        self.query = nn.Linear(feature_dim, feature_dim)
        self.key = nn.Linear(feature_dim, feature_dim)
        self.value = nn.Linear(feature_dim, feature_dim)
        self.concat_projection = nn.Linear(feature_dim, feature_dim)
    def forward(self, x):
        q = self.query(x)
        k = self.key(x)
        v = self.value(x)
        query = self.split_heads(q)
        key = self.split_heads(k)
        value = self.split_heads(v)
        scores = torch.matmul(query, key.transpose(-1, -2))
        scaled_scores = scores / torch.sqrt(torch.tensor(self.feature_dim / self.num_heads))
        attention_weights = torch.softmax(scaled_scores, dim=-1)
        attention_output = torch.matmul(attention_weights, value)
        attention_output = self.concat_heads(attention_output)
        return attention_output
    def split_heads(self, x):
        batch_size, seq_len, _ = x.size()
        x_ = x.view(batch_size, seq_len, self.num_heads, self.feature_dim // self.num_heads)
        x_ = x_.permute(0, 2, 1, 3)
        return x_
    def concat_heads(self, x):
        batch_size, _, seq_len, _ = x.size()
        x = x.permute(0, 2, 1, 3).contiguous()
        x = x.view(batch_size, seq_len, self.feature_dim)
        x = self.concat_projection(x)
        return x</code></pre> 
<p>【4】与【1】代码的区别：</p> 
<blockquote> 
 <p>这两种方法都是为了在模型中使用多头注意力机制，处理输入数据并计算注意力权重。它们的目标都是将输入数据分割为多个部分，每个部分对应一个注意力头，然后对每个部分进行投影转换。</p> 
 <p>不同之处在于具体的实现方式：</p> 
 <ul><li> <p>第一种方法中，每个注意力头有独立的权重参数，也就是说，每个注意力头都有自己的“专属”线性变换层。这样做的好处是可以对每个注意力头进行单独的处理，使其更具灵活性，以适应不同的任务需求。</p> </li><li> <p>第二种方法中，所有的注意力头共享相同的权重参数，使用统一的线性变换层来处理输入数据。这样做的好处是减少了模型的参数量，提高了计算效率，但也可能限制了每个注意力头的个性化表达能力。</p> </li></ul> 
</blockquote> 
<p>更通俗来讲区别就是：</p> 
<p>第一种 多头，每一头的数据不一样，第二这种 多头 每一头的数据都一样 </p> 
<p>第一种： 数据x 分为多头就是 heads1---&gt;x  heads2---&gt;x</p> 
<p>第二种：数据x 分为多头就是 heads1----&gt;x/num_heads</p> 
<hr> 
<hr> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b5db46ebcef997774e7dcfa8a36ad1cd/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">移动端声明及meta标签设置</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/04c1914591f723235c140c087bdbe095/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">(五) 空间查询</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>