<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>最新 | Ask Me Anything 一种提示(Prompt)语言模型的简单策略（斯坦福大学 &amp; 含源码） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="最新 | Ask Me Anything 一种提示(Prompt)语言模型的简单策略（斯坦福大学 &amp; 含源码）" />
<meta property="og:description" content="来源: AINLPer微信公众号（每日干货分享！！）
编辑: ShuYini
校稿: ShuYini
时间: 2022-09-30
引言 提示(Prompt)的微小变化就会引起大型语言模型(LLM)较大的性能变化，这将会有大量时间花费在提示（prompt）设计上。为此本文提出了ASK ME ANYTHING(AMA)方法，该方法首先产生多个有效且不完美的prompt，然后将它们进行聚合，最后产生高质量的提示(prompt)。
关注 AINLPer公众号，最新干货第一时间送达
背景介绍 大型语言模型(LLM)让我们更接近任务无关机器学习的目标。LLM 不是为新任务训练模型，而是开箱即用地应用于新任务。在上下文学习的范式中，通过自然语言任务规范或提示（prompt）来控制LLM。其中提示（prompt）由模板定义，该模板包含用于描述和表示任务输入和输出的占位符。
最近的工作评估了LLM在一系列任务中的提示（prompt）性能，实验发现，提示（prompt）的微小变化会导致较大的性能变化。并且提示（prompt）性能还取决于所选LLM系列和模型大小。为了提高可靠性，大量的工作致力于精心设计一个完美的提示（prompt）。例如，就有专家建议用户手动探索大型搜索空间的策略，以便在逐个任务的基础上优化提示（prompt）。
相反，本文考虑聚合多个有效但不完美的提示（prompt）的预测，以提高在各种的模型和任务上的提示（prompt）性能。 给定一个任务输入，每个提示（prompt）都会对输入的真实标签进行投票，这些投票被聚合以产生最终预测。
遇到的问题 在追求聚合的高质量提示（prompt）过程中，我们面临以下挑战：
高质量的提示（Effective prompts）：高质量的提示是聚合效果提升的首要条件。在两个SuperGLUE任务(CB, RTE)中，我们采用了原始提示，这些提示产生了近乎随机的性能。以相同的格式生成多个提示并在提示之间进行多数投票预测的影响较小(CB为&#43;4%)，甚至可能损害平均提示性能(RTE为-2%)。许多改进提示（prompt）的建议关注单一任务类型，并基于单一模型系列和/或大小进行评估。为此，我们需要一个跨任务和模型工作的提示结构。
可扩展的集合(Scalable collection) ：在确定有效的提示格式之后，我们需要获得这些格式的多个提示----这些提示主要是为输入的真实标签收集投票。任务的原始格式变化很大，之前的工作以特定于任务的方式手动将输入示例重写为新格式，这是具有挑战性的扩展。我们需要一种可伸缩的策略来重新格式化任务输入。
提示聚合(Prompt aggregation) ：使用上面的提示(对于CB和RTE)，我们看到准确性的平均变化为9.5%，并且错误的Jaccard指数比识别提示错误高出69%。之前提示工作中，多数投票(MV)是主要无监督聚合策略，但它没有考虑这两种特性，因此不可靠。我们需要一种策略来解释不同的准确性和依赖性。
AMA模型方法介绍 问题解决 1、识别提示的属性，这些属性可以提升跨任务、模型类型和模型大小的效率。我们研究了先前工作分类的标准提示格式，发现支持开放式回答(“约翰去哪儿了?”)的提示比将模型输出限制为特定tokens 的提示更有效。例如，将[Brown等人，2020]中最初的限制性格式中的三个SuperGLUE任务(CB、RTE、WSC)转换为开放式格式可以提高72%的性能。给定一个任务输入，我们发现根据输入形成问题、提示LLM回答问题的简单结构，可以适用于相当普遍的情况并在不同的基准测试任务中提升性能。
2、提出了一种可伸缩地将任务输入重新格式化为(1)中发现的有效格式的策略。通过在固定的两步管道中递归地使用LLM本身，将任务输入转换为有效的开放式问答格式。我们首先使用question()提示符，它包含如何将语句转换为各种(例如，yes-no，完形填空)问题的任务无关示例，然后使用answer()提示符演示回答问题的方法(例如，简明或冗长的回答)。应用提示链-答案(问题(x)) ----给出输入 x 2 x^2 x2的最终预测。该链可以在输入之间重复使用，并组合不同的功能提示对来产生多样性。我们将不同的功能提示链应用于输入，为输入的真实标签收集多次投票。
3、使用弱监督(WS)来可靠地聚合预测。实验发现，由不同链的预测所产生的误差可以是高度变化和相关的。虽然多数投票(MV)可能在某些提示集上表现良好，但在上述情况下表现不佳。AMA通过识别提示之间的依赖关系并使用WS来解释这些情况，WS是在没有任何标记数据的情况下建模和组合噪声预测的过程。这里，本文首次将WS广泛应用于提示，表明它提高了使用现成的LLM并且无需进一步训练。
AMA模型方法 总结以上问题解决方法，本文提出了 ASK ME ANYTHING PROMPTING (AMA)，这是一种简单的方法，它不仅使开源 LLM 的参数减少30倍，而且超过了 GPT3-175B 的Few-Shot性能。
其中如上图所示：AMA首先递归地使用LLM将任务和提示重新格式化为有效的格式，然后使用弱监督聚合跨提示的预测。重新格式化是使用提示链来执行的，提示链由在不同的任务输入上操作的功能性(固定的、可重用的)提示组成。在这里，给定输入示例，提示链包括一个question()提示符，LLM通过这个提示符将输入声明转换为一个问题，以及一个answer()提示符，LLM通过这个提示符回答它生成的问题。不同的提示链(即不同的上下文问题和答案演示)导致对输入的真实标签的不同预测。
实验结果 1、下表1中比较开源GPT-J-6B和Few-Shot(k∈[32…70])GPT3- 175B的基准测试结果。可以发现，在20个基准测试中，有15个开源6B参数模型超过了GPT3-175B模型的平均Few-Shot性能。在20个任务中，AMA比6B参数模型的少次数(k = 3)性能平均提高了41%。
2、跨模型大小的分析和基准评估。 我们报告了 AMA 对少样本 (k = 3) 性能的绝对提升，平均超过 7 个任务，置信区间为 95%（左）。 按 7 项任务的平均 AMA 提升排序（右）。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/75f595e793028787dbae1550af772043/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-10-08T20:04:11+08:00" />
<meta property="article:modified_time" content="2022-10-08T20:04:11+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">最新 | Ask Me Anything 一种提示(Prompt)语言模型的简单策略（斯坦福大学 &amp; 含源码）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <blockquote> 
 <p>来源: <strong>AINLPer微信公众号</strong>（每日干货分享！！）<br> 编辑: ShuYini<br> 校稿: ShuYini<br> 时间: 2022-09-30</p> 
</blockquote> 
<h3><a id="_5"></a>引言</h3> 
<p>提示(Prompt)的微小变化就会引起大型语言模型(LLM)较大的性能变化，这将会有大量时间花费在提示（prompt）设计上。为此本文提出了ASK ME ANYTHING(AMA)方法，该方法首先产生多个有效且不完美的prompt，然后将它们进行聚合，最后产生高质量的提示(prompt)。<br> <img src="https://images2.imgbox.com/30/28/nVxhq1Tq_o.png" alt="在这里插入图片描述"></p> 
<p><mark>关注 <a href="https://mp.weixin.qq.com/s?__biz=MzUzOTgwNDMzOQ==&amp;mid=2247485618&amp;idx=1&amp;sn=b60c2799f448dec67de8807aa2abbe0b&amp;chksm=fac39f6ecdb41678213b80ceb0a95f2c08a5a99cfc956a662ebe9dfcc73d7fba390f387a3ab4&amp;scene=21#wechat_redirect" rel="nofollow"><strong>AINLPer公众号</strong></a>，最新干货第一时间送达</mark></p> 
<h3><a id="_12"></a>背景介绍</h3> 
<p> 大型语言模型(LLM)让我们更接近任务无关机器学习的目标。<strong>LLM 不是为新任务训练模型，而是开箱即用地应用于新任务</strong>。在上下文学习的范式中，通过自然语言任务规范或提示（prompt）来控制LLM。其中提示（prompt）由模板定义，该模板包含用于描述和表示任务输入和输出的占位符。</p> 
<p> 最近的工作评估了LLM在一系列任务中的提示（prompt）性能，实验发现，提示（prompt）的微小变化会导致较大的性能变化。并且提示（prompt）性能还取决于所选LLM系列和模型大小。为了提高可靠性，大量的工作致力于精心设计一个完美的提示（prompt）。例如，就有专家建议用户手动探索大型搜索空间的策略，以便在逐个任务的基础上优化提示（prompt）。</p> 
<p> 相反，<strong>本文考虑聚合多个有效但不完美的提示（prompt）的预测，以提高在各种的模型和任务上的提示（prompt）性能</strong>。 给定一个任务输入，每个提示（prompt）都会对输入的真实标签进行投票，这些投票被聚合以产生最终预测。</p> 
<h3><a id="_20"></a>遇到的问题</h3> 
<p> 在追求聚合的高质量提示（prompt）过程中，我们面临以下挑战：</p> 
<p><strong>高质量的提示（Effective prompts）</strong>：高质量的提示是聚合效果提升的首要条件。在两个SuperGLUE任务(CB, RTE)中，我们采用了原始提示，这些提示产生了近乎随机的性能。以相同的格式生成多个提示并在提示之间进行多数投票预测的影响较小(CB为+4%)，甚至可能损害平均提示性能(RTE为-2%)。许多改进提示（prompt）的建议关注单一任务类型，并基于单一模型系列和/或大小进行评估。为此，<strong>我们需要一个跨任务和模型工作的提示结构</strong>。</p> 
<p><strong>可扩展的集合(Scalable collection)</strong> ：在确定有效的提示格式之后，我们需要获得这些格式的多个提示----这些提示主要是为输入的真实标签收集投票。任务的原始格式变化很大，之前的工作以特定于任务的方式手动将输入示例重写为新格式，这是具有挑战性的扩展。<strong>我们需要一种可伸缩的策略来重新格式化任务输入</strong>。</p> 
<p><strong>提示聚合(Prompt aggregation)</strong> ：使用上面的提示(对于CB和RTE)，我们看到准确性的平均变化为9.5%，并且错误的Jaccard指数比识别提示错误高出69%。之前提示工作中，多数投票(MV)是主要无监督聚合策略，但它没有考虑这两种特性，因此不可靠。<strong>我们需要一种策略来解释不同的准确性和依赖性</strong>。</p> 
<h3><a id="AMA_29"></a>AMA模型方法介绍</h3> 
<h4><a id="_31"></a>问题解决</h4> 
<p> 1、识别提示的属性，这些属性可以提升跨任务、模型类型和模型大小的效率。<strong>我们研究了先前工作分类的标准提示格式，发现支持开放式回答(“约翰去哪儿了?”)的提示比将模型输出限制为特定tokens 的提示更有效</strong>。例如，将[Brown等人，2020]中最初的限制性格式中的三个SuperGLUE任务(CB、RTE、WSC)转换为开放式格式可以提高72%的性能。给定一个任务输入，我们发现根据输入形成问题、提示LLM回答问题的简单结构，可以适用于相当普遍的情况并在不同的基准测试任务中提升性能。</p> 
<p> 2、<strong>提出了一种可伸缩地将任务输入重新格式化为(1)中发现的有效格式的策略</strong>。通过在固定的两步管道中递归地使用LLM本身，将任务输入转换为有效的开放式问答格式。我们首先使用question()提示符，它包含如何将语句转换为各种(例如，yes-no，完形填空)问题的任务无关示例，然后使用answer()提示符演示回答问题的方法(例如，简明或冗长的回答)。应用提示链-答案(问题(x)) ----给出输入<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          x 
         
        
          2 
         
        
       
      
        x^2 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8141em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>的最终预测。该链可以在输入之间重复使用，并组合不同的功能提示对来产生多样性。我们将不同的功能提示链应用于输入，为输入的真实标签收集多次投票。</p> 
<p> 3、<strong>使用弱监督(WS)来可靠地聚合预测</strong>。实验发现，由不同链的预测所产生的误差可以是高度变化和相关的。虽然多数投票(MV)可能在某些提示集上表现良好，但在上述情况下表现不佳。<strong>AMA通过识别提示之间的依赖关系并使用WS来解释这些情况，WS是在没有任何标记数据的情况下建模和组合噪声预测的过程</strong>。这里，本文首次将WS广泛应用于提示，表明它提高了使用现成的LLM并且无需进一步训练。</p> 
<h4><a id="AMA_39"></a>AMA模型方法</h4> 
<p> 总结以上问题解决方法，本文提出了 ASK ME ANYTHING PROMPTING (AMA)，这是一种简单的方法，它不仅使开源 LLM 的参数减少30倍，而且超过了 GPT3-175B 的Few-Shot性能。<br> <img src="https://images2.imgbox.com/59/29/OJNQDery_o.png" alt=""><br>  其中如上图所示：AMA首先递归地使用LLM将任务和提示重新格式化为有效的格式，然后使用弱监督聚合跨提示的预测。重新格式化是使用提示链来执行的，提示链由在不同的任务输入上操作的功能性(固定的、可重用的)提示组成。在这里，给定输入示例，提示链包括一个question()提示符，LLM通过这个提示符将输入声明转换为一个问题，以及一个answer()提示符，LLM通过这个提示符回答它生成的问题。不同的提示链(即不同的上下文问题和答案演示)导致对输入的真实标签的不同预测。</p> 
<h3><a id="_45"></a>实验结果</h3> 
<p>1、下表1中比较开源GPT-J-6B和Few-Shot(k∈[32…70])GPT3- 175B的基准测试结果。可以发现，在20个基准测试中，有15个开源6B参数模型超过了GPT3-175B模型的平均Few-Shot性能。在20个任务中，AMA比6B参数模型的少次数(k = 3)性能平均提高了41%。<br> <img src="https://images2.imgbox.com/22/43/VsD4P2Ly_o.png" alt=""><br> 2、跨模型大小的分析和基准评估。 我们报告了 AMA 对少样本 (k = 3) 性能的绝对提升，平均超过 7 个任务，置信区间为 95%（左）。 按 7 项任务的平均 AMA 提升排序（右）。<br> <img src="https://images2.imgbox.com/46/43/CDd1NLRi_o.png" alt=""><br> 3、Sanh等人实验结果T0的性能与prompt-source中10种不同提示格式的多数投票(MV)和弱监督(WS)相比。当使用prompt-source时，MV和WS的平均提升分别为3.6分和6.1分。<br> <img src="https://images2.imgbox.com/aa/0e/uRWHFz97_o.png" alt=""></p> 
<h4><a id="_52"></a><strong>推荐阅读</strong></h4> 
<p>[1] <a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzUzOTgwNDMzOQ%3D%3D%26mid%3D2247486784%26idx%3D1%26sn%3D17a2652ae7e0d9a4d8dc68b323756d8d%26chksm%3Dfac39a9ccdb4138ac7914d5004d11bd27a6ea5de7a145146bd51619f74455ec8f5738b5ddc67%26scene%3D21%23wechat_redirect" rel="nofollow">一文了解EMNLP国际顶会 &amp;&amp; 历年EMNLP论文下载 &amp;&amp; 含EMNLP2022</a></p> 
<p>[2]<a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzUzOTgwNDMzOQ%3D%3D%26mid%3D2247486764%26idx%3D1%26sn%3D6d4ba1f81785804bcf910ed053472a12%26chksm%3Dfac39af0cdb413e681dc7837a1a3c27ffe9c6b9f4d8cb298c9df84ba9a60a5933084da09ccea%26scene%3D21%23wechat_redirect" rel="nofollow">【历年NeurIPS论文下载】一文带你看懂NeurIPS国际顶会（内含NeurIPS2022）</a></p> 
<p>[3]<a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzUzOTgwNDMzOQ%3D%3D%26mid%3D2247486807%26idx%3D1%26sn%3D5ca34348952e28578b43507beaad0885%26chksm%3Dfac39a8bcdb4139db9e2425d5fb02c3293ff17f4048b7ec4db98550fad25d6291efbaa6d90cd%26scene%3D21%23wechat_redirect" rel="nofollow">【微软研究院 &amp;&amp; 含源码】相比黑盒模型，可解释模型同样可以获得理想的性能</a></p> 
<p>[4]<a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzUzOTgwNDMzOQ%3D%3D%26mid%3D2247486795%26idx%3D1%26sn%3Da30449c67a12e612ea525f330a8ae2f7%26chksm%3Dfac39a97cdb41381195ac183fa3daee431c6a49b38f8145645d18712696ef726533d16e1aad4%26scene%3D21%23wechat_redirect" rel="nofollow">【IJCAI2022&amp;&amp;知识图谱】联邦环境下，基于元学习的图谱知识外推（阿里&amp;浙大&amp;含源码）</a></p> 
<p>[5]<a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzUzOTgwNDMzOQ%3D%3D%26mid%3D2247486588%26idx%3D1%26sn%3D4c518d2aa50c1060cc9af8f7abf04e0a%26chksm%3Dfac39ba0cdb412b6a7c4da1b5277ab7e27ecfbcc6558028d2693a7a05a8400bdbd2b9712618d%26scene%3D21%23wechat_redirect" rel="nofollow">【NLP论文分享&amp;&amp;语言表示】有望颠覆Transformer的图循环神经网络（GNN）</a></p> 
<p>[6]<a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzUzOTgwNDMzOQ%3D%3D%26mid%3D2247486775%26idx%3D1%26sn%3D9962b5b4115c11a79eaed4b12504d455%26chksm%3Dfac39aebcdb413fdbb38c61ba97964edcfa9a06a8e66ed19005a39e8f44fb0184f87d413b3fd%26scene%3D21%23wechat_redirect" rel="nofollow">【NeurIPS &amp;&amp; 图谱问答】知识图谱(KG) Mutil-Hop推理的锥形嵌入方法（中科院–含源码）</a></p> 
<p>[7]<a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzUzOTgwNDMzOQ%3D%3D%26mid%3D2247486756%26idx%3D1%26sn%3Dab2385ae5d11f9371a63e5838a210b28%26chksm%3Dfac39af8cdb413eecccb630e28058e63d9fb0296e9ee8e4c5f1d31e54562a125e1c74a198138%26scene%3D21%23wechat_redirect" rel="nofollow">【NLP论文分享 &amp;&amp; QA问答】动态关联GNN建立直接关联，优化multi-hop推理（含源码）</a></p> 
<p>[8]<a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzUzOTgwNDMzOQ%3D%3D%26mid%3D2247486686%26idx%3D1%26sn%3D7c493d325a9210531f0a5df8959cdef3%26chksm%3Dfac39b02cdb41214cd181b27143091e0bb5a3c5154815d2bf94bae9ae7814b653841d3a07db9%26scene%3D21%23wechat_redirect" rel="nofollow">【历年IJCAI论文下载 &amp;&amp; 论文速递】无数据对抗蒸馏、垂直联合、预训练微调范式图神经网络（GNN）</a></p> 
<p>[9]<a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzUzOTgwNDMzOQ%3D%3D%26mid%3D2247486571%26idx%3D1%26sn%3D8c60fd6d2c8a6eca3caad21977252c6a%26chksm%3Dfac39bb7cdb412a19f8020dff943acfbfc792510335fc1aaf7fd974e37fa9b17f4f6802cbfe3%26scene%3D21%23wechat_redirect" rel="nofollow">【NLP论文分享&amp;&amp;中文命名实体识别】如何构建一个优秀的Gazetteer/地名词典（浙大&amp;含源码）</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8bb1660f1b33d5d4bf0ce3ab27d10dfd/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">eNSP基础命令_01</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/af943f1dd0a8f0915525c2bda3fd936a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">全排列demo</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>