<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>清华开源语言大模型ChatGLM3部署实战 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="清华开源语言大模型ChatGLM3部署实战" />
<meta property="og:description" content="清华开源语言大模型ChatGLM3部署实战 安装环境下载模型文件测试是否安装成功gpu推理cpu推理量化推理 ChatGLM3 是智谱AI和清华大学 KEG 实验室联合发布的新一代对话预训练模型。 项目库地址：https://github.com/THUDM/ChatGLM3 安装环境 建议使用虚拟环境
git clone https://github.com/THUDM/ChatGLM3 cd ChatGLM3 pip install -r requirements.txt 其中 transformers 库版本推荐为 4.30.2，torch 推荐使用 2.0 及以上的版本，以获得最佳的推理性能。
下载模型文件 git lfs install git clone https://www.modelscope.cn/ZhipuAI/chatglm3-6b.git 需等待较长时间
测试是否安装成功 推理时将THUDM/chatglm3-6b改为自己下载模型的路径
gpu推理 推理需13g以上显存
from transformers import AutoTokenizer, AutoModel tokenizer = AutoTokenizer.from_pretrained(&#34;THUDM/chatglm3-6b&#34;, trust_remote_code=True) model = AutoModel.from_pretrained(&#34;THUDM/chatglm3-6b&#34;, trust_remote_code=True, device=&#39;cuda&#39;) model = model.eval() response, history = model.chat(tokenizer, &#34;你好&#34;, history=[]) print(response) response, history = model.chat(tokenizer, &#34;晚上睡不着应该怎么办&#34;, history=history) print(response) cpu推理 推理需32g以上内存" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/21e9a2317c326366c2f6b135a79fff55/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-11-17T17:00:08+08:00" />
<meta property="article:modified_time" content="2023-11-17T17:00:08+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">清华开源语言大模型ChatGLM3部署实战</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>清华开源语言大模型ChatGLM3部署实战</h4> 
 <ul><li><a href="#_3" rel="nofollow">安装环境</a></li><li><a href="#_11" rel="nofollow">下载模型文件</a></li><li><a href="#_20" rel="nofollow">测试是否安装成功</a></li><li><ul><li><a href="#gpu_22" rel="nofollow">gpu推理</a></li><li><a href="#cpu_34" rel="nofollow">cpu推理</a></li><li><a href="#_48" rel="nofollow">量化推理</a></li></ul> 
 </li></ul> 
</div> 
<br> ChatGLM3 是智谱AI和清华大学 KEG 实验室联合发布的新一代对话预训练模型。 
<br> 项目库地址：https://github.com/THUDM/ChatGLM3 
<p></p> 
<h2><a id="_3"></a>安装环境</h2> 
<p>建议使用虚拟环境</p> 
<pre><code class="prism language-python">git clone https<span class="token punctuation">:</span><span class="token operator">//</span>github<span class="token punctuation">.</span>com<span class="token operator">/</span>THUDM<span class="token operator">/</span>ChatGLM3
cd ChatGLM3
pip install <span class="token operator">-</span>r requirements<span class="token punctuation">.</span>txt
</code></pre> 
<p>其中 transformers 库版本推荐为 4.30.2，torch 推荐使用 2.0 及以上的版本，以获得最佳的推理性能。</p> 
<h2><a id="_11"></a>下载模型文件</h2> 
<pre><code class="prism language-python">git lfs install
git clone https<span class="token punctuation">:</span><span class="token operator">//</span>www<span class="token punctuation">.</span>modelscope<span class="token punctuation">.</span>cn<span class="token operator">/</span>ZhipuAI<span class="token operator">/</span>chatglm3<span class="token operator">-</span>6b<span class="token punctuation">.</span>git
</code></pre> 
<p>需等待较长时间<br> <img src="https://images2.imgbox.com/fd/ec/kDHZdH7h_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_20"></a>测试是否安装成功</h2> 
<p>推理时将THUDM/chatglm3-6b改为自己下载模型的路径</p> 
<h3><a id="gpu_22"></a>gpu推理</h3> 
<p>推理需13g以上显存</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModel
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"THUDM/chatglm3-6b"</span><span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> AutoModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"THUDM/chatglm3-6b"</span><span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda'</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
response<span class="token punctuation">,</span> history <span class="token operator">=</span> model<span class="token punctuation">.</span>chat<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> <span class="token string">"你好"</span><span class="token punctuation">,</span> history<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span>
response<span class="token punctuation">,</span> history <span class="token operator">=</span> model<span class="token punctuation">.</span>chat<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> <span class="token string">"晚上睡不着应该怎么办"</span><span class="token punctuation">,</span> history<span class="token operator">=</span>history<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="cpu_34"></a>cpu推理</h3> 
<p>推理需32g以上内存</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModel
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"THUDM/chatglm3-6b"</span><span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> AutoModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"THUDM/chatglm3-6b"</span><span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span>
model <span class="token operator">=</span> model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
response<span class="token punctuation">,</span> history <span class="token operator">=</span> model<span class="token punctuation">.</span>chat<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> <span class="token string">"你好"</span><span class="token punctuation">,</span> history<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span>
response<span class="token punctuation">,</span> history <span class="token operator">=</span> model<span class="token punctuation">.</span>chat<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> <span class="token string">"晚上睡不着应该怎么办"</span><span class="token punctuation">,</span> history<span class="token operator">=</span>history<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="_48"></a>量化推理</h3> 
<pre><code class="prism language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModel
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"THUDM/chatglm3-6b"</span><span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> AutoModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"THUDM/chatglm3-6b"</span><span class="token punctuation">,</span>trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span>quantize<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
response<span class="token punctuation">,</span> history <span class="token operator">=</span> model<span class="token punctuation">.</span>chat<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> <span class="token string">"你好"</span><span class="token punctuation">,</span> history<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span>
response<span class="token punctuation">,</span> history <span class="token operator">=</span> model<span class="token punctuation">.</span>chat<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> <span class="token string">"晚上睡不着应该怎么办"</span><span class="token punctuation">,</span> history<span class="token operator">=</span>history<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span>
</code></pre> 
<p>如果遇到什么问题欢迎评论区交流<br> 群内交流更多技术<br> 130856474</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/43179ccad6ef259cccf93b2df7115982/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">基于UniGUI的管理系统</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a40cc2920a50d94ba29f02953843b602/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Gradio构建AI算法网页界面显示教程（附实战代码）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>