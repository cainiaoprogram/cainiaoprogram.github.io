<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>用Python实现神经网络（附完整代码）！ - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="用Python实现神经网络（附完整代码）！" />
<meta property="og:description" content="前言 在学习神经网络之前，我们需要对神经网络底层先做一个基本的了解。我们将在本节介绍感知机、反向传播算法以及多种梯度下降法以给大家一个全面的认识。
【----帮助Python学习，以下所有学习资料文末免费领！----】
一、感知机 数字感知机的本质是从数据集中选取一个样本（example），并将其展示给算法，然后让算法判断“是”或“不是”。一般而言，把单个特征表示为xi，其中i是整数。所有特征的集合表示为，表示一个向量：，
类似地，每个特征的权重表示为 其中 对应于与该权重关联的特征的下标，所有权重可统一表示为 一个向量:
这里有一个缺少的部分是是否激活神经元的阈值。一旦加权和超过某个阈值，感知机就输出1，否则输出0。我们可以使用一个简单的阶跃函数（在图5-2中标记为“激活函数”）来表示这个阈值。
注：所有神经网络的基本单位都是神经元，基本感知机是广义神经元的一个特例，从现在开始，我们将感知机称为一个神经元。
二、反向传播算法
2.1 代价函数 很多数据值之间的关系不是线性的，也没有好的线性回归或线性方程能够描述这些关系。许多数据集不能用直线或平面来线性分割。比如下图中左图为线性可分的数据，而右图为线性不可分的数据：
而我们训练神经网络（感知机）的目标是最小化所有输入样本数据的代价函数
2.2 反向传播 权重通过下一层的权重（）和（）来影响误差，因此我们需要一种方法来计算对误差的贡献，这个方法就是反向传播。
感知机的每个输入都有一个权重，第二层神经元的权重不是分配给原始输入的，而是分配给来自第一层的各个输出。从这里我们可以看到计算第一层权重对总体误差的影响的难度。第一层权重对误差的影响并不是只来自某个单独权重，而是通过下一层中每个神经元的权重来产生的。反向传播的推导过程较为复杂，这里仅简单展示其结果：
如果该层是输出层，借助于可微的激活函数，权重的更新比较简单, 对于第 个输出，误差的导数如下
如果要更新隐藏层的权重，则会稍微复杂一点儿：
函数表示实际结果向量，表示该向量第个位置上的值，，是倒数第二层第个节点和输出第个节点的输出，连接这两个节点的权重为，误差代价函数对求导的结果相当于用（学习率）乘以前一层的输出再乘以后一层代价函数的导数。公式中表示层第个节点上的误差项，前一层第个节点到层所有的节点进行加权求和。
2.3 多种梯度下降法 到目前为止，我们一直是把所有训练样本的误差聚合起来然后再做梯度下降，这种训练方法称为批量学习（batch learning）。一批是训练数据的一个子集。但是在批量学习中误差曲面对于整个批是静态的，如果从一个随机的起始点开始，得到的很可能是某个局部极小值，从而无法看到其他的权重值的更优解。这里有两种方法来避开这个陷阱。
第一种方法是随机梯度下降法。在随机梯度下降中，不用去查看所有的训练样本，而是在输入每个训练样本后就去更新网络权重。在这个过程中，每次都会重新排列训练样本的顺序，这样将为每个样本重新绘制误差曲面，由于每个相异的输入都可能有不同的预期答案，因此大多数样本的误差曲面都不一样。对每个样本来说，仍然使用梯度下降法来调整权重。不过不用像之前那样在每个训练周期结束后聚合所有误差再做权重调整，而是针对每个样本都会去更新一次权重。其中的关键点是，每一步都在向假定的极小值前进（不是所有路径都通往假定的极小值）。
使用正确的数据和超参数，在向这个波动误差曲面的各个最小值前进时，可以更容易地得到全局极小值。如果模型没有进行适当的调优，或者训练数据不一致，将导致原地踏步，模型无法收敛，也学不会任何东西。不过在实际应用中，随机梯度下降法在大多数情况下都能有效地避免局部极小值。这种方法的缺点是计算速度比较慢。计算前向传播和反向传播，然后针对每个样本进行权重更新，这在本来已经很慢的计算过程的基础上又增加了很多时间开销。
第二种方法，也是更常见的方法，是小批量学习。在小批量学习中，会传入训练集的一个小的子集，并按照批量学习中的误差聚合方法对这个子集对应的误差进行聚合。然后对每个子集按批将其误差进行反向传播并更新权重。下一批会重复这个过程，直到训练集处理完成为止，这就重新构成了一个训练周期。这是一种折中的办法，它同时具有批量学习（快速）和随机梯度下降（具有弹性）的优点。
三、Keras：用Python实现神经网络
用原生Python来编写神经网络是一个非常有趣的尝试，而且可以帮助大家理解神经网络中的各种概念，但是Python在计算速度上有明显缺陷，即使对于中等规模的网络，计算量也会变得非常棘手。不过有许多Python库可以用来提高运算速度，包括PyTorch、Theano、TensorFlow和Lasagne等。本书中的例子使用Keras。
Keras是一个高级封装器，封装了面向Python的API。API接口可以与3个不同的后端库相兼容：Theano、谷歌的TensorFlow和微软的CNTK。这几个库都在底层实现了基本的神经网络单元和高度优化的线性代数库，可以用于处理点积，以支持高效的神经网络矩阵乘法运算。
我们以简单的异或问题为例，看看如何用Keras来训练这个网络。
import numpy as np from keras.models import Sequential　# Kera的基础模型类 from keras.layers import Dense, Activation　# Dense是神经元的全连接层 from keras.optimizers import SGD　# 随机梯度下降，Keras中还有一些其他优化器 # Our examples for an exclusive OR. x_train = np.array([[0, 0],[0, 1],[1, 0],[1, 1]]) # x_train是二维特征向量表示的训练样本列表 y_train = np." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/d2f8364c77b74977a18a4d734e0b7c10/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-05T15:26:17+08:00" />
<meta property="article:modified_time" content="2024-01-05T15:26:17+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">用Python实现神经网络（附完整代码）！</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_0"></a>前言</h2> 
<p>在学习神经网络之前，我们需要对神经网络底层先做一个基本的了解。我们将在本节介绍感知机、反向传播算法以及多种梯度下降法以给大家一个全面的认识。</p> 
<p><img src="https://images2.imgbox.com/98/df/wqirNr3a_o.png" alt="48756dbd1e6dd490d0b8737d18c88805.png"><br> <mark><strong>【----帮助Python学习，以下所有学习资料文末免费领！----】</strong></mark></p> 
<h3><a id="_7"></a><strong>一、感知机</strong></h3> 
<p>数字感知机的本质是从数据集中选取一个样本（example），并将其展示给算法，然后让算法判断“是”或“不是”。一般而言，把单个特征表示为<em>xi</em>，其中<em>i</em>是整数。所有特征的集合表示为，表示一个向量：，</p> 
<p>类似地，每个特征的权重表示为 其中 对应于与该权重关联的特征的下标，所有权重可统一表示为 一个向量:</p> 
<p>这里有一个缺少的部分是是否激活神经元的阈值。一旦加权和超过某个阈值，感知机就输出1，否则输出0。我们可以使用一个简单的<strong>阶跃函数</strong>（在图5-2中标记为“激活函数”）来表示这个阈值。</p> 
<p><img src="https://images2.imgbox.com/0b/1f/kftxDkC2_o.png" alt="f2dfee6bb6135dc428593ed253761a0e.png"></p> 
<blockquote> 
 <p>注：所有神经网络的基本单位都是神经元，基本感知机是广义神经元的一个特例，从现在开始，我们将感知机称为一个神经元。</p> 
</blockquote> 
<p><strong>二、反向传播算法</strong></p> 
<h4><a id="21__21"></a>2.1 代价函数</h4> 
<p>很多数据值之间的关系不是线性的，也没有好的线性回归或线性方程能够描述这些关系。许多数据集不能用直线或平面来线性分割。比如下图中左图为线性可分的数据，而右图为线性不可分的数据：</p> 
<p><img src="https://images2.imgbox.com/b4/eb/BtTDW5hz_o.png" alt="327f7f4f00c551ff89b26920c774a06a.png"></p> 
<p>而我们训练神经网络（感知机）的目标是最小化所有输入样本数据的代价函数</p> 
<h4><a id="22__29"></a>2.2 反向传播</h4> 
<p>权重通过下一层的权重（）和（）来影响误差，因此我们需要一种方法来计算对误差的贡献，这个方法就是<strong>反向传播</strong>。</p> 
<p><img src="https://images2.imgbox.com/11/6a/KSSKbSwh_o.png" alt="a85abc3241e2695a592d81f01f489967.png"></p> 
<p>感知机的每个输入都有一个权重，第二层神经元的权重不是分配给原始输入的，而是分配给来自第一层的各个输出。从这里我们可以看到计算第一层权重对总体误差的影响的难度。第一层权重对误差的影响并不是只来自某个单独权重，而是通过下一层中每个神经元的权重来产生的。反向传播的推导过程较为复杂，这里仅简单展示其结果：</p> 
<p>如果该层是输出层，借助于可微的激活函数，权重的更新比较简单, 对于第 个输出，误差的导数如下</p> 
<p>如果要更新隐藏层的权重，则会稍微复杂一点儿：</p> 
<p>函数表示实际结果向量，表示该向量第个位置上的值，，是倒数第二层第个节点和输出第个节点的输出，连接这两个节点的权重为，误差代价函数对求导的结果相当于用（学习率）乘以前一层的输出再乘以后一层代价函数的导数。公式中表示层第个节点上的误差项，前一层第个节点到层所有的节点进行加权求和。</p> 
<h4><a id="23__43"></a>2.3 多种梯度下降法</h4> 
<p>到目前为止，我们一直是把所有训练样本的误差聚合起来然后再做梯度下降，这种训练方法称为<strong>批量学习</strong>（batch learning）。一批是训练数据的一个子集。但是在批量学习中误差曲面对于整个批是静态的，如果从一个随机的起始点开始，得到的很可能是某个局部极小值，从而无法看到其他的权重值的更优解。这里有两种方法来避开这个陷阱。</p> 
<p>第一种方法是<strong>随机</strong>梯度下降法。在随机梯度下降中，不用去查看所有的训练样本，而是在输入每个训练样本后就去更新网络权重。在这个过程中，每次都会重新排列训练样本的顺序，这样将为每个样本重新绘制误差曲面，由于每个相异的输入都可能有不同的预期答案，因此大多数样本的误差曲面都不一样。对每个样本来说，仍然使用梯度下降法来调整权重。不过不用像之前那样在每个训练周期结束后聚合所有误差再做权重调整，而是针对每个样本都会去更新一次权重。其中的关键点是，每一步都在向假定的极小值<strong>前进</strong>（不是所有路径都通往假定的极小值）。</p> 
<p>使用正确的数据和超参数，在向这个波动误差曲面的各个最小值前进时，可以更容易地得到全局极小值。如果模型没有进行适当的调优，或者训练数据不一致，将导致原地踏步，模型无法收敛，也学不会任何东西。不过在实际应用中，随机梯度下降法在大多数情况下都能有效地避免局部极小值。这种方法的缺点是计算速度比较慢。计算前向传播和反向传播，然后针对每个样本进行权重更新，这在本来已经很慢的计算过程的基础上又增加了很多时间开销。</p> 
<p>第二种方法，也是更常见的方法，是<strong>小批量</strong>学习。在小批量学习中，会传入训练集的一个小的子集，并按照批量学习中的误差聚合方法对这个子集对应的误差进行聚合。然后对每个子集按批将其误差进行反向传播并更新权重。下一批会重复这个过程，直到训练集处理完成为止，这就重新构成了一个训练周期。这是一种折中的办法，它同时具有<strong>批量</strong>学习（快速）和<strong>随机</strong>梯度下降（具有弹性）的优点。</p> 
<p><strong>三、Keras：用Python实现神经网络</strong></p> 
<p>用原生Python来编写神经网络是一个非常有趣的尝试，而且可以帮助大家理解神经网络中的各种概念，但是Python在计算速度上有明显缺陷，即使对于中等规模的网络，计算量也会变得非常棘手。不过有许多Python库可以用来提高运算速度，包括PyTorch、Theano、TensorFlow和Lasagne等。本书中的例子使用Keras。</p> 
<p>Keras是一个高级封装器，封装了面向Python的API。API接口可以与3个不同的后端库相兼容：Theano、谷歌的TensorFlow和微软的CNTK。这几个库都在底层实现了基本的神经网络单元和高度优化的线性代数库，可以用于处理点积，以支持高效的神经网络矩阵乘法运算。</p> 
<p>我们以简单的异或问题为例，看看如何用Keras来训练这个网络。</p> 
<pre><code class="prism language-go"><span class="token keyword">import</span> numpy as np
from keras<span class="token punctuation">.</span>models <span class="token keyword">import</span> Sequential　　# Kera的基础模型类
from keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Dense<span class="token punctuation">,</span> Activation　　# Dense是神经元的全连接层
from keras<span class="token punctuation">.</span>optimizers <span class="token keyword">import</span> SGD　　# 随机梯度下降，Keras中还有一些其他优化器
# Our examples <span class="token keyword">for</span> an exclusive OR<span class="token punctuation">.</span>
x_train <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token function">array</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span> 　　# x_train是二维特征向量表示的训练样本列表
y_train <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token function">array</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span> 　　# y_train是每个特征向量样本对应的目标输出值
model <span class="token operator">=</span> <span class="token function">Sequential</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
num_neurons <span class="token operator">=</span> <span class="token number">10</span>　　#　全连接隐藏层包含<span class="token number">10</span>个神经元
model<span class="token punctuation">.</span><span class="token function">add</span><span class="token punctuation">(</span><span class="token function">Dense</span><span class="token punctuation">(</span>num_neurons<span class="token punctuation">,</span> input_dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> 　　#　 input_dim仅在第一层中使用，后面的其他层会自动计算前一层输出的形状，这个例子中输入的XOR样本是二维特征向量，因此input_dim设置为<span class="token number">2</span>
model<span class="token punctuation">.</span><span class="token function">add</span><span class="token punctuation">(</span><span class="token function">Activation</span><span class="token punctuation">(</span><span class="token char">'tanh'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span><span class="token function">add</span><span class="token punctuation">(</span><span class="token function">Dense</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> 　　#　输出层包含一个神经元，输出结果是二分类值（<span class="token number">0</span>或<span class="token number">1</span>）
model<span class="token punctuation">.</span><span class="token function">add</span><span class="token punctuation">(</span><span class="token function">Activation</span><span class="token punctuation">(</span><span class="token char">'sigmoid'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span><span class="token function">summary</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>可以看到模型的结构为：</p> 
<pre><code class="prism language-go">Layer <span class="token punctuation">(</span><span class="token keyword">type</span><span class="token punctuation">)</span>                 Output Shape                Param 
<span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span>
dense_18 <span class="token punctuation">(</span>Dense<span class="token punctuation">)</span>             <span class="token punctuation">(</span>None<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>                  <span class="token number">30</span>
_________________________________________________________________
activation_6 <span class="token punctuation">(</span>Activation<span class="token punctuation">)</span>    <span class="token punctuation">(</span>None<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>                  <span class="token number">0</span>
_________________________________________________________________
dense_19 <span class="token punctuation">(</span>Dense<span class="token punctuation">)</span>             <span class="token punctuation">(</span>None<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>                   <span class="token number">11</span>
_________________________________________________________________
activation_7 <span class="token punctuation">(</span>Activation<span class="token punctuation">)</span>    <span class="token punctuation">(</span>None<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>                   <span class="token number">0</span>
<span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span>
Total params<span class="token punctuation">:</span> <span class="token number">41.0</span>
Trainable params<span class="token punctuation">:</span> <span class="token number">41.0</span>
Non<span class="token operator">-</span>trainable params<span class="token punctuation">:</span> <span class="token number">0.0</span>
</code></pre> 
<p><code>model.summary()</code>提供了网络参数及各阶段权重数（<code>Param \#</code>）的概览。我们可以快速计算一下：10个神经元，每个神经元有3个权重，其中有两个是输入向量的权重（输入向量中的每个值对应一个权重），还有一个是偏置对应的权重，所以一共有30个权重需要学习。输出层中有10个权重，分别与第一层的10个神经元一一对应，再加上1个偏置权重，所以该层共有11个权重。</p> 
<p>下面的代码可能有点儿不容易理解：</p> 
<pre><code class="prism language-go">sgd <span class="token operator">=</span> <span class="token function">SGD</span><span class="token punctuation">(</span>lr<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span><span class="token function">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span>'binary_crossentropy'<span class="token punctuation">,</span> optimizer<span class="token operator">=</span>sgd<span class="token punctuation">,</span> metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token char">'accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<p>SGD是之前导入的随机梯度下降<strong>优化器</strong>，模型用它来最小化误差或者<strong>损失</strong>。<code>lr</code>是学习速率，与每个权重的误差的导数结合使用，数值越大模型的学习速度越快，但可能会使模型无法找到全局极小值，数值越小越精确，但会增加训练时间，并使模型更容易陷入局部极小值。损失函数本身也定义为一个参数，在这里用的是<code>binary_crossentropy</code>。<code>metrics</code>参数是训练过程中输出流的选项列表。用<code>compile</code>方法进行编译，此时还未开始训练模型，只对权重进行了初始化，大家也可以尝试一下用这个随机初始状态来预测，当然得到的结果只是随机猜测：</p> 
<pre><code class="prism language-go">model<span class="token punctuation">.</span><span class="token function">predict</span><span class="token punctuation">(</span>x_train<span class="token punctuation">)</span>
<span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.5</span>       <span class="token punctuation">]</span><span class="token punctuation">[</span> <span class="token number">0.43494844</span><span class="token punctuation">]</span><span class="token punctuation">[</span> <span class="token number">0.50295198</span><span class="token punctuation">]</span><span class="token punctuation">[</span> <span class="token number">0.42517585</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
</code></pre> 
<p><code>predict</code>方法将给出最后一层的原始输出，在这个例子中是由<code>sigmoid</code>函数生成的。</p> 
<p>之后再没什么好写的了，但是这里还没有关于答案的任何知识，它只是对输入使用了随机权重。接下来可以试着进行训练。</p> 
<pre><code class="prism language-go">model<span class="token punctuation">.</span><span class="token function">fit</span><span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span> 　　#　从这里开始训练模型
Epoch <span class="token number">1</span><span class="token operator">/</span><span class="token number">100</span>
<span class="token number">4</span><span class="token operator">/</span><span class="token number">4</span> <span class="token punctuation">[</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token punctuation">]</span> <span class="token operator">-</span> 0s <span class="token operator">-</span> loss<span class="token punctuation">:</span> <span class="token number">0.6917</span> <span class="token operator">-</span> acc<span class="token punctuation">:</span> <span class="token number">0.7500</span>
Epoch <span class="token number">2</span><span class="token operator">/</span><span class="token number">100</span>
<span class="token number">4</span><span class="token operator">/</span><span class="token number">4</span> <span class="token punctuation">[</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token punctuation">]</span> <span class="token operator">-</span> 0s <span class="token operator">-</span> loss<span class="token punctuation">:</span> <span class="token number">0.6911</span> <span class="token operator">-</span> acc<span class="token punctuation">:</span> <span class="token number">0.5000</span>
Epoch <span class="token number">3</span><span class="token operator">/</span><span class="token number">100</span>
<span class="token number">4</span><span class="token operator">/</span><span class="token number">4</span> <span class="token punctuation">[</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token punctuation">]</span> <span class="token operator">-</span> 0s <span class="token operator">-</span> loss<span class="token punctuation">:</span> <span class="token number">0.6906</span> <span class="token operator">-</span> acc<span class="token punctuation">:</span> <span class="token number">0.5000</span>
<span class="token operator">...</span>
Epoch <span class="token number">100</span><span class="token operator">/</span><span class="token number">100</span>
<span class="token number">4</span><span class="token operator">/</span><span class="token number">4</span> <span class="token punctuation">[</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token punctuation">]</span> <span class="token operator">-</span> 0s <span class="token operator">-</span> loss<span class="token punctuation">:</span> <span class="token number">0.6661</span> <span class="token operator">-</span> acc<span class="token punctuation">:</span> <span class="token number">1.0000</span>
</code></pre> 
<p><strong>提示</strong></p> 
<p>在第一次训练时网络可能不会收敛。第一次编译可能以随机分布的参数结束，导致难以或者不能得到全局极小值。如果遇到这种情况，可以用相同的参数再次调用<code>model.fit</code>，或者添加更多训练周期，看看网络能否收敛。或者也可以用不同的随机起始点来重新初始化网络，然后再次尝试<code>fit</code>。如果使用后面这种方法，请确保没有设置随机种子，否则只会不断重复同样的实验结果。</p> 
<p>当网络一遍又一遍地学习这个小数据集时，它终于弄明白了这是怎么回事。它从样本中“学会”了什么是异或！这就是神经网络的神奇之处。</p> 
<pre><code class="prism language-go">model<span class="token punctuation">.</span><span class="token function">predict_classes</span><span class="token punctuation">(</span>x_train<span class="token punctuation">)</span>
<span class="token number">4</span><span class="token operator">/</span><span class="token number">4</span> <span class="token punctuation">[</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token punctuation">]</span> <span class="token operator">-</span> 0s
<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
model<span class="token punctuation">.</span><span class="token function">predict</span><span class="token punctuation">(</span>x_train<span class="token punctuation">)</span>
<span class="token number">4</span><span class="token operator">/</span><span class="token number">4</span> <span class="token punctuation">[</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token punctuation">]</span> <span class="token operator">-</span> 0s
<span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.0035659</span> <span class="token punctuation">]</span><span class="token punctuation">[</span> <span class="token number">0.99123639</span><span class="token punctuation">]</span><span class="token punctuation">[</span> <span class="token number">0.99285167</span><span class="token punctuation">]</span><span class="token punctuation">[</span> <span class="token number">0.00907462</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
</code></pre> 
<p>在这个经过训练的模型上再次调用<code>predict</code>（和<code>predict_classes</code>）会产生更好的结果。它在这个小数据集上获得了 100%的精确度。当然，精确率并不是评估预测模型的最佳标准，但对这个小例子来说完全可以说明问题。接下来展示了如何保存这个异或模型:</p> 
<pre><code class="prism language-go"><span class="token keyword">import</span> h5py
model_structure <span class="token operator">=</span> model<span class="token punctuation">.</span><span class="token function">to_json</span><span class="token punctuation">(</span><span class="token punctuation">)</span>　　#　用Keras的辅助方法将网络结构导出为JSON blob类型以备后用with <span class="token function">open</span><span class="token punctuation">(</span><span class="token string">"basic_model.json"</span><span class="token punctuation">,</span> <span class="token string">"w"</span><span class="token punctuation">)</span> as json_file<span class="token punctuation">:</span>json_file<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span>model_structure<span class="token punctuation">)</span>model<span class="token punctuation">.</span><span class="token function">save_weights</span><span class="token punctuation">(</span><span class="token string">"basic_weights.h5"</span><span class="token punctuation">)</span>　　#　训练好的权重必须被单独保存。第一部分只保存网络结构。在后面重新加载网络结构时必须对其重新实例化
</code></pre> 
<p>同样也有对应的方法来重新实例化模型，这样做预测时不必再去重新训练模型。虽然运行这个模型只需要几秒，但是在后面的章节中，模型的运行时间将会快速增长到以分钟、小时甚至天为单位，这取决于硬件性能和模型的复杂度，所以请准备好！</p> 
<h2><a id="Python_153"></a>Python入门全套学习资料附带源码：</h2> 
<h3><a id="_155"></a>学习编程前准备</h3> 
<p><img src="https://images2.imgbox.com/8e/ba/RKcHN4V7_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_164"></a>全套软件安装包</h3> 
<p>附带完整的安装包的安装视频教程资源（新手大礼包已备好）</p> 
<p><img src="https://images2.imgbox.com/40/16/YvZ72nqt_o.png" alt="img"></p> 
<h3><a id="_175"></a>整套零基础入门视频+课件笔记</h3> 
<p><img src="https://images2.imgbox.com/70/09/gOtbE30x_o.png" alt="img"></p> 
<h3><a id="_181"></a>👉实战案例👈</h3> 
<p>光学理论是没用的，要学会跟着一起敲，要动手实操，才能将自己的所学运用到实际当中去，这时候可以搞点实战案例来学习。<br> <img src="https://images2.imgbox.com/5f/13/fkyhl8PZ_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="100Python_187"></a>👉100道Python练习题👈</h3> 
<p>检查学习结果。<img src="https://images2.imgbox.com/cf/00/gqKmGbGu_o.png" alt=""></p> 
<h3><a id="_192"></a>👉面试刷题👈</h3> 
<p><img src="https://images2.imgbox.com/3c/d2/2KLaxsA3_o.png" alt=""></p> 
<p><img src="https://images2.imgbox.com/5a/4b/eGSwXYBb_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_199"></a>资料领取</h2> 
<p><mark>上述这份完整版的Python全套学习资料已经上传网盘，朋友们如果需要可以微信扫描下方二维码输入“领取资料” 即可自动领取</mark> <font color="red" size="3"><br> <strong>或者</strong></font><br> 【<a href="https://mp.weixin.qq.com/s/DQJlHPMjozDTnzuyoXJ6qA" rel="nofollow">点此链接</a>】领取</p> 
<img src="https://images2.imgbox.com/b1/b2/1cB4i81j_o.png"> 
<h3><a id="2_215"></a>2、学习方法</h3> 
<p>学习python，我觉得一定要快，用最快的时间快速入门，千万不要学困难的东西，先挑简单的来，越快越好，以免打击了你学习的积极性。</p> 
<p>找到一个切入点，比如爬虫就是一个十分有趣的切入点，学会了爬虫你就可以从网站上“偷数据”，还可以把这些数据拿来卖钱，你说好不好。</p> 
<h2><a id="_222"></a>总结</h2> 
<p>好啦，这就是今天的内容，入门知识点资料免费发送的哈，想要的小伙伴儿不要错过，带你直</p> 
<p>接弯道超车，少走一大波弯路，准备好了嘛？！我们要开始学习一项编程技术啦！</p> 
<p><img src="https://images2.imgbox.com/53/31/RhcC2VJx_o.png" alt="img"></p> 
<h2><a id="_235"></a>好文推荐</h2> 
<p><strong>了解python的前景：</strong><a href="https://blog.csdn.net/tingting11232/article/details/128565138">https://blog.csdn.net/weixin_49895216/article/details/127186741</a></p> 
<p><strong>了解python的兼职：</strong><a href="https://blog.csdn.net/tingting11232/article/details/128578996">https://blog.csdn.net/weixin_49895216/article/details/127124870</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f077bf7b3f5df2367c7c417916ba7962/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">ubuntu桥接方式上网</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ab6d4dd380cb5d2c87bb817f3d13a730/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">windows配置使用supervisor</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>