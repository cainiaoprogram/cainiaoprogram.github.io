<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>机器学习算法之_逻辑斯蒂回归（Logistics） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="机器学习算法之_逻辑斯蒂回归（Logistics）" />
<meta property="og:description" content="逻辑斯蒂回归虽然名字叫回归，但是其实分类， 而且这个分类很强大（以后工作是必须要用的（分类问题））。逻辑回归假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。
【关键词】Logistics函数，最大似然估计，梯度下降法 一.原理 利用Logistics回归进行分类的主要思想是：根据现有数据对分类边界线建立回归公式，以此进行分类。这里的“回归” 一词源于最佳拟合，表示要找到最佳拟合参数集。训练分类器时的做法就是寻找最佳拟合参数，使用的是最优化算法。接下来介绍这个二值型输出分类器的数学原理
1.实现步骤 Logistic Regression和Linear Regression的原理是相似的，可以简单的描述为这样的过程：
（1）找一个合适的预测函数，一般表示为h函数，该函数就是我们需要找的分类函数，它用来预测输入数据的判断结果。这个过程是非常关键的，需要对数据有一定的了解或分析，知道或者猜测预测函数的“大概”形式，比如是线性函数还是非线性函数。
（2）构造一个Cost函数（损失函数），该函数表示预测的输出（h）与训练数据类别（y）之间的偏差，可以是二者之间的差（h-y）或者是其他的形式。综合考虑所有训练数据的“损失”，将Cost求和或者求平均，记为J(θ)函数，表示所有训练数据预测值与实际类别的偏差。
（3）显然，J(θ)函数的值越小表示预测函数越准确（即h函数越准确），所以这一步需要做的是找到J(θ)函数的最小值。找函数的最小值有不同的方法，Logistic Regression实现时有梯度下降法（Gradient Descent）。
二、构造函数 1 构造预测函数
logistic Regression虽然名字里带“回归”，但是它实际上是一种分类方法，用于两分类问题（即输出只有两种）。首先需要先找到一个预测函数（h），显然，该函数的输出必须是两类值（分别代表两个类别），所以利用了Logistic函数（或称为Sigmoid函数），函数形式为
2构造损失函数
Cost函数和J(θ)函数是基于最大似然估计推导得到的。
最大似然估计就是要求得使l(θ)取最大值时的θ，其实这里可以使用梯度上升法求解，求得的θ就是要求的最佳参数
3梯度下降法求J(θ)的最小值
求J(θ)的最小值可以使用梯度下降法，根据梯度下降法可得θ的更新过程:
三、逻辑斯蒂回归算法的优缺点及常见问题 1、优点 形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大；
模型效果不错。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度；
训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型；
资源占用小,尤其是内存。因为只需要存储各个维度的特征值；
方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。
2、缺点 准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布；
很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好；
处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题；
逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。
3、常见问题 （1）逻辑回归的损失函数为什么要使用极大似然函数作为损失函数？
损失函数一般有四种，平方损失函数，对数损失函数，HingeLoss0-1损失函数，绝对值损失函数。将极大似然函数取对数以后等同于对数损失函数。在逻辑回归这个模型下，对数损失函数的训练求解参数的速度是比较快的。 因为目标是要让预测为正的的概率最大，且预测为负的概率也最大，即每一个样本预测都要得到最大的概率，将所有的样本预测后的概率进行相乘都最大，这就能到似然函数了。
（2）逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？
如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。
（3）为什么我们还是会在训练的过程当中将高度相关的特征去掉？
去掉高度相关的特征会让模型的可解释性更好，可以大大提高训练的速度。如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练的速度。其次是特征多了，本身就会增大训练的时间。
（4）使用L1L2正则化，为什么可以降低模型的复杂度？
模型越复杂，越容易过拟合，这大家都知道，加上L1正则化给了模型的拉普拉斯先验，加上L2正则化给了模型的高斯先验。从参数的角度来看，L1得到稀疏解，去掉一部分特征降低模型复杂度。L2得到较小的参数，如果参数很大，样本稍微变动一点，值就有很大偏差，这当然不是我们想看到的，相当于降低每个特征的权重。
（5）为什么L1能得到稀疏解呢？
L1正则化是L1范数而来，投到坐标图里面，是棱型的，最优解在坐标轴上取到，所以某些部分的特征的系数就为0。
（6）L1正则化不可导，怎么求解？
坐标轴下降法（按照每个坐标轴一个个使其收敛），最小角回归（是一个逐步的过程，每一步都选择一个相关性很大的特征，总的运算步数只和特征的数目有关，和训练集的大小无关）
四、实战 sklearn.linear_model.LogisticRegression(penalty=‘l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=‘liblinear’, max_iter=100, multi_class=‘ovr’, verbose=0, warm_start=False, n_jobs=1)
solver参数的选择：
“liblinear”：小数量级的数据集 “lbfgs”," />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/7fa7eed8cde2081fb8f8306687900917/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-01-14T16:21:52+08:00" />
<meta property="article:modified_time" content="2020-01-14T16:21:52+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">机器学习算法之_逻辑斯蒂回归（Logistics）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-github-gist">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>逻辑斯蒂回归虽然名字叫回归，但是其实分类， 而且这个分类很强大（以后工作是必须要用的（分类问题））。<strong>逻辑回归假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。</strong></p> 
<h6><a id="Logistics_1"></a>【关键词】Logistics函数，最大似然估计，梯度下降法</h6> 
<h4><a id="_2"></a>一.原理</h4> 
<p>利用Logistics回归进行分类的主要思想是：根据现有数据对分类边界线建立回归公式，以此进行分类。这里的“回归” 一词源于最佳拟合，表示要找到最佳拟合参数集。训练分类器时的做法就是寻找最佳拟合参数，使用的是最优化算法。接下来介绍这个二值型输出分类器的数学原理</p> 
<h6><a id="1_4"></a>1.实现步骤</h6> 
<p>Logistic Regression和Linear Regression的原理是相似的，可以简单的描述为这样的过程：</p> 
<blockquote> 
 <p>（1）找一个合适的预测函数，一般表示为h函数，该函数就是我们需要找的分类函数，它用来预测输入数据的判断结果。这个过程是非常关键的，需要对数据有一定的了解或分析，知道或者猜测预测函数的“大概”形式，比如是线性函数还是非线性函数。</p> 
 <p>（2）构造一个Cost函数（损失函数），该函数表示预测的输出（h）与训练数据类别（y）之间的偏差，可以是二者之间的差（h-y）或者是其他的形式。综合考虑所有训练数据的“损失”，将Cost求和或者求平均，记为J(θ)函数，表示所有训练数据预测值与实际类别的偏差。</p> 
 <p>（3）显然，J(θ)函数的值越小表示预测函数越准确（即h函数越准确），所以这一步需要做的是找到J(θ)函数的最小值。找函数的最小值有不同的方法，Logistic Regression实现时有梯度下降法（Gradient Descent）。</p> 
</blockquote> 
<h4><a id="_13"></a>二、构造函数</h4> 
<p>1 构造预测函数<br> logistic Regression虽然名字里带“回归”，但是它实际上是一种分类方法，用于两分类问题（即输出只有两种）。首先需要先找到一个预测函数（h），显然，该函数的输出必须是两类值（分别代表两个类别），所以利用了Logistic函数（或称为Sigmoid函数），函数形式为<br> 2构造损失函数<br> Cost函数和J(θ)函数是基于最大似然估计推导得到的。<br> 最大似然估计就是要求得使l(θ)取最大值时的θ，其实这里可以使用梯度上升法求解，求得的θ就是要求的最佳参数<br> 3梯度下降法求J(θ)的最小值<br> 求J(θ)的最小值可以使用梯度下降法，根据梯度下降法可得θ的更新过程:</p> 
<h4><a id="_22"></a>三、逻辑斯蒂回归算法的优缺点及常见问题</h4> 
<h6><a id="1_23"></a>1、优点</h6> 
<blockquote> 
 <ol><li> <p>形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大；</p> </li><li> <p>模型效果不错。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度；</p> </li><li> <p>训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型；</p> </li><li> <p>资源占用小,尤其是内存。因为只需要存储各个维度的特征值；</p> </li><li> <p>方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。</p> </li></ol> 
</blockquote> 
<h6><a id="2_35"></a>2、缺点</h6> 
<blockquote> 
 <ol><li> <p>准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布；</p> </li><li> <p>很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好；</p> </li><li> <p>处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题；</p> </li><li> <p>逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。</p> </li></ol> 
</blockquote> 
<h6><a id="3_45"></a>3、常见问题</h6> 
<p><strong>（1）逻辑回归的损失函数为什么要使用极大似然函数作为损失函数？</strong></p> 
<p></p> 
<blockquote> 
 <p>损失函数一般有四种，平方损失函数，对数损失函数，HingeLoss0-1损失函数，绝对值损失函数。将极大似然函数取对数以后等同于对数损失函数。在逻辑回归这个模型下，对数损失函数的训练求解参数的速度是比较快的。   <br> 因为目标是要让预测为正的的概率最大，且预测为负的概率也最大，即每一个样本预测都要得到最大的概率，将所有的样本预测后的概率进行相乘都最大，这就能到似然函数了。</p> 
</blockquote> 
<p>（2）<strong>逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？</strong></p> 
<p></p> 
<blockquote> 
 <p>如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。</p> 
</blockquote> 
<p><strong>（3）为什么我们还是会在训练的过程当中将高度相关的特征去掉？</strong></p> 
<blockquote> 
 <p>去掉高度相关的特征会让模型的可解释性更好，可以大大提高训练的速度。如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练的速度。其次是特征多了，本身就会增大训练的时间。</p> 
</blockquote> 
<p><strong>（4）使用L1L2正则化，为什么可以降低模型的复杂度？</strong></p> 
<blockquote> 
 <p>模型越复杂，越容易过拟合，这大家都知道，加上L1正则化给了模型的拉普拉斯先验，加上L2正则化给了模型的高斯先验。从参数的角度来看，L1得到稀疏解，去掉一部分特征降低模型复杂度。L2得到较小的参数，如果参数很大，样本稍微变动一点，值就有很大偏差，这当然不是我们想看到的，相当于降低每个特征的权重。</p> 
</blockquote> 
<p><strong>（5）为什么L1能得到稀疏解呢？</strong></p> 
<blockquote> 
 <p>L1正则化是L1范数而来，投到坐标图里面，是棱型的，最优解在坐标轴上取到，所以某些部分的特征的系数就为0。</p> 
</blockquote> 
<p><strong>（6）L1正则化不可导，怎么求解？</strong></p> 
<blockquote> 
 <p>坐标轴下降法（按照每个坐标轴一个个使其收敛），最小角回归（是一个逐步的过程，每一步都选择一个相关性很大的特征，总的运算步数只和特征的数目有关，和训练集的大小无关）</p> 
</blockquote> 
<h4><a id="_76"></a>四、实战</h4> 
<p>sklearn.linear_model.LogisticRegression(penalty=‘l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=‘liblinear’, max_iter=100, multi_class=‘ovr’, verbose=0, warm_start=False, n_jobs=1)</p> 
<p>solver参数的选择：</p> 
<blockquote> 
 <p>“liblinear”：小数量级的数据集 “lbfgs”,<br> “sag” or “newton-cg”：大数量级的数据集以及多分类问题<br> “sag”：极大的数据集</p> 
</blockquote> 
<h5><a id="1_85"></a>1.手写数字数据集的分类</h5> 
<p>使用KNN与Logistic回归两种方法</p> 
<pre><code class="prism language-python"><span class="token comment"># 导包</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>neighbors <span class="token keyword">import</span> KNeighborsClassifier
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LogisticRegression
<span class="token keyword">import</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">as</span> datasets
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token operator">%</span>matplotlib inline
</code></pre> 
<pre><code class="prism language-python"><span class="token comment">#导入数据load_digits()</span>
digits <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_digits<span class="token punctuation">(</span><span class="token punctuation">)</span>
digits
data <span class="token operator">=</span> digits<span class="token punctuation">.</span>data
data<span class="token punctuation">.</span>shape

images <span class="token operator">=</span> digits<span class="token punctuation">.</span>images
images<span class="token punctuation">.</span>shape

plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>images<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> cmap <span class="token operator">=</span> <span class="token string">"gray"</span><span class="token punctuation">)</span>
data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape

plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cmap <span class="token operator">=</span> <span class="token string">"gray"</span><span class="token punctuation">)</span>


target <span class="token operator">=</span> digits<span class="token punctuation">.</span>target

<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split
X_train<span class="token punctuation">,</span> X_test<span class="token punctuation">,</span>y_train<span class="token punctuation">,</span>y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>data<span class="token punctuation">,</span> target<span class="token punctuation">,</span> test_size <span class="token operator">=</span> <span class="token number">0.02</span><span class="token punctuation">)</span>

<span class="token comment">#创建模型，训练和预测</span>
knn <span class="token operator">=</span> KNeighborsClassifier<span class="token punctuation">(</span><span class="token punctuation">)</span>
knn<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span>
knn<span class="token punctuation">.</span>score<span class="token punctuation">(</span>X_test<span class="token punctuation">,</span>y_test<span class="token punctuation">)</span>

logic <span class="token operator">=</span> LogisticRegression<span class="token punctuation">(</span><span class="token punctuation">)</span>
logic<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span>
logic<span class="token punctuation">.</span>score<span class="token punctuation">(</span>X_test<span class="token punctuation">,</span>y_test<span class="token punctuation">)</span>
</code></pre> 
<h5><a id="2make_blobs_128"></a>2.使用make_blobs产生数据集进行分类</h5> 
<p>导包使用datasets.make_blobs创建一系列点</p> 
<pre><code class="prism language-python"><span class="token comment">#make_blobs是专门画三簇点的</span>
data<span class="token punctuation">,</span> target <span class="token operator">=</span> datasets<span class="token punctuation">.</span>make_blobs<span class="token punctuation">(</span>n_samples<span class="token operator">=</span><span class="token number">150</span><span class="token punctuation">)</span>
data<span class="token punctuation">.</span>shape
target
plt<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> c <span class="token operator">=</span> target<span class="token punctuation">)</span>

<span class="token comment">#创建机器学习模型，训练数据</span>
logic <span class="token operator">=</span> LogisticRegression<span class="token punctuation">(</span><span class="token punctuation">)</span>
logic<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>data<span class="token punctuation">,</span> target<span class="token punctuation">)</span>

<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token comment">#预测数据</span>
xmin<span class="token punctuation">,</span> xmax <span class="token operator">=</span>  data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
ymin<span class="token punctuation">,</span> ymax <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
x <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span>xmin<span class="token punctuation">,</span> xmax<span class="token punctuation">,</span> <span class="token number">500</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span>ymin<span class="token punctuation">,</span> ymax<span class="token punctuation">,</span> <span class="token number">300</span><span class="token punctuation">)</span>
xx<span class="token punctuation">,</span> yy <span class="token operator">=</span> np<span class="token punctuation">.</span>meshgrid<span class="token punctuation">(</span>x<span class="token punctuation">,</span>y<span class="token punctuation">)</span>
x_test <span class="token operator">=</span> np<span class="token punctuation">.</span>c_<span class="token punctuation">[</span>xx<span class="token punctuation">.</span>ravel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> yy<span class="token punctuation">.</span>ravel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
x_test<span class="token punctuation">.</span>shape

<span class="token comment">#预测坐标点数据，并进行reshape()</span>

<span class="token comment">#对数据进行预测</span>
y_<span class="token operator">=</span>  logic<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span>


<span class="token comment">#绘制图形</span>
xx<span class="token punctuation">.</span>shape
y_<span class="token punctuation">.</span>shape

plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment">#plt.scatter(xx, x_test[:,1], c = y_)</span>
<span class="token comment">#对坐标点进行着色</span>
plt<span class="token punctuation">.</span>pcolormesh<span class="token punctuation">(</span>xx<span class="token punctuation">,</span> yy<span class="token punctuation">,</span>y_<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>xx<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> c <span class="token operator">=</span> target<span class="token punctuation">,</span> cmap <span class="token operator">=</span> <span class="token string">"rainbow"</span><span class="token punctuation">)</span>
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/67f51ba9f79d555bb41f469c747a843b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">python使用opencv实现图像的重映射</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6ca36554b9a32a6f42a28edbc99d6df7/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">使可执行文件以管理员权限运行</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>