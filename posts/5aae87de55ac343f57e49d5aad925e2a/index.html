<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>神经网络骨架network backbones - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="神经网络骨架network backbones" />
<meta property="og:description" content="本文将列出神经网络几种骨架结构的历史沿袭，对从框架上了解backbones有重要帮助。阅读时间约10分钟。更多的机器视觉文献回顾可参阅：
https://github.com/senbinyu/Computer_Vision_Literatures
network backbones是神经网络最重要的体系结构。
1. Review papers 以下列出一些综述文章，推荐阅读第二篇
Neena Aloysius and Geetha M, A Review on Deep Convolutional Neural Networks, 2017 A bit old and not include many state-of-art research, refer to paper A Review on Deep Convolutional Neural Networks
Elhassouny, Azeddine et al, Trends in deep convolutional neural Networks architectures: a review, 2019. Recommand. Reviewed most of the CNN backbones, refer to paper Trends in deep convolutional neural Networks architectures: a review" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/5aae87de55ac343f57e49d5aad925e2a/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-08-14T04:38:54+08:00" />
<meta property="article:modified_time" content="2020-08-14T04:38:54+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">神经网络骨架network backbones</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>本文将列出神经网络几种骨架结构的历史沿袭，对从框架上了解backbones有重要帮助。阅读时间约10分钟。更多的机器视觉文献回顾可参阅：<br> https://github.com/senbinyu/Computer_Vision_Literatures<br> network backbones是神经网络最重要的体系结构。</p> 
<h3><a id="1_Review_papers_4"></a>1. Review papers</h3> 
<p>以下列出一些综述文章，推荐阅读第二篇</p> 
<ul><li>Neena Aloysius and Geetha M, A Review on Deep Convolutional Neural Networks, 2017</li></ul> 
<p>A bit old and not include many state-of-art research, refer to paper <a href="https://ieeexplore.ieee.org/abstract/document/8286426/" rel="nofollow">A Review on Deep Convolutional Neural Networks</a></p> 
<ul><li>Elhassouny, Azeddine et al, Trends in deep convolutional neural Networks architectures: a review, 2019.</li></ul> 
<p><strong>Recommand</strong>. Reviewed most of the CNN backbones, refer to paper <a href="https://ieeexplore.ieee.org/abstract/document/8807741?casa_token=NzJx5O4redQAAAAA:OhEtsKL2x8ryRkb21GgeTu9glwuesljMYIWUeYMt7dPyY2vOhrJk8kO0Qh1lMgjRNiC7T2OCaQ" rel="nofollow">Trends in deep convolutional neural Networks architectures: a review</a></p> 
<h3><a id="2_History_15"></a>2. History</h3> 
<p>下图显示了神经网络开发的时间表<br> <img src="https://images2.imgbox.com/70/0f/SMht12m9_o.jpg" alt="在这里插入图片描述"></p> 
<h4><a id="_Lenet_Yann_LeCun_et_al_1994_21"></a>- Lenet, Yann LeCun et al.， 1994</h4> 
<p>LeNet-5, 是一个简单但富有创造力的网络，最初用于手写识别。please refer to http://yann.lecun.com/exdb/lenet/<br> <img src="https://images2.imgbox.com/60/de/VMrpbZp4_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="_AlexNet_Alex_Krizhevsky_et_al_2012_26"></a>- AlexNet, Alex Krizhevsky et al.， 2012</h4> 
<p>在2012年9月30日赢得了ImageNet大规模视觉识别挑战赛。该网络的前五名错误率为15.3％，比第二名的错误率低10.8个百分点。 主要发现是神经网络的深度对于检测的高性能至关重要。并且伴随着GPU的逐步广泛使用，使用深度大的网络成为现实。</p> 
<p>卷积核大点, 11 * 11, 7 * 7, 5 * 5 etc., see from the figure<br> refer to paper [ImageNet Classification with Deep Convolutional Neural Networks] (https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)</p> 
<p><img src="https://images2.imgbox.com/0b/b1/PpKY6aVj_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="_VGGNet_Karen_Simonyan_Andrew_Zisserman_34"></a>- VGGNet, Karen Simonyan, Andrew Zisserman</h4> 
<p>2014 Imagenet 第二名.</p> 
<ol><li> <p>验证增加净深度可以有效地改善性能。 但这带来了一个问题：大量的参数。</p> </li><li> <p>减小卷积核尺寸，两个3 * 3代替5 * 5，减小参数量<br> 证明网络深度增加有助于检测，但引入更多的参数；于是发现了使用小卷积核能达到和使用大卷积核同样的目的，同时还能减少参数</p> </li></ol> 
<p>refer to paper <a href="https://arxiv.org/abs/1409.1556" rel="nofollow">Imagenet classification with deep convolutional neural networks</a><br> <img src="https://images2.imgbox.com/84/f8/LVtkoBfR_o.png" alt="VGG"></p> 
<h4><a id="GoogLeNet_Christian_Szegedy_Wei_Liu_et_al_45"></a>GoogLeNet, Christian Szegedy, Wei Liu et al.</h4> 
<ul><li>v1, 2014 ImageNet 第一名.</li></ul> 
<ol><li> <p>除了增加网络深度之外（对于googlenet v1为22层），还会增加网络的宽度</p> </li><li> <p>引入较小的内核，1 * 1卷积，减小尺寸并保存参数。 参数：AlexNet ~ 12 GoogLeNet, VGG ~ 3 AlexNet</p> </li><li> <p>初始模块很容易添加或删除，它是模仿人的大脑以建立稀疏连接。</p> </li></ol> 
<p>Refer to <a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/43022.pdf" rel="nofollow">Going deeper with convolutions</a></p> 
<p><img src="https://images2.imgbox.com/e9/5a/ewew2njv_o.png" alt="在这里插入图片描述"></p> 
<ul><li>v2, Christian Szegedy et al. v2, v3 在同一篇文章.</li></ul> 
<ol><li> <p>分解卷积，使用1 * n和n * 1替换3 * 3，如下图所示。 从理论上讲，当特征图很大（n很大）时，它可以大大节省计算成本。 但实际上，它不能在早期阶段很好地工作，n范围12-20似乎是一个合理的数字。</p> </li><li> <p>有效减少网格尺寸。 并行使用池化层（下图中的步幅2）和起始层（与步幅2的卷积等）。</p> </li><li> <p>为有效网络的设计提出一些建议：避免代表性瓶颈，尤其是在网络早期。 高维表示更易于在网络内本地处理； 可以在较低维的嵌入上进行空间聚合，而不会损失很多表示能力（例如RGB图像到灰色）； 平衡网络的宽度和深度（这在最近的EffectiveNet 2019中进一步得出结论）。 早期特征尺寸不能急剧减小，避免出现瓶颈；低维特征时进行空间融合，并不会特别明显的增加损失（这感觉也像是可以进行特征融合的一个体现）</p> </li></ol> 
<p>Refer to paper <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf" rel="nofollow">Rethinking the Inception Architecture for Computer Vision</a></p> 
<p><img src="https://images2.imgbox.com/1c/a4/75SETkWB_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/4b/f6/l2c1tqt1_o.png" alt="在这里插入图片描述"></p> 
<ul><li>v3, shares the same paper with v2, minor additions.</li></ul> 
<p>通过标签平滑进行模型正则化，减少模型的过度拟合。 培训方法：RMSProp代替SGD。 进行了分辨率测试。</p> 
<p>Refer to paper <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf" rel="nofollow">Rethinking the Inception Architecture for Computer Vision</a></p> 
<ul><li>v4 (pure inception-v4), inception-resnet, 在同一篇文章. 此处主要讲inception-resnet</li></ul> 
<ol><li> <p>将Inception模块与残差模块组合在一起以创建一个新模块：inception-resnet。 它增加了净深度并提高了速度。</p> </li><li> <p>比较：inception-v3与inception-resnet-v1； inception-vnet与inception-resnet-v2具有相似的精度。</p> </li></ol> 
<p>Refer to paper <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewPaper/14806" rel="nofollow">Inception-v4, inception-resnet and the impact of residual connections on learning</a></p> 
<h4><a id="ResNet_He_Kaiming2016_85"></a>ResNet, He Kaiming，2016</h4> 
<p>在 ILSVRC and COCO 2015等5个比赛中都获得第一名.</p> 
<p>人们发现，更深的CNN对几乎所有任务都非常有用。 但是由于存在网络退化问题，因此很难训练它们。 提出了一种带有剩余模块的新架构。</p> 
<ol><li> <p>较深的网络比较浅的网络性能更差。 一个创新的想法：如果什么也没学到，那就比以前更糟：因此提出了identity map（也称为shortcut连接）。 H（x）= x + F（x），F（x）= H（x）-x，称为残差。</p> </li><li> <p>ResNet的深度各不相同，从18层到34、50、101到非常深的152层。</p> </li><li> <p>不同的快捷方式，如果不更改尺寸，则可以使用身份映射。 但是在实践中，尺寸更改因此会产生“瓶颈”。 （这在基线后记中很流行，因为它可以保存参数。）</p> </li></ol> 
<p>Refer to paper <a href="https://arxiv.org/abs/1512.03385" rel="nofollow">Deep Residual Learning for Image Recognition</a></p> 
<p><img src="https://images2.imgbox.com/c2/52/VIQb5ROI_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/4c/00/rUI3Gk3A_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="Xception_Chollet_Francois_2016_102"></a>Xception, Chollet, Francois, 2016</h4> 
<p>基于Inception-v3，如果我们不想每次都设计Inception体系结构，而只在一个模块上均匀使用相同的结构，则网络设计会更容易。 （这个想法在其他基准，例如mobileNet也得到应用。）</p> 
<ol><li> <p>extreme inception. 从同等的初始结构，现在仅计算部分渠道, (group = xx in pytorch).</p> </li><li> <p>深度可分离卷积（最初来自phd论文：Laurent Sifre，Rigid-Motion Scattering For Image Classification），大大节省了参数。 通过均匀分离特征，下图中的参数可以是m * k + 3 * 3 * k，m是特征，k是核数。<br> 更多具体的关于深度分离卷积可参见如下中关于轻量网络的描述https://github.com/senbinyu/Computer_Vision_Literatures</p> </li></ol> 
<p>Refer to paper <a href="https://arxiv.org/abs/1610.02357" rel="nofollow">Xception: Deep Learning with Depthwise Separable Convolutions</a><br> <img src="https://images2.imgbox.com/02/ba/ZPJ6v7vx_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="ResNeXt_Xie_Saining_He_Kaiming2017_114"></a>ResNeXt, Xie Saining, He Kaiming，2017</h4> 
<p>ILSVRC 2016 第二名.</p> 
<p>inception-resnet的修改。 不同于人工设计的Inception-Resnet，Resnext使用相同的分支来完成设计。</p> 
<ol><li> <p>group convolution（群组卷积）, cardinality(基数). 这是正常卷积（所有通道）和深度可分离卷积（每个通道）之间的折衷。</p> </li><li> <p>ImageNet-1K dataset 上测试，发现增加基数在一定程度上会改善表现，且比更深更宽更有效。文中实验了从1到32 groups，误差逐步降低。但需要注意的是，这是将dimensions控制在4d以上，更小的dimension下作者认为不值得再实验。</p> </li></ol> 
<p>Refer to paper <a href="https://arxiv.org/abs/1611.05431" rel="nofollow">Aggregated Residual Transformations for Deep Neural Networks</a><br> <img src="https://images2.imgbox.com/73/20/srj8aNlv_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="denseNet_Huang_Gao_Liu_Zhuang2017_126"></a>denseNet, Huang Gao, Liu Zhuang，2017</h4> 
<p>CVPR2017 best paper, 特点：特征重用, 基于Resnet.</p> 
<ol><li> <p>由于连接紧密，当前层与之前的所有层都有连接。 传统的CNN，L层，L连接，densedNet，一个密集块，L层具有L *（L + 1）/ 2个连接。</p> </li><li> <p>增长率，第l层具有k_0 + k *（l-1）个特征图，k_0是输入通道数，k是增长率（每层特征数）。</p> </li><li> <p>bottleneck layer, 作者发现此层对于密集网特别有效。 使用1 * 1转换，功能图4k（提高效率），然后使用3 * 3转换，减少回k</p> </li><li> <p>压缩时，过渡层特征图会用<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           θ 
          
         
        
          \theta 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.02778em;">θ</span></span></span></span></span>缩小。 实验也证实了这一点，热图显示，致密块与先前的过渡层的相关性较小。 说明了transition layer输出了很多冗余信息，去除一些，可以使网络轻量化，但又不至于严重影响精度。</p> </li></ol> 
<p>Refer to paper <a href="https://arxiv.org/abs/1608.06993" rel="nofollow">Densely Connected Convolutional Networks</a><br> <img src="https://images2.imgbox.com/b8/2f/MfWf1wRy_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="SENet_Hu_Jie_Li_Shen2018_140"></a>SENet, Hu Jie, Li Shen,2018</h4> 
<p>ILSVRC 2017第一名. This is an application of “attention” mechanism, more similar to human brain. 这是“注意力”机制的一种应用，更类似于人的大脑。更多可参见https://github.com/senbinyu/Computer_Vision_Literatures/blob/master/0_Backbones/1_attention_in_backbones.md</p> 
<ol><li> <p>关注<em>通道关系</em>，并使用挤压和激励块。 1 * 1 * C，将所有通道信息放在一起，然后重新缩放（S型），重要的通道功能可以起到更大的作用。</p> </li><li> <p>挤压空间全局平均值，以使用不同渠道之间的联系及相关性，而非空间分布</p> </li><li> <p>激励中，实际上使用了两个完全连接的层：第一个层，使用ReLU将通道从C压缩到C / r，第二层以S形缩放到C通道。 那么重要的人可以做出更多的贡献。 r是压缩率。 作者发现16是最佳选择。</p> </li><li> <p>SE模块也可以与ResNet和Inception一起使用。</p> </li></ol> 
<p>Refer to paper <a href="https://arxiv.org/abs/1709.01507" rel="nofollow">Squeeze-and-Excitation Networks</a><br> <img src="https://images2.imgbox.com/57/97/kjJUV2oH_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="efficientNet_Tan_Mingxing_and_Quoc_V_Le2019_153"></a>efficientNet, Tan Mingxing and Quoc V. Le，2019</h4> 
<p>有没有适用于神经网络架构设计的指导方法？ 作者提出了一些基本要点和经验公式。</p> 
<ol><li> <p>“扩大网络宽度，深度或分辨率的任何尺寸都可以提高精度，但是对于较大的模型，精度增益会降低”.<br> 单独增加某个参数只能在一定范围内获得好的结果</p> </li><li> <p>模型缩放，平衡宽度，深度和分辨率可以带来更好的性能。 高效的复合系数。 三个主要参数的系数：宽度，w，深度，d，分辨率，r； 仅当<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           d 
          
         
           ∗ 
          
          
          
            w 
           
          
            2 
           
          
         
           ∗ 
          
          
          
            r 
           
          
            2 
           
          
         
           = 
          
         
           2 
          
         
        
          d * w ^ 2 * r ^ 2 = 2 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathdefault">d</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.814108em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.814108em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em;">r</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">2</span></span></span></span></span>时，它们才能实现相对平衡的体系结构。 如果要扩展或压缩2 ^ \ phi，\ phi是扩展系数，则它变为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           ( 
          
         
           d 
          
         
           ∗ 
          
          
          
            w 
           
          
            2 
           
          
         
           ∗ 
          
          
          
            r 
           
          
            2 
           
          
          
          
            ) 
           
          
            ϕ 
           
          
         
        
          (d * w ^ 2 * r ^ 2)^ \phi 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault">d</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.814108em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1.09911em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em;">r</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">ϕ</span></span></span></span></span></span></span></span></span></span></span></span>，所有三个参数都随指数<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           ϕ 
          
         
        
          \phi 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathdefault">ϕ</span></span></span></span></span>改变。</p> </li><li> <p>相对于比较模型而言，相对较小，但效率更高且更准确。</p> </li></ol> 
<p>Refer to paper <a href="https://arxiv.org/pdf/1905.11946.pdf" rel="nofollow">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a></p> 
<p><img src="https://images2.imgbox.com/1a/44/0t5VMUAF_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/f7/ad/1bn2FcYa_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="NAS_167"></a>NAS</h4> 
<p>以上是根据人们的经验观察设计的体系结构。 有没有自动设计网络的方法？ 在2016年，麻省理工学院和Google几乎同时提出了神经网络结构搜索。 但是计算成本非常昂贵。</p> 
<ol><li> <p>搜索策略，这里使用强化学习，但还有许多其他方法，即进化算法，基于梯度的方法，增强等。</p> </li><li> <p>提速，分层表示，权重共享等</p> </li></ol> 
<p>Refer to paper <a href="https://arxiv.org/abs/1611.01578" rel="nofollow">Neural Architecture Search with Reinforcement Learning</a><br> <a href="https://arxiv.org/abs/1611.02167" rel="nofollow">Designing Neural Network Architectures using Reinforcement Learning</a><br> <img src="https://images2.imgbox.com/82/12/ZliQxQwo_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="Res2Net_Gao_Shanghua_et_al_2019_178"></a>Res2Net, Gao Shanghua et al.， 2019</h4> 
<ol><li> <p>多尺度融合。 “大多数现有方法都以分层的方式表示多尺度特征。在本文中，我们通过在单个残差块内构造分层的类似残差的连接，为CNN提出了一种新颖的构建块，即Res2Net。” 它基于bottleNeck结构，依次为1 * 1、3 * 3和1 * 1，如下图所示。</p> </li><li> <p>可以与其他骨干网，ResNet，ResNeXt等结合使用。显着提高准确性。 从ResNet-50到Res2Net-50的COCO的70％到73％。</p> </li><li> <p>作者还尝试了各种任务，例如对象检测，语义/实例分割，关键点估计，所有这些都显示出令人满意的结果。<br> Refer to paper <a href="https://ieeexplore.ieee.org/abstract/document/8821313?casa_token=87qNMjgrnBYAAAAA:31BsZutpV6YptiDHrA-AOZ9p0b0nQjR-ONrBXX2DwBlFVi4nxwXOnRKcaLrnL0h5ysatVql9Vg" rel="nofollow">Res2Net: A New Multi-scale Backbone Architecture</a><br> <img src="https://images2.imgbox.com/fe/ec/6HSgZgxG_o.png" alt="在这里插入图片描述"><br> 更多的计算机视觉文献回顾可参阅：<br> https://github.com/senbinyu/Computer_Vision_Literatures</p> </li></ol>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/7c5ea5370f2f03b228a300dc8427e8e2/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">lotus导出钱包</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/688f89bcddbb570265826d4389823339/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">idea 枚举快速_快速可枚举的枚举</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>