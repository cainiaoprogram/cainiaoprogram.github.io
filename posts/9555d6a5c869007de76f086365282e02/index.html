<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>OpenCV探索之路（二十三）：特征检测和特征匹配方法汇总 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="OpenCV探索之路（二十三）：特征检测和特征匹配方法汇总" />
<meta property="og:description" content="转载自： https://www.cnblogs.com/skyfsm/p/7401523.html
一幅图像中总存在着其独特的像素点，这些点我们可以认为就是这幅图像的特征，成为特征点。计算机视觉领域中的很重要的图像特征匹配就是一特征点为基础而进行的，所以，如何定义和找出一幅图像中的特征点就非常重要。这篇文章我总结了视觉领域最常用的几种特征点以及特征匹配的方法。
在计算机视觉领域，兴趣点（也称关键点或特征点）的概念已经得到了广泛的应用， 包括目标识别、 图像配准、 视觉跟踪、 三维重建等。 这个概念的原理是， 从图像中选取某些特征点并对图像进行局部分析，而非观察整幅图像。 只要图像中有足够多可检测的兴趣点，并且这些兴趣点各不相同且特征稳定， 能被精确地定位，上述方法就十分有效。
以下是实验用的图像：第一幅是手机抓拍的风景图，第二幅是遥感图像。
1.SURF 特征检测的视觉不变性是一个非常重要的概念。 但是要解决尺度不变性问题，难度相当大。 为解决这一问题，计算机视觉界引入了尺度不变特征的概念。 它的理念是， 不仅在任何尺度下拍摄的物体都能检测到一致的关键点，而且每个被检测的特征点都对应一个尺度因子。 理想情况下，对于两幅图像中不同尺度的的同一个物体点， 计算得到的两个尺度因子之间的比率应该等于图像尺度的比率。近几年， 人们提出了多种尺度不变特征，本节介绍其中的一种：SURF特征。 SURF全称为“加速稳健特征”（Speeded Up Robust Feature），我们将会看到，它们不仅是尺度不变特征，而且是具有较高计算效率的特征。
我们首先进行常规的特征提取和特征点匹配，看看效果如何。
#include &#34;highgui/highgui.hpp&#34; #include &#34;opencv2/nonfree/nonfree.hpp&#34; #include &#34;opencv2/legacy/legacy.hpp&#34; #include &lt;iostream&gt; using namespace cv; using namespace std; int main() { Mat image01 = imread(&#34;2.jpg&#34;, 1); //右图 Mat image02 = imread(&#34;1.jpg&#34;, 1); //左图 namedWindow(&#34;p2&#34;, 0); namedWindow(&#34;p1&#34;, 0); imshow(&#34;p2&#34;, image01); imshow(&#34;p1&#34;, image02); //灰度图转换 Mat image1, image2; cvtColor(image01, image1, CV_RGB2GRAY); cvtColor(image02, image2, CV_RGB2GRAY); //提取特征点 SurfFeatureDetector surfDetector(800); // 海塞矩阵阈值，在这里调整精度，值越大点越少，越精准 vector&lt;KeyPoint&gt; keyPoint1, keyPoint2; surfDetector." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/9555d6a5c869007de76f086365282e02/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-20T20:52:32+08:00" />
<meta property="article:modified_time" content="2023-10-20T20:52:32+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">OpenCV探索之路（二十三）：特征检测和特征匹配方法汇总</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>转载自： </p> 
<p><a href="https://www.cnblogs.com/skyfsm/p/7401523.html" rel="nofollow" title="https://www.cnblogs.com/skyfsm/p/7401523.html">https://www.cnblogs.com/skyfsm/p/7401523.html</a></p> 
<p>一幅图像中总存在着其独特的像素点，这些点我们可以认为就是这幅图像的特征，成为特征点。计算机视觉领域中的很重要的图像特征匹配就是一特征点为基础而进行的，所以，如何定义和找出一幅图像中的特征点就非常重要。这篇文章我总结了视觉领域最常用的几种特征点以及特征匹配的方法。</p> 
<p>在计算机视觉领域，兴趣点（也称关键点或特征点）的概念已经得到了广泛的应用， 包括目标识别、 图像配准、 视觉跟踪、 三维重建等。 这个概念的原理是， 从图像中选取某些特征点并对图像进行局部分析，而非观察整幅图像。 只要图像中有足够多可检测的兴趣点，并且这些兴趣点各不相同且特征稳定， 能被精确地定位，上述方法就十分有效。</p> 
<p>以下是实验用的图像：第一幅是手机抓拍的风景图，第二幅是遥感图像。</p> 
<p class="img-center"><img alt="" height="514" src="https://images2.imgbox.com/fc/8b/4tjRpO0L_o.png" width="1200"></p> 
<p></p> 
<p class="img-center"><img alt="" height="661" src="https://images2.imgbox.com/7b/c5/9qisEFcw_o.png" width="1200"></p> 
<h3 id="1surf">1.SURF</h3> 
<p>特征检测的视觉不变性是一个非常重要的概念。 但是要解决尺度不变性问题，难度相当大。 为解决这一问题，计算机视觉界引入了尺度不变特征的概念。 它的理念是， 不仅在任何尺度下拍摄的物体都能检测到一致的关键点，而且每个被检测的特征点都对应一个尺度因子。 理想情况下，对于两幅图像中不同尺度的的同一个物体点， 计算得到的两个尺度因子之间的比率应该等于图像尺度的比率。近几年， 人们提出了多种尺度不变特征，本节介绍其中的一种：SURF特征。 SURF全称为“加速稳健特征”（Speeded Up Robust Feature），我们将会看到，它们不仅是尺度不变特征，而且是具有较高计算效率的特征。</p> 
<p>我们首先进行常规的特征提取和特征点匹配，看看效果如何。</p> 
<pre><code>#include "highgui/highgui.hpp"    
#include "opencv2/nonfree/nonfree.hpp"    
#include "opencv2/legacy/legacy.hpp"   
#include &lt;iostream&gt;  

using namespace cv;
using namespace std;


int main()
{
    Mat image01 = imread("2.jpg", 1);    //右图
    Mat image02 = imread("1.jpg", 1);    //左图
    namedWindow("p2", 0);
    namedWindow("p1", 0);
    imshow("p2", image01);
    imshow("p1", image02);

    //灰度图转换  
    Mat image1, image2;
    cvtColor(image01, image1, CV_RGB2GRAY);
    cvtColor(image02, image2, CV_RGB2GRAY);


    //提取特征点    
    SurfFeatureDetector surfDetector(800);  // 海塞矩阵阈值，在这里调整精度，值越大点越少，越精准 
    vector&lt;KeyPoint&gt; keyPoint1, keyPoint2;
    surfDetector.detect(image1, keyPoint1);
    surfDetector.detect(image2, keyPoint2);

    //特征点描述，为下边的特征点匹配做准备    
    SurfDescriptorExtractor SurfDescriptor;
    Mat imageDesc1, imageDesc2;
    SurfDescriptor.compute(image1, keyPoint1, imageDesc1);
    SurfDescriptor.compute(image2, keyPoint2, imageDesc2);

    //获得匹配特征点，并提取最优配对     
    FlannBasedMatcher matcher;
    vector&lt;DMatch&gt; matchePoints;

    matcher.match(imageDesc1, imageDesc2, matchePoints, Mat());
    cout &lt;&lt; "total match points: " &lt;&lt; matchePoints.size() &lt;&lt; endl;


    Mat img_match;
    drawMatches(image01, keyPoint1, image02, keyPoint2, matchePoints, img_match);
    namedWindow("match", 0);
    imshow("match",img_match);
    imwrite("match.jpg", img_match);

    waitKey();
    return 0;
}

</code></pre> 
<p></p> 
<p class="img-center"><img alt="" height="509" src="https://images2.imgbox.com/4e/12/WoQVRbmU_o.png" width="1200"></p> 
<p></p> 
<p class="img-center"><img alt="" height="698" src="https://images2.imgbox.com/2c/2e/L5XzQjn0_o.png" width="993"></p> 
<p>由上面的特征点匹配的效果来看，匹配的效果还是相当糟糕的，如果我们拿着这样子的匹配结果去实现图像拼接或者物体追踪，效果肯定是极差的。所以我们需要进一步筛选匹配点，来获取优秀的匹配点，这就是所谓的“去粗取精”。这里我们采用了Lowe’s算法来进一步获取优秀匹配点。</p> 
<p>为了排除因为图像遮挡和背景混乱而产生的无匹配关系的关键点，SIFT的作者Lowe提出了比较最近邻距离与次近邻距离的SIFT匹配方式：取一幅图像中的一个SIFT关键点，并找出其与另一幅图像中欧式距离最近的前两个关键点，在这两个关键点中，如果最近的距离除以次近的距离得到的比率ratio少于某个阈值T，则接受这一对匹配点。因为对于错误匹配，由于特征空间的高维性，相似的距离可能有大量其他的错误匹配，从而它的ratio值比较高。显然降低这个比例阈值T，SIFT匹配点数目会减少，但更加稳定，反之亦然。</p> 
<p>Lowe推荐ratio的阈值为0.8，但作者对大量任意存在尺度、旋转和亮度变化的两幅图片进行匹配，结果表明ratio取值在0. 4~0. 6 之间最佳，小于0. 4的很少有匹配点，大于0. 6的则存在大量错误匹配点，所以建议ratio的取值原则如下:</p> 
<p>ratio=0. 4：对于准确度要求高的匹配；</p> 
<p>ratio=0. 6：对于匹配点数目要求比较多的匹配；</p> 
<p>ratio=0. 5：一般情况下。</p> 
<pre><code>#include "highgui/highgui.hpp"    
#include "opencv2/nonfree/nonfree.hpp"    
#include "opencv2/legacy/legacy.hpp"   
#include &lt;iostream&gt;  

using namespace cv;
using namespace std;


int main()
{

    Mat image01 = imread("g2.jpg", 1);    
    Mat image02 = imread("g4.jpg", 1);    
    imshow("p2", image01);
    imshow("p1", image02);

    //灰度图转换  
    Mat image1, image2;
    cvtColor(image01, image1, CV_RGB2GRAY);
    cvtColor(image02, image2, CV_RGB2GRAY);


    //提取特征点    
    SurfFeatureDetector surfDetector(2000);  // 海塞矩阵阈值，在这里调整精度，值越大点越少，越精准 
    vector&lt;KeyPoint&gt; keyPoint1, keyPoint2;
    surfDetector.detect(image1, keyPoint1);
    surfDetector.detect(image2, keyPoint2);

    //特征点描述，为下边的特征点匹配做准备    
    SurfDescriptorExtractor SurfDescriptor;
    Mat imageDesc1, imageDesc2;
    SurfDescriptor.compute(image1, keyPoint1, imageDesc1);
    SurfDescriptor.compute(image2, keyPoint2, imageDesc2);

    FlannBasedMatcher matcher;
    vector&lt;vector&lt;DMatch&gt; &gt; matchePoints;
    vector&lt;DMatch&gt; GoodMatchePoints;

    vector&lt;Mat&gt; train_desc(1, imageDesc1);
    matcher.add(train_desc);
    matcher.train();

    matcher.knnMatch(imageDesc2, matchePoints, 2);
    cout &lt;&lt; "total match points: " &lt;&lt; matchePoints.size() &lt;&lt; endl;

    // Lowe's algorithm,获取优秀匹配点
    for (int i = 0; i &lt; matchePoints.size(); i++)
    {
        if (matchePoints[i][0].distance &lt; 0.6 * matchePoints[i][1].distance)
        {
            GoodMatchePoints.push_back(matchePoints[i][0]);
        }
    }

    Mat first_match;
    drawMatches(image02, keyPoint2, image01, keyPoint1, GoodMatchePoints, first_match);
    imshow("first_match ", first_match);
    waitKey();
    return 0;
}
</code></pre> 
<p></p> 
<p class="img-center"><img alt="" height="512" src="https://images2.imgbox.com/a4/99/IEyLq8D0_o.png" width="1200"></p> 
<p></p> 
<p class="img-center"><img alt="" height="697" src="https://images2.imgbox.com/29/e8/Ll1eW0Qo_o.png" width="992"></p> 
<p>为了体现所谓的尺度不变形，我特意加入了额外一组图片来测试</p> 
<p class="img-center"><img alt="" height="515" src="https://images2.imgbox.com/d2/12/KfOepec0_o.png" width="947"></p> 
<p>由特征点匹配的效果来看，现在的特征点匹配应该是非常精准了，因为我们已经把不合格的匹配点统统移除出去了。</p> 
<h3 id="2sift">2.SIFT</h3> 
<p>SURF算法是SIFT算法的加速版， 而SIFT（尺度不变特征转换， ScaleInvariant Feature Transform） 是另一种著名的尺度不变特征检测法。我们知道，SURF相对于SIFT而言，特征点检测的速度有着极大的提升，所以在一些实时视频流物体匹配上有着很强的应用。而SIFT因为其巨大的特征计算量而使得特征点提取的过程异常花费时间，所以在一些注重速度的场合难有应用场景。但是SIFT相对于SURF的优点就是，由于SIFT基于浮点内核计算特征点，因此通常认为， SIFT算法检测的特征在空间和尺度上定位更加精确，所以在要求匹配极度精准且不考虑匹配速度的场合可以考虑使用SIFT算法。</p> 
<p>SIFT特征检测的代码我们仅需要对上面的SURF代码作出一丁点修改即可。</p> 
<pre><code>#include "highgui/highgui.hpp"    
#include "opencv2/nonfree/nonfree.hpp"    
#include "opencv2/legacy/legacy.hpp"   
#include &lt;iostream&gt;  

using namespace cv;
using namespace std;


int main()
{
    Mat image01 = imread("1.jpg", 1);    //右图
    Mat image02 = imread("2.jpg", 1);    //左图
    namedWindow("p2", 0);
    namedWindow("p1", 0);
    imshow("p2", image01);
    imshow("p1", image02);

    //灰度图转换  
    Mat image1, image2;
    cvtColor(image01, image1, CV_RGB2GRAY);
    cvtColor(image02, image2, CV_RGB2GRAY);


    //提取特征点    
    SiftFeatureDetector siftDetector(2000);  // 海塞矩阵阈值，在这里调整精度，值越大点越少，越精准 
    vector&lt;KeyPoint&gt; keyPoint1, keyPoint2;
    siftDetector.detect(image1, keyPoint1);
    siftDetector.detect(image2, keyPoint2);

    //特征点描述，为下边的特征点匹配做准备    
    SiftDescriptorExtractor SiftDescriptor;
    Mat imageDesc1, imageDesc2;
    SiftDescriptor.compute(image1, keyPoint1, imageDesc1);
    SiftDescriptor.compute(image2, keyPoint2, imageDesc2);

    //获得匹配特征点，并提取最优配对     
    FlannBasedMatcher matcher;
    vector&lt;DMatch&gt; matchePoints;

    matcher.match(imageDesc1, imageDesc2, matchePoints, Mat());
    cout &lt;&lt; "total match points: " &lt;&lt; matchePoints.size() &lt;&lt; endl;


    Mat img_match;
    drawMatches(image01, keyPoint1, image02, keyPoint2, matchePoints, img_match);

    imshow("match",img_match);
    imwrite("match.jpg", img_match);

    waitKey();
    return 0;
}

</code></pre> 
<p></p> 
<p class="img-center"><img alt="" height="511" src="https://images2.imgbox.com/27/c7/WYD3SPjd_o.png" width="1200"></p> 
<p></p> 
<p class="img-center"><img alt="" height="695" src="https://images2.imgbox.com/48/b3/piNUixpG_o.png" width="993"></p> 
<p>没有经过点筛选的匹配效果同样糟糕。下面继续采用Lowe‘s的算法选出优秀匹配点。</p> 
<pre><code>#include "highgui/highgui.hpp"    
#include "opencv2/nonfree/nonfree.hpp"    
#include "opencv2/legacy/legacy.hpp"   
#include &lt;iostream&gt;  

using namespace cv;
using namespace std;


int main()
{

    Mat image01 = imread("1.jpg", 1);
    Mat image02 = imread("2.jpg", 1);
    imshow("p2", image01);
    imshow("p1", image02);

    //灰度图转换  
    Mat image1, image2;
    cvtColor(image01, image1, CV_RGB2GRAY);
    cvtColor(image02, image2, CV_RGB2GRAY);


    //提取特征点    
    SiftFeatureDetector siftDetector(800);  // 海塞矩阵阈值，在这里调整精度，值越大点越少，越精准 
    vector&lt;KeyPoint&gt; keyPoint1, keyPoint2;
    siftDetector.detect(image1, keyPoint1);
    siftDetector.detect(image2, keyPoint2);

    //特征点描述，为下边的特征点匹配做准备    
    SiftDescriptorExtractor SiftDescriptor;
    Mat imageDesc1, imageDesc2;
    SiftDescriptor.compute(image1, keyPoint1, imageDesc1);
    SiftDescriptor.compute(image2, keyPoint2, imageDesc2);

    FlannBasedMatcher matcher;
    vector&lt;vector&lt;DMatch&gt; &gt; matchePoints;
    vector&lt;DMatch&gt; GoodMatchePoints;

    vector&lt;Mat&gt; train_desc(1, imageDesc1);
    matcher.add(train_desc);
    matcher.train();

    matcher.knnMatch(imageDesc2, matchePoints, 2);
    cout &lt;&lt; "total match points: " &lt;&lt; matchePoints.size() &lt;&lt; endl;

    // Lowe's algorithm,获取优秀匹配点
    for (int i = 0; i &lt; matchePoints.size(); i++)
    {
        if (matchePoints[i][0].distance &lt; 0.6 * matchePoints[i][1].distance)
        {
            GoodMatchePoints.push_back(matchePoints[i][0]);
        }
    }

    Mat first_match;
    drawMatches(image02, keyPoint2, image01, keyPoint1, GoodMatchePoints, first_match);
    imshow("first_match ", first_match);
    imwrite("first_match.jpg", first_match);
    waitKey();
    return 0;
}
</code></pre> 
<p></p> 
<p class="img-center"><img alt="" height="512" src="https://images2.imgbox.com/03/42/nFSC96We_o.png" width="1200"></p> 
<p></p> 
<p class="img-center"><img alt="" height="699" src="https://images2.imgbox.com/33/47/SYb2gq2J_o.png" width="994"></p> 
<p></p> 
<p class="img-center"><img alt="" height="512" src="https://images2.imgbox.com/3a/4b/ngYIaLPG_o.png" width="946"></p> 
<h3 id="3orb">3.ORB</h3> 
<p>ORB是ORiented Brief的简称，是brief算法的改进版。ORB算法比SIFT算法快100倍，比SURF算法快10倍。在计算机视觉领域有种说法，ORB算法的综合性能在各种测评里较其他特征提取算法是最好的。</p> 
<p>ORB算法是brief算法的改进，那么我们先说一下brief算法有什么去缺点。<br> BRIEF的优点在于其速度，其缺点是：</p> 
<ul><li>不具备旋转不变性</li><li>对噪声敏感</li><li>不具备尺度不变性</li></ul> 
<p>而ORB算法就是试图解决上述缺点中1和2提出的一种新概念。值得注意的是，<strong>ORB没有解决尺度不变性</strong>。</p> 
<pre><code>#include "highgui/highgui.hpp"    
#include "opencv2/nonfree/nonfree.hpp"    
#include "opencv2/legacy/legacy.hpp"   
#include &lt;iostream&gt;  

using namespace cv;
using namespace std;


int main()
{

    Mat image01 = imread("g2.jpg", 1);
    Mat image02 = imread("g4.jpg", 1);
    imshow("p2", image01);
    imshow("p1", image02);

    //灰度图转换  
    Mat image1, image2;
    cvtColor(image01, image1, CV_RGB2GRAY);
    cvtColor(image02, image2, CV_RGB2GRAY);


    //提取特征点    
    OrbFeatureDetector OrbDetector(1000);  // 在这里调整精度，值越小点越少，越精准 
    vector&lt;KeyPoint&gt; keyPoint1, keyPoint2;
    OrbDetector.detect(image1, keyPoint1);
    OrbDetector.detect(image2, keyPoint2);

    //特征点描述，为下边的特征点匹配做准备    
    OrbDescriptorExtractor OrbDescriptor;
    Mat imageDesc1, imageDesc2;
    OrbDescriptor.compute(image1, keyPoint1, imageDesc1);
    OrbDescriptor.compute(image2, keyPoint2, imageDesc2);

    flann::Index flannIndex(imageDesc1, flann::LshIndexParams(12, 20, 2), cvflann::FLANN_DIST_HAMMING);

    vector&lt;DMatch&gt; GoodMatchePoints;

    Mat macthIndex(imageDesc2.rows, 2, CV_32SC1), matchDistance(imageDesc2.rows, 2, CV_32FC1);
    flannIndex.knnSearch(imageDesc2, macthIndex, matchDistance, 2, flann::SearchParams());

    // Lowe's algorithm,获取优秀匹配点
    for (int i = 0; i &lt; matchDistance.rows; i++)
    {
        if (matchDistance.at&lt;float&gt;(i,0) &lt; 0.6 * matchDistance.at&lt;float&gt;(i, 1))
        {
            DMatch dmatches(i, macthIndex.at&lt;int&gt;(i, 0), matchDistance.at&lt;float&gt;(i, 0));
            GoodMatchePoints.push_back(dmatches);
        }
    }

    Mat first_match;
    drawMatches(image02, keyPoint2, image01, keyPoint1, GoodMatchePoints, first_match);
    imshow("first_match ", first_match);
    imwrite("first_match.jpg", first_match);
    waitKey();
    return 0;
}
</code></pre> 
<p></p> 
<p class="img-center"><img alt="" height="700" src="https://images2.imgbox.com/2f/44/ODDjdiQE_o.png" width="991"></p> 
<p></p> 
<p class="img-center"><img alt="" height="511" src="https://images2.imgbox.com/7d/e2/EVMFwAE4_o.png" width="1200"></p> 
<h3 id="4fast">4.FAST</h3> 
<p>FAST（加速分割测试获得特征， Features from Accelerated Segment Test） 。 这种算子专门用来快速检测兴趣点， 只需要对比几个像素，就可以判断是否为关键点。</p> 
<p>跟Harris检测器的情况一样， FAST算法源于对构成角点的定义。FAST对角点的定义基于候选特征点周围的图像强度值。 以某个点为中心作一个圆， 根据圆上的像素值判断该点是否为关键点。 如果存在这样一段圆弧， 它的连续长度超过周长的3/4， 并且它上面所有像素的强度值都与圆心的强度值明显不同（全部更黑或更亮） ， 那么就认定这是一个关键点。</p> 
<p>用这个算法检测兴趣点的速度非常快， 因此十分适合需要优先考虑速度的应用。 这些应用包括实时视觉跟踪、 目标识别等， 它们需要在实<br> 时视频流中跟踪或匹配多个点。</p> 
<p>我们使用FastFeatureDetector 进行特征点提取，因为opencv没有提供fast专用的描述子提取器，所以我们借用SiftDescriptorExtractor 来实现描述子的提取。</p> 
<pre><code>#include "highgui/highgui.hpp"    
#include "opencv2/nonfree/nonfree.hpp"    
#include "opencv2/legacy/legacy.hpp"   
#include &lt;iostream&gt;  

using namespace cv;
using namespace std;


int main()
{

    Mat image01 = imread("1.jpg", 1);
    Mat image02 = imread("2.jpg", 1);
    imshow("p2", image01);
    imshow("p1", image02);

    //灰度图转换  
    Mat image1, image2;
    cvtColor(image01, image1, CV_RGB2GRAY);
    cvtColor(image02, image2, CV_RGB2GRAY);


    //提取特征点    
    FastFeatureDetector Detector(50);  //阈值 
    vector&lt;KeyPoint&gt; keyPoint1, keyPoint2;
    Detector.detect(image1, keyPoint1);
    Detector.detect(image2, keyPoint2);

    //特征点描述，为下边的特征点匹配做准备    
    SiftDescriptorExtractor   Descriptor;
    Mat imageDesc1, imageDesc2;
    Descriptor.compute(image1, keyPoint1, imageDesc1);
    Descriptor.compute(image2, keyPoint2, imageDesc2);

    BruteForceMatcher&lt; L2&lt;float&gt; &gt; matcher;   
    vector&lt;vector&lt;DMatch&gt; &gt; matchePoints;
    vector&lt;DMatch&gt; GoodMatchePoints;

    vector&lt;Mat&gt; train_desc(1, imageDesc1);
    matcher.add(train_desc);
    matcher.train();

    matcher.knnMatch(imageDesc2, matchePoints, 2);
    cout &lt;&lt; "total match points: " &lt;&lt; matchePoints.size() &lt;&lt; endl;

    // Lowe's algorithm,获取优秀匹配点
    for (int i = 0; i &lt; matchePoints.size(); i++)
    {
        if (matchePoints[i][0].distance &lt; 0.6 * matchePoints[i][1].distance)
        {
            GoodMatchePoints.push_back(matchePoints[i][0]);
        }
    }

    Mat first_match;
    drawMatches(image02, keyPoint2, image01, keyPoint1, GoodMatchePoints, first_match);
    imshow("first_match ", first_match);
    imwrite("first_match.jpg", first_match);
    waitKey();
    return 0;
}
</code></pre> 
<p></p> 
<p class="img-center"><img alt="" height="512" src="https://images2.imgbox.com/b3/c0/mTi9ZV8a_o.png" width="1200"></p> 
<p></p> 
<p class="img-center"><img alt="" height="697" src="https://images2.imgbox.com/1e/45/q9ugm7k0_o.png" width="993"></p> 
<p>如果我们把描述子换成SurfDescriptorExtractor，即FastFeatureDetector + SurfDescriptorExtractor的组合，看看效果</p> 
<p class="img-center"><img alt="" height="511" src="https://images2.imgbox.com/99/2d/xCQw2pZU_o.png" width="1200"></p> 
<p>可以看出，这种组合下的特征点匹配并不精确。</p> 
<p>如果我们把描述子换成SurfDescriptorExtractor，即FastFeatureDetector + BriefDescriptorExtractor 的组合，看看效果</p> 
<p class="img-center"><img alt="" height="511" src="https://images2.imgbox.com/91/bd/egs6Lq8J_o.png" width="1200"></p> 
<p>速度虽快，但是精度却差强人意。</p> 
<p>看到这里可能很多人会有疑惑：为什么FAST特征点可以用所以我们借用SiftDescriptorExtractor或者其他描述子提取器进行提取？</p> 
<p>在这里我说一下自己的理解。要完成特征点的匹配第一个步骤就是找出每幅图像的特征点，这叫做特征检测，比如我们使用FastFeatureDetector、SiftFeatureDetector都是特征检测的模块。我们得到这些图像的特征点后，我们就对这些特征点进行进一步的分析，用一些数学上的特征对其进行描述，如梯度直方图，局部随机二值特征等。所以在这一步我们可以选择其他描述子提取器对这些点进行特征描述，进而完成特征点的精确匹配。</p> 
<p>在opencv中，SURF,ORB,SIFT既包含FeatureDetector，又包含 DescriptorExtractor，所以我们使用上述三种算法做特征匹配时，都用其自带的方法配套使用。</p> 
<p>除此之外，如果我们相用FAST角点检测并作特征点匹配该怎么办？此时可以使用上述的FastFeatureDetector + BriefDescriptorExtractor 的方式，这种组合方式其实就是著名的ORB算法。所以特征点检测和特征点匹配是两种不同的步骤，我们只需根据自己项目的需求对这两个步骤的方法随意组合就好。</p> 
<h3 id="5harris角点">5.Harris角点</h3> 
<p>在图像中搜索有价值的特征点时，使用角点是一种不错的方法。 角点是很容易在图像中定位的局部特征， 并且大量存在于人造物体中（例如墙壁、 门、 窗户、 桌子等产生的角点）。 角点的价值在于它是两条边缘线的接合点， 是一种二维特征，可以被精确地定位（即使是子像素级精度）。 与此相反的是位于均匀区域或物体轮廓上的点以及在同一物体的不同图像上很难重复精确定位的点。 Harris特征检测是检测角点的经典方法。</p> 
<p>这里仅展示GoodFeaturesToTrackDetector +　SiftDescriptorExtractor的组合方式的代码，其他组合不再演示。</p> 
<pre><code>#include "highgui/highgui.hpp"    
#include "opencv2/nonfree/nonfree.hpp"    
#include "opencv2/legacy/legacy.hpp"   
#include &lt;iostream&gt;  

using namespace cv;
using namespace std;


int main()
{

    Mat image01 = imread("1.jpg", 1);
    Mat image02 = imread("2.jpg", 1);
    imshow("p2", image01);
    imshow("p1", image02);

    //灰度图转换  
    Mat image1, image2;
    cvtColor(image01, image1, CV_RGB2GRAY);
    cvtColor(image02, image2, CV_RGB2GRAY);


    //提取特征点    
    GoodFeaturesToTrackDetector Detector(500);  //最大点数,值越大，点越多
    vector&lt;KeyPoint&gt; keyPoint1, keyPoint2;
    Detector.detect(image1, keyPoint1);
    Detector.detect(image2, keyPoint2);

    //特征点描述，为下边的特征点匹配做准备    
    SiftDescriptorExtractor  Descriptor;
    Mat imageDesc1, imageDesc2;
    Descriptor.compute(image1, keyPoint1, imageDesc1);
    Descriptor.compute(image2, keyPoint2, imageDesc2);

    BruteForceMatcher&lt; L2&lt;float&gt; &gt; matcher;   
    vector&lt;vector&lt;DMatch&gt; &gt; matchePoints;
    vector&lt;DMatch&gt; GoodMatchePoints;

    vector&lt;Mat&gt; train_desc(1, imageDesc1);
    matcher.add(train_desc);
    matcher.train();

    matcher.knnMatch(imageDesc2, matchePoints, 2);
    cout &lt;&lt; "total match points: " &lt;&lt; matchePoints.size() &lt;&lt; endl;

    // Lowe's algorithm,获取优秀匹配点
    for (int i = 0; i &lt; matchePoints.size(); i++)
    {
        if (matchePoints[i][0].distance &lt; 0.6 * matchePoints[i][1].distance)
        {
            GoodMatchePoints.push_back(matchePoints[i][0]);
        }
    }

    Mat first_match;
    drawMatches(image02, keyPoint2, image01, keyPoint1, GoodMatchePoints, first_match);
    imshow("first_match ", first_match);
    imwrite("first_match.jpg", first_match);
    waitKey();
    return 0;
}
</code></pre> 
<p>匹配相当精准</p> 
<p class="img-center"><img alt="" height="689" src="https://images2.imgbox.com/73/37/S1w4YEHb_o.png" width="995"></p> 
<p></p> 
<p class="img-center"><img alt="" height="515" src="https://images2.imgbox.com/d9/a7/fghvkiZf_o.png" width="1200"></p> 
<p>计算机视觉领域其实还有了很多特征检测的方法，比如HOG、Harr、LBP等，这里就不再叙述了，因为方法都是类似，我们根据自己的需求挑选相应的方法就好了。</p> 
<p></p> 
<p><a id="!comments"></a></p> 
<p><a name="commentform"></a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6e33691bce881632fd8df9e41cda1f92/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">如何训练一个简单的stable diffusion模型(附详细注释）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c146ffb4abd47347e53ce32b57b55957/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">C&#43;&#43; 智能指针常用总结</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>