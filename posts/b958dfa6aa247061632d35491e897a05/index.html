<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>使用auto-encoder实现数据压缩、分类预测 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="使用auto-encoder实现数据压缩、分类预测" />
<meta property="og:description" content="原理简介： （文章编辑bug太多，白码字了，只能简化说了。）通过encoder-decoder的结构形成一个负、正金字塔的瓶颈结构(必须是瓶颈结构，才能起到压缩的效果，不然就成了复制了)，对数据进行压缩并还原，以还原程度为指标来训练网络。训练完网络就有了一个encoder和decoder，decoder先不管，encoder能把数据压缩，因为压缩还能还原，证明原数据有冗余，压缩后的code可以直接用来分类，而不是直接用原数据。所以在encoder后边接一个分类器就能完成分类工作。（如果你要进行分类工作，当你训练完auto-encoder，decoder应该是没有用的，下边会说）
结构示意
&#43;&#43;&#43;&#43;&#43;&#43;&#43;&#43;&#43;&#43;&#43;&#43; input
&#43;&#43;&#43;&#43;&#43;&#43; encoder
&#43;&#43;&#43; code
&#43;&#43;&#43;&#43;&#43;&#43; decoder
&#43;&#43;&#43;&#43;&#43;&#43;&#43;&#43;&#43;&#43;&#43;&#43; output
下边放一个示意图：
如果要直观的解释auto-encoder实现了什么功能，如何实现，如果说压缩到z之后信息已经损失了，为什么还要恢复到x^？况且，本来最后也用不到decoder？
可以这样说，encoder把成绩从100～0分成了优良中差，既然z是瓶颈，恢复到x^时确实已经丢失信息了，优不知道对应91还是99了，但是，仍然要reconstruct到x^的意义是，有个参照来矫正，用x^和目标x对比，至少优还让你在九十多分，而不会到六十分以下，这一部分是要实现训练的功能，让auto-encoder有据可依。（那么，为何不直接用优良中差来训练z呢？因为你没有优良中差的标签，现实的神经网络，也不一定刚好用60分来标记差，如果可以，还要神经网络干嘛？回过头来讲，不就因为你不知道怎么给成绩分类优良中差，所以才要让它自动学习的嘛）
下图有loss公式（不用label训练），训练之后，decoder就没用了，注意，是没用了！！！要记住，目的是得到encoder！而VAE则是保留Decoder（当然，VAE的核心是学到z分布，只是相对auto-encoder，VAE是保留decoder）
这个才是完全体，抛弃decoder，用encoder直接作为输入的过渡（不过，反思一下，一定需要它么？我觉得在深度网络大行其道的当下，这个必要性不存在，网络容量够大，什么特征提取不来？都不耽误训练！够用！当然，这块是个人认为，有待求证）
大概去搜了一下网上的说法，关于auto-encoder在深度学习的必要性：
autoencoder方法可能的一些优势
（1）用于supervised learning可作为特征转换的手段
在观察到的数据上能探测到一些潜藏的structures，可以用于深度神经网络的pre-training部分，得到较好的初始的权重。(这是前边提到的分类问题，监督学习，也仅仅是预训练，感觉不是绝对的必要)
（2）用于unsupervised learning
density estimation和离群点检测（所谓离群点检测，大概也就是通过encoder再通过decoder，两者都要有，看重建之后的偏差，大于一个阈值就算离群点（前提，训练auto-encoder的数据一定要正常）。总的来说，这是偏向于人工去观察和可视化方面的，也就是说，一定程度上近似PCA的功能）
（3）学习数据的典型表示
小结：可能在不同场景下确实有些用处，没用到的暂时不讨论。理论上，压缩数据确实可能节省不少参数量和计算量。至少在本文提到的用于预处理特征从而进行分类，在网络性能没达到瓶颈的前提下（毕竟resnet都一百多层了，虽然我的显卡不行，土豪的显卡也不是瓶颈），用encoder处理不具有十足的必要性。尤其是BN的使用，使得一个精心挑选的初始化显得不是那么必要。相似的还有word2vec的n-gram，也算是一个简单的数据压缩，算是输入层面的一个转换。
todo:上边三点也不一定代表全部，有空也看看论文怎么说。
训练过程： 用三层auto-encoder进行训练，encoder逐层递减，decoder逐层递增。
训练结束后把encode的输出接到全连接层，实现分类预测。
图像压缩到3维大概是77%的准确率。可以把不同层的encode（en2、en1、en0）输出接入到全连接层，根据压缩程度不同，准确率不同。激活函数也有略微影响。暂时就不试所有情况了，有兴趣的可以多试试。
因为两层全连接层的输出同是10（encode输出是3维，所以fc1没设置太大，没意义；en2以前不小于12，所以fc1也没设太小），有点像平行输出，所以第二层不能再加tanh激活，准确率会下降。
import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D from matplotlib import cm import numpy as np tf.set_random_seed(1) # Hyper Parameters BATCH_SIZE = 64 LR = 0.002 # learning rate CLR = 0." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/b958dfa6aa247061632d35491e897a05/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-09-29T12:31:52+08:00" />
<meta property="article:modified_time" content="2018-09-29T12:31:52+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">使用auto-encoder实现数据压缩、分类预测</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>原理简介：</h2> 
<p>（文章编辑bug太多，白码字了，只能简化说了。）通过encoder-decoder的结构形成一个负、正金字塔的瓶颈结构(必须是瓶颈结构，才能起到压缩的效果，不然就成了复制了)，对数据进行压缩并还原，以还原程度为指标来训练网络。训练完网络就有了一个encoder和decoder，decoder先不管，encoder能把数据压缩，因为压缩还能还原，证明原数据有冗余，压缩后的code可以直接用来分类，而不是直接用原数据。<strong>所以在encoder后边接一个分类器就能完成分类工作。（如果你要进行分类工作，当你训练完auto-encoder，decoder应该是没有用的，下边会说）</strong></p> 
<p>结构示意</p> 
<p>++++++++++++ input</p> 
<p>++++++             encoder</p> 
<p>+++                    code</p> 
<p>++++++              decoder</p> 
<p>++++++++++++ output</p> 
<p> </p> 
<p> </p> 
<p><strong>下边放一个示意图：</strong></p> 
<p><img alt="" class="has" height="473" src="https://images2.imgbox.com/81/cc/Wtdhc89H_o.png" width="926"></p> 
<p>如果要直观的解释auto-encoder实现了什么功能，如何实现，如果说压缩到z之后信息已经损失了，为什么还要恢复到x^？况且，本来最后也用不到decoder？</p> 
<p>可以这样说，encoder把成绩从100～0分成了优良中差，既然z是瓶颈，恢复到x^时确实已经丢失信息了，优不知道对应91还是99了，但是，仍然要reconstruct到x^的意义是，有个参照来矫正，用x^和目标x对比，至少优还让你在九十多分，而不会到六十分以下，这一部分是要实现训练的功能，让auto-encoder有据可依。（那么，为何不直接用优良中差来训练z呢？因为你没有优良中差的标签，现实的神经网络，也不一定刚好用60分来标记差，如果可以，还要神经网络干嘛？回过头来讲，不就因为你不知道怎么给成绩分类优良中差，所以才要让它自动学习的嘛）</p> 
<p> </p> 
<p><strong>下图有loss公式（不用label训练），训练之后，decoder就没用了，注意，是没用了！！！要记住，目的是得到encoder！而VAE则是保留Decoder（当然，VAE的核心是学到z分布，只是相对auto-encoder，VAE是保留decoder）</strong></p> 
<p><img alt="" class="has" height="471" src="https://images2.imgbox.com/73/ba/yyiQfnXC_o.png" width="931"></p> 
<p><img alt="" class="has" height="275" src="https://images2.imgbox.com/30/52/T80B0z3o_o.png" width="718"></p> 
<p> </p> 
<p> </p> 
<p>这个才是完全体，抛弃decoder，用encoder直接作为输入的过渡<span style="color:#f33b45;"><strong>（不过，反思一下，一定需要它么？我觉得在深度网络大行其道的当下，这个必要性不存在，网络容量够大，什么特征提取不来？都不耽误训练！够用！当然，这块是个人认为，有待求证）</strong></span></p> 
<p><img alt="" class="has" height="390" src="https://images2.imgbox.com/ab/d6/qafLgqq9_o.png" width="937"></p> 
<p><strong>大概去搜了一下网上的说法，关于auto-encoder在深度学习的必要性：</strong></p> 
<p><u>autoencoder方法可能的一些优势</u></p> 
<p><u>（1）用于supervised learning可作为特征转换的手段</u></p> 
<p><u>　　在观察到的数据上能探测到一些潜藏的structures，可以用于深度神经网络的pre-training部分，<strong>得到较好的初始的权重</strong>。</u><strong>(这是前边提到的分类问题，监督学习，也仅仅是预训练，感觉不是绝对的必要)</strong></p> 
<p><u>（2）<strong>用于unsupervised learning</strong></u></p> 
<p><u>　　density estimation和离群点检测</u><strong>（所谓离群点检测，大概也就是通过encoder再通过decoder，两者都要有，看重建之后的偏差，大于一个阈值就算离群点<span style="color:#f33b45;">（前提，训练auto-encoder的数据一定要正常）</span>。总的来说，这是偏向于人工去观察和可视化方面的，也就是说，一定程度上近似PCA的功能）</strong></p> 
<p><u>（3）学习数据的典型表示</u></p> 
<p><img alt="" class="has" height="181" src="https://images2.imgbox.com/e4/fd/nR5o1mVt_o.png" width="386"></p> 
<p><strong>小结：可能在不同场景下确实有些用处，没用到的暂时不讨论。理论上，压缩数据确实可能节省不少参数量和计算量。至少在本文提到的用于预处理特征从而进行分类，在网络性能没达到瓶颈的前提下（毕竟resnet都一百多层了，虽然我的显卡不行，土豪的显卡也不是瓶颈），用encoder处理不具有十足的必要性。尤其是BN的使用，使得一个精心挑选的初始化显得不是那么必要。相似的还有word2vec的n-gram，也算是一个简单的数据压缩，算是输入层面的一个转换。</strong></p> 
<p>todo:上边三点也不一定代表全部，有空也看看论文怎么说。</p> 
<p> </p> 
<p> </p> 
<p> </p> 
<h2>训练过程：</h2> 
<p>用三层auto-encoder进行训练，encoder逐层递减，decoder逐层递增。</p> 
<p>训练结束后把encode的输出接到全连接层，实现分类预测。</p> 
<p>图像压缩到3维大概是77%的准确率。可以把不同层的encode（en2、en1、en0）输出接入到全连接层，根据压缩程度不同，准确率不同。激活函数也有略微影响。暂时就不试所有情况了，有兴趣的可以多试试。</p> 
<p>因为两层全连接层的输出同是10（encode输出是3维，所以fc1没设置太大，没意义；en2以前不小于12，所以fc1也没设太小），有点像平行输出，所以第二层不能再加tanh激活，准确率会下降。</p> 
<p> </p> 
<pre class="has"><code class="language-python">
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm
import numpy as np

tf.set_random_seed(1)

# Hyper Parameters
BATCH_SIZE = 64
LR = 0.002         # learning rate
CLR = 0.05         # classification learning rate
N_TEST_IMG = 5
IMG_SHOW_START = 5

# Mnist digits
mnist = input_data.read_data_sets('./mnist', one_hot=True)     # use not one-hotted target data
test_x = mnist.test.images[:200]
test_y = mnist.test.labels[:200]
print(test_y.shape)

# plot one example
print(mnist.train.images.shape)     # (55000, 28 * 28)
print(mnist.train.labels.shape)     # (55000, 10)
# plt.imshow(mnist.train.images[0].reshape((28, 28)), cmap='gray')
# plt.title('%i' % np.argmax(mnist.train.labels[0]))
# plt.show()

# tf placeholder
tf_x = tf.placeholder(tf.float32, [None, 28*28])    # value in the range of (0, 1)
tf_y = tf.placeholder(tf.float32, [None, 10])

# encoder
en0 = tf.layers.dense(tf_x, 128, tf.nn.tanh)
en1 = tf.layers.dense(en0, 64, tf.nn.tanh)
en2 = tf.layers.dense(en1, 12, tf.nn.tanh)
# encoded = tf.layers.dense(en2, 3)
encoded = tf.layers.dense(en2, 3)

# decoder
de0 = tf.layers.dense(encoded, 12, tf.nn.tanh)
de1 = tf.layers.dense(de0, 64, tf.nn.tanh)
de2 = tf.layers.dense(de1, 128, tf.nn.tanh)
decoded = tf.layers.dense(de2, 28*28, tf.nn.sigmoid)

loss = tf.losses.mean_squared_error(labels=tf_x, predictions=decoded)
train = tf.train.AdamOptimizer(LR).minimize(loss)

#########################################################################################
#classification training
weights_fc1 = tf.Variable(initial_value = tf.truncated_normal([12,10],stddev=0.1),dtype=tf.float32)
biases_fc1 = tf.Variable(initial_value = tf.constant(0.1,shape = [10]))
weights_out = tf.Variable(initial_value = tf.truncated_normal([10,10],stddev=0.1),dtype=tf.float32)
biases_out = tf.Variable(initial_value = tf.constant(0.1,shape = [10]))

#fc1 = tf.nn.relu(tf.matmul(encoded, weights_fc1) + biases_fc1)#use layer encoded:0.754+
#fc1 = (tf.matmul(encoded, weights_fc1) + biases_fc1)#use layer encoded:0.63-
#fc1 = tf.nn.tanh(tf.matmul(encoded, weights_fc1) + biases_fc1)#use layer encoded:0.773+
fc1 = tf.nn.tanh(tf.matmul(en2,weights_fc1) + biases_fc1)#use layer en2:0.786+     #modify weights_fc1 correspondingly

pred = (tf.matmul(fc1, weights_out) + biases_out)#bad result in tanh
#pred = fc1#just use fc1 to predict  #use layer en2:0.719+ in 80000 steps,0.7639 in 800000 steps

pred_loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(onehot_labels=tf_y, logits = pred))
pred_train = tf.train.AdamOptimizer(CLR).minimize(pred_loss,var_list=[weights_fc1,weights_out,biases_fc1,biases_out])
pred_accuracy = tf.metrics.accuracy(labels = tf.argmax(tf_y,axis=1), predictions = tf.argmax(pred,axis=1))#dont forget axis=1
#########################################################################################
sess = tf.Session()
sess.run(tf.global_variables_initializer())
sess.run(tf.local_variables_initializer())

# initialize figure
f, a = plt.subplots(2, N_TEST_IMG, figsize=(5, 2))
print(type(a))
print(a.shape)
plt.ion()   # continuously plot

# original data (first row) for viewing
view_data = mnist.test.images[IMG_SHOW_START : IMG_SHOW_START + N_TEST_IMG]
idx = 0#can not change to other!!!!!!!!!!!!!!!!!!!!!!this is axes,not data#this is fixed rows 0
for i in range(N_TEST_IMG):
    a[idx][i].imshow(np.reshape(view_data[i], (28, 28)), cmap='gray')
    a[idx][i].set_xticks(()); a[idx][i].set_yticks(())

for step in range(8000):
    b_x, b_y = mnist.train.next_batch(BATCH_SIZE)
    _, encoded_, decoded_, loss_ = sess.run([train, encoded, decoded, loss], {tf_x: b_x})

    if step % 100 == 0:     # plotting
        print('train loss: %.4f' % loss_)
        #plotting decoded image (second row)
        decoded_data = sess.run(decoded, {tf_x: view_data})
        for i in range(N_TEST_IMG):
            a[1][i].clear()#this is fixed rows 1
            a[1][i].imshow(np.reshape(decoded_data[i], (28, 28)), cmap='gray')
            a[1][i].set_xticks(()); a[1][i].set_yticks(())
        plt.draw(); #plt.pause(0.01)
plt.ioff()

# visualize in 3D plot
view_data = test_x[:200]
encoded_data = sess.run(encoded, {tf_x: view_data})
fig = plt.figure(2); ax = Axes3D(fig)
X, Y, Z = encoded_data[:, 0], encoded_data[:, 1], encoded_data[:, 2]
for x, y, z, s in zip(X, Y, Z, np.argmax(test_y,axis = 1)):
    c = cm.rainbow(int(255*s/9)); ax.text(x, y, z, s, backgroundcolor=c)
ax.set_xlim(X.min(), X.max()); ax.set_ylim(Y.min(), Y.max()); ax.set_zlim(Z.min(), Z.max())
plt.show()

#
for step in range(80000):
    b_x, b_y = mnist.train.next_batch(BATCH_SIZE)
    _  = sess.run([pred_train], {tf_x: b_x, tf_y:b_y})

    if step % 100 == 0:     # plotting
        pred_, loss_, pred_loss_, accuracy_ = sess.run([ pred, loss, pred_loss, pred_accuracy],
                                                          {tf_x: test_x, tf_y: test_y})
        print('auto-encoder train loss: %.4f' % loss_)
        print('classification train loss: %.4f' % pred_loss_)
        print('classification train accuracy: %.4f' % accuracy_[1])
        #print('pred_: ' ,np.argmax(pred_,axis=1), ' label: ' ,np.argmax(test_y,axis=1))

plt.ioff()</code></pre> 
<p><img alt="" class="has" height="455" src="https://images2.imgbox.com/12/dc/LNOWTo3F_o.png" width="504"></p> 
<p> </p> 
<p><strong>压缩前后对比</strong> </p> 
<p><img alt="" class="has" height="305" src="https://images2.imgbox.com/20/1d/Ki3ERUwt_o.png" width="516"></p> 
<p><a href="https://github.com/huqinwei/tensorflow_demo/blob/master/auto_encoder_demo.py">https://github.com/huqinwei/tensorflow_demo/blob/master/auto_encoder_demo.py</a></p> 
<p> </p> 
<p>================================================================================================</p> 
<p>20200908补充：</p> 
<h4>可能有些人没看太明白，集中的问题是，降维后的东西到底是什么，以及我能用encoder-decoder干什么？</h4> 
<p>首先，降维后的东西已经抽象了，既然叫encoder，编码，产出就是code，想象一段摩斯码，摩斯码你能看到什么？没有解码器你什么都看不到。所以降维后已经不是图形了，如果有，意义也不太一样，而且也取决于你不同的压缩程度和编码容量。</p> 
<p> </p> 
<p>那么如果我想做分类，怎么用？encoder-decoder是两个东西，如果是训练过程中，当然两个都需要。如果训练结束，分类只需要encoder就行了，encoder用来做数据降维，把数据压缩到更小的维度，降低信息量，同时仍然可能保留分类关系，这时候，你的分类器输入就是code，输出就是label。</p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8d7aa3e728fa1e3dcae3550d975e03c0/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">使用CentOS6.8搭建Hadoop集群</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/3036580dbf1a4b557534f408440cc742/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">基于MATLAB的二进制数字调制与解调信号的仿真——2ASK</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>