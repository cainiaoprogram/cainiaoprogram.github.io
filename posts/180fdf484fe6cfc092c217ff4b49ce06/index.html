<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>TAFFC | 清华大学刘永进教授课题组提出基于情绪字典与注意力机制的多模态情绪分布学习方法... - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="TAFFC | 清华大学刘永进教授课题组提出基于情绪字典与注意力机制的多模态情绪分布学习方法..." />
<meta property="og:description" content="与实验室诱发的单一情绪不同，真实世界中人类个体往往在同一时刻展现的多种混合情绪状态，比如悲喜交加、又惊又喜等。清华大学刘永进教授课题组提出一种从多模态数据中学习并预测混合情绪的方法，该项研究于2023年被IEEE Transactions on Affective Computing期刊录用，论文的共同第一作者为刘永进教授指导的博士后刘舫和博士生杨培，刘永进教授指导的博士生舒叶芷、访问学者闫飞博士、硕士毕业生张冠华共同参与了讨论和论文撰写。IEEE Transactions on Affective Computing是国际上情感计算领域内的著名期刊，SCI影响因子为11.2。
全文官方链接：
https://doi.org/10.1109/TAFFC.2023.3334520
多模态情绪分析作为情绪计算的一个重要方向，近年来受到越来越多的关注。大多数现有的多模态情绪识别研究针对的是一个分类任务，目的是为几种不同模态的输入数据（包括多媒体信号和生理信号）的组合分配一个特定的情绪类别。与单一情绪识别相比，越来越多的心理学证据表明，不同的离散情绪可能同时存在，这促进了混合情绪识别研究的发展，即识别基本情绪的混合。
虽然当前的大多数研究将其视为一个多标签分类任务，但在这项工作中，本文关注同时出现积极和消极情绪的挑战性情况，并提出了一种多模态混合情绪识别框架（EmotionDict）。EmotionDict的主要特点包括：
（1）受心理学研究的启发，混合状态可以用基本情绪的组合来表示，将混合情绪识别视为标签分布学习任务。设计情绪字典（Emotion Dictionary）作为在共享的隐空间中的情绪表征。
（2）虽然大多数多模态情绪分布研究建立在多媒体信号（如文本、图像、音频和视频）的基础上，但本文使用生理和显式行为多模态信号，包括脑电（EEG）、外周生理信号和面部视频。多模态信号与中央或外周神经系统以及运动皮层有关，各种模态具有不同的特点。
（3）本文进一步设计了辅助任务来学习各模态的注意力进行多模态融合。在两个数据集上的实验结果表明，本文的方法在混合情绪识别方面超过了现有的最先进的方法。
图 1.多模态情绪分布学习任务示例。
情绪识别已经成为情绪计算领域的一个重要话题，不仅由于它是许多下游任务和应用（例如，媒体分析任务、人机交互和心理治疗）的基础，而且因为情绪在人的心理状态中起着关键作用。目前学术研究中情绪空间主要由两种模型描述：
（1）离散模型，它将情绪状态映射到一组基本情绪类别，如快乐、悲伤、惊讶、恐惧、愤怒和厌恶；
（2）维度模型，它将空间分为“效价-唤醒度”（valence-arousal，VA）维度或“效价-唤醒度-控制度”（valence-arousal-dominance，VAD ）维度，其中效价表示情绪是积极的还是消极的，唤醒度反映情绪的强度，控制度指用户是否可以控制情绪。
离散模型主要面向分类问题，将输入映射到预定义的类别。维度模型试图在连续空间内对情绪建模，能够提供比离散模型更丰富的情绪表示。因此，这两种情绪模型在理解和建模情绪方面具有互补作用。尽管近期的研究在情绪识别方面取得了很有希望的结果，但仍然存在一个重要问题：大多数情绪识别工作只从输入信号中识别出主导情绪，而现有研究表明，用户可以同时以不同的强度体验两个或更多情绪感受。当前的单一情绪识别研究没有考虑到用户情绪的多样性、复杂性和模糊性。本文针对混合情绪，考虑多种基本情绪的强度，改进了基于离散模型的情绪识别方法。混合情绪识别被视为情绪分布学习（emotion distribution learning，EDL）任务。
本文利用了外显行为的面部视频和生理信号，包括脑电（EEG）、光电容积图（PPG）和皮肤电反应（GSR）。虽然多模态信号可以提供补充的情绪信息，但整合涉及不同神经系统部分（运动皮层、中枢和外周神经系统）的多模态信号使得我们的情绪分布学习任务更具挑战性。本文多模态情绪分布学习的动机如下：
（1）由于多模态信息可以从不同的角度反映混合情绪的多个方面，因此提出将混合情绪分析问题作为多模态分布学习任务来解决。考虑到情绪是用户客观感受，在外显行为和生理表现中反映，本文使用脑电、外周生理信号（即PPG和GSR）和面部视频来进行混合情绪分析。
（2）受“情绪的普遍心理进化理论（General Psychoevolutionary Theory of Emotion）”的启发，该理论指出：（i）存在少数基本的、原始的或典型的情绪；（ii）所有其他情绪都是混合或派生状态，即它们以基本情绪的组合、混合或复合形式出现。
图 2. EmotionDict多模态情绪分布学习方法框架。首先，采用一个情绪字典模块将混合情绪分解为一组基本情绪向量的组合和其权重；然后，基于多模态融合设置辅助任务，为情绪字典提供注意力相关的约束，增强情绪分布学习性能；最后，由分类器预测最终的情绪分布结果。
此外，受到深度学习领域的隐空间特征分解和学习方法的启发，提出使用一组基本隐空间向量及其权重的组合来表示每个情绪分布，即提出了一种EmotionDict的情绪分布学习框架，通过在隐空间中学习由一组基本情绪表示组成的情绪字典，为情绪分布学习高效的情绪特征。最后，多模态分析中最重要的问题之一是对每个模态的重要性进行建模并建立恰当的融合机制，而在混合情绪的情况下更加困难，本文进一步设计了一个多模态集成模块，利用情绪字典的注意力机制辅助模态信息融合。
本文的主要贡献如下：（1）将混合情绪分析问题视为一种多模态信号的分布学习任务，其中包括用户的外显行为和生理信号；通过整合外显行为和生理反应，可以提供补充信息来进行情绪识别。提出了一种端到端的混合情绪识别模型EmotionDict用于情绪分布学习任务，可以融合多模态信号以提高情绪分布学习的性能。（2）受到混合情绪可以由一组基本情绪元素表示的启发，在情绪分布学习方法中设计了一个情绪字典模块，将情绪状态的情绪特征分解为一组基本情绪元素和它们在隐空间中的相关权重的加权组合。（3）设计了两个辅助任务作为显式的监督来约束对情绪字典的注意力，利用多模态信号的特征相关性来帮助提取情绪特征，并进一步改善情绪分布学习的性能。此外，这两个辅助任务通过整合异构模态（行为和生理信号）的一致性和多样性信息，改善了多模态特征融合效率。
图 3. 对比方法与本文方法预测的情绪分布结果示例
图 4. 情绪字典中基本情绪元素数量对情绪分布学习性能的影响。三角形表示数值越小越好，圆形表示数值越大越好。
EmotionDict利用用户的外显行为（面部视频）和生理指标（EEG、PPG和GSR）信息的组合作为输入进行情绪分布学习，以上多模态信号可以表示用户情绪的不同方面。大量实验证明，本文提出的情绪分布学习模型在被试依赖和被试独立的设置上均取得了优越的性能。
图 5. DMER数据集上情绪字典可视化图
—— End ——
仅用于学术分享，若侵权请留言，即时删侵！
加入社群 欢迎加入脑机接口社区交流群，
探讨脑机接口领域话题，实时跟踪脑机接口前沿。
加微信群：
添加微信:RoseBrain【备注：姓名&#43;行业/专业】。
加QQ群：913607986
欢迎来稿 1.欢迎来稿。投稿咨询，请联系微信：RoseBrain
2.加入社区成为兼职创作者，请联系微信：RoseBrain
一键三连「分享」、「点赞」和「在看」
不错每一条脑机前沿进展 ~" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/180fdf484fe6cfc092c217ff4b49ce06/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-27T09:30:58+08:00" />
<meta property="article:modified_time" content="2023-12-27T09:30:58+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">TAFFC | 清华大学刘永进教授课题组提出基于情绪字典与注意力机制的多模态情绪分布学习方法...</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/20/f0/IY9OZ0Ai_o.png" alt="43f9f2dccf495c9af501cdc0d46e19d0.png"></p> 
 <p style="text-align:justify;">与实验室诱发的单一情绪不同，真实世界中人类个体往往在同一时刻展现的多种混合情绪状态，比如悲喜交加、又惊又喜等。清华大学<strong>刘永进</strong>教授课题组提出一种从多模态数据中学习并预测混合情绪的方法，该项研究于2023年被<strong><em>IEEE Transactions on Affective Computing</em></strong>期刊录用，论文的共同第一作者为刘永进教授指导的博士后<strong>刘舫</strong>和博士生<strong>杨培</strong>，刘永进教授指导的博士生<strong>舒叶芷</strong>、访问学者<strong>闫飞</strong>博士、硕士毕业生<strong>张冠华</strong>共同参与了讨论和论文撰写。<em>IEEE Transactions on Affective Computing</em>是国际上情感计算领域内的著名期刊，SCI影响因子为11.2。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/c9/56/ltEAlxyN_o.png" alt="0b1b33bc201fc9c2b592a2d4bd598412.png"></p> 
 <p style="text-align:justify;">全文官方链接：</p> 
 <p style="text-align:justify;">https://doi.org/10.1109/TAFFC.2023.3334520</p> 
 <p style="text-align:justify;">多模态情绪分析作为情绪计算的一个重要方向，近年来受到越来越多的关注。大多数现有的多模态情绪识别研究针对的是一个分类任务，目的是为几种不同模态的输入数据（包括多媒体信号和生理信号）的组合分配一个特定的情绪类别。与单一情绪识别相比，越来越多的心理学证据表明，不同的离散情绪可能同时存在，这促进了混合情绪识别研究的发展，即识别基本情绪的混合。</p> 
 <p style="text-align:justify;">虽然当前的大多数研究将其视为一个多标签分类任务，但在这项工作中，本文关注同时出现积极和消极情绪的挑战性情况，并提出了一种多模态混合情绪识别框架（EmotionDict）。EmotionDict的主要特点包括：</p> 
 <p style="text-align:justify;">（1）受心理学研究的启发，混合状态可以用基本情绪的组合来表示，将混合情绪识别视为标签分布学习任务。设计情绪字典（Emotion Dictionary）作为在共享的隐空间中的情绪表征。</p> 
 <p style="text-align:justify;">（2）虽然大多数多模态情绪分布研究建立在多媒体信号（如文本、图像、音频和视频）的基础上，但本文使用生理和显式行为多模态信号，包括脑电（EEG）、外周生理信号和面部视频。多模态信号与中央或外周神经系统以及运动皮层有关，各种模态具有不同的特点。</p> 
 <p style="text-align:justify;">（3）本文进一步设计了辅助任务来学习各模态的注意力进行多模态融合。在两个数据集上的实验结果表明，本文的方法在混合情绪识别方面超过了现有的最先进的方法。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/23/7a/GcGrKUvZ_o.png" alt="5d9b3378457f80c959a3f2cf100776b6.png"></p> 
 <p style="text-align:center;">图 1.多模态情绪分布学习任务示例。</p> 
 <p style="text-align:justify;">情绪识别已经成为情绪计算领域的一个重要话题，不仅由于它是许多下游任务和应用（例如，媒体分析任务、人机交互和心理治疗）的基础，而且因为情绪在人的心理状态中起着关键作用。目前学术研究中情绪空间主要由两种模型描述：</p> 
 <p style="text-align:justify;"><strong>（1）离散模型</strong>，它将情绪状态映射到一组基本情绪类别，如快乐、悲伤、惊讶、恐惧、愤怒和厌恶；</p> 
 <p style="text-align:justify;"><strong>（2）维度模型</strong>，它将空间分为“效价-唤醒度”（valence-arousal，VA）维度或“效价-唤醒度-控制度”（valence-arousal-dominance，VAD ）维度，其中效价表示情绪是积极的还是消极的，唤醒度反映情绪的强度，控制度指用户是否可以控制情绪。</p> 
 <p style="text-align:justify;">离散模型主要面向分类问题，将输入映射到预定义的类别。维度模型试图在连续空间内对情绪建模，能够提供比离散模型更丰富的情绪表示。因此，这两种情绪模型在理解和建模情绪方面具有互补作用。尽管近期的研究在情绪识别方面取得了很有希望的结果，但仍然存在一个重要问题：大多数情绪识别工作只从输入信号中识别出主导情绪，而现有研究表明，用户可以同时以不同的强度体验两个或更多情绪感受。当前的单一情绪识别研究没有考虑到用户情绪的多样性、复杂性和模糊性。本文针对混合情绪，考虑多种基本情绪的强度，改进了基于离散模型的情绪识别方法。混合情绪识别被视为情绪分布学习（emotion distribution learning，EDL）任务。</p> 
 <p style="text-align:justify;">本文利用了外显行为的面部视频和生理信号，包括脑电（EEG）、光电容积图（PPG）和皮肤电反应（GSR）。虽然多模态信号可以提供补充的情绪信息，但整合涉及不同神经系统部分（运动皮层、中枢和外周神经系统）的多模态信号使得我们的情绪分布学习任务更具挑战性。本文多模态情绪分布学习的动机如下：</p> 
 <p style="text-align:justify;">（1）由于多模态信息可以从不同的角度反映混合情绪的多个方面，因此提出将混合情绪分析问题作为多模态分布学习任务来解决。考虑到情绪是用户客观感受，在外显行为和生理表现中反映，本文使用脑电、外周生理信号（即PPG和GSR）和面部视频来进行混合情绪分析。</p> 
 <p style="text-align:justify;">（2）受“情绪的普遍心理进化理论（General Psychoevolutionary Theory of Emotion）”的启发，该理论指出：（i）存在少数基本的、原始的或典型的情绪；（ii）所有其他情绪都是混合或派生状态，即它们以基本情绪的组合、混合或复合形式出现。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/f6/13/EiAAcLeP_o.png" alt="2592027f9c3bb72a745878053ab69de9.png"></p> 
 <p style="text-align:left;">图 2. EmotionDict多模态情绪分布学习方法框架。首先，采用一个情绪字典模块将混合情绪分解为一组基本情绪向量的组合和其权重；然后，基于多模态融合设置辅助任务，为情绪字典提供注意力相关的约束，增强情绪分布学习性能；最后，由分类器预测最终的情绪分布结果。</p> 
 <p style="text-align:justify;">此外，受到深度学习领域的隐空间特征分解和学习方法的启发，提出使用一组基本隐空间向量及其权重的组合来表示每个情绪分布，即提出了一种EmotionDict的情绪分布学习框架，通过在隐空间中学习由一组基本情绪表示组成的情绪字典，为情绪分布学习高效的情绪特征。最后，多模态分析中最重要的问题之一是对每个模态的重要性进行建模并建立恰当的融合机制，而在混合情绪的情况下更加困难，本文进一步设计了一个多模态集成模块，利用情绪字典的注意力机制辅助模态信息融合。</p> 
 <p style="text-align:justify;">本文的主要贡献如下：（1）将混合情绪分析问题视为一种多模态信号的分布学习任务，其中包括用户的外显行为和生理信号；通过整合外显行为和生理反应，可以提供补充信息来进行情绪识别。提出了一种端到端的混合情绪识别模型EmotionDict用于情绪分布学习任务，可以融合多模态信号以提高情绪分布学习的性能。（2）受到混合情绪可以由一组基本情绪元素表示的启发，在情绪分布学习方法中设计了一个情绪字典模块，将情绪状态的情绪特征分解为一组基本情绪元素和它们在隐空间中的相关权重的加权组合。（3）设计了两个辅助任务作为显式的监督来约束对情绪字典的注意力，利用多模态信号的特征相关性来帮助提取情绪特征，并进一步改善情绪分布学习的性能。此外，这两个辅助任务通过整合异构模态（行为和生理信号）的一致性和多样性信息，改善了多模态特征融合效率。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/ab/e4/fxUuHySp_o.png" alt="33019500622110606bf0282e5fea70e9.png"></p> 
 <p style="text-align:center;">图 3. 对比方法与本文方法预测的情绪分布结果示例</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/f2/26/0SyZLhX1_o.png" alt="ee57f9b705612bc8589b2c6c7b197fd3.png"></p> 
 <p style="text-align:left;">图 4. 情绪字典中基本情绪元素数量对情绪分布学习性能的影响。三角形表示数值越小越好，圆形表示数值越大越好。</p> 
 <p style="text-align:justify;">EmotionDict利用用户的外显行为（面部视频）和生理指标（EEG、PPG和GSR）信息的组合作为输入进行情绪分布学习，以上多模态信号可以表示用户情绪的不同方面。大量实验证明，本文提出的情绪分布学习模型在被试依赖和被试独立的设置上均取得了优越的性能。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/37/78/y367JXtd_o.png" alt="b6f4ef82862a1f367e4d8cd034828256.png"></p> 
 <p style="text-align:center;">图 5. DMER数据集上情绪字典可视化图</p> 
 <p>—— End ——</p> 
 <p style="text-align:center;">仅用于学术分享，若侵权请留言，即时删侵！</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/3d/8f/ODK8kIm3_o.png" alt="51c3fc50b20deee78c4b0be118874bd4.png"></p> 
 <p style="text-align:center;">   加入社群  </p> 
 <p>欢迎加入脑机接口社区交流群，</p> 
 <p>探讨脑机接口领域话题，实时跟踪脑机接口前沿。</p> 
 <p>加微信群：</p> 
 <p>添加微信:RoseBrain【备注：姓名+行业/专业】。</p> 
 <p>加QQ群：913607986</p> 
 <p style="text-align:center;">  欢迎来稿  </p> 
 <p>1.欢迎来稿。投稿咨询，请联系微信：RoseBrain</p> 
 <p>2.加入社区成为兼职创作者，请联系微信：RoseBrain</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/50/7f/SHfRWXzh_o.jpg" alt="b6e48f55b5e2ea07e67a8a29abd62948.jpeg"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/9c/1e/CvPYHQEl_o.jpg" alt="b53f89d3ecad94de2acd81973fdda294.jpeg"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/0c/e8/5xf8Anmt_o.png" alt="0e55f8368cc4673d63aced06c09f947e.png"></p> 
 <p><strong>一键三连「分享」、「点赞」和「在看」</strong></p> 
 <p><strong>不错每一条脑机前沿进展 ~</strong></p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/165225deedc792531b9cdf912a3a67d1/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">机器学习系列--R语言随机森林进行生存分析（1）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/01c3dd50ede27766a331ae7ea9fa5e29/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">CAT: LoCalization and IdentiﬁcAtion Cascade Detection Transformer for Open-World Object Detection</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>