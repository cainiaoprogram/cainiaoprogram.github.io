<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>LongLLaMA：LLaMA的升级版，处理超长上下文的利器！ - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="LongLLaMA：LLaMA的升级版，处理超长上下文的利器！" />
<meta property="og:description" content="原文来源：芝士AI吃鱼
有效整合新知识：大模型面临的挑战
大家使用过大型模型产品的时候可能会遇到一个共同的问题：在进行多轮对话时，模型可能会忘记之前的对话内容，导致回答不连贯。这实际上是由于大型模型在处理大量新内容时有时会力不从心，给人一种分心的感觉。
这个问题实际上也是当前大型模型面临的一个主要挑战，即如何有效地将大量新知识整合到模型中。目前常见的解决方法之一是微调（fine-tune），但这种方法不仅需要大量资源和复杂的流程，而且并不能总是清晰地指导模型如何整合新知识。例如，对《爱丽丝梦游仙境》这样的文本进行微调，不能让模型回答与故事本身相关的问题，只能让模型预测下一个词或者补全句子。这种情况让人感到很沮丧。
整合新知识的替代方法：上下文整合
除了微调外，另一种有效的替代方法是将新知识整合到上下文中，而无需进行模型训练。然而，这种方法受到模型上下文长度的限制。为了处理大型知识数据库，模型需要将上下文长度扩展到数百万个标记，但这在现实中是不可行的。即使是强大的GPT-4模型，其上下文长度也只有32K。
谷歌DeepMind研究团队最近提出了一种名为&#34;Focused Transformer&#34;（FoT）的注意力集中的Transformer架构，旨在解决大型模型的分心问题。他们使用FoT对LLaMA模型进行微调，从而获得了名为LongLLaMA的模型，其架构与LLaMA相同。通过解决大型模型的分心问题，LongLLaMA显著提高了模型的上下文长度，并且在passkey检索任务中甚至可以扩展到256K长度的上下文。更重要的是，LongLLaMA对标准Transformer的改动非常小，可以无缝切换到其他任务的大型语言模型。
Focused Transformer（FoT）是一种注重注意力集中的Transformer架构。它通过优化模型的注意力机制，帮助模型更好地处理大量新知识和上下文信息。这项创新技术的引入使得模型能够有效整合新知识，提高对话的连贯性。
在FoT的基础上，研究团队对LLaMA模型进行了微调，得到了LongLLaMA模型。与LLaMA相比，LongLLaMA在处理上下文长度方面取得了显著的改进。在passkey检索任务中，LongLLaMA能够处理长达256K的上下文信息，这在过去是难以实现的。
LongLLaMA-3B模型在上下文长度为100k时准确率达到94.5％，在上下文长度达到256k的准确率为73％，而标准的LLaMA-3B模型在上下文长度2k时准确率接近于0，已基本不可用了。
FoT的实现方法很简单，它实际和谷歌2022年提出的一个内存加大版的transormer—Memoryrizing Transormer非常相似，后面我们也会详细讨论FoT和它的区别。FoT额外使用了一块较大的内存来存储历史信息的key-value对，然后借鉴了对比学习的思想在训练阶段中使用跨批次训练（cross-btach）将大量历史信息融入到样本中以增强key-value对的空间结构，这样模型就能对更加专注在和当前问题非常相关的历史信息中。
目前LongLLaMA模型的代码和权重已经公布在github和Hugging Face：
论文链接:
https://arxiv.org/pdf/2307.03170.pdf
项目地址：
https://github.com/CStanKonrad/long_llama
Hugging Face：
https://huggingface.co/syzymon/long_llama_3b" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/3f43a19ac1cbd8c28d3aafb81ec2cf3c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-10T19:50:04+08:00" />
<meta property="article:modified_time" content="2023-07-10T19:50:04+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">LongLLaMA：LLaMA的升级版，处理超长上下文的利器！</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <blockquote> 
 <p>原文来源：<a class="link-info" href="https://mp.weixin.qq.com/s?__biz=MzIxMjY3NzMwNw==&amp;mid=2247483834&amp;idx=1&amp;sn=df732f080971d9811766711f9d619ea4&amp;chksm=9743266ea034af782dd601be048f42eda81e2bd1d184a91a24a236422e01b7f62769d39bf6f2&amp;token=555516528&amp;lang=zh_CN#rd" rel="nofollow" title="芝士AI吃鱼">芝士AI吃鱼</a></p> 
</blockquote> 
<p><strong>有效整合新知识：大模型面临的挑战</strong></p> 
<p>大家使用过大型模型产品的时候可能会遇到一个共同的问题：在进行多轮对话时，模型可能会忘记之前的对话内容，导致回答不连贯。这实际上是由于大型模型在处理大量新内容时有时会力不从心，给人一种分心的感觉。</p> 
<p>这个问题实际上也是当前大型模型面临的一个主要挑战，即如何有效地将大量新知识整合到模型中。目前常见的解决方法之一是微调（fine-tune），但这种方法不仅需要大量资源和复杂的流程，而且并不能总是清晰地指导模型如何整合新知识。例如，对《爱丽丝梦游仙境》这样的文本进行微调，不能让模型回答与故事本身相关的问题，只能让模型预测下一个词或者补全句子。这种情况让人感到很沮丧。</p> 
<p><strong>整合新知识的替代方法：上下文整合</strong></p> 
<p>除了微调外，另一种有效的替代方法是将新知识整合到上下文中，而无需进行模型训练。然而，这种方法受到模型上下文长度的限制。为了处理大型知识数据库，模型需要将上下文长度扩展到数百万个标记，但这在现实中是不可行的。即使是强大的GPT-4模型，其上下文长度也只有32K。</p> 
<p>谷歌DeepMind研究团队最近提出了一种名为"Focused Transformer"（FoT）的注意力集中的Transformer架构，旨在解决大型模型的分心问题。他们使用FoT对LLaMA模型进行微调，从而获得了名为LongLLaMA的模型，其架构与LLaMA相同。通过解决大型模型的分心问题，LongLLaMA显著提高了模型的上下文长度，并且在passkey检索任务中甚至可以扩展到256K长度的上下文。更重要的是，LongLLaMA对标准Transformer的改动非常小，可以无缝切换到其他任务的大型语言模型。</p> 
<p>Focused Transformer（FoT）是一种注重注意力集中的Transformer架构。它通过优化模型的注意力机制，帮助模型更好地处理大量新知识和上下文信息。这项创新技术的引入使得模型能够有效整合新知识，提高对话的连贯性。</p> 
<p>在FoT的基础上，研究团队对LLaMA模型进行了微调，得到了LongLLaMA模型。与LLaMA相比，LongLLaMA在处理上下文长度方面取得了显著的改进。在passkey检索任务中，LongLLaMA能够处理长达256K的上下文信息，这在过去是难以实现的。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/9c/03/wJGQW70Q_o.png"></p> 
<p>LongLLaMA-3B模型在上下文长度为100k时准确率达到94.5％，在上下文长度达到256k的准确率为73％，而标准的LLaMA-3B模型在上下文长度2k时准确率接近于0，已基本不可用了。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/55/ee/a6J1MA2u_o.png"></p> 
<p>FoT的实现方法很简单，它实际和谷歌2022年提出的一个内存加大版的transormer—<strong>Memoryrizing Transormer</strong>非常相似，后面我们也会详细讨论FoT和它的区别。FoT额外使用了一块较大的内存来存储历史信息的key-value对，然后借鉴了对比学习的思想在训练阶段中使用跨批次训练（cross-btach）将大量历史信息融入到样本中以增强key-value对的空间结构，这样模型就能对更加专注在和当前问题非常相关的历史信息中。</p> 
<p>目前LongLLaMA模型的代码和权重已经公布在github和Hugging Face：</p> 
<p><strong>论文链接</strong>:<br><em>https://arxiv.org/pdf/2307.03170.pdf</em><br><strong>项目地址：</strong><br><em>https://github.com/CStanKonrad/long_llama</em><br><strong>Hugging Face：</strong><br><em>https://huggingface.co/syzymon/long_llama_3b</em></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/42f62905f92cbb6c73784f268e79cfb6/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Windows VScode如何配置与使用git？</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/5edf39d9c549c5e79cacf473fa64e132/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">YOLOV8原理和实现全解析</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>