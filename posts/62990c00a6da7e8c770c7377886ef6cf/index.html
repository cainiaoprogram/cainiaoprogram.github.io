<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Better to Follow, Follow to Be Better: Towards Precise Supervision of Feature Super-Resolution - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Better to Follow, Follow to Be Better: Towards Precise Supervision of Feature Super-Resolution" />
<meta property="og:description" content="参考 Towards Precise Supervision of Feature Super-Resolution - 云&#43;社区 - 腾讯云
摘要 虽然最近基于区域建议的CNN模型在目标检测方面取得了成功，但是由于小兴趣区域(small Region of Interest, RoI)所包含的信息有限且失真，小目标的检测仍然比较困难。解决这一问题的一种方法是使用超分辨率(SR)技术来增强小型RoI的特性。我们研究如何提高级的超分辨率特别是对小目标检测,并发现它的性能可以显著提高通过：
(1)、利用适当的高分辨率目标特性作为SR的训练监督信号模型。
(2)、匹配输入的相对接受训练领域对低分辨率的特性和目标高分辨率特性。
我们提出了一种新的特征级超分辨率方法，它不仅能正确地解决这两个问题，而且可以与任何基于特征池的检测器集成。在我们的实验中，我们的方法显著提高了Faster R-CNN在清华-腾讯100K、PASCAL VOC和MS COCO三个基准上的性能。对于小目标的改进是非常大的，令人鼓舞的是，对于中、大目标的改进也不是微不足道的。因此，我们在Tsinghua-Tencent 100K上取得了最新的技术水平，在PASCAL VOC和MS COCO上取得了极具竞争力的成绩。
1、简介 自深度卷积神经网络(CNN)出现以来，目标检测方法的性能迅速提高。目前主要有两种方法：基于两阶段建议的模型具有准确性优势，而基于单阶段建议的模型具有速度优势。尽管近年来在目标检测方面取得了巨大的进展，但在某些特定条件下，如小、闭塞或截断，仍然很难检测到目标。在这项工作中，我们的重点是改进小目标检测在基于建议的检测框架，如Faster R-CNN。
基于建议的检测器从根本上存在小目标区域建议太小无法识别的问题。例如，Huang等人表明，小目标的平均精度(mAP)分数大约比大目标低10倍。对于小的建议，感兴趣区域(RoI)池层通常提取复制的特征向量作为box预测器的输入，而box预测器最终在对小目标没有足够详细信息的情况下进行预测。此外，很可能RoI池池化的位置和它在图像中的实际位置是不匹配的。通过RoI align和和PrRoI pooling等先进的池化技术，可以部分缓解这种RoI池的失真。但是，它们没有提供框预测器可以用来更好地检测小目标的附加信息。
为了使得小建议的信息丰富，超分超分辨率是一个很好的方法。由于超分辨整个图像的效率很低，Bai等人提出将小建议的图像像素超分辨为类似于大建议的像素。但是，它的RoI超分辨率无法考虑上下文信息，因为它只关注RoI。该方法利用上下文信息作为建议的特征，通过大范围的连续卷积操作来提取建议的特征。特别是，Perceptual GAN利用生成对抗网络(GAN)对建议特征进行超分辨，提高了对小目标的检测精度。
然而，现有的用于小目标检测的特征级超分辨率模型存在一个明显的局限性:缺乏直接的监督。也就是说，它们的超分辨率模型训练没有明确的目标特征，导致训练不稳定，限制了超分辨率特征的质量。在图像检索任务中，Tan等人表明，低分辨率和高分辨率特征对之间的特征-内容丢失导致了更好的超分辨率特征，具有更快的收敛速度。
对更好的训练不仅是重要构造适当的高分辨率特性为目标,我们的分析还表明,这至关重要的比赛之间的相对接受字段对,尤其是对小RoI(图1)。也就是说，在的图像检索任务只考虑整体图像的特点,相对接受字段不多对高、低分辨率之间的不同特征。另一方面，对于目标检测任务中常见的小RoI，差异非常大，导致小建议的超分辨率质量较差。
在这方面，这项工作的贡献有三个方面:
(1)、我们深入研究了现有的特征级超分辨率小目标检测方法，发现：(i)、利用高分辨率目标特征作为监督信号。(ii)、匹配输入与目标特征的相对接受域，显著提高了性能。
(2)、我们提出了一种新的特征级超分辨方法，该方法可以垂直地应用于任何基于特征池的检测器。它充分利用了新目标提取器创建的高分辨率目标特征的直接监督，利用不需要额外参数的卷积，因为它与基础检测器的CNN主干共享参数。此外，我们提出了一种迭代细化生成器作为超分辨特征的新方法。
(3)、利用ResNet-50、ResNet-101、MobileNet等多种CNN骨干，在Tsinghua-Tencent 100K、PASCAL VOC、MS COCO三个基准数据集上，显著提高了Fast R-CNN小目标检测性能。对于小目标的改进是非常大的，令人鼓舞的是，对于中、大目标的改进也不是微不足道的。因此，我们在清华-腾讯100K上取得了最新的技术水平，在PASCAL VOC和MS COCO上取得了极具竞争力的成绩。
2、相关工作 综述了小目标检测的三个主要研究方向。
高分辨率图像：小目标检测的一种直接方法是生成高分辨率图像作为检测模型的输入。Hu等人使用双线性插值获得两次上采样的输入图像，Fookes等人使用传统的超分辨率技术来更好地识别人脸。然而，像级超分辨率还存在两个潜在的问题。首先，超分辨率和检测模型通常是独立训练的；超分辨率模型被训练成生成高分辨率的图像，即使是对于那些由于其独立性而对检测不重要的部分。其次，整体架构可能过于沉重，因为它采用放大的超分辨率图像作为输入，这可能会大大增加推断时间。虽然Haris等人提出了一种端到端联合训练超分辨率和检测模型的模型，但是对与检测任务无关的大量图像执行超分辨率仍然是低效的。SOD-MTGAN不是对整个图像进行超分辨，而是先将RoI池化，然后使用这些池化的RoI训练超分辨模型。虽然他们的工作通过只关注RoI来解决这两个问题，但仍然没有考虑RoI的上下文信息。
高分辨率特征：感知GAN是一种显著的特征级超分辨率小目标检测方法。由于它只关注RoI的特性，所以不存在图像级超分辨率的两个问题。此外，由于这些特征是通过与较大的接收域卷积来提取的，所以SOD-MTGAN的问题也得到了缓解。然而，由于缺乏直接监督，其超分辨率训练可能不稳定;没有训练低分辨率RoI特征对及其对应的高分辨率特征对。相反，它隐含地利用了分类、本地化和对抗性损失。对于图像检索任务，Tan等人在训练特征级超分辨率模型时加入了特征级损失。他们报告说，增加这种更强的约束有助于生成网络产生更快收敛的更好的特征。然而，我们观察到在[34]中这样的直接监督对目标检测是不够的，因为它可能会由于高分辨率和低分辨率特征之间的相对接受域不匹配而误导超分辨率过程。在第3节中，我们将进一步阐述这个问题。
纹理信息：许多研究已经经验证明，上下文信息也有助于检测小目标。如[27]所示，CNNs中来自顶层的特征足够捕获大目标，但太粗糙，无法检测小目标；而来自底层的特征包含的局部信息太具体，对检测大目标没有帮助，但对检测小目标有帮助。因此，许多方法使用额外的层来从多个层构建上下文特性。使用上下文的另一个简单方法是在RoI集中时也考虑附近的区域。Hu等人利用提取周围区域和ROI来检测人脸，因为知道附近区域存在人体是有帮助的。还研究了目标之间的关系信息，以增强检测模型。最后，一些研究建议使用卷积和非卷积层的混合来更好地分割小目标，因为非卷积层覆盖了更大的接收域，而不会丢失分辨率。由于这一特性，我们也使用了卷积层来匹配高分辨率和低分辨率特征之间的相对接受域。第3节提供了更详细的解释。
3、相对感受野不匹配 在这一节中，我们将讨论为什么匹配相对接受域对于获得足够的低分辨率输入特征和高分辨率目标特征非常重要。在此基础上，我们提出了一种新的超分辨率目标提取器。
一种直接的方法是从原始图像中获取较大的ROI，从下采样图像特征超分中获取较小的ROI。不幸的是，就相对接受域而言，这些配对的特征并不完全匹配。为了清楚地了解为什么会出现这种差异，我们在图2中给出了一个带有符号的直观示例。为便于讨论，仅考虑一个水平轴，宽度为w的RoI特征的绝对感受野(ARF)为：
相对感受野(RRF)定义为相对于图像大小的ARF：
让我们讨论输入尺寸如何影响输入尺寸。在0.5倍的下采样输入图像中，图像的宽度为/2，而特征图上的ROI宽度为w/2。我们将ROI在原始图像和下采样图像之间的差异定义为：
其中为常数。式(3)很容易从式(2)推导出来。
根据式(3)，当趋近于0时，DRRF收敛于2，当增大时，DRRF收敛于1。也就是说，对于小的RoI，相同RoI的相对接受域(RRF)可以是原始图像和下采样图像的差值的2倍。另一方面，如果建议的规模足够大，那么RRFs也会变得相似。
例如，对于输入图像的ROI ，，如果我们使用Faster R-CNN和ResNet-50主干网，其中= 291, = 16，那么接近1.8。也就是说，下采样图像的RoI的RRF大约是原始图像的1.8倍。Tan等人使用超分特征处理整个图像特征被超分辨的图像检索任务，因此RRF中的差异不显著。相反，对于我们工作中检测的小ROI的超分辨率，RRF的差异非常大，会严重误导超分辨率模型。
4、本文的方法 提出了一种基于两个关键思想的小目标检测特征超分辨方法：
(1)、直接监督超分辨发生器;
(2)、通过卷积进行感受野匹配。
在基本检测模型的基础上，我们引入了四个附加组件：SR特征发生器和鉴别器、SR目标提取器和小预测器。SR特征发生器作为一种基于GAN的模型，利用SR目标提取器的特征作为目标，在SR特征鉴别器的引导下产生高分辨率的特征。此外，小的预测器是基本检测器中预测器的复制，我们称之为大预测器。大预测器对大建议进行分类和定位的置信度的计算与普通检测器相同，而小预测器对小建议执行相同的任务，这些小建议首先由SR特征生成器进行增强。我们为小型建议设定的门槛为：清华-腾讯(Tsinghua-Tencent)为32×32,VOC和COCO数据集为96×96。图3显示了我们模型的整体架构。我们解释了基于Faster R-CNN的模型，尽管我们的方法可以与任何基于特征池的提议检测器集成。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/62990c00a6da7e8c770c7377886ef6cf/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-06-16T22:07:24+08:00" />
<meta property="article:modified_time" content="2022-06-16T22:07:24+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Better to Follow, Follow to Be Better: Towards Precise Supervision of Feature Super-Resolution</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p style="margin-left:.0001pt;text-align:justify;"><strong>参考</strong><strong> <a href="https://cloud.tencent.com/developer/article/1538309" rel="nofollow" title="Towards Precise Supervision of Feature Super-Resolution - 云+社区 - 腾讯云">Towards Precise Supervision of Feature Super-Resolution - 云+社区 - 腾讯云</a></strong></p> 
<h2></h2> 
<h2>摘要</h2> 
<p><span style="color:#e579b6;"><strong>虽然最近基于区域建议的CNN模型在目标检测方面取得了成功，但是由于小兴趣区域(small Region of Interest, RoI)所包含的信息有限且失真，小目标的检测仍然比较困难。</strong></span>解决这一问题的一种方法是使用超分辨率(SR)技术来增强小型<span style="color:#e579b6;"><strong>RoI</strong></span>的特性。我们研究如何提高级的超分辨率特别是对小目标检测,并发现它的性能可以显著提高通过：</p> 
<p>(1)、利用适当的高分辨率目标特性作为SR的训练监督信号模型。</p> 
<p>(2)、匹配输入的相对接受训练领域对低分辨率的特性和目标高分辨率特性。</p> 
<p>我们提出了一种新的特征级超分辨率方法，它不仅能正确地解决这两个问题，而且可以与任何基于特征池的检测器集成。在我们的实验中，我们的方法显著提高了Faster R-CNN在清华-腾讯100K、PASCAL VOC和MS COCO三个基准上的性能。对于小目标的改进是非常大的，令人鼓舞的是，对于中、大目标的改进也不是微不足道的。因此，我们在Tsinghua-Tencent 100K上取得了最新的技术水平，在PASCAL VOC和MS COCO上取得了极具竞争力的成绩。</p> 
<h2>1、简介</h2> 
<p>自深度卷积神经网络(CNN)出现以来，目标检测方法的性能迅速提高。目前主要有两种方法：基于两阶段建议的模型具有准确性优势，而基于单阶段建议的模型具有速度优势。尽管近年来在目标检测方面取得了巨大的进展，但在某些特定条件下，如小、闭塞或截断，仍然很难检测到目标。在这项工作中，我们的重点是改进小目标检测在基于建议的检测框架，如Faster R-CNN。</p> 
<p>基于建议的检测器从根本上存在小目标区域建议太小无法识别的问题。例如，Huang等人表明，小目标的平均精度(mAP)分数大约比大目标低10倍。<span style="color:#e579b6;"><strong>对于小的建议，感兴趣区域(RoI)池层通常提取复制的特征向量作为box预测器的输入，而box预测器最终在对小目标没有足够详细信息的情况下进行预测。</strong></span>此外，很可能RoI池池化的位置和它在图像中的实际位置是不匹配的。通过RoI align和和PrRoI pooling等先进的池化技术，可以部分缓解这种RoI池的失真。但是，它们没有提供框预测器可以用来更好地检测小目标的附加信息。</p> 
<p>为了使得小建议的信息丰富，超分超分辨率是一个很好的方法。由于超分辨整个图像的效率很低，Bai等人提出将小建议的图像像素超分辨为类似于大建议的像素。但是，它的RoI超分辨率无法考虑上下文信息，因为它只关注RoI。该方法利用上下文信息作为建议的特征，通过大范围的连续卷积操作来提取建议的特征。特别是，Perceptual GAN利用生成对抗网络(GAN)对建议特征进行超分辨，提高了对小目标的检测精度。</p> 
<p><span style="color:#e579b6;"><strong>然而，现有的用于小目标检测的特征级超分辨率模型存在一个明显的局限性:缺乏直接的监督。</strong></span>也就是说，它们的超分辨率模型训练没有明确的目标特征，导致训练不稳定，限制了超分辨率特征的质量。在图像检索任务中，Tan等人表明，低分辨率和高分辨率特征对之间的特征-内容丢失导致了更好的超分辨率特征，具有更快的收敛速度。</p> 
<p>对更好的训练不仅是重要构造适当的高分辨率特性为目标,我们的分析还表明,这至关重要的比赛之间的相对接受字段对,尤其是对小RoI(图1)。也就是说，在的图像检索任务只考虑整体图像的特点,相对接受字段不多对高、低分辨率之间的不同特征。另一方面，对于目标检测任务中常见的小RoI，差异非常大，导致小建议的超分辨率质量较差。</p> 
<p>                                 <img alt="" class="has" height="599" src="https://images2.imgbox.com/0f/5b/kWH2TnJP_o.png" width="499"></p> 
<p>在这方面，这项工作的贡献有三个方面:</p> 
<p><span style="color:#333333;">(1)、我们深入研究了现有的特征级超分辨率小目标检测方法，发现：(i)、利用高分辨率目标特征作为监督信号。(ii)、匹配输入与目标特征的相对接受域，显著提高了性能。</span></p> 
<p>(2)、我们提出了一种新的特征级超分辨方法，该方法可以垂直地应用于任何基于特征池的检测器。它充分利用了新目标提取器创建的高分辨率目标特征的直接监督，利用不需要额外参数的卷积，因为它与基础检测器的CNN主干共享参数。此外，我们提出了一种迭代细化生成器作为超分辨特征的新方法。</p> 
<p>(3)、利用ResNet-50、ResNet-101、MobileNet等多种CNN骨干，在Tsinghua-Tencent 100K、PASCAL VOC、MS COCO三个基准数据集上，显著提高了Fast R-CNN小目标检测性能。对于小目标的改进是非常大的，令人鼓舞的是，对于中、大目标的改进也不是微不足道的。因此，我们在清华-腾讯100K上取得了最新的技术水平，在PASCAL VOC和MS COCO上取得了极具竞争力的成绩。</p> 
<h2>2、相关工作</h2> 
<p>综述了小目标检测的三个主要研究方向。</p> 
<p><strong>高分辨率图像：</strong>小目标检测的一种直接方法是生成高分辨率图像作为检测模型的输入。Hu等人使用双线性插值获得两次上采样的输入图像，Fookes等人使用传统的超分辨率技术来更好地识别人脸。然而，像级超分辨率还存在两个潜在的问题。<span style="color:#e579b6;"><strong>首先，超分辨率和检测模型通常是独立训练的；超分辨率模型被训练成生成高分辨率的图像，即使是对于那些由于其独立性而对检测不重要的部分。其次，整体架构可能过于沉重，因为它采用放大的超分辨率图像作为输入，这可能会大大增加推断时间。</strong></span>虽然Haris等人提出了一种端到端联合训练超分辨率和检测模型的模型，但是对与检测任务无关的大量图像执行超分辨率仍然是低效的。SOD-MTGAN不是对整个图像进行超分辨，而是先将RoI池化，然后使用这些池化的RoI训练超分辨模型。虽然他们的工作通过只关注RoI来解决这两个问题，但仍然没有考虑RoI的上下文信息。</p> 
<p><strong>高分辨率特征：</strong>感知GAN是一种显著的特征级超分辨率小目标检测方法。由于它只关注RoI的特性，所以不存在图像级超分辨率的两个问题。此外，由于这些特征是通过与较大的接收域卷积来提取的，所以SOD-MTGAN的问题也得到了缓解。<span style="color:#e579b6;"><strong>然而，由于缺乏直接监督，其超分辨率训练可能不稳定;没有训练低分辨率RoI特征对及其对应的高分辨率特征对。相反，它隐含地利用了分类、本地化和对抗性损失。</strong></span>对于图像检索任务，Tan等人在训练特征级超分辨率模型时加入了特征级<img alt="L_2" class="mathcode" src="https://images2.imgbox.com/90/08/KeQ6R4M3_o.png">损失。他们报告说，增加这种更强的约束有助于生成网络产生更快收敛的更好的特征。然而，我们观察到在[34]中这样的直接监督对目标检测是不够的，因为它可能会由于高分辨率和低分辨率特征之间的相对接受域不匹配而误导超分辨率过程。在第3节中，我们将进一步阐述这个问题。</p> 
<p><strong>纹理信息：</strong>许多研究已经经验证明，上下文信息也有助于检测小目标。如[27]所示，CNNs中来自顶层的特征足够捕获大目标，但太粗糙，无法检测小目标；而来自底层的特征包含的局部信息太具体，对检测大目标没有帮助，但对检测小目标有帮助。因此，许多方法使用额外的层来从多个层构建上下文特性。使用上下文的另一个简单方法是在RoI集中时也考虑附近的区域。Hu等人利用提取周围区域和ROI来检测人脸，因为知道附近区域存在人体是有帮助的。还研究了目标之间的关系信息，以增强检测模型。最后，一些研究建议使用卷积和非卷积层的混合来更好地分割小目标，因为非卷积层覆盖了更大的接收域，而不会丢失分辨率。由于这一特性，我们也使用了卷积层来匹配高分辨率和低分辨率特征之间的相对接受域。第3节提供了更详细的解释。</p> 
<h2>3、相对感受野不匹配</h2> 
<p>在这一节中，我们将讨论为什么匹配相对接受域对于获得足够的低分辨率输入特征和高分辨率目标特征非常重要。在此基础上，我们提出了一种新的超分辨率目标提取器。</p> 
<p>一种直接的方法是从原始图像中获取较大的ROI，从下采样图像特征超分中获取较小的ROI。不幸的是，就相对接受域而言，这些配对的特征并不完全匹配。为了清楚地了解为什么会出现这种差异，我们在图2中给出了一个带有符号的直观示例。为便于讨论，仅考虑一个水平轴，宽度为w的RoI特征的绝对感受野(ARF)为：</p> 
<p>                                       <img alt="" class="has" height="37" src="https://images2.imgbox.com/14/ff/lwbt5iie_o.png" width="327"></p> 
<p>相对感受野(RRF)定义为相对于图像<img alt="I_W" class="mathcode" height="15" src="https://images2.imgbox.com/bc/48/bF5O7qkz_o.gif" width="20">大小的ARF：</p> 
<p>                                      <img alt="" class="has" height="36" src="https://images2.imgbox.com/15/ef/mdjLV08f_o.png" width="359"></p> 
<p>让我们讨论输入尺寸如何影响输入尺寸。在0.5倍的下采样输入图像中，图像的宽度为<img alt="I_W" class="mathcode" height="15" src="https://images2.imgbox.com/f4/20/yatPRW1V_o.gif" width="20">/2，而特征图上的ROI宽度为w/2。我们将ROI在原始图像和下采样图像之间的差异定义为：</p> 
<p>                                  <img alt="" class="has" height="65" src="https://images2.imgbox.com/de/31/yJjYOcdA_o.png" width="385"></p> 
<p>其中<img alt="c = R_W/D-1" class="mathcode" src="https://images2.imgbox.com/37/3e/1Dv94hM9_o.gif">为常数。式(3)很容易从式(2)推导出来。</p> 
<p>根据式(3)，当<img alt="w" class="mathcode" src="https://images2.imgbox.com/f8/f7/gc5XN5AB_o.png">趋近于0时，DRRF收敛于2，当<img alt="w" class="mathcode" src="https://images2.imgbox.com/bc/95/gqF2ULSL_o.png">增大时，DRRF收敛于1。也就是说，对于小的RoI，相同RoI的相对接受域(RRF)可以是原始图像和下采样图像的差值的2倍。另一方面，如果建议的规模足够大，那么RRFs也会变得相似。</p> 
<p>例如，对于输入图像的ROI <img alt="w = 4" class="mathcode" src="https://images2.imgbox.com/3e/fd/X0rNIwIX_o.png">，<img alt="I_W= 1600" class="mathcode" height="15" src="https://images2.imgbox.com/2d/10/OXllZ9QK_o.gif" width="76">，如果我们使用Faster R-CNN和ResNet-50主干网，其中<img alt="R_W" class="mathcode" src="https://images2.imgbox.com/9b/54/8me9uU8H_o.gif">= 291, <img alt="D" class="mathcode" src="https://images2.imgbox.com/4a/99/rjT0JOyG_o.png">= 16，那么<img alt="DRRF_{1/2}(4,1600)" class="mathcode" height="15" src="https://images2.imgbox.com/85/5d/9mjLKzzO_o.gif" width="106">接近1.8。也就是说，下采样图像的RoI的RRF大约是原始图像的1.8倍。Tan等人使用超分特征处理整个图像特征被超分辨的图像检索任务，因此RRF中的差异不显著。相反，对于我们工作中检测的小ROI的超分辨率，RRF的差异非常大，会严重误导超分辨率模型。</p> 
<h2>4、本文的方法</h2> 
<p>提出了一种基于两个关键思想的小目标检测特征超分辨方法：</p> 
<p>(1)、直接监督超分辨发生器;</p> 
<p>(2)、通过卷积进行感受野匹配。</p> 
<p>在基本检测模型的基础上，我们引入了四个附加组件：SR特征发生器和鉴别器、SR目标提取器和小预测器。<span style="color:#e579b6;"><strong>SR特征发生器作为一种基于GAN的模型，利用SR目标提取器的特征作为目标，在SR特征鉴别器的引导下产生高分辨率的特征。</strong></span>此外，小的预测器是基本检测器中预测器的复制，我们称之为大预测器。大预测器对大建议进行分类和定位的置信度的计算与普通检测器相同，而小预测器对小建议执行相同的任务，这些小建议首先由SR特征生成器进行增强。我们为小型建议设定的门槛为：清华-腾讯(Tsinghua-Tencent)为32×32,VOC和COCO数据集为96×96。图3显示了我们模型的整体架构。我们解释了基于Faster R-CNN的模型，尽管我们的方法可以与任何基于特征池的提议检测器集成。</p> 
<p>                            <img alt="" class="has" height="285" src="https://images2.imgbox.com/d3/bf/ZY1WMYE1_o.png" width="564"></p> 
<h3>4.1、超分目标提取器</h3> 
<p>我们用<img alt="I^{1.0}" class="mathcode" height="15" src="https://images2.imgbox.com/ee/c5/XQCpV75E_o.gif" width="24">表示原始输入图像，用<img alt="I^{0.5}" class="mathcode" height="15" src="https://images2.imgbox.com/df/42/gBKbfNry_o.gif" width="25">表示其×0.5的下采样图像。我们使用<img alt="F^{1.0}_i" class="mathcode" height="19" src="https://images2.imgbox.com/ae/e8/sUkQDnAK_o.gif" width="27">来表示原始图像的第i个RoI的特征。在第3节中，我们揭示了使用<img alt="F^{1.0}_i" class="mathcode" src="https://images2.imgbox.com/06/c8/f5rNVtz2_o.gif">作为<img alt="F^{0.5}_i" class="mathcode" src="https://images2.imgbox.com/c8/56/93kryLpu_o.gif">的超分辨率目标不是一个好方法。我们需要提取与低分辨率特征<img alt="F^{0.5}_i" class="mathcode" src="https://images2.imgbox.com/8e/17/YTAJ64BU_o.gif">相似的<img alt="T^{1.0}_i" class="mathcode" src="https://images2.imgbox.com/dd/2f/GOLTc4jF_o.gif">高分辨率目标特征。为此，我们引入了一个名为超分目标提取器的附加CNN特征提取器来生成<img alt="T^{1.0}_i" class="mathcode" height="15" src="https://images2.imgbox.com/54/e7/yUxgG0lr_o.gif" width="21">，如图3所示。我们让SR目标提取器与CNN主干网共享相同的参数(即基检测器中的常规特征提取器)，因为对于相同的输入，它们不应该根据通道产生不同的特征。</p> 
<p><span style="color:#e579b6;"><strong>SR目标提取器的一个重要要求是在感受野扩展的每一层充分处理RRF。在一般的CNNs中，当卷积层或池化层应用的滤波器尺寸大于1时，感受野就会扩大。</strong></span><span style="color:#e579b6;"><strong>因此，我们的SR目标提取器应该设计成在CNN主干中使用这些层中的任何一层时覆盖相同的扩展接受域。</strong></span><span style="color:#e579b6;"><strong>对于无参数的池化层，可以通过增加过滤器大小来轻松实现。</strong></span><span style="color:#e579b6;"><strong>然而，对于卷积层，增加滤波器的大小是无效的，因为这会使参数与CNN主干的参数不同。</strong></span><span style="color:#e579b6;"><strong>因此，我们采用了膨胀卷积层，其参数数目与常规卷积层相同，但其感受野由膨胀率控制。</strong></span><span style="color:#e579b6;"><strong>我们在每个卷积层上应用膨胀率为2的卷积层，并且在CNN主干上的滤波器尺寸大于1。</strong></span></p> 
<p>一个额外的处理是关于步幅的。如图4(a)所示，如果CNN主干中convolution layer的stride不是1(如2)，那么对于atrous convolution使用相同的stride大小是无效的，因为它会跳过图4(b)中每一个像素。这个问题可以通过将step为1的atrous convolution和max pooling与2结合来解决，如图4(c)所示。</p> 
<p>综上所述，SR目标提取器由若干个卷积层和池化层组成，这些层被安排在与CNN主干保持相同的RRF的同时共享相同的参数。SR目标提取器的特征<img alt="T^{1.0}_i" class="mathcode" src="https://images2.imgbox.com/b7/a7/4QrM0vrF_o.gif">比CNN骨干网的<img alt="T^{1.0}_i" class="mathcode" src="https://images2.imgbox.com/3f/11/gD3bLd85_o.gif">更适合训练超分辨率模型。<img alt="T^{1.0}_i" class="mathcode" src="https://images2.imgbox.com/d0/29/oP2cNQos_o.gif">比<img alt="F^{1.0}_i" class="mathcode" src="https://images2.imgbox.com/90/75/AJyLv4aL_o.gif">的感受域更大;它们包含更多的上下文信息，有助于更好地检测小目标。</p> 
<p>                                       <img alt="" class="has" height="321" src="https://images2.imgbox.com/7e/40/R4HWYYkH_o.png" width="477"></p> 
<h3>4.2、超分特征生成器</h3> 
<p>我们的特征级超分辨率模型基于生成式对抗网络(GAN)。它的最终目标是将小建议的集合特性<img alt="F^{1.0}_i" class="mathcode" height="15" src="https://images2.imgbox.com/86/52/28GGz34N_o.gif" width="22">转换为超分辨特性<img alt="S^{1.0}_i" class="mathcode" height="16" src="https://images2.imgbox.com/16/12/EIe6LyIr_o.gif" width="21">。为了得到一对低分辨率和高分辨率的目标特征，我们首先在0.5倍处对原始图像进行降采样，对于第<img alt="i" class="mathcode" src="https://images2.imgbox.com/78/d8/LcJfBoW3_o.png">个方案获得<img alt="F^{0.5}_i" class="mathcode" height="15" src="https://images2.imgbox.com/45/6b/XZ5lEy9Q_o.gif" width="21">，并与SR目标提取器生成的<img alt="T^{1.0}_i" class="mathcode" height="20" src="https://images2.imgbox.com/9b/88/zb25naZz_o.gif" width="27">进行配对。也就是说，通过学习图5中的超分辨率特征生成器，迭代地将<img alt="F^{0.5}_i" class="mathcode" src="https://images2.imgbox.com/0a/3d/wwu7cadg_o.gif">细化为超分辨率特征<img alt="S^{0.5}_i" class="mathcode" height="17" src="https://images2.imgbox.com/87/24/WMMFul5Q_o.gif" width="22">，使<img alt="S^{0.5}_i" class="mathcode" height="16" src="https://images2.imgbox.com/09/e8/4GNPfRac_o.gif" width="21">尽可能地与<img alt="T^{1.0}_i" class="mathcode" height="16" src="https://images2.imgbox.com/a5/f2/Uv2Yrvl3_o.gif" width="22">相似。对于这个目标，我们设计逐特征内容<img alt="\ell_{2}" class="mathcode" src="https://images2.imgbox.com/20/59/daK2FGI0_o.png">损失：</p> 
<p>                                        <img alt="" class="has" height="57" src="https://images2.imgbox.com/c8/22/JUF4OyVe_o.png" width="291"></p> 
<p>                                           <img alt="" class="has" height="289" src="https://images2.imgbox.com/2e/1e/fKruwPsq_o.png" width="381"></p> 
<p>在这个过程中，作为生成器的输入，我们使用了前一层<img alt="F^{0.5}_{sub,i}" class="mathcode" height="18" src="https://images2.imgbox.com/dd/dc/CoeR79bW_o.gif" width="29">(子层)和后一层<img alt="F^{0.5}_i" class="mathcode" height="15" src="https://images2.imgbox.com/b5/42/b0PsfbYe_o.gif" width="22">(基础层)的特性。由于<img alt="F^{0.5}_i" class="mathcode" height="15" src="https://images2.imgbox.com/84/44/l850Zxq9_o.gif" width="21">只包含粗低频信息，RoI较小，我们对其精细高频信息<img alt="F^{0.5}_{sub,i}" class="mathcode" height="21" src="https://images2.imgbox.com/bf/9e/mPWq3A72_o.gif" width="34">进行补充。</p> 
<p>对于SR特征鉴别器，我们使用了一个三层的多层感知器(MLP)。训练鉴别器能够区分<img alt="T^{1.0}_i" class="mathcode" height="16" src="https://images2.imgbox.com/68/0c/A96IIYID_o.gif" width="22">和<img alt="S^{0.5}_i" class="mathcode" height="20" src="https://images2.imgbox.com/7e/ba/Agd3dPy1_o.gif" width="26">，而训练生成器将<img alt="F^{0.5}_i" class="mathcode" src="https://images2.imgbox.com/63/85/aoDEdidO_o.gif">转换成<img alt="S^{0.5}_i" class="mathcode" height="15" src="https://images2.imgbox.com/b0/2b/z2G1zqMw_o.gif" width="20">，与<img alt="T^{1.0}_i" class="mathcode" height="19" src="https://images2.imgbox.com/28/da/6QzCotc0_o.gif" width="26">没有区别。因此，发生器和鉴别器分别最小化</p> 
<p>                                     <img alt="" class="has" height="101" src="https://images2.imgbox.com/99/f6/n6qhMNUD_o.png" width="345"></p> 
<p>最后，当我们为不同的损失构造低分辨率的输入和高分辨率的目标特征时，我们使用了阈值化。虽然不同的阈值用于不同的损失，我们应用以下一般规则;对于小到不能作为目标的高分辨率特征，我们将其弃之不顾;对于大到不需要超分辨率的低分辨率特征，我们将其弃之不顾。我们为不同的数据集应用不同的阈值，如第4节概述中指定的那样。在补充资料中对阈值化进行了更详细的说明。</p> 
<p>到目前为止，我们已经讨论了生成器如何将低分辨率的特征<img alt="F^{0.5}_i" class="mathcode" src="https://images2.imgbox.com/10/6a/ra9vlpbR_o.gif">细化为类似于目标特征<img alt="T^{1.0}_i" class="mathcode" src="https://images2.imgbox.com/cb/67/cTuSxuGB_o.gif">。然而，我们的最终目标是更好地检测小目标;因此，我们需要训练生成器以一种确实有助于很好地检测小目标的方式来超级解析特性。为此，我们进一步训练发生器如下。当生成器从<img alt="F^{1.0}_i" class="mathcode" src="https://images2.imgbox.com/05/8b/jyIWdjRf_o.gif">生成超分辨特征<img alt="S^{1.0}_i" class="mathcode" src="https://images2.imgbox.com/dc/7b/CY7HfQSq_o.gif">后，我们将其输入到小盒预测器中。然后，我们计算了盒形预测器在[31]中的分类损失(<img alt="L_{cls}" class="mathcode" src="https://images2.imgbox.com/1b/d5/DrdPZiFR_o.gif">)和定位损失(<img alt="L_{loc}" class="mathcode" src="https://images2.imgbox.com/d7/7a/OnkkOnYl_o.gif">)，并将梯度信号流到发生器进行微调。</p> 
<h3>4.3、训练</h3> 
<p>我们首先训练基检测器模型，该模型由特征提取器、区域建议网络(RPN)和大预测器组成。然后，使用特征(<img alt="F^{1.0}_i" class="mathcode" height="18" src="https://images2.imgbox.com/eb/e8/oQUeFF8b_o.gif" width="25">,<img alt="F^{0.5}_i" class="mathcode" height="15" src="https://images2.imgbox.com/fd/8a/LsReojHg_o.gif" width="21">和<img alt="T^{1.0}_i" class="mathcode" height="16" src="https://images2.imgbox.com/e7/e6/oL78SzqM_o.gif" width="22">)交替训练生成器和鉴别器，同时冻结特征提取器和RPN。在生成器、内容、分类和定位损耗加权和的指导下训练生成器，而鉴别器仅从鉴别器损失开始训练。同时利用超分辨特征<img alt="S^{1.0}_i" class="mathcode" src="https://images2.imgbox.com/a6/f8/g6LEiL3g_o.gif">从分类和局部化损失两个方面对小预测器进行训练。注意，我们分别使用特征提取器和基检测器的大预测器的权值初始化SR目标提取器和小预测器。</p> 
<p>一旦生成器和鉴别器都收敛，我们将进一步对大小预测器进行微调，同时冻结其他所有预测器。微调对于小的预测器是有用的，因为它只针对可能与目标特征不完全相同的超分辨特征进行训练。它还通过只关注分类和本地化损失来进一步提高性能。大型预测器只对大型建议进行微调，因为小型建议的特性不再传递给它。</p> 
<h3>4.4、推断</h3> 
<p>一旦训练完成，推理就简单多了。我们只使用了基本模型顶部的SR特征发生器和小的预测器，与图3中的主要预测部分相对应。给定一个输入图像<img alt="I^{1.0}" class="mathcode" src="https://images2.imgbox.com/a4/9d/Ft3kuowV_o.gif">，我们从CNN主干<img alt="F^{1.0}" class="mathcode" src="https://images2.imgbox.com/0e/ef/mWaOCHCq_o.gif">获取特征。如果特征建议很大，那么大预测器就会采用它来对其类和位置进行预测。另一方面，如果特征建议很小，则首先使用SR特征生成器对其进行超分辨，然后将其传递给小的预测器。</p> 
<h2>5、实验</h2> 
<p>我们评估了我们的方法在速度Faster R-CNN上的性能，作为各种各样的基础网络(ResNet-50, ResNet-101，和MobileNet)在三个基准数据集上的清华-腾讯100K， PASCAL VOC和MS COCO。我们在补充文件中提供了更多的实验结果和分析。</p> 
<h3>5.1、Tsinghua-Tencent 100K结果</h3> 
<p>Tsinghua-Tencent100K是一个大型的交通标志基准，由于天气和复杂的背景造成了严重的照度变化。它提供了一个真实世界中的交通标志数据集，其中目标对象的大小与图像大小相比非常小(2048×2048)。数据集有6K个训练图像和3K个测试图像。它对数据大小的划分与MS COCO相同，分为小(面积≤32×32)、中(32×32 &lt;面积≤96×96)和大(面积&gt; 96×96)目标。小、中、大物体的比例分别为(42,50,8)%。由于小目标占主导地位，清华-腾讯100K是验证小目标检测性能的最佳基准之一。</p> 
<p><strong>评价标准：</strong></p> 
<p>遵循[38]协议，我们对182个类中包含100多个实例的45个类进行评估。虽然在[38]中只报告了大小方面的召回和准确性，但我们还报告了F1分数，因为它们可以平衡这两个指标。如果IoU的groundtruth大于或等于0.5，则该检测被视为正确。定量的结果：我们将模型的性能与前面指定的三个基本模型进行比较。我们将小提案的大小设置为32×32;只有面积小于阈值的建议才被视为超分辨率模型的输入。</p> 
<p><strong>定量结果：</strong></p> 
<p>我们将模型的性能与前面指定的具有三个主干的基本模型进行比较。 我们将小提案的门槛设为32 × 32；只有面积小于阈值的提案才作为超分辨率模型的输入。  </p> 
<p>表1总结了在TsinghuaTencent 100K测试数据集上的性能。我们将输入图像的大小从2048调整到1600，以使学习和推断更快，就像在[23]中一样。通过我们的方法，性能的提高在小(75.2→84.3在F1评分与ResNet-101)，中等(92.2→94.6)和大目标(92.2→93.2)的顺序是显著的。对于不同的CNN骨架，对于小目标的大的改进是一致的，例如对于MobileNet的63.4→71.0，对于ResNet-50的74.9→82.2。     <img alt="" class="has" height="164" src="https://images2.imgbox.com/6a/3e/y9OfwrcD_o.png" width="678"></p> 
<p><span style="color:#fe2c24;"><strong>有一点值得注意的是，虽然我们只对小的建议进行了超分解，但我们也获得了中、大型目标的性能增益。这可能是因为大型预测器在没有考虑小建议的情况下进行了微调，这有助于将其建模能力集中在中型和大型目标上。改进中等子集的另一个原因是，由于在最后一步中添加到建议的偏移量，最终落在中等子集中的一些建议是使用小型预测器进行预测的。</strong></span>考虑到大约14%的目标在32×32和40×40之间，这可能是一个合理的原因来解释中等子集的性能提升。</p> 
<p><strong>和最先进的方法的比较：</strong></p> 
<p>表2显示，我们提出的模型在清华-腾讯100K数据集上实现了最新的性能。在这些实验中，我们使用ResNet-101作为原始尺寸图像的主干来训练我们的模型。在所有的子集，我们的表现优于所有以前的先进模式，特别是在F1得分方面。</p> 
<h3>5.2、在PASCAL VOC和COCO上的结果</h3> 
<p>我们也在PASCAL VOC和MS COCO上评估我们的模型，虽然这些基准中小目标的比例远低于清华-腾讯100K。PASCAL VOC包含20个目标类别，2007年有5K trainval和5K测试图片，2012年有11K trainval图片。我们使用2007年的trainval + 2012年的trainval进行培训，使用2007年的test set进行测试。COCO 2017包含80个目标类别，115K的训练集，5K的val和20K的testdev图像。我们使用训练集，而val和测试开发集用于测试。我们在补充资料中补充了val集的结果。</p> 
<p><strong>评价措施：</strong></p> 
<p>对于PASCAL VOC，我们使用mAP@.5个度量，这是所有类的平均AP，当匹配IoU阈值与groundtruth大于或等于0.5时。对于COCO女士，我们使用的是map@ .95，这是在0.5到0.95之间的不同匹配IoU阈值上的平均映射。我们还根据目标大小将PASCAL VOC的结果分为三类;小型(AP-S)，中型(AP-M)和大型(AP-L)，如MS COCO。对于小的提案，我们设置了96×96的门槛，因为目标的大小比清华-腾讯100K大得多。</p> 
<p><strong>定量的结果：</strong></p> 
<p>表3比较了我们的模型与VOC 2007测试基线和COCO 2017测试基线的性能。我们观察到与清华大学100K类似的趋势，即检测增强在小、中、大目标的顺序中更为显著。</p> 
<p><img alt="" class="has" height="181" src="https://images2.imgbox.com/54/1f/uB3Pwpsq_o.png" width="784"></p> 
<h3>5.3、和超分方法的比较</h3> 
<p>在本节中，我们进行了消融研究，定量和定性地分析了不同的超分辨率方法。我们使用ResNet-50作为CNN的主干。我们比较了我们的超分辨率方法与两个较差的变体：</p> 
<p>（1）、没有监督：模型没有内容的损失(<img alt="L_{cont}" class="mathcode" src="https://images2.imgbox.com/a4/b1/PEIOPiut_o.gif">)和(2)、SR监督：督模型训练使用目标特性的基本特征提取器,而不是我们的目标提取。</p> 
<p>表4比较了不同超分辨率车型在Tsinghua-Tencent100K上的F1成绩。与基本模型相比，其他两个SR变种只能获得有限的性能收益。另一方面，我们的SR模型获得了显著的性能收益，特别是对于小子集。这里的话是没有监督执行比SR和初始监督,这意味着RRF的监督由于不匹配不当可能会降低性能。图6定性地显示了我们的模型在特征级超分辨率方面优于SR的优势。</p> 
<p>                                                    <img alt="" class="has" height="318" src="https://images2.imgbox.com/2b/e4/weVO41go_o.png" width="355"></p> 
<h3>5.4、定性的结果</h3> 
<p>图7显示了一些选定的检测结果。对于每一对，我们显示基本检测器的结果(左)和我们的方法(右)。与基本模型相比，该方法能更好地检测小目标，具有较高的可信度。我们提供了更多的定性结果，包括接近失败的案例在补充文件。</p> 
<h2>6、结论</h2> 
<p>提出了一种基于特征级超分辨率的小目标检测方法。我们的方法适用于任何基于特征池的检测器。在清华-腾讯100K、PASCAL VOC和MS COCO基准上的实验验证了我们的超分辨率方法确实能够有效地检测小目标。特别是，我们的工作证明了使用适当的高分辨率目标特征提供直接监督是重要的，这些特征与低分辨率输入特征共享相同的相对接受域。</p> 
<p>作为未来的工作，我们的模型可以通过以下几种方式进一步增强。首先，我们可以采用图像超分辨率任务中开发的最新模型来更新SR特征生成器。其次，可以自适应地选择超分辨率。虽然我们在这项工作中只使用固定的比率2，但最佳比率可能取决于RoI的特性。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/36fe010e77b7de86315f0d657dca77e9/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">小程序登录流程</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/65fcfb78f18300a3d27dea889a9fabb2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">go集成nacos测试grpc负载均衡</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>