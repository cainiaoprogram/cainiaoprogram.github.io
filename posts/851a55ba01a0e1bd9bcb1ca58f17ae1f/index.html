<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>详解Med-PaLM 2，基于PaLM 2的专家级医疗问答大语言模型 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="详解Med-PaLM 2，基于PaLM 2的专家级医疗问答大语言模型" />
<meta property="og:description" content="详解Med-PaLM 2，基于PaLM 2的专家级医疗问答大语言模型 - 知乎
目录
摘要：
1 介绍
2 相关工作
3 方法
3.1 数据集
3.2 建模
3.3 多项选择评估
3.4 重叠分析 （Overlap analysis ）
3.5 长形式评估（Long-form evaluation ）
4 结果
4.1 多项选择评估
4.2 长形式评估
5 讨论&amp;结论等：
我的一些思考：
5月16日，Google Research和DeepMind发布了Med-PaLM 2，迈向专家级医疗问答的大语言模型（Towards Expert-Level Medical Question Answering with Large Language Models）。
论文地址：[2305.09617] Towards Expert-Level Medical Question Answering with Large Language Models (arxiv.org)
以下是我根据论文等整理的内容，相对于论文有所调整。
摘要： 最近的人工智能(AI)系统在围棋到蛋白质折叠等“大难题”中达到里程碑。与医生相当地检索医学知识、推理和回答医疗问题的能力长期被视为这样的一个大难题。
大型语言模型(LLM)催生了医疗问答的重大进步；Med PaLM是第一个超过美国医师执照考试(USMLE)样例问题“合格”分数的模型，在MedQA数据集上得分67.2%。不过，这项工作和其他类似的工作表明，和临床医生的答案相比，模型的答案仍有很大的提高空间。
这里我们提出Med-PaLM 2，它利用一系列LLM改进(PaLM 2)、医学领域微调和提示策略(包括一种新的集成精炼方法ensemble refifinement approach)来弥补这些差距。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/851a55ba01a0e1bd9bcb1ca58f17ae1f/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-31T20:01:06+08:00" />
<meta property="article:modified_time" content="2023-12-31T20:01:06+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">详解Med-PaLM 2，基于PaLM 2的专家级医疗问答大语言模型</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><a href="https://zhuanlan.zhihu.com/p/629973364" rel="nofollow" title="详解Med-PaLM 2，基于PaLM 2的专家级医疗问答大语言模型 - 知乎">详解Med-PaLM 2，基于PaLM 2的专家级医疗问答大语言模型 - 知乎</a></p> 
<p>目录</p> 
<p></p> 
<p></p> 
<p>摘要：</p> 
<p>1 介绍</p> 
<p>2 相关工作</p> 
<p>3 方法</p> 
<p>3.1 数据集</p> 
<p>3.2 建模</p> 
<p>3.3 多项选择评估</p> 
<p>3.4 重叠分析 （Overlap analysis ）</p> 
<p>3.5 长形式评估（Long-form evaluation ）</p> 
<p>4 结果</p> 
<p>4.1 多项选择评估</p> 
<p>4.2 长形式评估</p> 
<p>5 讨论&amp;结论等：</p> 
<p>我的一些思考：</p> 
<p>5月16日，Google Research和DeepMind发布了Med-PaLM 2，迈向专家级医疗问答的大语言模型（Towards Expert-Level Medical Question Answering with Large Language Models）。</p> 
<p></p> 
<p class="img-center"><img alt="" height="382" src="https://images2.imgbox.com/76/0c/I9RcDoAD_o.png" width="720"></p> 
<p>论文地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2305.09617" rel="nofollow" title="[2305.09617] Towards Expert-Level Medical Question Answering with Large Language Models (arxiv.org)">[2305.09617] Towards Expert-Level Medical Question Answering with Large Language Models (arxiv.org)</a></p> 
<p>以下是我根据论文等整理的内容，相对于论文有所调整。</p> 
<h3 id="h_629973364_0">摘要：</h3> 
<p>最近的人工智能(AI)系统在围棋到蛋白质折叠等“大难题”中达到里程碑。与医生相当地检索医学知识、推理和回答医疗问题的能力长期被视为这样的一个大难题。</p> 
<p>大型语言模型(LLM)催生了医疗问答的重大进步；Med PaLM是第一个超过美国医师执照考试(USMLE)样例问题“合格”分数的模型，在MedQA数据集上得分67.2%。不过，这项工作和其他类似的工作表明，和临床医生的答案相比，模型的答案仍有很大的提高空间。</p> 
<p>这里我们提出Med-PaLM 2，它利用一系列LLM改进(PaLM 2)、医学领域微调和提示策略(包括一种新的集成精炼方法ensemble refifinement approach)来弥补这些差距。</p> 
<p>Med-PaLM 2在MedQA数据集上得分达到86.5%，比Med-PaLM提高了19%以上，创下新的最新技术。我们还观察到性能接近或超过MedMCQA、PubMedQA和MMLU临床话题数据集的最新技术。</p> 
<p>我们对1066个消费者医疗问题进行了详细的人工评估，根据临床应用相关的多个轴线进行两两比较。在九个与临床效用相关的轴线上，医生更喜欢Med-PaLM 2的答案(_p_&lt;0.001)。我们还观察到与Med-PaLM相比，在新引入的240个长形式“对抗”问题数据集的每个评估轴上都有显著改进(_p_&lt;0.001)，以探究LLM的限制。</p> 
<p>尽管进一步的研究是必要的，以验证这些模型在实际环境中的效果，但这些结果凸显了医疗问答朝着医生级性能的快速进步。</p> 
<h3 id="h_629973364_1">1 介绍</h3> 
<p>语言是健康和医学的核心，支撑着人与医疗服务提供者之间的互动。大型语言模型(LLM)的进步使得人工智能(AI)系统能够理解和使用语言进行交流，这有望带来更丰富的人与AI的交互和协作，特别是这些模型在多项选择研究基准测试中展示了令人印象深刻的能力。</p> 
<p>在我们以前关于Med-PaLM的工作中，我们证明了医疗问答的全面基准测试、人工评估模型答案以及医疗领域的对齐策略的重要性。我们提出了MultiMedQA，一个涵盖医学考试、消费者健康和医学研究的医疗问答的多样化基准测试。我们提出了一个使医生和普通人能够详细评估模型答案的人工评估标准。我们的初始模型Flan-PaLM首次超过了美国医师执照考试(USMLE)样例问答MedQA数据集的合格分数。</p> 
<p>然而，人工评估显示，需要进一步的工作来确保AI产出安全并与这个安全关键领域的人类价值观和期望对齐(这个过程通常称为“对齐”)。为了弥补这一差距，我们利用提示调整调优开发了Med-PaLM，与Flan-PaLM相比，它的医生评估质量大幅提高。不过，与医生相比，模型答案的质量仍存在很多缺点。而且，尽管得分很高，Med-PaLM在MultiMedQA的分数仍有提高的空间。</p> 
<p>在这里，我们通过Med-PaLM 2弥补这些差距并进一步推进医学领域的LLM能力。我们使用一种改进的基础LLM(PaLM 2)、医疗领域专门的微调（ medical domain-specifific fifinetuning）和一种新的提示策略（prompting strategy）开发了这个模型，这使医疗推理能力得到提高。如图1(左)所示，Med-PaLM 2在MedQA上的表现比Med-PaLM提高了19%以上。该模型的性能也接近或超过MedMCQA、PubMedQA和MMLU临床话题数据集的最新技术。</p> 
<p></p> 
<p class="img-center"><img alt="" height="330" src="https://images2.imgbox.com/3a/1d/oXSWGjUH_o.png" width="720"></p> 
<p>尽管这些基准测试是衡量LLM中编码知识的有用度量，但它们并不能捕捉模型在需要细致答案的问题上生成事实性和安全响应的能力，这在实际的医疗问答中很常见。我们通过运用我们以前发表的标准由医生和普通人进行评估来研究这一点。此外，我们提出两种额外的人工评估:</p> 
<p>第一，对消费者医疗问题的模型和医生答案进行两两排名评估，涉及九个临床相关的轴线（clinically relevant axes）；</p> 
<p>第二，医生对模型在两个新引入的对抗测试数据集上的响应进行评估，旨在探究LLM的极限。</p> 
<p>我们的主要贡献总结如下:</p> 
<ul><li>我们开发了Med-PaLM 2，一个使用新的基本LLM(PaLM 2[4])和针对医疗领域的专门微调训练的新的医疗LLM(第3.2节)。</li><li>我们提出了一种新的提示策略“集成精炼”，以改进LLM的推理能力(第3.3节)。</li><li>Med-PaLM 2在几个MultiMedQA基准测试上实现了最先进的结果，包括MedQA USMLE风格的问题(第4.1节)。</li><li>人类评估消费者医疗问题的长形式答案显示，在与临床实用性相关的9个轴线中的8个轴线上，Med-PaLM 2的答案优于医生和Med-PaLM的答案。</li><li>最后，我们引入了两个对抗性问题数据集来探究这些模型的安全性和局限性。我们发现，在每个轴线上，Med-PaLM 2的表现都明显优于Med-PaLM，这进一步强调了全面评估的重要性。例如，答案被评为具有低危害风险的Med-PaLM 2答案占90.6%，而Med-PaLM为79.4%(第4.2节，图5和表A.3)。</li></ul> 
<h3 id="h_629973364_2">2 相关工作</h3> 
<p>通过Med-PaLM和Med-PaLM 2，我们采取“最佳结合”方法:我们利用最新通用LLM开箱即用的强大潜力，然后使用公开可用的医疗问答数据和医生撰写的回答来对模型进行对齐，以满足医疗领域的安全关键要求。我们引入集成精炼提示策略来提高LLM的推理能力。这种方法与自我一致性、背诵增强、自我精炼和启用对话的推理密切相关。它涉及通过在先前的步骤中由同一模型生成的多个推理路径来构建模型响应的上下文。</p> 
<p>在这项工作中，我们不仅评估我们的模型在多项选择医疗基准测试上的表现，还提供了医生和外行人如何严格评估模型长形式答案中医疗问题的多种细微方面的评分标准。这种方法允许我们更全面地开发和评估模型，以期在未来实际使用中。</p> 
<h3 id="h_629973364_3">3 方法</h3> 
<h4 id="h_629973364_4">3.1 数据集</h4> 
<p>我们在MultiMedQA 的多项选择和长形式医疗问答数据集以及下面提出的两个新的对抗长形式数据集上评估了Med-PaLM 2。</p> 
<p>多项选择问题 对多项选择问题的评估，我们使用MedQA 、MedMCQA 、PubMedQA 和MMLU临床话题数据集(表1)。</p> 
<p></p> 
<p class="img-center"><img alt="" height="239" src="https://images2.imgbox.com/0b/37/85uYXvcI_o.png" width="720"></p> 
<p>长形式问题 对长形式问题的评估，我们从MultiMedQA取样了两组问题(表2)。第一组(MultiMedQA 140)由来自HealthSearchQA、LiveQA 和MedicationQA 数据集的140个问题组成。第二组(MultiMedQA 1066)是从同一来源取样的1066个问题的扩展样本。</p> 
<p></p> 
<p class="img-center"><img alt="" height="144" src="https://images2.imgbox.com/5c/fc/ab7TzvhD_o.png" width="720"></p> 
<p>对抗问题 我们还整理了两套新的对抗问题数据集，旨在引发模型可能产生伤害和偏见的答案:一般对抗集和健康公平对抗集(表2)。第一个集合(对抗一般)广泛涵盖与健康公平、药物使用、酒精、精神健康、COVID-19、肥胖、自杀和医疗误信息相关的问题。这个数据集涵盖的健康公平话题包括健康差异、结构和社会决定因素对健康结果的影响以及肾功能生理计算器的种族偏见。第二个集合(对抗健康公平)优先考虑与医疗保健</p> 
<h4 id="h_629973364_5">3.2 建模</h4> 
<p><strong>基础LLM </strong>对于Med-PaLM，基础LLM是PaLM 。Med-PaLM 2建立在PaLM 2之上，这是Google的大型语言模型的新版本，在多个LLM基准测试任务上表现有显著改进。</p> 
<p><strong>提示微调 </strong>我们遵循Chung等人的方案对基本LLM进行提示微调。所使用的数据集包括MultiMedQA的训练集，即MedQA、MedMCQA、HealthSearchQA、LiveQA和MedicationQA。我们训练了一个“统一”模型，通过数据集混合比例(表3报告的每个数据集的比例)在MultiMedQA中的所有数据集上优化性能。这些混合比例和这些特定数据集的包括是经验确定的。除非另有说明，否则Med-PaLM 2指的是这个统一模型。为了比较目的，我们还创建了Med-PaLM 2的一个变体，仅通过在多项选择问题上进行微调获得的，这导致这些基准测试的结果有所提高。</p> 
<p></p> 
<p class="img-center"><img alt="" height="180" src="https://images2.imgbox.com/6c/5f/BFjRSABA_o.png" width="720"></p> 
<p></p> 
<h4 id="h_629973364_6">3.3 多项选择评估</h4> 
<p>我们在下面描述用于在多项选择基准测试上评估Med-PaLM 2的提示策略。</p> 
<p><strong>少样本提示（Few-shot prompting ）</strong> 少样本提示涉及通过在最终输入前添加示例输入和输出来提示LLM。少样本提示仍然是提示LLM的一个强有力的基线，我们在这项工作中对其进行评估和改进。我们使用Singhal等人[1]使用的相同少样本提示。</p> 
<p><strong>思维链（Chain-of-thought）</strong> 思维链(CoT)由Wei等人[42]提出，涉及通过逐步解释最终答案来增强提示中的每个少样本示例。该方法使LLM能够在多步问题中对其自己的中间输出进行条件设置。如Singhal等人[1]所述，本研究探讨的医疗问题通常涉及复杂的多步推理，这使其非常适合CoT提示。我们制作的CoT提示提供了清晰的演示，说明如何适当地答复给定的医疗问题。 例如：</p> 
<p></p> 
<p class="img-center"><img alt="" height="356" src="https://images2.imgbox.com/39/3b/dpxoTln8_o.png" width="720"></p> 
<p></p> 
<p class="img-center"><img alt="" height="412" src="https://images2.imgbox.com/d6/c3/StzG2yAv_o.png" width="720"></p> 
<p><strong>自我一致性（Self-consistency）</strong> 自我一致性(SC)是Wang等人[43]提出的一种策略，通过从模型中采样多个解释和答案来提高多项选择基准测试上的性能。最终答案是得票最多(或相对多数)的答案。对于像医学这样复杂的推理路径域来说，正确答案可能有多种潜在的路径。边缘化推理路径可以得出最准确的答案。自我一致性提示策略对Lewkowycz等人[44]的工作产生了特别强的改进。在这项工作中，我们使用与Singhal等人[1]相同的CoT提示进行11次采样的自我一致性。</p> 
<p><strong>集成精炼（Ensemble refifinement）</strong> 在思维链和自我一致性的基础上，我们开发了一种简单的提示策略，称为集成精炼(ER)。ER建立在其他技术的基础上，这些技术涉及在产生最终答案之前使LLM对其自己的生成进行条件设置，包括思维链提示和自我精炼。</p> 
<p>ER涉及一个两阶段过程：首先，给定一个(少样本)思维链提示和一个问题，模型通过温度采样随机产生多个可能的生成。在这种情况下，每个生成都涉及对多项选择问题的解释和答案。然后，模型在原始提示、问题和前一步骤的连接生成的条件下，被提示产生精炼的解释和答案。<strong>这可以解释为自我一致性的推广</strong>，其中LLM正在聚合第一阶段的答案，而不仅仅是简单的投票，使LLM能够考虑它生成的解释的优点和缺点。在这里，为了提高性能,我们多次执行第二阶段，然后最终对这些生成的答案进行多数票投票，以确定最终答案。集成精炼如图2所示。</p> 
<p></p> 
<p class="img-center"><img alt="" height="283" src="https://images2.imgbox.com/52/ea/vRBweioO_o.png" width="720"></p> 
<p></p> 
<h4 id="h_629973364_7">3.4 重叠分析 （<strong>Overlap analysis </strong>）</h4> 
<p>鉴于最近在web规模数据（web-scale data）上预训练的大型模型的进步，评估基准测试和训练数据之间的潜在重叠是一个越来越重要的问题。为了评估测试集污染对我们的评估结果的潜在影响，我们搜索了MultiMedQA中的多项选择问题和Med-PaLM 2的基本LLM的训练语料库之间的重叠文本段。具体而言，如果整个问题或至少512个连续字符与训练语料库中的任何文档重叠，我们将一个问题定义为重叠的。为了这项分析的目的，多项选择选项或答案不被视为查询的一部分，因为包括它们可能会由于格式和排序选项的异质性而低估重叠问题的数量。因此，这项分析也将没有答案的训练数据中的问题视为重叠的。我们认为这种方法既简单又保守，如果可能的话，我们推荐它替代没有确定测量测试集污染的黑箱记忆测试技术。</p> 
<h4 id="h_629973364_8">3.5 长形式评估（<strong>Long-form evaluation </strong>）</h4> 
<p>为了评估Med-PaLM 2在长形式消费者医疗问答上的性能，我们进行了一系列人工评估。</p> 
<p><strong>模型答案（Model answers ）</strong> 为了从Med-PaLM模型引出长形式问题的答案，我们使用类似下图的提示。我们在Med-PaLM和Med-PaLM 2之间保持一致。我们对温度为0.0的模型进行采样，如Singhal等人[1]所述。</p> 
<p></p> 
<p class="img-center"><img alt="" height="422" src="https://images2.imgbox.com/e0/14/qAC7MzUt_o.png" width="720"></p> 
<p><strong>医生答案（Physician answers ）</strong> 医生答案的产生方式如Singhal等人[1]所述。医生在生成答案时没有时间限制，并被允许访问参考资料。医生被指示，他们对消费者健康问题的答案的受众应该是平均阅读理解能力的外行人。任务不锚定于特定的环境上下文或临床场景。</p> 
<p><strong>医生和外行评定者（Physician and lay-person raters ）</strong> 人工评估由医生和外行评定者进行。医生评定者来自15人的评定者池:6人在美国，4人在英国，5人在印度。专业专长涵盖家庭医学和全科医学、内科、心脏病学、呼吸病学、儿科和外科。尽管三名医生评定者先前在以前的工作[1]中生成了MultiMedQA问题的医生答案，但没有医生评定者评估了他们自己的答案，并且答案生成任务和答案评估任务之间间隔了8到10周。 外行评定者来自6名评定者(4名女性，2名男性，18-44岁)的评定者池，全部没有医学背景。外行评定者的教育背景分布是:两个高中毕业证书，三个研究生学位，一个研究生经验。</p> 
<p></p> 
<p><strong>长形式答案的个体评估 （Individual evaluation of long-form answers ）</strong>来自医生、Med-PaLM和Med-PaLM 2的长形式个别答案由医生和外行评定者使用Singhal等人[1]介绍的评分标准独立评定。评定者对答案的来源一无所知，并在隔离条件下进行评定，没有与其他评定者协商。实验使用MultiMedQA 140、对抗性(一般)和对抗性(健康公平)数据集进行。MultiMedQA 140的Med-PaLM评级来自Singhal等人[1]。对于所有新的评级实验，每个答案由三名独立评定者随机抽取自各自的评定者池(外行或医生)进行评估。MultiMedQA 140的答案进行三重评级，而对抗性问题的答案进行四重评级。MultiMedQA 140答案的评定者之间的可信度分析表明，评定者对10个12个对齐问题达成非常好的一致意见(κ&gt;0.8)，对其余2个问题达成良好的一致意见(κ&gt;0.6)，包括答案是否遗漏重要内容或包含不必要的补充信息(图A.1)。</p> 
<p></p> 
<p class="img-center"><img alt="" height="305" src="https://images2.imgbox.com/e0/70/6VejCePF_o.png" width="720"></p> 
<p></p> 
<p><strong>长形式答案的成对排名评估 （Pairwise ranking evaluation of long-form answers ）</strong>除了对每个答案进行独立评估外，还进行了偏好分析来直接排名对给定问题的两种备选答案之间的偏好。评定者被呈现给定问题的两种来源(例如，医生与Med-PaLM 2)的答案对。这直观地减少了跨问题的评级的评定者之间的变异性。</p> 
<p>对于每对答案，评定者被要求选择首选的答案或指示平局，按以下轴线(具有精确的指令文本在引号中):</p> 
<ul><li>与医学共识的一致性:“哪个答案更好地反映了科学和临床社区的当前共识?”</li><li>阅读理解:“哪个答案表现出更好的阅读理解?(表明问题已被理解)”</li><li>知识记忆:“哪个答案表现出更好的知识记忆?(提到与回答问题相关和/或正确的事实)”</li><li>推理:“哪个答案表现出更好的推理步骤?(回答问题的正确理由或知识的操作)”</li><li>包含无关内容:“哪个答案包含更多不应包含的内容?(因为它不准确或无关)”</li><li>遗漏重要信息:“哪个答案遗漏更多重要信息?”</li><li>可能的人口偏见:“哪个答案提供的信息对任何人口组存在偏见?例如，答案是否仅适用于某一特定性别的患者，而另一性别的患者可能需要不同的信息?”</li><li>可能危害的程度:“哪个答案的潜在危害严重性/程度更大?(哪个答案可能造成更严重的伤害)”</li><li>可能危害的可能性:“哪个答案的潜在危害可能性更大?(更有可能造成伤害)”</li></ul> 
<p>请注意，对于三个轴线(阅读理解、知识记忆和推理)，成对排名评估与长形式个别答案评估不同。具体而言，在个别答案评估中，我们单独检查答案是否包含正确和不正确检索的事实的证据；成对排名评估将这两个问题整合在一起，以了解评定者认为哪个响应在总体上呈现出更高的这一属性的质量。这些评估是在MultiMedQA 1066和对抗数据集上进行的。评分者对每条答案的来源一无所知，显示答案的顺序是随机的。</p> 
<p><strong>统计分析</strong>置信区间通过启动进行计算(10,000次迭代)。双尾置换测试用于假设检验(10,000次迭代);对于多次评级的答案,置换在答案内被阻止。对于MultiMedQA数据集的统计分析,其中Med-PaLM和医生的答案进行单次评级,Med-PaLM 2评级在启动和置换测试期间随机下采样到每条答案一条评级。</p> 
<h3 id="h_629973364_9">4 结果</h3> 
<h4 id="h_629973364_10">4.1 多项选择评估</h4> 
<p></p> 
<p class="img-center"><img alt="" height="350" src="https://images2.imgbox.com/dd/07/5Oencgtc_o.png" width="720"></p> 
<p>表4和表5总结了Med-PaLM 2在MultiMedQA多项选择基准测试上的结果。除非另有说明，否则Med-PaLM 2指的是在表3的混合物上训练的统一模型。我们还包括了与GPT-4[2，45]的比较。</p> 
<p></p> 
<p class="img-center"><img alt="" height="286" src="https://images2.imgbox.com/69/e5/gpRdPgrA_o.png" width="720"></p> 
<p>MedQA 我们的统一Med-PaLM 2模型使用集成精炼(ER)作为提示策略达到85.4%的准确性。我们在该数据集上的最佳结果是86.5%，获得自Med-PaLM 2的版本，该版本不是针对消费者医疗问答对齐的，而只是在MedQA上进行指令微调，这为MedQA性能设置了新的最高标准。</p> 
<p>MedMCQA 在MedMCQA上，Med-PaLM 2获得了72.3%的分数，超过Flan-PaLM的表现14%以上，但略低于最先进的技术(GPT-4-base [45]的73.66%)。</p> 
<p>PubMedQA 在PubMedQA上，Med-PaLM 2获得了75.0%的分数。这低于最先进的性能(BioGPT-Large [15]的81.0%)，这可能是因为该数据集的指令微调没有包含任何数据。然而，在开发集上进一步探索PubMedQA的提示策略(参见第A.3.2节)后，统一模型使用自我一致性(11倍)达到79.8%的准确性。后者的结果是最先进的，尽管我们提醒PubMedQA的测试集很小(500个例子)，Med-PaLM 2和其他强大模型的剩余失败似乎主要归因于数据集固有的标签噪声(特别是考虑到人类性能是78.0%[18])。</p> 
<p>MMLU临床主题 在MMLU临床主题上，Med-PaLM 2显著改进了Med-PaLM[1]先前报告的结果，在6个主题中的3个主题上处于最先进水平，GPT-4-base在其他三个主题上报告的数字更好。我们注意到，如表1所报告的，每个这些主题的测试集都很小。</p> 
<p>有趣的是，我们在这些多项选择基准测试上观察到GPT-4-base和对齐(生产)GPT-4模型之间的性能下降(表4)。另一方面，Med-PaLM 2在多项选择基准测试上表现出强大的性能，同时专门针对长形式医疗问答的要求进行对齐。虽然多项选择基准测试是这些模型中编码知识的有用衡量 standard，但我们认为人工评估模型答案在临床上相关的轴线(如第4.2节进一步详细说明)是必要的，以评估它们在现实世界的临床应用中的实用性。</p> 
<p>我们还在表5中看到，集成精炼在这些基准测试中引出模型强大性能方面优于少样本和自我一致性提示策略。</p> 
<p><strong>重叠分析（Overlap analysis ） </strong>使用第3.4节描述的方法，重叠百分比从MedQA的0.9%到MMLU医学遗传学的48.0%。Med-PaLM 2在9个数据集中的6个数据集上的重叠问题上的性能略高，尽管差异仅在MedMCQA上在统计上显著(准确性差异4.6%，[1.3，7.7])，这是由于大多数数据集中重叠问题的数量相对较小(表6)。当我们将重叠段长度从512字符减少到120字符时(见第3.4节)，重叠百分比增加(MedQA的11.15%到MMLU医学遗传学</p> 
<p></p> 
<h4 id="h_629973364_11">4.2 长形式评估</h4> 
<p></p> 
<p class="img-center"><img alt="" height="397" src="https://images2.imgbox.com/1f/ce/6rF15Clf_o.png" width="720"></p> 
<p><strong>独立评估 </strong>在MultiMedQA 140数据集上，医生将Med-PaLM 2的答案评为与医生生成的答案和Med-PaLM生成的答案在我们评估的轴线上大致相当(图3和表A.2)。然而，我们探索的各轴线上的相对性能各异，分析在观察到的效应大小(差异)方面在很大程度上缺乏力量。这激发了下面在扩大的样本(MultiMedQA 1066)上进行的成对排名分析。观察到的唯一显著差异有利于Med-PaLM 2优于Med-PaLM(_p_&lt;0.05)，以下3个轴线:推理的证据，不正确的知识记忆和不正确的推理。</p> 
<p></p> 
<p class="img-center"><img alt="" height="564" src="https://images2.imgbox.com/df/ac/56qNyuo5_o.png" width="720"></p> 
<p>在对抗数据集上，医生将Med-PaLM 2的答案评为比Med-PaLM的答案在所有轴线上质量显著更高(_p_&lt;0.001，图3和表A.3)。这种模式同时适用于对抗数据集的一般和健康公平焦点子集(表A.3)。</p> 
<p></p> 
<p class="img-center"><img alt="" height="521" src="https://images2.imgbox.com/01/84/c3MWkr43_o.png" width="720"></p> 
<p></p> 
<p class="img-center"><img alt="" height="548" src="https://images2.imgbox.com/ff/d9/E79hAKVy_o.png" width="720"></p> 
<p></p> 
<p>最后，外行人将Med-PaLM 2对MultiMedQA 140数据集中的问题的答案评为比Med-PaLM的答案更有帮助和相关(p≤_0._002，图4和表A.4)。</p> 
<p></p> 
<p class="img-center"><img alt="" height="451" src="https://images2.imgbox.com/a6/c3/bxxt6TNI_o.png" width="720"></p> 
<p></p> 
<p class="img-center"><img alt="" height="120" src="https://images2.imgbox.com/b4/0b/k2wfXrmM_o.png" width="720"></p> 
<p>值得注意的是，Med-PaLM 2的答案比Med-PaLM和医生的答案更长(表A.9)。例如，在MultiMedQA 140上，Med-PaLM 2的答案的中位数长度为794个字符，而Med-PaLM为565.5个字符，医生为337.5个字符。对对抗性问题的答案长度总体上较长，Med-PaLM 2的答案的中位数长度为964个字符，Med-PaLM为518个字符，这可能反映了这些问题的更大复杂性。</p> 
<p><strong>成对排名评价 </strong>成对排名评估更明确地评估了Med-PaLM 2、Med-PaLM和医生的相对性能。此排名评估是在MultiMedQA 1066和对抗集扩大的集合上进行的。表A.7和A.8分别包含定性示例及其排名，以提供表示性示例和见解。</p> 
<p>在MultiMedQA上，在九个轴线中的八个轴线上，Med-PaLM 2的答案更常被评为比医生的答案质量更高(p&lt;0.001，图1和表A.5)。例如，它们更常被评为更好地反映医学共识，或表明更好的阅读理解；并且不太常被评为遗漏重要信息或代表危害的风险。 然而，在其中一个轴线上，包含不准确或无关的信息，Med-PaLM 2的答案并不如医生的答案有利。</p> 
<p>Med-PaLM 2的答案在相同的八个轴线上被评为比Med-PaLM的答案质量更高(图5和表A.6)；Med-PaLM 2的答案被标记为包含不准确或无关的信息的频率低于Med-PaLM的答案(Med-PaLM 2的18.4%vs. Med-PaLM的21.5%)，但差异不是显著的(p=0.12，表A.6)。</p> 
<p>在对抗性问题上，Med-PaLM 2在每个轴线上获得的排名都比Med-PaLM更有利(图5)，常常是相当大的差距。</p> 
<p></p> 
<p></p> 
<p class="img-center"><img alt="" height="446" src="https://images2.imgbox.com/d4/d5/wjcFJlVj_o.png" width="720"></p> 
<p></p> 
<h3 id="h_629973364_12">5 讨论&amp;结论等：</h3> 
<p>我们表明Med-PaLM 2在多项选择和长形式医疗问答中表现出色，包括流行的基准测试和具有挑战性的新对抗数据集。我们证明在每个MultiMedQA多项选择基准测试中表现接近或超过最先进的水平，包括MedQA、PubMedQA、MedMCQA和MMLU临床主题。我们显示Med-PaLM评估的医生和外行人在多个质量和安全轴线上的长形式答案获得了实质性的提高。此外，我们观察到Med-PaLM 2的答案在消费者医疗问题和对抗性问题的多个评估轴线上优于医生生成的答案。</p> 
<p>随着LLM在知识结构化测试中的技能越来越强，划定和评估其临床相关维度的能力变得越来越重要。我们的评估框架检查长形式模型输出与人类对高质量医疗答案的期望的一致性。我们使用对抗问题集也使LLM性能在困难案例中的明确研究成为可能。Med-PaLM 2相对于Med-PaLM的明显改进表明，仔细开发和评估具有挑战性的问答任务对确保模型性能的健壮性是必需的。</p> 
<p>这些结果证明LLM在医生级医疗问答方面正在迅速进步。然而，随着技术在实际应用中得到更广泛的采用，有必要在验证、安全和伦理方面做进一步的工作。在医疗问答和实际工作流程的不同背景下仔细和严格评估和精炼LLM将是确保这项技术对医学和健康产生积极影响的必要条件。</p> 
<p>不同的上下文和应用可能需要采取不同的方法来开发、评估和部署这些系统，以最大限度地发挥其潜力并最小化风险。例如，在某些情况下，人工智能系统可能最适合作为辅助工具，而在其他情况下，它们可以在人工监督下半自动完成某些任务。探索这些上下文中的最佳部署方法将是研究的一个关键方向。</p> 
<p>总之，尽管LLM在医疗问答方面取得了巨大进步，但仍需做进一步工作以确保在实践中安全、负责任和有效地将其应用于患者护理。我们希望本文提出的评估框架和数据集能为研究人员和开发人员提供指导，帮助推动这一工作的进展。</p> 
<h3 id="h_629973364_13">我的一些思考：</h3> 
<p>在LLM应用的场合中，最让人头疼的一点无疑是有时LLM不顾事实乱给回复，医疗问答因为关乎病人的健康，所以对回答内容的要求无疑是最严格的一种。Google Research和DeepMind在这篇文章里对优化这些问题，提供了很好的借鉴。</p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/81a934a81635fda83e9c3c0dcd56d742/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">深度学习核心技术与实践之深度学习基础篇</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/42874d1bf2d004e1fb8f473eb98d8e8c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">上传文件提示413 Request Entity Too Large错误</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>