<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>神经网络部署--网络压缩技术Network Compression - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="神经网络部署--网络压缩技术Network Compression" />
<meta property="og:description" content="1 network pruning network pruning技术的思路是：训练一个大的神经网络→评价网络中参数和神经元的重要度→根据重要度排序，移除不重要的weight和神经元（置0）
为什么要训练一个大的网络再压缩，而不是直接训练一个小的网络呢？
因为研究表明，大的网络比较容易训练，效果也更好。
2 knowledge distillation 训练一个大的，效果好的神经网络→训练一个小的网络，但是损失函数不以实际值为参照，而是以大的网络的输出为参照进行训练
这种做法通常效果不佳
3 parameter quantization parameter quantization是一种减少神经网络需要的储存空间的技术
采用较少的bit来储存神经网络的值首先将weight进行聚类，直接储存类别和类别代表的值，而不是储存每一个weight，这样就会减少存储的需要的空间也可以采用huffman编码等方式来减少储存采用binary connect来设置权重，即权重都是&#43;1或者-1 4 Architecture Design 设计网络架构来减少权重的个数，例如下图的网络，从前一层神经元个数N到后面一层神经元个数是M，W的个数为N×M。
如果在两层之间增加一层的话，如下图所示，那么W的个数为N×K&#43;M×K，合理设置K的值，就能使权重的数量减少
现在采用这种技术的网络主要有：
depth separable convolutionsqueezeNetmobileNetshuffleNetXception 5 dynamic computaion 动态计算追寻的是一种“先求有，再求好”理念，例如著名的Multi-Scale Dense Networks，首先训练一个多重分类器，然后在选择其中某一层的输出作为最终的分类器。
训练完一整个分类器后，可以将layer1或者layer2的输出作为压缩的分类器使用。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/74c809a350b32b72135a9759d6ea6ec5/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-04-07T11:29:23+08:00" />
<meta property="article:modified_time" content="2020-04-07T11:29:23+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">神经网络部署--网络压缩技术Network Compression</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h4><a id="1_network_pruning_0"></a>1 network pruning</h4> 
<p>network pruning技术的思路是：训练一个大的神经网络→评价网络中参数和神经元的重要度→根据重要度排序，移除不重要的weight和神经元（置0）<br> <img src="https://images2.imgbox.com/fc/f6/TE0M13qp_o.png" alt="在这里插入图片描述"><br> 为什么要训练一个大的网络再压缩，而不是直接训练一个小的网络呢？<br> 因为研究表明，大的网络比较容易训练，效果也更好。</p> 
<h4><a id="2_knowledge_distillation_5"></a>2 knowledge distillation</h4> 
<p>训练一个大的，效果好的神经网络→训练一个小的网络，但是损失函数不以实际值为参照，而是以大的网络的输出为参照进行训练<br> 这种做法通常效果不佳</p> 
<h4><a id="3_parameter_quantization_9"></a>3 parameter quantization</h4> 
<p>parameter quantization是一种减少神经网络需要的储存空间的技术</p> 
<ul><li>采用较少的bit来储存神经网络的值</li><li>首先将weight进行聚类，直接储存类别和类别代表的值，而不是储存每一个weight，这样就会减少存储的需要的空间</li><li>也可以采用huffman编码等方式来减少储存</li><li>采用binary connect来设置权重，即权重都是+1或者-1</li></ul> 
<h4><a id="4_Architecture_Design_16"></a>4 Architecture Design</h4> 
<p>设计网络架构来减少权重的个数，例如下图的网络，从前一层神经元个数N到后面一层神经元个数是M，W的个数为N×M。<br> <img src="https://images2.imgbox.com/45/ed/nVIL3TX1_o.png" alt="在这里插入图片描述"><br> 如果在两层之间增加一层的话，如下图所示，那么W的个数为N×K+M×K，合理设置K的值，就能使权重的数量减少<br> <img src="https://images2.imgbox.com/97/2e/3pfqUrTQ_o.png" alt="在这里插入图片描述"><br> 现在采用这种技术的网络主要有：</p> 
<ul><li>depth separable convolution</li><li>squeezeNet</li><li>mobileNet</li><li>shuffleNet</li><li>Xception</li></ul> 
<h4><a id="5_dynamic_computaion_29"></a>5 dynamic computaion</h4> 
<p>动态计算追寻的是一种“先求有，再求好”理念，例如著名的Multi-Scale Dense Networks，首先训练一个多重分类器，然后在选择其中某一层的输出作为最终的分类器。<br> <img src="https://images2.imgbox.com/8d/76/LbSapsdY_o.png" alt="在这里插入图片描述"><br> 训练完一整个分类器后，可以将layer1或者layer2的输出作为压缩的分类器使用。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/3c44bc8fce0d9113b39151ce4e51eb24/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">什么是跳板机？XShell如何通过跳板机连接内网机器？</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/9e502eaa0fb7acfb297d32dc843b38e2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Python学习笔记#4：快速生成二维矩阵的方法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>