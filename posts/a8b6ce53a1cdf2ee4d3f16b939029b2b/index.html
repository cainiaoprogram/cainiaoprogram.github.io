<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Torch - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Torch" />
<meta property="og:description" content="torch的安装 torch的版本与python的版本是挂钩的，python低版本安装不了高版本的torch（亲测）。
python 3.6版本出问题
在下面的网站中找到 https://pytorch.org/
python版本将对应torch的版本
https://www.cnblogs.com/tingtin/p/13601104.html(对应关系表)
cuda官网
https://developer.nvidia.cn/cuda-toolkit-archive torch对应cuda版本 先配置好cuda然后再装torch
https://download.pytorch.org/whl/torch_stable.html
print(torch.version.cuda)
#-f表示在给定链接中发现版本 pip install torch===1.4.0&#43;cu100 -f https://download.pytorch.org/whl/torch_stable.html torch_scatter对应的cuda版本 https://pytorch-geometric.com/whl/torch-1.4.0.html
检查torch是否可用 import torch torch.cuda.is_available() torch可用gpu的数量 import torch print(torch.cuda.device_count()) # 可用gpu数量 数据加载 torch.load()
*.pt文件保存了模型所有的参数load_state_dcit
而load_state_dict是net的一个方法
是将torch.load加载出来的数据加载到net中
源码详解 https://www.cnblogs.com/marsggbo/p/12075356.html
函数 one_hoe编码 https://blog.csdn.net/stay_zezo/article/details/121931429
F.one_hot(torch.Tensor([1,1,1]).long()) grad_fn grad_fn： grad_fn用来记录变量是怎么来的，方便计算梯度
https://blog.csdn.net/zphuangtang/article/details/112788037#:~:text=grad_fn%EF%BC%9A%20grad_fn%E7%94%A8%E6%9D%A5%E8%AE%B0%E5%BD%95,%E6%9F%A5%E7%9C%8Bx%E7%9A%84%E6%A2%AF%E5%BA%A6%E5%80%BC%E3%80%82
reset_parameters https://blog.csdn.net/weixin_43593330/article/details/107580084
repeat https://www.cnblogs.com/luckforefforts/p/13663529.html
mm和mul mm是矩阵相乘
mul是矩阵对应位相乘法
https://blog.csdn.net/qq_39938666/article/details/86004474
bmm 计算两个tensor的矩阵乘积
torch.contiguous() toch.contiguous() 与torch.view配合使用
https://blog.csdn.net/qq_37828380/article/details/107855070
torch.permute() 改变对应位置，则对用位置进行转置，例如tensor[0,1]和tensor[1,0]进行了位置的转换。
torch.gather 在某一维度上按照索引值取对应的元素
https://zhuanlan.zhihu.com/p/344962512
向量拼接 import torch A=torch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/a8b6ce53a1cdf2ee4d3f16b939029b2b/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-10-14T20:42:47+08:00" />
<meta property="article:modified_time" content="2022-10-14T20:42:47+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Torch</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h5><a id="torch_0"></a>torch的安装</h5> 
<ul><li>torch的版本与python的版本是挂钩的，python低版本安装不了高版本的torch（亲测）。<br> python 3.6版本出问题<br> <img src="https://images2.imgbox.com/90/9e/v7XFhbEg_o.png" alt="在这里插入图片描述"><br> 在下面的网站中找到</li></ul> 
<blockquote> 
 <p>https://pytorch.org/</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/54/82/9tt6RAiO_o.png" alt="在这里插入图片描述"><br> python版本将对应torch的版本</p> 
<blockquote> 
 <p>https://www.cnblogs.com/tingtin/p/13601104.html(对应关系表)<br> <img src="https://images2.imgbox.com/e7/55/O9tdvVXS_o.png" alt="在这里插入图片描述"><br> cuda官网</p> 
</blockquote> 
<pre><code class="prism language-bash">https://developer.nvidia.cn/cuda-toolkit-archive
</code></pre> 
<h5><a id="torchcuda_16"></a>torch对应cuda版本</h5> 
<p>先配置好cuda然后再装torch</p> 
<blockquote> 
 <p>https://download.pytorch.org/whl/torch_stable.html</p> 
</blockquote> 
<blockquote> 
 <p>print(torch.version.cuda)</p> 
</blockquote> 
<pre><code class="prism language-bash"><span class="token comment">#-f表示在给定链接中发现版本</span>
pip <span class="token function">install</span> <span class="token assign-left variable">torch</span><span class="token operator">==</span><span class="token operator">=</span><span class="token number">1.4</span>.0+cu100 -f https://download.pytorch.org/whl/torch_stable.html
</code></pre> 
<h6><a id="torch_scattercuda_28"></a>torch_scatter对应的cuda版本</h6> 
<blockquote> 
 <p>https://pytorch-geometric.com/whl/torch-1.4.0.html</p> 
</blockquote> 
<h5><a id="torch_31"></a>检查torch是否可用</h5> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h5><a id="torchgpu_37"></a>torch可用gpu的数量</h5> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device_count<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 可用gpu数量</span>
</code></pre> 
<h5><a id="_44"></a>数据加载</h5> 
<ol><li>torch.load()<br> *.pt文件保存了模型所有的参数</li><li>load_state_dcit<br> 而load_state_dict是net的一个方法<br> 是将torch.load加载出来的数据加载到net中<br> 源码详解</li></ol> 
<blockquote> 
 <p>https://www.cnblogs.com/marsggbo/p/12075356.html</p> 
</blockquote> 
<h5><a id="_53"></a>函数</h5> 
<h6><a id="one_hoe_54"></a>one_hoe编码</h6> 
<blockquote> 
 <p>https://blog.csdn.net/stay_zezo/article/details/121931429</p> 
</blockquote> 
<pre><code class="prism language-bash">F.one_hot<span class="token punctuation">(</span>torch.Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1,1</span>,1<span class="token punctuation">]</span><span class="token punctuation">)</span>.long<span class="token punctuation">(</span><span class="token punctuation">))</span>
</code></pre> 
<h6><a id="grad_fn_61"></a>grad_fn</h6> 
<p>grad_fn： grad_fn用来记录变量是怎么来的，方便计算梯度</p> 
<blockquote> 
 <p>https://blog.csdn.net/zphuangtang/article/details/112788037#:~:text=grad_fn%EF%BC%9A%20grad_fn%E7%94%A8%E6%9D%A5%E8%AE%B0%E5%BD%95,%E6%9F%A5%E7%9C%8Bx%E7%9A%84%E6%A2%AF%E5%BA%A6%E5%80%BC%E3%80%82</p> 
</blockquote> 
<h6><a id="reset_parameters_64"></a>reset_parameters</h6> 
<blockquote> 
 <p>https://blog.csdn.net/weixin_43593330/article/details/107580084</p> 
</blockquote> 
<h6><a id="repeat_67"></a>repeat</h6> 
<blockquote> 
 <p>https://www.cnblogs.com/luckforefforts/p/13663529.html</p> 
</blockquote> 
<h6><a id="mmmul_70"></a>mm和mul</h6> 
<p>mm是矩阵相乘<br> mul是矩阵对应位相乘法</p> 
<blockquote> 
 <p>https://blog.csdn.net/qq_39938666/article/details/86004474</p> 
</blockquote> 
<h6><a id="bmm_75"></a>bmm</h6> 
<p>计算两个tensor的矩阵乘积</p> 
<h6><a id="torchcontiguous_78"></a>torch.contiguous()</h6> 
<p>toch.contiguous() 与torch.view配合使用</p> 
<blockquote> 
 <p>https://blog.csdn.net/qq_37828380/article/details/107855070</p> 
</blockquote> 
<h6><a id="torchpermute_82"></a>torch.permute()</h6> 
<p>改变对应位置，则对用位置进行转置，例如tensor[0,1]和tensor[1,0]进行了位置的转换。<br> <img src="https://images2.imgbox.com/26/0f/KcDB4NZL_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="torchgather_85"></a>torch.gather</h5> 
<p>在某一维度上按照索引值取对应的元素</p> 
<blockquote> 
 <p>https://zhuanlan.zhihu.com/p/344962512</p> 
</blockquote> 
<h6><a id="_88"></a>向量拼接</h6> 
<pre><code class="prism language-bash">
<span class="token function">import</span> torch
<span class="token assign-left variable">A</span><span class="token operator">=</span>torch.ones<span class="token punctuation">(</span><span class="token number">2,3</span><span class="token punctuation">)</span> <span class="token comment">#2x3的张量（矩阵）</span>
<span class="token assign-left variable">B</span><span class="token operator">=</span><span class="token number">2</span>*torch.ones<span class="token punctuation">(</span><span class="token number">4,3</span><span class="token punctuation">)</span><span class="token comment">#4x3的张量（矩阵）</span>
<span class="token assign-left variable">C</span><span class="token operator">=</span>torch.cat<span class="token variable"><span class="token punctuation">((</span>A<span class="token punctuation">,</span>B<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span>#按维数<span class="token number">0</span>（行）拼接，要求两个矩阵另一个维度（列）相等<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>表示使用最后一个维度进行拼接
print<span class="token punctuation">(</span>C<span class="token punctuation">)</span>#
# tensor<span class="token punctuation">(</span>[[<span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span>]<span class="token punctuation">,</span>
#         [<span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span>]<span class="token punctuation">,</span>
#         [<span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span>]<span class="token punctuation">,</span>
#         [<span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span>]<span class="token punctuation">,</span>
#         [<span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span>]<span class="token punctuation">,</span>
#         [<span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span>]]<span class="token punctuation">)</span>
print<span class="token punctuation">(</span>C.size<span class="token punctuation">(</span><span class="token punctuation">))</span></span><span class="token comment">#torch.Size([6, 3])</span>
<span class="token assign-left variable">A</span><span class="token operator">=</span>torch.ones<span class="token punctuation">(</span><span class="token number">4,1</span><span class="token punctuation">)</span> <span class="token comment">#2x3的张量（矩阵）</span>
<span class="token assign-left variable">B</span><span class="token operator">=</span><span class="token number">2</span>*torch.ones<span class="token punctuation">(</span><span class="token number">4,3</span><span class="token punctuation">)</span><span class="token comment">#4x3的张量（矩阵）</span>
<span class="token assign-left variable">d</span><span class="token operator">=</span>torch.cat<span class="token variable"><span class="token punctuation">((</span>A<span class="token punctuation">,</span>B<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>#按维数<span class="token number">1</span>（列）拼接<span class="token punctuation">,</span>要求连个矩阵另一个维度相等，即这里要求两个矩阵行相等
print<span class="token punctuation">(</span>'d'<span class="token punctuation">,</span>d<span class="token punctuation">)</span>
#tensor<span class="token punctuation">(</span>[[<span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span>]<span class="token punctuation">,</span>
        # [<span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span>]<span class="token punctuation">,</span>
        # [<span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span>]<span class="token punctuation">,</span>
        # [<span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span>]]<span class="token punctuation">)</span>
print<span class="token punctuation">(</span>d.size<span class="token punctuation">(</span><span class="token punctuation">))</span></span><span class="token comment">#([4, 4])</span>
</code></pre> 
<h6><a id="torch__114"></a>torch 损失函数</h6> 
<p>l2损失就是torch.nn.MSELoss</p> 
<h5><a id="squeeze_116"></a>squeeze</h5> 
<p>对维度进行压缩或者填充</p> 
<blockquote> 
 <p>https://blog.csdn.net/xiexu911/article/details/80820028</p> 
</blockquote> 
<h5><a id="_120"></a>稀疏转稠密</h5> 
<pre><code class="prism language-bash">to_dense<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h5><a id="expand_126"></a>expand</h5> 
<blockquote> 
 <p>这个函数的作用就是对指定的维度进行数值大小的改变。只能改变维大小为1的维，否则就会报错。不改变的维可以传入-1或者原来的数值。</p> 
</blockquote> 
<h5><a id="torchnnLinear_129"></a>torch.nn.Linear</h5> 
<p>权重采用Xvaier initialization 初始化方式初始参数。</p> 
<h6><a id="torchnnModuleList_132"></a>torch.nn.ModuleList</h6> 
<p>可以主要应用于模型的参数初始化</p> 
<pre><code class="prism language-bash">class MyModel<span class="token punctuation">(</span>nn.Module<span class="token punctuation">)</span>:
    def __init__<span class="token punctuation">(</span>self<span class="token punctuation">)</span>:
        super<span class="token punctuation">(</span>MyModel, self<span class="token punctuation">)</span>.__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self.linears <span class="token operator">=</span> nn.ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>nn.linear <span class="token keyword">for</span> <span class="token for-or-select variable">i</span> <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token comment"># ModuleList can act as an iterable, or be indexed using ints</span>
    def forward<span class="token punctuation">(</span>self, x<span class="token punctuation">)</span>:
        <span class="token keyword">for</span> i, l <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>self.linears<span class="token punctuation">)</span>:
            x <span class="token operator">=</span> self.linears<span class="token punctuation">[</span>i // <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> + l<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token builtin class-name">return</span> x
</code></pre> 
<h6><a id="torchclamp_147"></a>torch.clamp</h6> 
<p><img src="https://images2.imgbox.com/1c/f7/PgCewfwv_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="expand_as_150"></a>expand_as</h5> 
<pre><code class="prism language-python">tensor_1<span class="token punctuation">.</span>expand<span class="token punctuation">(</span>size<span class="token punctuation">)</span>：把tensor_1扩展成size的形状
tensor_1<span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>tensor_2<span class="token punctuation">)</span> ：把tensor_1扩展成和tensor_2一样的形状
</code></pre> 
<h5><a id="masked_fill_157"></a>masked_fill</h5> 
<p>如果值等于括号左边的值，则填充为右边的值。</p> 
<pre><code class="prism language-bash"><span class="token function">import</span> torch
 
attn <span class="token operator">=</span> torch.randn<span class="token punctuation">(</span><span class="token number">3</span>, <span class="token number">3</span><span class="token punctuation">)</span>
<span class="token comment"># tensor([[-0.5928,  0.9060,  1.6383],</span>
<span class="token comment">#         [-0.1666,  0.6266,  0.6513],</span>
<span class="token comment">#         [-0.5957, -1.1430, -1.2773]])</span>
 
mask <span class="token operator">=</span> torch.tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span>, <span class="token number">0</span>, <span class="token number">0</span><span class="token punctuation">]</span>, <span class="token punctuation">[</span><span class="token number">0</span>, <span class="token number">1</span>, <span class="token number">0</span><span class="token punctuation">]</span>, <span class="token punctuation">[</span><span class="token number">0</span>, <span class="token number">0</span>, <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># tensor([[1, 0, 0],</span>
<span class="token comment">#         [0, 1, 0],</span>
<span class="token comment">#         [0, 0, 1]])</span>
 
<span class="token comment"># 将mask矩阵中为0的值填充为-1e9</span>
attn <span class="token operator">=</span> attn.masked_fill<span class="token punctuation">(</span>mask <span class="token operator">==</span> <span class="token number">0</span>, -1e9<span class="token punctuation">)</span>
<span class="token comment"># tensor([[-5.9281e-01, -1.0000e+09, -1.0000e+09],</span>
<span class="token comment">#         [-1.0000e+09,  6.2657e-01, -1.0000e+09],</span>
<span class="token comment">#         [-1.0000e+09, -1.0000e+09, -1.2773e+00]])</span>
</code></pre> 
<h5><a id="0_179"></a>删除为0的元素</h5> 
<pre><code class="prism language-python">nonZeroRows <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token number">0</span>
x <span class="token operator">=</span> x<span class="token punctuation">[</span>nonZeroRows<span class="token punctuation">]</span>
</code></pre> 
<h5><a id="_186"></a>找到任意元素的位置</h5> 
<pre><code class="prism language-bash">torch.nonzero<span class="token punctuation">(</span>a<span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">)</span>
</code></pre> 
<h5><a id="_192"></a>进行简单的模型搭建</h5> 
<blockquote> 
 <p>https://blog.csdn.net/littlle_yan/article/details/116131963</p> 
</blockquote> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> os

x <span class="token operator">=</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># x data (tensor), shape=(100, 1)</span>
y <span class="token operator">=</span> x<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">0.2</span><span class="token operator">*</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                 <span class="token comment"># noisy y data (tensor), shape=(100, 1)</span>

<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n_feature<span class="token punctuation">,</span> n_hidden<span class="token punctuation">,</span> n_output<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>hidden <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>n_feature<span class="token punctuation">,</span> n_hidden<span class="token punctuation">)</span>   <span class="token comment"># hidden layer</span>
        self<span class="token punctuation">.</span>predict <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>n_hidden<span class="token punctuation">,</span> n_output<span class="token punctuation">)</span>   <span class="token comment"># output layer</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token comment"># activation function for hidden layer</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x<span class="token punctuation">)</span>             <span class="token comment"># linear output</span>
        <span class="token keyword">return</span> x

net <span class="token operator">=</span> Net<span class="token punctuation">(</span>n_feature<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> n_hidden<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> n_output<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>     <span class="token comment"># define the network</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span>  <span class="token comment"># net architecture</span>

optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span>
loss_func <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># this is for regression mean squared loss</span>

plt<span class="token punctuation">.</span>ion<span class="token punctuation">(</span><span class="token punctuation">)</span>   <span class="token comment"># something about plotting</span>

<span class="token keyword">for</span> t <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    prediction <span class="token operator">=</span> net<span class="token punctuation">(</span>x<span class="token punctuation">)</span>     <span class="token comment"># input x and predict based on x</span>

    loss <span class="token operator">=</span> loss_func<span class="token punctuation">(</span>prediction<span class="token punctuation">,</span> y<span class="token punctuation">)</span>     <span class="token comment"># must be (1. nn output, 2. target)</span>

    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>   <span class="token comment"># clear gradients for next train</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>         <span class="token comment"># backpropagation, compute gradients</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># apply gradients</span>

    <span class="token keyword">if</span> t <span class="token operator">%</span> <span class="token number">5</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        <span class="token comment"># plot and show learning process</span>
        plt<span class="token punctuation">.</span>cla<span class="token punctuation">(</span><span class="token punctuation">)</span>
        plt<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>x<span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> prediction<span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'r-'</span><span class="token punctuation">,</span> lw<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span>
        plt<span class="token punctuation">.</span>text<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'Loss=%.4f'</span> <span class="token operator">%</span> loss<span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> fontdict<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token string">'size'</span><span class="token punctuation">:</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token string">'color'</span><span class="token punctuation">:</span>  <span class="token string">'red'</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
        plt<span class="token punctuation">.</span>pause<span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ioff<span class="token punctuation">(</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h5><a id="Fnll_loss_243"></a>F.nll_loss</h5> 
<p>常用于多分类任务，NLLLoss 函数输入 input 之前，需要对 input 进行 log_softmax 处理，即将 input 转换成概率分布的形式，并且取对数，底数为 e</p> 
<blockquote> 
 <p>https://www.cnblogs.com/leebxo/p/11913939.html</p> 
</blockquote> 
<h6><a id="torchsoftmax_247"></a>torch.softmax</h6> 
<p>torch.softmax实现</p> 
<pre><code class="prism language-bash"><span class="token function">import</span> torch
logits <span class="token operator">=</span> torch.Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span>, <span class="token number">2.0</span>, <span class="token number">3.0</span><span class="token punctuation">]</span>, <span class="token punctuation">[</span><span class="token number">1.0</span>, <span class="token number">2.0</span>, <span class="token number">3.0</span><span class="token punctuation">]</span>, <span class="token punctuation">[</span><span class="token number">1.0</span>, <span class="token number">2.0</span>, <span class="token number">3.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> torch.softmax<span class="token punctuation">(</span>logits,dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
y_ <span class="token operator">=</span> torch.Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span>, <span class="token number">0</span>, <span class="token number">1.0</span><span class="token punctuation">]</span>, <span class="token punctuation">[</span><span class="token number">0</span>, <span class="token number">0</span>, <span class="token number">1.0</span><span class="token punctuation">]</span>, <span class="token punctuation">[</span><span class="token number">0</span>, <span class="token number">0</span>, <span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
tf_log <span class="token operator">=</span> torch.log<span class="token punctuation">(</span>y<span class="token punctuation">)</span>
pixel_wise_mult <span class="token operator">=</span> -y_*tf_log

print<span class="token punctuation">(</span>torch.sum<span class="token punctuation">(</span>pixel_wise_mult,axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">))</span>
</code></pre> 
<h5><a id="_261"></a>梯度计算</h5> 
<pre><code class="prism language-bash"><span class="token assign-left variable">requires_grad</span><span class="token operator">=</span>True 要求计算梯度
<span class="token assign-left variable">requires_grad</span><span class="token operator">=</span>False 不要求计算梯度
with torch.no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>或者@torch.no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>中的数据不需要计算梯度，也不会进行反向传播
</code></pre> 
<h6><a id="_269"></a>梯度</h6> 
<p>当我们想对某个变量求导数时</p> 
<pre><code class="prism language-bash">x <span class="token operator">=</span> torch.tensor<span class="token punctuation">(</span><span class="token number">1</span>., <span class="token assign-left variable">requires_grad</span><span class="token operator">=</span>True<span class="token punctuation">)</span> <span class="token comment"># 第二种</span>
</code></pre> 
<h6><a id="_276"></a>参数不更新</h6> 
<pre><code class="prism language-bash">class Model<span class="token punctuation">(</span>nn.Module<span class="token punctuation">)</span>:
 def __init__<span class="token punctuation">(</span>self<span class="token punctuation">)</span>:
  super<span class="token punctuation">(</span>Transfer_model, self<span class="token punctuation">)</span>.__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
  self.linear1 <span class="token operator">=</span> nn.Linear<span class="token punctuation">(</span><span class="token number">20</span>, <span class="token number">50</span><span class="token punctuation">)</span>
  self.linear2 <span class="token operator">=</span> nn.Linear<span class="token punctuation">(</span><span class="token number">50</span>, <span class="token number">20</span><span class="token punctuation">)</span>
  self.linear3 <span class="token operator">=</span> nn.Linear<span class="token punctuation">(</span><span class="token number">20</span>, <span class="token number">2</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> Model<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 使用linear1层的参数不更新</span>
<span class="token keyword">for</span> <span class="token for-or-select variable">para</span> <span class="token keyword">in</span> model.linear1.parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>:
 para.requires_grad <span class="token operator">=</span> False
<span class="token comment"># 假如真的只有一层也可以这样操作：</span>
<span class="token comment"># model.linear1.weight.requires_grad = False</span>
<span class="token comment">#看到对应模型的名字</span>
<span class="token keyword">for</span> name, param <span class="token keyword">in</span> model.named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>:
    print<span class="token punctuation">(</span>name<span class="token punctuation">)</span>
</code></pre> 
<h5><a id="torchaddcmul__296"></a>torch.addcmul_</h5> 
<blockquote> 
 <p>https://guoyuantao.github.io/2019/06/23/pytorch-xue-xi-zhi-torch-shu-xue-cao-zuo-yi/</p> 
</blockquote> 
<h5><a id="torchnumpy_300"></a>torch转为numpy</h5> 
<pre><code class="prism language-bash">tensor.cpu<span class="token punctuation">(</span><span class="token punctuation">)</span>.detach<span class="token punctuation">(</span><span class="token punctuation">)</span>.numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h5><a id="numpytorch_305"></a>numpy转为torch</h5> 
<pre><code class="prism language-bash">numpy.cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h6><a id="_309"></a>给模型加参数</h6> 
<blockquote> 
 <p>https://blog.csdn.net/u011501388/article/details/100016577</p> 
</blockquote> 
<h5><a id="_313"></a>模型训练参数清零</h5> 
<blockquote> 
 <p>pyTorch中在反向传播前为什么要手动将梯度清零？</p> 
</blockquote> 
<p>pytorch不进行梯度清零操作，要想进行权重更新，则先需要进行梯度计算。</p> 
<h6><a id="torchcuda_319"></a>torch的cuda版本</h6> 
<pre><code class="prism language-bash">torch.version.cuda
</code></pre> 
<h6><a id="torchsoftmax_324"></a>torch.softmax</h6> 
<p>可见默认按行计算，即dim=1</p> 
<h6><a id="torchnnSoftmax_327"></a>torch.nn.Softmax</h6> 
<blockquote> 
 <p>https://blog.csdn.net/nefetaria/article/details/114411953</p> 
</blockquote> 
<h6><a id="_330"></a>@和*</h6> 
<p>@表示矩阵相乘<br> *表示矩阵对应元素相乘</p> 
<h6><a id="torchparameters_335"></a>torch中的parameters</h6> 
<blockquote> 
 <p>https://zhuanlan.zhihu.com/p/119305088</p> 
</blockquote> 
<h6><a id="torchnnEmbedding_338"></a>torch.nn.Embedding</h6> 
<p>一般用在自然语言处理中</p> 
<pre><code class="prism language-bash">
</code></pre> 
<h6><a id="torch__343"></a>torch 不同的学习率</h6> 
<p>torch依据不同层设置不同的学习率</p> 
<blockquote> 
 <p>https://blog.csdn.net/qq_17464457/article/details/101846874</p> 
</blockquote> 
<h5><a id="torchstack_348"></a>torch.stack</h5> 
<p>通过竖排形式拼接向量</p> 
<h5><a id="torchsplit_351"></a>torch.split</h5> 
<p>1表示占一列，dim=1表示使用列维度进行划分</p> 
<pre><code class="prism language-bash">torch.split<span class="token punctuation">(</span>features,1,dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre> 
<h5><a id="torch_357"></a>torch损失函数</h5> 
<p>torch 自带的损失函数与交叉熵损失还有一定的区别。</p> 
<blockquote> 
 <p>https://www.jb51.net/article/177665.htm</p> 
</blockquote> 
<h5><a id="torch_361"></a>torch权重分割</h5> 
<pre><code class="prism language-bash">    <span class="token comment">#############无用###############################################</span>
    tweights <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> <span class="token for-or-select variable">k</span> <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">0</span>,weights.shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>:
        <span class="token comment"># print("(weights[:,k]-weights[:,k-1]).tolist()",(weights[:,k]-weights[:,k-1]).tolist())</span>
        <span class="token comment"># if k&gt;2:</span>
        <span class="token comment">#     tweights.append((weights[:,k]-weights[:,k-1]).tolist())</span>
        <span class="token comment"># else:</span>
        <span class="token comment">#     tweights.append((weights[:,k]).tolist())</span>
        tweights.append<span class="token variable"><span class="token punctuation">((</span>weights[<span class="token operator">:</span><span class="token punctuation">,</span>k].tolist<span class="token punctuation">(</span><span class="token punctuation">))</span></span><span class="token punctuation">)</span>
    <span class="token comment"># print("torch.Tensor(tweights)",torch.Tensor(tweights))</span>
    tweights <span class="token operator">=</span> torch.Tensor<span class="token punctuation">(</span>tweights<span class="token punctuation">)</span>.T.cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment">##############有用##############################################</span>
    <span class="token comment"># tweights = []</span>
    <span class="token comment"># for tb in torch.split(weights,1,dim=1):</span>
    <span class="token comment">#     tweights.append(tb)</span>
    <span class="token comment"># tweights = torch.cat(tweights,axis=-1)</span>

    <span class="token comment"># print("weights",weights)</span>
    <span class="token comment"># print("tweights",tweights)</span>
    <span class="token comment">############################################################</span>
</code></pre> 
<h5><a id="Tensor_386"></a>返回Tensor中指定值对应的元素索引</h5> 
<pre><code class="prism language-bash">a <span class="token operator">=</span> torch.Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1,2</span>,3,4,5,2<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token punctuation">(</span>a<span class="token operator">==</span><span class="token number">2</span><span class="token punctuation">)</span>.nonzero<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#非0元素对应的索引值</span>
</code></pre> 
<h5><a id="torch_sparse_394"></a>torch_sparse</h5> 
<pre><code class="prism language-bash">a <span class="token operator">=</span>  torch.Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0,0</span>,1<span class="token punctuation">]</span>,<span class="token punctuation">[</span><span class="token number">1,0</span>,1<span class="token punctuation">]</span>,<span class="token punctuation">[</span><span class="token number">1,0</span>,0<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment">#此处有代码问题，做以参考</span>
torch_sparse.to_torch_sparse<span class="token punctuation">(</span>a.to_sparse<span class="token punctuation">(</span><span class="token punctuation">))</span>
torch_sparse.to_torch_sparse<span class="token punctuation">(</span>index<span class="token operator">=</span>torch.Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span>, <span class="token number">1</span>, <span class="token number">1</span>, <span class="token number">2</span><span class="token punctuation">]</span>,<span class="token punctuation">[</span><span class="token number">2</span>, <span class="token number">0</span>, <span class="token number">2</span>, <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>, <span class="token assign-left variable">value</span><span class="token operator">=</span>torch.Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span>., <span class="token number">1</span>., <span class="token number">1</span>., <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>,m<span class="token operator">=</span><span class="token number">3</span>,n<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>
<span class="token comment">#下面的做以参考</span>
adj <span class="token operator">=</span> adj.to_sparse<span class="token punctuation">(</span><span class="token punctuation">)</span>
rows <span class="token operator">=</span> torch.cat<span class="token punctuation">(</span><span class="token punctuation">[</span>ey_row,adj.coalesce<span class="token punctuation">(</span><span class="token punctuation">)</span>.indices<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>.long<span class="token punctuation">(</span><span class="token punctuation">)</span>
cols <span class="token operator">=</span> torch.cat<span class="token punctuation">(</span><span class="token punctuation">[</span>ey_col,adj.coalesce<span class="token punctuation">(</span><span class="token punctuation">)</span>.indices<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>.long<span class="token punctuation">(</span><span class="token punctuation">)</span>
values <span class="token operator">=</span> torch.cat<span class="token punctuation">(</span><span class="token punctuation">[</span>torch.Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>*len<span class="token punctuation">(</span>ey_index<span class="token punctuation">))</span>,adj.coalesce<span class="token punctuation">(</span><span class="token punctuation">)</span>.values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
adj <span class="token operator">=</span> SparseTensor<span class="token punctuation">(</span>row<span class="token operator">=</span>rows, <span class="token assign-left variable">col</span><span class="token operator">=</span>cols, <span class="token assign-left variable">value</span><span class="token operator">=</span>values,
    <span class="token assign-left variable">sparse_sizes</span><span class="token operator">=</span>adj.coalesce<span class="token punctuation">(</span><span class="token punctuation">)</span>.size<span class="token punctuation">(</span><span class="token punctuation">))</span>
</code></pre> 
<h5><a id="_410"></a>获取稀疏向量中的值</h5> 
<pre><code class="prism language-bash"> tdata.coalesce<span class="token punctuation">(</span><span class="token punctuation">)</span>.indices<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h5><a id="_415"></a>模型参数量统计</h5> 
<pre><code class="prism language-bash">num_params <span class="token operator">=</span> sum<span class="token punctuation">(</span>param.numel<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> <span class="token for-or-select variable">param</span> <span class="token keyword">in</span> model.parameters<span class="token punctuation">(</span><span class="token punctuation">))</span>
print<span class="token punctuation">(</span><span class="token string">"statistical model parameters"</span>, num_params<span class="token punctuation">)</span>
</code></pre> 
<h5><a id="torch__422"></a>torch 可视化网络</h5> 
<blockquote> 
 <p>https://blog.csdn.net/CFH1021/article/details/105359587/</p> 
</blockquote> 
<h5><a id="torchTensor_425"></a>torch取Tensor中的值</h5> 
<pre><code class="prism language-bash">Tensor.item<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/38c3c8cde7bb8eb242e9fe6dc539086d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">在docker 训练新环境中mmclassification装包遇见的一些问题</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f00f47ac23fb560aa88f4510d5573b01/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">VB.NET语法基础学习</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>