<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>transformer - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="transformer" />
<meta property="og:description" content=" 简介 transformer最早于2017年google机器翻译团队提出，也就是著名的
《Attention Is All You Need》，transformer完全取代了以往的RNN和CNN结构，改为由transformer堆叠的方式构建模型。
transformer在NLP领域首先取得了非常惊人的效果，随后,ECCV2020，DETR：《End-to-End Object Detection with Transformers 》首次将transformer引入到了CV的目标检测任务重，随后VIT：《An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale》完全抛弃了CNN，改为完全由transformer实现基础的图像分类任务，之后transformer在CV领域的应用也变得一发不可收拾。
基本概念 Transformer transformer是一种网络结构，是一种seq2seq的模型，最开始用于处理机器翻译任务，transformer由encoder和decoder组成，encoder或者decoder又是由多个encoder block和decoder block堆叠而成，encoder block和decoder block分别是用直连，Multi-Head Attention，BN，全连接等基础层通过不同的方式组合连接而成。
Multi-Head Attention Multi-Head Attention 包含多个 Self-Attention 层，同一个输入分别传递到 n个不同的 Self-Attention 中，计算得到 n 个输出结果。得到n个输出矩阵之后，Multi-Head Attention 将它们拼接在一起 (Concat)，然后传入一个Linear层，得到 Multi-Head Attention 最终的输出 。
Self-attention self-attention是一种新的layer，输入输出都是sequence，不同于RNN的是，self-attention layer可以做到并行。
Positional Encoding self-attention的特性很好的实现了时序上的并行，但是也带来了其他问题，那就是没有了位置信息，一个sequence上不同位置的信息，self-attention是做同等处理的，这显然不符合NLP天然的时序逻辑，positional encoding的引入就是为了解决这个问题。
self-attention Multi-Head Attention Positional Encoding " />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/5fe05153b77725154d13c7675a99c675/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-09-28T15:43:10+08:00" />
<meta property="article:modified_time" content="2021-09-28T15:43:10+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">transformer</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_0"></a>简介</h2> 
<p>transformer最早于2017年google机器翻译团队提出，也就是著名的<br> <a href="https://arxiv.org/abs/1706.03762" rel="nofollow">《Attention Is All You Need》</a>，transformer完全取代了以往的RNN和CNN结构，改为由transformer堆叠的方式构建模型。<br> transformer在NLP领域首先取得了非常惊人的效果，随后,ECCV2020，DETR：<a href="https://ai.facebook.com/research/publications/end-to-end-object-detection-with-transformers" rel="nofollow">《End-to-End Object Detection with Transformers 》</a>首次将transformer引入到了CV的目标检测任务重，随后VIT：<a href="https://arxiv.org/abs/2010.11929" rel="nofollow">《An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale》</a>完全抛弃了CNN，改为完全由transformer实现基础的图像分类任务，之后transformer在CV领域的应用也变得一发不可收拾。</p> 
<h2><a id="_4"></a>基本概念</h2> 
<h3><a id="Transformer_5"></a>Transformer</h3> 
<p>transformer是一种网络结构，是一种seq2seq的模型，最开始用于处理机器翻译任务，transformer由encoder和decoder组成，encoder或者decoder又是由多个encoder block和decoder block堆叠而成，encoder block和decoder block分别是用直连，Multi-Head Attention，BN，全连接等基础层通过不同的方式组合连接而成。<br> <img src="https://images2.imgbox.com/a4/16/gxGgKuwB_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="MultiHead_Attention_9"></a>Multi-Head Attention</h3> 
<p>Multi-Head Attention 包含多个 Self-Attention 层，同一个输入分别传递到 n个不同的 Self-Attention 中，计算得到 n 个输出结果。得到n个输出矩阵之后，Multi-Head Attention 将它们拼接在一起 (Concat)，然后传入一个Linear层，得到 Multi-Head Attention 最终的输出 。</p> 
<h3><a id="Selfattention_12"></a>Self-attention</h3> 
<p>self-attention是一种新的layer，输入输出都是sequence，不同于RNN的是，self-attention layer可以做到并行。</p> 
<h3><a id="Positional_Encoding_15"></a>Positional Encoding</h3> 
<p>self-attention的特性很好的实现了时序上的并行，但是也带来了其他问题，那就是没有了位置信息，一个sequence上不同位置的信息，self-attention是做同等处理的，这显然不符合NLP天然的时序逻辑，positional encoding的引入就是为了解决这个问题。</p> 
<h2><a id="selfattention_18"></a>self-attention</h2> 
<h2><a id="MultiHead_Attention_19"></a>Multi-Head Attention</h2> 
<h2><a id="Positional_Encoding_20"></a>Positional Encoding</h2>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/63a52eeeefaa1e929e3135a75855a3a0/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">位运算小技巧</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/64a0435e95852974778af67c88b98952/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">python图像灰度变换</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>