<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>autograd与逻辑回归 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="autograd与逻辑回归" />
<meta property="og:description" content="一、autograd—自动求导系统 torch.autograd.backward() torch.autograd.backward()是PyTorch中用于计算梯度的函数。以下是对该函数的参数的解释：
功能：自动求取梯度
• tensors: 用于求导的张量，如 loss
• retain_graph : 保存计算图
• create_graph : 创建导数计算图，用于高阶求导
• grad_tensors：多梯度权重
tensors：需要计算梯度的张量或张量的列表。这些张量的requires_grad属性必须为True。grad_tensors：可选参数，用于指定关于tensor的外部梯度。默认为None，表示使用默认的梯度为1。retain_graph：可选参数，用于指定是否保留计算图以供后续计算。默认为None，表示根据需要自动释放计算图。create_graph：可选参数，用于指定是否创建计算图以支持高阶梯度计算。默认为False，表示不创建计算图。 该函数的作用是计算tensors中张量的梯度，使用链式法则将梯度传播到叶子结点。它会自动构建计算图，并使用反向传播算法计算梯度。
当y = (x &#43; w) * (w &#43; 1)，a = x &#43; w，b = w &#43; 1，y = a * b时对于w的梯度的推导如下：
𝜕y/𝜕w = (𝜕y/𝜕a) * (𝜕a/𝜕w) &#43; (𝜕y/𝜕b) * (𝜕b/𝜕w)
= b * 1 &#43; a * 1
= b &#43; a
= (w &#43; 1) &#43; (x &#43; w)" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/243d1d3ad2f2cd1734ee6e5c84537a2d/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-02T16:59:01+08:00" />
<meta property="article:modified_time" content="2024-01-02T16:59:01+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">autograd与逻辑回归</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night-eighties">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="autograd_0"></a>一、autograd—自动求导系统</h2> 
<h4><a id="torchautogradbackward_2"></a>torch.autograd.backward()</h4> 
<p><code>torch.autograd.backward()</code>是PyTorch中用于计算梯度的函数。以下是对该函数的参数的解释：</p> 
<p>功能：自动求取梯度<br> • tensors: 用于求导的张量，如 loss<br> • retain_graph : 保存计算图<br> • create_graph : 创建导数计算图，用于高阶求导<br> • grad_tensors：多梯度权重</p> 
<ul><li><code>tensors</code>：需要计算梯度的张量或张量的列表。这些张量的<code>requires_grad</code>属性必须为<code>True</code>。</li><li><code>grad_tensors</code>：可选参数，用于指定关于<code>tensor</code>的外部梯度。默认为<code>None</code>，表示使用默认的梯度为1。</li><li><code>retain_graph</code>：可选参数，用于指定是否保留计算图以供后续计算。默认为<code>None</code>，表示根据需要自动释放计算图。</li><li><code>create_graph</code>：可选参数，用于指定是否创建计算图以支持高阶梯度计算。默认为<code>False</code>，表示不创建计算图。</li></ul> 
<p>该函数的作用是计算<code>tensors</code>中张量的梯度，使用链式法则将梯度传播到叶子结点。它会自动构建计算图，并使用反向传播算法计算梯度。</p> 
<p>当y = (x + w) * (w + 1)，a = x + w，b = w + 1，y = a * b时对于w的梯度的推导如下：<br> 𝜕y/𝜕w = (𝜕y/𝜕a) * (𝜕a/𝜕w) + (𝜕y/𝜕b) * (𝜕b/𝜕w)<br> = b * 1 + a * 1<br> = b + a<br> = (w + 1) + (x + w)<br> = 2w + x + 1<br> = 2 * 1 + 2 + 1<br> = 5<br> 因此，当y = (x + w) * (w + 1)时，对于w的梯度为5。<br> <img src="https://images2.imgbox.com/fe/1c/vztYBtNM_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/97/f1/1w4eKnsv_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="torchautogradgrad_33"></a>torch.autograd.grad（）</h4> 
<p><code>torch.autograd.grad()</code>是PyTorch中用于计算梯度的函数。以下是对该函数的参数的解释：<br> 功能：求取梯度<br> • outputs: 用于求导的张量，如 loss<br> • inputs : 需要梯度的张量<br> • create_graph : 创建导数计算图，用于高阶求导<br> • retain_graph : 保存计算图<br> • grad_outputs：多梯度权重</p> 
<ul><li><code>outputs</code>：需要计算梯度的标量或标量的列表。这些标量通常是模型的损失函数。</li><li><code>inputs</code>：关于哪些输入变量计算梯度。可以是单个张量或张量的列表。</li><li><code>grad_outputs</code>：可选参数，用于指定关于<code>outputs</code>的外部梯度。默认为<code>None</code>，表示使用默认的梯度为1。</li><li><code>retain_graph</code>：可选参数，用于指定是否保留计算图以供后续计算。默认为<code>None</code>，表示根据需要自动释放计算图。</li><li><code>create_graph</code>：可选参数，用于指定是否创建计算图以支持高阶梯度计算。默认为<code>False</code>，表示不创建计算图。<br> 该函数的作用是计算<code>outputs</code>关于<code>inputs</code>的梯度。它会自动构建计算图，并使用反向传播算法计算梯度。</li></ul> 
<p><strong>autograd小贴士：</strong></p> 
<ol><li>梯度不自动清零</li><li>依赖于叶子结点的结点，requires_grad默认为True</li><li>叶子结点不可执行in-place</li></ol> 
<h2><a id="_54"></a>二、逻辑回归</h2> 
<p><img src="https://images2.imgbox.com/3d/c3/XEvnt3cc_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/f7/07/Mgj8jNDb_o.png" alt="在这里插入图片描述"></p> 
<p><strong>线性回归是分析自变量x与因变量y(标量)之间关系的方法<br> 逻辑回归是分析自变量x与因变量y(概率)之间关系的方法</strong><br> <img src="https://images2.imgbox.com/a0/ab/A2LWNAwT_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/c2/f0/NEPzC4sQ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/a1/4f/lsQp25dU_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/3c/00/OS7zdeVD_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/54/57/is9dblAB_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/c2/a6/dvfsQ2CH_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/201d2375aa030a53492b6384a268016a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">1394C-SJT05-T-RL AB罗克韦尔伺服控制器</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1760bbf4d76d5432b77d1920ade01efd/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">解释 Git 的基本概念和使用方式</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>