<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>深度学习mAP - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="深度学习mAP" />
<meta property="og:description" content="mAP，其中代表P（Precision）精确率。AP（Average precision）单类标签平均（各个召回率中最大精确率的平均数）的精确率，mAP(Mean Average Precision)所有类标签的平均精确率。
1.准确率(accuracy)、精确率(Precision)、召回率(Recall) 准确率(accuracy),精确率(Precision)和召回率(Recall)是信息检索，人工智能，和搜索引擎的设计中很重要的几个概念和指标。
准确率：预测标签与ground truth的正确率。
精确率：预测为正标签的样本中的准确率。Precision = TP / (TP &#43; FP)
召回率：体现了预测为负样本中标签为正（正样本漏检）的程度。Recall = TP/(TP &#43; FN)
举例：先假定一个具体场景作为例子。假如某个班级有男生80人,女生20人,共计100人，目标是找出所有女生,某人挑选出50个人，其中20人是女生，另外还错误的把30个男生也当作女生挑选出来了。
accuracy:分类正确的人占总人数的比例。他把其中70(20女&#43;50男)人判定正确了,而总人数是100人，所以它的accuracy就是70 %。
当正负样本比例失调时，单纯的依靠准确率不靠谱。例如99个负样本，1个正样本，将样本全部判断为负即可达到99%的准确率。
其中TP代表正确（TURE）预测出正样本（POSITIVE）、FN代表错误（FALSE）预测成负样本（NEGTIVE）而真实标签为正样本。
通过这张表,我们可以很容易得到例子中这几个分类的值:TP=20,FP=30,FN=0,TN=50。
“精确率”与“召回率”虽然没有必然的关系（从上面公式中可以看到），然而在大规模数据集合中，这两个指标却是相互制约的。放宽“检索策略”时，往往也会伴随出现一些不相关的结果，从而使准确率受到影响。而希望去除检索结果中的不相关样本时，务必要将“检索策略”定的更加严格，这样也会使有一些相关的样本不再能被检索到，从而使召回率受到影响。
2.P-R曲线 用训练好的模型得到所有测试样本的confidence score，本例中某一类有20个测试样本。（每一类的P-R曲线、AP均单独计算）
对该类的confidence score排序，得到：
计算top-1到top-N（N是所有测试样本个数，本文中为20）对应的precision和recall，这两个标准的定义如下：
直观的理解就是，第一次我们排序后第一个样本的confidence作为划分正负样本的阈值，此时，只有第一个判断为正，其他均为负样本（因为其他样本的confidence均小于第一个样本的confidence），计算该阈值情况下的recall(1/1=1)和precision(1/1=1)；然后第二次将排序后第二个样本的confidence作为划分正负样本的阈值，依次类推到最后一个。显然随着阈值的降低，我们选定的样本越来也多，recall一定会越来越高，而precision整体上会呈下降趋势。把recall当成横坐标，precision当成纵坐标，即可得到常用的precision-recall曲线。这个例子的P-R曲线如下： 3.AP计算 PASCAL VOC CHALLENGE自2010年后就换了新计算方法。新的计算方法假设这N个样本中有M个正例，那么我们会得到M个recall值（1/M, 2/M, …, M/M）,对于每个recall值r，我们可以计算出对应（r’ &gt; r）的最大precision，然后对这M个precision值取平均即得到最后的AP值。计算方法如下：​ 相应的Precision-Recall曲线中被用于计算AP的部分如下（每一个recall的节点都取precision的最大值连接起来）： 4.mAP AP衡量的是学出来的模型在每个类别上的好坏，mAP衡量的是学出的模型在所有类别上的好坏，得到AP后mAP的计算就变得很简单了，就是取所有AP的平均值。
参考文献：https://blog.csdn.net/hust_lmj/article/details/79052559" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/250be31daffd5fd1a6d5eef3035934ce/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-01-14T16:24:31+08:00" />
<meta property="article:modified_time" content="2019-01-14T16:24:31+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深度学习mAP</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><strong>mAP，其中代表P（Precision）精确率。AP（Average precision）单类标签平均（各个召回率中最大精确率的平均数）的精确率，mAP(Mean Average Precision)所有类标签的平均精确率。</strong></p> 
<h3><strong>1.</strong>准确率(accuracy)、精确率(Precision)、召回率(Recall)</h3> 
<p>准确率(accuracy),精确率(Precision)和召回率(Recall)是信息检索，人工智能，和搜索引擎的设计中很重要的几个概念和指标。</p> 
<p>准确率：预测标签与ground truth的正确率。</p> 
<p>精确率：预测为正标签的样本中的准确率。Precision = TP / (TP + FP)</p> 
<p>召回率：体现了预测为负样本中标签为正（正样本漏检）的程度。Recall =  TP/(TP + FN)</p> 
<p>举例：先假定一个具体场景作为例子。假如某个班级有男生80人,女生20人,共计100人，目标是找出所有女生,某人挑选出50个人，其中20人是女生，另外还错误的把30个男生也当作女生挑选出来了。</p> 
<p>accuracy:分类正确的人占总人数的比例。他把其中70(20女+50男)人判定正确了,而总人数是100人，所以它的accuracy就是70 %。</p> 
<p>当正负样本比例失调时，单纯的依靠准确率不靠谱。例如99个负样本，1个正样本，将样本全部判断为负即可达到99%的准确率。</p> 
<p><img alt="" class="has" height="205" src="https://images2.imgbox.com/e1/55/Y09H6KJV_o.png" width="600"></p> 
<p>其中TP代表正确（TURE）预测出正样本（POSITIVE）、FN代表错误（FALSE）预测成负样本（NEGTIVE）而真实标签为正样本。</p> 
<p>通过这张表,我们可以很容易得到例子中这几个分类的值:TP=20,FP=30,FN=0,TN=50。</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/b3/c9/MqiAZP2K_o.png"></p> 
<p>“精确率”与“召回率”虽然没有必然的关系（从上面公式中可以看到），然而在大规模数据集合中，这两个指标却是相互制约的。放宽“检索策略”时，往往也会伴随出现一些不相关的结果，从而使准确率受到影响。而希望去除检索结果中的不相关样本时，务必要将“检索策略”定的更加严格，这样也会使有一些相关的样本不再能被检索到，从而使召回率受到影响。</p> 
<h3>2.P-R曲线</h3> 
<p>用训练好的模型得到所有测试样本的confidence score，本例中某一类有20个测试样本。（每一类的P-R曲线、AP均单独计算）</p> 
<p><img alt="这里写图片描述" class="has" src="https://images2.imgbox.com/48/18/dm5z9JSd_o.png"></p> 
<p>对该类的confidence score排序，得到：</p> 
<p><img alt="这里写图片描述" class="has" src="https://images2.imgbox.com/73/e9/Ssicuyms_o.png"></p> 
<p>计算top-1到top-N（N是所有测试样本个数，本文中为20）对应的precision和recall，这两个标准的定义如下：</p> 
<p><img alt="这里写图片描述" class="has" height="215" src="https://images2.imgbox.com/97/23/cxQuzJJF_o.png" width="164"></p> 
<p>直观的理解就是，第一次我们排序后第一个样本的confidence作为划分正负样本的阈值，此时，只有第一个判断为正，其他均为负样本（因为其他样本的confidence均小于第一个样本的confidence），计算该阈值情况下的recall(1/1=1)和precision(1/1=1)；然后第二次将排序后第二个样本的confidence作为划分正负样本的阈值，依次类推到最后一个。显然随着阈值的降低，我们选定的样本越来也多，recall一定会越来越高，而precision整体上会呈下降趋势。把recall当成横坐标，precision当成纵坐标，即可得到常用的precision-recall曲线。这个例子的P-R曲线如下： </p> 
<p><img alt="这里写图片描述" class="has" src="https://images2.imgbox.com/00/97/m9HUSGU8_o.png"></p> 
<h3>3.AP计算</h3> 
<p>PASCAL VOC CHALLENGE自2010年后就换了新计算方法。新的计算方法假设这N个样本中有M个正例，那么我们会得到M个recall值（1/M, 2/M, …, M/M）,对于每个recall值r，我们可以计算出对应（r’ &gt; r）的最大precision，然后对这M个precision值取平均即得到最后的AP值。计算方法如下：​ <br><img alt="这里写图片描述" class="has" src="https://images2.imgbox.com/b0/10/CaIOu25Y_o.jpg"></p> 
<p>相应的Precision-Recall曲线中被用于计算AP的部分如下（每一个recall的节点都取precision的最大值连接起来）： </p> 
<p><img alt="这里写图片描述" class="has" src="https://images2.imgbox.com/e7/d6/j72ApAYR_o.png"></p> 
<h3>4.mAP</h3> 
<p>AP衡量的是学出来的模型在每个类别上的好坏，mAP衡量的是学出的模型在所有类别上的好坏，得到AP后mAP的计算就变得很简单了，就是取所有AP的平均值。</p> 
<p>参考文献：<a href="https://blog.csdn.net/hust_lmj/article/details/79052559">https://blog.csdn.net/hust_lmj/article/details/79052559</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a29383ec514534f4386055943452d916/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">浏览器CA认证流程</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/be527e8521e4693de417231029b84669/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">POJ1321 棋盘问题</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>