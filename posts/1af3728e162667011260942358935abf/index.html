<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【论文读后感】：A simple yet effective baseline for 3d human pose estimation - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【论文读后感】：A simple yet effective baseline for 3d human pose estimation" />
<meta property="og:description" content="原文链接，感谢原作者
先验知识摘要引言2.相关工作3.解决方法 3.1 我们的目标——没有蛀牙！（误！是网络的设计）3.2 数据预处理 4.实验评估 4.1 定量结果 先验知识 阅读本论文，要先稍微了解一下3d人体姿态估计的一些常用方法。其中本文所涉及到的，如下：
1.一般3d姿态估计的问题描述为：以一张RGB图像作为输入，要求程序输出图像中人体主要关节点的三维坐标，比如头部、两肩、腰部，腿部等等重要关节点。
2.有些算法使用端对端的思想，直接给网络输入一张RGB图，要求网络输出重要关节点的三维坐标。当然训练的数据也是输入RGB，输出三维坐标。
3.有些算法使用两步走的思想，第一步，从RGB图像中获取到二维的重要关节点坐标。第二步，训练一个网络，该网络输入是一系列二维关节点坐标，输出是一系列三维关节点坐标。【本文就是使用这种方式，本文主要描述第二步】
4.精确的，真实的数据叫做groundtruth数据，而观测数据，生成数据等等都是不能完全信的数据，比如，现实生活中用传感器测出来的三维关节点坐标，这是groundtruth数据，而用算法估计出来的数据，就是生成数据或预测数据。对于机器学习或深度学习的其他方向，这些名词也是通用的。
好的，先验知识就这么多，现在开始本文的翻译
摘要 3d人体姿态估计领域由于深度神经网络的使用，在端对端系统预测中已经取得不错的成果。但是我们还不确定那些错误的预测是因为以下哪种原因：
1.从2d姿态（视觉）理解中出的错【就是从图像中预测图像中的2d关节点这一步骤】。
2.从2d关节点到3d关节点的映射。
所以本文就是要找到错误的根源，并解决它。我们建立了一个模型，输入2d关节点信息，输出3d关节点对应的信息。发现对于这个过程，错误率很低。然后把我们的这个模型放在最新的2d检测模型上训练。最终得出结论：大多数的3d姿态估计的错误来自于视觉分析这一步【也就是上述的第1点】。
引言 现在很多3d姿态估计的方法使用从2d图像直接向3d姿态转换的端对端方式。也有使用2d语义信息向3d姿态转换的方式。本文对第一种方式进行解耦合：
1.先利用2d图像输入来估计2d姿态。
2.利用2d姿态信息映射3d姿态。
本文实验结果显示：用2d的groundtruth关节点来生成3d的关节点错误率非常低——比目前最好的结果好了30%。而我们用从2d图像生成的用关键点检测生成的2d关节点来生成3d关节点，这样的错误率只比目前最好的结果稍微好一点。
所以本文结果显示，我们以后的精力应当放在提高2d图像向2d姿态的转化这一步骤。
此外，本文发布了一个高性能、轻量级的baseline，代码见：https://github.com/una-dinosauria/3d-pose-baseline
2.相关工作 从图像中获取深度 从纯粹的二维输入中感知深度是一个经典的问题，至少从文艺复兴时期开始就吸引了科学家和艺术家的注意，当时布鲁内莱斯基在他的佛罗伦萨建筑绘画中使用了透视的数学概念来传达空间感。
自上而下的3d推理 最早的深度估计算法之一采用了一种不同的方法:利用场景中物体的已知三维结构。当感知人体运动时，抽象成一组稀疏的点投影。
从2d到3d关节点 从2d投影推理3d关节点的方法可以追溯到 Lee 和 Chen。
基于2d到3d关节点的深度网络
3.解决方法 我们的目标是给定一个二维输入x∈R 2n，输出一个三维空间的关节点坐标估计。也就是说，我们的输入是一系列二维的点x∈R 3n 。也就是说，我们希望训练出一个函数 f ∗ : R 2 n → R 3 n 个姿态中有最小的预测错误率： 在实际操作中， xi可以是二维的groundtruth关节点坐标，也可以是根据图像估计出来的二维关节点坐标。而我们的目标就是找到一个简单的、可扩展的、高效的架构来设计一个神经网络去完成f*的任务。
3.1 我们的目标——没有蛀牙！（误！是网络的设计） 上图就是我们的网络的主要架构。我们的方法就是基于一个简单的、深度的、多层的神经网络以及批处理标准化、丢弃、RELUs以及残差连接。瞎子也能看出来，上图还有两个额外的线性层：第一个直接应用于输入，将输入增维至1024维。还有一个在产生预测之前应用，产生大小为3n的输出。在我们的实验中，我们一般有2个这样的残差块，所以一共有6个线性层，而我们的这个模型大概有4-5百万个参数需要训练。
2d/3d坐标 其他算法有使用二维原图像作为输入的，有些是以二维概率分布作为输入的，有些是以三维概率作为输出的，还有照相机参数估计作为输出的，与那些妖艳贱货不同，我们算法使用2d/3d姿态的坐标分别作为输入与输出。虽然二维所携带的信息较少，但它们的低维性非常吸引人。比如你可以吧Human 3.6M整个数据集都储存在GPU中以供训练，这样会大大地减少训练时长，而且，这样也让我们很自然地训练超参数。
线性-RELU层 大多数算法处理3d姿态估计的问题一般都是基于卷积神经网络的，因为卷积神经网络具有平移不变性。但是，因为我们以2d关节点坐标作为输入，所以我们不需要这个特性，我们可以采用更加节省成本的线性层，而RELUs则是一个标准的非线性层的选择。
残差连接 我们发现残差连接是最近提出的一种促进深度卷积神经网络训练的技术，还可以提高泛化性能、降低训练时间。在本文中，残差连接帮我们降低了10%的错误率。
批处理正则化和丢弃(dropout) 虽然有了上述的三个组件可以使这个网络在2d的groundtruth数据中表现良好，但是如果输入变成2d的估计坐标，或者在2d groundtruth数据中训练，而在有噪音的2d 观测数据中测试，那效果就不尽人意了。而批处理正则化和dropout使我们的网络在这两种情况中的性能得以提升，当然，代价就是训练时长稍稍增加。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/1af3728e162667011260942358935abf/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-16T18:19:27+08:00" />
<meta property="article:modified_time" content="2022-07-16T18:19:27+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【论文读后感】：A simple yet effective baseline for 3d human pose estimation</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><a href="https://blog.csdn.net/kid_14_12/article/details/86713412">原文链接，感谢原作者</a></p> 
<ul><li><a href="#_1" rel="nofollow noopener noreferrer" target="_self">先验知识</a></li><li><a href="#_8" rel="nofollow noopener noreferrer" target="_self">摘要</a></li><li><a href="#_15" rel="nofollow noopener noreferrer" target="_self">引言</a></li><li><a href="#2_24" rel="nofollow noopener noreferrer" target="_self">2.相关工作</a></li><li><a href="#3_31" rel="nofollow noopener noreferrer" target="_self">3.解决方法</a></li><li> 
  <ul><li><a href="#31__36" rel="nofollow noopener noreferrer" target="_self">3.1 我们的目标——没有蛀牙！（误！是网络的设计）</a></li><li><a href="#32__45" rel="nofollow noopener noreferrer" target="_self">3.2 数据预处理</a></li></ul> </li><li><a href="#4_53" rel="nofollow noopener noreferrer" target="_self">4.实验评估</a></li><li> 
  <ul><li><a href="#41__56" rel="nofollow noopener noreferrer" target="_self">4.1 定量结果</a></li></ul> </li></ul> 
<p></p> 
<h2><a id="_1"></a>先验知识</h2> 
<p>阅读本论文，要先稍微了解一下3d人体姿态估计的一些常用方法。其中本文所涉及到的，如下：<br> 1.一般3d姿态估计的问题描述为：以一张RGB图像作为输入，要求程序输出图像中人体主要关节点的三维坐标，比如头部、两肩、腰部，腿部等等重要关节点。<br> 2.有些算法使用<strong>端对端</strong>的思想，直接给网络输入一张RGB图，要求网络输出重要关节点的三维坐标。当然训练的数据也是输入RGB，输出三维坐标。<br> 3.有些算法使用两步走的思想，第一步，从RGB图像中获取到二维的重要关节点坐标。第二步，训练一个网络，该网络输入是一系列二维关节点坐标，输出是一系列三维关节点坐标。【本文就是使用这种方式，本文主要描述第二步】<br> 4.精确的，真实的数据叫做groundtruth数据，而观测数据，生成数据等等都是不能完全信的数据，比如，现实生活中用传感器测出来的三维关节点坐标，这是groundtruth数据，而用算法估计出来的数据，就是生成数据或预测数据。对于机器学习或深度学习的其他方向，这些名词也是通用的。<br> <strong>好的，先验知识就这么多，现在开始本文的翻译</strong></p> 
<h2><a id="_8"></a>摘要</h2> 
<p>3d人体姿态估计领域由于深度神经网络的使用，在端对端系统预测中已经取得不错的成果。但是我们还不确定那些错误的预测是因为以下哪种原因：<br> 1.从2d姿态（视觉）理解中出的错【就是从图像中预测图像中的2d关节点这一步骤】。<br> 2.从2d关节点到3d关节点的映射。<br> 所以本文就是要找到错误的根源，并解决它。我们建立了一个模型，输入2d关节点信息，输出3d关节点对应的信息。发现对于这个过程，错误率很低。然后把我们的这个模型放在最新的2d检测模型上训练。最终得出结论：大多数的3d姿态估计的错误来自于视觉分析这一步【也就是上述的第1点】。</p> 
<h2><a id="_15"></a>引言</h2> 
<p>现在很多3d姿态估计的方法使用从2d图像直接向3d姿态转换的端对端方式。也有使用2d语义信息向3d姿态转换的方式。本文对第一种方式进行解耦合：<br> 1.先利用2d图像输入来估计2d姿态。<br> 2.利用2d姿态信息映射3d姿态。<br> 本文实验结果显示：用2d的groundtruth关节点来生成3d的关节点错误率非常低——比目前最好的结果好了30%。而我们用从2d图像生成的用关键点检测生成的2d关节点来生成3d关节点，这样的错误率只比目前最好的结果稍微好一点。<br> 所以本文结果显示，我们以后的精力应当放在提高2d图像向2d姿态的转化这一步骤。<br> 此外，本文发布了一个高性能、轻量级的baseline，代码见：<a href="https://github.com/una-dinosauria/3d-pose-baseline">https://github.com/una-dinosauria/3d-pose-baseline</a></p> 
<h2><a id="2_24"></a>2.相关工作</h2> 
<p><strong>从图像中获取深度</strong> 从纯粹的二维输入中感知深度是一个经典的问题，至少从文艺复兴时期开始就吸引了科学家和艺术家的注意，当时布鲁内莱斯基在他的佛罗伦萨建筑绘画中使用了透视的数学概念来传达空间感。<br> <strong>自上而下的3d推理</strong> 最早的深度估计算法之一采用了一种不同的方法:利用场景中物体的已知三维结构。当感知人体运动时，抽象成一组稀疏的点投影。<br> <strong>从2d到3d关节点</strong> 从2d投影推理3d关节点的方法可以追溯到 Lee 和 Chen。<br> <strong>基于2d到3d关节点的深度网络</strong></p> 
<h2><a id="3_31"></a>3.解决方法</h2> 
<p>我们的目标是给定一个二维输入x∈R 2n，输出一个三维空间的关节点坐标估计。也就是说，我们的输入是一系列二维的点x∈R 3n 。也就是说，我们希望训练出一个函数 f ∗ : R 2 n → R 3 n 个姿态中有最小的预测错误率： </p> 
<p><img src="https://images2.imgbox.com/ab/07/8kq3D0SW_o.png" alt="在这里插入图片描述"></p> 
<p>在实际操作中， xi可以是二维的groundtruth关节点坐标，也可以是根据图像估计出来的二维关节点坐标。而我们的目标就是找到一个简单的、可扩展的、高效的架构来设计一个神经网络去完成f*的任务。</p> 
<h3><a id="31__36"></a>3.1 我们的目标——没有蛀牙！（误！是网络的设计）</h3> 
<p><img src="https://images2.imgbox.com/a7/16/n3WJdAnN_o.png" alt="图1"><br> 上图就是我们的网络的主要架构。我们的方法就是基于一个简单的、深度的、多层的神经网络以及批处理标准化、丢弃、RELUs以及残差连接。瞎子也能看出来，上图还有两个额外的线性层：第一个直接应用于输入，将输入增维至1024维。还有一个在产生预测之前应用，产生大小为3n的输出。在我们的实验中，我们一般有2个这样的残差块，所以一共有6个线性层，而我们的这个模型大概有4-5百万个参数需要训练。<br> <strong>2d/3d坐标</strong> 其他算法有使用二维原图像作为输入的，有些是以二维概率分布作为输入的，有些是以三维概率作为输出的，还有照相机参数估计作为输出的，与那些<strong>妖艳贱货</strong>不同，我们算法使用2d/3d姿态的坐标分别作为输入与输出。虽然二维所携带的信息较少，但它们的低维性非常吸引人。比如你可以吧Human 3.6M整个数据集都储存在GPU中以供训练，这样会大大地减少训练时长，而且，这样也让我们很自然地训练超参数。<br> <strong>线性-RELU层</strong> 大多数算法处理3d姿态估计的问题一般都是基于卷积神经网络的，因为卷积神经网络具有平移不变性。但是，因为我们以2d关节点坐标作为输入，所以我们不需要这个特性，我们可以采用更加节省成本的线性层，而RELUs则是一个标准的非线性层的选择。<br> <strong>残差连接</strong> 我们发现残差连接是最近提出的一种促进深度卷积神经网络训练的技术，还可以提高泛化性能、降低训练时间。在本文中，残差连接帮我们降低了10%的错误率。<br> <strong>批处理正则化和丢弃(dropout)</strong> 虽然有了上述的三个组件可以使这个网络在2d的groundtruth数据中表现良好，但是如果输入变成2d的估计坐标，或者在2d groundtruth数据中训练，而在有噪音的2d 观测数据中测试，那效果就不尽人意了。而批处理正则化和dropout使我们的网络在这两种情况中的性能得以提升，当然，代价就是训练时长稍稍增加。<br> <strong>2范数最大值约束</strong> 我们还对每一层的权值进行了约束，使其2范数最大值小于或等于1。结合批处理正则化，我们发现当训练和测试实例的分布不同时，这可以使训练稳定并提升泛化效果。</p> 
<h3><a id="32__45"></a>3.2 <a href="https://so.csdn.net/so/search?q=%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86&amp;spm=1001.2101.3001.7020" target="_blank" class="hl hl-1" rel="noopener noreferrer">数据预处理</a></h3> 
<p>我们对2d输入和3d输出使用标准的正则化，即减去中值除以标准差。由于我们无法预测3d预测的全局位置，我们以髋关节周围为零点。(符合之前的工作和Human3.6M标准协议)</p> 
<p><strong>相机坐标</strong> 在我们看来，期望算法推断任意坐标空间中的三维关节位置是不现实的，因为任意坐标空间的平移或旋转都不会导致输入数据的变化。全局坐标系的自然选择是摄像机坐标系，因为这使得不同摄像机之间的2d到3d问题相似，隐式地为每个摄像机提供更多的训练数据，并防止对特定全局坐标系的过度拟合。在任意的全局坐标系下推导三维位姿的一个直接影响是不能回归到每个子节点的全局方向，从而导致所有关节产生较大的误差。注意，这个坐标系的定义是任意的，并不意味着我们在我们的测试中使用了位姿的groundtruth。<br> <strong>2d探测结果</strong> 我们的2d探测结果来自于最新的在MPII数据集上预训练的Newell的堆叠沙漏网络。与先前的工作一样，我们使用H3.6M提供的bounding boxes来估计图像中的人的中心。我们剪裁一个440x440大小的图像。<br> 我们还在Human3.6M数据集上微调了堆叠沙漏网络（在MPII上预训练过），这样使得2d关节点坐标检测更加精确，进而减少3d姿态估计的误差。因为GPU的内存限制，我们将原来的最小batch szie从6改成3。除此之外，我们对堆叠沙漏网络都使用默认的参数。我们设置学习率为0.00025 ，迭代训练40 000次。<br> <strong>训练细节</strong> 我们使用Adam训练我们的网络200 epochs，开始的学习率是0.001并且指数递减，最小的batch size 是64。初始时，我们的线性层的权值使用Kaiming初始化。我们用tensorflow实现，初一个前向和反向传播过程花费大约5ms，在Titan Xp GPU只要2ms。也就是说，与最新的实时地2d探测模块一起，我们的网络可以达到实时的效果。在整个Human3.6M的数据集上训练一个epoch需要大概2分钟，这使我们可以训练一些架构的变种和超参数。</p> 
<h2><a id="4_53"></a>4.实验评估</h2> 
<p><strong>数据集和协议</strong> 我们使用Human3.6M的1,5,6,7,8来训练，用9,11来评估。</p> 
<h3><a id="41__56"></a>4.1 定量结果</h3> 
<p>。。。<br> 后面不翻译了，就放几张论文附带的图好了…<br> <img src="https://images2.imgbox.com/5b/27/UL9S6eGs_o.png" alt="test"><br> 上图左边是2d观测图，也就是输入图，中间的是3D groundtruth图，右边的是本文的预测结果图。</p> 
<p>对于作者提供的网络模型如图：<br> <img src="https://images2.imgbox.com/0e/c3/99CUmp74_o.png" alt="网络模型"><br> 建议保存下来放大看，当然，自己运行一下作者提供的demo，然后用tensorboard可视化一下也可以看到。</p> 
<div class="print_watermark_info"> 
 <p>内容来源：csdn.net</p> 
 <p>作者昵称：艾与代码</p> 
 <p>原文链接：https://blog.csdn.net/kid_14_12/article/details/86713412</p> 
 <p>作者主页：https://blog.csdn.net/kid_14_12kid_14_12</p> 
</div> 
<p><a href="https://blog.csdn.net/qq_45364953/article/details/108320215?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~OPENSEARCH~default-1-108320215-blog-125813201.pc_relevant_vip_default&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~OPENSEARCH~default-1-108320215-blog-125813201.pc_relevant_vip_default&amp;utm_relevant_index=2">另一个版本的解析，欢迎更多的分享</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6ef30397a244c3f67ac45f0a972b176a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">华为od js 日志排序</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/074737f76395939a887f8655b6e882ce/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">JDK1.8安装以及环境变量配置(win10)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>