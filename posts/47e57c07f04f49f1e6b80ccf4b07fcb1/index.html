<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>LSTM/GRU详细代码解析&#43;完整代码实现 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="LSTM/GRU详细代码解析&#43;完整代码实现" />
<meta property="og:description" content="LSTM和GRU目前被广泛的应用在各种预测场景中，并与卷积神经网络CNN或者图神经网络GCN这里等相结合，对数据的结构特征和时序特征进行提取，从而预测下一时刻的数据。在这里整理一下详细的LSTM/GRU的代码，并基于heatmap热力图实现对结果的展示。
一、GRU GRU的公式如下图所示：
其代码部分：
class GRU(torch.nn.Module): def __init__(self, hidden_size, output_size, num_layers): super().__init__() self.input_size = 1 self.hidden_size = hidden_size self.num_layers = num_layers self.output_size = output_size self.num_directions = 1 self.gru = torch.nn.GRU(self.input_size, self.hidden_size, self.num_layers, batch_first=True) self.linear = torch.nn.Linear(self.hidden_size, self.output_size) def forward(self, input_seq): # input(batch_size, seq_len, input_size) batch_size, seq_len = input_seq.shape[0], input_seq.shape[1] h_0 = torch.randn(self.num_directions * self.num_layers, batch_size, self.hidden_size).to(device) # output(batch_size, seq_len, num_directions * hidden_size) output, _ = self.gru(input_seq, (h_0)) pred = self." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/47e57c07f04f49f1e6b80ccf4b07fcb1/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-27T10:53:49+08:00" />
<meta property="article:modified_time" content="2023-07-27T10:53:49+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">LSTM/GRU详细代码解析&#43;完整代码实现</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>LSTM和GRU目前被广泛的应用在各种预测场景中，并与卷积神经网络CNN或者图神经网络GCN<a class="link-info" href="https://blog.csdn.net/m0_53961910/article/details/128007946" title="这里">这里</a>等相结合，对数据的结构特征和时序特征进行提取，从而预测下一时刻的数据。在这里整理一下详细的LSTM/GRU的代码，并基于heatmap热力图实现对结果的展示。</p> 
<h2>一、GRU</h2> 
<p>GRU的公式如下图所示：</p> 
<p><img alt="" height="461" src="https://images2.imgbox.com/6e/39/DtXi66ow_o.png" width="869"></p> 
<p> 其代码部分：</p> 
<pre><code class="language-python">class GRU(torch.nn.Module):
    def __init__(self, hidden_size, output_size, num_layers):
        super().__init__()
        self.input_size = 1
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.output_size = output_size
        self.num_directions = 1
        self.gru = torch.nn.GRU(self.input_size, self.hidden_size, self.num_layers, batch_first=True)
        self.linear = torch.nn.Linear(self.hidden_size, self.output_size)

    def forward(self, input_seq):
        # input(batch_size, seq_len, input_size)
        batch_size, seq_len = input_seq.shape[0], input_seq.shape[1]
        h_0 = torch.randn(self.num_directions * self.num_layers, batch_size, self.hidden_size).to(device)
        # output(batch_size, seq_len, num_directions * hidden_size)
        output, _ = self.gru(input_seq, (h_0))
        pred = self.linear(output)
        pred = pred[:, -1, :]
        return pred</code></pre> 
<p>这里主要对里面主要的五个参数进行介绍： </p> 
<blockquote> 
 <p><span style="color:#fe2c24;"><em> input_size：</em></span>输入节点特征的维度。这里需要注意的是，如果你输入的是节点的交通流量数据，一般只使用一个值表示，那么你的input_size为1；若是想基于该节点在t时刻的多个特征，如：流量、速度、车辆数这三个指标对交通流量进行预测，这是input_size=3。</p> 
 <p><span style="color:#fe2c24;"><em>hidden_size：</em></span>隐藏层数，也就是可调参数。该值决定了模型的预测效果，计算复杂度。</p> 
 <p><span style="color:#fe2c24;"><em>num_layers：</em></span>堆叠的GRU的层数，num_layers=2说明堆叠了两层GRU，第一层GRU输出的隐藏特征h会作为第二层GRU的数据再进行一次计算。</p> 
 <p><span style="color:#fe2c24;"><em>batch_first：</em></span>主要为了规范输入数据各个维度所代表的含义。这里其实只需要记住一种情况即可，batch_first=True代表输入数据的三个维度分别代表<span style="color:#4da8ee;"><em>input(batch_size, seq_len, input_size)</em></span>，输出数据的三个维度分别代表<span style="color:#4da8ee;"><em>output(batch_size, seq_len, num_directions * hidden_size)</em></span>。</p> 
 <p><em><span style="color:#fe2c24;">bidirectional：</span></em>bidirectional=True代表双向GRU，在计算时，GRU不仅按从0到t的顺序对数据进行计算，还会按照从t到0的顺序对数据进行二次计算。</p> 
 <p>注：这里需要注意<span style="color:#fe2c24;">input_size</span>和<span style="color:#4da8ee;">seq_len</span>的区别，input_size代表某个时间t，节点特征的维度；seq_len则代表你要基于多长的历史数据对未来的数据状态进行预测。</p> 
</blockquote> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<h2>二、LSTM</h2> 
<p>LSTM公式如下： </p> 
<h2><img alt="" height="765" src="https://images2.imgbox.com/95/47/upoTNSAy_o.png" width="1064"></h2> 
<p></p> 
<p></p> 
<p></p> 
<p> 其代码部分：</p> 
<pre><code class="language-python">class LSTM(torch.nn.Module):
    def __init__(self, hidden_size, output_size):
        super().__init__()
        self.input_size = 1
        self.hidden_size = hidden_size
        self.num_layers = 1
        self.output_size = output_size
        self.num_directions = 1 # 单向LSTM
        self.lstm = torch.nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first=True)
        self.lin = torch.nn.Linear(self.hidden_size, self.output_size)

    def forward(self, input_seq):
        batch_size, seq_len = input_seq.shape[0], input_seq.shape[1]
        # input(batch_size, seq_len, input_size)
        h_0 = torch.zeros(self.num_directions * self.num_layers, batch_size, self.hidden_size).to(device)
        c_0 = torch.zeros(self.num_directions * self.num_layers, batch_size, self.hidden_size).to(device)
        # output(batch_size, seq_len, num_directions * hidden_size)
        output, _ = self.lstm(input_seq, (h_0.detach(), c_0.detach())) 
        pred = output[:, -1, :]
        pred = self.lin(pred)
        return pred</code></pre> 
<blockquote> 
 <p>这里其实只是比GRU代码中多了一段对c_0状态的初始化描述，其他部分是一样的这里不在赘述。</p> 
</blockquote> 
<h2> 三、基于GRU的完整代码及结果展示</h2> 
<pre><code class="language-python">import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from tqdm import tqdm


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
value = pd.read_csv(r'dataset/A5M.txt', header=None)#(14772, 1)
time = pd.date_range(start='200411190930', periods=len(value), freq='5min')
ts = pd.Series(value.iloc[:, 0].values, index=time)
ts_sample_h = ts.resample('H').sum()
# plt.plot(ts_sample_h)
# plt.xlabel("Time")
# plt.ylabel("traffic demand")
# plt.title("resample history traffic demand from 5M to H")
# plt.show()
class MyDataset(Dataset):
    def __init__(self, data):
        self.data = data

    def __getitem__(self, item):
        return self.data[item]

    def __len__(self):
        return len(self.data)

def nn_seq_us(B):
    dataset = ts_sample_h
    # split
    train = dataset[:int(len(dataset) * 0.7)]
    test = dataset[int(len(dataset) * 0.7):]
    m, n = np.max(train.values), np.min(train.values)
    # print(m,n)

    def process(data, batch_size, shuffle):
        load = data
        load = (load - n) / (m - n)
        seq = []
        for i in range(len(data) - 6):
            train_seq = []
            train_label = []
            for j in range(i, i + 6):
                x = [load[j]]
                train_seq.append(x)
            train_label.append(load[i + 6])
            train_seq = torch.FloatTensor(train_seq)
            train_label = torch.FloatTensor(train_label).view(-1)
            seq.append((train_seq, train_label))

        seq = MyDataset(seq)
        seq = DataLoader(dataset=seq, batch_size=batch_size, shuffle=shuffle, num_workers=0, drop_last=False)

        return seq

    Dtr = process(train, B, False)
    Dte = process(test, B, False)

    return Dtr, Dte, m, n

class GRU(torch.nn.Module):
    def __init__(self, hidden_size, num_layers):
        super().__init__()
        self.input_size = 1
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.output_size = 1
        self.num_directions = 1
        self.gru = torch.nn.GRU(self.input_size, self.hidden_size, self.num_layers, batch_first=True)
        self.linear = torch.nn.Linear(self.hidden_size, self.output_size)

    def forward(self, input_seq):
        batch_size, seq_len = input_seq.shape[0], input_seq.shape[1]
        h_0 = torch.randn(self.num_directions * self.num_layers, batch_size, self.hidden_size).to(device)
        # output(batch_size, seq_len, num_directions * hidden_size)
        output, _ = self.gru(input_seq, (h_0))
        pred = self.linear(output)
        pred = pred[:, -1, :]
        return pred

Dtr, Dte, m, n= nn_seq_us(64)
hidden_size, num_layers = 10, 2
model = GRU(hidden_size, num_layers).to(device)
loss_function = torch.nn.MSELoss().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1.5e-3)

# training
trainloss_list = []
model.train()
for epoch in tqdm(range(50)):
    train_loss = []
    for (seq, label) in Dtr:
        seq = seq.to(device)#torch.Size([64, 80, 1])
        label = label.to(device)#torch.Size([64, 1])
        y_pred = model(seq)
        loss = loss_function(y_pred, label)
        train_loss.append(loss.item())
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    trainloss_list.append(np.mean(train_loss))
# training_loss的图
plt.plot(trainloss_list)
plt.xlabel("Epoch")
plt.ylabel("MSE")
plt.title("average of Training loss")
plt.show()


pred = []
y = []
model.eval()
for (seq, target) in Dte:
    seq = seq.to(device)
    target = target.to(device)
    y_pred = model(seq)
    pred.append(y_pred)
    y.append(target)


y=torch.cat(y, dim=0)
pred=torch.cat(pred, dim=0)
y = (m - n) * y + n
pred = (m - n) * pred + n#torch.Size([179, 1])
print('MSE:', loss_function(y, pred))
# plot
plt.plot(y.cpu().detach().numpy(), label='ground-truth')
plt.plot(pred.cpu().detach().numpy(), label='prediction')
plt.xlabel("Time")
plt.ylabel("traffic demand")
plt.title("history traffic demand from 5M to H")
plt.show()
</code></pre> 
<p class="img-center"><img alt="" height="444" src="https://images2.imgbox.com/93/24/sXkr927B_o.png" width="595"></p> 
<p> <img alt="" height="455" src="https://images2.imgbox.com/41/b9/wfN38kuT_o.png" width="1048"></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f1c3cb1cbd62ed5b22f9d27dd5ec0687/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">AI语音合成 VITS Fast Fine-tuning，半小时合成专属模型，部署训练使用讲解</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0d3b8045137579ee6f0d8115bd997c99/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">uniapp 实现多语言切换</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>