<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【tensorflow】保存模型、再次加载模型等操作 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【tensorflow】保存模型、再次加载模型等操作" />
<meta property="og:description" content="由于经常要使用tensorflow进行网络训练，但是在用的时候每次都要把模型重新跑一遍，这样就比较麻烦；另外由于某些原因程序意外中断，也会导致训练结果拿不到，而保存中间训练过程的模型可以以便下次训练时继续使用。
所以练习了tensorflow的save model和load model。
参考于http://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/，这篇教程简单易懂！！
1、保存模型 # 首先定义saver类 saver = tf.train.Saver(max_to_keep=4) # 定义会话 with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print &#34;------------------------------------------------------&#34; for epoch in range(300): if epoch % 10 == 0: print &#34;------------------------------------------------------&#34; # 保存模型 saver.save(sess, &#34;model/my-model&#34;, global_step=epoch) print &#34;save the model&#34; # 训练 sess.run(train_step) print &#34;------------------------------------------------------&#34; 注意点：
创建saver时，可以指定需要存储的tensor，如果没有指定，则全部保存。
创建saver时，可以指定保存的模型个数，利用max_to_keep=4，则最终会保存4个模型（下图中我保存了160、170、180、190step共4个模型）。
saver.save()函数里面可以设定global_step，说明是哪一步保存的模型。
程序结束后，会生成四个文件：存储网络结构.meta、存储训练好的参数.data和.index、记录最新的模型checkpoint。
如：
2、加载模型 def load_model(): with tf.Session() as sess: saver = tf.train.import_meta_graph(&#39;model/my-model-290.meta&#39;) saver.restore(sess, tf.train.latest_checkpoint(&#34;model/&#34;)) 注意点：
首先import_meta_graph，这里填的名字meta文件的名字。然后restore时，是检查checkpoint，所以只填到checkpoint所在的路径下即可，不需要填checkpoint，不然会报错“ValueError: Can’t load save_path when it is None." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/2c5496308dfa464d8ab06304d36356ba/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-01-12T21:04:16+08:00" />
<meta property="article:modified_time" content="2018-01-12T21:04:16+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【tensorflow】保存模型、再次加载模型等操作</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <hr> 
<p>由于经常要使用tensorflow进行网络训练，但是在用的时候每次都要把模型重新跑一遍，这样就比较麻烦；另外由于某些原因程序意外中断，也会导致训练结果拿不到，而保存中间训练过程的模型可以以便下次训练时继续使用。</p> 
<p>所以练习了tensorflow的save model和load model。</p> 
<p>参考于<a href="http://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/" rel="nofollow noopener noreferrer" target="_blank">http://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/</a>，这篇教程简单易懂！！</p> 
<hr> 
<h4 id="1保存模型">1、保存模型</h4> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-comment"># 首先定义saver类</span>
saver = tf.train.Saver(max_to_keep=<span class="hljs-number">4</span>)

<span class="hljs-comment"># 定义会话</span>
<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:

    sess.run(tf.global_variables_initializer())
    <span class="hljs-keyword">print</span> <span class="hljs-string">"------------------------------------------------------"</span>

    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">300</span>):
        <span class="hljs-keyword">if</span> epoch % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:
            <span class="hljs-keyword">print</span> <span class="hljs-string">"------------------------------------------------------"</span>
            <span class="hljs-comment"># 保存模型</span>
            saver.save(sess, <span class="hljs-string">"model/my-model"</span>, global_step=epoch)
            <span class="hljs-keyword">print</span> <span class="hljs-string">"save the model"</span>

        <span class="hljs-comment"># 训练</span>
        sess.run(train_step)
    <span class="hljs-keyword">print</span> <span class="hljs-string">"------------------------------------------------------"</span></code></pre> 
<p>注意点：</p> 
<ol><li><p>创建saver时，可以指定需要存储的tensor，如果没有指定，则全部保存。</p></li><li><p>创建saver时，可以指定保存的模型个数，利用max_to_keep=4，则最终会保存4个模型（下图中我保存了160、170、180、190step共4个模型）。</p></li><li><p>saver.save()函数里面可以设定global_step，说明是哪一步保存的模型。</p></li><li><p>程序结束后，会生成四个文件：存储网络结构.meta、存储训练好的参数.data和.index、记录最新的模型checkpoint。</p></li></ol> 
<p>如：</p> 
<p><img src="https://images2.imgbox.com/ae/a2/NbUSlNNa_o.png" alt="这里写图片描述" title=""></p> 
<h4 id="2加载模型">2、加载模型</h4> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load_model</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:
        saver = tf.train.import_meta_graph(<span class="hljs-string">'model/my-model-290.meta'</span>)
        saver.restore(sess, tf.train.latest_checkpoint(<span class="hljs-string">"model/"</span>))
</code></pre> 
<p>注意点：</p> 
<ol><li><p>首先import_meta_graph，这里填的名字meta文件的名字。然后restore时，是检查checkpoint，所以只填到checkpoint所在的路径下即可，不需要填checkpoint，不然会报错“ValueError: Can’t load save_path when it is None.”。</p></li><li><p>后面根据具体例子，介绍如何利用加载后的模型得到训练的结果，并进行预测。</p></li></ol> 
<h4 id="3线性拟合例子">3、线性拟合例子</h4> 
<p>首先，上代码。</p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_model</span><span class="hljs-params">()</span>:</span>

    <span class="hljs-comment"># prepare the data</span>
    x_data = np.random.rand(<span class="hljs-number">100</span>).astype(np.float32)
    <span class="hljs-keyword">print</span> x_data
    y_data = x_data * <span class="hljs-number">0.1</span> + <span class="hljs-number">0.2</span>
    <span class="hljs-keyword">print</span> y_data

    <span class="hljs-comment"># define the weights</span>
    W = tf.Variable(tf.random_uniform([<span class="hljs-number">1</span>], -<span class="hljs-number">20.0</span>, <span class="hljs-number">20.0</span>), dtype=tf.float32, name=<span class="hljs-string">'w'</span>)
    b = tf.Variable(tf.random_uniform([<span class="hljs-number">1</span>], -<span class="hljs-number">10.0</span>, <span class="hljs-number">10.0</span>), dtype=tf.float32, name=<span class="hljs-string">'b'</span>)
    y = W * x_data + b

    <span class="hljs-comment"># define the loss</span>
    loss = tf.reduce_mean(tf.square(y - y_data))
    train_step = tf.train.GradientDescentOptimizer(<span class="hljs-number">0.5</span>).minimize(loss)

    <span class="hljs-comment"># save model</span>
    saver = tf.train.Saver(max_to_keep=<span class="hljs-number">4</span>)

    <span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:

        sess.run(tf.global_variables_initializer())
        <span class="hljs-keyword">print</span> <span class="hljs-string">"------------------------------------------------------"</span>
        <span class="hljs-keyword">print</span> <span class="hljs-string">"before the train, the W is %6f, the b is %6f"</span> % (sess.run(W), sess.run(b))

        <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">300</span>):
            <span class="hljs-keyword">if</span> epoch % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:
                <span class="hljs-keyword">print</span> <span class="hljs-string">"------------------------------------------------------"</span>
                <span class="hljs-keyword">print</span> (<span class="hljs-string">"after epoch %d, the loss is %6f"</span> % (epoch, sess.run(loss)))
                <span class="hljs-keyword">print</span> (<span class="hljs-string">"the W is %f, the b is %f"</span> % (sess.run(W), sess.run(b)))
                saver.save(sess, <span class="hljs-string">"model/my-model"</span>, global_step=epoch)
                <span class="hljs-keyword">print</span> <span class="hljs-string">"save the model"</span>
            sess.run(train_step)
        <span class="hljs-keyword">print</span> <span class="hljs-string">"------------------------------------------------------"</span>

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load_model</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:
        saver = tf.train.import_meta_graph(<span class="hljs-string">'model/my-model-290.meta'</span>)
        saver.restore(sess, tf.train.latest_checkpoint(<span class="hljs-string">"model/"</span>))
        <span class="hljs-keyword">print</span> sess.run(<span class="hljs-string">'w:0'</span>)
        <span class="hljs-keyword">print</span> sess.run(<span class="hljs-string">'b:0'</span>)

train_model()
load_model()</code></pre> 
<ol><li><p>首先定义了y=ax+b的线性关系，a=0.1，b=0.2，然后给定训练数据集，w是-20.0到20.0之间的任意数，b是-10.0到10.0之间的任意数。</p></li><li><p>然后定义损失函数，定义随机梯度下降训练器。</p></li><li><p>定义saver后进入训练阶段，边训练边保存模型。并输出中间的训练loss，w和b。可以看到w和b在逐步接近我们设定的0.1和0.2。</p></li><li><p>在load_model函数中，我们首先利用第2小节中的方法加载模型，然后就可以根据模型中权值的名字，打印其结果。</p></li></ol> 
<p><strong>注意：</strong></p> 
<p>这里说明一点，如何知道tensor的名字，最好是定义tensor的时候就指定名字，如上面代码中的<code>name='w'</code>，如果你没有定义name，tensorflow也会设置name，只不过这个name就是根据你的tensor或者操作的性质，像上面的w，这是“Variable:0”，loss则是“Mean:0”。所以最好还是自己定义好name。</p> 
<p>最后给出结果：</p> 
<p><img src="https://images2.imgbox.com/3d/f0/sbLJMbna_o.png" alt="这里写图片描述" title=""></p> 
<p><img src="https://images2.imgbox.com/c6/09/vCgC2i29_o.png" alt="这里写图片描述" title=""></p> 
<h4 id="4卷积神经网络例子">4、卷积神经网络例子</h4> 
<p>首先，上代码：</p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> os

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load_data</span><span class="hljs-params">(resultpath)</span>:</span>

    datapath = os.path.join(resultpath, <span class="hljs-string">"data10_4.npz"</span>)
    <span class="hljs-keyword">if</span> os.path.exists(datapath):
        data = np.load(datapath)
        X, Y = data[<span class="hljs-string">"X"</span>], data[<span class="hljs-string">"Y"</span>]
    <span class="hljs-keyword">else</span>:
        X = np.array(np.arange(<span class="hljs-number">30720</span>)).reshape(<span class="hljs-number">10</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>)
        Y = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>]
        X = X.astype(<span class="hljs-string">'float32'</span>)
        Y = np.array(Y)
        np.savez(datapath, X=X, Y=Y)
        print(<span class="hljs-string">'Saved dataset to dataset.npz.'</span>)
    print(<span class="hljs-string">'X_shape:{}\nY_shape:{}'</span>.format(X.shape, Y.shape))
    <span class="hljs-keyword">return</span> X, Y

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">define_model</span><span class="hljs-params">(x)</span>:</span>

    x_image = tf.reshape(x, [-<span class="hljs-number">1</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>])
    <span class="hljs-keyword">print</span> x_image.shape

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">weight_variable</span><span class="hljs-params">(shape)</span>:</span>
        initial = tf.truncated_normal(shape, stddev=<span class="hljs-number">0.1</span>)
        <span class="hljs-keyword">return</span> tf.Variable(initial, name=<span class="hljs-string">"w"</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bias_variable</span><span class="hljs-params">(shape)</span>:</span>
        initial = tf.constant(<span class="hljs-number">0.1</span>, shape=shape)
        <span class="hljs-keyword">return</span> tf.Variable(initial, name=<span class="hljs-string">"b"</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">conv3d</span><span class="hljs-params">(x, W)</span>:</span>
        <span class="hljs-keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], padding=<span class="hljs-string">'SAME'</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">max_pool_2d</span><span class="hljs-params">(x)</span>:</span>
        <span class="hljs-keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>], strides=[<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>], padding=<span class="hljs-string">'SAME'</span>)

    <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">"conv1"</span>):  <span class="hljs-comment"># [-1,32,32,3]</span>
        weights = weight_variable([<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">32</span>])
        biases = bias_variable([<span class="hljs-number">32</span>])
        conv1 = tf.nn.relu(conv3d(x_image, weights) + biases)
        pool1 = max_pool_2d(conv1)  <span class="hljs-comment"># [-1,11,11,32]</span>

    <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">"conv2"</span>):
        weights = weight_variable([<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">64</span>])
        biases = bias_variable([<span class="hljs-number">64</span>])
        conv2 = tf.nn.relu(conv3d(pool1, weights) + biases)
        pool2 = max_pool_2d(conv2) <span class="hljs-comment"># [-1,4,4,64]</span>

    <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">"fc1"</span>):
        weights = weight_variable([<span class="hljs-number">4</span> * <span class="hljs-number">4</span> * <span class="hljs-number">64</span>, <span class="hljs-number">128</span>]) <span class="hljs-comment"># [-1,1024]</span>
        biases = bias_variable([<span class="hljs-number">128</span>])
        fc1_flat = tf.reshape(pool2, [-<span class="hljs-number">1</span>, <span class="hljs-number">4</span> * <span class="hljs-number">4</span> * <span class="hljs-number">64</span>])
        fc1 = tf.nn.relu(tf.matmul(fc1_flat, weights) + biases)
        fc1_drop = tf.nn.dropout(fc1, <span class="hljs-number">0.5</span>) <span class="hljs-comment"># [-1,128]</span>

    <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">"fc2"</span>):
        weights = weight_variable([<span class="hljs-number">128</span>, <span class="hljs-number">4</span>])
        biases = bias_variable([<span class="hljs-number">4</span>])
        fc2 = tf.matmul(fc1_drop, weights) + biases <span class="hljs-comment"># [-1,4]</span>

    <span class="hljs-keyword">return</span> fc2

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_model</span><span class="hljs-params">()</span>:</span>

    x = tf.placeholder(tf.float32, shape=[<span class="hljs-keyword">None</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>], name=<span class="hljs-string">"x"</span>)
    y_ = tf.placeholder(<span class="hljs-string">'int64'</span>, shape=[<span class="hljs-keyword">None</span>], name=<span class="hljs-string">"y_"</span>)

    initial_learning_rate = <span class="hljs-number">0.001</span>
    y_fc2 = define_model(x)
    y_label = tf.one_hot(y_, <span class="hljs-number">4</span>, name=<span class="hljs-string">"y_labels"</span>)

    loss_temp = tf.losses.softmax_cross_entropy(onehot_labels=y_label, logits=y_fc2)
    cross_entropy_loss = tf.reduce_mean(loss_temp)

    train_step = tf.train.AdamOptimizer(learning_rate=initial_learning_rate, beta1=<span class="hljs-number">0.9</span>, beta2=<span class="hljs-number">0.999</span>,
                                        epsilon=<span class="hljs-number">1e-08</span>).minimize(cross_entropy_loss)

    correct_prediction = tf.equal(tf.argmax(y_fc2, <span class="hljs-number">1</span>), tf.argmax(y_label, <span class="hljs-number">1</span>))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

    <span class="hljs-comment"># save model</span>
    saver = tf.train.Saver(max_to_keep=<span class="hljs-number">4</span>)
    tf.add_to_collection(<span class="hljs-string">"predict"</span>, y_fc2)

    <span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:

        sess.run(tf.global_variables_initializer())
        <span class="hljs-keyword">print</span> <span class="hljs-string">"------------------------------------------------------"</span>
        X, Y = load_data(<span class="hljs-string">"model1/"</span>)
        X = np.multiply(X, <span class="hljs-number">1.0</span> / <span class="hljs-number">255.0</span>)
        <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">200</span>):

            <span class="hljs-keyword">if</span> epoch % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:
                <span class="hljs-keyword">print</span> <span class="hljs-string">"------------------------------------------------------"</span>

                train_accuracy = accuracy.eval(feed_dict={x: X, y_: Y})
                train_loss = cross_entropy_loss.eval(feed_dict={x: X, y_: Y})

                <span class="hljs-keyword">print</span> (<span class="hljs-string">"after epoch %d, the loss is %6f"</span> % (epoch, train_loss))
                <span class="hljs-keyword">print</span> (<span class="hljs-string">"after epoch %d, the acc is %6f"</span> % (epoch, train_accuracy))

                saver.save(sess, <span class="hljs-string">"model1/my-model"</span>, global_step=epoch)
                <span class="hljs-keyword">print</span> <span class="hljs-string">"save the model"</span>

            train_step.run(feed_dict={x: X, y_: Y})

        <span class="hljs-keyword">print</span> <span class="hljs-string">"------------------------------------------------------"</span>

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load_model</span><span class="hljs-params">()</span>:</span>

    <span class="hljs-comment"># prepare the test data</span>
    X = np.array(np.arange(<span class="hljs-number">6144</span>, <span class="hljs-number">12288</span>)).reshape(<span class="hljs-number">2</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>)
    Y = [<span class="hljs-number">3</span>, <span class="hljs-number">1</span>]
    Y = np.array(Y)
    X = X.astype(<span class="hljs-string">'float32'</span>)
    X = np.multiply(X, <span class="hljs-number">1.0</span> / <span class="hljs-number">255.0</span>)
    <span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:

        <span class="hljs-comment"># load the meta graph and weights</span>
        saver = tf.train.import_meta_graph(<span class="hljs-string">'model1/my-model-190.meta'</span>)
        saver.restore(sess, tf.train.latest_checkpoint(<span class="hljs-string">"model1/"</span>))

        <span class="hljs-comment"># get weights</span>
        graph = tf.get_default_graph()
        fc2_w = graph.get_tensor_by_name(<span class="hljs-string">"fc2/w:0"</span>)
        fc2_b = graph.get_tensor_by_name(<span class="hljs-string">"fc2/b:0"</span>)

        <span class="hljs-keyword">print</span> <span class="hljs-string">"------------------------------------------------------"</span>
        <span class="hljs-keyword">print</span> sess.run(fc2_w)
        <span class="hljs-keyword">print</span> <span class="hljs-string">"#######################################"</span>
        <span class="hljs-keyword">print</span> sess.run(fc2_b)
        <span class="hljs-keyword">print</span> <span class="hljs-string">"------------------------------------------------------"</span>

        input_x = graph.get_operation_by_name(<span class="hljs-string">"x"</span>).outputs[<span class="hljs-number">0</span>]

        feed_dict = {<!-- --><span class="hljs-string">"x:0"</span>:X, <span class="hljs-string">"y_:0"</span>:Y}
        y = graph.get_tensor_by_name(<span class="hljs-string">"y_labels:0"</span>)
        yy = sess.run(y, feed_dict)
        <span class="hljs-keyword">print</span> yy
        <span class="hljs-keyword">print</span> <span class="hljs-string">"the answer is: "</span>, sess.run(tf.argmax(yy, <span class="hljs-number">1</span>))
        <span class="hljs-keyword">print</span> <span class="hljs-string">"------------------------------------------------------"</span>

        pred_y = tf.get_collection(<span class="hljs-string">"predict"</span>)
        pred = sess.run(pred_y, feed_dict)[<span class="hljs-number">0</span>]
        <span class="hljs-keyword">print</span> pred, <span class="hljs-string">'\n'</span>

        pred = sess.run(tf.argmax(pred, <span class="hljs-number">1</span>))
        <span class="hljs-keyword">print</span> <span class="hljs-string">"the predict is: "</span>, pred
        <span class="hljs-keyword">print</span> <span class="hljs-string">"------------------------------------------------------"</span>

        acc = graph.get_operation_by_name(<span class="hljs-string">"acc"</span>)
        acc = sess.run(acc, feed_dict)
        <span class="hljs-keyword">print</span> <span class="hljs-string">"the accuracy is: "</span>, acc
        <span class="hljs-keyword">print</span> <span class="hljs-string">"------------------------------------------------------"</span>

<span class="hljs-comment">#train_model()</span>
load_model()</code></pre> 
<ol><li><p>定义了一个简单的卷积神经网络：有两个卷积层、两个池化层和两个全连接层。</p></li><li><p>加载的数据是无意义的数据，模拟的是10张32x32的RGB图像，共4个类别0、1、2、3。</p></li><li><p>在train_model中，定义了一下可能需要的tensor或操作的name，以便加载模型后使用。</p></li><li><p>在定义saver时，对要预测的值fc2添加了进去，并定义name为“predict”，以便在预测时使用。</p></li><li><p>在load_model中，输出了一些中间结果，如最后一层的W和b的值。然后根据随机创建的测试数据集，模拟2张32x32的RGB图，预测这两张图像的类别，放入feed_dict，输出预测结果。</p></li><li><p>首先返回了测试数据的真实标签。</p></li><li><p>返回的是一个2位矩阵，第一行是第一个图像的结果，长度为4，因为有4个种类，第二行是第二张图像的结果。所以我们要将这个返回我们熟悉的0、1、2、3，只要返回最大值的下标即可。使用tf.argmax即可。</p></li><li><p>返回准确度，不知道为什么，是None，后面再找找问题出在哪。</p></li></ol> 
<p>给出输出结果：</p> 
<p><img src="https://images2.imgbox.com/5a/83/CUFRFakU_o.png" alt="这里写图片描述" title=""></p> 
<p><img src="https://images2.imgbox.com/9c/2b/G9iaAD31_o.png" alt="这里写图片描述" title=""></p> 
<p><img src="https://images2.imgbox.com/c5/91/d7iLijE9_o.png" alt="这里写图片描述" title=""></p> 
<p>虽然我们的训练数据和测试数据都是随机无意义的数，所以这个预测结果也不必认真纠结。</p> 
<h4 id="5fine-tuning">5、fine-tuning</h4> 
<p>使用已经预训练好的模型，自己fine-tuning。</p> 
<p>1、首先获得pre-traing的graph结构，<code>saver = tf.train.import_meta_graph('my_test_model-1000.meta')</code></p> 
<p>2、加载参数，<code>saver.restore(sess,tf.train.latest_checkpoint('./'))</code></p> 
<p>3、准备feed_dict，新的训练数据或者测试数据。这样就可以使用同样的模型，训练或者测试不同的数据。</p> 
<p>4、如果想在已有的网络结构上添加新的层，如前面卷积网络，获得fc2时，然后添加了一个全连接层和输出层。</p> 
<pre class="prettyprint"><code class=" hljs avrasm">pred_y = graph<span class="hljs-preprocessor">.get</span>_tensor_by_name(<span class="hljs-string">"fc2/add:0"</span>)

        <span class="hljs-preprocessor">## add the new layers</span>
        weights = tf<span class="hljs-preprocessor">.Variable</span>(tf<span class="hljs-preprocessor">.truncated</span>_normal([<span class="hljs-number">4</span>, <span class="hljs-number">6</span>], stddev=<span class="hljs-number">0.1</span>), name=<span class="hljs-string">"w"</span>)
        biases = tf<span class="hljs-preprocessor">.Variable</span>(tf<span class="hljs-preprocessor">.constant</span>(<span class="hljs-number">0.1</span>, shape=[<span class="hljs-number">6</span>]), name=<span class="hljs-string">"b"</span>)
        conv1 = tf<span class="hljs-preprocessor">.matmul</span>(pred_y, weights) + biases
        output1 = tf<span class="hljs-preprocessor">.nn</span><span class="hljs-preprocessor">.softmax</span>(conv1)</code></pre> 
<p>5、只要加载模型的前一部分，然后从后面开始fine-tuning。</p> 
<pre class="prettyprint"><code class=" hljs avrasm"><span class="hljs-preprocessor"># pre-train and fine-tuning</span>
        fc2 = graph<span class="hljs-preprocessor">.get</span>_tensor_by_name(<span class="hljs-string">"fc2/add:0"</span>)
        fc2 = tf<span class="hljs-preprocessor">.stop</span>_gradient(fc2)  <span class="hljs-preprocessor"># stop the gradient compute</span>
        fc2_shape = fc2<span class="hljs-preprocessor">.get</span>_shape()<span class="hljs-preprocessor">.as</span>_list()

        <span class="hljs-preprocessor"># fine -tuning</span>
        new_nums = <span class="hljs-number">6</span>
        weights = tf<span class="hljs-preprocessor">.Variable</span>(tf<span class="hljs-preprocessor">.truncated</span>_normal([fc2_shape[<span class="hljs-number">1</span>], new_nums], stddev=<span class="hljs-number">0.1</span>), name=<span class="hljs-string">"w"</span>)
        biases = tf<span class="hljs-preprocessor">.Variable</span>(tf<span class="hljs-preprocessor">.constant</span>(<span class="hljs-number">0.1</span>, shape=[new_nums]), name=<span class="hljs-string">"b"</span>)
        conv2 = tf<span class="hljs-preprocessor">.matmul</span>(fc2, weights) + biases
        output2 = tf<span class="hljs-preprocessor">.nn</span><span class="hljs-preprocessor">.softmax</span>(conv2)</code></pre> 
<h4 id="7知识点">7、知识点</h4> 
<p>1、.meta文件：一个协议缓冲，保存tensorflow中完整的graph、variables、operation、collection。</p> 
<p>2、checkpoint文件：一个二进制文件，包含了weights, biases, gradients和其他variables的值。但是0.11版本后的都修改了，用.data和.index保存值，用checkpoint记录最新的记录。</p> 
<p>3、在进行保存时，因为meta中保存的模型的graph，这个是一样的，只需保存一次就可以，所以可以设置<code>saver.save(sess, 'my-model', write_meta_graph=False)</code>即可。</p> 
<p>4、如果想设置每多长时间保存一次，可以设置<code>saver = tf.train.Saver(keep_checkpoint_every_n_hours=2)</code>，这个是每2个小时保存一次。</p> 
<p>5、如果不想保存所有变量，可以在创建saver实例时，指定保存的变量，可以以list或者dict的类型保存。如：</p> 
<pre class="prettyprint"><code class=" hljs vhdl">w1 = tf.<span class="hljs-keyword">Variable</span>(tf.random_normal(shape=[<span class="hljs-number">2</span>]), name=<span class="hljs-attribute">'w1</span>')
w2 = tf.<span class="hljs-keyword">Variable</span>(tf.random_normal(shape=[<span class="hljs-number">5</span>]), name=<span class="hljs-attribute">'w2</span>')
saver = tf.train.Saver([w1,w2])</code></pre> 
<p>6、</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/77199aa21c673e8ccccf751d67ccce43/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">实现滚动条丝滑滚动，流畅不卡顿，有回弹效果。-webkit-overflow-scrolling</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f77fed5b18003975053d92293e8b6ff4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">socket 读、写字节流数据</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>