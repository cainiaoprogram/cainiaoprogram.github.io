<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Kafka SASL/SCRAM&#43;ACL实现动态创建用户及权限控制 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Kafka SASL/SCRAM&#43;ACL实现动态创建用户及权限控制" />
<meta property="og:description" content="文章目录 SASL_SCRAM&#43;ACL实现动态创建用户及权限控制使用SASL / SCRAM进行身份验证1. 创建SCRAM Credentials创建broker建通信用户(或称超级用户)创建客户端用户fanboshi查看SCRAM证书删除SCRAM证书 2. 配置Kafka Brokers客户端配置kafka-console-producerkafka-console-consumer ACL配置授予fanboshi用户对test topic 写权限, 只允许 192.168.2.* 网段授予fanboshi用户对test topic 读权限, 只允许 192.168.2.* 网段授予fanboshi用户, fanboshi-group 消费者组 对test topic 读权限, 只允许 192.168.2.* 网段查看acl配置删除配置再次测试 如何查看我们创建了哪些&#34;用户&#34;参考文献 SASL_SCRAM&#43;ACL实现动态创建用户及权限控制 研究了一段时间Kafka的权限控制. 之前一直看SASL_PLAINTEXT, 结果发现这玩意不能动态创建用户. 今天没死心与查了查,发现这个人也问了这个问题
https://stackoverflow.com/questions/54147460/kafka-adding-sasl-users-dynamically-without-cluster-restart
于是研究了下SCRAM. 写了这篇完整的文档
本篇文档中使用的是自己部署的zookeeper, zookeeper无需做任何特殊配置
使用SASL / SCRAM进行身份验证 请先在不配置任何身份验证的情况下启动Kafka
1. 创建SCRAM Credentials Kafka中的SCRAM实现使用Zookeeper作为凭证(credential)存储。 可以使用kafka-configs.sh在Zookeeper中创建凭据。 对于启用的每个SCRAM机制，必须通过添加具有机制名称的配置来创建凭证。 必须在启动Kafka broker之前创建代理间通信的凭据。 可以动态创建和更新客户端凭证，并使用更新的凭证来验证新连接。
创建broker建通信用户(或称超级用户) bin/kafka-configs.sh --zookeeper 192.168.2.229:2182 --alter --add-config &#39;SCRAM-SHA-256=[password=admin-secret],SCRAM-SHA-512=[password=admin-secret]&#39; --entity-type users --entity-name admin 创建客户端用户fanboshi bin/kafka-configs.sh --zookeeper 192.168.2.229:2182 --alter --add-config &#39;SCRAM-SHA-256=[iterations=8192,password=fanboshi],SCRAM-SHA-512=[password=fanboshi]&#39; --entity-type users --entity-name fanboshi 查看SCRAM证书 [root@node002229 kafka]# bin/kafka-configs." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/f8d49212f1ad4d31275efb1264e8eb5f/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-01-26T21:50:11+08:00" />
<meta property="article:modified_time" content="2019-01-26T21:50:11+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Kafka SASL/SCRAM&#43;ACL实现动态创建用户及权限控制</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#SASL_SCRAMACL_1" rel="nofollow">SASL_SCRAM+ACL实现动态创建用户及权限控制</a></li><li><ul><li><a href="#SASL__SCRAM_7" rel="nofollow">使用SASL / SCRAM进行身份验证</a></li><li><ul><li><a href="#1_SCRAM_Credentials_9" rel="nofollow">1. 创建SCRAM Credentials</a></li><li><ul><li><a href="#broker_11" rel="nofollow">创建broker建通信用户(或称超级用户)</a></li><li><a href="#fanboshi_15" rel="nofollow">创建客户端用户fanboshi</a></li><li><a href="#SCRAM_19" rel="nofollow">查看SCRAM证书</a></li><li><a href="#SCRAM_24" rel="nofollow">删除SCRAM证书</a></li></ul> 
    </li><li><a href="#2_Kafka_Brokers_30" rel="nofollow">2. 配置Kafka Brokers</a></li><li><a href="#_96" rel="nofollow">客户端配置</a></li><li><ul><li><a href="#kafkaconsoleproducer_98" rel="nofollow">kafka-console-producer</a></li><li><a href="#kafkaconsoleconsumer_157" rel="nofollow">kafka-console-consumer</a></li></ul> 
   </li></ul> 
   </li><li><a href="#ACL_178" rel="nofollow">ACL配置</a></li><li><ul><li><a href="#fanboshitest_topic___1921682__179" rel="nofollow">授予fanboshi用户对test topic 写权限, 只允许 192.168.2.* 网段</a></li><li><a href="#fanboshitest_topic___1921682__183" rel="nofollow">授予fanboshi用户对test topic 读权限, 只允许 192.168.2.* 网段</a></li><li><a href="#fanboshi_fanboshigroup__test_topic___1921682__187" rel="nofollow">授予fanboshi用户, fanboshi-group 消费者组 对test topic 读权限, 只允许 192.168.2.* 网段</a></li><li><a href="#acl_191" rel="nofollow">查看acl配置</a></li><li><a href="#_201" rel="nofollow">删除配置</a></li><li><a href="#_206" rel="nofollow">再次测试</a></li></ul> 
   </li><li><a href="#_224" rel="nofollow">如何查看我们创建了哪些"用户"</a></li><li><a href="#_251" rel="nofollow">参考文献</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="SASL_SCRAMACL_1"></a>SASL_SCRAM+ACL实现动态创建用户及权限控制</h2> 
<p>  研究了一段时间Kafka的权限控制. 之前一直看SASL_PLAINTEXT, 结果发现这玩意不能动态创建用户. 今天没死心与查了查,发现这个人也问了这个问题<br> <a href="https://stackoverflow.com/questions/54147460/kafka-adding-sasl-users-dynamically-without-cluster-restart" rel="nofollow">https://stackoverflow.com/questions/54147460/kafka-adding-sasl-users-dynamically-without-cluster-restart</a><br> 于是研究了下SCRAM. 写了这篇完整的文档</p> 
<blockquote> 
 <p>本篇文档中使用的是自己部署的zookeeper, zookeeper无需做任何特殊配置</p> 
</blockquote> 
<h3><a id="SASL__SCRAM_7"></a>使用SASL / SCRAM进行身份验证</h3> 
<p>请先在不配置任何身份验证的情况下启动Kafka</p> 
<h4><a id="1_SCRAM_Credentials_9"></a>1. 创建SCRAM Credentials</h4> 
<p>Kafka中的SCRAM实现使用Zookeeper作为凭证(credential)存储。 可以使用<code>kafka-configs.sh</code>在Zookeeper中创建凭据。 对于启用的每个SCRAM机制，必须通过添加具有机制名称的配置来创建凭证。 必须在启动Kafka broker之前创建代理间通信的凭据。 可以动态创建和更新客户端凭证，并使用更新的凭证来验证新连接。</p> 
<h5><a id="broker_11"></a>创建broker建通信用户(或称超级用户)</h5> 
<pre><code>bin/kafka-configs.sh --zookeeper 192.168.2.229:2182 --alter --add-config 'SCRAM-SHA-256=[password=admin-secret],SCRAM-SHA-512=[password=admin-secret]' --entity-type users --entity-name admin
</code></pre> 
<h5><a id="fanboshi_15"></a>创建客户端用户fanboshi</h5> 
<pre><code>bin/kafka-configs.sh --zookeeper 192.168.2.229:2182 --alter --add-config 'SCRAM-SHA-256=[iterations=8192,password=fanboshi],SCRAM-SHA-512=[password=fanboshi]' --entity-type users --entity-name fanboshi
</code></pre> 
<h5><a id="SCRAM_19"></a>查看SCRAM证书</h5> 
<pre><code>[root@node002229 kafka]# bin/kafka-configs.sh --zookeeper localhost:2182 --describe --entity-type users --entity-name fanboshi
Configs for user-principal 'fanboshi' are SCRAM-SHA-512=salt=MWwwdWJqcjBncmUwdzY1Mzdoa2NwNXppd3A=,stored_key=mGCJy5k3LrE2gs6Dp4ALRhgy37l1WYPUIdoOncCF+B3Ti3wL2sQNmzg8oEz3tUs9DFsclFCygjbysb0S0BU9bA==,server_key=iTyX0U0Jt02dkddUm6QrVwNf3lJk72dBNs9EDHTqe8kLlNGIp9ypzRkcgkc+WVMd1bkAF3cg8vk9Q1LrJ/2i/A==,iterations=4096,SCRAM-SHA-256=salt=ZDg5MHVlYW40dW9jbXJ6MndvZDVlazd3ag==,stored_key=cgX1ldpXnDL1+TlLHJ3IHn7tAQS/7pQ7BVZUtECpQ3A=,server_key=i7Mcnb5sPUqfIFs6qKWWHZ2ortoKiRc7oabHOV5dawI=,iterations=8192
</code></pre> 
<h5><a id="SCRAM_24"></a>删除SCRAM证书</h5> 
<p>这里只演示,不操作</p> 
<pre><code>[root@node002229 kafka]# bin/kafka-configs.sh --zookeeper localhost:2182 --alter  --delete-config 'SCRAM-SHA-512' --entity-type users --entity-name fanboshi
</code></pre> 
<h4><a id="2_Kafka_Brokers_30"></a>2. 配置Kafka Brokers</h4> 
<ol><li>在每个Kafka broker的config目录中添加一个类似下面的JAAS文件，我们称之为kafka_server_jaas.conf：</li></ol> 
<pre><code>[root@node002229 config]# cat kafka_server_jaas.conf
KafkaServer {
  org.apache.kafka.common.security.scram.ScramLoginModule required
  username="admin"
  password="admin-secret";
};
</code></pre> 
<blockquote> 
 <p>注意不要少写了分号</p> 
</blockquote> 
<ol start="2"><li>将JAAS配置文件位置作为JVM参数传递给每个Kafka broker：<br> 修改 /usr/local/kafka/bin/kafka-server-start.sh<br> 将<code>exec $base_dir/kafka-run-class.sh $EXTRA_ARGS kafka.Kafka "$@"</code> 注释, 增加下面的内容</li></ol> 
<pre><code>#exec $base_dir/kafka-run-class.sh $EXTRA_ARGS kafka.Kafka "$@"
exec $base_dir/kafka-run-class.sh $EXTRA_ARGS -Djava.security.auth.login.config=$base_dir/../config/kafka_server_jaas.conf kafka.Kafka "$@"
</code></pre> 
<p>或者不修改<code>kafka-server-start.sh</code>脚本, 而是将下面的内容添加到<code>~/.bashrc</code></p> 
<pre><code>export KAFKA_PLAIN_PARAMS="-Djava.security.auth.login.config=/usr/local/kafka/config/kafka_server_jaas.conf"
export KAFKA_OPTS="$KAFKA_PLAIN_PARAMS $KAFKA_OPTS"
</code></pre> 
<ol start="3"><li>如此处所述，在server.properties中配置SASL端口和SASL机制。 例如：</li></ol> 
<pre><code># 认证配置
listeners=SASL_PLAINTEXT://node002229:9092
security.inter.broker.protocol=SASL_PLAINTEXT
sasl.mechanism.inter.broker.protocol=SCRAM-SHA-256
sasl.enabled.mechanisms=SCRAM-SHA-256

# ACL配置
allow.everyone.if.no.acl.found=false
super.users=User:admin
authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer
</code></pre> 
<p>在官方文档中写的是</p> 
<pre><code>listeners=SASL_SSL://host.name:port
security.inter.broker.protocol=SASL_SSL
</code></pre> 
<p>这里其实没必要写成<code>SASL_SSL</code> , 我们可以根据自己的需求选择SSL或PLAINTEXT, 我这里选择PLAINTEXT不加密明文传输, 省事, 性能也相对好一些</p> 
<ol start="4"><li>重启ZK/Kafka<br> 重启ZK / Kafka服务. 所有broker在连接之前都会引用’kafka_server_jaas.conf’.<br> Zookeeper所有节点</li></ol> 
<pre><code>[root@node002229 zookeeper]# zkServer.sh stop /usr/local/zookeeper/bin/../conf/zoo.cfg  
ZooKeeper JMX enabled by default
Using config: /usr/local/zookeeper/bin/../conf/zoo1.cfg
Stopping zookeeper ... STOPPED

[root@node002229 zookeeper]# zkServer.sh start /usr/local/zookeeper/bin/../conf/zoo.cfg  
ZooKeeper JMX enabled by default
Using config: /usr/local/zookeeper/bin/../conf/zoo1.cfg
</code></pre> 
<p>Kafka所有Broker</p> 
<pre><code>cd /usr/local/kafka/;bin/kafka-server-stop.sh
cd /usr/local/kafka/;bin/kafka-server-start.sh -daemon config/server.properties
</code></pre> 
<h4><a id="_96"></a>客户端配置</h4> 
<p>先使用kafka-console-producer 和 kafka-console-consumer 测试一下</p> 
<h5><a id="kafkaconsoleproducer_98"></a>kafka-console-producer</h5> 
<ol><li>创建 config/client-sasl.properties 文件</li></ol> 
<pre><code>[root@node002229 kafka]# vim config/client-sasl.properties
security.protocol=SASL_PLAINTEXT
sasl.mechanism=SCRAM-SHA-256
</code></pre> 
<ol start="2"><li>创建config/kafka_client_jaas_admin.conf文件</li></ol> 
<pre><code>[root@node002229 kafka]# vim config/kafka_client_jaas_admin.conf 
KafkaClient {
  org.apache.kafka.common.security.scram.ScramLoginModule required
  username="admin"
  password="admin-secret";
};
</code></pre> 
<ol start="3"><li>修改kafka-console-producer.sh脚本<br> 这里我复制一份,再改</li></ol> 
<pre><code>cp bin/kafka-console-producer.sh bin/kafka-console-producer-admin.sh
vim bin/kafka-console-producer-admin.sh
#exec $(dirname $0)/kafka-run-class.sh kafka.tools.ConsoleProducer "$@"
exec $(dirname $0)/kafka-run-class.sh -Djava.security.auth.login.config=$(dirname $0)/../config/kafka_client_jaas_admin.conf kafka.tools.ConsoleProducer "$@"
</code></pre> 
<ol start="4"><li>创建测试topic</li></ol> 
<pre><code>bin/kafka-topics.sh --create --zookeeper localhost:2182 --partitions 1 --replication-factor 1 --topic test
</code></pre> 
<ol start="5"><li>测试生产消息</li></ol> 
<pre><code>bin/kafka-console-producer-admin.sh --broker-list 192.168.2.229:9092 --topic test --producer.config config/client-sasl.properties
&gt;1
&gt;
</code></pre> 
<p>可以看到admin用户无需配置ACL就可以生成消息<br> 6. 测试fanboshi用户<br> 如法炮制, 我们创建一个bin/kafka-console-producer-fanboshi.sh文件, 只是修改其中的kafka_client_jaas_admin.conf 为 kafka_client_jaas_fanboshi.conf</p> 
<pre><code>vim config/kafka_client_jaas_fanboshi.conf 
KafkaClient {
  org.apache.kafka.common.security.scram.ScramLoginModule required
  username="fanboshi"
  password="fanboshi";
};

cp bin/kafka-console-producer-admin.sh bin/kafka-console-producer-fanboshi.sh
vi bin/kafka-console-producer-fanboshi.sh
exec $(dirname $0)/kafka-run-class.sh -Djava.security.auth.login.config=$(dirname $0)/../config/kafka_client_jaas_fanboshi.conf kafka.tools.ConsoleProducer "$@"
</code></pre> 
<p>生产消息</p> 
<pre><code>[root@node002229 kafka]# bin/kafka-console-producer-fanboshi.sh --broker-list 192.168.2.229:9092 --topic test --producer.config config/client-sasl.properties
&gt;1
[2019-01-26 18:07:50,099] WARN [Producer clientId=console-producer] Error while fetching metadata with correlation id 1 : {test=TOPIC_AUTHORIZATION_FAILED} (org.apache.kafka.clients.NetworkClient)
[2019-01-26 18:07:50,100] ERROR Error when sending message to topic test with key: null, value: 1 bytes with error: (org.apache.kafka.clients.producer.internals.ErrorLoggingCallback)
org.apache.kafka.common.errors.TopicAuthorizationException: Not authorized to access topics: [test]
</code></pre> 
<p>可以看到报错了, 因为fanboshi用户还没有权限</p> 
<h5><a id="kafkaconsoleconsumer_157"></a>kafka-console-consumer</h5> 
<ol><li>创建 config/consumer-fanboshi.properties 文件</li></ol> 
<pre><code>[root@node002229 kafka]# vim config/consumer-fanboshi.properties
security.protocol=SASL_PLAINTEXT
sasl.mechanism=SCRAM-SHA-256
group.id=fanboshi-group
</code></pre> 
<ol start="2"><li>创建 bin/kafka-console-consumer-fanboshi.sh 文件</li></ol> 
<pre><code>cp bin/kafka-console-consumer.sh bin/kafka-console-consumer-fanboshi.sh
vim bin/kafka-console-consumer-fanboshi.sh
#exec $(dirname $0)/kafka-run-class.sh kafka.tools.ConsoleConsumer "$@"
exec $(dirname $0)/kafka-run-class.sh -Djava.security.auth.login.config=$(dirname $0)/../config/kafka_client_jaas_fanboshi.conf kafka.tools.ConsoleConsumer "$@"
</code></pre> 
<ol start="3"><li>测试消费者</li></ol> 
<pre><code>bin/kafka-console-consumer-fanboshi.sh --bootstrap-server 192.168.2.229:9092 --topic test --consumer.config config/consumer-fanboshi.properties --from-beginning
</code></pre> 
<p>其实也会报错的, 报错内容就不贴了</p> 
<h3><a id="ACL_178"></a>ACL配置</h3> 
<h4><a id="fanboshitest_topic___1921682__179"></a>授予fanboshi用户对test topic 写权限, 只允许 192.168.2.* 网段</h4> 
<pre><code>bin/kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect=localhost:2182 --add --allow-principal User:fanboshi --operation Write --topic test --allow-host 192.168.2.*
</code></pre> 
<h4><a id="fanboshitest_topic___1921682__183"></a>授予fanboshi用户对test topic 读权限, 只允许 192.168.2.* 网段</h4> 
<pre><code>bin/kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect=localhost:2182 --add --allow-principal User:fanboshi --operation Read --topic test --allow-host 192.168.2.*
</code></pre> 
<h4><a id="fanboshi_fanboshigroup__test_topic___1921682__187"></a>授予fanboshi用户, fanboshi-group 消费者组 对test topic 读权限, 只允许 192.168.2.* 网段</h4> 
<pre><code>bin/kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect=localhost:2182 --add --allow-principal User:fanboshi --operation Read --group fanboshi-group --allow-host 192.168.2.*
</code></pre> 
<h4><a id="acl_191"></a>查看acl配置</h4> 
<pre><code>[root@node002229 kafka]# bin/kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect=localhost:2182 --list
Current ACLs for resource `Group:LITERAL:fanboshi-group`: 
        User:fanboshi has Allow permission for operations: Read from hosts: * 

Current ACLs for resource `Topic:LITERAL:test`: 
        User:fanboshi has Allow permission for operations: Write from hosts: *
        User:fanboshi has Allow permission for operations: Read from hosts: * 
</code></pre> 
<h4><a id="_201"></a>删除配置</h4> 
<pre><code>bin/kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect=localhost:2182 --remove --allow-principal User:bob --operation Read --topic example10 --allow-host 192.168.1.158
</code></pre> 
<h4><a id="_206"></a>再次测试</h4> 
<p>生产者</p> 
<pre><code>[root@node002229 kafka]# bin/kafka-console-producer-fanboshi.sh --broker-list 192.168.2.229:9092 --topic test --producer.config config/client-sasl.properties
&gt;1
[2019-01-26 18:07:50,099] WARN [Producer clientId=console-producer] Error while fetching metadata with correlation id 1 : {test=TOPIC_AUTHORIZATION_FAILED} (org.apache.kafka.clients.NetworkClient)
[2019-01-26 18:07:50,100] ERROR Error when sending message to topic test with key: null, value: 1 bytes with error: (org.apache.kafka.clients.producer.internals.ErrorLoggingCallback)
org.apache.kafka.common.errors.TopicAuthorizationException: Not authorized to access topics: [test]
&gt;1
</code></pre> 
<p>消费者</p> 
<pre><code>[root@node002229 kafka]# bin/kafka-console-consumer-fanboshi.sh --bootstrap-server 192.168.2.229:9092 --topic test --consumer.config config/consumer-fanboshi.properties --from-beginning
1
1
</code></pre> 
<p>都没问题了</p> 
<h3><a id="_224"></a>如何查看我们创建了哪些"用户"</h3> 
<p>好像只能去zookeeper看?</p> 
<pre><code>zkCli.sh -server node002229:2182
ls /config/users
[admin, alice, fanboshi]
</code></pre> 
<p>尝试删除alice</p> 
<pre><code>[root@node002229 kafka]# bin/kafka-configs.sh --zookeeper localhost:2182 --describe --entity-type users --entity-name alice
Configs for user-principal 'alice' are SCRAM-SHA-512=salt=MWt1OHRhZnd3cWZvZ2I4bXcwdTM0czIyaTQ=,stored_key=JYeud1Cx5Z2+FaJgJsZGbMcIi63B9XtA9Wyc+KEm2gXK8+2IxxAVvi1CfSjlkqeupfeIMFJ7/EUkOw+zqvYz6w==,server_key=O4NIgjleroia7puK01/ZZoagFeoxh+zHzckGXXooBsWTdx/7Shb0pMHniMu4IY2jb5orWB2t9K8MZkxCliJDsg==,iterations=4096,SCRAM-SHA-256=salt=MTJ3bXRod3EyN3FtZWdsNHk0NXoyeWdlNjE=,stored_key=chQX35reoBYtfg/U5HBtkzvBAk+gSCgskNzUiScOrUE=,server_key=rRTbUzAehwVMUDTMuoOMumGEuvc7wDecKcqK6yYlbWY=,iterations=8192
[root@node002229 kafka]# bin/kafka-configs.sh --zookeeper localhost:2182 --alter --delete-config 'SCRAM-SHA-512' --entity-type users --entity-name alice
Completed Updating config for entity: user-principal 'alice'.
[root@node002229 kafka]# bin/kafka-configs.sh --zookeeper localhost:2182 --describe --entity-type users --entity-name alice                              
Configs for user-principal 'alice' are SCRAM-SHA-256=salt=MTJ3bXRod3EyN3FtZWdsNHk0NXoyeWdlNjE=,stored_key=chQX35reoBYtfg/U5HBtkzvBAk+gSCgskNzUiScOrUE=,server_key=rRTbUzAehwVMUDTMuoOMumGEuvc7wDecKcqK6yYlbWY=,iterations=8192
[root@node002229 kafka]# bin/kafka-configs.sh --zookeeper localhost:2182 --alter --delete-config 'SCRAM-SHA-256' --entity-type users --entity-name alice   
Completed Updating config for entity: user-principal 'alice'.
[root@node002229 kafka]# bin/kafka-configs.sh --zookeeper localhost:2182 --describe --entity-type users --entity-name alice                              
Configs for user-principal 'alice' are 
</code></pre> 
<p>去ZK查看</p> 
<pre><code>[zk: node002229:2182(CONNECTED) 0] ls /config/users
[admin, alice, fanboshi]
</code></pre> 
<p>还是有, 难道只能在ZK手动删除?</p> 
<h3><a id="_251"></a>参考文献</h3> 
<p><a href="http://kafka.apache.org/documentation/#security_sasl_scram" rel="nofollow">http://kafka.apache.org/documentation/#security_sasl_scram</a><br> <a href="https://sharebigdata.wordpress.com/category/kafka/kafka-sasl-scram-with-w-o-ssl/" rel="nofollow">https://sharebigdata.wordpress.com/category/kafka/kafka-sasl-scram-with-w-o-ssl/</a><br> <a href="https://developer.ibm.com/opentech/2017/05/31/kafka-acls-in-practice/" rel="nofollow">https://developer.ibm.com/opentech/2017/05/31/kafka-acls-in-practice/</a><br> <a href="https://developer.ibm.com/tutorials/kafka-authn-authz/" rel="nofollow">https://developer.ibm.com/tutorials/kafka-authn-authz/</a><br> Kafka并不难学! 入门、进阶、商业实战</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/95dc461b6b64d26295cdd182468b9f2e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">C&#43;&#43; Primer 笔记五 const限定符</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/52a5ba6b0a3ddcbd02c15cef1d5f60fd/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【常用模块】ESP8266 WIFI串口通信模块使用详解（实例：附STM32详细代码）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>