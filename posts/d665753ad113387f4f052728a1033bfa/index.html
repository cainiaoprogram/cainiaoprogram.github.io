<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>X-former系列（Transformer大家族） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="X-former系列（Transformer大家族）" />
<meta property="og:description" content="Transformer提出大致时间线：
Transformer 分类：
目录
Vanilla Transformer（2017）Reformer（2020.1）Linear Transformer（2020.1）：注意力计算线性复杂度Performer（2020.9）Longformer（2020.4）Informer（2020.12）Autoformer（2021.6）Transformer-XL（2019.1）：引入跨层的循环机制，支持长距离依赖建模Compressive Transformer（2019.11）：增加压缩记忆模块，增长捕捉的语义长度Infinite-former（2021.9）:使用连续空间注意力框架，它的注意力复杂性与上下文的长度无关 1. Vanilla Transformer（2017） 注意力机制：
理想情况下，位置嵌入（Positional Encoding）的设计应该满足以下条件：
它应该为每个字输出唯一的编码不同长度的句子之间，任何两个字之间的差值应该保持一致它的值应该是有界的 相对位置编码（Sinusoidal Position Encoding）：可区别位置关系但无法区别前后关系
绝对位置编码（Learned Positional Embedding）：不同位置随机初始化可学习参数编码
绝对位置编码展开：
2. Reformer（2020） 一个基于局部敏感哈希（LSH）的注意力模型，引入了可逆的Transformer层，有助于进一步减少内存占用量。复杂度从 O(N^2) 降为 O(N logN)，N 为句子长度。
模型的关键思想，是附近的向量应获得相似的哈希值，而远距离的向量则不应获得相似的哈希值，因此被称为“局部敏感”。
并且，标准残差 Layer 替换为可逆残差 Layer，使得训练中只存储一次激活值，而不是 N 次，N 为 Layer 数量。
3. Linear Transformer（2020） 【论文翻译】
这个模型通过使用基于核的自注意力机制、和矩阵产品的关联特性，将自注意力的复杂性从二次降低为线性O(N)。已经被证明可以在基本保持预测性能的情况下，将推理速度提高多达三个数量级。
将softmax attention 线性化： 使用特征图的线性点积来逼近 softmax具有线性复杂度和常数内存的自回归变换器模型 Attention自注意力机制广义公式，Q 和 K 利用相似（sim）函数计算出一个分数，之后除以分数总和获得 Attention 权重，之后去 V 里取值:
接着用核函数 ，对 sim 函数进行处理，再利用结合律，将 Q 运算提出来
简化
4. Performer（2020） 这个模型利用正交随机特征（ORF），采用近似的方法避免存储和计算注意力矩阵。
5. Autoformer Autoformer算法与代码分析_布川酷籽的博客-CSDN博客" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/d665753ad113387f4f052728a1033bfa/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-12T12:07:15+08:00" />
<meta property="article:modified_time" content="2023-04-12T12:07:15+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">X-former系列（Transformer大家族）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>Transformer提出大致时间线：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/7f/f0/9LMNh0d5_o.jpg"></p> 
<p>Transformer 分类：</p> 
<p class="img-center"><img alt="" height="845" src="https://images2.imgbox.com/6e/79/NQdaCc90_o.png" width="669"></p> 
<p id="main-toc"><strong>目录</strong></p> 
<ul><li><strong>Vanilla Transformer（2017）</strong></li><li><strong>Reformer（2020.1）</strong></li><li><strong>Linear Transformer（2020.1）：</strong>注意力计算线性复杂度</li><li><strong>Performer（2020.9）</strong></li><li><strong>Longformer（2020.4）</strong></li><li><strong>Informer（2020.12）</strong></li><li><strong>Autoformer（2021.6）</strong></li><li><strong>Transformer-XL（2019.1）：</strong>引入跨层的循环机制，支持长距离依赖建模</li><li><strong>Compressive Transformer（2019.11）：</strong>增加压缩记忆模块，增长捕捉的语义长度</li><li><strong>Infinite-former（2021.9）</strong>:使用<strong>连续空间注意力框架</strong>，它的注意力复杂性与上下文的长度无关</li></ul> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="Vanilla%20Transformer%EF%BC%882017%EF%BC%89"><span style="color:#1a439c;"><strong>1. Vanilla Transformer（2017）</strong></span></h2> 
<p class="img-center"><img alt="" height="407" src="https://images2.imgbox.com/a6/28/z3iyEuky_o.png" width="445"></p> 
<p>注意力机制：</p> 
<p><img alt="" height="193" src="https://images2.imgbox.com/90/c2/LFkT5x0c_o.png" width="417"></p> 
<p>理想情况下，<strong>位置嵌入（Positional Encoding）</strong>的设计应该满足以下条件：</p> 
<ul><li>它应该为每个字输出唯一的编码</li><li>不同长度的句子之间，任何两个字之间的差值应该保持一致</li><li>它的值应该是有界的</li></ul> 
<p><strong>相对位置编码</strong>（<strong>Sinusoidal Position Encoding</strong>）：可区别位置关系但无法区别前后关系<br><strong>绝对位置编码</strong>（<strong>Learned Positional Embedding</strong>）：不同位置随机初始化可学习参数编码</p> 
<p><img alt="" height="44" src="https://images2.imgbox.com/06/6b/VCgd1Hy4_o.png" width="394"> 绝对位置编码展开：</p> 
<p><img alt="" height="92" src="https://images2.imgbox.com/29/66/vXdxeJp3_o.png" width="883"></p> 
<p></p> 
<h2 id="Reformer%EF%BC%882020%EF%BC%89"><span style="color:#1a439c;"><strong>2. Reformer（2020）</strong></span></h2> 
<p>一个基于局部敏感哈希（LSH）的注意力模型，引入了可逆的Transformer层，有助于进一步减少内存占用量。复杂度从 O(N^2) 降为 O(N logN)，N 为句子长度。</p> 
<p>模型的关键思想，是附近的向量应获得相似的哈希值，而远距离的向量则不应获得相似的哈希值，因此被称为“局部敏感”。</p> 
<p>并且，标准残差 Layer 替换为可逆残差 Layer，使得训练中只存储一次激活值，而不是 N 次，N 为 Layer 数量。</p> 
<p></p> 
<h2 id="Linear%20Transformer%EF%BC%882020%EF%BC%89"><span style="color:#1a439c;"><strong>3. Linear Transformer（2020）</strong></span></h2> 
<p><a class="link-info" href="https://blog.csdn.net/hymn1993/article/details/125254897" title="【论文翻译】">【论文翻译】</a></p> 
<p>这个模型通过使用<strong>基于核</strong>的自注意力机制、和矩阵产品的关联特性，将自注意力的复杂性从二次降低为线性O(N)。已经被证明可以在基本保持预测性能的情况下，将推理速度提高多达三个数量级。</p> 
<ul><li>将softmax  attention 线性化： 使用特征图的线性点积来逼近 softmax</li><li>具有线性复杂度和常数内存的自回归变换器模型</li></ul> 
<p>Attention自注意力机制广义公式，Q 和 K 利用相似（sim）函数计算出一个分数，之后除以分数总和获得 Attention 权重，之后去 V 里取值:</p> 
<p class="img-center"><img alt="" height="79" src="https://images2.imgbox.com/a6/a2/l4XVxVWW_o.png" width="417"></p> 
<p>接着用核函数 ，对 sim 函数进行处理，再利用结合律，将 Q 运算提出来</p> 
<p class="img-center"><img alt="" height="71" src="https://images2.imgbox.com/06/17/RquPWZlQ_o.png" width="369"></p> 
<p>简化</p> 
<p><img alt="" class="left" height="89" src="https://images2.imgbox.com/68/1b/IshV9TpQ_o.jpg" width="336"><img alt="" class="left" height="101" src="https://images2.imgbox.com/29/b0/Dsx5PmZs_o.png" width="299"></p> 
<p></p> 
<h2 id="Performer%EF%BC%882020%EF%BC%89"><span style="color:#1a439c;"><strong>4. Performer（2020）</strong></span></h2> 
<p>这个模型利用正交随机特征（ORF），采用近似的方法避免存储和计算注意力矩阵。<br>  </p> 
<p></p> 
<h2><span style="color:#1a439c;"><strong>5. Autoformer</strong></span></h2> 
<p><a href="https://blog.csdn.net/buchuankuzi_/article/details/128390246" title="Autoformer算法与代码分析_布川酷籽的博客-CSDN博客">Autoformer算法与代码分析_布川酷籽的博客-CSDN博客</a></p> 
<p></p> 
<h2 id="Transformer-XL%EF%BC%882020%EF%BC%89"><span style="color:#1a439c;"><strong>6. Transformer-XL（2020）</strong></span></h2> 
<h4 id="%E5%8E%9FTransformer%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%9A">原Transformer的问题：</h4> 
<ul><li>segments之间<strong>独立训练</strong>，所以不同的token之间，最长的依赖关系，就取决于segment的长度；</li><li>出于效率的考虑，在划分segments的时候，不考虑句子的自然边界，而是根据<strong>固定的长度</strong>来划分序列，导致分割出来的segments在语义上是不完整的（<strong>context fragmentation</strong>）</li></ul> 
<p class="img-center"><img alt="" height="293" src="https://images2.imgbox.com/9c/71/EYZFawmG_o.gif" width="586"></p> 
<p>Transformer-XL很有RNN的味道：设法利用之前的segment留下的信息eval时滚动向前，但每次要重新计算整个segment，eval的时候滚动向前，不用重复计算前面segment因为每次在滚动向前，这样就需要使用相对位置编码，而不能使用绝对位置编码。</p> 
<p>围绕如何建模长距离依赖，提出Transformer-XL：</p> 
<h4 id="Segment-Level%20Recurrence%20%E7%89%87%E6%AE%B5%E9%80%92%E5%BD%92"><span style="color:#511b78;"><strong>Segment-Level Recurrence 片段递归</strong></span></h4> 
<p>提出<strong>片段级递归机制(segment-level recurrence mechanism)</strong>，引入一个<strong>记忆(memory)</strong>模块（类似于cache或cell），循环用来建模片段之间的联系。这部分输出是通过cache的机制传导过来，所以不会参与梯度的计算。<strong>原则上只要GPU内存允许，该方法可以利用前面更多段的信息。</strong></p> 
<p class="img-center"><img alt="" height="209" src="https://images2.imgbox.com/3e/73/5s8uoAJA_o.png" width="725"></p> 
<p> 在<strong>预测阶段</strong>：<br> 如果预测 x4 ​我们只要拿之前预测好的 [x1, x2, x3] 的结果拿过来，直接预测。同理在预测 x5 ​的时候，直接在 [x1,x2,x3,x4] 的基础上计算，不用像Vanilla Transformer一样每次预测一个字就要重新计算前面固定个数的词。</p> 
<p><img alt="" height="347" src="https://images2.imgbox.com/4f/3e/rDS8XMH7_o.png" width="609"></p> 
<h4 id="Relative%20Position%20Encodings%20%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span style="color:#511b78;"><strong>Relative Position Encodings 相对位置编码</strong></span></h4> 
<p>在memory的循环计算过程中，避免时序混淆，需要位置编码可重用</p> 
<p><img alt="" height="105" src="https://images2.imgbox.com/46/56/zEXKZDef_o.png" width="776"></p> 
<p>其中：</p> 
<ul><li><img alt="R_{i-j}" class="mathcode" src="https://images2.imgbox.com/65/53/Z6NWcrcd_o.png"> 表示的是i和j的相对距离，是sinusoid encoding matrix，没有额外的训练参数。实际上和vanilla的位置编码一样的，关键是这里的位置编码只给key用，而key的长度，在第一个片段和query的长度一样，之后的片段，key长度=上一个片段hidden state长度+当前片段key的长度，因此 <img alt="R_{i-j}" class="mathcode" src="https://images2.imgbox.com/b3/f3/i2gLDxVj_o.png"> 是能够表示出key的相对距离的</li><li>因为无论query在序列中的绝对位置如何，其相对于自身的相对位置都是一样的与在序列中的绝对位置无关，应当保持不变.。因此用两个可训练的参数u、v</li><li>vanilla版本的key位置编码与embedding都是采用同样的变化矩阵，xl中，把key的位置编码和embedding分别用了不同的线性变化</li></ul> 
<p>每个分项分别代表的含义如下：</p> 
<ul><li>(a)描述了基于内容的Attention,即没有添加原始位置编码的原始分数；</li><li>(b)描述了内容对于每个相对位置的bias，即相对于当前内容的位置偏置；</li><li>(c)描述了全局的内容偏置，用于衡量key的重要性；</li><li>(d)描述了全局的位置偏置，根据query和key之间的距离调整重要性。</li></ul> 
<p></p> 
<p></p> 
<h2 id="Compressive%20Transformers%EF%BC%882020%EF%BC%89"><span style="color:#1a439c;"><strong>7. Compressive Transformers（2020）</strong></span></h2> 
<p><strong><a class="link-info" href="https://github.com/lucidrains/compressive-transformer-pytorch" title="【代码】">【代码】</a></strong></p> 
<p>这个模型是Transformer-XL的扩展，但不同于Transformer-XL，后者在跨段移动时会<strong>丢弃过去的激活</strong>，而它的关键思想则是<strong>保持对过去段激活的细粒度记忆。</strong></p> 
<p>压缩如下:</p> 
<p><img alt="" height="39" src="https://images2.imgbox.com/07/b4/btCLbMMF_o.png" width="284">其中d是隐藏层的维度，c是压缩比例，c越大 压缩越多</p> 
<p><img alt="" height="696" src="https://images2.imgbox.com/82/15/8hF4geCM_o.png" width="1200"></p> 
<h4 id="%E5%8E%8B%E7%BC%A9%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4"><span style="color:#511b78;"><strong>压缩算法步骤</strong></span></h4> 
<p>首先 有 <strong>2个memory</strong>： 一个是存放正常的前几个segment的<strong>hidden state</strong> ,计做 m, 一个是存放<strong>压缩的记忆</strong>模块 cm。</p> 
<p><img alt="" height="950" src="https://images2.imgbox.com/1a/69/xp3Y1CLa_o.png" width="1200"></p> 
<h4 id="%E5%8E%8B%E7%BC%A9%E5%87%BD%E6%95%B0%20f_c%E4%B8%8D%E5%90%8C%E6%9E%84%E5%BB%BA%E6%96%B9%E5%BC%8F%EF%BC%9A"><span style="color:#511b78;"><strong>压缩函数 f_c不同构建方式：</strong></span></h4> 
<ul><li>max/mean pooling： kernel 和步长设置为 压缩比例 c。 这个是最快最简单的baseline。</li><li>1D convolution ：kernel和步长也是设置为c.</li><li>dilated convolutions ：膨胀卷积。卷积压缩方法 包含需要训练的参数。</li><li>most-used ：memories 存储 通过他们的平均attention 和最常使用来存储。这个来源于垃圾回收机制，不常用的记忆模块被删除掉。</li></ul> 
<h4 id="Compressive%20Transformer%20%E5%92%8C%20Transformer-XL%E6%AF%94%E8%BE%83"><span style="color:#511b78;"><strong>Compressive Transformer 和 Transformer-XL比较</strong></span></h4> 
<p> <img alt="" height="173" src="https://images2.imgbox.com/53/5f/2BcZLfZF_o.png" width="453"> 计算复杂度</p> 
<p></p> 
<p>7.  Infinite-transformer</p> 
<p></p> 
<p></p> 
<p></p> 
<h2 id="References"><span style="color:#be191c;"><strong>References</strong></span></h2> 
<p>A Survey of Transformers（2021），<a href="https://arxiv.org/abs/2106.04554" rel="nofollow" title="https://arxiv.org/abs/2106.04554">https://arxiv.org/abs/2106.04554</a></p> 
<p>Efficient Transformers: A Survey（2020）</p> 
<p><a class="link-info" href="https://arxiv.org/abs/2203.12944v1" rel="nofollow" title="Transformers Meet Visual Learning Understanding: A Comprehensive Review（2022）">Transformers Meet Visual Learning Understanding: A Comprehensive Review（2022）</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/163216524" rel="nofollow" title="Transformer 是 CNN 是 GNN 是 RNN ? - 知乎">Transformer 是 CNN 是 GNN 是 RNN ? - 知乎</a></p> 
<p><a href="https://www.zhihu.com/question/349958732/answer/1966379524" rel="nofollow" title="有哪些令你印象深刻的魔改transformer？">有哪些令你印象深刻的魔改transformer？</a></p> 
<p></p> 
<p></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a7ed204044dea8e0a9537a53eaec3c72/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【论文精读】Arxiv 2023 - Segment Anything</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/3f8c9eb0fa2540d9c281c89c733b6e7a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">如何在webstorm中配置node</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>