<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>基于tensorflow的LSTM 时间序列预测模型 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="基于tensorflow的LSTM 时间序列预测模型" />
<meta property="og:description" content="RNN 递归神经网络（RNN）相对于MLP和CNN的主要优点是，它能够处理序列数据，在传统神经网络或卷积神经网络中，样本（sample）输入与输出是没有“顺序”概念的，可以理解为，如果把输入序列和输出序列重新排布，对整体模型的理论性能不会有影响。RNN则不同，它保证了输入和输出至少有一端是有序列特征的。
传统的神经网络结构可以归纳会下图左边的形式，隐藏层h的状态是不保存的，而在RNN中，每一个时间步的隐藏层状态都是由上一层的输入和上一个时间的状态共同计算得到。
RNN算法的细节这里就不赘述，RNN的缺点在于，误差反向传播时，由于时间步t的梯度是由t时的状态h分别对前面所有时间步的状态求导，再相乘得到，在状态权重的模大于1时，若时间步t较长，梯度就会消失（趋近于0），即长期的依赖很小，相反，在状态权重的模小于1时，若时间步t较短，梯度就会爆炸（很大），即短期的依赖很大。具体分析请[参考文献]。(http://ARXIV.org/pdf/1211.5063.pdf)，解决这一问题有两个主要的方法：
截断的时间反向传播法（TBPTT）：将梯度限制在某个范围，这种做法简单粗暴，却能解决梯度爆炸和消失的问题，但是限制了梯度的传播；长短期记忆（LSTM） LSTM LSTM最先是被引入来解决梯度小时问题，LSTM在神经网络中是一个特殊的隐藏层，他将时间步t的隐藏层状态更新结构表示如下：
图来源以及LSTM的原理介绍可以参考
LSTM由三个门来控制细胞状态，这三个门分别称为遗忘门、输入门和输出门。
遗忘门：允许记忆单元在有限增长的情况下重置信元状态。遗忘门类似于一个过滤器，决定上一个时间步的信元状态C能否通过输入门：负责根据输入值和遗忘门的输出，来更新信元状态C输出们：更新隐藏单元的值
当然，LSTM的形式也是存在很多变式的，不同的变式在大部分任务上效果都差不多，在一些特殊任务上，一些变式要优于标准的LSTM 利用LSTM进行时间序列预测 一般在时间序列预测上，常用的方法主要有ARIMA之类的统计分析，机器学习中经典的回归分析等
统计分析中（如ARIMA），将时间序列分为三个部分：趋势，季节性和波动，通过统计学算法将序列分离，得到每个部分的模型再相加，但是模型对于序列的方差、均值平稳性以及自相关性都有很高的要求，否则模型偏差会很大。回归分析注重模型偏差和方差，对特征的要求也很高，回归分析中处理非线性问题是一个很艰难的过程。 这里采用LSTM来进行时间序列预测，结构为:
训练数据生成—&gt;隐藏输入层—&gt;LSTM神经层—&gt;隐藏输出层（全连接层）—&gt;结果
当然，也可以根据任务增加隐藏层，LSTM层以及全连接层的数量。
tensorflow中已经为我们准备好了LSTM层的接口，根据需要配置即可。
这里列举几个重要的注意点：
首先要理解什么是序列和序列化数据，比如如果我要预测24小时的天气，那将会有很多种方案，每种方案的序列化都不一样，若模型输出就是24小时的序列，那么输入序列可以是 t-1之前任意长度的序列，输出序列是t &gt; t&#43;23；也可以输入序列为t-24之前的序列来预测t时候的值，进行24次预测；也可以用t-1之前的序列要预测t时，每次预测结果再代入输入中预测t时刻之后的值。总之，每种做法效果不一样，具体问题还需要具体分析；TIME_STEPS参数，可以理解为时间步，就是你需要几个时刻的样本来预测，INPUT_SIZE 为每个样本的维度，如果你的样本数据是一个单一序列，没有其他特征的话，则可设置为1；OUTPUT_SIZE 为输出的维度，就是输出序列的长度；如果输出也是一个序列的话，可以将y的维度设置为[None,TIME_STEPS,OUTPUT_SIZE] import numpy as np import tensorflow as tf import matplotlib.pyplot as plt %matplotlib inline #------------------------------------Generate Data-----------------------------------------------# TIME_STEPS = 20 BATCH_SIZE = 1 INPUT_SIZE = 1 OUTPUT_SIZE = 1 CELL_SIZE = 50 LEARNING_RATE = 0.001 EPOCH = 100 LAYER_NUM =3 # 参数说明：TIME_STEPS:输入序列的时间步，； # INPUT_SIZE:输入序列中每个向量的维度 # BATCH_SIZE：训练的批次 # OUTPUT_SIZE：输出序列的向量维度 # CELL_SIZE:LSTM神经层的细胞数，也是LSTM层的输入和输出维度（这两个维度相同）,也即为LSTMCell中的num_units参数; # LEARNING_RATE：tensorflow中optimizer的学习率； # EPOCH：迭代次数或训练次数；; # LAYER_NUM：LSTM神经层的层数。 # #生成训练数据和测试数据 def generate(seq,time_step,output_size): X = [] Y = [] for i in range(len(seq)-time_step-output_size): X." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/300d128745d36d72b892ae14e6907646/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-07-09T17:22:44+08:00" />
<meta property="article:modified_time" content="2019-07-09T17:22:44+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">基于tensorflow的LSTM 时间序列预测模型</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h4><a id="RNN_0"></a>RNN</h4> 
<p>递归神经网络（RNN）相对于MLP和CNN的主要优点是，它能够处理序列数据，在传统神经网络或卷积神经网络中，样本（sample）输入与输出是没有“顺序”概念的，可以理解为，如果把输入序列和输出序列重新排布，对整体模型的理论性能不会有影响。RNN则不同，它保证了输入和输出至少有一端是有序列特征的。</p> 
<p>传统的神经网络结构可以归纳会下图左边的形式，隐藏层h的状态是不保存的，而在RNN中，每一个时间步的隐藏层状态都是由上一层的输入和上一个时间的状态共同计算得到。<br> <img src="https://images2.imgbox.com/10/e8/oHk88TWv_o.png" alt="在这里插入图片描述"><br> RNN算法的细节这里就不赘述，RNN的缺点在于，误差反向传播时，由于时间步t的梯度是由t时的状态h分别对前面所有时间步的状态求导，再相乘得到，在状态权重的模大于1时，若时间步t较长，梯度就会消失（趋近于0），即长期的依赖很小，相反，在状态权重的模小于1时，若时间步t较短，梯度就会爆炸（很大），即短期的依赖很大。具体分析请[参考文献]。(http://ARXIV.org/pdf/1211.5063.pdf)，解决这一问题有两个主要的方法：</p> 
<ul><li>截断的时间反向传播法（TBPTT）：将梯度限制在某个范围，这种做法简单粗暴，却能解决梯度爆炸和消失的问题，但是限制了梯度的传播；</li><li>长短期记忆（LSTM）</li></ul> 
<h4><a id="LSTM_8"></a>LSTM</h4> 
<p>LSTM最先是被引入来解决梯度小时问题，LSTM在神经网络中是一个特殊的<strong>隐藏层</strong>，他将时间步t的隐藏层状态更新结构表示如下：<br> <img src="https://images2.imgbox.com/f1/1f/WTvoZ4Ow_o.png" alt="在这里插入图片描述"><br> 图来源以及LSTM的原理介绍可以<a href="https://www.jianshu.com/p/95d5c461924c" rel="nofollow">参考</a><br> LSTM由三个门来控制细胞状态，这三个门分别称为遗忘门、输入门和输出门。</p> 
<ul><li>遗忘门：允许记忆单元在有限增长的情况下重置信元状态。遗忘门类似于一个过滤器，决定上一个时间步的信元状态C能否通过</li><li>输入门：负责根据输入值和遗忘门的输出，来更新信元状态C</li><li>输出们：更新隐藏单元的值<br> 当然，LSTM的形式也是存在很多变式的，不同的变式在大部分任务上效果都差不多，在一些特殊任务上，一些变式要优于标准的LSTM</li></ul> 
<h4><a id="LSTM_17"></a>利用LSTM进行时间序列预测</h4> 
<p>一般在时间序列预测上，常用的方法主要有ARIMA之类的统计分析，机器学习中经典的回归分析等</p> 
<ul><li>统计分析中（如ARIMA），将时间序列分为三个部分：趋势，季节性和波动，通过统计学算法将序列分离，得到每个部分的模型再相加，但是模型对于序列的方差、均值平稳性以及自相关性都有很高的要求，否则模型偏差会很大。</li><li>回归分析注重模型偏差和方差，对特征的要求也很高，回归分析中处理非线性问题是一个很艰难的过程。</li></ul> 
<p>这里采用LSTM来进行时间序列预测，结构为:</p> 
<blockquote> 
 <p><em>训练数据生成—&gt;隐藏输入层—&gt;LSTM神经层—&gt;隐藏输出层（全连接层）—&gt;结果</em></p> 
</blockquote> 
<p>当然，也可以根据任务增加隐藏层，LSTM层以及全连接层的数量。<br> tensorflow中已经为我们准备好了LSTM层的接口，根据需要配置即可。<br> 这里列举几个重要的注意点：</p> 
<ol><li>首先要理解什么是序列和序列化数据，比如如果我要预测24小时的天气，那将会有很多种方案，每种方案的序列化都不一样，若模型输出就是24小时的序列，那么输入序列可以是 t-1之前任意长度的序列，输出序列是t &gt; t+23；也可以输入序列为t-24之前的序列来预测t时候的值，进行24次预测；也可以用t-1之前的序列要预测t时，每次预测结果再代入输入中预测t时刻之后的值。总之，每种做法效果不一样，具体问题还需要具体分析；</li><li>TIME_STEPS参数，可以理解为时间步，就是你需要几个时刻的样本来预测，INPUT_SIZE 为每个样本的维度，如果你的样本数据是一个单一序列，没有其他特征的话，则可设置为1；OUTPUT_SIZE 为输出的维度，就是输出序列的长度；如果输出也是一个序列的话，可以将y的维度设置为[None,TIME_STEPS,OUTPUT_SIZE]</li></ol> 
<pre><code class="prism language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token operator">%</span>matplotlib inline
<span class="token comment">#------------------------------------Generate Data-----------------------------------------------#</span>
TIME_STEPS <span class="token operator">=</span> <span class="token number">20</span>
BATCH_SIZE <span class="token operator">=</span> <span class="token number">1</span>
INPUT_SIZE <span class="token operator">=</span> <span class="token number">1</span>
OUTPUT_SIZE <span class="token operator">=</span> <span class="token number">1</span>
CELL_SIZE <span class="token operator">=</span> <span class="token number">50</span>
LEARNING_RATE <span class="token operator">=</span> <span class="token number">0.001</span>
EPOCH <span class="token operator">=</span> <span class="token number">100</span>
LAYER_NUM <span class="token operator">=</span><span class="token number">3</span>

<span class="token comment"># 参数说明：TIME_STEPS:输入序列的时间步，；</span>
<span class="token comment">#         INPUT_SIZE:输入序列中每个向量的维度</span>
<span class="token comment">#         BATCH_SIZE：训练的批次</span>
<span class="token comment">#         OUTPUT_SIZE：输出序列的向量维度</span>
<span class="token comment">#         CELL_SIZE:LSTM神经层的细胞数，也是LSTM层的输入和输出维度（这两个维度相同）,也即为LSTMCell中的num_units参数;</span>
<span class="token comment">#         LEARNING_RATE：tensorflow中optimizer的学习率；</span>
<span class="token comment">#         EPOCH：迭代次数或训练次数；;</span>
<span class="token comment">#         LAYER_NUM：LSTM神经层的层数。</span>
<span class="token comment">#</span>
<span class="token comment">#生成训练数据和测试数据</span>
<span class="token keyword">def</span> <span class="token function">generate</span><span class="token punctuation">(</span>seq<span class="token punctuation">,</span>time_step<span class="token punctuation">,</span>output_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
    X <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    Y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>seq<span class="token punctuation">)</span><span class="token operator">-</span>time_step<span class="token operator">-</span>output_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        X<span class="token punctuation">.</span>append<span class="token punctuation">(</span>seq<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i<span class="token operator">+</span>time_step<span class="token punctuation">]</span><span class="token punctuation">)</span>
        Y<span class="token punctuation">.</span>append<span class="token punctuation">(</span>seq<span class="token punctuation">[</span>i<span class="token operator">+</span>time_step<span class="token punctuation">:</span>i<span class="token operator">+</span>time_step<span class="token operator">+</span>output_size<span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>X<span class="token punctuation">,</span>dtype <span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">,</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>Y<span class="token punctuation">,</span>dtype <span class="token operator">=</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>


seq <span class="token operator">=</span> np<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">100</span><span class="token punctuation">,</span><span class="token number">10000</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token operator">/</span>np<span class="token punctuation">.</span>pi<span class="token punctuation">)</span>
<span class="token comment"># seq_y = data_day.values.reshape([-1])</span>
X_train<span class="token punctuation">,</span>y_train <span class="token operator">=</span> generate<span class="token punctuation">(</span>seq<span class="token punctuation">,</span>TIME_STEPS<span class="token punctuation">,</span>OUTPUT_SIZE<span class="token punctuation">)</span>
X_test<span class="token punctuation">,</span> y_test <span class="token operator">=</span> generate<span class="token punctuation">(</span>seq<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">24</span><span class="token operator">*</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">,</span>TIME_STEPS<span class="token punctuation">,</span>OUTPUT_SIZE<span class="token punctuation">)</span>



X_train <span class="token operator">=</span> np<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>newshape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>TIME_STEPS<span class="token punctuation">,</span>INPUT_SIZE<span class="token punctuation">)</span><span class="token punctuation">)</span>
X_test <span class="token operator">=</span> np<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>X_test<span class="token punctuation">,</span>newshape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>TIME_STEPS<span class="token punctuation">,</span>INPUT_SIZE<span class="token punctuation">)</span><span class="token punctuation">)</span>
y_train <span class="token operator">=</span> np<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>y_train<span class="token punctuation">,</span>newshape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>OUTPUT_SIZE<span class="token punctuation">)</span><span class="token punctuation">)</span>
y_test <span class="token operator">=</span> np<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span>newshape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>OUTPUT_SIZE<span class="token punctuation">)</span><span class="token punctuation">)</span>

TRAIN_EXAMPLES <span class="token operator">=</span> X_train<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
TEST_EXAMPLES <span class="token operator">=</span> X_test<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>



<span class="token keyword">def</span> <span class="token function">tf_init_state</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span>cells<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>cells<span class="token punctuation">,</span><span class="token builtin">list</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        states <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> cell <span class="token keyword">in</span> cells<span class="token punctuation">:</span>
            c_size <span class="token operator">=</span> cell<span class="token punctuation">.</span>state_size<span class="token punctuation">.</span>c
            h_size <span class="token operator">=</span> cell<span class="token punctuation">.</span>state_size<span class="token punctuation">.</span>h
            c <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">[</span>batch_size<span class="token punctuation">,</span>c_size<span class="token punctuation">]</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
            h <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">[</span>batch_size<span class="token punctuation">,</span>c_size<span class="token punctuation">]</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
            states<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>rnn_cell<span class="token punctuation">.</span>LSTMStateTuple<span class="token punctuation">(</span>c<span class="token operator">=</span>c<span class="token punctuation">,</span>h<span class="token operator">=</span>h<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> <span class="token builtin">tuple</span><span class="token punctuation">(</span>states<span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        c_size <span class="token operator">=</span> cells<span class="token punctuation">.</span>state_size<span class="token punctuation">.</span>c
        h_size <span class="token operator">=</span> cells<span class="token punctuation">.</span>state_size<span class="token punctuation">.</span>h
        c <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">'{}/c'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>cells<span class="token punctuation">.</span>name<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">[</span>batch_size<span class="token punctuation">,</span>c_size<span class="token punctuation">]</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span>initializer<span class="token operator">=</span>tf<span class="token punctuation">.</span>zeros_initializer<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">)</span>
        h <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">'{}/h'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>cells<span class="token punctuation">.</span>name<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">[</span>batch_size<span class="token punctuation">,</span>h_size<span class="token punctuation">]</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span>initializer<span class="token operator">=</span>tf<span class="token punctuation">.</span>zeros_initializer<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>rnn_cell<span class="token punctuation">.</span>LSTMStateTuple<span class="token punctuation">(</span>c<span class="token operator">=</span>c<span class="token punctuation">,</span>h<span class="token operator">=</span>h<span class="token punctuation">)</span>
    
<span class="token keyword">def</span> <span class="token function">get_states</span><span class="token punctuation">(</span>statetuple<span class="token punctuation">,</span>batch_pre<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>statetuple<span class="token punctuation">,</span><span class="token builtin">tuple</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        newstate <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> state <span class="token keyword">in</span> <span class="token builtin">list</span><span class="token punctuation">(</span>statetuple<span class="token punctuation">)</span><span class="token punctuation">:</span>
            c <span class="token operator">=</span> state<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span>batch_pre<span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
            h <span class="token operator">=</span> state<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span>batch_pre<span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
            newstate<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>rnn_cell<span class="token punctuation">.</span>LSTMStateTuple<span class="token punctuation">(</span>c<span class="token operator">=</span>c<span class="token punctuation">,</span>h<span class="token operator">=</span>h<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> <span class="token builtin">tuple</span><span class="token punctuation">(</span>newstate<span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        c <span class="token operator">=</span> statetuple<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        h <span class="token operator">=</span> statetuple<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
        <span class="token keyword">return</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>rnn_cell<span class="token punctuation">.</span>LSTMStateTuple<span class="token punctuation">(</span>c<span class="token operator">=</span>c<span class="token punctuation">[</span><span class="token punctuation">:</span>batch_pre<span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span>h<span class="token operator">=</span>h<span class="token punctuation">[</span><span class="token punctuation">:</span>batch_pre<span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment">#------定义权重和偏差生成函数</span>
<span class="token keyword">def</span> <span class="token function">weight_variable</span><span class="token punctuation">(</span>shape<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'weights'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    initializer <span class="token operator">=</span> tf<span class="token punctuation">.</span>random_normal_initializer<span class="token punctuation">(</span>mean<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> stddev<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span>name<span class="token operator">=</span>name<span class="token punctuation">,</span> shape<span class="token operator">=</span>shape<span class="token punctuation">,</span> initializer<span class="token operator">=</span>initializer<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">bias_variable</span><span class="token punctuation">(</span>shape<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'biases'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    initializer <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant_initializer<span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span>name<span class="token operator">=</span>name<span class="token punctuation">,</span> shape<span class="token operator">=</span>shape<span class="token punctuation">,</span> initializer<span class="token operator">=</span>initializer<span class="token punctuation">)</span>

<span class="token comment">#定义计算图---------------------------------------------------#</span>
graph <span class="token operator">=</span> tf<span class="token punctuation">.</span>Graph<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">with</span> graph<span class="token punctuation">.</span>as_default<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>name_scope<span class="token punctuation">(</span><span class="token string">'inputs'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        x_p <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span>TIME_STEPS<span class="token punctuation">,</span>INPUT_SIZE<span class="token punctuation">)</span><span class="token punctuation">,</span>name<span class="token operator">=</span><span class="token string">'xs'</span><span class="token punctuation">)</span>
        y_p <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span>OUTPUT_SIZE<span class="token punctuation">)</span><span class="token punctuation">,</span>name <span class="token operator">=</span> <span class="token string">'ys'</span><span class="token punctuation">)</span>
        cell_batch <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span>shape <span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>name<span class="token operator">=</span><span class="token string">'batch_size'</span><span class="token punctuation">)</span>
<span class="token comment">#定义输入隐藏层，x_p维度（batch_size,time_steps,input_size）,l_in_x维度(batch_size*time_steps,input_size),l_in_y维度(batch_size*time_steps,cell_size)</span>
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">'in_hidden'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        l_in_x <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>x_p<span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>INPUT_SIZE<span class="token punctuation">]</span><span class="token punctuation">)</span>
        ws_in <span class="token operator">=</span> weight_variable<span class="token punctuation">(</span><span class="token punctuation">[</span>INPUT_SIZE<span class="token punctuation">,</span>CELL_SIZE<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token string">'ws_in'</span><span class="token punctuation">)</span>
        bs_in <span class="token operator">=</span> bias_variable<span class="token punctuation">(</span><span class="token punctuation">[</span>CELL_SIZE<span class="token punctuation">,</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token string">'bs_in'</span><span class="token punctuation">)</span>
        l_in_y <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>l_in_x<span class="token punctuation">,</span>ws_in<span class="token punctuation">)</span> <span class="token operator">+</span>  bs_in
    
<span class="token comment">#定义lstm神经层，l_cell_x维度（batch_size,time_steps,cell_size），l_cell_y维度（batch_size,time——steps，cell_size）</span>
<span class="token comment">#-----单层lstm神经层</span>
<span class="token comment">#     lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=HIDDEN_UNITS)</span>
<span class="token comment">#     lstm_model = lstm_cell</span>
<span class="token comment">#-----dynamic rnn</span>
<span class="token comment">#     outputs,states=tf.nn.dynamic_rnn(cell=lstm_cell,inputs=x_p,initial_state=init_state,dtype=tf.float32)</span>
<span class="token comment">#     h=outputs[:,-1,:]</span>
<span class="token comment">#     lstm_cell1 = rnn.BasicLSTMCell(num_units=HIDDEN_UNITS)</span>
<span class="token comment">#-----多层lstm神经层</span>
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">'Lstm_cell_train'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        l_cell_x<span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>l_in_y<span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>TIME_STEPS<span class="token punctuation">,</span>CELL_SIZE<span class="token punctuation">]</span><span class="token punctuation">)</span>
        lstm_cell <span class="token operator">=</span> <span class="token punctuation">[</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>rnn_cell<span class="token punctuation">.</span>BasicLSTMCell<span class="token punctuation">(</span>num_units<span class="token operator">=</span>CELL_SIZE<span class="token punctuation">,</span>state_is_tuple<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>LAYER_NUM<span class="token punctuation">)</span><span class="token punctuation">]</span>
        lstm_model <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>MultiRNNCell<span class="token punctuation">(</span>cells<span class="token operator">=</span>lstm_cell<span class="token punctuation">)</span>
        init_state <span class="token operator">=</span> lstm_model<span class="token punctuation">.</span>zero_state<span class="token punctuation">(</span>batch_size<span class="token operator">=</span>BATCH_SIZE<span class="token punctuation">,</span>dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
        l_cell_y<span class="token punctuation">,</span>cell_states<span class="token operator">=</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>dynamic_rnn<span class="token punctuation">(</span>cell<span class="token operator">=</span>lstm_model<span class="token punctuation">,</span>inputs<span class="token operator">=</span>l_cell_x<span class="token punctuation">,</span>initial_state<span class="token operator">=</span>init_state<span class="token punctuation">,</span>dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
   
    
<span class="token comment">#------这里可以直接用输出状态叠加一个全连接层得到最终结果，全连接层可以用函数定义或者手写</span>
<span class="token comment">#     dnn_out=dnn_stack(outputs[:,-1,:],layers={'layers':[32,16]})</span>


<span class="token comment">#定义输出隐藏层，这里为手写单层神经层，l_out_x的维度(batch_size,cell_size),l_out_y的维度为(batch_size,ouputsize)</span>
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">'out_hidden'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        l_out_x <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>l_cell_y<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>CELL_SIZE<span class="token punctuation">]</span><span class="token punctuation">)</span>
        ws_out <span class="token operator">=</span> weight_variable<span class="token punctuation">(</span><span class="token punctuation">[</span>CELL_SIZE<span class="token punctuation">,</span>OUTPUT_SIZE<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token string">'ws_out'</span><span class="token punctuation">)</span>
        bs_out <span class="token operator">=</span> bias_variable<span class="token punctuation">(</span><span class="token punctuation">[</span>OUTPUT_SIZE<span class="token punctuation">,</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token string">'bs_out'</span><span class="token punctuation">)</span>
        l_out_y <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>l_out_x<span class="token punctuation">,</span>ws_out<span class="token punctuation">)</span> <span class="token operator">+</span>  bs_out


<span class="token comment">#定义损失函数和优化器，损失函数采用MSE（mean-squared error），优化器有多种选择，基础优化器梯度下降法（GradientDescentOptimizer）</span>
<span class="token comment">#---------------------------------------------------------------------这里采用的是adam算法（AdamOptimizer），优化器一般在tf.train包中</span>
<span class="token comment">#     def ms_error(labels, logits):</span>
<span class="token comment">#         return tf.square(tf.subtract(labels, logits))</span>
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>name_scope<span class="token punctuation">(</span><span class="token string">'loss_funtion'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        mse <span class="token operator">=</span> tf<span class="token punctuation">.</span>losses<span class="token punctuation">.</span>mean_squared_error<span class="token punctuation">(</span>labels <span class="token operator">=</span> y_p<span class="token punctuation">,</span>predictions <span class="token operator">=</span> l_out_y<span class="token punctuation">)</span>
<span class="token comment">#     losses = tf.contrib.legacy_seq2seq.sequence_loss_by_example(logits=[tf.reshape(l_out_y,[-1],name='reshaped_pred')],</span>
<span class="token comment">#                                                              targets=[tf.reshape(y_p,[-1],name='reshaped_target')],</span>
<span class="token comment">#                                                              weights = [tf.ones([BATCH_SIZE*TIME_STEPS],dtype=tf.float32)],</span>
<span class="token comment">#                                                              softmax_loss_function=ms_error)</span>
<span class="token comment">#     mse = tf.div(tf.reduce_sum(losses, name='losses_sum'),BATCH_SIZE,name='average_cost')</span>
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>name_scope<span class="token punctuation">(</span><span class="token string">'train'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>AdamOptimizer<span class="token punctuation">(</span>LEARNING_RATE<span class="token punctuation">)</span><span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>loss <span class="token operator">=</span> mse<span class="token punctuation">)</span>
<span class="token comment">#变量初始化</span>
    init <span class="token operator">=</span> tf<span class="token punctuation">.</span>global_variables_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span>

    
<span class="token comment">#计算图对话-----session   </span>
<span class="token comment"># with tf.Session(graph=graph) as sess:</span>
sess <span class="token operator">=</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span>graph <span class="token operator">=</span> graph<span class="token punctuation">)</span>
sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>init<span class="token punctuation">)</span>
<span class="token comment">#训练过程</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>EPOCH<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>   
    train_losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

    <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>TRAIN_EXAMPLES<span class="token operator">//</span>BATCH_SIZE<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> epoch<span class="token operator">==</span><span class="token number">1</span> <span class="token operator">and</span> j<span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">:</span>            
            feed_dict <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
                x_p<span class="token punctuation">:</span>X_train<span class="token punctuation">[</span>j<span class="token operator">*</span>BATCH_SIZE<span class="token punctuation">:</span><span class="token punctuation">(</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">*</span>BATCH_SIZE<span class="token punctuation">]</span><span class="token punctuation">,</span>
                y_p<span class="token punctuation">:</span>y_train<span class="token punctuation">[</span>j<span class="token operator">*</span>BATCH_SIZE<span class="token punctuation">:</span><span class="token punctuation">(</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">*</span>BATCH_SIZE<span class="token punctuation">]</span><span class="token punctuation">,</span>
            <span class="token punctuation">}</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            feed_dict <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
                x_p<span class="token punctuation">:</span>X_train<span class="token punctuation">[</span>j<span class="token operator">*</span>BATCH_SIZE<span class="token punctuation">:</span><span class="token punctuation">(</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">*</span>BATCH_SIZE<span class="token punctuation">]</span><span class="token punctuation">,</span>
                y_p<span class="token punctuation">:</span>y_train<span class="token punctuation">[</span>j<span class="token operator">*</span>BATCH_SIZE<span class="token punctuation">:</span><span class="token punctuation">(</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">*</span>BATCH_SIZE<span class="token punctuation">]</span><span class="token punctuation">,</span>
                init_state<span class="token punctuation">:</span>final_states
            <span class="token punctuation">}</span>
        _<span class="token punctuation">,</span>train_loss<span class="token punctuation">,</span>final_states <span class="token operator">=</span> sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>
        fetches <span class="token operator">=</span><span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span>mse<span class="token punctuation">,</span>cell_states<span class="token punctuation">)</span><span class="token punctuation">,</span>feed_dict <span class="token operator">=</span> feed_dict<span class="token punctuation">)</span>
        train_losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>train_loss<span class="token punctuation">)</span>
    <span class="token keyword">if</span> epoch<span class="token operator">%</span><span class="token number">10</span> <span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'epoch:'</span><span class="token punctuation">,</span>epoch<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'average training loss:'</span><span class="token punctuation">,</span><span class="token builtin">sum</span><span class="token punctuation">(</span>train_losses<span class="token punctuation">)</span><span class="token operator">/</span><span class="token builtin">len</span><span class="token punctuation">(</span>train_losses<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment">#预测过程</span>
<span class="token keyword">if</span> X_test<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">&gt;=</span>TIME_STEPS<span class="token punctuation">:</span>
    results <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>TEST_EXAMPLES<span class="token punctuation">,</span>OUTPUT_SIZE<span class="token punctuation">)</span><span class="token punctuation">)</span>
    test_losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  
    <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>TEST_EXAMPLES<span class="token operator">//</span>BATCH_SIZE<span class="token punctuation">)</span><span class="token punctuation">:</span>
        result<span class="token punctuation">,</span>test_loss<span class="token punctuation">,</span>wsin<span class="token punctuation">,</span>bsin<span class="token punctuation">,</span>wsout<span class="token punctuation">,</span>bsout<span class="token operator">=</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>
        fetches <span class="token operator">=</span><span class="token punctuation">(</span>l_out_y<span class="token punctuation">,</span>mse<span class="token punctuation">,</span>ws_in<span class="token punctuation">,</span>bs_in<span class="token punctuation">,</span>ws_out<span class="token punctuation">,</span>bs_out<span class="token punctuation">)</span><span class="token punctuation">,</span>feed_dict <span class="token operator">=</span><span class="token punctuation">{<!-- --></span>
                x_p<span class="token punctuation">:</span>X_test<span class="token punctuation">[</span>j<span class="token operator">*</span>BATCH_SIZE<span class="token punctuation">:</span><span class="token punctuation">(</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">*</span>BATCH_SIZE<span class="token punctuation">]</span><span class="token punctuation">,</span>
                y_p<span class="token punctuation">:</span>y_test<span class="token punctuation">[</span>j<span class="token operator">*</span>BATCH_SIZE<span class="token punctuation">:</span><span class="token punctuation">(</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">*</span>BATCH_SIZE<span class="token punctuation">]</span><span class="token punctuation">,</span>
                init_state<span class="token punctuation">:</span>final_states
            <span class="token punctuation">}</span> <span class="token punctuation">)</span>
        results<span class="token punctuation">[</span>j<span class="token operator">*</span>BATCH_SIZE<span class="token punctuation">:</span><span class="token punctuation">(</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">*</span>BATCH_SIZE<span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">=</span>result
        test_losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>test_loss<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'average testing loss:'</span><span class="token punctuation">,</span><span class="token builtin">sum</span><span class="token punctuation">(</span>test_losses<span class="token punctuation">)</span><span class="token operator">/</span><span class="token builtin">len</span><span class="token punctuation">(</span>test_losses<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment">#绘制图形</span>
plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>facecolor <span class="token operator">=</span><span class="token string">'white'</span><span class="token punctuation">,</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">12</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span>j<span class="token operator">*</span>BATCH_SIZE<span class="token punctuation">)</span><span class="token punctuation">,</span>y_test<span class="token punctuation">[</span><span class="token punctuation">:</span>j<span class="token operator">*</span>BATCH_SIZE<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token string">'r-'</span><span class="token punctuation">,</span><span class="token builtin">range</span><span class="token punctuation">(</span>j<span class="token operator">*</span>BATCH_SIZE<span class="token punctuation">)</span><span class="token punctuation">,</span>results<span class="token punctuation">[</span><span class="token punctuation">:</span>j<span class="token operator">*</span>BATCH_SIZE<span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/5bc14367a088f6d3b2c8813e4dacad3a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">常用的Linux shell命令</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d155dbcc28ba81fcf1af51c232d55ef4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">快速处理小米手机(包括MI9和K20等)挂木第后还无法登录谷歌账户的问题和google服务器通信时出现问题的解决方案！</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>