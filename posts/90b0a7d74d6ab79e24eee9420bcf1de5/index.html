<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【scikit-learn】04：sklearn库下进行文本数据分析 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【scikit-learn】04：sklearn库下进行文本数据分析" />
<meta property="og:description" content="【scikit-learn】01：使用案例对sklearn库进行简单介绍【scikit-learn】02：使用sklearn库进行统计学习【scikit-learn】03：将sklearn库用于非监督性学习 聚类 【scikit-learn】04：sklearn库下进行文本数据分析 【scikit-learn】05：sklearn文本分类及评价指标【scikit-learn】06：make_blobs聚类数据生成器【scikit-learn】07：数据加载，数据归一，特征选择，逻辑回归，贝叶斯，k近邻，决策树，SVM # -*-coding:utf-8-*- # ---------------------- # Author：kevinelstri # Datetime:2017.2.21 # ---------------------- # ----------------------- # Working With Text Data 文本数据处理 # http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html # ----------------------- import numpy as np &#34;&#34;&#34; 这个指南的目的是在一个实际任务上探索scikit-learn的主要工具，在二十个不同的主题上分析一个文本集合。 在这一节中，可以看到： 1、加载文本文件和类别 2、适合机器学习的特征向量提取 3、训练线性模型进行分类 4、使用网格搜索策略，找到一个很好的配置的特征提取组件和分类器 &#34;&#34;&#34; &#34;&#34;&#34; 1、Loading the 20 newsgroups dataset 加载20个新闻组数据集 为了获得更快的执行时间为第一个例子，我们将工作在部分数据集只有4个类别的数据集中： &#34;&#34;&#34; categories = [&#39;alt.atheism&#39;, &#39;soc.religion.christian&#39;, &#39;comp.graphics&#39;, &#39;sci.med&#39;] from sklearn.datasets import fetch_20newsgroups twenty_train = fetch_20newsgroups(subset=&#39;train&#39;, categories=categories, shuffle=True, random_state=42) print twenty_train.target print twenty_train.target_names # 训练集中类别的名字，这里只有四个类别 print len(twenty_train." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/90b0a7d74d6ab79e24eee9420bcf1de5/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2017-02-21T15:15:48+08:00" />
<meta property="article:modified_time" content="2017-02-21T15:15:48+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【scikit-learn】04：sklearn库下进行文本数据分析</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <ul><li><a href="http://blog.csdn.net/kevinelstri/article/details/55270915">【scikit-learn】01：使用案例对sklearn库进行简单介绍</a></li><li><a href="http://blog.csdn.net/kevinelstri/article/details/55520591">【scikit-learn】02：使用sklearn库进行统计学习</a></li><li><a href="http://blog.csdn.net/kevinelstri/article/details/55532403">【scikit-learn】03：将sklearn库用于非监督性学习 聚类</a> </li><li><a href="http://blog.csdn.net/kevinelstri/article/details/56283777">【scikit-learn】04：sklearn库下进行文本数据分析</a> </li><li><a href="http://blog.csdn.net/kevinelstri/article/details/52638091">【scikit-learn】05：sklearn文本分类及评价指标</a></li><li><a href="http://blog.csdn.net/kevinelstri/article/details/52622960">【scikit-learn】06：make_blobs聚类数据生成器</a></li><li><a href="http://blog.csdn.net/kevinelstri/article/details/52636661">【scikit-learn】07：数据加载，数据归一，特征选择，逻辑回归，贝叶斯，k近邻，决策树，SVM</a></li></ul> 
<hr> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-comment"># -*-coding:utf-8-*-</span>

<span class="hljs-comment"># ----------------------</span>
<span class="hljs-comment">#   Author：kevinelstri</span>
<span class="hljs-comment">#   Datetime:2017.2.21</span>
<span class="hljs-comment"># ----------------------</span>

<span class="hljs-comment"># -----------------------</span>
<span class="hljs-comment">#   Working With Text Data  文本数据处理</span>
<span class="hljs-comment">#   http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html</span>
<span class="hljs-comment"># -----------------------</span>

<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-string">"""
    这个指南的目的是在一个实际任务上探索scikit-learn的主要工具，在二十个不同的主题上分析一个文本集合。
    在这一节中，可以看到：
        1、加载文本文件和类别
        2、适合机器学习的特征向量提取
        3、训练线性模型进行分类
        4、使用网格搜索策略，找到一个很好的配置的特征提取组件和分类器
"""</span>

<span class="hljs-string">"""
    1、Loading the 20 newsgroups dataset 加载20个新闻组数据集
    为了获得更快的执行时间为第一个例子，我们将工作在部分数据集只有4个类别的数据集中：
"""</span>
categories = [<span class="hljs-string">'alt.atheism'</span>, <span class="hljs-string">'soc.religion.christian'</span>, <span class="hljs-string">'comp.graphics'</span>, <span class="hljs-string">'sci.med'</span>]
<span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> fetch_20newsgroups

twenty_train = fetch_20newsgroups(subset=<span class="hljs-string">'train'</span>, categories=categories, shuffle=<span class="hljs-keyword">True</span>, random_state=<span class="hljs-number">42</span>)
<span class="hljs-keyword">print</span> twenty_train.target
<span class="hljs-keyword">print</span> twenty_train.target_names  <span class="hljs-comment"># 训练集中类别的名字，这里只有四个类别</span>
<span class="hljs-keyword">print</span> len(twenty_train.data)  <span class="hljs-comment"># 训练集中数据的长度</span>
<span class="hljs-keyword">print</span> len(twenty_train.filenames)  <span class="hljs-comment"># 训练集文件名长度</span>
<span class="hljs-keyword">print</span> <span class="hljs-string">'-----'</span>
<span class="hljs-keyword">print</span> <span class="hljs-string">"\n"</span>.join(twenty_train.data[<span class="hljs-number">0</span>].split(<span class="hljs-string">"\n"</span>)[:<span class="hljs-number">3</span>])
<span class="hljs-keyword">print</span> <span class="hljs-string">'-----'</span>
<span class="hljs-keyword">print</span> twenty_train.target_names[twenty_train.target[<span class="hljs-number">0</span>]]
<span class="hljs-keyword">print</span> <span class="hljs-string">'-----'</span>
<span class="hljs-keyword">print</span> twenty_train.target[:<span class="hljs-number">10</span>]  <span class="hljs-comment"># 前十个的类别</span>
<span class="hljs-keyword">print</span> <span class="hljs-string">'-----'</span>
<span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> twenty_train.target[:<span class="hljs-number">10</span>]:
    <span class="hljs-keyword">print</span> twenty_train.target_names[t]  <span class="hljs-comment"># 类别的名字</span>
<span class="hljs-keyword">print</span> <span class="hljs-string">'-----'</span>
<span class="hljs-string">"""
    2、Extracting features from text files 从文本文件中提取特征
    为了在文本文件中使用机器学习算法，首先需要将文本内容转换为数值特征向量
"""</span>

<span class="hljs-string">"""
    Bags of words 词袋
    最直接的方式就是词袋表示法
        1、为训练集的任何文档中的每个单词分配一个固定的整数ID（例如通过从字典到整型索引建立字典）
        2、对于每个文档，计算每个词出现的次数，并存储到X[i,j]中。

    词袋表示：n_features 是语料中不同单词的数量，这个数量通常大于100000.
    如果 n_samples == 10000，存储X的数组就需要10000*10000*4byte=4GB,这么大的存储在今天的计算机上是不可能实现的。
    幸运的是，X中的大多数值都是0，基于这种原因，我们说词袋是典型的高维稀疏数据集，我们可以只存储那些非0的特征向量。
    scipy.sparse 矩阵就是这种数据结构，而scikit-learn内置了这种数据结构。
"""</span>

<span class="hljs-string">"""
    Tokenizing text with scikit-learn 使用scikit-learn标记文本
    文本处理、分词、过滤停用词都在这些高级组件中，能够建立特征字典并将文档转换成特征向量。
"""</span>
<span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> CountVectorizer  <span class="hljs-comment"># sklearn中的文本特征提取组件中，导入特征向量计数函数</span>

count_vect = CountVectorizer()  <span class="hljs-comment"># 特征向量计数函数</span>
X_train_counts = count_vect.fit_transform(twenty_train.data)  <span class="hljs-comment"># 对文本进行特征向量处理</span>
<span class="hljs-keyword">print</span> X_train_counts  <span class="hljs-comment"># 特征向量和特征标签</span>
<span class="hljs-keyword">print</span> X_train_counts.shape  <span class="hljs-comment"># 形状</span>
<span class="hljs-keyword">print</span> <span class="hljs-string">'-----'</span>

<span class="hljs-string">"""
    CountVectorizer支持计算单词或序列的N-grams，一旦合适，这个向量化就可以建立特征词典。
    在整个训练预料中，词汇中的词汇索引值与其频率有关。
"""</span>
<span class="hljs-keyword">print</span> count_vect.vocabulary_.get(<span class="hljs-string">u'algorithm'</span>)
<span class="hljs-keyword">print</span> <span class="hljs-string">'-----'</span>

<span class="hljs-string">"""
    From occurrences to frequencies 从事件到频率
    计数是一个好的开始，但是也存在一个问题：较长的文本将会比较短的文本有很高的平均计数值，即使他们所表示的话题是一样的。
    为了避免潜在的差异，它可以将文档中的每个单词出现的次数在文档的总字数的比例：这个新的特征叫做词频：tf
    tf-idf:词频-逆文档频率
"""</span>
<span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> TfidfTransformer  <span class="hljs-comment"># sklearn中的文本特征提取组件中，导入词频统计函数</span>

tf_transformer = TfidfTransformer(use_idf=<span class="hljs-keyword">False</span>).fit(X_train_counts)  <span class="hljs-comment"># 建立词频统计函数,注意这里idf=False</span>
<span class="hljs-keyword">print</span> tf_transformer  <span class="hljs-comment"># 输出函数属性 TfidfTransformer(norm=u'l2', smooth_idf=True, sublinear_tf=False, use_idf=False)</span>
<span class="hljs-keyword">print</span> <span class="hljs-string">'-----'</span>
X_train_tf = tf_transformer.transform(X_train_counts)  <span class="hljs-comment"># 使用函数对文本文档进行tf-idf频率计算</span>
<span class="hljs-keyword">print</span> X_train_tf
<span class="hljs-keyword">print</span> <span class="hljs-string">'-----'</span>
<span class="hljs-keyword">print</span> X_train_tf.shape
<span class="hljs-keyword">print</span> <span class="hljs-string">'-----'</span>
<span class="hljs-string">"""
    在上面的例子中，使用fit()方法来构建基于数据的预测器，然后使用transform()方法来将计数矩阵用tf-idf表示。
    这两个步骤可以通过跳过冗余处理，来更快的达到相同的最终结果。
    这些可以通过使用fit_transform()方法来实现：
"""</span>
tfidf_transformer = TfidfTransformer()  <span class="hljs-comment"># 这里使用的是tf-idf</span>
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
<span class="hljs-keyword">print</span> X_train_tfidf
<span class="hljs-keyword">print</span> X_train_tfidf.shape
<span class="hljs-keyword">print</span> <span class="hljs-string">'-----'</span>
<span class="hljs-string">"""
    Training a classifier 训练一个分类器
    既然已经有了特征，就可以训练分类器来试图预测一个帖子的类别，先使用贝叶斯分类器，贝叶斯分类器提供了一个良好的基线来完成这个任务。
    scikit-learn中包括这个分类器的许多变量，最适合进行单词计数的是多项式变量。
"""</span>
<span class="hljs-keyword">from</span> sklearn.naive_bayes <span class="hljs-keyword">import</span> MultinomialNB  <span class="hljs-comment"># 使用sklearn中的贝叶斯分类器，并且加载贝叶斯分类器</span>

<span class="hljs-comment"># 中的MultinomialNB多项式函数</span>
clf = MultinomialNB()  <span class="hljs-comment"># 加载多项式函数</span>
x_clf = clf.fit(X_train_tfidf, twenty_train.target)  <span class="hljs-comment"># 构造基于数据的分类器</span>
<span class="hljs-keyword">print</span> x_clf  <span class="hljs-comment"># 分类器属性：MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</span>
<span class="hljs-keyword">print</span> <span class="hljs-string">'-----'</span>
<span class="hljs-string">"""
    为了预测输入的新的文档，我们需要使用与前面相同的特征提取链进行提取特征。
    不同的是，在转换中，使用transform来代替fit_transform，因为训练集已经构造了分类器
"""</span>
docs_new = [<span class="hljs-string">'God is love'</span>, <span class="hljs-string">'OpenGL on the GPU is fast'</span>]  <span class="hljs-comment"># 文档</span>
X_new_counts = count_vect.transform(docs_new)  <span class="hljs-comment"># 构建文档计数</span>
X_new_tfidf = tfidf_transformer.transform(X_new_counts)  <span class="hljs-comment"># 构建文档tfidf</span>
predicted = clf.predict(X_new_tfidf)  <span class="hljs-comment"># 预测文档</span>
<span class="hljs-keyword">print</span> predicted  <span class="hljs-comment"># 预测类别 [3 1]，一个属于3类，一个属于1类</span>
<span class="hljs-keyword">for</span> doc, category <span class="hljs-keyword">in</span> zip(docs_new, predicted):
    <span class="hljs-keyword">print</span> <span class="hljs-string">'%r =&gt; %s'</span> % (doc, twenty_train.target_names[category])  <span class="hljs-comment"># 将文档和类别名字对应起来</span>
<span class="hljs-keyword">print</span> <span class="hljs-string">'-----'</span>
<span class="hljs-string">"""
    Building a pipeline 建立管道
    为了使向量转换更加简单(vectorizer =&gt; transformer =&gt; classifier)，scikit-learn提供了pipeline类来表示为一个复合分类器
"""</span>
<span class="hljs-keyword">from</span> sklearn.pipeline <span class="hljs-keyword">import</span> Pipeline

text_clf = Pipeline([(<span class="hljs-string">'vect'</span>, CountVectorizer()), (<span class="hljs-string">'tfidf'</span>, TfidfTransformer()), (<span class="hljs-string">'clf'</span>, MultinomialNB())])
text_clf = text_clf.fit(twenty_train.data, twenty_train.target)
<span class="hljs-keyword">print</span> text_clf  <span class="hljs-comment"># 构造分类器，分类器的属性</span>
predicted = text_clf.predict(docs_new)  <span class="hljs-comment"># 预测新文档</span>
<span class="hljs-keyword">print</span> predicted  <span class="hljs-comment"># 获取预测值</span>
<span class="hljs-keyword">print</span> <span class="hljs-string">'-----'</span>

<span class="hljs-string">"""
    分析总结：
        1、加载数据集，主要是加载训练集，用于对数据进行训练
        2、文本特征提取：
                对文本进行计数统计 CountVectorizer
                词频统计  TfidfTransformer  （先计算tf,再计算tfidf）
        3、训练分类器：
                贝叶斯多项式训练器 MultinomialNB
        4、预测文档：
                通过构造的训练器进行构造分类器，来进行文档的预测
        5、最简单的方式：
                通过使用pipeline管道形式，来讲上述所有功能通过管道来一步实现，更加简单的就可以进行预测
"""</span>

<span class="hljs-string">"""
    Evaluation of the performance on the test set 测试集性能评价
    评估模型的预测精度同样容易：
"""</span>
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

twenty_test = fetch_20newsgroups(subset=<span class="hljs-string">'test'</span>, categories=categories, shuffle=<span class="hljs-keyword">True</span>, random_state=<span class="hljs-number">42</span>)
docs_test = twenty_test.data
predicted = text_clf.predict(docs_test)
<span class="hljs-keyword">print</span> np.mean(predicted == twenty_test.target)  <span class="hljs-comment"># 预测的值和测试值的比例，mean就是比例函数</span>
<span class="hljs-keyword">print</span> <span class="hljs-string">'-----'</span>  <span class="hljs-comment"># 精度已经为0.834886817577</span>

<span class="hljs-string">"""
    精度已经实现了83.4%，那么使用支持向量机(SVM)是否能够做的更好呢，支持向量机(SVM)被广泛认为是最好的文本分类算法之一。
    尽管，SVM经常比贝叶斯要慢一些。
    我们可以改变学习方式，使用管道来实现分类：
"""</span>
<span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> SGDClassifier

text_clf = Pipeline(
    [(<span class="hljs-string">'vect'</span>, CountVectorizer()), (<span class="hljs-string">'tfidf'</span>, TfidfTransformer()),
     (<span class="hljs-string">'clf'</span>, SGDClassifier(loss=<span class="hljs-string">'hinge'</span>, penalty=<span class="hljs-string">'l2'</span>, alpha=<span class="hljs-number">1e-3</span>, n_iter=<span class="hljs-number">5</span>, random_state=<span class="hljs-number">42</span>))])
<span class="hljs-comment"># _ = text_clf.fit(twenty_train.data, twenty_train.target)  # 和下面一句的意思一样，一个杠，表示本身</span>
text_clf = text_clf.fit(twenty_train.data, twenty_train.target)
predicted = text_clf.predict(docs_test)
<span class="hljs-keyword">print</span> np.mean(predicted == twenty_test.target)  <span class="hljs-comment"># 精度 0.912782956059</span>
<span class="hljs-keyword">print</span> <span class="hljs-string">'-----'</span>
<span class="hljs-string">"""
    sklearn进一步提供了结果的更详细的性能分析工具：
"""</span>
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics
<span class="hljs-keyword">print</span> metrics.classification_report(twenty_test.target, predicted, target_names=twenty_test.target_names)
<span class="hljs-keyword">print</span> metrics.confusion_matrix(twenty_test.target, predicted)
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/090b26bb2f2359124b62e33d475ef724/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">css之背景图片和插入图片的区别以及精灵图的使用</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/8688e3094af2d38f6e10626d34967eb6/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">语法基础——Objective-C语法基础</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>