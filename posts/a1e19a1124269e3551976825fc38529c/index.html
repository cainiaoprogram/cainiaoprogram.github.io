<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>LOL-v2数据集和VE-LOL数据集的区别 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="LOL-v2数据集和VE-LOL数据集的区别" />
<meta property="og:description" content="LOL-v2数据集和VE-LOL数据集的区别 LOL-v2 LOL-v2数据集[64]包括两个不同的子集，即LOL-v2-real和LOL-v2-synthetic。LOL-v2-real子集是通过改变ISO和曝光时间在真实场景中捕获的，包括689对用于训练和测试的图像。在LOL-v2-synthetic子集中，通过分析低光照图像的照明分布，从RAW图像合成低光照图像。该子集包含1,000对低光照/正常图像，选择了900对用于训练和100对用于测试。
VE-LOL VE-LOL [32]是一个用于低光照图像增强的大规模数据集，提供了2,500对低光照/正常光照的图像，场景更加多样化。与LOL不同，VE-LOL还在RAW图像级别考虑了噪声建模，并且使用了四种不同的相机捕获了噪声：索尼A7R、奥林巴斯E-M10、索尼RX100 IV和华为Nexus 6P。
为了公平比较，我们对这些方法使用已发布的代码，无需任何修改。由于 Zero-DCE(Guo et al., 2020) 和 EnlightenGAN(Jiang et al., 2021) 使用未配对的数据进行训练，我们使用他们发布的预训练模型进行比较。LOL 数据集 (Wei et al., 2018) 通过改变相机的曝光时间和 ISO 来捕获 500 对真实的低/正常光图像，其中包括 485 个训练图像和 15 个测试图像。VE-LOL(Liu et al., 2021) 包含两个子集：配对 VE-LOL-L 用于训练和评估LLIE 方法，并使用非配对 VE-LOLH 来评估LLIE 方法对人脸检测的影响。在这里，我们使用 VELOL-L。VE-LOL-L 中有 500 个真实场景图像，其中 400 个用于训练，100 个用于测试。
参考文献
Under review as a conference paper at ICLR 2023 RETINEXUTV: ROBUST RETINEX MODEL WITH UNFOLDING TOTAL VARIATION
LLDiffusion: Learning Degradation Representations in Diffusion Models for Low-Light Image Enhancement" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/a1e19a1124269e3551976825fc38529c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-08-05T21:22:47+08:00" />
<meta property="article:modified_time" content="2023-08-05T21:22:47+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">LOL-v2数据集和VE-LOL数据集的区别</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="LOLv2VELOL_0"></a>LOL-v2数据集和VE-LOL数据集的区别</h2> 
<h3><a id="LOLv2_2"></a>LOL-v2</h3> 
<p>LOL-v2数据集[64]包括两个不同的子集，即LOL-v2-real和LOL-v2-synthetic。LOL-v2-real子集是通过改变ISO和曝光时间在真实场景中捕获的，包括689对用于训练和测试的图像。在LOL-v2-synthetic子集中，通过分析低光照图像的照明分布，从RAW图像合成低光照图像。该子集包含1,000对低光照/正常图像，选择了900对用于训练和100对用于测试。</p> 
<h3><a id="VELOL_7"></a>VE-LOL</h3> 
<p>VE-LOL [32]是一个用于低光照图像增强的大规模数据集，提供了2,500对低光照/正常光照的图像，场景更加多样化。与LOL不同，VE-LOL还在RAW图像级别考虑了噪声建模，并且使用了四种不同的相机捕获了噪声：索尼A7R、奥林巴斯E-M10、索尼RX100 IV和华为Nexus 6P。</p> 
<p>为了公平比较，我们对这些方法使用已发布的代码，无需任何修改。由于 Zero-DCE(Guo et al., 2020) 和 EnlightenGAN(Jiang et al., 2021) 使用未配对的数据进行训练，我们使用他们发布的预训练模型进行比较。LOL 数据集 (Wei et al., 2018) 通过改变相机的曝光时间和 ISO 来捕获 500 对真实的低/正常光图像，其中包括 485 个训练图像和 15 个测试图像。VE-LOL(Liu et al., 2021) 包含两个子集：配对 VE-LOL-L 用于训练和评估LLIE 方法，并使用非配对 VE-LOLH 来评估LLIE 方法对人脸检测的影响。在这里，我们使用 VELOL-L。VE-LOL-L 中有 500 个真实场景图像，其中 400 个用于训练，100 个用于测试。</p> 
<p>参考文献<br> Under review as a conference paper at ICLR 2023 RETINEXUTV: ROBUST RETINEX MODEL WITH UNFOLDING TOTAL VARIATION</p> 
<p>LLDiffusion: Learning Degradation Representations in Diffusion Models for Low-Light Image Enhancement</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6114f40d2aadd1c500cae1833bc2290e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Vue 项目 实现阻止浏览器记住密码功能</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f5ffe3c7ea8c2c6cda945fdc66d1a26a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">什么是Linux，如何在Windows操作系统下搭建Linux环境，远程连接Linux系统</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>