<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>PyTorch|构建自己的卷积神经网络 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="PyTorch|构建自己的卷积神经网络" />
<meta property="og:description" content="如何搭建网络，这在深度学习中非常重要。简单来讲，我们是要实现一个类，这个类中有属性和方法，能够进行计算。
一般来讲，使用PyTorch创建神经网络需要三步：
继承基类：nn.Module
定义层属性
实现前向传播方法
如果你对于python面向对象编程非常熟练，那么这里也就非常简单，就是定义一些属性，实现一些方法。
开始建立一个网络，就像这样：
import torchimport torch.nn as nnimport torch.nn.functional as Fclass Network(nn.Module): def __init__(self): super(Network, self).__init__() self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5) self.fc1 = nn.Linear(in_features=12*4*4, out_features=120) self.fc2 = nn.Linear(in_features=120,out_features=60) self.out = nn.Linear(in_features=60, out_features=10) def forward(self,t): # (1) input layer t = t # (2) hidden conv layer1 t = self.conv1(t) t = F.relu(t) t = F.max_pool2d(t, kernel_size=2, stride=2) # (3) hidden conv layer2 t = self." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/d2f1ab139d877582554fa5339277ca13/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-06T17:04:59+08:00" />
<meta property="article:modified_time" content="2024-01-06T17:04:59+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">PyTorch|构建自己的卷积神经网络</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p></p> 
<p class="img-center"><img alt="图片" height="377" src="https://images2.imgbox.com/e5/6d/4mOA8Uww_o.jpg" width="941"></p> 
<p>如何搭建网络，这在深度学习中非常重要。<strong>简单来讲，我们是要实现一个类，这个类中有属性和方法，能够进行计算</strong>。</p> 
<p>一般来讲，使用PyTorch创建神经网络需要三步：</p> 
<ol><li> <p>继承基类：nn.Module</p> </li><li> <p>定义层属性</p> </li><li> <p>实现前向传播方法</p> <p></p> </li></ol> 
<p></p> 
<p>如果你对于python面向对象编程非常熟练，那么这里也就非常简单，就是定义一些属性，实现一些方法。</p> 
<p>开始建立一个网络，就像这样：</p> 
<pre><code>import torch</code><code>import torch.nn as nn</code><code>import torch.nn.functional as F</code><code>class Network(nn.Module):</code><code>    def __init__(self):</code><code>        super(Network, self).__init__()</code><code>        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)</code><code>        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)</code><code>        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)</code><code>        self.fc2 = nn.Linear(in_features=120,out_features=60)</code><code>        self.out = nn.Linear(in_features=60, out_features=10)</code><code>    def forward(self,t):</code><code>        # (1) input layer</code><code>        t = t</code><code>        # (2) hidden conv layer1</code><code>        t = self.conv1(t)</code><code>        t = F.relu(t)</code><code>        t = F.max_pool2d(t, kernel_size=2, stride=2)</code><code>        # (3) hidden conv layer2</code><code>        t = self.conv2(t)</code><code>        t = F.relu(t)</code><code>        t = F.max_pool2d(t, kernel_size=2, stride=2)</code><code>        # relu 和 max pooling 都没有权重；激活层和池化层的本质都是对传入的数据按照一定的算法变换。</code><code>        #（4）hidden linear layer2</code><code>        t = t.reshape(-1, 12*4*4)</code><code>        t = self.fc1(t)</code><code>        t = F.relu(t)</code><code>        # (5) hidden linear layer2</code><code>        t = self.fc2(t)</code><code>        t = F.relu(t)</code><code>        # (6) output layer</code><code>        t = self.out(t)</code></pre> 
<p>对于上述代码，就是定义了一个新的类，叫做Network,这个类继承自nn.Module,同时，我们又新定义了一些属性，比如conv1,conv2,fc1,fc2,out，并实现了一个方法，叫做：forawrd。</p> 
<p><strong>好吧，接下来我们进行初始化</strong></p> 
<pre><code>&gt;&gt;&gt;network=Network()</code></pre> 
<p><strong>访问对象network的一些属性</strong></p> 
<pre><code>&gt;&gt;&gt; network.conv1</code><code>Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))</code><code>&gt;&gt;&gt; network.conv2</code><code>Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))</code><code>&gt;&gt;&gt; network.out</code><code>Linear(in_features=60, out_features=10, bias=True)</code></pre> 
<p>更进一步，我们也可获得每一层的<strong>权重，形状</strong>：</p> 
<p>​​​​​​​</p> 
<pre><code>&gt;&gt;&gt; network.conv2.weight</code><code>Parameter containing:</code><code>tensor([[[[-8.0741e-02, -7.1281e-02,  7.6540e-02,  6.2786e-02,  1.1018e-03],</code><code>          [ 6.5041e-02, -3.5665e-02,  7.8475e-02, -1.1228e-02, -2.9447e-02],</code><code>          [-8.0508e-02,  7.0457e-02,  7.7877e-02,  7.2872e-02,  4.5671e-02],</code><code>          [ 3.9757e-03,  7.7676e-02,  3.3951e-02,  6.3745e-02,  7.0577e-02],</code><code>          [-2.0165e-02,  2.2356e-02,  2.9137e-02,  8.0388e-02,  5.9048e-02]]......</code><code>......</code><code>&gt;&gt;&gt; network.conv2.weight.shape</code><code>torch.Size([12, 6, 5, 5])</code></pre> 
<p><strong>好</strong>吧，上述操作比较繁琐，我们不想这样做，但我们还是非常想<strong>了解一个神经网络</strong>，那么应该怎么办呢？其实，可以这样。</p> 
<p>好吧，接下来我们进行初始化​​​​​​​</p> 
<pre><code>&gt;&gt;&gt;network=Network()</code><code>&gt;&gt;&gt; network</code><code>Network(</code><code>  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))</code><code>  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))</code><code>  (fc1): Linear(in_features=192, out_features=120, bias=True)</code><code>  (fc2): Linear(in_features=120, out_features=60, bias=True)</code><code>  (out): Linear(in_features=60, out_features=10, bias=True)</code><code>)</code>
<code>&gt;&gt;&gt; for name, param in network.named_parameters():</code><code>    print(name,'\t\t', param.shape)    </code><code>conv1.weight      torch.Size([6, 1, 5, 5])</code><code>conv1.bias      torch.Size([6])</code><code>conv2.weight      torch.Size([12, 6, 5, 5])</code><code>conv2.bias      torch.Size([12])</code><code>fc1.weight      torch.Size([120, 192])</code><code>fc1.bias      torch.Size([120])</code><code>fc2.weight      torch.Size([60, 120])</code><code>fc2.bias      torch.Size([60])</code><code>out.weight      torch.Size([10, 60])</code><code>out.bias      torch.Size([10])</code></pre> 
<p></p> 
<p>我们实现的新的类是基于<strong>基类nn.Module</strong>实现的，nn.Linear(in_features=120, out_features=60)是一个线性层，是PyTorch已经实现好的。主要功能就是我们输入一个数据，这个层对数据进行一个线性变换，最后输出一个数据。</p> 
<p>还记得之前我们获得了神经网络各层的权重尺寸：</p> 
<pre><code>&gt;&gt;&gt; network.fc2.weight.shape</code><code>torch.Size([60, 120])</code></pre> 
<p>没错，是一个60x120的矩阵。</p> 
<p><strong>一个1x120的数据，进入线性层，经过线性变换，最后变成形状为1x60的矩阵</strong>。当然，这里解释不是非常正确，Linear层复杂得多，这里仅仅是为了便于理解。</p> 
<p>如果你还不理解，下面这个例子可能更加简单：​​​​​​​</p> 
<pre><code># 1. 张量的乘法</code><code>in_features = torch.tensor([1,2,3,4], dtype=torch.float32)</code><code>weight_matrix = torch.tensor([</code><code>    [1,2,3,4],</code><code>    [2,3,4,5],</code><code>    [3,4,5,6]</code><code>], dtype = torch.float32)</code><code>result1=weight_matrix.matmul(in_features)</code><code># 可将上述的权重矩阵看作是一个线性映射.</code>
<code># 在parameter类中包装一个权重矩阵，以使得输出结果与1中一样</code><code>fc = nn.Linear(in_features=4, out_features=3)</code><code>fc.weight= nn.Parameter(weight_matrix)</code><code>result2=fc(in_features)</code><code># 此时的结果接近1中的结果却不精确，是因为由bias的存在</code>
<code>&gt;&gt;&gt; print(result1)</code><code>tensor([30., 40., 50.])</code><code>&gt;&gt;&gt; print(result2)</code><code>tensor([29.5786, 39.9564, 49.7925], grad_fn=&lt;AddBackward0&gt;)</code></pre> 
<p>上文实现的网络除了线性层（全连接层），还有<strong>卷积层，池化，激活函数</strong>，等等，这些内容是卷积神经网络的核心。当然，上述各层都有许多参数，如何确定每一层的参数需要一定的计算。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="449" src="https://images2.imgbox.com/1c/af/tSP1jxU1_o.gif" width="395"></p> 
<p></p> 
<p>到了这里，我们对于如何构建神经网络，以及访问神经网络的一些属性，以及线性层大致做了什么有了一个大致的理解。</p> 
<p></p> 
<p></p> 
<p></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/bd548ad6414142b47c7b13c8f59a2239/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【数据分享】2023年我国省市县三级的商务住宅数量（4类设施/Excel/Shp格式）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/cf3538a45d2339eb999293386fcdc91c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Vue组件封装</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>