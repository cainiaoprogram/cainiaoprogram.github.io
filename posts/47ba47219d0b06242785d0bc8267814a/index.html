<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>python自然语言处理实战核心技术与算法——HMM模型代码详解 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="python自然语言处理实战核心技术与算法——HMM模型代码详解" />
<meta property="og:description" content="本人初学NLP，当我看着《python自然语言处理实战核心技术与算法》书上这接近200行的代码看着有点头皮发麻，于是我读了接近一天基本把每行代码的含义给读的个七七八八，考虑到可能会有人和我一样有点迷茫，所以写下这篇文章与大家分享。
目录 一、HMM模型与Viterbi算法1. HMM模型2. Viterbi算法 二、代码讲解1. __ init __(self):2. try_load_model(self, trained):3. train(self, path):3.1 init_parameters()：3.2 make_label(text)：3.3 其余代码 4. viterbi(self, text, states, start_p, trans_p, emit_p):5.cut(self, text): 三、代码与效果展示1. 代码2. 效果演示 四、参考 一、HMM模型与Viterbi算法 1. HMM模型 HMM模型的核心是：从可观察的参数中确定该过程的隐含参数。
在本例子中，可观察的参数是句子或者说每个字，隐含参数是每个字的标签。
这里还要提到韩梅梅模型的两个假设：
观测独立性假设：每个字的输出仅仅与当前字有关；即：
当然，从概率论的公式也可以推导出，这些事件都是相互独立的，也反推了“观测独立性”。
齐次马尔科夫假设：每个输出仅仅与上一个输出有关。即：
关于齐次马尔科夫假设为什么只与上一个输出有关，是因为语言模型中有一个名叫n元模型的存在，这里推荐一篇文章讲解这个模型：NLP(二)：n元模型
注：o：B、M、E、S这四种标签；λ：句子中的每个字（包括标点等非中文字符）。
每个字的标签作者分为了4个，即B：词首；M：词中；E：词尾；S：单独成词。我不知道会不会有人和我之前一样混淆了词语与句子的关系，但是我这里还是要说明一下，帮大家排下坑。这里的词首 ！= 句首，虽然肯定句首也是个词，无论是词首还是单独成词，但是不能一概而论，因为一句话里面肯定只有一个句首，但是会有无数个词语，这个在后面讲代码的时候会再讲到，之后再详谈。
HMM模型中有3个概率：
初始概率：自然语言序列中第一个字λ1的标记是ok的概率，即π = P(λ1 = ok)发射概率：即输出概率，就是隐含状态输出可见状态的概率，P(λk|ok)转移概率：由前一个隐含状态转移到另一个隐含状态的概率，P(ok|ok-1) 由于本文核心不是讨论该模型的概念，所以我这里放上一个我刚看韩梅梅模型的时候看到的一篇博客，说实话，有点让人高潮，链接如下：一文搞懂HMM（隐马尔可夫模型）
2. Viterbi算法 这是一种动态规划的算法，其核心思想是：如果最终的最优路径经过某个oi，那么从初始节点到oi-1点的路径必然也是一个最优路径，因为每一个节点oi只会影响前后两个P(oi-1|oi)和P(oi|oi&#43;1)。
关于维特比算法，给大家推荐这篇文章，讲的十分浅显易懂，一分钟解决维特比算法概念：如何通俗地讲解 viterbi 算法？
二、代码讲解 回归正题，由于代码好几百行，为了保证大家的阅读质量与逻辑梳理，我将这个HMM类分开为方法，依据我认为实际编程时搭建的步骤来进行讲解。先看一下代码框架：
1. __ init __(self): 大家都知道，__init__方法都是用来初始化的，我在注释上写明了每个参数的说明，这里就贴代码，不多赘述了。
# 提取文件hmm_model.pkl # 主要用于存取算法中间结果，不用每次都训练模型 self.model_file = &#39;./data/hmm_model.pkl&#39; # 状态值集合 self." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/47ba47219d0b06242785d0bc8267814a/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-07-10T15:40:08+08:00" />
<meta property="article:modified_time" content="2020-07-10T15:40:08+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">python自然语言处理实战核心技术与算法——HMM模型代码详解</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <blockquote> 
 <p>本人初学NLP，当我看着《python自然语言处理实战核心技术与算法》书上这接近200行的代码看着有点头皮发麻，于是我读了接近一天基本把每行代码的含义给读的个七七八八，考虑到可能会有人和我一样有点迷茫，所以写下这篇文章与大家分享。</p> 
</blockquote> 
<p></p> 
<div class="toc"> 
 <h4>目录</h4> 
 <ul><li><a href="#HMMViterbi_3" rel="nofollow">一、HMM模型与Viterbi算法</a></li><li><ul><li><a href="#1_HMM_4" rel="nofollow">1. HMM模型</a></li><li><a href="#2_Viterbi_29" rel="nofollow">2. Viterbi算法</a></li></ul> 
  </li><li><a href="#_34" rel="nofollow">二、代码讲解</a></li><li><ul><li><a href="#1____init___self_37" rel="nofollow">1. __ init __(self):</a></li><li><a href="#2_try_load_modelself_trained_51" rel="nofollow">2. try_load_model(self, trained):</a></li><li><a href="#3_trainself_path_80" rel="nofollow">3. train(self, path):</a></li><li><ul><li><a href="#31_init_parameters_87" rel="nofollow">3.1 init_parameters()：</a></li><li><a href="#32_make_labeltext_116" rel="nofollow">3.2 make_label(text)：</a></li><li><a href="#33__147" rel="nofollow">3.3 其余代码</a></li></ul> 
   </li><li><a href="#4_viterbiself_text_states_start_p_trans_p_emit_p_260" rel="nofollow">4. viterbi(self, text, states, start_p, trans_p, emit_p):</a></li><li><a href="#5cutself_text_329" rel="nofollow">5.cut(self, text):</a></li></ul> 
  </li><li><a href="#_379" rel="nofollow">三、代码与效果展示</a></li><li><ul><li><a href="#1__380" rel="nofollow">1. 代码</a></li><li><a href="#2__382" rel="nofollow">2. 效果演示</a></li></ul> 
  </li><li><a href="#_392" rel="nofollow">四、参考</a></li></ul> 
</div> 
<p></p> 
<h2><a id="HMMViterbi_3"></a>一、HMM模型与Viterbi算法</h2> 
<h3><a id="1_HMM_4"></a>1. HMM模型</h3> 
<p>HMM模型的核心是：<strong>从可观察的参数中确定该过程的隐含参数。</strong></p> 
<p>在本例子中，可观察的参数是<strong>句子或者说每个字</strong>，隐含参数是<strong>每个字的标签</strong>。</p> 
<p>这里还要提到韩梅梅模型的两个假设：</p> 
<ol><li> <p>观测独立性假设：每个字的输出仅仅与当前字有关；即：<br> <img src="https://images2.imgbox.com/18/95/nENlqPEV_o.png" alt="观测独立性假设"><br> 当然，从概率论的公式也可以推导出，这些事件都是相互独立的，也反推了“观测独立性”。</p> </li><li> <p>齐次马尔科夫假设：每个输出仅仅与上一个输出有关。即：<br> <img src="https://images2.imgbox.com/8a/37/snxLNNUG_o.png" alt="齐次马尔科夫假设"><br> 关于齐次马尔科夫假设为什么只与上一个输出有关，是因为语言模型中有一个名叫n元模型的存在，这里推荐一篇文章讲解这个模型：<a href="https://blog.csdn.net/h__ang/article/details/88372626">NLP(二)：n元模型</a></p> <p>注：o：B、M、E、S这四种标签；λ：句子中的每个字（包括标点等非中文字符）。</p> </li></ol> 
<p>每个字的标签作者分为了4个，即<strong>B：词首</strong>；<strong>M：词中</strong>；<strong>E：词尾</strong>；<strong>S：单独成词</strong>。我不知道会不会有人和我之前一样混淆了词语与句子的关系，但是我这里还是要说明一下，帮大家排下坑。这里的词首 ！= 句首，虽然肯定句首也是个词，无论是词首还是单独成词，但是不能一概而论，因为一句话里面肯定只有一个句首，但是会有无数个词语，这个在后面讲代码的时候会再讲到，之后再详谈。</p> 
<p>HMM模型中有3个概率：</p> 
<ol><li>初始概率：自然语言序列中第一个字λ<sub>1</sub>的标记是o<sub>k</sub>的概率，即π = P(λ<sub>1</sub> = o<sub>k</sub>)</li><li>发射概率：即输出概率，就是隐含状态输出可见状态的概率，P(λ<sub>k</sub>|o<sub>k</sub>)</li><li>转移概率：由前一个隐含状态转移到另一个隐含状态的概率，P(o<sub>k</sub>|o<sub>k-1</sub>)</li></ol> 
<p>由于本文核心不是讨论该模型的概念，所以我这里放上一个我刚看韩梅梅模型的时候看到的一篇博客，说实话，有点让人高潮，链接如下：<a href="https://blog.csdn.net/hellozhxy/article/details/85254279">一文搞懂HMM（隐马尔可夫模型）</a></p> 
<h3><a id="2_Viterbi_29"></a>2. Viterbi算法</h3> 
<p>这是一种动态规划的算法，其核心思想是：如果最终的最优路径经过某个o<sub>i</sub>，那么从初始节点到o<sub>i-1</sub>点的路径必然也是一个最优路径，因为每一个节点o<sub>i</sub>只会影响前后两个P(o<sub>i-1</sub>|o<sub>i</sub>)和P(o<sub>i</sub>|o<sub>i+1</sub>)。</p> 
<p>关于维特比算法，给大家推荐这篇文章，讲的十分浅显易懂，一分钟解决维特比算法概念：<a href="https://www.zhihu.com/question/20136144" rel="nofollow">如何通俗地讲解 viterbi 算法？</a></p> 
<h2><a id="_34"></a>二、代码讲解</h2> 
<p>回归正题，由于代码好几百行，为了保证大家的阅读质量与逻辑梳理，我将这个HMM类分开为方法，依据我认为实际编程时搭建的步骤来进行讲解。先看一下代码框架：<br> <img src="https://images2.imgbox.com/83/79/mKNtsb7r_o.png" alt="具体代码框架"></p> 
<h3><a id="1____init___self_37"></a>1. __ init __(self):</h3> 
<p>大家都知道，__init__方法都是用来初始化的，我在注释上写明了每个参数的说明，这里就贴代码，不多赘述了。</p> 
<pre><code class="prism language-python"><span class="token comment"># 提取文件hmm_model.pkl</span>
<span class="token comment"># 主要用于存取算法中间结果，不用每次都训练模型</span>
self<span class="token punctuation">.</span>model_file <span class="token operator">=</span> <span class="token string">'./data/hmm_model.pkl'</span>

<span class="token comment"># 状态值集合</span>
self<span class="token punctuation">.</span>state_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'B'</span><span class="token punctuation">,</span> <span class="token string">'M'</span><span class="token punctuation">,</span> <span class="token string">'E'</span><span class="token punctuation">,</span> <span class="token string">'S'</span><span class="token punctuation">]</span>

<span class="token comment"># 参数加载，用于判断是否需要重新加载model_file</span>
self<span class="token punctuation">.</span>load_para <span class="token operator">=</span> <span class="token boolean">False</span>
</code></pre> 
<h3><a id="2_try_load_modelself_trained_51"></a>2. try_load_model(self, trained):</h3> 
<p>这个方法主要也是进行初始化的，只不过是先判断我们的是否训练过，如果训练过就直接加载中间文件，节省系统开销，没训练过则进行相应的训练。</p> 
<pre><code class="prism language-python">    <span class="token keyword">def</span> <span class="token function">try_load_model</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> trained<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        判别加载中间文件结果。
        当直接加载中间结果时，可以不通过语料库训练，直接进行分词调用。
        否则该函数用于初始化初始概率、转移概率以及发射概率等信息。(当需要重新训练时，需要初始化清空结果)
        :param trained: 是否需要直接加载中间结果:
                        True: 加载; False: 初始化清空结果
        :return:
        """</span>
        <span class="token keyword">if</span> trained<span class="token punctuation">:</span>
            <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>model_file<span class="token punctuation">,</span> <span class="token string">'rb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>A_dic <span class="token operator">=</span> pickle<span class="token punctuation">.</span>load<span class="token punctuation">(</span>f<span class="token punctuation">)</span>
                self<span class="token punctuation">.</span>B_dic <span class="token operator">=</span> pickle<span class="token punctuation">.</span>load<span class="token punctuation">(</span>f<span class="token punctuation">)</span>
                self<span class="token punctuation">.</span>Pi_dic <span class="token operator">=</span> pickle<span class="token punctuation">.</span>load<span class="token punctuation">(</span>f<span class="token punctuation">)</span>
                self<span class="token punctuation">.</span>load_para <span class="token operator">=</span> <span class="token boolean">True</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment"># 状态转移概率 P(ok | ok-1)</span>
            self<span class="token punctuation">.</span>A_dic <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>
            <span class="token comment"># 发射概率 P(λk | ok)</span>
            self<span class="token punctuation">.</span>B_dic <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>
            <span class="token comment"># 状态的初始概率 P(λ1 = ok)</span>
            self<span class="token punctuation">.</span>Pi_dic <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>
            self<span class="token punctuation">.</span>load_para <span class="token operator">=</span> <span class="token boolean">False</span>
</code></pre> 
<p>需要注意的是文件打开这里，因为打开的文件是.pkl类型，这是python保存的一种文件类型，而.pkl是二进制文件不是文本文件，所以需要rb方式才能打开。</p> 
<h3><a id="3_trainself_path_80"></a>3. train(self, path):</h3> 
<p>这是训练的方法，训练的依据为：</p> 
<blockquote> 
 <p>通过给定的分词语料进行训练，计算转移概率、发射概率和初始概率。<br> 语料格式为每行一句话，逗号隔开也算依据，每个词以空格分隔。</p> 
</blockquote> 
<p>里面封装了两个函数，先从函数开始说：</p> 
<h4><a id="31_init_parameters_87"></a>3.1 init_parameters()：</h4> 
<pre><code class="prism language-python">        <span class="token keyword">def</span> <span class="token function">init_parameters</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token triple-quoted-string string">"""
            初始化参数

            Pi_di: 初始概率，为列向量，直接赋值为0
            A_dic: 转移概率，因为转移情况只有16种，只是概率不同，所以可以先把标签给上并赋值0.0
            B_dic: 发射概率，因为并没有训练，所以里面为空
            count_dic: 统计每个标签出现的次数

            转移概率表明了: 从某个隐含状态转移到另一个(包括自己)隐含状态的概率
            输出概率表明了: 从某个隐含状态输出可见状态的概率
            :return:
            """</span>
            <span class="token keyword">for</span> state <span class="token keyword">in</span> self<span class="token punctuation">.</span>state_list<span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>A_dic<span class="token punctuation">[</span>state<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>s<span class="token punctuation">:</span> <span class="token number">0.0</span> <span class="token keyword">for</span> s <span class="token keyword">in</span> self<span class="token punctuation">.</span>state_list<span class="token punctuation">}</span>
                self<span class="token punctuation">.</span>Pi_dic<span class="token punctuation">[</span>state<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0.0</span>
                self<span class="token punctuation">.</span>B_dic<span class="token punctuation">[</span>state<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>

                count_dic<span class="token punctuation">[</span>state<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>
</code></pre> 
<p>这里的state是指的每一个标签，即“B”，“M”，“E”，“S”。<br> 关于这里初始化为何是这样，是因为如下：</p> 
<ol><li>对于转移概率矩阵A_dic，因为大家都知道，转移是从一个状态转移到另一个状态，而状态（标签）我们已经是确定了的，也就是只有16种情况，即{<!-- -->{B→B，B→M，B→E，B→S}…}，所以这里可以直接确定字典里面的内容，并且先赋值概率为0.0。</li><li>对于初始概率矩阵Pi_dic，这是一个列向量，毕竟是初始概率，每一句只会有第一个字记录，所以整体矩阵为[“B”, “M”, “E”, “S”]<sup>T</sup>，这里也就可以直接给每一列的概率赋值给0.0.</li><li>对于发射概率矩阵B_dic，因为我们还没开始训练，不清楚究竟有哪些汉字，也不清楚每个汉字对应的标签，所以这里给一个空的字典。</li><li>关于count_dic[state]是用于统计每个标签出现的次数，后续会讲解到，这里先一笔带过。</li></ol> 
<h4><a id="32_make_labeltext_116"></a>3.2 make_label(text)：</h4> 
<p>这个是为输入的文本赋值标签的操作，先上代码：</p> 
<pre><code class="prism language-python">        <span class="token keyword">def</span> <span class="token function">make_label</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token triple-quoted-string string">"""
            根据传入文本的字数来给这个小词语的每个字赋予一个标签
            :param text: 传入的词语
            :return out_text: 该词语中每个字出现位置的标签列表
            """</span>
            out_text <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
            <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
                <span class="token comment"># 如果只有一个字，则归类为S</span>
                out_text<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">'S'</span><span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                <span class="token comment"># 否则返回B，M，E</span>
                <span class="token comment"># 因为长度大于1，所以至少2个字，如果是多个字，那么在这个词中第一个字肯定是B，最后一个肯定是E，其余的为M</span>
                out_text <span class="token operator">+=</span> <span class="token punctuation">[</span><span class="token string">'B'</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token string">'M'</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token string">'E'</span><span class="token punctuation">]</span>

            <span class="token keyword">return</span> out_text
</code></pre> 
<p>这里传入的text是一个一个的小词语而不是整个句子，后面的代码中会有讲解，这里先有这么个概念。</p> 
<p>那么对于词语，一般来说都是两个字以上的，所以如果只有一个字，那么这个肯定是“S”（单独成词）；而如果多个字，那么我们就依据字出现的位置来进行标签赋值，可能有些同学想象力不够（比如我），光看这个公式有点莫名其妙，我这里以一个词语“秃然”和成语“聪明绝顶”来讲解。</p> 
<p>我们先看“秃然”，len（“秃然”）大于1，所以进入else，而len(“秃然”) - 2 = 0，所以[“M”]这个就为0，那么整个out_text = [‘B’, ‘E’]，而我们再回忆下，B为begin，E为end，那么是不是"秃"就是这个词语的begin，"然"就是这个词语的end了？</p> 
<p>"聪明绝顶"也一样，[‘M’] * (len(“聪明绝顶”) - 2) = 2，整个out_text = [‘B’, ‘M’, ‘M’, ‘E’]，那么’聪’是begin，'明’和’绝’为middle，'顶’为end。</p> 
<p>综上所述，整个中间的含义为：去掉词首和词尾后，中间汉字共有多少个。</p> 
<h4><a id="33__147"></a>3.3 其余代码</h4> 
<pre><code class="prism language-python">    <span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> path<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        通过给定的分词语料进行训练，计算转移概率、发射概率和初始概率
        语料格式为每行一句话，逗号隔开也算依据，每个词以空格分隔
        :param path: 训练文件所在路径
        :return self: 返回该类的实例
        """</span>

        <span class="token comment"># 重置概率矩阵</span>
        self<span class="token punctuation">.</span>try_load_model<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">)</span>

        <span class="token comment"># 统计每个标签的出现次数，求P(o)</span>
        count_dic <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>

        <span class="token keyword">def</span> <span class="token function">init_parameters</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
            
        <span class="token keyword">def</span> <span class="token function">make_label</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>

        init_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>
        line_num <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span>

        <span class="token comment"># 观察者集合，主要是字以及标点等</span>
        words <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 无序不重复的元素序列</span>
        <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>path<span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf8'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
            <span class="token keyword">for</span> line <span class="token keyword">in</span> f<span class="token punctuation">:</span>
                line_num <span class="token operator">+=</span> <span class="token number">1</span>  <span class="token comment"># 计算一共有多少行，用于后面计算初始概率</span>

                line <span class="token operator">=</span> line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 除去首位空格</span>
                <span class="token keyword">if</span> <span class="token operator">not</span> line<span class="token punctuation">:</span>
                    <span class="token comment"># 如果本行为空，则跳过本轮循环</span>
                    <span class="token keyword">continue</span>

                word_list <span class="token operator">=</span> <span class="token punctuation">[</span>i <span class="token keyword">for</span> i <span class="token keyword">in</span> line <span class="token keyword">if</span> i <span class="token operator">!=</span> <span class="token string">' '</span><span class="token punctuation">]</span>  <span class="token comment"># 除去空格后的每一行句子的列表</span>

                <span class="token comment"># 更新字的集合 words = words | set(word_list)，</span>
                <span class="token comment"># 即更新words，使words和set(word_list)中的字全部去重后加入到words中</span>
                words <span class="token operator">|</span><span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span>word_list<span class="token punctuation">)</span>

                line_list <span class="token operator">=</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 以空格为分隔符，将文本切片为一个列表</span>
                line_state <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

                <span class="token keyword">for</span> w <span class="token keyword">in</span> line_list<span class="token punctuation">:</span>
                    <span class="token comment"># 循环列表中的每一个字或词，获得make_label(w)中的结果，追加到line_state中</span>
                    line_state<span class="token punctuation">.</span>extend<span class="token punctuation">(</span>make_label<span class="token punctuation">(</span>w<span class="token punctuation">)</span><span class="token punctuation">)</span>

                <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>word_list<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token builtin">len</span><span class="token punctuation">(</span>line_state<span class="token punctuation">)</span>  <span class="token comment"># 断言，如果词语长度不等于状态长度，则报异常</span>

                <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>line_state<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 将列表组合为一个索引序列，包括数据下标和数据本身，比如(0, 'B')</span>
                    count_dic<span class="token punctuation">[</span>v<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>  <span class="token comment"># 该标签出现次数 + 1</span>
                    <span class="token keyword">if</span> k <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
                        self<span class="token punctuation">.</span>Pi_dic<span class="token punctuation">[</span>v<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>  <span class="token comment"># 每个句子的第一个字的状态，用于计算初始状态概率</span>
                    <span class="token keyword">else</span><span class="token punctuation">:</span>
                        self<span class="token punctuation">.</span>A_dic<span class="token punctuation">[</span>line_state<span class="token punctuation">[</span>k <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">[</span>v<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>  <span class="token comment"># 用于计算转移概率，即从上一个标签转移到这个标签发生了多少次</span>
                        <span class="token comment"># 用于计算发射概率，在该状态下每出现一个这个字，这个字的次数 + 1，如果没有，则加入该字典中</span>
                        self<span class="token punctuation">.</span>B_dic<span class="token punctuation">[</span>line_state<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">[</span>word_list<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>B_dic<span class="token punctuation">[</span>line_state<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span>word_list<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1.0</span>

        <span class="token comment"># 第一个字每个标签出现次数 / 句子个数 = 初始概率</span>
        self<span class="token punctuation">.</span>Pi_dic <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>k<span class="token punctuation">:</span> v <span class="token operator">*</span> <span class="token number">1.0</span> <span class="token operator">/</span> line_num <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> self<span class="token punctuation">.</span>Pi_dic<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>

        <span class="token comment"># 统计所有转移的次数，转移次数 / 这个标签出现次数 = 转移概率</span>
        self<span class="token punctuation">.</span>A_dic <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>k<span class="token punctuation">:</span> <span class="token punctuation">{<!-- --></span>k1<span class="token punctuation">:</span> v1 <span class="token operator">/</span> count_dic<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token keyword">for</span> k1<span class="token punctuation">,</span> v1 <span class="token keyword">in</span> v<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span> <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> self<span class="token punctuation">.</span>A_dic<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>

        <span class="token comment"># 数据稀疏问题: 由于训练样本不足而导致所估计的分布不可靠的问题</span>
        <span class="token comment"># 有可能出现某个数据(比如名字)就导致整个词语出现的概率为0</span>
        <span class="token comment"># 问题提出:</span>
        <span class="token comment"># 研究表明，语言中只有很少的常用词，大部分词都是低频词。</span>
        <span class="token comment"># 将语料库的规模扩大，主要是高频词词例的增加，大多数词(n元组)在语料中的出现是稀疏的，</span>
        <span class="token comment"># 因此扩大语料规模不能从根本上解决稀疏问题。</span>
        <span class="token comment"># 解决方案:</span>
        <span class="token comment"># 平滑: 1. 把在训练样本中出现过的事件的概率适当减小；</span>
        <span class="token comment">#       2. 把减小得到的概率质量分配给训练语料中没有出现过的事件；</span>
        <span class="token comment">#       3. 这个过程有时候也称为减值法(discounting)。</span>
        <span class="token comment"># 但是最简单的策略是"加1平滑"，</span>
        <span class="token comment"># 加1平滑: 规定n元组比真实出现次数多一次，没有出现过的n元组的概率不再是0，而是一个较小的概率值，实现了概率质量的重新分配</span>
        <span class="token comment"># 统计在每个标签中出现每个字的次数，该次数 / 这个标签出现的次数 = 发射概率</span>
        self<span class="token punctuation">.</span>B_dic <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>k<span class="token punctuation">:</span> <span class="token punctuation">{<!-- --></span>k1<span class="token punctuation">:</span> <span class="token punctuation">(</span>v1 <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> count_dic<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token keyword">for</span> k1<span class="token punctuation">,</span> v1 <span class="token keyword">in</span> v<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span> <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> self<span class="token punctuation">.</span>B_dic<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>  <span class="token comment"># 序列化</span>

        <span class="token comment"># 保存数据到pkl文件中</span>
        <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>model_file<span class="token punctuation">,</span> <span class="token string">'wb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
            pickle<span class="token punctuation">.</span>dump<span class="token punctuation">(</span>self<span class="token punctuation">.</span>A_dic<span class="token punctuation">,</span> f<span class="token punctuation">)</span>
            pickle<span class="token punctuation">.</span>dump<span class="token punctuation">(</span>self<span class="token punctuation">.</span>B_dic<span class="token punctuation">,</span> f<span class="token punctuation">)</span>
            pickle<span class="token punctuation">.</span>dump<span class="token punctuation">(</span>self<span class="token punctuation">.</span>Pi_dic<span class="token punctuation">,</span> f<span class="token punctuation">)</span>

        <span class="token keyword">return</span> self
</code></pre> 
<p>我基本上每一行代码都给了注释，这里就只用挑几个我觉得比较重要的来讲解。</p> 
<p>上文提到的关于count_dic[state]字典的作用这些也就是在这里使用了，用于计算概率，具体的在代码中有相应的注释，大家可以看代码配合上下文的解释进行理解。</p> 
<p>首先是概率计算这一块，要从for循环这里开始说：</p> 
<ol><li>初始字标签统计：这个是最简单的一个了，就是只需要把第一个字出现的标签+1就行了，毕竟列向量嘛。</li><li>转移标签统计：首先我们来明确下k和v的含义，k是索引，v是标签，所以这里统计的方式是：从k - 1的字的状态到k字的状态每出现一次就+1.</li><li>发射统计：有了上面这句话，那么这个也好理解了，就是在k字的状态获得k字的出现次数。</li></ol> 
<p>对于下面概率的计算，说明如下：</p> 
<ol><li>初始概率计算：k：标签名称；v：该标签出现的次数。为什么v要乘以1.0，就是避免这个不是浮点数类型，相除之后出错。我这里给一个运行结果大家更方便理解：</li></ol> 
<pre><code class="prism language-python">Pi_dic<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span> dict_items<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'B'</span><span class="token punctuation">,</span> <span class="token number">173416.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'M'</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'E'</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'S'</span><span class="token punctuation">,</span> <span class="token number">124543.0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<ol start="2"><li>转移概率计算：k：转移前的标签名；v：转移后的标签字典；k1：转移后的标签名；v1：转移后这个标签出现的次数。明白了这个后整个计算过程就没啥难度了，运行结果如下：</li></ol> 
<pre><code class="prism language-python">A_dic<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span> dict_items<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'B'</span><span class="token punctuation">,</span> <span class="token punctuation">{<!-- --></span><span class="token string">'B'</span><span class="token punctuation">:</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token string">'M'</span><span class="token punctuation">:</span> <span class="token number">162066.0</span><span class="token punctuation">,</span> <span class="token string">'E'</span><span class="token punctuation">:</span> <span class="token number">1226466.0</span><span class="token punctuation">,</span> <span class="token string">'S'</span><span class="token punctuation">:</span> <span class="token number">0.0</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'M'</span><span class="token punctuation">,</span> <span class="token punctuation">{<!-- --></span><span class="token string">'B'</span><span class="token punctuation">:</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token string">'M'</span><span class="token punctuation">:</span> <span class="token number">62332.0</span><span class="token punctuation">,</span> <span class="token string">'E'</span><span class="token punctuation">:</span> <span class="token number">162066.0</span><span class="token punctuation">,</span> <span class="token string">'S'</span><span class="token punctuation">:</span> <span class="token number">0.0</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<ol start="3"><li>发射概率计算：k：发射这个字的标签名；v：发射的字的集合；k1：发射的汉字；v1：发射该汉字的次数。这里最主要的是讲解“+1平滑”，为什么需要+1平滑？就是因为很有可能我们有个低频词比如“肏”（现在大多写成草、操、艹等），那么我们就算扩大训练集，出现这个字的频率估计也不会出现对吧，不仅增加了复杂度，而且也达不到我们想要的效果，为了防止这种某个低频字出现概率为0，最简单的方法就是+1，那么这就是一个趋近于0的出现频率但并非0，避免了这样的情况。运行结果如下：</li></ol> 
<pre><code class="prism language-python">B_dic<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span> dict_items<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'B'</span><span class="token punctuation">,</span> <span class="token punctuation">{<!-- --></span><span class="token string">'中'</span><span class="token punctuation">:</span> <span class="token number">12812.0</span><span class="token punctuation">,</span> <span class="token string">'儿'</span><span class="token punctuation">:</span> <span class="token number">464.0</span><span class="token punctuation">,</span> <span class="token string">'踏'</span><span class="token punctuation">:</span> <span class="token number">62.0</span><span class="token punctuation">,</span> <span class="token string">'全'</span><span class="token punctuation">:</span> <span class="token number">7279.0</span><span class="token punctuation">,</span> <span class="token string">'各'</span><span class="token punctuation">:</span> <span class="token number">4884.0</span><span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">}</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="4_viterbiself_text_states_start_p_trans_p_emit_p_260"></a>4. viterbi(self, text, states, start_p, trans_p, emit_p):</h3> 
<p>接着是维特比算法，这一块主要是在最后判断最后一个字的时候这里还有些许没有弄懂，在注释中写明了我个人的理解，如果后续有新的理解或者认识会进行更改：</p> 
<pre><code class="prism language-python">    <span class="token keyword">def</span> <span class="token function">viterbi</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">,</span> states<span class="token punctuation">,</span> start_p<span class="token punctuation">,</span> trans_p<span class="token punctuation">,</span> emit_p<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        维特比算法: 动态规划方法

        如果最终的最优路径经过某个oi，那么从初始节点到oi-1点的路径必然也是一个最优路径
        因为每一个节点oi只会影响前后两个P(oi-1 | oi)和P(oi | oi+1)
        :param text: 输入的需要切分的文本内容
        :param states: 状态值集合
        :param start_p: 初始概率 Pi_dic
        :param trans_p: 转移概率 A_dic
        :param emit_p: 发射概率 B_dic
        :return prob: 最佳路径的概率
        :return path[state]: 最佳路径
        """</span>
        V <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span><span class="token punctuation">]</span>  <span class="token comment"># 记录输入的文本中每个字属于每个标签的概率</span>
        path <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>  <span class="token comment"># 标签</span>
        <span class="token keyword">for</span> y <span class="token keyword">in</span> states<span class="token punctuation">:</span>
            V<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span>y<span class="token punctuation">]</span> <span class="token operator">=</span> start_p<span class="token punctuation">[</span>y<span class="token punctuation">]</span> <span class="token operator">*</span> emit_p<span class="token punctuation">[</span>y<span class="token punctuation">]</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span>text<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># 计算这个文本的第一个字属于4个标签的概率</span>
            path<span class="token punctuation">[</span>y<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span>y<span class="token punctuation">]</span>

        <span class="token keyword">for</span> t <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># 循环这个文本的第二个字到最后(因为第一个字是属于初始概率)</span>

            V<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span><span class="token punctuation">)</span>  <span class="token comment"># 列表中新增一个字典(用于存放第二个字以及之后的字出现标签的概率)</span>
            new_path <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>

            <span class="token comment"># 检查训练的发射概率矩阵中是否有该字</span>
            never_seen <span class="token operator">=</span> text<span class="token punctuation">[</span>t<span class="token punctuation">]</span> <span class="token operator">not</span> <span class="token keyword">in</span> emit_p<span class="token punctuation">[</span><span class="token string">'S'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">and</span> \
                text<span class="token punctuation">[</span>t<span class="token punctuation">]</span> <span class="token operator">not</span> <span class="token keyword">in</span> emit_p<span class="token punctuation">[</span><span class="token string">'M'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">and</span> \
                text<span class="token punctuation">[</span>t<span class="token punctuation">]</span> <span class="token operator">not</span> <span class="token keyword">in</span> emit_p<span class="token punctuation">[</span><span class="token string">'E'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">and</span> \
                text<span class="token punctuation">[</span>t<span class="token punctuation">]</span> <span class="token operator">not</span> <span class="token keyword">in</span> emit_p<span class="token punctuation">[</span><span class="token string">'B'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span>

            <span class="token keyword">for</span> y <span class="token keyword">in</span> states<span class="token punctuation">:</span>
                <span class="token comment"># 循环4个标签</span>

                <span class="token comment"># 设置未知字单独成词，未知字的发射概率设置为1</span>
                <span class="token comment"># 这句话翻译为: 如果这个字在训练结果中没有没见过，那么发射概率为这个字的发射概率，否则为1.0</span>
                p_emit <span class="token operator">=</span> emit_p<span class="token punctuation">[</span>y<span class="token punctuation">]</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span>text<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token keyword">if</span> <span class="token operator">not</span> never_seen <span class="token keyword">else</span> <span class="token number">1.0</span>

                <span class="token comment"># 如果t - 1的字的y0标签出现过，那么t这个字取: t - 1字y0标签出现的概率 * 从y0转移到y的转移概率 * 发射概率 中最大值</span>
                <span class="token comment"># 即state是t这个字从y0转移到y最有可能出现的t - 1时刻的标签，prob是t这个字从y0转移到y取到state标签的概率</span>
                <span class="token comment"># y0是t - 1的字出现过的标签</span>
                <span class="token comment"># 如果最终的最优路径经过某个oi，那么从初始节点到oi-1点的路径必然也是一个最优路径</span>
                <span class="token punctuation">(</span>prob<span class="token punctuation">,</span> state<span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span>
                    <span class="token punctuation">[</span><span class="token punctuation">(</span>V<span class="token punctuation">[</span>t <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span>y0<span class="token punctuation">]</span> <span class="token operator">*</span> trans_p<span class="token punctuation">[</span>y0<span class="token punctuation">]</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span>y<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">*</span> p_emit<span class="token punctuation">,</span> y0<span class="token punctuation">)</span>
                     <span class="token keyword">for</span> y0 <span class="token keyword">in</span> states <span class="token keyword">if</span> V<span class="token punctuation">[</span>t <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span>y0<span class="token punctuation">]</span> <span class="token operator">&gt;</span> <span class="token number">0</span>
                     <span class="token punctuation">]</span><span class="token punctuation">)</span>

                V<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">[</span>y<span class="token punctuation">]</span> <span class="token operator">=</span> prob  <span class="token comment"># 从y0转移到y，第t个字取到第y个标签的最有可能的概率，每次添加的概率又成为下一轮循环的前一个节点概率</span>
                new_path<span class="token punctuation">[</span>y<span class="token punctuation">]</span> <span class="token operator">=</span> path<span class="token punctuation">[</span>state<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span>y<span class="token punctuation">]</span>  <span class="token comment"># 从t - 1字到t字的路径，如:如果t字是B标签，那么t - 1字最优结果是S标签</span>

            path <span class="token operator">=</span> new_path  <span class="token comment"># 更新路径</span>

        <span class="token comment"># 个人理解: 最后一个字如果出现M的概率比S大(因为按理来说处于中间位置的标签不该最后出现)，</span>
        <span class="token comment">#          那么极有可能因为是二元模型，历史信息较少判断出错，所以需要重新判断这个字标签为E和M的概率，</span>
        <span class="token comment">#          而如果最后一个字单独成词，那么要再看看有没有其他的可能性</span>
        <span class="token keyword">if</span> emit_p<span class="token punctuation">[</span><span class="token string">'M'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span>text<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">&gt;</span> emit_p<span class="token punctuation">[</span><span class="token string">'S'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span>text<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># 如果最后一个字的标签为M的发射概率大于S,那么就取最后一个字的E或者M中最大的概率以及标签</span>
            <span class="token punctuation">(</span>prob<span class="token punctuation">,</span> state<span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span>V<span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span>y<span class="token punctuation">]</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span> <span class="token keyword">for</span> y <span class="token keyword">in</span> <span class="token punctuation">(</span><span class="token string">'E'</span><span class="token punctuation">,</span> <span class="token string">'M'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment"># 如果最后一个字的标签为S的发射概率大于M，那么就取最后一个字所有标签中的最大概率和标签</span>
            <span class="token punctuation">(</span>prob<span class="token punctuation">,</span> state<span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span>V<span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span>y<span class="token punctuation">]</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span> <span class="token keyword">for</span> y <span class="token keyword">in</span> states<span class="token punctuation">]</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> prob<span class="token punctuation">,</span> path<span class="token punctuation">[</span>state<span class="token punctuation">]</span>
</code></pre> 
<p>由于维特比算法只需要考虑最优的路径，会将其余的内容给排除掉，所以你会看到在代码中有许多max()的地方，就是选择概率最大的部分，这也是最优的情况。</p> 
<h3><a id="5cutself_text_329"></a>5.cut(self, text):</h3> 
<p>这也是最后一个方法，切词的方法，这一部分大家可以使用数据结构的线性表来理解，即头指针begin和指向下一个元素的next指针：</p> 
<pre><code class="prism language-python">    <span class="token keyword">def</span> <span class="token function">cut</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        切词，通过加载中间文件，调用维特比算法完成。
        :param text: 输入的文本
        :return:
        """</span>
        <span class="token keyword">if</span> <span class="token operator">not</span> self<span class="token punctuation">.</span>load_para<span class="token punctuation">:</span>
            <span class="token comment"># 如果load_para为False，那么判断文件是否存在以决定是否需要重新训练</span>
            self<span class="token punctuation">.</span>try_load_model<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>self<span class="token punctuation">.</span>model_file<span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token comment"># 获取维特比算法返回的最佳路径概率与最佳路径列表</span>
        prob<span class="token punctuation">,</span> pos_list <span class="token operator">=</span> self<span class="token punctuation">.</span>viterbi<span class="token punctuation">(</span>text<span class="token punctuation">,</span> self<span class="token punctuation">.</span>state_list<span class="token punctuation">,</span> self<span class="token punctuation">.</span>Pi_dic<span class="token punctuation">,</span> self<span class="token punctuation">.</span>A_dic<span class="token punctuation">,</span> self<span class="token punctuation">.</span>B_dic<span class="token punctuation">)</span>
        begin<span class="token punctuation">,</span> <span class="token builtin">next</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span>  <span class="token comment"># begin是一个词的开始，next是下一个词开始的索引</span>

        <span class="token keyword">for</span> i<span class="token punctuation">,</span> char <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># 将输入的文本组合为一个索引序列，i为索引，char为每个字</span>
            pos <span class="token operator">=</span> pos_list<span class="token punctuation">[</span>i<span class="token punctuation">]</span>  <span class="token comment"># 路径中的第i个节点</span>
            <span class="token keyword">if</span> pos <span class="token operator">==</span> <span class="token string">'B'</span><span class="token punctuation">:</span>
                <span class="token comment"># 如果这一节点为"B"，那么begin为该索引，意思就是这一节点是这个词的开始</span>
                begin <span class="token operator">=</span> i
            <span class="token keyword">elif</span> pos <span class="token operator">==</span> <span class="token string">'E'</span><span class="token punctuation">:</span>
                <span class="token comment"># 如果这一节点为"E", 那么生成器生成从begin到i的内容，并且next为下一个字的开始，</span>
                <span class="token comment"># 意思就是，这个词结束了，并且next指针指向下个词开始的位置</span>
                <span class="token keyword">yield</span> text<span class="token punctuation">[</span>begin<span class="token punctuation">:</span> i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span>
                <span class="token builtin">next</span> <span class="token operator">=</span> i <span class="token operator">+</span> <span class="token number">1</span>
            <span class="token keyword">elif</span> pos <span class="token operator">==</span> <span class="token string">'S'</span><span class="token punctuation">:</span>
                <span class="token comment"># 如果这一节点为"S"，那么生成器生成这个字，并且next为下一个字的开始，</span>
                <span class="token comment"># 意思就是，这个字单独成词，所以下个词开始</span>
                <span class="token keyword">yield</span> char
                <span class="token builtin">next</span> <span class="token operator">=</span> i <span class="token operator">+</span> <span class="token number">1</span>

        <span class="token keyword">if</span> <span class="token builtin">next</span> <span class="token operator">&lt;</span> <span class="token builtin">len</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># 如果next指针的位置比整个文本长度小，那么生成器生成后面的内容，意思就是后面的内容整体为一个词</span>
            <span class="token keyword">yield</span> text<span class="token punctuation">[</span><span class="token builtin">next</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
</code></pre> 
<p>至于yield生成器，大家可以自行查询，这里不多赘述。</p> 
<p>主要是关于为什么判断里面不判断’M’，因为大家在训练的过程中已经看到了’M’的判定方式了，'M’大概率是属于’B’和’E’之间的（为什么说大概率，是因为维特比算法中没看懂的那部分），所以既然已经有了’E’里面的算法，把词开头和结尾都算在内了，就没有必要用更多的计算步骤来处理’M’的问题，这样可以节省一部分资源，防止开销过大。我这里用几张线性表的图，大家就可以更方便的理解上面的过程了：<br> <img src="https://images2.imgbox.com/37/2d/rOqWt9A6_o.png" alt="切词开始"></p> 
<ul><li>最开始begin和next都在线性表的第一个位置，当遇到第一个字发现是’S’，所以执行next = i + 1这一步，于是变为下图这样：<br> <img src="https://images2.imgbox.com/6c/97/4agLSCVi_o.png" alt="切词第一步"></li><li>这时候发现’秃’是B，于是执行’B’这一步，begin = i，如下图：<br> <img src="https://images2.imgbox.com/5b/a4/VpCO8t9q_o.png" alt="切词第二步"></li><li>下一轮循环开始，i + 1，遇到’然’，这个是E，所以将’秃然’输出，然后移动next指针。如下图所示：<br> <img src="https://images2.imgbox.com/46/4a/wmHla5qP_o.png" alt="切词第三步"></li><li>后面的过程就不多赘述了，就是i和begin移动到索引3的位置，然后随着i的增加，发现’明’和’绝’都是’M’，于是跳过，来到顶，也就是最后一步，输出最后一个词语，切词结束。如下图所示：<br> <img src="https://images2.imgbox.com/a7/d9/n4MnpqRJ_o.png" alt="最后一次切词"></li></ul> 
<h2><a id="_379"></a>三、代码与效果展示</h2> 
<h3><a id="1__380"></a>1. 代码</h3> 
<p>由于代码加上注释接近300行，放在这里有占篇幅的嫌疑，估计大家看着也头皮发麻，所以我放在了github上，有需要的可以自提：<a href="https://github.com/Balding-Lee/NLP_learning">https://github.com/Balding-Lee/NLP_learning</a>（数据集在data文件夹中，文件名为hidden_markov_model）</p> 
<h3><a id="2__382"></a>2. 效果演示</h3> 
<p><img src="https://images2.imgbox.com/81/7c/e8BqHVVI_o.png" alt="效果演示"><br> 嗯，好像这句话的效果和我们预计的不太像哈，那么问题来了，为什么会是这样的？我们不妨来推测一下：</p> 
<ol><li>第一步我们肯定要看为什么’我秃然’这么刺激的东西会合并为一个词语：<br> <img src="https://images2.imgbox.com/36/02/yy7niH1Q_o.png" alt="出现情况"><br> 那么这就说得通了，'我’这个字作为begin出现的频率最高，所以难怪会出现在词首，而’然’作为词尾出现的频率也是最高的。可能会有人问“为什么没有’秃’这个字，这个字这么没有牌面的么？”其实并不是我没有测试，而是训练集里面压根没这个字，我人傻了你知道么。<br> <img src="https://images2.imgbox.com/de/6f/zuWCJ3vv_o.png" alt="秃字的报错"></li><li>那么顺着上面的思路，也可以清晰地判定，在训练集中就没有“聪明绝顶”这个成语，或者说出现频率太低，直接被忽视了。</li></ol> 
<p>综上所述，如果想要结果准确，不仅需要我们的算法功能强大，更需要我们的数据集是很适合的。</p> 
<h2><a id="_392"></a>四、参考</h2> 
<p>[1] 涂铭,刘祥,刘树春.python自然语言处理实战核心技术与算法[M].机械工业出版社:北京,2018:38.<br> [2] 路生.如何通俗地讲解 viterbi 算法？[EB/OL].https://www.zhihu.com/question/20136144,2020-04-09.<br> [3] hellozhxy.一文搞懂HMM（隐马尔可夫模型）[EB/OL].https://blog.csdn.net/hellozhxy/article/details/85254279,2018-12-25.<br> [4] 月臻.NLP(二)：n元模型[EB/OL].https://blog.csdn.net/h__ang/article/details/88372626,2019-03-10.</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d135194954ded66666171a57908d6e25/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">centos7 python安装包移动发生软连接错误</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e73292657f13494f3914f095b61e7e86/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">A bean with that name has already been defined in class path resource</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>