<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【标准化方法】(4) Weight Normalization 原理解析、代码复现，附Pytorch代码 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【标准化方法】(4) Weight Normalization 原理解析、代码复现，附Pytorch代码" />
<meta property="og:description" content="今天和各位分享一下深度学习中常用的归一化方法，权重归一化（Weight Normalization， WN），通过理论解析，用 Pytorch 复现一下代码。
Weight Normalization 的论文地址如下：https://arxiv.org/pdf/1903.10520.pdf
1. 原理解析 权重归一化（Weight Normalization，WN）选择对神经网络的权值向量 W 进行参数重写，参数化权重改善条件最优问题来加速收敛，灵感来自批归一化算法，但是并不像批归一化算法一样依赖于批次大小，不会对梯度增加噪声且计算量很小。权重归一化成功用于 LSTM 和对噪声敏感的模型，如强化学习和生成模型。
对深度学习网络权值 W 进行归一化的操作公式如下：
通过一个 k 维标量 g 和一个向量 V 对权重向量 W 进行解耦合。标量 ，即权重 W 的大小， 表示 v 的欧几里得范数（二范数）。
作者提出对参数 v，g 直接重新参数化然后执行新的随机梯度下降，并且认为通过将权重向量（g）的范数与的方向解耦，加速了随机梯度下降的收敛。
假设代价函数记为 L，此时的深度学习网络权值的梯度计算公式为：
设 ，其中 是投影矩阵。梯度计算可以写成：。
设 ，当梯度噪声大时，c 变大，有 ，则 变小。
当梯度较小时，c 变小趋于0，有 。即：权重归一化 WN 使用这种机制做到梯度稳定。另外，作者也发现 ||v|| 对学习率有很强的鲁棒性。
WN 不像 BN 还具有固定神经网络各层产生的特征尺度的好处，WN 需要小心的参数初始化。给 v 的范数设定一个范围（正态分布均值为零，标准差为 0.05），这样虽然延长了参数更新的时间，但收敛后的测试性能会比较好。
设 ，仅在初始化期间取 可以得到应用 WN 后，
由上式可得，当 WN 进行参数初始化时可以在一开始达到和 BN 相同的作用。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/abdb380bb2e0bef082745869ef055e5d/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-09T18:15:14+08:00" />
<meta property="article:modified_time" content="2023-05-09T18:15:14+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【标准化方法】(4) Weight Normalization 原理解析、代码复现，附Pytorch代码</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><span style="color:#0d0016;">今天和各位分享一下深度学习中常用的归一化方法，权重归一化（Weight Normalization， WN），通过理论解析，用 Pytorch 复现一下代码。</span></p> 
<p><span style="color:#0d0016;">Weight Normalization 的论文地址如下：</span><a href="https://arxiv.org/pdf/1903.10520.pdf" rel="nofollow" title="https://arxiv.org/pdf/1903.10520.pdf">https://arxiv.org/pdf/1903.10520.pdf</a></p> 
<hr> 
<h2><span style="color:#ff9900;"><strong>1. 原理解析</strong></span></h2> 
<p><span style="color:#be191c;"><strong>权重归一化</strong></span><span style="color:#0d0016;">（Weight  Normalization，</span><span style="color:#be191c;"><strong>WN</strong></span><span style="color:#0d0016;">）选择对神经网络的权值向量 W 进行参数重写，</span><span style="color:#1a439c;"><strong>参数化权重改善条件最优问题来加速收敛</strong></span><span style="color:#0d0016;">，灵感来自批归一化算法，但是并不像批归一化算法一样依赖于批次大小，<strong>不会对梯度增加噪声且计算量很小</strong>。权重归一化成功用于 LSTM 和对噪声敏感的模型，如强化学习和生成模型。</span></p> 
<p><span style="color:#be191c;"><strong>对深度学习网络权值 W 进行归一化的操作公式如下：</strong></span></p> 
<p>  </p> 
<p style="text-align:center;"><img alt="w = \frac{g}{||v||} v" class="mathcode" src="https://images2.imgbox.com/32/ac/6RFm4iVR_o.png"></p> 
<p><span style="color:#0d0016;">通过一个 k 维标量 g 和一个向量 V 对权重向量 W 进行解耦合。标量 <img alt="g=||W||" class="mathcode" src="https://images2.imgbox.com/95/f9/Srczu3Px_o.png"> ，即权重 W 的大小，<img alt="||v||" class="mathcode" src="https://images2.imgbox.com/37/31/GGFNF3N2_o.png"> 表示 v 的欧几里得范数（二范数）。</span></p> 
<p><span style="color:#0d0016;">作者提出</span><span style="color:#1a439c;"><strong>对参数 v，g 直接重新参数化然后执行新的随机梯度下降</strong></span><span style="color:#0d0016;">，并且认为通过<strong>将权重向量（g）的范数与<img alt="(\frac{v}{||v||})" class="mathcode" src="https://images2.imgbox.com/bc/22/jAbt23CG_o.png">的方向解耦，加速了随机梯度下降的收敛</strong>。</span></p> 
<p><span style="color:#0d0016;">假设代价函数记为 L，此时的深度学习</span><strong><span style="color:#be191c;">网络权值的梯度计算公式</span></strong><span style="color:#0d0016;">为：</span></p> 
<p style="text-align:center;"><img alt="\Delta_{_g}L=\Delta_{_w}L\cdot\Delta_{_g}W=\frac{\Delta_{_w}L\cdot\nu}{||\nu||}" class="mathcode" src="https://images2.imgbox.com/71/e0/oC6eZ8Az_o.png"></p> 
<p style="text-align:center;"><img alt="" height="135" src="https://images2.imgbox.com/7f/53/LG8C7j9u_o.png" width="291"></p> 
<p><span style="color:#0d0016;">设 <img alt="M_w=I-\frac{ww'}{||w||^2}" class="mathcode" src="https://images2.imgbox.com/2b/8b/oWIk7JvO_o.png">，其中 <img alt="M_w" class="mathcode" src="https://images2.imgbox.com/9b/8d/DRRawp2S_o.png"> 是投影矩阵。<strong>梯度计算可以写成</strong>：<img alt="\Delta_{_v}L=\frac{g}{||v||}\cdot M_{_w}\Delta_{_w}L" class="mathcode" src="https://images2.imgbox.com/13/85/4P82EMHn_o.png">。</span></p> 
<p><span style="color:#0d0016;">设 <img alt="\frac{||\Delta v||}{||v||} = c" class="mathcode" src="https://images2.imgbox.com/e3/89/AF86km5V_o.png">，</span><span style="color:#1a439c;"><strong>当梯度噪声大时</strong></span><span style="color:#0d0016;">，c 变大，有 <img alt="\|v'\|=(\|v\|^2+c^2\|v\|^2)^{1/2}&gt;\|v\|" class="mathcode" src="https://images2.imgbox.com/de/0b/4t5c20Vf_o.png">，则 <img alt="\Delta_{v'}L" class="mathcode" src="https://images2.imgbox.com/f4/b6/RxuF7yZt_o.png"> 变小。</span></p> 
<p><span style="color:#1a439c;"><strong>当梯度较小时</strong></span><span style="color:#0d0016;">，c 变小趋于0，有 <img alt="\|v'\|=(\|v\|^2+c^2\|v\|^2)^{1/2} \approx \|v\|" class="mathcode" src="https://images2.imgbox.com/5f/1d/dTbQmLwk_o.png">。即：权重归一化 WN 使用这种机制做到梯度稳定。另外，作者也发现 ||v|| 对学习率有很强的鲁棒性。</span></p> 
<p><span style="color:#0d0016;">WN 不像 BN 还具有固定神经网络各层产生的特征尺度的好处，</span><span style="color:#956fe7;"><strong>WN 需要小心的参数初始化</strong></span><span style="color:#0d0016;">。<strong>给 v 的范数设定一个范围（正态分布均值为零，标准差为 0.05）</strong>，这样虽然延长了参数更新的时间，但收敛后的测试性能会比较好。</span></p> 
<p><span style="color:#0d0016;">设 <img alt="t = \frac{v \cdot x}{||v||}" class="mathcode" src="https://images2.imgbox.com/b7/a5/tdu1ldPq_o.png">，仅在初始化期间取 <img alt="g\leftarrow\frac{1}{\sigma[t]},b\leftarrow\frac{-\mu[t]}{\sigma[t]}" class="mathcode" src="https://images2.imgbox.com/1c/00/wSZsneQm_o.png"></span></p> 
<p><span style="color:#0d0016;">可以得到应用 WN 后，</span></p> 
<p style="text-align:center;"><span style="color:#0d0016;"><img alt="\begin{aligned} &amp; y=\phi(w\cdot x+b) \\ &amp;=\phi(g\cdot{\frac{v}{||v||}}x+b) \\ &amp;=\phi(\frac{1}{\sigma[t]}\cdot\frac{v}{||v||}x-\frac{\mu[t]}{\sigma[t]}) \\ &amp;=\phi(\frac{t-\mu[t]}{\sigma[t]}) \end{aligned}" class="mathcode" src="https://images2.imgbox.com/52/36/MX8Wdwkj_o.png"></span></p> 
<p><span style="color:#0d0016;">由上式可得，当 WN 进行参数初始化时可以在一开始达到和 BN 相同的作用。</span></p> 
<hr> 
<h2><span style="color:#ff9900;"><strong>2. 代码演示</strong></span></h2> 
<p><span style="color:#0d0016;">这里以《Micro-Batch Training with Batch-Channel Normalization and Weight Standardization》这篇文章中的权重归一化方法为例，展示一下代码，比较简单，只需要对权重文件的每个通道做归一化处理。示意图如下。</span></p> 
<p class="img-center"><img alt="" height="308" src="https://images2.imgbox.com/f2/e2/i90vFOZI_o.png" width="605"></p> 
<pre><code class="language-python">import torch

def WS(weight:torch.Tensor, eps:float):
    # 权重shape=[c_out, c_in, k_h, k_w]
    c_out, c_in, *kernel_shape = weight.shape
    # [c_out, c_in, k_h, k_w]--&gt;[c_out, c_in*k_h*k_w]
    weight = weight.view(c_out, -1)
    # 计算 [c_in*k_h*k_w] 维度上的均值和方差 --&gt; [c_out,1]
    var, mean = torch.var_mean(weight, dim=1, keepdim=True)
    # 权重标准化
    weight = (weight-mean) / torch.sqrt(var+eps)
    # [c_out, c_in*k_h*k_w]--&gt;[c_out, c_in, k_h, k_w]
    return weight.view(c_out, c_in, *kernel_shape)
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e089ac71a50681d845f052baf934a3cf/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">CSS-复合选择器</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b4dad02100d14c011fe68b8c885594c8/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Android 存储目录</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>