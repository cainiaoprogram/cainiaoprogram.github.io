<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>XGBoost - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="XGBoost" />
<meta property="og:description" content="LeetCode题目记录 1.XGBoost概念2.集成思想3.分析XGboost思路4.原理推导5.正则化6.优缺点7. sklearn 参数 1.XGBoost概念 XGBoost全名叫（eXtreme Gradient Boosting）极端梯度提升，经常被用在一些比赛中，其效果显著。它是大规模并行boosted tree的工具，它是目前最快最好的开源boosted tree工具包。XGBoost 所应用的算法就是 GBDT（gradient boosting decision tree）的改进，既可以用于分类也可以用于回归问题中。
1、回归树与决策树
事实上，分类与回归是一个型号的东西，只不过分类的结果是离散值，回归是连续的，本质是一样的，都是特征（feature）到结果/标签（label）之间的映射。说说决策树和回归树，在上面决策树的讲解中相信决策树分类已经很好理解了。
分类树的样本输出（即响应值）是类的形式，如判断蘑菇是有毒还是无毒，周末去看电影还是不去。而回归树的样本输出是数值的形式，比如给某人发放房屋贷款的数额就是具体的数值，可以是0到120万元之间的任意值。
那么，这时候你就没法用上述的信息增益、信息增益率、基尼系数来判定树的节点分裂了，你就会采用新的方式，预测误差，常用的有均方误差、对数误差等。而且节点不再是类别，是数值（预测值），那么怎么确定呢，有的是节点内样本均值，有的是最优化算出来的比如Xgboost。
2、boosting集成学习
boosting集成学习，由多个相关联的决策树联合决策，什么叫相关联，举个例子，有一个样本[数据-&gt;标签]是[(2，4，5)-&gt; 4]，第一棵决策树用这个样本训练得预测为3.3，那么第二棵决策树训练时的输入，这个样本就变成了[(2，4，5)-&gt; 0.7]，也就是说，下一棵决策树输入样本会与前面决策树的训练和预测相关。
与之对比的是random foreast（随机森林）算法，各个决策树是独立的、每个决策树在样本堆里随机选一批样本，随机选一批特征进行独立训练，各个决策树之间没有啥毛线关系。
所以首先Xgboost首先是一个boosting的集成学习，这样应该很通俗了
3、这个时候大家就能感觉到一个回归树形成的关键点：（1）分裂点依据什么来划分（如前面说的均方误差最小，loss）；（2）分类后的节点预测值是多少（如前面说，有一种是将叶子节点下各样本实际值得均值作为叶子节点预测误差，或者计算所得）
2.集成思想 在学习XGBoost之前，我们得需要先明白集成思想。集成学习方法是指将多个学习模型组合，以获得更好的效果，使组合后的模型具有更强的泛化能力。另外XGBoost是以分类回归树(CART树)进行组合。故在此之前，我们先看下CART树(CART树具体原理请自行复习，或者可以留言)。如下，通过输入用户年龄、性别进行判断用户是否喜欢玩游戏的得分值。由此得到一颗CART树模型。
我们知道对于单个的决策树模型容易出现过拟合，并且不能在实际中有效应用。所以出现了集成学习方法。如下图，通过两棵树组合进行玩游戏得分值预测。其中tree1中对小男生的预测分值为2，tree2对小男生的预测分值为0.9。则该小男生的最后得分值为2.9。
将上面集成学习方法推广到一般情况，可知其预测模型为：
其中为树的总个数，表示第颗树，表示样本的预测结果。
损失函数为：
3.分析XGboost思路 首先明确下我们的目标，希望建立K个回归树，使得树群的预测值尽量接近真实值（准确率）而且有尽量大的泛化能力（更为本质的东西），从数学角度看这是一个泛函最优化，多目标，看下目标函数：
其中i表示第i个样本，表示第i个样本的预测误差，误差越小越好，不然你算得上预测么？后面在这里描述表示树的复杂度的函数，越小复杂度越低，泛化能力越强，这意味着啥不用我多说。表达式为
直观上看，目标要求预测误差尽量小，叶子节点尽量少，节点数值尽量不极端（这个怎么看，如果某个样本label数值为4，那么第一个回归树预测3，第二个预测为1；另外一组回归树，一个预测2，一个预测2，那么倾向后一种，为什么呢？前一种情况，第一棵树学的太多，太接近4，也就意味着有较大的过拟合的风险）
ok，听起来很美好，可是怎么实现呢，上面这个目标函数跟实际的参数怎么联系起来，记得我们说过，回归树的参数:（1）选取哪个feature分裂节点呢；（2）节点的预测值（总不能靠取平均值这么粗暴不讲道理的方式吧，好歹高级一点）。上述形而上的公式并没有“直接”解决这两个，那么是如何间接解决的呢？
先说答案：贪心策略&#43;最优化（二次最优化）
通俗解释贪心策略：就是决策时刻按照当前目标最优化决定，说白了就是眼前利益最大化决定，“目光短浅”策略，他的优缺点细节大家自己去了解，经典背包问题等等。
这里是怎么用贪心策略的呢，刚开始你有一群样本，放在第一个节点，这时候T=1
如果这里的l(w−yi)误差表示用的是平方误差，那么上述函数就是一个关于w的二次函数求最小值，取最小值的点就是这个节点的预测值，最小的函数值为最小损失函数。
这里处理的就是二次函数最优化！
要是损失函数不是二次函数咋办，哦，泰勒展开式会否？，不是二次的想办法近似为二次。
接着来，接下来要选个feature分裂成两个节点，变成一棵弱小的树苗，那么需要：（1）确定分裂用的feature，how？最简单的是粗暴的枚举，选择loss function效果最好的那个（关于粗暴枚举，Xgboost的改良并行方式咱们后面看）；（2）如何确立节点的w
那么节奏是，选择一个feature分裂，计算loss function最小值，然后再选一个feature分裂，又得到一个loss function最小值…你枚举完，找一个效果最好的，把树给分裂，就得到了小树苗。在分裂的时候，你可以注意到，每次节点分裂，loss function被影响的只有这个节点的样本，因而每次分裂，计算分裂的增益（loss function的降低量）只需要关注打算分裂的那个节点的样本。
接下来，继续分裂，按照上述的方式，形成一棵树，再形成一棵树，每次在上一次的预测基础上取最优进一步分裂/建树，是不是贪心策略？！
凡是这种循环迭代的方式必定有停止条件，什么时候停止呢：
（1）当引入的分裂带来的增益小于一个阀值的时候，我们可以剪掉这个分裂，所以并不是每一次分裂loss function整体都会增加的，有点预剪枝的意思，阈值参数为 γ 正则项里叶子节点数 T 的系数；
（2）当树达到最大深度时则停止建立决策树，设置一个超参数max_depth，这个好理解吧，树太深很容易出现的情况学习局部样本，过拟合；
（3）当样本权重和小于设定阈值时则停止建树，这个解释一下，涉及到一个超参数-最小的样本权重和min_child_weight，和GBM的 min_child_leaf 参数类似，但不完全一样，大意就是一个叶子节点样本太少了，也终止同样是过拟合；
（4）貌似看到过有树的最大数量的…这个不确定
那节点分裂的时候是按照哪个顺序来的，比如第一次分裂后有两个叶子节点，先裂哪一个？答：同一层级的（多机）并行，确立如何分裂或者不分裂成为叶子节点
4.原理推导 上面一部分我们知道了集成学习方法的预测模型，因为XGBoost也是集成学习方法的一种。对于XGBoost的预测模型同样可以表示为：
其中K为树的总个数，表fk示第k颗树，y表示样本x的预测结果。
其中损失函数也同样表示为：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/64292903922bb4728890de21f0e54d28/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-04-10T21:38:25+08:00" />
<meta property="article:modified_time" content="2019-04-10T21:38:25+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">XGBoost</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>LeetCode题目记录</h4> 
 <ul><li><ul><li><a href="#1XGBoost_4" rel="nofollow">1.XGBoost概念</a></li><li><a href="#2_27" rel="nofollow">2.集成思想</a></li><li><a href="#3XGboost_45" rel="nofollow">3.分析XGboost思路</a></li><li><a href="#4_81" rel="nofollow">4.原理推导</a></li><li><a href="#5_163" rel="nofollow">5.正则化</a></li><li><a href="#6_171" rel="nofollow">6.优缺点</a></li><li><a href="#7_sklearn__176" rel="nofollow">7. sklearn 参数</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h3><a id="1XGBoost_4"></a>1.XGBoost概念</h3> 
<p>XGBoost全名叫（eXtreme Gradient Boosting）极端梯度提升，经常被用在一些比赛中，其效果显著。它是大规模并行boosted tree的工具，它是目前最快最好的开源boosted tree工具包。XGBoost 所应用的算法就是 GBDT（gradient boosting decision tree）的改进，既可以用于分类也可以用于回归问题中。</p> 
<p>1、回归树与决策树</p> 
<p>事实上，分类与回归是一个型号的东西，只不过分类的结果是离散值，回归是连续的，本质是一样的，都是特征（feature）到结果/标签（label）之间的映射。说说决策树和回归树，在上面决策树的讲解中相信决策树分类已经很好理解了。</p> 
<p>分类树的样本输出（即响应值）是类的形式，如判断蘑菇是有毒还是无毒，周末去看电影还是不去。而回归树的样本输出是数值的形式，比如给某人发放房屋贷款的数额就是具体的数值，可以是0到120万元之间的任意值。</p> 
<p>那么，这时候你就没法用上述的信息增益、信息增益率、基尼系数来判定树的节点分裂了，你就会采用新的方式，预测误差，常用的有均方误差、对数误差等。而且节点不再是类别，是数值（预测值），那么怎么确定呢，有的是节点内样本均值，有的是最优化算出来的比如Xgboost。</p> 
<p>2、boosting集成学习</p> 
<p>boosting集成学习，由多个相关联的决策树联合决策，什么叫相关联，举个例子，有一个样本[数据-&gt;标签]是[(2，4，5)-&gt; 4]，第一棵决策树用这个样本训练得预测为3.3，那么第二棵决策树训练时的输入，这个样本就变成了[(2，4，5)-&gt; 0.7]，也就是说，下一棵决策树输入样本会与前面决策树的训练和预测相关。</p> 
<p>与之对比的是random foreast（随机森林）算法，各个决策树是独立的、每个决策树在样本堆里随机选一批样本，随机选一批特征进行独立训练，各个决策树之间没有啥毛线关系。</p> 
<p>所以首先Xgboost首先是一个boosting的集成学习，这样应该很通俗了</p> 
<p>3、这个时候大家就能感觉到一个回归树形成的关键点：（1）分裂点依据什么来划分（如前面说的均方误差最小，loss）；（2）分类后的节点预测值是多少（如前面说，有一种是将叶子节点下各样本实际值得均值作为叶子节点预测误差，或者计算所得）</p> 
<h3><a id="2_27"></a>2.集成思想</h3> 
<p>在学习XGBoost之前，我们得需要先明白集成思想。集成学习方法是指将多个学习模型组合，以获得更好的效果，使组合后的模型具有更强的泛化能力。另外XGBoost是以分类回归树(CART树)进行组合。故在此之前，我们先看下CART树(CART树具体原理请自行复习，或者可以留言)。如下，通过输入用户年龄、性别进行判断用户是否喜欢玩游戏的得分值。由此得到一颗CART树模型。<br> <img src="https://images2.imgbox.com/3b/53/q69TzQWI_o.png" alt="在这里插入图片描述"></p> 
<p>我们知道对于单个的决策树模型容易出现过拟合，并且不能在实际中有效应用。所以出现了集成学习方法。如下图，通过两棵树组合进行玩游戏得分值预测。其中tree1中对小男生的预测分值为2，tree2对小男生的预测分值为0.9。则该小男生的最后得分值为2.9。</p> 
<p><img src="https://images2.imgbox.com/c5/bb/kvnz8WEK_o.png" alt="在这里插入图片描述"></p> 
<p>将上面集成学习方法推广到一般情况，可知其预测模型为：</p> 
<p><img src="https://images2.imgbox.com/18/d3/zXYuAKnS_o.png" alt="在这里插入图片描述"><br> 其中为树的总个数，表示第颗树，表示样本的预测结果。</p> 
<p>损失函数为：<br> <img src="https://images2.imgbox.com/91/0a/j3vBtoYR_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="3XGboost_45"></a>3.分析XGboost思路</h3> 
<p>首先明确下我们的目标，希望建立K个回归树，使得树群的预测值尽量接近真实值（准确率）而且有尽量大的泛化能力（更为本质的东西），从数学角度看这是一个泛函最优化，多目标，看下目标函数：<br> <img src="https://images2.imgbox.com/ff/19/W80tCsJ9_o.png" alt="在这里插入图片描述"><br> 其中i表示第i个样本，表示第i个样本的预测误差，误差越小越好，不然你算得上预测么？后面在这里<img src="https://images2.imgbox.com/a3/99/nz7GomjU_o.png" alt="插入图片">描述表示树的复杂度的函数，越小复杂度越低，泛化能力越强，这意味着啥不用我多说。表达式为</p> 
<p><img src="https://images2.imgbox.com/5f/48/u2e2jvxj_o.png" alt="在这里插入图片描述"><br> 直观上看，目标要求预测误差尽量小，叶子节点尽量少，节点数值尽量不极端（这个怎么看，如果某个样本label数值为4，那么第一个回归树预测3，第二个预测为1；另外一组回归树，一个预测2，一个预测2，那么倾向后一种，为什么呢？前一种情况，第一棵树学的太多，太接近4，也就意味着有较大的过拟合的风险）</p> 
<p>ok，听起来很美好，可是怎么实现呢，上面这个目标函数跟实际的参数怎么联系起来，记得我们说过，回归树的参数:（1）选取哪个feature分裂节点呢；（2）节点的预测值（总不能靠取平均值这么粗暴不讲道理的方式吧，好歹高级一点）。上述形而上的公式并没有“直接”解决这两个，那么是如何间接解决的呢？</p> 
<p>先说答案：贪心策略+最优化（二次最优化）</p> 
<p>通俗解释贪心策略：就是决策时刻按照当前目标最优化决定，说白了就是眼前利益最大化决定，“目光短浅”策略，他的优缺点细节大家自己去了解，经典背包问题等等。</p> 
<p>这里是怎么用贪心策略的呢，刚开始你有一群样本，放在第一个节点，这时候T=1</p> 
<p>如果这里的l(w−yi)误差表示用的是平方误差，那么上述函数就是一个关于w的二次函数求最小值，取最小值的点就是这个节点的预测值，最小的函数值为最小损失函数。</p> 
<p>这里处理的就是二次函数最优化！<br> 　　要是损失函数不是二次函数咋办，哦，泰勒展开式会否？，不是二次的想办法近似为二次。</p> 
<p>接着来，接下来要选个feature分裂成两个节点，变成一棵弱小的树苗，那么需要：（1）确定分裂用的feature，how？最简单的是粗暴的枚举，选择loss function效果最好的那个（关于粗暴枚举，Xgboost的改良并行方式咱们后面看）；（2）如何确立节点的w</p> 
<p>那么节奏是，选择一个feature分裂，计算loss function最小值，然后再选一个feature分裂，又得到一个loss function最小值…你枚举完，找一个效果最好的，把树给分裂，就得到了小树苗。在分裂的时候，你可以注意到，每次节点分裂，loss function被影响的只有这个节点的样本，因而每次分裂，计算分裂的增益（loss function的降低量）只需要关注打算分裂的那个节点的样本。</p> 
<p>接下来，继续分裂，按照上述的方式，形成一棵树，再形成一棵树，每次在上一次的预测基础上取最优进一步分裂/建树，是不是贪心策略？！</p> 
<p>凡是这种循环迭代的方式必定有停止条件，什么时候停止呢：<br> 　　（1）当引入的分裂带来的增益小于一个阀值的时候，我们可以剪掉这个分裂，所以并不是每一次分裂loss function整体都会增加的，有点预剪枝的意思，阈值参数为 γ 正则项里叶子节点数 T 的系数；<br> 　　（2）当树达到最大深度时则停止建立决策树，设置一个超参数max_depth，这个好理解吧，树太深很容易出现的情况学习局部样本，过拟合；<br> 　　（3）当样本权重和小于设定阈值时则停止建树，这个解释一下，涉及到一个超参数-最小的样本权重和min_child_weight，和GBM的 min_child_leaf 参数类似，但不完全一样，大意就是一个叶子节点样本太少了，也终止同样是过拟合；<br> 　　（4）貌似看到过有树的最大数量的…这个不确定<br> 　　那节点分裂的时候是按照哪个顺序来的，比如第一次分裂后有两个叶子节点，先裂哪一个？答：同一层级的（多机）并行，确立如何分裂或者不分裂成为叶子节点</p> 
<h3><a id="4_81"></a>4.原理推导</h3> 
<p>上面一部分我们知道了集成学习方法的预测模型，因为XGBoost也是集成学习方法的一种。对于XGBoost的预测模型同样可以表示为：</p> 
<p><img src="https://images2.imgbox.com/48/b0/1wDl8XPT_o.png" alt="在这里插入图片描述"><br> 其中K为树的总个数，表fk示第k颗树，y表示样本x的预测结果。</p> 
<p>其中损失函数也同样表示为：</p> 
<p><img src="https://images2.imgbox.com/71/d1/ZCcvNpwA_o.png" alt="在这里插入图片描述"></p> 
<p>其中l为样本x的训练误差，在这里<img src="https://images2.imgbox.com/1f/d5/s3k6UECq_o.png" alt="在这里插入图片描述"> 描述表示第k棵树的正则项。</p> 
<p>看到了这里，我们可能会想到，现在知道了模型预测函数和损失函数，那我们是不是直接就能求出其预测模型了呢？答案肯定不是，我们首先需要明确知道优化和求解的参数是什么呢？由上面的预测模型中，我们可以看到对于每棵树的预测值是如何计算的？想到这里，你就已经知道了需要做的事了，我需要求解和优化的就是每个叶子节点的得分值，也就是的值。另外我们知道XGBoost是以CART树中的回归树作为基分类器，在给定训练数据后，其单个树的结构(叶子节点个数、树深度等等)基本可以确定了。但XGBoost并不是简单重复的将几个CART树进行组合。它是一种加法模型，将模型上次预测(由t-1棵树组合而成的模型)产生的误差作为参考进行下一棵树(第t棵树)的建立。以此，每加入一棵树，将其损失函数不断降低。如下图就为加法模型案例，它将模型预测值与实际值残差作为下一颗树的输入数据。</p> 
<p><img src="https://images2.imgbox.com/0d/54/PxYQp3CK_o.png" alt="在这里插入图片描述"></p> 
<p>对于加法策略可以表示如下：<img src="https://images2.imgbox.com/33/ad/mNABhIWi_o.png" alt="在这里插入图片描述"></p> 
<p>初始化(模型中没有树时，其预测结果为0)：</p> 
<p>往模型中加入第一棵树：<img src="https://images2.imgbox.com/87/fd/ScCQz9SL_o.png" alt="在这里插入图片描述"></p> 
<p>往模型中加入第二棵树：<img src="https://images2.imgbox.com/55/f2/IjSLjTFa_o.png" alt="在这里插入图片描述"></p> 
<p>…</p> 
<p>往模型中加入第t棵树：<img src="https://images2.imgbox.com/57/7f/USYUYOpO_o.png" alt="在这里插入图片描述"></p> 
<p>我们知道，每次往模型中加入一棵树，其损失函数便会发生变化。另外在加入第t棵树时，则前面第t-1棵树已经训练完成，此时前面t-1棵树的正则项和训练误差都成已知常数项。对于每棵树的正则项部分，我们将在后面再细说。<br> <img src="https://images2.imgbox.com/c9/53/k2k01nfd_o.png" alt="在这里插入图片描述"><br> 如果损失函数采用均方误差时，其目标损失函数变为：</p> 
<p><img src="https://images2.imgbox.com/1e/bc/V21WblrY_o.png" alt="在这里插入图片描述"></p> 
<p>另外对于目标损失函数中的正则项(复杂度)部分，我们从单一的树来考虑。对于其中每一棵回归树，其模型可以写成：<br> <img src="https://images2.imgbox.com/6a/9a/WzaCEcaX_o.png" alt="在这里插入图片描述"><br> 因此，在这里。我们将该树的复杂度写成：</p> 
<p><img src="https://images2.imgbox.com/53/0e/YqxycDuT_o.png" alt="在这里插入图片描述"></p> 
<p>复杂度计算例子如下：<br> <img src="https://images2.imgbox.com/59/db/jJ2WDOXA_o.png" alt="在这里插入图片描述"></p> 
<p>此时，对于XGBoost的目标函数我们可以写为：</p> 
<p><img src="https://images2.imgbox.com/e4/28/JjCKo3Oe_o.png" alt="在这里插入图片描述"><br> 现在我们只需要找到f(t)来优化上式目标。</p> 
<p><img src="https://images2.imgbox.com/35/60/6Kp7xaYc_o.png" alt="在这里插入图片描述"><br> 在推导之前，我们先介绍下泰勒展开式：</p> 
<p><img src="https://images2.imgbox.com/68/4e/EviWKpoT_o.png" alt="在这里插入图片描述"></p> 
<p>这里我们用泰勒展开式来近似原来的目标函数。则原目标函数可以写成：<br> <img src="https://images2.imgbox.com/64/67/beFSfa6s_o.png" alt="在这里插入图片描述"></p> 
<p>令<img src="https://images2.imgbox.com/19/52/uSSEJeNA_o.png" alt="在这里插入图片描述">，<img src="https://images2.imgbox.com/9f/e5/PUggL0Ya_o.png" alt="在这里插入图片描述">，同时对于第t棵树时，<img src="https://images2.imgbox.com/47/e7/jSfnwBdD_o.png" alt="在这里插入图片描述">为常数。同时去除所有常数项。故目标损失函数可以写成：</p> 
<p><img src="https://images2.imgbox.com/56/3a/26k9el0P_o.png" alt="在这里插入图片描述"></p> 
<p>则：<br> <img src="https://images2.imgbox.com/ba/54/WFBIvU6A_o.png" alt="在这里插入图片描述"><br> 对w求偏导，并使其导函数等于0，则有：<br> <img src="https://images2.imgbox.com/89/44/gnQSzmRK_o.png" alt="在这里插入图片描述"></p> 
<p>求解得：<img src="https://images2.imgbox.com/75/68/HphVHfvB_o.png" alt="在这里插入图片描述"></p> 
<p>其目标函数可以为：<img src="https://images2.imgbox.com/b8/13/PZZp9vJp_o.png" alt="在这里插入图片描述"><br> 根据目标函数，如何分裂样本数据呢</p> 
<p><img src="https://images2.imgbox.com/0a/12/6t9pMmjX_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/0a/30/rsZqBrzM_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="5_163"></a>5.正则化</h3> 
<h3><a id="6_171"></a>6.优缺点</h3> 
<h3><a id="7_sklearn__176"></a>7. sklearn 参数</h3> 
<p>参考文献：<br> [1]:<a href="https://blog.csdn.net/Kaiyuan_sjtu/article/details/80012063">https://blog.csdn.net/Kaiyuan_sjtu/article/details/80012063</a><br> [2]:<a href="https://blog.csdn.net/Kaiyuan_sjtu/article/details/80018580">https://blog.csdn.net/Kaiyuan_sjtu/article/details/80018580</a><br> [3]:<a href="https://blog.csdn.net/Guiabbey/article/details/89085101">https://blog.csdn.net/Guiabbey/article/details/89085101</a><br> [4]:<a href="https://blog.csdn.net/u013239656/article/details/89190224">https://blog.csdn.net/u013239656/article/details/89190224</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c88dcc0c50539e91eedce2496eb6756e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">OpenVX, 运算加速库, NVIDIA</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/3a4cb2aa58bd3332a64bf77a83ec7582/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">VC&#43;&#43;学习-数据库篇（记录集recordset常用操作代码）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>