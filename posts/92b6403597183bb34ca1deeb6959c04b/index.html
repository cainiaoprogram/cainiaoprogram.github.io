<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>主流卷积神经网络结构探索和分析（一） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="主流卷积神经网络结构探索和分析（一）" />
<meta property="og:description" content="主流卷积神经网络结构探索和分析（一） 0. 引言1. 主流卷积网络概览1.1 VGG1.2 GoogLeNet1.3 ResNet1.4 WideResNet1.5 ResNeXt1.6 总结和思考（待完善） 0. 引言 去年7月因为各种原因辞职后终于如愿进入高校做博后，完成多年的心愿。9月份正式入职，老师的20篇 论文的要求真是让我完全懵圈的状态。无论如何，硬着头皮上吧。于是，熬到现在第四篇了。也借此机会把主流的卷积网络的结构梳理梳理。借着新型冠装病毒疫情这个封闭在家的时间，静心思考，也把自己思考的内容与大家分享一下，抛砖引玉。
1. 主流卷积网络概览 目前参考了文章《CIFAR10 to Compare Visual Recognition Performance between Deep Neural》，感兴趣的小伙伴可以自行搜索谷歌学术。文章中将主流的CNN网络结构分为了五类：VGG，GoogLeNet, ResNet,WideResNet, ResNeXt. 本文对这几个网络进行的结构和特点进行详细的分析和讨论。
1.1 VGG GGNet是牛津大学计算机视觉组（Visual Geometry Group）和Google DeepMind公司的研究员一起研发的深度卷积神经网络。通过反复堆叠3X3的小型卷积核和2X2的最大池化层，通过不断加深网络结构来提升性能。每次池化后刚好缩小一半，信道数目不断增加一倍。
1.2 GoogLeNet GoogLeNet的主要思想是对网络进行抽取和分解。从V1到V4经历了四个版本的改进。思路上来说，主要是用较小的卷积核替代较大的卷积核。比如从V1到V2的核心思想可以认为是：用2个3x3的卷积核替代了5x5的卷积核；V2到V3，主要是引入了非对称卷积核的方法，将nxn的卷积核改进为：一个nx1和一个1xn的组合，但是作者在实验的过程中发现，该方法在高分辨率的低层特征中使用的效果不好，因此将非对称卷积放在了网络的中部。V3到V4的进步主要是引入了残差网络。
GoogleNet的作者还有对网络的其他改进，比如目标函数，用avgpooling层替代FC,用BN层替代droopout等改进。在此不一一赘述。相关内容可参考博客：Inception in CNN
1.3 ResNet ResNet是在2015年有何凯明，张翔宇，任少卿，孙剑共同提出的。ReNet的数学表达很简单，就是在网络之间增加跨层的连接。即在某一层之间实现如下的连接关系。其网络结构示意图如下图所示。
Y o u t l = f ( X o u t l ) &#43; X o u t l t Y_{out}^{l}=f(X_{out}^{l})&#43;X_{out}^{l_{t}} Youtl​=f(Xoutl​)&#43;Xoutlt​​
1.4 WideResNet 前面讨论的几个网络的思路主要是在网络的深度和广度方面做探索。而ResNet这类网络也会存在由于梯度在反向传播的时候，可以直接经过shortcut，而不用被强制经过residual block，这会导致可能只有很有限的layer学到了有用的知识，而更多的layers对最终结果只做出了很少的贡献。这个问题也被作者称之为diminishing feature reuse。WideResNet和ResNeXt都是对residual block的改进。WideResNet主要是在卷积层之间加入了随机失活的droop out层。其网络结构示意图如下图所示。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/92b6403597183bb34ca1deeb6959c04b/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-02-08T17:10:59+08:00" />
<meta property="article:modified_time" content="2020-02-08T17:10:59+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">主流卷积神经网络结构探索和分析（一）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>主流卷积神经网络结构探索和分析（一）</h4> 
 <ul><li><a href="#0__2" rel="nofollow">0. 引言</a></li><li><a href="#1__6" rel="nofollow">1. 主流卷积网络概览</a></li><li><ul><li><a href="#11_VGG_10" rel="nofollow">1.1 VGG</a></li><li><a href="#12_GoogLeNet_12" rel="nofollow">1.2 GoogLeNet</a></li><li><a href="#13_ResNet_15" rel="nofollow">1.3 ResNet</a></li><li><a href="#14_WideResNet_21" rel="nofollow">1.4 WideResNet</a></li><li><a href="#15_ResNeXt_24" rel="nofollow">1.5 ResNeXt</a></li><li><a href="#16__29" rel="nofollow">1.6 总结和思考（待完善）</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="0__2"></a>0. 引言</h2> 
<p>去年7月因为各种原因辞职后终于如愿进入高校做博后，完成多年的心愿。9月份正式入职，老师的20篇 论文的要求真是让我完全懵圈的状态。无论如何，硬着头皮上吧。于是，熬到现在第四篇了。也借此机会把主流的卷积网络的结构梳理梳理。借着新型冠装病毒疫情这个封闭在家的时间，静心思考，也把自己思考的内容与大家分享一下，抛砖引玉。</p> 
<h2><a id="1__6"></a>1. 主流卷积网络概览</h2> 
<p>目前参考了文章《CIFAR10 to Compare Visual Recognition Performance between Deep Neural》，感兴趣的小伙伴可以自行搜索谷歌学术。文章中将主流的CNN网络结构分为了五类：VGG，GoogLeNet, ResNet,WideResNet, ResNeXt. 本文对这几个网络进行的结构和特点进行详细的分析和讨论。</p> 
<h3><a id="11_VGG_10"></a>1.1 VGG</h3> 
<p>GGNet是牛津大学计算机视觉组（Visual Geometry Group）和Google DeepMind公司的研究员一起研发的深度卷积神经网络。通过反复堆叠3X3的小型卷积核和2X2的最大池化层，通过不断加深网络结构来提升性能。每次池化后刚好缩小一半，信道数目不断增加一倍。</p> 
<h3><a id="12_GoogLeNet_12"></a>1.2 GoogLeNet</h3> 
<p>GoogLeNet的主要思想是对网络进行抽取和分解。从V1到V4经历了四个版本的改进。思路上来说，主要是用较小的卷积核替代较大的卷积核。比如从V1到V2的核心思想可以认为是：用2个3x3的卷积核替代了5x5的卷积核；V2到V3，主要是引入了非对称卷积核的方法，将nxn的卷积核改进为：一个nx1和一个1xn的组合，但是作者在实验的过程中发现，该方法在高分辨率的低层特征中使用的效果不好，因此将非对称卷积放在了网络的中部。V3到V4的进步主要是引入了残差网络。<br> GoogleNet的作者还有对网络的其他改进，比如目标函数，用avgpooling层替代FC,用BN层替代droopout等改进。在此不一一赘述。相关内容可参考博客：<a href="https://blog.csdn.net/stdcoutzyx/article/details/51052847">Inception in CNN</a></p> 
<h3><a id="13_ResNet_15"></a>1.3 ResNet</h3> 
<p>ResNet是在2015年有何凯明，张翔宇，任少卿，孙剑共同提出的。ReNet的数学表达很简单，就是在网络之间增加跨层的连接。即在某一层之间实现如下的连接关系。其网络结构示意图如下图所示。<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           Y 
          
          
          
            o 
           
          
            u 
           
          
            t 
           
          
         
           l 
          
         
        
          = 
         
        
          f 
         
        
          ( 
         
         
         
           X 
          
          
          
            o 
           
          
            u 
           
          
            t 
           
          
         
           l 
          
         
        
          ) 
         
        
          + 
         
         
         
           X 
          
          
          
            o 
           
          
            u 
           
          
            t 
           
          
          
          
            l 
           
          
            t 
           
          
         
        
       
         Y_{out}^{l}=f(X_{out}^{l})+X_{out}^{l_{t}} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.14611em; vertical-align: -0.247em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.899108em;"><span class="" style="top: -2.453em; margin-left: -0.22222em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.247em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.14911em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.899108em;"><span class="" style="top: -2.453em; margin-left: -0.07847em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.247em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1.17676em; vertical-align: -0.245756em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.931008em;"><span class="" style="top: -2.45424em; margin-left: -0.07847em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span><span class="" style="top: -3.1449em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.296343em;"><span class="" style="top: -2.357em; margin-left: -0.01968em; margin-right: 0.0714286em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em;"><span class=""></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.245756em;"><span class=""></span></span></span></span></span></span></span></span></span></span></span><br> <img src="https://images2.imgbox.com/61/29/mJ3EibFL_o.png" alt=""></p> 
<h3><a id="14_WideResNet_21"></a>1.4 WideResNet</h3> 
<p>前面讨论的几个网络的思路主要是在网络的深度和广度方面做探索。而ResNet这类网络也会存在由于梯度在反向传播的时候，可以直接经过shortcut，而不用被强制经过residual block，这会导致可能只有很有限的layer学到了有用的知识，而更多的layers对最终结果只做出了很少的贡献。这个问题也被作者称之为diminishing feature reuse。WideResNet和ResNeXt都是对residual block的改进。WideResNet主要是在卷积层之间加入了随机失活的droop out层。其网络结构示意图如下图所示。<br> <img src="https://images2.imgbox.com/9a/3b/p2NET4Ur_o.jpg" alt="在这里插入图片描述"></p> 
<h3><a id="15_ResNeXt_24"></a>1.5 ResNeXt</h3> 
<p>为了打破或deeper，或wider的常规思路，ResNeXt则认为可以引入一个新维度，称之为cardinality。作者在实验上也证明了： increasing cardinality is more effective than going deeper or wider when we increase the capacity。<br> 实际上的思路应该类似与channel group的思想。下图显示了ResNeXt的思路：<br> <img src="https://images2.imgbox.com/48/a1/kD9aiF2Q_o.jpg" alt="在这里插入图片描述"><br> 其中划分为（256，1x1，4）（4，3x3,4）的32个卷积核共享参数，这样整个网络的参数量就不会增加了。</p> 
<h3><a id="16__29"></a>1.6 总结和思考（待完善）</h3> 
<p>从CNN的网络结构发展历程可以看出，网络结构不断以提高精度、提高收敛速度的目的不断变化。凝结了很多人不断的思考。这里我个人的理解和思考还不够深入，以后有新的思考的时候再补充。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/9fbc4d91c248f63f1d725f6cc10e0a8e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Python Matplotlib画图基础介绍</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/175173e68ba89e4bbe9d763e9572562c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Cube Stacking —— 带权并查集（终于搞懂了）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>