<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>轻量化网络：MobileNet-V2 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="轻量化网络：MobileNet-V2" />
<meta property="og:description" content="目录
MobileNet-V2 基本情况
创新点：
正文：
MobileNet-V2网络结构
MobileNet-V2 基本情况 《Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation》于2018年1月公开在arXiv(美[ˈɑ:rkaɪv]) ：https://arxiv.org/abs/1801.04381MobileNetV2是对MobileNetV1的改进，同样是一个轻量化卷积神经网络。 创新点： 1. Inverted residuals，通常的residuals block是先经过一个1*1的Conv layer，把feature map的通道数“压”下来，再经过3*3 Conv layer，最后经过一个1*1 的Conv layer，将feature map 通道数再“扩张”回去。即先“压缩”，最后“扩张”回去。
而 inverted residuals就是 先“扩张”，最后“压缩”。为什么这么做呢？请往下看。
2.Linear bottlenecks，为了避免Relu对特征的破坏，在residual block的Eltwise sum之前的那个 1*1 Conv 不再采用Relu，为什么？请往下看。
创新点全写在论文标题上了！
由于才疏学浅，对本论文理论部分不太明白，所以选取文中重要结论来说明MobileNet-V2。
先看看MobileNetV2 和 V1之间有啥不同
（原图链接）
主要是两点：
Depth-wise convolution之前多了一个1*1的“扩张”层，目的是为了提升通道数，获得更多特征；最后不采用Relu，而是Linear，目的是防止Relu破坏特征。 再看看MobileNetV2的block 与ResNet 的block：
（原图链接）
主要不同之处就在于，ResNet是：压缩”→“卷积提特征”→“扩张”，MobileNetV2则是Inverted residuals,即：“扩张”→“卷积提特征”→ “压缩”
正文： MobileNet-V1 最大的特点就是采用depth-wise separable convolution来减少运算量以及参数量，而在网络结构上，没有采用shortcut的方式。
Resnet及Densenet等一系列采用shortcut的网络的成功，表明了shortcut是个非常好的东西，于是MobileNet-V2就将这个好东西拿来用。
拿来主义，最重要的就是要结合自身的特点，MobileNet的特点就是depth-wise separable convolution，但是直接把depth-wise separable convolution应用到 residual block中，会碰到如下问题：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/120adf85f93c5cbf8ef4d9acbdc445aa/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-08-19T08:19:19+08:00" />
<meta property="article:modified_time" content="2020-08-19T08:19:19+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">轻量化网络：MobileNet-V2</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="MobileNet-V2%20%E5%9F%BA%E6%9C%AC%E6%83%85%E5%86%B5-toc" style="margin-left:0px;"><a href="#MobileNet-V2%20%E5%9F%BA%E6%9C%AC%E6%83%85%E5%86%B5" rel="nofollow">MobileNet-V2 基本情况</a></p> 
<p id="创新点-toc" style="margin-left:0px;"><a href="#%E5%88%9B%E6%96%B0%E7%82%B9" rel="nofollow">创新点：</a></p> 
<p id="正文-toc" style="margin-left:0px;"><a href="#%E6%AD%A3%E6%96%87" rel="nofollow">正文：</a></p> 
<p id="mobilenet-v2网络结构-toc" style="margin-left:0px;"><a href="#mobilenet-v2%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84" rel="nofollow">MobileNet-V2网络结构</a></p> 
<hr id="hr-toc"> 
<h2 id="MobileNet-V2%20%E5%9F%BA%E6%9C%AC%E6%83%85%E5%86%B5">MobileNet-V2 基本情况</h2> 
<ul><li>《Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation》</li><li>于2018年1月公开在arXiv(美[ˈɑ:rkaɪv]) ：<a href="https://arxiv.org/abs/1801.04381" rel="nofollow">https://arxiv.org/abs/1801.04381</a></li><li>MobileNetV2是对<a href="http://blog.csdn.net/u011995719/article/details/78850275">MobileNetV1</a>的改进，同样是一个轻量化卷积神经网络。</li></ul> 
<h2 id="创新点"><a name="t0"></a><span style="color:#ff0000;">创新点：</span></h2> 
<p><strong>1.</strong> Inverted residuals，通常的residuals block是先经过一个1*1的Conv layer，把feature map的通道数“压”下来，再经过3*3 Conv layer，最后经过一个1*1 的Conv layer，将feature map 通道数再“扩张”回去。即先“压缩”，最后“扩张”回去。<br> 而 inverted residuals就是 先“扩张”，最后“压缩”。为什么这么做呢？请往下看。</p> 
<p><strong>2</strong>.Linear bottlenecks，为了避免Relu对特征的破坏，在residual block的Eltwise sum之前的那个 1*1 Conv 不再采用Relu，为什么？请往下看。</p> 
<p>创新点全写在论文标题上了！</p> 
<p>由于才疏学浅，对本论文理论部分不太明白，所以选取文中重要结论来说明MobileNet-V2。</p> 
<p>先看看MobileNetV2 和 V1之间有啥不同<br> （<a href="https://zhuanlan.zhihu.com/p/33075914" rel="nofollow">原图链接</a>）<br><img alt="这里写图片描述" src="https://images2.imgbox.com/70/49/eA8DSKtE_o.png"></p> 
<p>主要是两点：</p> 
<ol><li>Depth-wise convolution之前多了一个1*1的“扩张”层，目的是为了提升通道数，获得更多特征；</li><li>最后不采用Relu，而是Linear，目的是防止Relu破坏特征。</li></ol> 
<p>再看看MobileNetV2的block 与ResNet 的block：<br> （<a href="https://zhuanlan.zhihu.com/p/33075914" rel="nofollow">原图链接</a>）<br><img alt="这里写图片描述" src="https://images2.imgbox.com/27/56/exVROtgd_o.png"><br> 主要不同之处就在于，ResNet是：压缩”→“卷积提特征”→“扩张”，MobileNetV2则是Inverted residuals,即：“扩张”→“卷积提特征”→ “压缩”</p> 
<h2 id="正文"><a name="t1"></a><span style="color:#ff0000;">正文：</span></h2> 
<p>MobileNet-V1 最大的特点就是采用depth-wise separable convolution来减少运算量以及参数量，而在网络结构上，没有采用shortcut的方式。<br> Resnet及Densenet等一系列采用shortcut的网络的成功，表明了shortcut是个非常好的东西，于是MobileNet-V2就将这个好东西拿来用。</p> 
<p>拿来主义，最重要的就是要结合自身的特点，MobileNet的特点就是depth-wise separable convolution，但是直接把depth-wise separable convolution应用到 residual block中，会碰到如下问题：</p> 
<p>1.DWConv layer层提取得到的特征受限于输入的通道数，若是采用以往的residual block，先“压缩”，再卷积提特征，那么DWConv layer可提取得特征就太少了，因此一开始不“压缩”，MobileNetV2反其道而行，一开始先“扩张”，本文实验“扩张”倍数为6。 通常residual block里面是 “压缩”→“卷积提特征”→“扩张”，MobileNetV2就变成了 “扩张”→“卷积提特征”→ “压缩”，因此称为<span style="color:#ff0000;"><strong><em>Inverted residuals</em></strong></span></p> 
<p>2.当采用“扩张”→“卷积提特征”→ “压缩”时，在“压缩”之后会碰到一个问题，那就是Relu会破坏特征。为什么这里的Relu会破坏特征呢？这得从Relu的性质说起，Relu对于负的输入，输出全为零；而本来特征就已经被“压缩”，再经过Relu的话，又要“损失”一部分特征，因此这里不采用Relu，实验结果表明这样做是正确的，这就称为<span style="color:#ff0000;"><strong><em>Linear bottlenecks</em></strong></span></p> 
<h2 id="mobilenet-v2网络结构"><a name="t2"></a><span style="color:#ff0000;">MobileNet-V2网络结构</span></h2> 
<p>附赠苏师兄的prototxt：<br><a href="https://github.com/suzhenghang/MobileNetv2/tree/master/.gitignore">https://github.com/suzhenghang/MobileNetv2/tree/master/.gitignore</a><br> 另外一位朋友的prototxt：<br><a href="https://github.com/austingg/MobileNet-v2-caffe">https://github.com/austingg/MobileNet-v2-caffe</a></p> 
<p><img alt="这里写图片描述" height="418" src="https://images2.imgbox.com/60/3b/LyyQIBUx_o.png" width="545"></p> 
<p>其中：t表示“扩张”倍数，c表示输出通道数，n表示重复次数，s表示步长stride。<br> 先说两点有误之处吧：<br> 1. 第五行，也就是第7~10个bottleneck，stride=2，分辨率应该从28降低到14；如果不是分辨率出错，那就应该是stride=1；<br> 2. 文中提到共计采用19个bottleneck，但是这里只有17个。</p> 
<p>Conv2d 和avgpool和传统CNN里的操作一样；最大的特点是bottleneck，一个bottleneck由如下三个部分构成：</p> 
<p><img alt="这里写图片描述" height="137" src="https://images2.imgbox.com/d6/4b/3X9L52nH_o.png" width="614"></p> 
<p>这就是之前提到的inverted residuals结构，一个inverted residuals结构的Multiply Add=<br> h*w*d’ * 1*1*td’ +<br> h*w*td’ * k*k*1 +<br> h*w*t d’ * 1*1*d” =<br><span style="color:#ff0000;">h*w*d’*t(d’+ k*k + d”)</span></p> 
<p>特别的，针对stride=1 和stride=2，在block上有稍微不同，主要是为了与shortcut的维度匹配，因此，stride=2时，不采用shortcut。 具体如下图：<br><img alt="这里写图片描述" height="417" src="https://images2.imgbox.com/b3/8a/zUA9kJkC_o.png" width="386"></p> 
<p>可以发现，除了最后的avgpool，整个网络并没有采用pooling进行下采样，而是利用stride=2来下采样，此法已经成为主流，不知道是否pooling层对速度有影响，因此舍弃pooling层?是否有朋友知道那篇论文里提到这个操作？</p> 
<p>看看MobileNet-V2 分类时，inference速度：</p> 
<p><img alt="这里写图片描述" height="217" src="https://images2.imgbox.com/a2/4c/7akeld6B_o.png" width="509"></p> 
<p>这是在手机的CPU上跑出来的结果(Google pixel 1 for TF-Lite)</p> 
<p>同时还进行了目标检测和图像分割实验，效果都不错，详细请看原文。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6dd456d58589ef1fab74f0228128fc1b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">线程池ThreadPoolExecutor--Executors.newFixedThreadPool()</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/36873f51d5df4ec07ba1a6119ca1ca80/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【SSL1382】车</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>