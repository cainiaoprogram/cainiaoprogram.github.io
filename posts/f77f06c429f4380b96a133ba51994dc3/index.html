<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>论文阅读：Towards Open-vocabulary Scene Graph Generation with Prompt-based Finetuning(ECCV22) - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="论文阅读：Towards Open-vocabulary Scene Graph Generation with Prompt-based Finetuning(ECCV22)" />
<meta property="og:description" content="第一次看这种prompt的论文，虽然好久没写论文笔记了但是这篇有必要记录一下()
pipeline如下：
先看左边的Pretraining VRM
这部分分为两个encoder，分别是上方的图像encoder(εi)和下方的文本encoder(εt)
1.图像encoder
图像encoder分为两个部分，分别是区域proposal特征提取器(Faster RCNN)和关系Transformer(RelTrans)
对于一对主客体对(s-o)，使用关系Transformer对其编码，编码过程如下：
其中，rt是主客体对中左上角的区域，rb是右下角的区域，[r1,…rm]是与主客体对的union box有重叠的其他区域。区域来自Faster RCNN的输出。l是每个区域的位置embedding。经过关系Transformer的编码后，得到了[ht,h1,…hm,hb]的编码结果，我们需要的是ht和hb（即hs和ho）。
对于主客体对的union box，也需要用关系Transformer对其编码，编码过程跟上面的公式基本一样，只是将rt和rb换成了rso(即主客体的union box)，编码结果为[hso,h1,…,hm]，我们需要的是hso。然后对hso再执行下面的公式，得到hr。hr计算公式如下：
综上，hs，ho和hr是图像encoder的输出
2.文本encoder
这里应该用的是预训练好的文本encoder。使用的训练数据是VG里的dense caption，每张图像平均有50个caption，只是描述对应区域的，如下图左侧的框。
文本encoder也分为两个部分，先是embedding，再是文本Transformer，前面的embedding我觉得是下式的W2。下式的[w1,w2,…wk]是第i个caption的单词。l’是每个单词的位置embedding。[CLS]和[EOF]是可学习的特殊token，表示第一个单词和最后一个单词。ei是对第i个caption的编码结果。
综上，ei是文本encoder的输出
3.预训练的loss
loss分为三个部分：
（1）Lmrl，针对图像encoder里的关系Transformer，将输入的任一区域用特殊标记[mask]代替，通过对比损失使h[mask]尽可能和h[ground truth]接近，训练的是图像encoder
（2）Lmtl，针对文本encoder里的文本Transformer，使用的交叉熵，也是将任一token mask掉，训练的是文本encoder
（3）Lc，图像encoder的输出和文本encoder的输出尽可能接近，使用的余弦对比损失
至此，预训练结束。在后续的微调过程中，这两个encoder的参数不更新。
再看右边的基于prompt的微调
1.基于hard prompt的微调
模板如下，[CLS]表示第一个单词，[SEP]是句子和句子之间的分隔词。xs是主语的类别，xo是宾语的类别，[mask]是给谓语类别留出来的slot。
谓语的预测公式如下式所示：
其中，模板是Xpro，y是caption中的单词，用M()把它从caption的单词集合中映射到了谓语类别的集合中，ffill()是已经填充完slot的prompt。hr是关系Transformer里的，计算方式为hr=LN(hs,hso,ho)。θ是线性投影函数LN的参数，只有这部分参数更新。P()是得分函数，y_hat是置信度最高的谓语。
得分函数计算如下：
其中，ein( p )是p这个prompt经过文本Transformer的编码结果。这个公式是评估主客体的视觉区域和主语-谓语-宾语这种形式的prompt的余弦相似度，然后做了一个softmax。q是所有填充完的prompt。
1.基于soft视觉-关系prompt的微调
模板如下：
其中，xs’,…,xo’来自下图的V2T decoder。注意下图的εi/εt和预训练VRM那里的εi/εt是一个东西。
谓语的预测公式如下式所示：
相比hard prompt，少了hr。θ‘是V2T decoder的参数，只有这部分参数更新。
得分函数P如下：
其中，wr是数据集的谓语类别经过文本encoder的embedding，e[mask]是prompt中[mask]这个token的文本encoder输出
------------------------------------------------一些碎碎念----------------------------------------------------
在做新实验了
想方设法证明加的都是有用的()
最近沉迷史莱姆的视频
但是史莱姆都好贵啊。
感觉我的实验就像做史莱姆一样
加一堆胶水 搅和搅和 面多了加水水多了加面(bushi
最后变成一块()" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/f77f06c429f4380b96a133ba51994dc3/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-03-24T21:12:01+08:00" />
<meta property="article:modified_time" content="2023-03-24T21:12:01+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">论文阅读：Towards Open-vocabulary Scene Graph Generation with Prompt-based Finetuning(ECCV22)</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>第一次看这种prompt的论文，虽然好久没写论文笔记了但是这篇有必要记录一下()</p> 
<p>pipeline如下：<img src="https://images2.imgbox.com/05/e1/HFAzg3y5_o.png" alt="在这里插入图片描述"><br> <strong>先看左边的<font color="red">Pretraining</font> VRM</strong><br> 这部分分为两个encoder，分别是上方的图像encoder(εi)和下方的文本encoder(εt)</p> 
<p><font color="red"><strong>1.图像encoder</strong></font><br> 图像encoder分为两个部分，分别是区域proposal特征提取器(Faster RCNN)和关系Transformer(RelTrans)<br> <img src="https://images2.imgbox.com/99/1c/1rK22XTc_o.png" width="50%"><br> <strong>对于一对主客体对(s-o)</strong>，使用关系Transformer对其编码，编码过程如下：<br> <img src="https://images2.imgbox.com/f5/62/yjvGvZ1o_o.png" width="50%"><br> 其中，rt是主客体对中左上角的区域，rb是右下角的区域，[r1,…rm]是与主客体对的union box有重叠的其他区域。区域来自Faster RCNN的输出。l是每个区域的位置embedding。经过关系Transformer的编码后，得到了[ht,h1,…hm,hb]的编码结果，我们需要的是ht和hb（即hs和ho）。</p> 
<p><strong>对于主客体对的union box</strong>，也需要用关系Transformer对其编码，编码过程跟上面的公式基本一样，只是将rt和rb换成了rso(即主客体的union box)，编码结果为[hso,h1,…,hm]，我们需要的是hso。然后对hso再执行下面的公式，得到hr。hr计算公式如下：<br> <img src="https://images2.imgbox.com/e5/2f/w2BP4gL9_o.png" width="30%"><br> <strong>综上，hs，ho和hr是图像encoder的输出</strong><br> <br><br> <font color="red"><strong>2.文本encoder</strong></font><br> 这里应该用的是预训练好的文本encoder。使用的训练数据是VG里的dense caption，每张图像平均有50个caption，只是描述对应区域的，如下图左侧的框。<br> <img src="https://images2.imgbox.com/ed/79/2QRoswpp_o.png" width="50%"><br> 文本encoder也分为两个部分，先是embedding，再是文本Transformer，前面的embedding我觉得是下式的W2。下式的[w1,w2,…wk]是第i个caption的单词。l’是每个单词的位置embedding。[CLS]和[EOF]是可学习的特殊token，表示第一个单词和最后一个单词。ei是对第i个caption的编码结果。<br> <img src="https://images2.imgbox.com/c0/05/ap5qQ02g_o.png" width="60%"><br> <strong>综上，ei是文本encoder的输出</strong><br> <br><br> <font color="red"><strong>3.预训练的loss</strong></font><br> <img src="https://images2.imgbox.com/d8/62/oLtaX5At_o.png" width="40%"><br> loss分为三个部分：<br> （1）Lmrl，针对<strong>图像encoder里的关系Transformer</strong>，将输入的任一区域用特殊标记[mask]代替，通过对比损失使h[mask]尽可能和h[ground truth]接近，训练的是图像encoder<br> （2）Lmtl，针对<strong>文本encoder里的文本Transformer</strong>，使用的交叉熵，也是将任一token mask掉，训练的是文本encoder<br> （3）Lc，图像encoder的输出和文本encoder的输出尽可能接近，使用的余弦对比损失</p> 
<p><font color="red">至此，预训练结束。在后续的微调过程中，这两个encoder的参数不更新。</font></p> 
<p><br> <strong>再看右边的基于prompt的微调</strong><br> <font color="red"><strong>1.基于hard prompt的微调</strong></font><br> 模板如下，[CLS]表示第一个单词，[SEP]是句子和句子之间的分隔词。xs是主语的类别，xo是宾语的类别，[mask]是给谓语类别留出来的slot。<br> <img src="https://images2.imgbox.com/2b/6a/QpQZ6YNO_o.png" width="50%"><br> 谓语的预测公式如下式所示：<br> <img src="https://images2.imgbox.com/36/33/s0n6ZUAq_o.png" width="60%"><br> 其中，模板是Xpro，y是caption中的单词，用M()把它从caption的单词集合中映射到了谓语类别的集合中，ffill()是已经填充完slot的prompt。hr是关系Transformer里的，计算方式为hr=LN(hs,hso,ho)。θ是线性投影函数LN的参数，只有这部分参数更新。P()是得分函数，y_hat是置信度最高的谓语。</p> 
<p>得分函数计算如下：<br> <img src="https://images2.imgbox.com/c3/e1/W5eLGBkx_o.png" width="50%"><br> 其中，ein( p )是p这个prompt经过文本Transformer的编码结果。这个公式是评估<strong>主客体的视觉区域</strong>和<strong>主语-谓语-宾语这种形式的prompt</strong>的余弦相似度，然后做了一个softmax。q是所有填充完的prompt。</p> 
<p><font color="red"><strong>1.基于soft视觉-关系prompt的微调</strong></font><br> 模板如下：<br> <img src="https://images2.imgbox.com/77/85/OGbZvecU_o.png" width="50%"><br> 其中，xs’,…,xo’来自下图的V2T decoder。注意下图的εi/εt和预训练VRM那里的εi/εt是一个东西。<br> <img src="https://images2.imgbox.com/ab/e2/QrZWb4lY_o.png" width="50%"><br> 谓语的预测公式如下式所示：<br> <img src="https://images2.imgbox.com/1a/59/8y53gPug_o.png" width="50%"><br> 相比hard prompt，少了hr。θ‘是V2T decoder的参数，只有这部分参数更新。</p> 
<p>得分函数P如下：<br> <img src="https://images2.imgbox.com/5a/20/p55NVcF9_o.png" width="50%"></p> 
<p>其中，wr是数据集的谓语类别经过文本encoder的embedding，e[mask]是prompt中[mask]这个token的文本encoder输出</p> 
<p>------------------------------------------------一些碎碎念----------------------------------------------------<br> 在做新实验了<br> 想方设法证明加的都是有用的()<br> 最近沉迷史莱姆的视频<br> 但是史莱姆都好贵啊。<br> 感觉我的实验就像做史莱姆一样<br> 加一堆胶水 搅和搅和 面多了加水水多了加面(bushi<br> 最后变成一块()</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/910b423626289b32e0554f3765b3b253/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">记录一次pickle load报错问题</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/8b954ada273cc904e88c0fadfbc16599/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">SourceTree Git管理神器使用教程详解</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>