<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>目标检测中的损失函数汇总 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="目标检测中的损失函数汇总" />
<meta property="og:description" content="和图像分割中将损失函数分为基于分布，基于区域以及基于边界的损失函数不一样，目标检测经常可以认为由2类最基础的损失，分类损失和回归损失而组成。
分类损失 CE loss，交叉熵损失
交叉熵损失，二分类损失（binary CE loss）是它的一种极端情况. 在机器学习部分就有介绍它。
如下图所示,y是真实标签，a是预测标签，一般可通过sigmoid，softmax得到，x是样本，n是样本数目，和对数似然等价。
focal loss,
用改变loss的方式来缓解样本的不平衡，因为改变loss只影响train部分的过程和时间，而对推断时间影响甚小，容易拓展。
focal loss就是把CE里的p替换为pt，当预测正确的时候，pt接近1，在FL(pt)中，其系数 ( 1 − p t ) γ (1-p_t)^\gamma (1−pt​)γ越小（只要 γ &gt; 0 \gamma&gt;0 γ&gt;0）；简而言之，就是简单的样例比重越小，难的样例比重相对变大
Rankings类型的损失
在这有两类，DR(Distributional Ranking) Loss和AP Loss
DR Loss, 分布排序损失， Qian et al., 2020, DR loss: Improving object detection by distributional ranking
DR loss的研究背景和focal loss一样，one-stage方法中样本不平衡。它进行分布的转换以及用ranking作为loss。将分类问题转换为排序问题，从而避免了正负样本不平衡的问题。同时针对排序，提出了排序的损失函数DR loss。具体流程可参考：https://zhuanlan.zhihu.com/p/75896297
AP Loss, Chen et al., 2019, Towards Accurate One-Stage Object Detection with AP-Loss
AP loss也是解决one-stage方法中样本不平衡问题,同时也和DR loss类似，是一种排序loss。将单级检测器中的分类任务替换为排序任务，并采用平均精度损失(AP-loss)来处理排序问题。由于AP-loss的不可微性和非凸性，使得APloss不能直接优化。因此，本文开发了一种新的优化算法，它将感知器学习中的错误驱动更新方案和深度网络中的反向传播机制无缝地结合在一起。具体可参见：https://blog.csdn.net/jiaoyangwm/article/details/91479594
回归损失 回归损失在这里更多的是对应与bounding box的回归。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/339a7b40d18989032fd02e6838b1c6c9/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-08-31T05:21:24+08:00" />
<meta property="article:modified_time" content="2020-08-31T05:21:24+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">目标检测中的损失函数汇总</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>和图像分割中将损失函数分为基于分布，基于区域以及基于边界的损失函数不一样，目标检测经常可以认为由2类最基础的损失，分类损失和回归损失而组成。<br> <img src="https://images2.imgbox.com/d7/1b/txwbl6eg_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_3"></a>分类损失</h3> 
<ul><li> <p>CE loss，交叉熵损失<br> 交叉熵损失，二分类损失（binary CE loss）是它的一种极端情况. 在机器学习部分就有介绍它。<br> 如下图所示,y是真实标签，a是预测标签，一般可通过sigmoid，softmax得到，x是样本，n是样本数目，和对数似然等价。<br> <img src="https://images2.imgbox.com/b7/7f/23ueOIMo_o.png" alt="在这里插入图片描述"></p> </li><li> <p>focal loss,<br> 用改变loss的方式来缓解样本的不平衡，因为改变loss只影响train部分的过程和时间，而对推断时间影响甚小，容易拓展。<br> focal loss就是把CE里的p替换为pt，当预测正确的时候，pt接近1，在FL(pt)中，其系数<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           ( 
          
         
           1 
          
         
           − 
          
          
          
            p 
           
          
            t 
           
          
          
          
            ) 
           
          
            γ 
           
          
         
        
          (1-p_t)^\gamma 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.280556em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.664392em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.05556em;">γ</span></span></span></span></span></span></span></span></span></span></span></span>越小（只要<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           γ 
          
         
           &gt; 
          
         
           0 
          
         
        
          \gamma&gt;0 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.73354em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.05556em;">γ</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">0</span></span></span></span></span>）；简而言之，就是简单的样例比重越小，难的样例比重相对变大<br> <img src="https://images2.imgbox.com/5e/a5/m0NIvmr2_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/c1/ae/vOr9BQXE_o.png" alt="在这里插入图片描述"></p> </li><li> <p>Rankings类型的损失<br> 在这有两类，DR(Distributional Ranking) Loss和AP Loss</p> </li><li> <p>DR Loss, 分布排序损失， Qian et al., 2020, <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Qian_DR_Loss_Improving_Object_Detection_by_Distributional_Ranking_CVPR_2020_paper.pdf" rel="nofollow">DR loss: Improving object detection by distributional ranking</a><br> DR loss的研究背景和focal loss一样，one-stage方法中样本不平衡。它进行分布的转换以及用ranking作为loss。将分类问题转换为排序问题，从而避免了正负样本不平衡的问题。同时针对排序，提出了排序的损失函数DR loss。具体流程可参考：https://zhuanlan.zhihu.com/p/75896297<br> <img src="https://images2.imgbox.com/58/1c/Qp8WoMyf_o.png" alt="在这里插入图片描述"></p> </li><li> <p>AP Loss, Chen et al., 2019, <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Towards_Accurate_One-Stage_Object_Detection_With_AP-Loss_CVPR_2019_paper.pdf" rel="nofollow">Towards Accurate One-Stage Object Detection with AP-Loss</a><br> AP loss也是解决one-stage方法中样本不平衡问题,同时也和DR loss类似，是一种排序loss。将单级检测器中的分类任务替换为排序任务，并采用平均精度损失(AP-loss)来处理排序问题。由于AP-loss的不可微性和非凸性，使得APloss不能直接优化。因此，本文开发了一种新的优化算法，它将感知器学习中的错误驱动更新方案和深度网络中的反向传播机制无缝地结合在一起。具体可参见：https://blog.csdn.net/jiaoyangwm/article/details/91479594<br> <img src="https://images2.imgbox.com/f8/80/DC0vuMKd_o.png" alt="在这里插入图片描述"></p> </li></ul> 
<h3><a id="_25"></a>回归损失</h3> 
<p>回归损失在这里更多的是对应与bounding box的回归。</p> 
<ul><li> <p>MSE， RMSE，同样在机器学习中也会用来做回归损失。<br> 常用在回归任务中，MSE的特点是光滑连续，可导，方便用于梯度下降。因为MSE是模型预测值 f(x) 与样本真实值 y 之间距离平方的平均值，故离得越远，误差越大，即受离群点的影响较大<br> <img src="https://images2.imgbox.com/c1/c7/MrnuZEna_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/e0/d3/hC6SkuZi_o.png" alt="在这里插入图片描述"></p> </li><li> <p>Smooth L1 loss，<br> 特殊的，smoothL1Loss是huber loss中的delta=1时的情况。这个损失函数用在了faster RCNN中，用于定位框的回归损失。<br> <img src="https://images2.imgbox.com/d8/cb/Uxht83aP_o.png" alt="在这里插入图片描述"></p> </li><li> <p>balanced L1 Loss,<br> https://zhuanlan.zhihu.com/p/101303119<br> 用在了Libra RCNN中，基于smoothL1Loss的改进。作者发现平均每个easy sample对梯度的贡献为hard sample的30%，相当于作者在找一个平衡的点，能让easy和hard的sample所占的梯度贡献差不多，因此引入了这个balancedL1Loss,其在接近于0的时候飞速下降，而在接近于1的时候缓慢上升，而不至于向smoothL1Loss那样只有中间regression error为1的时候有个突变，由此让他变得更加平衡，如下图所示。<br> 文章：Pang et al., 2019, <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Pang_Libra_R-CNN_Towards_Balanced_Learning_for_Object_Detection_CVPR_2019_paper.pdf" rel="nofollow">Libra R-CNN: Towards Balanced Learning for Object Detection</a><br> <img src="https://images2.imgbox.com/ad/b4/vLrofwQ7_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/2b/6c/10bkzuke_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/28/50/EhKBSoP2_o.png" alt="在这里插入图片描述"></p> </li><li> <p>KL Loss, He et al., 2019, <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/He_Bounding_Box_Regression_With_Uncertainty_for_Accurate_Object_Detection_CVPR_2019_paper.pdf" rel="nofollow">Bounding Box Regression with Uncertainty for Accurate Object Detection</a><br> 这篇文章是为了解决边界不确定的box的regression问题(不被模糊样例造成大的loss干扰). 文章预测坐标（x1,y1,x2,y2）的偏移值，对于每个偏移值，假设预测值服从高斯分布，标准值为狄拉克函数（即偏移一直为0），计算这两个分布的距离（这里用KL散度表示距离）作为损失函数。参考smooth L1 loss，也分为|xg-xe|&lt;=1和&gt;1的两段，如下所示：<br> <img src="https://images2.imgbox.com/eb/0c/tZsCuVXJ_o.png" alt="在这里插入图片描述"></p> </li><li> <p>region-based loss，基于区域的损失函数，IOU类<br> 以上是针对样本分布的回归损失，后来发现基于区域的损失在回归框的任务中，起到了很好的效果，因此用基于框的回归损失函数来进行回归预测。具体可以看以下提供的实例，详细介绍了IOU的系列发展。</p> </li></ul> 
<h6><a id="YOLO_52"></a>随后，我将基于YOLO系列给出的损失函数作为实例，因为它包括了多数情况。</h6> 
<p>YOLO系列的损失包括三个部分: 回归框loss, 置信度loss, 分类loss.</p> 
<ol><li>从最重要的部分开始: 回归框loss.</li></ol> 
<ul><li>从 v1 to v3, 回归框loss更像是MSE，v1是(x-x’)^2 + (y-y’)^2，而对w和h分别取平方根做差，再求平方，用以消除一些物体大小不均带来的不利。</li><li>v2和v3则利用(2 - w * h)[(x-x’)^2 + (y-y’)^2 + (w-w’)^2 + (h-h’)^2], 将框大小的影响放在前面作为系数，连x和y部分也一块考虑了进去。</li><li>v4作者认为v1-v3建立的类MSE损失是不合理的。因为MSE的四者是需要解耦独⽴的，但实际上这四者并不独⽴，应该需要⼀个损失函数能捕捉到它们之间的相互关系。因此引入了IOU系列。经过其验证，GIOU，DIOU, CIOU，最终作者发现CIOU效果最好。注意，使用的时候是他们的loss，应该是1-IOUs，因为IOU越大表示重合越好，而loss是越小越好，因此前面加1-，令其和平常使用规则一致。</li><li>v5作者采用了GIOU，具体还需要等他论文出现。</li></ul> 
<p>下面介绍一下IOU系列：<br> 这里有篇博客文章参考：https://zhuanlan.zhihu.com/p/94799295</p> 
<ul><li> <p>IOU, A与B交集 / A与B并集，在这一般是ground truth和predict box之间的相交面积/他们的并面积<br> <img src="https://images2.imgbox.com/9a/5d/B59Zbkla_o.png" alt="在这里插入图片描述"></p> </li><li> <p>GIOU, Rezatofighi et al., 2019, Stanford University, <a href="https://arxiv.org/abs/1902.09630" rel="nofollow">Generalized Intersection Over Union: A Metric and a Loss for Bounding Box Regression</a><br> GIOU,针对IOU只是一个比值，IoU无法区分两个对象之间不同的对齐方式，因此引入了GIOU。下图中的A_c是两个框的最小闭包区域面积(通俗理解：同时包含了预测框和真实框的最小框的面积)，减去两个框的并集，即通过计算闭包区域中不属于两个框的区域占闭包区域的比重，最后用IoU减去这个比重即可得到GIoU。<br> GIoU是IoU的下界，在两个框无限重合的情况下，IoU=GIoU=1；<br> IoU取值[0,1]，但GIoU有对称区间，取值范围[-1,1]。在两者重合的时候取最大值1，在两者无交集且无限远的时候取最小值-1，因此GIoU是一个非常好的距离度量指标；<br> 与IoU只关注重叠区域不同，GIoU不仅关注重叠区域，还关注其他的非重合区域，能更好的反映两者的重合度。<br> <img src="https://images2.imgbox.com/25/03/0nJIJkIi_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/8d/34/M2cbAWAf_o.png" alt="在这里插入图片描述"></p> </li><li> <p>DIOU，Zheng et al., 2019, Tianjin University, <a href="https://arxiv.org/pdf/1911.08287.pdf" rel="nofollow">Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression</a><br> DIoU要比GIou更加符合目标框回归的机制，将目标与anchor之间的距离，重叠率以及尺度都考虑进去，使得目标框回归变得更加稳定，不会像IoU和GIoU一样出现训练过程中发散等问题. 如下图所示，b和b^{gt}是预测的中心和ground truth的中心坐标，\rho是指这两点之间的欧氏距离，c是两个框的闭包区域面积的对角线的距离。<br> DIoU loss可以直接最小化两个目标框的距离，因此比GIoU loss收敛快得多。<br> 对于包含两个框在水平方向和垂直方向上这种情况，DIoU损失可以使回归非常快，而GIoU损失几乎退化为IoU损失。<br> DIoU还可以替换普通的IoU评价策略，应用于NMS中，使得NMS得到的结果更加合理和有效。<br> <img src="https://images2.imgbox.com/e9/58/fWFTMYph_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/4c/48/dAlaQv5X_o.png" alt="在这里插入图片描述"></p> </li><li> <p>CIOU, Zheng et al., 2019, Tianjin University, <a href="https://arxiv.org/pdf/1911.08287.pdf" rel="nofollow">Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression</a><br> 虽然DIOU考虑了两中心的距离，但是没有考虑到⻓宽⽐。⼀个好的预测框，应该和 ground truth 的⻓宽⽐尽量保持⼀致。因此有了CIOU，在DIOU基础上加入了惩罚项。如下图是其CIOU loss，前面有了1-。而\nu是衡量长宽比的相似性。<br> <img src="https://images2.imgbox.com/ed/f2/QmRhGIHU_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/45/79/us1rNENn_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/4a/d1/FruZWIqT_o.png" alt="在这里插入图片描述"></p> </li></ul> 
<ol start="2"><li>置信度损失和分类Loss.<br> 这里先给出v1-v3的损失函数，可以看出，v1-v2中置信度误差和分类误差均使用的是MSE；<br> 从v2到v3, 不同的地⽅在于，对于类别和置信度的损失使⽤交叉熵。<br> <img src="https://images2.imgbox.com/e4/09/UsC4JiU9_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/4d/f9/2SAeIZSQ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/1a/0a/bhvr4kOV_o.png" alt="在这里插入图片描述"><br> v4在v3的基础上对回归框进行的loss回归预测，就是基于CIOU的回归函数。</li></ol>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/3ce76430db229c0b652b15a8afa8bb0d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">CTFHub学习笔记(2) - 技能树 - 信息泄露[只做了部分，待更新]</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/cf1ce5a8b25aa29af515dfa3ab479f8a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">深度优先搜索算法（DFS）原理及示例详解</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>