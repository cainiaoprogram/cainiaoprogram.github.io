<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【词嵌入网络】word2vec词向量网络步骤构建详解（附python代码实现） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【词嵌入网络】word2vec词向量网络步骤构建详解（附python代码实现）" />
<meta property="og:description" content="前言 构建网络的基础要素有两个，节点和节点间的联系。
初始数据集存在的情况下，节点间的一层联系就可以通过最基础的共现概率求得。
再往深就到剖析节点间的二层联系的时候了。
word2vec词向量训练 Word2Vec是谷歌在 2013 年推出来的一个自然语言处理工具，可用来计算文本的词向量，从而实现定量的度量词与词之间的关系。
本篇使用gensim库中现成的word2vec模型训练词向量，需要准备的只是语句集和参数。
语句集示例 假定初始数据列表为
单次购置商品A，BB，CC，DE，B，FD，E 那么转换后的语句集就为
sentences = [[A,B],[B,C],[C,D],[E,B,F],[D,E]] 参数说明 min_count：词语频度，低于这个阈值的词语不做词向量
size：每个词对应向量的维度，也就是向量长度
workers：并行训练任务数
其他一般常用default，站里也有不少博客说明，我这里就略过了。
源码 一次训练
import gensim from gensim.models import word2vec SIZE = 30 model = word2vec.Word2Vec(sentences, size=SIZE, min_count=10) # 保存词向量模型，下次只需要load就可以用了 model.save(&#34;../dataset/wordvector_network/word2vec_atec&#34;) 二次增量训练
# 加载已有模型 model = gensim.models.Word2Vec.load(&#34;../dataset/wordvector_network/word2vec_atec&#34;) # 进行增量训练 corpus = [[B,C,D,F],[A,C,D]] # 新增语料 model.build_vocab(corpus, update=True) # 训练该行 model.train(corpus, total_examples=model.corpus_count, epochs=model.iter) # 保存增量训练后的新模型 model.save(&#34;../dataset/wordvector_network/word2vec_atec&#34;) 查看训练完的节点词向量
model.wv[A] cosine相似度计算 由于角度的余弦值区间为-1到1，而0度的角余弦值正好是1，因此当余弦相似度为1时代表两个向量的指向相同，-1时相反。因此可以通过余弦值来度量相似度，向量夹角越小证明cosine值越大，同时也说明两个节点越相似。
同样调sklearn里的cosine_similarity函数，直接计算。
源码 from sklearn." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/c044226ce646e12add2e611368d67f7f/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-05-03T18:17:53+08:00" />
<meta property="article:modified_time" content="2021-05-03T18:17:53+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【词嵌入网络】word2vec词向量网络步骤构建详解（附python代码实现）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="_0"></a>前言</h3> 
<p>构建网络的基础要素有两个，<strong>节点</strong>和<strong>节点间的联系</strong>。<br> 初始数据集存在的情况下，节点间的一层联系就可以通过最基础的<strong>共现概率</strong>求得。<br> 再往深就到剖析节点间的<strong>二层联系</strong>的时候了。</p> 
<h3><a id="word2vec_4"></a>word2vec词向量训练</h3> 
<blockquote> 
 <p>Word2Vec是谷歌在 2013 年推出来的一个自然语言处理工具，可用来计算文本的词向量，从而实现定量的度量词与词之间的关系。</p> 
</blockquote> 
<p>本篇使用<strong>gensim</strong>库中现成的word2vec模型训练词向量，需要准备的只是<strong>语句集</strong>和<strong>参数</strong>。</p> 
<h5><a id="_9"></a>语句集示例</h5> 
<p>假定初始数据列表为</p> 
<table><thead><tr><th>单次购置商品</th></tr></thead><tbody><tr><td>A，B</td></tr><tr><td>B，C</td></tr><tr><td>C，D</td></tr><tr><td>E，B，F</td></tr><tr><td>D，E</td></tr></tbody></table> 
<p>那么转换后的语句集就为</p> 
<pre><code class="prism language-python">sentences <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span>A<span class="token punctuation">,</span>B<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span>B<span class="token punctuation">,</span>C<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span>C<span class="token punctuation">,</span>D<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span>E<span class="token punctuation">,</span>B<span class="token punctuation">,</span>F<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span>D<span class="token punctuation">,</span>E<span class="token punctuation">]</span><span class="token punctuation">]</span>
</code></pre> 
<h5><a id="_22"></a>参数说明</h5> 
<p><strong>min_count</strong>：词语频度，低于这个阈值的词语不做词向量<br> <strong>size</strong>：每个词对应向量的维度，也就是向量长度<br> <strong>workers</strong>：并行训练任务数<br> 其他一般常用default，站里也有不少博客说明，我这里就略过了。</p> 
<h5><a id="_27"></a>源码</h5> 
<p><strong>一次训练</strong></p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> gensim
<span class="token keyword">from</span> gensim<span class="token punctuation">.</span>models <span class="token keyword">import</span> word2vec

SIZE <span class="token operator">=</span> <span class="token number">30</span>

model <span class="token operator">=</span> word2vec<span class="token punctuation">.</span>Word2Vec<span class="token punctuation">(</span>sentences<span class="token punctuation">,</span> size<span class="token operator">=</span>SIZE<span class="token punctuation">,</span> min_count<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>

<span class="token comment"># 保存词向量模型，下次只需要load就可以用了</span>
model<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token string">"../dataset/wordvector_network/word2vec_atec"</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>二次增量训练</strong></p> 
<pre><code class="prism language-python"><span class="token comment"># 加载已有模型</span>
model <span class="token operator">=</span> gensim<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Word2Vec<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"../dataset/wordvector_network/word2vec_atec"</span><span class="token punctuation">)</span>

<span class="token comment"># 进行增量训练</span>
corpus <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span>B<span class="token punctuation">,</span>C<span class="token punctuation">,</span>D<span class="token punctuation">,</span>F<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span>A<span class="token punctuation">,</span>C<span class="token punctuation">,</span>D<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token comment"># 新增语料</span>
model<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>corpus<span class="token punctuation">,</span> update<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token comment"># 训练该行</span>
model<span class="token punctuation">.</span>train<span class="token punctuation">(</span>corpus<span class="token punctuation">,</span> total_examples<span class="token operator">=</span>model<span class="token punctuation">.</span>corpus_count<span class="token punctuation">,</span> epochs<span class="token operator">=</span>model<span class="token punctuation">.</span><span class="token builtin">iter</span><span class="token punctuation">)</span>

<span class="token comment"># 保存增量训练后的新模型</span>
model<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token string">"../dataset/wordvector_network/word2vec_atec"</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>查看训练完的节点词向量</strong></p> 
<pre><code class="prism language-python">model<span class="token punctuation">.</span>wv<span class="token punctuation">[</span>A<span class="token punctuation">]</span>
</code></pre> 
<h3><a id="cosine_60"></a>cosine相似度计算</h3> 
<blockquote> 
 <p>由于角度的余弦值区间为-1到1，而0度的角余弦值正好是1，因此当余弦相似度为1时代表两个向量的指向相同，-1时相反。因此可以通过余弦值来度量相似度，向量夹角越小证明cosine值越大，同时也说明两个节点越相似。</p> 
</blockquote> 
<p>同样调<strong>sklearn</strong>里的<strong>cosine_similarity</strong>函数，直接计算。</p> 
<h5><a id="_65"></a>源码</h5> 
<pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics<span class="token punctuation">.</span>pairwise <span class="token keyword">import</span> cosine_similarity
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

cosine_similarity<span class="token punctuation">(</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>model<span class="token punctuation">.</span>wv<span class="token punctuation">[</span>A<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>SIZE<span class="token punctuation">)</span><span class="token punctuation">,</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>model<span class="token punctuation">.</span>wv<span class="token punctuation">[</span>B<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>SIZE<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="_73"></a>构建网络</h3> 
<p>遍历节点集，分别计算节点间关联度，剔除低于阈值的边后网络构建完毕。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/5ec07eaf0027bdac283499031d35bfcf/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">解决Error: L6218E: Undefined symbol xxxx (referred from xxx.o)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/bea30695efc0150acdd6198a25909549/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">AlmaLinux 简要概述</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>