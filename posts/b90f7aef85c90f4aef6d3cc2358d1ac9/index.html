<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>记模型训练损失为NAN - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="记模型训练损失为NAN" />
<meta property="og:description" content="前段时间想把我模型的输入由DWT子带改为分块的图像块，一顿魔改后，模型跑着跑着损失就朝着奇怪的方向跑去了：要么突然增大，要么变为NAN。
为什么训练损失会突然变为NAN呢？这个作者将模型训练过程中loss为NAN或INF的原因解释得好好详尽（感谢）：Pytorch训练模型损失Loss为Nan或者无穷大（INF）原因_pytorch loss nan-CSDN博客https://blog.csdn.net/ytusdc/article/details/122321907 我经过输入几番输入打印测试，确认我的输入确实没有问题，那么问题只能出现在模型的前向传播或者反向梯度传播过程中。我跟着这个作者的排查思路，最终定位问题出在梯度反向传播上，于是通过梯度剪裁成功解决NAN问题（我还增大了batch_size的大小，输入修改后，我发现模型运算量减小了，显存支持我每个step跑更大的batch_size了）。pytorch训练过程中出现nan的排查思路_torch判断nan-CSDN博客https://blog.csdn.net/mch2869253130/article/details/111034068修改部分：
if mode == &#39;train&#39;: # # 1.debug loss # assert torch.isnan(total_loss).sum() == 0, print(total_loss) total_loss.backward() # # 2. 如果loss不是nan,那么说明forward过程没问题，可能是梯度爆炸，所以用梯度裁剪试试 nn.utils.clip_grad_norm(net.parameters(), max_norm=3, norm_type=2) optim.step() optim.zero_grad() 梯度剪裁：
对超出值域范围的梯度进行约束，避免梯度持续大于1，造成梯度爆炸。（没办法规避梯度消失）
torch.nn.utils.clip_grad_norm_(parameters, max_norm, norm_type) parameters参数是需要进行梯度裁剪的参数列表。通常是模型的参数列表，即model.parameters()；max_norm参数可以理解为梯度（默认是L2 范数）范数的最大阈值；norm_type参数可以理解为指定范数的类型，比如norm_type=1 表示使用L1 范数，norm_type=2 表示使用L2 范数。
【Pytorch】梯度裁剪——torch.nn.utils.clip_grad_norm_的原理及计算过程-CSDN博客https://blog.csdn.net/m0_46412065/article/details/131396098?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522170435889016800215059432%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=170435889016800215059432&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-131396098-null-null.142^v99^pc_search_result_base7&amp;utm_term=%E6%A2%AF%E5%BA%A6%E5%89%AA%E8%A3%81&amp;spm=1018.2226.3001.4187" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/b90f7aef85c90f4aef6d3cc2358d1ac9/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-04T17:07:33+08:00" />
<meta property="article:modified_time" content="2024-01-04T17:07:33+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">记模型训练损失为NAN</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>        前段时间想把我模型的输入由DWT子带改为分块的图像块，一顿魔改后，模型跑着跑着损失就朝着奇怪的方向跑去了：要么突然增大，要么变为NAN。<br><img alt="" height="61" src="https://images2.imgbox.com/7b/0b/MIVmGe7H_o.png" width="701"></p> 
<p> <img alt="" height="45" src="https://images2.imgbox.com/fd/9c/FSiN9cOR_o.png" width="653"></p> 
<hr> 
<p> </p> 
<p>         为什么训练损失会突然变为NAN呢？这个作者将模型训练过程中loss为NAN或INF的原因解释得好好详尽（感谢）：<a class="has-card" href="https://blog.csdn.net/ytusdc/article/details/122321907" title="Pytorch训练模型损失Loss为Nan或者无穷大（INF）原因_pytorch loss nan-CSDN博客"><span class="link-card-box"><span class="link-title">Pytorch训练模型损失Loss为Nan或者无穷大（INF）原因_pytorch loss nan-CSDN博客</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/16/f7/dmbF5AC9_o.png" alt="icon-default.png?t=N7T8">https://blog.csdn.net/ytusdc/article/details/122321907</span></span></a>        我经过输入几番输入打印测试，确认我的输入确实没有问题，那么问题只能出现在模型的前向传播或者反向梯度传播过程中。我跟着这个作者的排查思路，最终定位问题出在梯度反向传播上，于是通过梯度剪裁成功解决NAN问题（我还增大了batch_size的大小，输入修改后，我发现模型运算量减小了，显存支持我每个step跑更大的batch_size了）。<a class="has-card" href="https://blog.csdn.net/mch2869253130/article/details/111034068" title="pytorch训练过程中出现nan的排查思路_torch判断nan-CSDN博客"><span class="link-card-box"><span class="link-title">pytorch训练过程中出现nan的排查思路_torch判断nan-CSDN博客</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/e4/18/NYOX6EJ3_o.png" alt="icon-default.png?t=N7T8">https://blog.csdn.net/mch2869253130/article/details/111034068</span></span></a>修改部分：</p> 
<pre><code class="language-python">        if mode == 'train':
            # # 1.debug loss
            # assert torch.isnan(total_loss).sum() == 0, print(total_loss)
            total_loss.backward()

            # # 2. 如果loss不是nan,那么说明forward过程没问题，可能是梯度爆炸，所以用梯度裁剪试试
            nn.utils.clip_grad_norm(net.parameters(), max_norm=3, norm_type=2)

            optim.step()
            optim.zero_grad()</code></pre> 
<p><strong>梯度剪裁：</strong></p> 
<p>        对超出值域范围的梯度进行约束，避免梯度持续大于1，造成梯度爆炸。（没办法规避梯度消失）</p> 
<pre><code class="language-python">torch.nn.utils.clip_grad_norm_(parameters, max_norm, norm_type) </code></pre> 
<ul><li><strong>parameters</strong>参数是需要进行梯度裁剪的参数列表。通常是模型的参数列表，即model.parameters()；</li><li><strong>max_norm</strong>参数可以理解为梯度（默认是L2 范数）范数的最大阈值；</li><li><strong>norm_type</strong>参数可以理解为指定范数的类型，比如norm_type=1 表示使用L1 范数，norm_type=2 表示使用L2 范数。<br>  </li></ul> 
<p><a class="has-card" href="https://blog.csdn.net/m0_46412065/article/details/131396098?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522170435889016800215059432%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=170435889016800215059432&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-131396098-null-null.142%5Ev99%5Epc_search_result_base7&amp;utm_term=%E6%A2%AF%E5%BA%A6%E5%89%AA%E8%A3%81&amp;spm=1018.2226.3001.4187" title="【Pytorch】梯度裁剪——torch.nn.utils.clip_grad_norm_的原理及计算过程-CSDN博客"><span class="link-card-box"><span class="link-title">【Pytorch】梯度裁剪——torch.nn.utils.clip_grad_norm_的原理及计算过程-CSDN博客</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/9a/9c/Ai7be4cW_o.png" alt="icon-default.png?t=N7T8">https://blog.csdn.net/m0_46412065/article/details/131396098?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522170435889016800215059432%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=170435889016800215059432&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-131396098-null-null.142^v99^pc_search_result_base7&amp;utm_term=%E6%A2%AF%E5%BA%A6%E5%89%AA%E8%A3%81&amp;spm=1018.2226.3001.4187</span></span></a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/10f1f638e5095392ce235dee3fed9f51/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Nginx(一)概述</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a88f61fe5f62f700876208334c68b8b8/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">vue前端常用工具类汇总</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>