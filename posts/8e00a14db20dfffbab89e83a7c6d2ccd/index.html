<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>MATLAB算法实战应用案例精讲-【图像处理】目标检测 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="MATLAB算法实战应用案例精讲-【图像处理】目标检测" />
<meta property="og:description" content="目录
前言
算法模型
两阶段（2-stage）检测模型
R-CNN: R-CNN系列的开山之作
Fast R-CNN: 共享卷积运算
Faster R-CNN: 两阶段模型的深度化
单阶段（1-stage）检测模型
YOLO
SSD: Single Shot Multibox Detector
模型的评测与训练技巧
检测模型的评测指标
标准评测数据集
检测模型中的Bells and wisthles
基础网络演进、分类与定位的权衡
基础网络结构的演进
卷积网络结构演进的趋势
ResNet: 残差学习
分类与定位问题的权衡
R-FCN
Deformable Convolution Networks
特征复用、实时性
特征复用与整合
FPN
TDM
DSSD
RON
FSSD
RefineDet
面向实时性的工作
Light Head R-CNN
YOLOv2
SSDLite(MobileNets V2)
目标检测新趋势
YOLO9000
Mask R-CNN
Focal Loss（RetinaNet）
Mimicking
CGBN（Cross GPU Batch Normalization）
DSOD（Deeply Supervised Object Detector）
A-Fast-RCNN
Relation Module
前言 如何理解一张图片？根据后续任务的需要，有三个主要的层次。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/8e00a14db20dfffbab89e83a7c6d2ccd/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-11-05T00:30:00+08:00" />
<meta property="article:modified_time" content="2023-11-05T00:30:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">MATLAB算法实战应用案例精讲-【图像处理】目标检测</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p> </p> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="%E5%89%8D%E8%A8%80-toc" style="margin-left:0px;"><a href="#%E5%89%8D%E8%A8%80" rel="nofollow">前言</a></p> 
<p id="%E7%AE%97%E6%B3%95%E6%A8%A1%E5%9E%8B-toc" style="margin-left:0px;"><a href="#%E7%AE%97%E6%B3%95%E6%A8%A1%E5%9E%8B" rel="nofollow">算法模型</a></p> 
<p id="%E4%B8%A4%E9%98%B6%E6%AE%B5%EF%BC%882-stage%EF%BC%89%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B-toc" style="margin-left:80px;"><a href="#%E4%B8%A4%E9%98%B6%E6%AE%B5%EF%BC%882-stage%EF%BC%89%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B" rel="nofollow">两阶段（2-stage）检测模型</a></p> 
<p id="R-CNN%3A%20R-CNN%E7%B3%BB%E5%88%97%E7%9A%84%E5%BC%80%E5%B1%B1%E4%B9%8B%E4%BD%9C-toc" style="margin-left:120px;"><a href="#R-CNN%3A%20R-CNN%E7%B3%BB%E5%88%97%E7%9A%84%E5%BC%80%E5%B1%B1%E4%B9%8B%E4%BD%9C" rel="nofollow">R-CNN: R-CNN系列的开山之作</a></p> 
<p id="Fast%20R-CNN%3A%20%E5%85%B1%E4%BA%AB%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97-toc" style="margin-left:120px;"><a href="#Fast%20R-CNN%3A%20%E5%85%B1%E4%BA%AB%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97" rel="nofollow">Fast R-CNN: 共享卷积运算</a></p> 
<p id="Faster%20R-CNN%3A%20%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%8C%96-toc" style="margin-left:120px;"><a href="#Faster%20R-CNN%3A%20%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%8C%96" rel="nofollow">Faster R-CNN: 两阶段模型的深度化</a></p> 
<p id="%E5%8D%95%E9%98%B6%E6%AE%B5%EF%BC%881-stage%EF%BC%89%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B-toc" style="margin-left:80px;"><a href="#%E5%8D%95%E9%98%B6%E6%AE%B5%EF%BC%881-stage%EF%BC%89%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B" rel="nofollow">单阶段（1-stage）检测模型</a></p> 
<p id="YOLO-toc" style="margin-left:120px;"><a href="#YOLO" rel="nofollow">YOLO</a></p> 
<p id="SSD%3A%20Single%20Shot%20Multibox%20Detector-toc" style="margin-left:120px;"><a href="#SSD%3A%20Single%20Shot%20Multibox%20Detector" rel="nofollow">SSD: Single Shot Multibox Detector</a></p> 
<p id="%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E6%B5%8B%E4%B8%8E%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7-toc" style="margin-left:40px;"><a href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E6%B5%8B%E4%B8%8E%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7" rel="nofollow">模型的评测与训练技巧</a></p> 
<p id="%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E6%B5%8B%E6%8C%87%E6%A0%87-toc" style="margin-left:80px;"><a href="#%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E6%B5%8B%E6%8C%87%E6%A0%87" rel="nofollow">检测模型的评测指标</a></p> 
<p id="%E6%A0%87%E5%87%86%E8%AF%84%E6%B5%8B%E6%95%B0%E6%8D%AE%E9%9B%86-toc" style="margin-left:80px;"><a href="#%E6%A0%87%E5%87%86%E8%AF%84%E6%B5%8B%E6%95%B0%E6%8D%AE%E9%9B%86" rel="nofollow">标准评测数据集</a></p> 
<p id="%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84Bells%20and%20wisthles-toc" style="margin-left:80px;"><a href="#%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84Bells%20and%20wisthles" rel="nofollow">检测模型中的Bells and wisthles</a></p> 
<p id="%E5%9F%BA%E7%A1%80%E7%BD%91%E7%BB%9C%E6%BC%94%E8%BF%9B%E3%80%81%E5%88%86%E7%B1%BB%E4%B8%8E%E5%AE%9A%E4%BD%8D%E7%9A%84%E6%9D%83%E8%A1%A1-toc" style="margin-left:40px;"><a href="#%E5%9F%BA%E7%A1%80%E7%BD%91%E7%BB%9C%E6%BC%94%E8%BF%9B%E3%80%81%E5%88%86%E7%B1%BB%E4%B8%8E%E5%AE%9A%E4%BD%8D%E7%9A%84%E6%9D%83%E8%A1%A1" rel="nofollow">基础网络演进、分类与定位的权衡</a></p> 
<p id="%E5%9F%BA%E7%A1%80%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E7%9A%84%E6%BC%94%E8%BF%9B-toc" style="margin-left:80px;"><a href="#%E5%9F%BA%E7%A1%80%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E7%9A%84%E6%BC%94%E8%BF%9B" rel="nofollow">基础网络结构的演进</a></p> 
<p id="%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E6%BC%94%E8%BF%9B%E7%9A%84%E8%B6%8B%E5%8A%BF-toc" style="margin-left:80px;"><a href="#%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E6%BC%94%E8%BF%9B%E7%9A%84%E8%B6%8B%E5%8A%BF" rel="nofollow">卷积网络结构演进的趋势</a></p> 
<p id="ResNet%3A%20%E6%AE%8B%E5%B7%AE%E5%AD%A6%E4%B9%A0-toc" style="margin-left:80px;"><a href="#ResNet%3A%20%E6%AE%8B%E5%B7%AE%E5%AD%A6%E4%B9%A0" rel="nofollow">ResNet: 残差学习</a></p> 
<p id="%E5%88%86%E7%B1%BB%E4%B8%8E%E5%AE%9A%E4%BD%8D%E9%97%AE%E9%A2%98%E7%9A%84%E6%9D%83%E8%A1%A1-toc" style="margin-left:80px;"><a href="#%E5%88%86%E7%B1%BB%E4%B8%8E%E5%AE%9A%E4%BD%8D%E9%97%AE%E9%A2%98%E7%9A%84%E6%9D%83%E8%A1%A1" rel="nofollow">分类与定位问题的权衡</a></p> 
<p id="R-FCN-toc" style="margin-left:120px;"><a href="#R-FCN" rel="nofollow">R-FCN</a></p> 
<p id="Deformable%20Convolution%20Networks-toc" style="margin-left:120px;"><a href="#Deformable%20Convolution%20Networks" rel="nofollow">Deformable Convolution Networks</a></p> 
<p id="%E7%89%B9%E5%BE%81%E5%A4%8D%E7%94%A8%E3%80%81%E5%AE%9E%E6%97%B6%E6%80%A7-toc" style="margin-left:40px;"><a href="#%E7%89%B9%E5%BE%81%E5%A4%8D%E7%94%A8%E3%80%81%E5%AE%9E%E6%97%B6%E6%80%A7" rel="nofollow">特征复用、实时性</a></p> 
<p id="%E7%89%B9%E5%BE%81%E5%A4%8D%E7%94%A8%E4%B8%8E%E6%95%B4%E5%90%88-toc" style="margin-left:80px;"><a href="#%E7%89%B9%E5%BE%81%E5%A4%8D%E7%94%A8%E4%B8%8E%E6%95%B4%E5%90%88" rel="nofollow">特征复用与整合</a></p> 
<p id="FPN-toc" style="margin-left:120px;"><a href="#FPN" rel="nofollow">FPN</a></p> 
<p id="TDM-toc" style="margin-left:120px;"><a href="#TDM" rel="nofollow">TDM</a></p> 
<p id="DSSD-toc" style="margin-left:120px;"><a href="#DSSD" rel="nofollow">DSSD</a></p> 
<p id="RON-toc" style="margin-left:120px;"><a href="#RON" rel="nofollow">RON</a></p> 
<p id="FSSD-toc" style="margin-left:120px;"><a href="#FSSD" rel="nofollow">FSSD</a></p> 
<p id="RefineDet-toc" style="margin-left:120px;"><a href="#RefineDet" rel="nofollow">RefineDet</a></p> 
<p id="%E9%9D%A2%E5%90%91%E5%AE%9E%E6%97%B6%E6%80%A7%E7%9A%84%E5%B7%A5%E4%BD%9C-toc" style="margin-left:80px;"><a href="#%E9%9D%A2%E5%90%91%E5%AE%9E%E6%97%B6%E6%80%A7%E7%9A%84%E5%B7%A5%E4%BD%9C" rel="nofollow">面向实时性的工作</a></p> 
<p id="Light%20Head%20R-CNN-toc" style="margin-left:120px;"><a href="#Light%20Head%20R-CNN" rel="nofollow">Light Head R-CNN</a></p> 
<p id="YOLOv2-toc" style="margin-left:120px;"><a href="#YOLOv2" rel="nofollow">YOLOv2</a></p> 
<p id="SSDLite(MobileNets%20V2)-toc" style="margin-left:120px;"><a href="#SSDLite%28MobileNets%20V2%29" rel="nofollow">SSDLite(MobileNets V2)</a></p> 
<p id="%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%96%B0%E8%B6%8B%E5%8A%BF-toc" style="margin-left:40px;"><a href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%96%B0%E8%B6%8B%E5%8A%BF" rel="nofollow">目标检测新趋势</a></p> 
<p id="YOLO9000-toc" style="margin-left:80px;"><a href="#YOLO9000" rel="nofollow">YOLO9000</a></p> 
<p id="Mask%20R-CNN-toc" style="margin-left:80px;"><a href="#Mask%20R-CNN" rel="nofollow">Mask R-CNN</a></p> 
<p id="Focal%20Loss%EF%BC%88RetinaNet%EF%BC%89-toc" style="margin-left:80px;"><a href="#Focal%20Loss%EF%BC%88RetinaNet%EF%BC%89" rel="nofollow">Focal Loss（RetinaNet）</a></p> 
<p id="Mimicking-toc" style="margin-left:80px;"><a href="#Mimicking" rel="nofollow">Mimicking</a></p> 
<p id="CGBN%EF%BC%88Cross%20GPU%20Batch%20Normalization%EF%BC%89-toc" style="margin-left:80px;"><a href="#CGBN%EF%BC%88Cross%20GPU%20Batch%20Normalization%EF%BC%89" rel="nofollow">CGBN（Cross GPU Batch Normalization）</a></p> 
<p id="DSOD%EF%BC%88Deeply%20Supervised%20Object%20Detector%EF%BC%89-toc" style="margin-left:80px;"><a href="#DSOD%EF%BC%88Deeply%20Supervised%20Object%20Detector%EF%BC%89" rel="nofollow">DSOD（Deeply Supervised Object Detector）</a></p> 
<p id="A-Fast-RCNN-toc" style="margin-left:80px;"><a href="#A-Fast-RCNN" rel="nofollow">A-Fast-RCNN</a></p> 
<p id="Relation%20Module-toc" style="margin-left:80px;"><a href="#Relation%20Module" rel="nofollow">Relation Module</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="%E5%89%8D%E8%A8%80">前言</h2> 
<p>如何理解一张图片？根据后续任务的需要，有三个主要的层次。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="149" src="https://images2.imgbox.com/db/25/Y8rEGUCw_o.jpg" width="720"></p> 
<p><em>图像理解的三个层次</em></p> 
<p><strong>一是分类（Classification）</strong>，即是将图像结构化为某一类别的信息，用事先确定好的类别(string)或实例ID来描述图片。这一任务是最简单、最基础的图像理解任务，也是深度学习模型最先取得突破和实现大规模应用的任务。其中，ImageNet是最权威的评测集，每年的ILSVRC催生了大量的优秀深度网络结构，为其他任务提供了基础。在应用领域，人脸、场景的识别等都可以归为分类任务。</p> 
<p><strong>二是检测（Detection）。</strong>分类任务关心整体，给出的是整张图片的内容描述，而检测则关注特定的物体目标，要求同时获得这一目标的类别信息和位置信息。相比分类，检测给出的是对图片前景和背景的理解，我们需要从背景中分离出感兴趣的目标，并确定这一目标的描述（类别和位置），因而，检测模型的输出是一个列表，列表的每一项使用一个数据组给出检出目标的类别和位置（常用矩形检测框的坐标表示）。</p> 
<p><strong>三是分割（Segmentation）。</strong>分割包括语义分割（semantic segmentation）和实例分割（instance segmentation），前者是对前背景分离的拓展，要求分离开具有不同语义的图像部分，而后者是检测任务的拓展，要求描述出目标的轮廓（相比检测框更为精细）。分割是对图像的像素级描述，它赋予每个像素类别（实例）意义，适用于理解要求较高的场景，如无人驾驶中对道路和非道路的分割。</p> 
<h2 id="%E7%AE%97%E6%B3%95%E6%A8%A1%E5%9E%8B">算法模型</h2> 
<p></p> 
<p class="img-center"><img alt="图片" height="262" src="https://images2.imgbox.com/b9/33/HJleNCAG_o.jpg" width="720"></p> 
<h4 id="%E4%B8%A4%E9%98%B6%E6%AE%B5%EF%BC%882-stage%EF%BC%89%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B"><strong>两阶段（2-stage）检测模型</strong></h4> 
<p>两阶段模型因其对图片的两阶段处理得名，也称为基于区域（Region-based）的方法，我们选取R-CNN系列工作作为这一类型的代表。</p> 
<h5 id="R-CNN%3A%20R-CNN%E7%B3%BB%E5%88%97%E7%9A%84%E5%BC%80%E5%B1%B1%E4%B9%8B%E4%BD%9C">R-CNN: R-CNN系列的开山之作</h5> 
<blockquote> 
 <p>Rich feature hierarchies for accurate object detection and semantic segmentation</p> 
 <p>论文链接： https://arxiv.org/abs/1311.2524</p> 
</blockquote> 
<p><strong>本文的两大贡献：</strong></p> 
<p>1）CNN可用于基于区域的定位和分割物体；</p> 
<p>2）监督训练样本数紧缺时，在额外的数据上预训练的模型经过fine-tuning可以取得很好的效果。</p> 
<p>第一个贡献影响了之后几乎所有2-stage方法，而第二个贡献中用分类任务（Imagenet）中训练好的模型作为基网络，在检测问题上fine-tuning的做法也在之后的工作中一直沿用。</p> 
<p>传统的计算机视觉方法常用精心设计的手工特征(如SIFT, HOG)描述图像，而深度学习的方法则倡导习得特征，从图像分类任务的经验来看，CNN网络自动习得的特征取得的效果已经超出了手工设计的特征。本篇在局部区域应用卷积网络，以发挥卷积网络学习高质量特征的能力。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="248" src="https://images2.imgbox.com/cf/8f/fAFrAE5r_o.jpg" width="720"></p> 
<p><em>R-CNN网络结构</em></p> 
<p>R-CNN将检测抽象为两个过程，一是基于图片提出若干可能包含物体的区域（即图片的局部裁剪，被称为Region Proposal），文中使用的是Selective Search算法；二是在提出的这些区域上运行当时表现最好的分类网络（AlexNet），得到每个区域内物体的类别。</p> 
<p>另外，文章中的两个做法值得注意。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="187" src="https://images2.imgbox.com/50/20/H2ty0T6w_o.jpg" width="240"></p> 
<p><em>IoU的计算</em></p> 
<p><strong>一是数据的准备。</strong>输入CNN前，我们需要根据Ground Truth对提出的Region Proposal进行标记，这里使用的指标是IoU（Intersection over Union，交并比）。IoU计算了两个区域之交的面积跟它们之并的比，描述了两个区域的重合程度。</p> 
<p>文章中特别提到，IoU阈值的选择对结果影响显著，这里要谈两个threshold，一个用来识别正样本（如跟ground truth的IoU大于0.5），另一个用来标记负样本（即背景类，如IoU小于0.1），而介于两者之间的则为难例（Hard Negatives），若标为正类，则包含了过多的背景信息，反之又包含了要检测物体的特征，因而这些Proposal便被忽略掉。</p> 
<p><strong>另一点是位置坐标的回归（Bounding-Box Regression）</strong>，这一过程是Region Proposal向Ground Truth调整，实现时加入了log/exp变换来使损失保持在合理的量级上，可以看做一种标准化（Normalization)操作。</p> 
<p>小结</p> 
<p>R-CNN的想法直接明了，即将检测任务转化为区域上的分类任务，是深度学习方法在检测任务上的试水。模型本身存在的问题也很多，如需要训练三个不同的模型（proposal, classification, regression）、重复计算过多导致的性能问题等。尽管如此，这篇论文的很多做法仍然广泛地影响着检测任务上的深度模型革命，后续的很多工作也都是针对改进这一工作而展开，此篇可以称得上"The First Paper"。</p> 
<h5 id="Fast%20R-CNN%3A%20%E5%85%B1%E4%BA%AB%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97"><strong>Fast R-CNN: 共享卷积运算</strong></h5> 
<blockquote> 
 <p>Fast R-CNN</p> 
 <p>论文链接：https://arxiv.org/abs/1504.08083</p> 
</blockquote> 
<p>文章指出R-CNN耗时的原因是CNN是在每一个Proposal上单独进行的，没有共享计算，便提出将基础网络在图片整体上运行完毕后，再传入R-CNN子网络，共享了大部分计算，故有Fast之名。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="287" src="https://images2.imgbox.com/c9/72/Wm4BThbN_o.jpg" width="720"></p> 
<p><em>Fast R-CNN网络结构</em></p> 
<p>上图是Fast R-CNN的架构。图片经过feature extractor得到feature map, 同时在原图上运行Selective Search算法并将RoI（Region of Interset，实为坐标组，可与Region Proposal混用）映射到到feature map上，再对每个RoI进行RoI Pooling操作便得到等长的feature vector，将这些得到的feature vector进行正负样本的整理（保持一定的正负样本比例），分batch传入并行的R-CNN子网络，同时进行分类和回归，并将两者的损失统一起来。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="388" src="https://images2.imgbox.com/7f/0f/7Etstt4J_o.jpg" width="373"></p> 
<p><em>RoI Pooling图示，来源：https://blog.deepsense.ai/region-of-interest-pooling-explained/</em></p> 
<p>RoI Pooling 是对输入R-CNN子网络的数据进行准备的关键操作。我们得到的区域常常有不同的大小，在映射到feature map上之后，会得到不同大小的特征张量。RoI Pooling先将RoI等分成目标个数的网格，再在每个网格上进行max pooling，就得到等长的RoI feature vector。</p> 
<p>文章最后的讨论也有一定的借鉴意义：</p> 
<ul><li> <p>multi-loss traing相比单独训练classification确有提升</p> </li><li> <p>multi-scale相比single-scale精度略有提升，但带来的时间开销更大。一定程度上说明CNN结构可以内在地学习尺度不变性</p> </li><li> <p>在更多的数据(VOC)上训练后，精度是有进一步提升的</p> </li><li> <p>Softmax分类器比"one vs rest"型的SVM表现略好，引入了类间的竞争</p> </li><li> <p>更多的Proposal并不一定带来精度的提升</p> </li></ul> 
<p>小结</p> 
<p>Fast R-CNN的这一结构正是检测任务主流2-stage方法所采用的元结构的雏形。</p> 
<p>文章将Proposal, Feature Extractor, Object Classification&amp;Localization统一在一个整体的结构中，并通过共享卷积计算提高特征利用效率，是最有贡献的地方。</p> 
<h5 id="Faster%20R-CNN%3A%20%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%8C%96"><strong>Faster R-CNN: 两阶段模型的深度化</strong></h5> 
<blockquote> 
 <p>Faster R-CNN: Towards Real Time Object Detection with Region Proposal Networks</p> 
 <p>论文链接：https://arxiv.org/abs/1506.01497</p> 
</blockquote> 
<p>Faster R-CNN是2-stage方法的奠基性工作，提出的RPN网络取代Selective Search算法使得检测任务可以由神经网络端到端地完成。粗略的讲，Faster R-CNN = RPN + Fast R-CNN，跟RCNN共享卷积计算的特性使得RPN引入的计算量很小，使得Faster R-CNN可以在单个GPU上以5fps的速度运行，而在精度方面达到SOTA（State of the Art，当前最佳）。</p> 
<p>本文的主要贡献是提出Regional Proposal Networks，替代之前的SS算法。RPN网络将Proposal这一任务建模为二分类（是否为物体）的问题。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="198" src="https://images2.imgbox.com/24/a0/XUneIPzR_o.jpg" width="720"></p> 
<p><em>Faster R-CNN网络结构</em></p> 
<p>第一步是在一个滑动窗口上生成不同大小和长宽比例的anchor box（如上图右边部分），取定IoU的阈值，按Ground Truth标定这些anchor box的正负。于是，传入RPN网络的样本数据被整理为anchor box（坐标）和每个anchor box是否有物体（二分类标签）。</p> 
<p>RPN网络将每个样本映射为一个概率值和四个坐标值，概率值反应这个anchor box有物体的概率，四个坐标值用于回归定义物体的位置。最后将二分类和坐标回归的损失统一起来，作为RPN网络的目标训练。</p> 
<p>由RPN得到Region Proposal在根据概率值筛选后经过类似的标记过程，被传入R-CNN子网络，进行多分类和坐标回归，同样用多任务损失将二者的损失联合。</p> 
<p><strong>小结</strong></p> 
<p>Faster R-CNN的成功之处在于用RPN网络完成了检测任务的"深度化"。使用滑动窗口生成anchor box的思想也在后来的工作中越来越多地被采用（YOLO v2等）。这项工作奠定了"RPN+RCNN"的两阶段方法元结构，影响了大部分后续工作。</p> 
<h4 id="%E5%8D%95%E9%98%B6%E6%AE%B5%EF%BC%881-stage%EF%BC%89%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B"><strong>单阶段（1-stage）检测模型</strong></h4> 
<p>单阶段模型没有中间的区域检出过程，直接从图片获得预测结果，也被成为Region-free方法。</p> 
<h5 id="YOLO"><strong>YOLO</strong></h5> 
<blockquote> 
 <p>You Only Look Once: Unified, Real-Time Object Detection</p> 
 <p>论文链接：https://arxiv.org/abs/1506.02640</p> 
</blockquote> 
<p>YOLO是单阶段方法的开山之作。它将检测任务表述成一个统一的、端到端的回归问题，并且以只处理一次图片同时得到位置和分类而得名。</p> 
<p><strong>YOLO的主要优点：</strong></p> 
<ul><li> <p>快。</p> </li><li> <p>全局处理使得背景错误相对少，相比基于局部（区域）的方法， 如Fast RCNN。</p> </li><li> <p>泛化性能好，在艺术作品上做检测时，YOLO表现比Fast R-CNN好。</p> </li></ul> 
<p></p> 
<p class="img-center"><img alt="图片" height="288" src="https://images2.imgbox.com/9b/f1/CzYiDYoc_o.jpg" width="720"></p> 
<p><em>YOLO网络结构</em></p> 
<p><strong>YOLO的工作流程如下：</strong></p> 
<p><strong>1.准备数据：</strong>将图片缩放，划分为等分的网格，每个网格按跟Ground Truth的IoU分配到所要预测的样本。</p> 
<p><strong>2.卷积网络：</strong>由GoogLeNet更改而来，每个网格对每个类别预测一个条件概率值，并在网格基础上生成B个box，每个box预测五个回归值，四个表征位置，第五个表征这个box含有物体（注意不是某一类物体）的概率和位置的准确程度（由IoU表示）。测试时，分数如下计算：</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="46" src="https://images2.imgbox.com/5d/ec/zS74VxEt_o.jpg" width="533"></p> 
<p>等式左边第一项由网格预测，后两项由每个box预测，以条件概率的方式得到每个box含有不同类别物体的分数。因而，卷积网络共输出的预测值个数为S×S×(B×5+C)，其中S为网格数，B为每个网格生成box个数，C为类别数。</p> 
<p><strong>3.后处理：</strong>使用NMS（Non-Maximum Suppression，非极大抑制）过滤得到最后的预测框。</p> 
<p>损失函数的设计</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="497" src="https://images2.imgbox.com/96/06/WPQDctFn_o.jpg" width="720"></p> 
<p><em>YOLO的损失函数分解，来源：https://zhuanlan.zhihu.com/p/24916786</em></p> 
<p>损失函数被分为三部分：坐标误差、物体误差、类别误差。为了平衡类别不均衡和大小物体等带来的影响，损失函数中添加了权重并将长宽取根号。</p> 
<p>小结</p> 
<p>YOLO提出了单阶段的新思路，相比两阶段方法，其速度优势明显，实时的特性令人印象深刻。但YOLO本身也存在一些问题，如划分网格较为粗糙，每个网格生成的box个数等限制了对小尺度物体和相近物体的检测。</p> 
<h5 id="SSD%3A%20Single%20Shot%20Multibox%20Detector"><strong>SSD: Single Shot Multibox Detector</strong></h5> 
<blockquote> 
 <p>SSD: Single Shot Multibox Detector</p> 
 <p>论文链接：https://arxiv.org/abs/1512.02325</p> 
</blockquote> 
<p></p> 
<p class="img-center"><img alt="图片" height="213" src="https://images2.imgbox.com/1e/d8/o2OGI7la_o.jpg" width="720"></p> 
<p><em>SSD网络结构</em></p> 
<p>SSD相比YOLO有以下突出的特点：</p> 
<ul><li> <p>多尺度的feature map：基于VGG的不同卷积段，输出feature map到回归器中。这一点试图提升小物体的检测精度。</p> </li><li> <p>更多的anchor box，每个网格点生成不同大小和长宽比例的box，并将类别预测概率基于box预测（YOLO是在网格上），得到的输出值个数为(C+4)×k×m×n，其中C为类别数，k为box个数，m×n为feature map的大小。</p> </li></ul> 
<p><strong>小结</strong></p> 
<p>SSD是单阶段模型早期的集大成者，达到跟接近两阶段模型精度的同时，拥有比两阶段模型快一个数量级的速度。后续的单阶段模型工作大多基于SSD改进展开。</p> 
<p>检测模型基本特点</p> 
<p>最后，我们对检测模型的基本特征做一个简单的归纳。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="154" src="https://images2.imgbox.com/a7/b6/BGD4DqwK_o.jpg" width="720"></p> 
<p><em>两阶段检测模型Pipeline，来源：https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/</em></p> 
<p>检测模型整体上由基础网络（Backbone Network）和检测头部（Detection Head）构成。前者作为特征提取器，给出图像不同大小、不同抽象层次的表示；后者则依据这些表示和监督信息学习类别和位置关联。检测头部负责的类别预测和位置回归两个任务常常是并行进行的，构成多任务的损失进行联合训练。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="283" src="https://images2.imgbox.com/e0/28/Dx4i9Nqk_o.jpg" width="720"></p> 
<p><em>检测模型头部并行的分支，来源同上</em></p> 
<p>相比单阶段，两阶段检测模型通常含有一个串行的头部结构，即完成前背景分类和回归后，把中间结果作为RCNN头部的输入再进行一次多分类和位置回归。这种设计带来了一些优点：</p> 
<ul><li> <p>对检测任务的解构，先进行前背景的分类，再进行物体的分类，这种解构使得监督信息在不同阶段对网络参数的学习进行指导</p> </li><li> <p>RPN网络为RCNN网络提供良好的先验，并有机会整理样本的比例，减轻RCNN网络的学习负担</p> </li></ul> 
<p>这种设计的缺点也很明显：中间结果常常带来空间开销，而串行的方式也使得推断速度无法跟单阶段相比；级联的位置回归则会导致RCNN部分的重复计算（如两个RoI有重叠）。</p> 
<p>另一方面，单阶段模型只有一次类别预测和位置回归，卷积运算的共享程度更高，拥有更快的速度和更小的内存占用。读者将会在接下来的文章中看到，两种类型的模型也在互相吸收彼此的优点，这也使得两者的界限更为模糊。</p> 
<h3 id="%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E6%B5%8B%E4%B8%8E%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7"><strong>模型的评测与训练技巧</strong></h3> 
<p></p> 
<p class="img-center"><img alt="图片" height="397" src="https://images2.imgbox.com/61/4f/AdOPtOFp_o.jpg" width="720"></p> 
<h4 id="%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E6%B5%8B%E6%8C%87%E6%A0%87"><strong>检测模型的评测指标</strong></h4> 
<p>目标检测模型本源上可以用统计推断的框架描述，我们关注其犯第一类错误和第二类错误的概率，通常用准确率和召回率来描述。准确率描述了模型有多准，即在预测为正例的结果中，有多少是真正例；召回率则描述了模型有多全，即在为真的样本中，有多少被我们的模型预测为正例。不同的任务，对两类错误有不同的偏好，常常在某一类错误不多于一定阈值的情况下，努力减少另一类错误。在检测中，mAP（mean Average Precision）作为一个统一的指标将这两种错误兼顾考虑。</p> 
<p>具体地，对于每张图片，检测模型输出多个预测框（常常远超真实框的个数），我们使用IoU（Intersection Over Union，交并比）来标记预测框是否为预测正确。标记完成后，随着预测框的增多，召回率总会提升，在不同的召回率水平下对准确率做平均，即得到AP，最后再对所有类别按其所占比例做平均，即得到mAP。</p> 
<p>在较早的Pascal VOC数据集上，常采用固定的一个IoU阈值（如0.5, 0.75）来计算mAP，现阶段较为权威的MS COCO数据集上，对不同的IoU阈值（0.5-0.95，0.05为步长）分别计算AP，再综合平均，并且给出了不同大小物体分别的AP表现，对定位准确的模型给予奖励并全面地展现不同大小物体上检测算法的性能，更为科学合理。</p> 
<p>在实践中，我们不仅关注检测模型的精度，还关注其运行的速度，常常用FPS（Frame Per Second，每秒帧率）来表示检测模型能够在指定硬件上每秒处理图片的张数。通常来讲，在单块GPU上，两阶段方法的FPS一般在个位数，而单阶段方法可以达到数十。现在检测模型运行的平台并不统一，实践中也不能部署较为昂贵的GPU进行推断。事实上，很多文章并没有严谨讨论其提出模型的速度表现（加了较多的trick以使精度达到SOTA），另外，考虑到目前移动端专用芯片的发展速度和研究进展，速度方面的指标可能较难形成统一的参考标准，需要谨慎看待文章中汇报的测试结果。</p> 
<h4 id="%E6%A0%87%E5%87%86%E8%AF%84%E6%B5%8B%E6%95%B0%E6%8D%AE%E9%9B%86"><strong>标准评测数据集</strong></h4> 
<p><strong>Pascal VOC（Pascal Visual Object Classes）</strong></p> 
<p>链接：http://host.robots.ox.ac.uk/pascal/VOC/</p> 
<p>自2005年起每年举办一次比赛，最开始只有4类，到2007年扩充为20个类，共有两个常用的版本：2007和2012。学术界常用5k的trainval2007和16k的trainval2012作为训练集（07+12），test2007作为测试集，用10k的trainval2007+test2007和和16k的trainval2012作为训练集（07++12），test2012作为测试集，分别汇报结果。</p> 
<p>Pascal VOC对早期检测工作起到了重要的推动作用，目前提升的空间相对有限，权威评测集的交接棒也逐渐传给了下面要介绍的COCO。</p> 
<p><strong>MS COCO（Common Objects in COntext-http://cocodataset.org）</strong></p> 
<p></p> 
<p class="img-center"><img alt="图片" height="442" src="https://images2.imgbox.com/8c/35/kfMWXExn_o.jpg" width="720"></p> 
<p><em>检测任务在COCO数据集上的进展</em></p> 
<p>COCO数据集收集了大量包含常见物体的日常场景图片，并提供像素级的实例标注以更精确地评估检测和分割算法的效果，致力于推动场景理解的研究进展。依托这一数据集，每年举办一次比赛，现已涵盖检测、分割、关键点识别、注释等机器视觉的中心任务，是继ImageNet Chanllenge以来最有影响力的学术竞赛之一。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="245" src="https://images2.imgbox.com/98/fc/TsbvbYzk_o.jpg" width="720"></p> 
<p><em>iconic与non-iconic图片对比</em></p> 
<p>相比ImageNet，COCO更加偏好目标与其场景共同出现的图片，即non-iconic images。这样的图片能够反映视觉上的语义，更符合图像理解的任务要求。而相对的iconic images则更适合浅语义的图像分类等任务。</p> 
<p>COCO的检测任务共含有80个类，在2014年发布的数据规模分train/val/test分别为80k/40k/40k，学术界较为通用的划分是使用train和35k的val子集作为训练集（trainval35k），使用剩余的val作为测试集（minival），同时向官方的evaluation server提交结果（test-dev）。除此之外，COCO官方也保留一部分test数据作为比赛的评测集。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="438" src="https://images2.imgbox.com/3c/fd/1b3lKTxm_o.jpg" width="720"></p> 
<p><em>COCO数据集分布</em></p> 
<p>在分布方面，COCO的每个类含有更多实例，分布也较为均衡（上图a），每张图片包含更多类和更多的实例（上图b和c，均为直方图，每张图片平均分别含3.3个类和7.7个实例），相比Pascal VOC，COCO还含有更多的小物体（下图，横轴是物体占图片的比例）。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="331" src="https://images2.imgbox.com/5a/33/3a5n92sV_o.jpg" width="577"></p> 
<p><em>COCO数据集物体大小分布</em></p> 
<p>如本文第一节所述，COCO提供的评测标准更为精细化，提供的API不仅包含了可视化、评测数据的功能，还有对模型的错误来源分析脚本，能够更清晰地展现算法的不足之处。COCO所建立的这些标准也逐渐被学术界认可，成为通用的评测标准。您可以在这里找到目前检测任务的LeaderBoard。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="595" src="https://images2.imgbox.com/ff/0b/4AuRygyK_o.jpg" width="720"></p> 
<p><em>错误来源分解，详见http://cocodataset.org/#detections-eval</em></p> 
<p><strong><em>Cityscapes（https://www.cityscapes-dataset.com）</em></strong></p> 
<p></p> 
<p class="img-center"><img alt="图片" height="149" src="https://images2.imgbox.com/1c/36/nQKTDP4M_o.jpg" width="300"></p> 
<p><em>Cityscapes数据示例</em></p> 
<p>Cityscapes数据集专注于现代城市道路场景的理解，提供了30个类的像素级标注，是自动驾驶方向较为权威的评测集。</p> 
<h4 id="%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84Bells%20and%20wisthles"><strong>检测模型中的Bells and wisthles</strong></h4> 
<p>本节介绍常见的提升检测模型性能的技巧，它们常作为trick在比赛中应用。其实，这样的名称有失公允，部分工作反映了作者对检测模型有启发意义的观察，有些具有成为检测模型标准组件的潜力（如果在早期的工作中即被应用则可能成为通用做法）。读者将它们都看作学术界对解决这一问题的努力即可。对研究者，诚实地报告所引用的其他工作并添加有说服力的消融实验（ablation expriments）以支撑自己工作的原创性和贡献之处，则是值得倡导的行为。</p> 
<p><strong>Data augmentation 数据增强</strong></p> 
<p>数据增强是增加深度模型鲁棒性和泛化性能的常用手段，随机翻转、随机裁剪、添加噪声等也被引入到检测任务的训练中来，其信念是通过数据的一般性来迫使模型学习到诸如对称不变性、旋转不变性等更一般的表示。通常需要注意标注的相应变换，并且会大幅增加训练的时间。个人认为数据（监督信息）的适时传入可能是更有潜力的方向。</p> 
<p><strong>Multi-scale Training/Testing 多尺度训练/测试</strong></p> 
<p>输入图片的尺寸对检测模型的性能影响相当明显，事实上，多尺度是提升精度最明显的技巧之一。在基础网络部分常常会生成比原图小数十倍的特征图，导致小物体的特征描述不容易被检测网络捕捉。通过输入更大、更多尺寸的图片进行训练，能够在一定程度上提高检测模型对物体大小的鲁棒性，仅在测试阶段引入多尺度，也可享受大尺寸和多尺寸带来的增益。</p> 
<p>multi-scale training/testing最早见于[1]，训练时，预先定义几个固定的尺度，每个epoch随机选择一个尺度进行训练。测试时，生成几个不同尺度的feature map，对每个Region Proposal，在不同的feature map上也有不同的尺度，我们选择最接近某一固定尺寸（即检测头部的输入尺寸）的Region Proposal作为后续的输入。在[2]中，选择单一尺度的方式被Maxout（element-wise max，逐元素取最大）取代：随机选两个相邻尺度，经过Pooling后使用Maxout进行合并，如下图所示</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="357" src="https://images2.imgbox.com/74/19/fz9cMzpX_o.jpg" width="661"></p> 
<p><em>使用Maxout合并feature vector</em></p> 
<p>近期的工作如FPN等已经尝试在不同尺度的特征图上进行检测，但多尺度训练/测试仍作为一种提升性能的有效技巧被应用在MS COCO等比赛中。</p> 
<p><strong>Global Context 全局语境</strong></p> 
<p>这一技巧在ResNet的工作[3]中提出，做法是把整张图片作为一个RoI，对其进行RoI Pooling并将得到的feature vector拼接于每个RoI的feature vector上，作为一种辅助信息传入之后的R-CNN子网络。目前，也有把相邻尺度上的RoI互相作为context共同传入的做法。</p> 
<p><strong>Box Refinement/Voting 预测框微调/投票法</strong></p> 
<p>微调法和投票法由工作[4]提出，前者也被称为Iterative Localization。微调法最初是在SS算法得到的Region Proposal基础上用检测头部进行多次迭代得到一系列box，在ResNet的工作中，作者将输入R-CNN子网络的Region Proposal和R-CNN子网络得到的预测框共同进行NMS（见下面小节）后处理，最后，把跟NMS筛选所得预测框的IoU超过一定阈值的预测框进行按其分数加权的平均，得到最后的预测结果。投票法可以理解为以顶尖筛选出一流，再用一流的结果进行加权投票决策。</p> 
<p><strong>OHEM 在线难例挖掘</strong></p> 
<p>OHEM(Online Hard negative Example Mining，在线难例挖掘)见于[5]。两阶段检测模型中，提出的RoI Proposal在输入R-CNN子网络前，我们有机会对正负样本（背景类和前景类）的比例进行调整。通常，背景类的RoI Proposal个数要远远多于前景类，Fast R-CNN的处理方式是随机对两种样本进行上采样和下采样，以使每一batch的正负样本比例保持在1:3，这一做法缓解了类别比例不均衡的问题，是两阶段方法相比单阶段方法具有优势的地方，也被后来的大多数工作沿用。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="319" src="https://images2.imgbox.com/6f/4a/cDDlrlCY_o.jpg" width="720"></p> 
<p><em>OHEM图解</em></p> 
<p>但在OHEM的工作中，作者提出用R-CNN子网络对RoI Proposal预测的分数来决定每个batch选用的样本，这样，输入R-CNN子网络的RoI Proposal总为其表现不好的样本，提高了监督学习的效率。实际操作中，维护两个完全相同的R-CNN子网络，其中一个只进行前向传播来为RoI Proposal的选择提供指导，另一个则为正常的R-CNN，参与损失的计算并更新权重，并且将权重复制到前者以使两个分支权重同步。</p> 
<p>OHEM以额外的R-CNN子网络的开销来改善RoI Proposal的质量，更有效地利用数据的监督信息，成为两阶段模型提升性能的常用部件之一。</p> 
<p><strong>Soft NMS 软化非极大抑制</strong></p> 
<p></p> 
<p class="img-center"><img alt="图片" height="447" src="https://images2.imgbox.com/b9/2e/PDM5WIhF_o.jpg" width="574"></p> 
<p><em>NMS后处理图示</em></p> 
<p>NMS(Non-Maximum Suppression，非极大抑制）是检测模型的标准后处理操作，用于去除重合度（IoU）较高的预测框，只保留预测分数最高的预测框作为检测输出。Soft NMS由[6]提出。在传统的NMS中，跟最高预测分数预测框重合度超出一定阈值的预测框会被直接舍弃，作者认为这样不利于相邻物体的检测。提出的改进方法是根据IoU将预测框的预测分数进行惩罚，最后再按分数过滤。配合Deformable Convnets（将在之后的文章介绍），Soft NMS在MS COCO上取得了当时最佳的表现。算法改进如下：</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="583" src="https://images2.imgbox.com/30/e9/kggE8brP_o.jpg" width="480"></p> 
<p><em>Soft-NMS算法改进</em></p> 
<p>上图中的f即为软化函数，通常取线性或高斯函数，后者效果稍好一些。当然，在享受这一增益的同时，Soft-NMS也引入了一些超参，对不同的数据集需要试探以确定最佳配置。</p> 
<p><strong>RoIAlign RoI对齐</strong></p> 
<p>RoIAlign是Mask R-CNN（[7]）的工作中提出的，针对的问题是RoI在进行Pooling时有不同程度的取整，这影响了实例分割中mask损失的计算。文章采用双线性插值的方法将RoI的表示精细化，并带来了较为明显的性能提升。这一技巧也被后来的一些工作（如light-head R-CNN）沿用。</p> 
<p><strong>拾遗</strong></p> 
<p>除去上面所列的技巧外，还有一些做法也值得注意：</p> 
<ul><li> <p>更好的先验（YOLOv2）：使用聚类方法统计数据中box标注的大小和长宽比，以更好的设置anchor box的生成配置</p> </li><li> <p>更好的pre-train模型：检测模型的基础网络通常使用ImageNet（通常是ImageNet-1k）上训练好的模型进行初始化，使用更大的数据集（ImageNet-5k）预训练基础网络对精度的提升亦有帮助</p> </li><li> <p>超参数的调整：部分工作也发现如NMS中IoU阈值的调整（从0.3到0.5）也有利于精度的提升，但这一方面尚无最佳配置参照</p> </li></ul> 
<p>最后，集成（Ensemble）作为通用的手段也被应用在比赛中。</p> 
<h3 id="%E5%9F%BA%E7%A1%80%E7%BD%91%E7%BB%9C%E6%BC%94%E8%BF%9B%E3%80%81%E5%88%86%E7%B1%BB%E4%B8%8E%E5%AE%9A%E4%BD%8D%E7%9A%84%E6%9D%83%E8%A1%A1"><strong>基础网络演进、分类与定位的权衡</strong></h3> 
<p></p> 
<p class="img-center"><img alt="图片" height="193" src="https://images2.imgbox.com/fd/41/DiKv6SkY_o.jpg" width="720"></p> 
<h4 id="%E5%9F%BA%E7%A1%80%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E7%9A%84%E6%BC%94%E8%BF%9B"><em>基础网络结构的演进</em></h4> 
<p>基础网络（Backbone network）作为特征提取器，对检测模型的性能有着至关重要的影响。在分类任务的权威评测集ImageNet上，基于卷积网络的方法已经取得超越人类水平的进步，并也促使ImageNet完成了她的历史使命。这也是机器视觉领域的整体进步，优秀的特征、深刻的解释都为其他任务的应用提供了良好的基础。在本节中，我们选取了几个在检测任务上成功应用的基础网络做一些介绍。</p> 
<h4 id="%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E6%BC%94%E8%BF%9B%E7%9A%84%E8%B6%8B%E5%8A%BF"><strong>卷积网络结构演进的趋势</strong></h4> 
<p>笔者认为，卷积网络已经有如下几个经典的设计范式：</p> 
<ul><li> <p><strong>Repeat.</strong> 由AlexNet和VGG等开拓，被之后几乎所有的网络采用。即堆叠相同的拓扑结构，整个网络成为模块化的结构。</p> </li><li> <p><strong>Multi-path.</strong> 由Inception系列发扬，将前一层的输入分割到不同的路径上进行变换，最后拼接结果。</p> </li><li> <p><strong>Skip-connection.</strong> 最初出现于Highway Network，由ResNet发扬并成为标配。即建立浅层信息与深层信息的传递通道，改变原有的单一线性结构。</p> </li></ul> 
<p>以这些范式为脉络整理卷积网络的演进历程，可以归纳出下面的图景：</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="429" src="https://images2.imgbox.com/16/a6/HR1W8kMs_o.jpg" width="720"></p> 
<p><em>CNN的经典设计范式</em></p> 
<p>需要说明的是，上图并不能概括完全近年来卷积网络的进步，各分支之间也有很多相互借鉴和共通的特征，而致力于精简网络结构的工作如SqueezeNet等则没有出现。除了上面归纳的三个范式，卷积网络结构方面另一个重要的潮流是深度可分离卷积（Depth-wise seperable convolution）的应用。下面我们选择几个在检测任务上成功应用的基础网络结构进行介绍。</p> 
<h4 id="ResNet%3A%20%E6%AE%8B%E5%B7%AE%E5%AD%A6%E4%B9%A0"><strong>ResNet: 残差学习</strong></h4> 
<blockquote> 
 <p>Deep Residual Learning for Image Recognition</p> 
 <p>https://arxiv.org/abs/1512.03385</p> 
</blockquote> 
<p></p> 
<p class="img-center"><img alt="图片" height="164" src="https://images2.imgbox.com/54/8a/jojf0cRe_o.jpg" width="308"></p> 
<p><em>残差单元将原函数分解为残差</em></p> 
<p>作者将网络的训练解释为对某一复杂函数的拟合，通过添加跳跃连接，变对这一函数的拟合为每层对某一残差的拟合（有点Boosting的意思），引入的恒等项也让BP得到的梯度更为稳定。</p> 
<p>残差网络以skip-connection的设计较为成功地缓解了深层网络难以收敛的问题，将网络的深度提高了一个数量级，也带动了一系列对残差网络的解释研究和衍生网络的提出。</p> 
<p>在检测领域，VGG作为特征提取器的地位也逐渐被ResNet系列网络替代，文章中以ResNet作为基础网络的Faster R-CNN也常作为后续工作的基线进行比较。</p> 
<p><strong>Xception：可分离卷积的大面积应用</strong></p> 
<blockquote> 
 <p>Xception: Deep Learning with Depthwise Separable Convolutions</p> 
 <p>https://arxiv.org/abs/1610.02357</p> 
</blockquote> 
<p>Xception网络可以看做对Inception系列网络的推进，也是深度可分离卷积的成功应用。</p> 
<p>文章指出，Inception单元背后的假设是跨Channel和跨空间的相关性可以充分解耦，类似的还有长度和高度方向上的卷积结构（在Inception-v3里的3×3卷积被1×3和3×1卷积替代）。</p> 
<p>进一步的，Xception基于更强的假设：跨channel和跨空间的相关性完全解耦。这也是深度可分离卷积所建模的理念。</p> 
<p>一个简化的Inception单元：</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="352" src="https://images2.imgbox.com/c8/5d/4Do9l2dg_o.jpg" width="476"></p> 
<p><em>简化的Inception单元，去掉了Pooling分支</em></p> 
<p>等价于：</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="380" src="https://images2.imgbox.com/92/42/jy75iRlG_o.jpg" width="592"></p> 
<p><em>等价的简化Inception单元，将1x1卷积合并</em></p> 
<p>将channel的group推向极端，即每个channel都由独立的3×3卷积处理：</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="359" src="https://images2.imgbox.com/89/57/k4RxCiIZ_o.jpg" width="589"></p> 
<p><em>把分组的粒度降为1</em></p> 
<p>这样就得到了深度可分离卷积。</p> 
<p>Xception最终的网络结构如下，简单讲是线性堆叠的Depthwise Separable卷积，并附加了Skip-connection。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="471" src="https://images2.imgbox.com/a9/75/xquD9oZn_o.jpg" width="720"></p> 
<p><em>Xceptiong的网络结构</em></p> 
<p>在MS COCO Chanllege 2017中，MSRA团队以对齐版本的Xception为基础网络取得前列的成绩，一定程度上说明了这一网络提取特征的能力；另一方面，Xception的一个改编版本也被Light-head R-CNN的工作（将在下一篇的实时性部分介绍）应用，以两阶段的方式取得了精度和速度都超越SSD等单阶段检测器的表现。</p> 
<p><strong>ResNeXt：新的维度</strong></p> 
<blockquote> 
 <p>Aggregated Residual Transformations for Deep Neural Networks</p> 
 <p>https://arxiv.org/abs/1611.05431</p> 
</blockquote> 
<p>本文提出了深度网络的新维度，除了深度、宽度（Channel数）外，作者将在某一层并行transform的路径数提取为第三维度，称为"cardinality"。跟Inception单元不同的是，这些并行路径均共享同一拓扑结构，而非精心设计的卷积核并联。除了并行相同的路径外，也添加了层与层间的shortcut connection。</p> 
<p>相比Inception-ResNet，ResNeXt相当于将其Inception Module的每条路径规范化了，并将规范后的路径数目作为新的超参数。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="351" src="https://images2.imgbox.com/9e/a3/29zsr6Ws_o.jpg" width="592"></p> 
<p><em>ResNeXt的基本单元</em></p> 
<p>上图中，路径被扩展为多条，而每条路径的宽度（channel数）也变窄了（64-&gt;4）。</p> 
<p>在近期Facebook开源的Detectron框架中，ResNeXt作为Mask R-CNN的基础网络也取得了非常高的精度。</p> 
<p><strong>SENet：卷积网络的Attention组件</strong></p> 
<blockquote> 
 <p>Squeeze and Excitation Network</p> 
 <p>https://arxiv.org/abs/1709.01507</p> 
</blockquote> 
<p>SENet是最后一届ImageNet Challenge的夺冠架构，中心思想是添加旁路为channel之间的相关性进行建模，可以认为是channel维度的attention。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="167" src="https://images2.imgbox.com/61/21/1gasQr5F_o.jpg" width="720"></p> 
<p><em>Squeeze和Excitation分支</em></p> 
<p>SENet通过'特征重标定'（Feature Recalibration）来完成channel层面的注意力机制。具体地，先通过Squeeze操作将特征的空间性压缩掉，仅保留channel维度，再通过Excitation操作为每个特征通道生成一个权重，用于显式地建模channel之间的相关性，最后通过Reweight操作将权重加权到原来的channel上，即构成不同channel间重标定。</p> 
<p>SENet可以作为网络中模块间的额外连接附属到原有的经典结构上，其Squeeze操作在压缩信息的同时也降低了这一附属连接的开销。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="350" src="https://images2.imgbox.com/7d/e1/hNAclenP_o.jpg" width="640"></p> 
<p><em>SE作为额外部件添加在经典结构上</em></p> 
<p>经SENet改进的ResNet被UCenter团队应用在MS COCO Chanllenge 2017上，也取得了不错的效果。</p> 
<p><strong>NASNet：网络结构搜索</strong></p> 
<blockquote> 
 <p>Learning Transferable Architectures for Scalable Image Recognition</p> 
 <p>https://arxiv.org/abs/1707.07012</p> 
</blockquote> 
<p>NAS（Neural Architecture Searh，神经网络结构搜索）的框架最早出现于作者的另一项工作Neural Architecture Search with Reinforcement Learning，其核心思想是用一个RNN学习定义网络结构的超参，通过强化学习的框架来更新这一RNN来得到更好表现的网络结构。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="289" src="https://images2.imgbox.com/94/75/FktraJ9e_o.jpg" width="600"></p> 
<p><em>NAS的结构</em></p> 
<p>在本文中，作者参考本节最初提到的"Repeat"范式，认为在小数据集上搜索到的结构单元具有移植性和扩展性，将这个结构单元通过堆叠得到的大网络能够在较大数据集上取得较好的表现。这就构成了文章的基本思路：将网络搜索局限在微观的局部结构上，以相对原工作较小的开销（实际开销仍然巨大）得到可供扩展的网络单元，再由这些单元作为基本部件填入人工设计的"元结构"。</p> 
<p>微观层面，作者仍选择用RNN作为Controller，挑选跳跃连接、最大池化、空洞卷积、深度可分离卷积等等操作构成基本搜索空间，以逐元素相加（element-wise addition）和拼接（concatenation）作为合并操作，并重复一定的构建次数来搜索此基本单元。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="225" src="https://images2.imgbox.com/af/35/8C0anxln_o.jpg" width="720"></p> 
<p><em>RNN作为Controller的微观结构搜索，右为示例结构</em></p> 
<p>宏观层面，将基本单元分为Normal Cell（不改变feature map大小）和Reduction Cell（使feature map的spatial维度减半，即stride=2），交替堆叠一定数量的Normal Cell和Reduction Cell形成下面的元结构。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="629" src="https://images2.imgbox.com/a4/da/2v52y8YX_o.jpg" width="512"></p> 
<p><em>NASNet在不同数据集上的元结构，ImageNet的图片具有更多的像素数，需要更多的Reduction单元</em></p> 
<p>NASNet采取了自动搜索的方式去设计网络的结构，人工的部分迁移到对搜索空间的构建和评测指标的设立上，是一种"元学习"的策略。应用在检测领域，NASNet作为基础框架的Faster R-CNN取得了SOTA的表现，也支撑了这一搜索得到结构的泛化性能。在最近的工作中，作者团队又设计了ENAS降低搜索的空间和时间开销，继续推动着这一方向的研究。</p> 
<h4 id="%E5%88%86%E7%B1%BB%E4%B8%8E%E5%AE%9A%E4%BD%8D%E9%97%AE%E9%A2%98%E7%9A%84%E6%9D%83%E8%A1%A1"><strong>分类与定位问题的权衡</strong></h4> 
<p>从R-CNN开始，检测模型常采用分类任务上表现最好的卷积网络作为基础网络提取特征，在其基础上添加额外的头部结构来实现检测功能。然而，分类和检测所面向的场景不尽相同：分类常常关注具有整体语义的图像（第二篇中介绍COCO数据集中提到的iconic image），而检测则需要区分前景和背景（non-iconic image）。</p> 
<p>分类网络中的Pooling层操作常常会引入平移不变性等使得整体语义的理解更加鲁棒，而在检测任务中我们则需要位置敏感的模型来保证预测位置的精确性，这就产生了分类和定位两个任务间的矛盾。</p> 
<h5 id="R-FCN"><strong>R-FCN</strong></h5> 
<blockquote> 
 <p>R-FCN: Object Detection via Region-based Fully Convolutinal Networks</p> 
 <p>https://arxiv.org/abs/1605.06409</p> 
</blockquote> 
<p>文章指出了检测任务之前的框架存在不自然的设计，即全卷积的特征提取部分+全连接的分类器，而表现最好的图像分类器都是全卷积的结构（ResNet等）。这篇文章提出采用"位置敏感分数图（Position Sensitive Score Map）"的方法来使检测网络保持全卷积结构的同时又拥有位置感知能力。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="351" src="https://images2.imgbox.com/82/79/TVJaQIsY_o.jpg" width="682"></p> 
<p><em>R-FCN中位置敏感分数图</em></p> 
<p>位置敏感分数图的生成有两个重要操作，一是生成更"厚"的feature map，二是在RoI Pooling时选择性地输入feature map。</p> 
<p>Faster R-CNN中，经过RPN得到RoI，转化成分类任务，还加入了一定量的卷积操作（ResNet中的conv5部分），而这一部分卷积操作是不能共享的。R-FCN则着眼于全卷积结构，利用卷积操作在Channel这一维度上的自由性，赋予其位置敏感的意义。下面是具体的操作：</p> 
<ul><li> <p>在全卷积网络的最后一层，生成 </p> <p class="img-center"><img alt="图片" height="27" src="https://images2.imgbox.com/41/6f/tZc1ClPN_o.png" width="87"></p> <p>个Channel的Feature map，其中 C为类别数， </p> <p class="img-center"><img alt="图片" height="21" src="https://images2.imgbox.com/c3/80/m5BQEpHv_o.png" width="22"></p> <p>代表k*k网格，用于分别检测目标物体的k*k个部分。即是用不同channel的feature map代表物体的不同局部（如左上部分，右下部分）。</p> </li><li> <p>将RPN网络得到的Proposal映射到上一步得到的feature map（厚度为</p> <p class="img-center"><img alt="图片" height="27" src="https://images2.imgbox.com/7a/30/Ga4iLjor_o.png" width="87"></p> <p> ）后，相应的，将RoI等分为k*k个bin，对第（i，j）个bin，仅考虑对应（i，j）位置的（C+1）个feature map，进行如下计算：其中（x0,y0）是这个RoI的锚点，得到的即是（i，j）号bin对 类别的相应分数。</p> </li></ul> 
<p></p> 
<p class="img-center"><img alt="图片" height="92" src="https://images2.imgbox.com/7e/40/IcNLEEBE_o.jpg" width="572"></p> 
<ul><li> <p>经过上一步，每个RoI得到的结果是</p> <p class="img-center"><img alt="图片" height="27" src="https://images2.imgbox.com/6b/ee/2wespGZI_o.png" width="87"></p> <p>大小的分数张量， K*K编码着物体的局部分数信息，进行vote（平均）后得到（C+1）维的分数向量，再接入softmax得到每一类的概率。</p> </li></ul> 
<p>上面第二步操作中"仅选取第（i，j）号feature map"是位置信息产生意义的关键。这样设计的网络结构，所有可学习的参数都分布在可共享的卷积层，因而在训练和测试性能上均有提升。</p> 
<p><strong>小结</strong></p> 
<p>R-FCN是对Faster R-CNN结构上的改进，部分地解决了位置不变性和位置敏感性的矛盾。通过最大化地共享卷积参数，使得在精度相当的情况下训练和测试效率都有了很大的提升。</p> 
<h5 id="Deformable%20Convolution%20Networks"><strong>Deformable Convolution Networks</strong></h5> 
<blockquote> 
 <p>Deformable Convolution Networks</p> 
 <p>https://arxiv.org/abs/1703.06211</p> 
</blockquote> 
<p>本篇文章则提出在卷积和RoI Pooling两个层添加旁路显式学习偏置，来建模物体形状的可变性。这样的设计使得在保持目标全局上位置敏感的同时，对目标局部的建模添加灵活性。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="493" src="https://images2.imgbox.com/ce/94/0W5UOGoG_o.jpg" width="720"></p> 
<p><em>可变形卷积的旁支</em></p> 
<p></p> 
<p class="img-center"><img alt="图片" height="481" src="https://images2.imgbox.com/76/b3/ZIgg4yYX_o.jpg" width="720"></p> 
<p><em>RoI Pooling的旁支</em></p> 
<p>如上两图所示，通过在卷积部分添加旁路，显式地用一部分张量表示卷积核在图片不同部分的偏移情况，再添加到原有的卷积操作上，使卷积具有灵活性的特征，提取不同物体的特征时，其形状可变。而在RoI Pooling部分，旁路的添加则赋予采样块可活动的特性，更加灵活地匹配不同物体的形状。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="417" src="https://images2.imgbox.com/1f/b0/zse9y9z2_o.jpg" width="720"></p> 
<p><em>可变形卷积和RoIPooling的示例</em></p> 
<p>在MS COCO Chanllege 2017上，MSRA团队的结果向我们展示了可变形卷积在提升检测模型性能上的有效性：</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="404" src="https://images2.imgbox.com/0c/77/vr6NDZDp_o.jpg" width="720"></p> 
<p><em>可变形卷积带来的增益</em></p> 
<h3 id="%E7%89%B9%E5%BE%81%E5%A4%8D%E7%94%A8%E3%80%81%E5%AE%9E%E6%97%B6%E6%80%A7"><strong>特征复用、实时性</strong></h3> 
<p>本文的第一部分关注检测模型的头部部分。对与每张图片，深度网络其实是通过级联的映射获得了在某一流形上的一个表征，这个表征相比原图片更有计算机视角下的语义性。例如，使用Softmax作为损失函数的分类网络，最后一层获得的张量常常展现出成簇的分布。</p> 
<p>深度网络因分布式表示带来的指数级增益，拥有远超其他机器学习模型的表示能力，近年来，有不少致力于对深度网络习得特征进行可视化的工作，为研究者提供了部分有直观意义的感知，如浅层学习线条纹理，深层学习物体轮廓。然而，现阶段的深度模型仍然是一个灰盒，缺乏有效的概念去描述网络容量、特征的好坏、表达能力等等被研究者常常提到但又给不出精确定义的指代。</p> 
<p>本篇的第一节将介绍通过头部网络结构的设计来更有效利用基础网络所提供特征的工作，帮助读者进一步理解检测任务的难点和研究者的解决思路。</p> 
<p>第二部分则关注面向实时性检测的工作，这也是检测任务在应用上的目标。如本系列文章第二篇所述，实时性这一要求并没有通用的评价标准，应用领域也涉及到更多网络的压缩、加速和工程上的优化乃至硬件层面的工作等，则不在本文的介绍范围。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="266" src="https://images2.imgbox.com/88/be/ztr4gX8x_o.jpg" width="720"></p> 
<h4 id="%E7%89%B9%E5%BE%81%E5%A4%8D%E7%94%A8%E4%B8%8E%E6%95%B4%E5%90%88"><strong>特征复用与整合</strong></h4> 
<h5 id="FPN"><strong>FPN</strong></h5> 
<blockquote> 
 <p>Feature Pyramid Networks for Object Detection</p> 
 <p>https://arxiv.org/abs/1612.03144</p> 
</blockquote> 
<p>对图片信息的理解常常关系到对位置和规模上不变性的建模。在较为成功的图片分类模型中，Max-Pooling这一操作建模了位置上的不变性：从局部中挑选最大的响应，这一响应在局部的位置信息就被忽略掉了。而在规模不变性的方向上，添加不同大小感受野的卷积核（VGG），用小卷积核堆叠感受较大的范围（GoogLeNet），自动选择感受野的大小（Inception）等结构也展现了其合理的一面。</p> 
<p>回到检测任务，与分类任务不同的是，检测所面临的物体规模问题是跨类别的、处于同一语义场景中的。</p> 
<p>一个直观的思路是用不同大小的图片去生成相应大小的feature map，但这样带来巨大的参数，使本来就只能跑个位数图片的显存更加不够用。另一个思路是直接使用不同深度的卷积层生成的feature map，但较浅层的feature map上包含的低等级特征又会干扰分类的精度。</p> 
<p>本文提出的方法是在高等级feature map上将特征向下回传，反向构建特征金字塔。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="403" src="https://images2.imgbox.com/f6/3e/ZsvOnT0p_o.jpg" width="640"></p> 
<p><em>FPN结构</em></p> 
<p>从图片开始，照常进行级联式的特征提取，再添加一条回传路径：从最高级的feature map开始，向下进行最近邻上采样得到与低等级的feature map相同大小的回传feature map，再进行逐元素相加（lateral connection），构成这一深度上的特征。</p> 
<p>这种操作的信念是，低等级的feature map包含更多的位置信息，高等级的feature map则包含更好的分类信息，将这两者结合，力图达到检测任务的位置分类双要求。</p> 
<p>特征金字塔本是很自然的想法，但如何构建金字塔同时平衡检测任务的定位和分类双目标，又能保证显存的有效利用，是本文做的比较好的地方。如今，FPN也几乎成为特征提取网络的标配，更说明了这种组合方式的有效性。</p> 
<h5 id="TDM"><strong>TDM</strong></h5> 
<blockquote> 
 <p>Beyond Skip Connections: Top-down Modulation for Object Detection</p> 
 <p>https://arxiv.org/abs/1612.06851</p> 
</blockquote> 
<p>本文跟FPN是同一时期的工作，其结构也较为相似。作者认为低层级特征对小物体的检测至关重要，但对低层级特征的选择要依靠高层及特征提供的context信息，于是设计TDM（Top-Down Modulation）结构来将这两种信息结合起来处理。</p> 
<p><img alt="" height="260" src="https://images2.imgbox.com/ce/11/Tyb2EtlN_o.png" width="828"></p> 
<p><em>TDM整体结构</em></p> 
<p>可以看到，TDM的结构跟FPN相当类似，但也有如下显著的不同：</p> 
<ul><li> <p>T模块和L模块都是可供替换的子网络单元，可以是Residual或者Inception单元，而在FPN中，二者分别是最近邻上采样（Neareast UpSample）和逐元素相加（Element-wise Addition）。</p> </li><li> <p>FPN在每个层级得到的feature map都进行RoI Proposal和RoI Pooling，而TDM只在自上而下传播后的最大feature map上接入检测头部。</p> </li></ul> 
<p></p> 
<p class="img-center"><img alt="图片" height="264" src="https://images2.imgbox.com/e5/87/LADpjXE8_o.jpg" width="594"></p> 
<p><em>TDM中的T模块和L模块</em></p> 
<p>TDM的设计相比FPN拥有更多可学习的参数和灵活性，文章的实验显示，TDM结构对小物体检测精度的提升帮助明显。而且，TDM是对检测头部的改进，也有推广到单阶段模型的潜力。</p> 
<h5 id="DSSD"><strong>DSSD</strong></h5> 
<blockquote> 
 <p>Deconvolutional Single Shot Multibox Detector</p> 
 <p>https://arxiv.org/abs/1701.06659</p> 
</blockquote> 
<p>本文是利用反卷积操作对SSD的改进。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="426" src="https://images2.imgbox.com/70/40/rSEVwnkt_o.jpg" width="720"></p> 
<p><em>DSSD的网络结构</em></p> 
<p>在原版SSD中，检测头部不仅从基础网络提取特征，还添加了额外的卷积层，而本文则在这些额外卷积层后再添加可学习的反卷积层，并将feature map的尺度扩展为原有尺寸，把两个方向上具有相同尺度的feature map叠加后再进行检测，这种设计使检测头部同时利用不同尺度上的低级特征和高级特征。跟FPN不同的是，反传的特征通过反卷积得到而非简单的最近邻上采样。</p> 
<p>同时，在反卷积部分添加了额外的卷积层提供"缓冲"，以免反卷积分支影响网络整体的收敛性。另外，文章也通过加入跳跃连接改进了检测头部，使得头部结构相比原版SSD更加复杂。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="274" src="https://images2.imgbox.com/a5/2b/5fRREkIY_o.jpg" width="720"></p> 
<p><em>DSSD的头部结构</em></p> 
<h5 id="RON"><strong>RON</strong></h5> 
<blockquote> 
 <p>RON: Reverse Connection with Objectness Prior Networksfor Object Detection</p> 
 <p>https://arxiv.org/abs/1707.01691</p> 
</blockquote> 
<p><img alt="" height="593" src="https://images2.imgbox.com/92/34/fpymN2o8_o.png" width="772"></p> 
<p><em>RON结构</em></p> 
<p>文章关注两个问题：1)多尺度目标检测，2）正负样本比例失衡的问题。</p> 
<p>对于前者，文章将相邻的feature map通过reverse connection相连，并在每个feature map上都进行检测，最后再整合过滤。对于后者，类似RPN，对每个anchor box生成一个Objectness priori，作为一个指标来过滤过多的box（但不对box进行调整，RPN对box进行调整，作者指出这会造成重复计算）。文章的实验显示RON在较低的分辨率下取得了超过SSD的表现。</p> 
<h5 id="FSSD"><strong>FSSD</strong></h5> 
<blockquote> 
 <p>Feature Fusion Single Shot Multibox Detector</p> 
 <p>https://arxiv.org/abs/1712.00960</p> 
</blockquote> 
<p>FSSD提出了另一种对不同层级特征进行融合的方式，从基础网络不同层级得到feature map后，利用采样操作将它们在spatial方向上规整化，再拼接到一起，并通过BN层以使不同层级特征的激活值数量级一致。最后，拼接后的feature map经过一系列的卷积操作，产生不同大小的融合feature map传入检测头部的预测网络。</p> 
<p><img alt="" height="427" src="https://images2.imgbox.com/23/5e/hTo2RXo4_o.png" width="780"></p> 
<p><em>FSSD的特征融合方式</em></p> 
<p>文章指出，特征融合的初衷还是同时利用高层级feature map提供的语义信息和低层级feature map的位置信息，而像FPN中的逐元素相加操作进行融合的方式要求不同层级的feature map具有完全一致的大小，本文则采用拼接的方式，不受channel数的限制。</p> 
<h5 id="RefineDet"><strong>RefineDet</strong></h5> 
<blockquote> 
 <p>Single-Shot Refinement Neural Network for Object Detection</p> 
 <p>https://arxiv.org/abs/1711.06897</p> 
</blockquote> 
<p><img alt="" height="493" src="https://images2.imgbox.com/4f/ac/M6Oi8JF3_o.png" width="859"></p> 
<p><em>RefineDet的ARM和ODM</em></p> 
<p>本文是单阶段的模型，但思路上却是两阶段的。文章指出两阶段方法精度有优势的原因有三点：</p> 
<p>1）两阶段的设计使之有空间来用采样策略处理类别不均衡的问题；</p> 
<p>2）级联的方式进行box回归；</p> 
<p>3）两阶段的特征描述。</p> 
<p>文章提出两个模块来在一阶段检测器中引入两阶段设计的优势：Anchor Refinement Module(ARM)和Object Detection Module(ODM)。前者用于识别并过滤背景类anchor来降低分类器的负担，并且调整anchor位置以更好的向分类器输入，后者用于多分类和box的进一步回归。</p> 
<p>Single-shot的体现在上面两个模块通过Transfer Connection Block共用特征。除此之外，Transfer Connection Block还将特征图反传，构成类似FPN的效果。两个模块建立联合的损失使网络能够端到端训练。</p> 
<p>实验结果显示RefineNet的效果还是不错的，速度跟YOLOv2相当，精度上更有优势。之后的Ablation experiments也分别支撑了负样本过滤、级联box回归和Transfer Connection Block的作用。可以说这篇文章的工作让两阶段和一阶段检测器的界限更加模糊了。</p> 
<h4 id="%E9%9D%A2%E5%90%91%E5%AE%9E%E6%97%B6%E6%80%A7%E7%9A%84%E5%B7%A5%E4%BD%9C"><strong>面向实时性的工作</strong></h4> 
<h5 id="Light%20Head%20R-CNN"><strong>Light Head R-CNN</strong></h5> 
<blockquote> 
 <p>Light-Head R-CNN: In Defense of Two-Stage Object Detector</p> 
 <p>https://arxiv.org/abs/1711.07264</p> 
</blockquote> 
<p>文章指出两阶段检测器通常在生成Proposal后进行分类的"头"(head)部分进行密集的计算，如ResNet为基础网络的Faster-RCNN将整个stage5（或两个FC）放在RCNN部分， R-FCN要生成一个具有随类别数线性增长的channel数的Score map，这些密集计算正是两阶段方法在精度上领先而在推断速度上难以满足实时要求的原因。</p> 
<p>针对这两种元结构(Faster-RCNN和RFCN)，文章提出了"头"轻量化方法，试图在保持精度的同时又能减少冗余的计算量，从而实现精度和速度的Trade-off。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="618" src="https://images2.imgbox.com/17/0d/goWrbY7V_o.jpg" width="720"></p> 
<p><em>Light-head R-CNN与Faster R-CNN, R-FCN的对比</em></p> 
<p>如上图，虚线框出的部分是三种结构的R-CNN子网络（在每个RoI上进行的计算），light-head R-CNN中，在生成Score map前，ResNet的stage5中卷积被替换为深度可分离卷积，产生的Score map也减少至10×p×p（相比原先的类别数×p×p，p为网格划分粒度，R-FCN中取7）。</p> 
<p>一个可能的解释是，"瘦"（channel数较少）的score map使用于分类的特征信息更加紧凑，原先较"厚"的score map在经过PSROIPooling的操作时，大部分信息并没有提取（只提取了特定类和特定位置的信息，与这一信息处在同一score map上的其他数据都被忽略了）。</p> 
<p>进一步地，位置敏感的思路将位置性在channel上表达出来，同时隐含地使用了更类别数相同长度的向量表达了分类性（这一长度相同带来的好处即是RCNN子网络可以免去参数）。</p> 
<p>light-head在这里的改进则是把这一个隐藏的嵌入空间压缩到较小的值，而在RCNN子网络中加入FC层再使这个空间扩展到类别数的规模，相当于是把计算量分担到了RCNN子网络中。</p> 
<p>粗看来，light-head将原来RFCN的score map的职责两步化了：thin score map主攻位置信息，RCNN子网络中的FC主攻分类信息。另外，global average pool的操作被去掉，用于保持精度。</p> 
<h5 id="YOLOv2"><strong>YOLOv2</strong></h5> 
<p>YOLO9000: Better, Faster, Stronger</p> 
<p>单阶段检测模型的先驱工作YOLO迎来了全面的更新：</p> 
<ul><li> <p>在卷积层添加BN，舍弃Dropout</p> </li><li> <p>更大尺寸的输入</p> </li><li> <p>使用Anchor Boxes，并在头部运用卷积替代全连接层</p> </li><li> <p>使用聚类方法得到更好的先验，用于生成Anchor Boxes</p> </li><li> <p>参考Fast R-CNN的方法对位置坐标进行log/exp变换使坐标回归的损失保持在合适的数量级</p> </li><li> <p>passthrough层：类似ResNet的skip-connection，将不同尺寸的feature map拼接到一起</p> </li><li> <p>多尺度训练</p> </li><li> <p>更高效的网络Darknet-19，类似VGG的网络，在ImageNet上以较少的参数量达到跟当前最佳相当的精度</p> </li></ul> 
<p><img alt="" height="344" src="https://images2.imgbox.com/ae/be/iPseFlSw_o.png" width="874"></p> 
<p><em>YOLOv2的改进</em></p> 
<p>此次改进后，YOLOv2吸收了很多工作的优点，达到跟SSD相当的精度和更快的推断速度。</p> 
<h5 id="SSDLite(MobileNets%20V2)"><strong>SSDLite(MobileNets V2)</strong></h5> 
<p>SSDLite是在介绍MobileNets V2的论文Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation(https://arxiv.org/abs/1801.04381)中提出的。</p> 
<p>MobileNets是一系列大面积应用深度可分离卷积的网络结构，试图以较小的参数量来达到跟大型网络相当的精度，以便能够在移动端部署。在本文中，作者提出了对MobileNets的改进版本，通过移动跳跃连接的位置并去掉某些ReLU层来实现更好的参数利用。可参考这个问题了解更多关于这一改进的解释。</p> 
<p>在检测方面，SSDLite的改进之处在于将SSD的检测头部中的卷积运算替换为深度可分离卷积，降低了头部计算的参数量。另外，这项工作首次给出了检测模型在移动设备CPU上单核运行的速度，提供了现在移动终端执行类似任务性能的一个参考。</p> 
<p>总结</p> 
<p>从基础网络的不同层级提取习得的feature map并通过一定的连接将它们整合，是近年来检测模型的重要趋势。这些针对检测头部网络的改进也越来越多地体现着研究者们对检测任务要求的表述和探索。另一方面，面向实时性的改进则继续推动这检测任务在应用领域的发展。</p> 
<h3 id="%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%96%B0%E8%B6%8B%E5%8A%BF"><strong>目标检测新趋势</strong></h3> 
<h4 id="YOLO9000"><strong>YOLO9000</strong></h4> 
<p>YOLO9000: Better, Faster, Stronger</p> 
<p>这篇文章里，YOLO的作者不仅提出YOLOv2，大幅改进了原版YOLO，而且介绍了一种新的联合训练方式：同时训练分类任务和检测任务，使得检测模型能够泛化到检测训练集之外的目标类上。</p> 
<p>YOLO9000使用了ImageNet和COCO数据集联合训练，在合并两者的标签时，根据WordNet的继承关系构建了了树状的类别预测图：</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="725" src="https://images2.imgbox.com/aa/06/JZgW8mpR_o.jpg" width="678"></p> 
<p><em>标签的合并</em></p> 
<p>类似条件概率的方式计算每个子标签的概率值，超出一定的阈值com时则选定该类作为输出，训练时也仅对其路径上的类别进行损失的计算和BP。</p> 
<p>YOLO9000为我们提供了一种泛化检测模型的训练方式，文章的结果显示YOLO9000在没有COCO标注的类别上有约20的mAP表现，能够检测的物体类别超过9000种。当然，其泛化性能也受检测标注类别的制约，在有类别继承关系的类上表现不错，而在完全没有语义联系的类上表现很差。</p> 
<h4 id="Mask%20R-CNN"><strong>Mask R-CNN</strong></h4> 
<p>Mask R-CNN通过将检测和实例分割联合训练的方式同时提高了分割和检测的精度。在原有Faster R-CNN的头部中分类和位置回归两个并行分支外再加入一个实例分割的并行分支，并将三者的损失联合训练。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="236" src="https://images2.imgbox.com/6e/81/g6gp2BZU_o.jpg" width="428"></p> 
<p><em>Mask R-CNN的头部结构</em></p> 
<p>在分割方面，文章发现对每个类别单独生成二值掩膜（Binary Mask）相比之前工作中的多分类掩膜更有效，这样避免了类间竞争，仍是分类和标记的解耦。文章另外的一个贡献是RoIAlign的提出，笔者认为会是检测模型的标配操作。</p> 
<p>FAIR团队在COCO Chanllege 2017上基于Mask R-CNN也取得了前列的成绩，但在实践领域，实例分割的标注相比检测标注要更加昂贵，而且按照最初我们对图像理解的三个层次划分，中层次的检测任务借用深层次的分割信息训练，事实上超出了任务的要求。</p> 
<h4 id="Focal%20Loss%EF%BC%88RetinaNet%EF%BC%89"><strong>Focal Loss（RetinaNet）</strong></h4> 
<p>Focal Loss for Dense Object Detection</p> 
<p>由于缺少像两阶段模型的样本整理操作，单阶段模型的检测头部常常会面对比两阶段多出1-2个数量级的Region Proposal，文章作者认为，这些Proposal存在类别极度不均衡的现象，导致了简单样本的损失掩盖了难例的损失，这一easy example dominating的问题是单阶段模型精度不如两阶段的关键。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="298" src="https://images2.imgbox.com/74/bc/IJQeA8Ie_o.jpg" width="480"></p> 
<p><em>Focal Loss随概率变化曲线</em></p> 
<p>于是，文章提出的解决措施即是在不同样本间制造不平衡，让简单样本的损失在整体的贡献变小，使模型更新时更关注较难的样本。具体的做法是根据预测概率给交叉熵的相应项添加惩罚系数，使得预测概率越高（越有把握）的样本，计算损失时所占比例越小。</p> 
<p></p> 
<p></p> 
<p class="img-center"><img alt="图片" height="187" src="https://images2.imgbox.com/77/ea/6pN0E68s_o.jpg" width="720"></p> 
<p><em>RetinaNet结构</em></p> 
<p>以ResNet的FPN为基础网络，添加了Focal Loss的RetinaNet取得了跟两阶段模型相当甚至超出的精度。另外，Focal Loss的应用也不只局限在单阶段检测器，其他要处理类别不均衡问题任务上的应用也值得探索。</p> 
<h4 id="Mimicking"><strong>Mimicking</strong></h4> 
<blockquote> 
 <p>Mimicking Very Efficient Network for Object Detection</p> 
 <p>http://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Mimicking_Very_Efficient_CVPR_2017_paper.pdf</p> 
</blockquote> 
<p>本篇文章是Mimicking方法在检测任务上的尝试。mimicking作为一种模型压缩的方法，采用大网络指导小网络的方式将大网络习得的信息用小网络表征出来，在损失较小精度的基础上大幅提升速度。</p> 
<p>Mimicking方法通常会学习概率输出的前一层，被称为"Deep-ID"，这一层的张量被认为是数据在经过深度网络后得到的一个高维空间嵌入，在这个空间中，不同类的样例可分性要远超原有表示，从而达到表示学习的效果。本文作者提出的mimicking框架则是选择检测模型中基础网络输出的feature map进行学习，构成下面的结构：</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="319" src="https://images2.imgbox.com/cd/d2/hqsKc8Ou_o.jpg" width="720"></p> 
<p><em>Mimicking网络结构</em></p> 
<p>图中，上面分支是进行学习的小网络，下面分支的大网络则由较好表现的模型初始化，输入图片后，分别得到不同的feature map，小网络同时输入RPN的分类和位置回归，根据这一RoI Proposal，在两个分支的feature map上提取区域feature，令大网络的feature作为监督信息跟小网络计算L2 Loss，并跟RPN的损失构成联合损失进行学习。而对RCNN子网络，可用分类任务的mimicking方法进行监督。</p> 
<p>文章在Pascal VOC上的实验显示这种mimicking框架可以在相当的精度下实现2倍以上的加速效果。</p> 
<h4 id="CGBN%EF%BC%88Cross%20GPU%20Batch%20Normalization%EF%BC%89"><strong>CGBN（Cross GPU Batch Normalization）</strong></h4> 
<blockquote> 
 <p>MegDet: A Large Mini-Batch Object Detector</p> 
 <p>https://arxiv.org/abs/1711.07240</p> 
</blockquote> 
<p>这篇文章提出了多卡BN的实现思路，使得检测模型能够以较大的batch进行训练。</p> 
<p>之前的工作中，两阶段模型常常仅在一块GPU上处理1-2张图片，生成数百个RoI Proposal供RCNN子网络训练。这样带来的问题是每次更新只学习了较少语义场景的信息，不利于优化的稳定收敛。要提高batch size，根据Linear Scaling Rule，需要同时增大学习率，但较大的学习率又使得网络不易收敛，文章尝试用更新BN参数的方式来稳定优化过程（基础网络的BN参数在检测任务上fine-tuning时通常固定）。加上检测中常常需要较大分辨率的图片，而GPU内存限制了单卡上的图片个数，提高batch size就意味着BN要在多卡（Cross-GPU）上进行。</p> 
<p>BN操作需要对每个batch计算均值和方差来进行标准化，对于多卡，具体做法是，单卡独立计算均值，聚合（类似Map-Reduce中的Reduce）算均值，再将均值下发到每个卡，算差，再聚合起来，计算batch的方差，最后将方差下发到每个卡，结合之前下发的均值进行标准化。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="588" src="https://images2.imgbox.com/49/c2/4xkCprEd_o.jpg" width="547"></p> 
<p><em>CGBN实现流程</em></p> 
<p>更新BN参数的检测模型能够在较大的batch size下收敛，也大幅提高了检测模型的训练速度，加快了算法的迭代速度。</p> 
<h4 id="DSOD%EF%BC%88Deeply%20Supervised%20Object%20Detector%EF%BC%89"><strong>DSOD（Deeply Supervised Object Detector）</strong></h4> 
<blockquote> 
 <p>DSOD: Learning Deeply Supervised Object Detectors from Scratch</p> 
 <p>https://arxiv.org/abs/1708.01241</p> 
</blockquote> 
<p>R-CNN工作的一个深远影响是在大数据集（分类）上pre-train，在小数据集（检测）fine-tune的做法，本文指出这限制了检测任务上基础网络结构的调整（需要在ImageNet上等预训练的分类网络），也容易引入分类任务的bias，因而提出从零训练检测网络的方法。</p> 
<p>作者认为，由于RoI的存在，两阶段检测模型从零训练难以收敛，从而选择Region-free的单阶段方法进行尝试。一个关键的发现是，从零训练的网络需要Deep Supervision，文中采用紧密连接的方式来达到隐式Deep Supervision的效果，因而DSOD的基础网络部分类似DenseNet，浅层的feature map也有机会得到更接近损失函数的监督。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="617" src="https://images2.imgbox.com/1c/46/J1aQM1ek_o.jpg" width="595"></p> 
<p><em>DSOD结构</em></p> 
<p>文章的实验显示，DSOD从零开始训练也可以达到更SSD等相当的精度，并且模型的参数更少，但速度方面有所下降。</p> 
<h4 id="A-Fast-RCNN"><strong>A-Fast-RCNN</strong></h4> 
<blockquote> 
 <p>A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection</p> 
 <p>https://arxiv.org/abs/1704.03414</p> 
</blockquote> 
<p>本文将GAN引入检测模型，用GAN生成较难的样本以提升检测网络应对遮挡(Occlusion)、形变(Deformation)的能力。</p> 
<p>对于前者，作者设计了ASDN(Adversarial Spatial Dropout Network)，在feature map层面生成mask来产生对抗样本。对于feature map，在旁支上为每个位置生成一个概率图，根据一定的阈值将部分feature map上的值drop掉，再传入后面的头部网络。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="265" src="https://images2.imgbox.com/63/08/cGJ4xmC6_o.jpg" width="720"></p> 
<p><em>ASDN</em></p> 
<p>类似的，ASTN(Adversarial Spatial Transformer Network)在旁支上生成旋转等形变并施加到feature map上。整体上，两个对抗样本生成的子网络串联起来，加入到RoI得到的feature和头部网络之间。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="262" src="https://images2.imgbox.com/e8/69/T5e2vHIW_o.jpg" width="720"></p> 
<p><em>ASDN和ASTN被串联</em></p> 
<p>文中的实验显示，在VOC上，对抗训练对plant, bottle等类别的检测精度有提升，但对个别类别却有损害。这项工作是GAN在检测任务上的试水，在feature空间而不是原始数据空间生成对抗样本的思路值得借鉴。</p> 
<h4 id="Relation%20Module"><strong>Relation Module</strong></h4> 
<blockquote> 
 <p>Relation Networks for Object Detection</p> 
 <p>https://arxiv.org/abs/1711.11575</p> 
</blockquote> 
<p>Attention机制在自然语言处理领域取得了有效的进展，也被SENet等工作引入的计算机视觉领域。本文试图用Attention机制建模目标物体之间的相关性。</p> 
<p>理解图像前背景的语义关系是检测任务的潜在目标，权威数据集COCO的收集过程也遵循着在日常情景中收集常见目标的原则，本文则从目标物体间的关系入手，用geometric feature(fG)和appearance feature(fA)来表述某一RoI，并联合其他RoI建立relation后，生成一个融合后的feature map。计算如下图：</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="395" src="https://images2.imgbox.com/fe/9e/SrFKlzls_o.jpg" width="584"></p> 
<p><em>Relation Module</em></p> 
<p>作者将这样的模块插入两阶段检测器的头部网络，并用改装后的duplicate removal network替代传统的NMS后处理操作，形成真正端到端的检测网络。在Faster R-CNN, FPN, Deformable ConvNets上的实验显示，加入Relation Module均能带来精度提升。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="473" src="https://images2.imgbox.com/fc/5f/QB0JElLZ_o.jpg" width="607"></p> 
<p><em>Relation Module应用在头部网络和替代NMS</em></p> 
<blockquote> 
 <p>检测领域在近年来取得的进展只是这场深度模型潮流的一个缩影。理解图像、理解视觉这一机器视觉的中心问题上，仍不断有新鲜的想法出现。推动整个机器视觉行业跃进的同时，深度模型也越来越来暴露出自身的难收敛、难解释等等问题，整个领域仍在负重前行。</p> 
 <p>本系列文章梳理了检测任务上深度方法的经典工作和较新的趋势，并总结了常用的测评集和训练技巧，期望为读者建立对这一任务的基本认识。在介绍对象的选择和章节的划分上，都带有笔者自己的偏见，本文仅仅可作为一个导读，更多的细节应参考实现的代码，更多的讨论应参考文章作者的扩展实验。</p> 
 <p>事实上，每项工作都反映着作者对这一问题的洞察，而诸多工作间的横向对比也有助于培养独立和成熟的视角来评定每项工作的贡献。另外，不同文献间的相互引述、所投会议期刊审稿人的意见等，都是比本文更有参考价值的信息来源。</p> 
 <p>在工业界还有更多的问题，比如如何做到单模型满足各种不同场景的需求，如何解决标注中的噪声和错误干扰，如何针对具体业务类型（如人脸）设计特定的网络，如何在廉价的硬件设备上做更高性能的检测，如何利用优化的软件库、模型压缩方法进行加速等等</p> 
</blockquote>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f7fc0f50b24344df389b64f0f3244d13/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">使用Gorm进行高级查询</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7ce171dc3bf2cd4bba143201c9ec30ec/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">go语言 ｜ grpc原理介绍（三）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>