<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【文献阅读】MCAN——用于VQA的深层模块化的协同注意力网络（Z. Yu等人，CVPR，2019，有代码） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【文献阅读】MCAN——用于VQA的深层模块化的协同注意力网络（Z. Yu等人，CVPR，2019，有代码）" />
<meta property="og:description" content="一、文章概况 文章题目：《Deep Modular Co-Attention Networks for Visual Question Answering》
前面三位作者是杭电的，第四作者是陶老师，感觉陶老师在VQA领域非常高产，已经在CVPR2019上看到他的好几篇文章了。
文章下载地址：http://openaccess.thecvf.com/content_CVPR_2019/papers/Yu_Deep_Modular_Co-Attention_Networks_for_Visual_Question_Answering_CVPR_2019_paper.pdf
或者https://arxiv.org/abs/1906.10770
文章引用格式：Z. Yu, J. Yu, Y. Cui, D. Tao, Q. Tian. &#34;Deep Modular Co-Attention Networks for Visual Question Answering.&#34; In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019
项目地址：参考这个https://github.com/MILVLG/mcan-vqa
二、文章导读 网上目前还是没有看到这篇文章的介绍，所以还是自己来记录。
首先给出文章的摘要：
Visual Question Answering (VQA) requires a finegrained and simultaneous understanding of both the visual content of images and the textual content of questions. Therefore, designing an effective ‘co-attention’ model to associate key words in questions with key objects in images is central to VQA performance." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/04017258d102f09ea53c4dbe72cdd2ad/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-08-26T16:19:29+08:00" />
<meta property="article:modified_time" content="2019-08-26T16:19:29+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【文献阅读】MCAN——用于VQA的深层模块化的协同注意力网络（Z. Yu等人，CVPR，2019，有代码）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>一、文章概况</h2> 
<p style="text-indent:50px;"><strong>文章题目：《Deep Modular Co-Attention Networks for Visual Question Answering》</strong></p> 
<p style="text-indent:50px;">前面三位作者是杭电的，第四作者是陶老师，感觉陶老师在VQA领域非常高产，已经在CVPR2019上看到他的好几篇文章了。</p> 
<p style="text-indent:50px;"><span style="color:#f33b45;"><strong>文章下载地址：</strong></span><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Yu_Deep_Modular_Co-Attention_Networks_for_Visual_Question_Answering_CVPR_2019_paper.pdf" rel="nofollow">http://openaccess.thecvf.com/content_CVPR_2019/papers/Yu_Deep_Modular_Co-Attention_Networks_for_Visual_Question_Answering_CVPR_2019_paper.pdf</a><br> 或者<a href="https://arxiv.org/abs/1906.10770" rel="nofollow">https://arxiv.org/abs/1906.10770</a></p> 
<p style="text-indent:50px;"><span style="color:#f33b45;"><strong>文章引用格式：</strong></span>Z. Yu, J. Yu, Y. Cui, D. Tao, Q. Tian. "Deep Modular Co-Attention Networks for Visual Question Answering." <em>In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2019</p> 
<p style="text-indent:50px;"><strong><span style="color:#f33b45;">项目地址：</span></strong>参考这个<a href="https://github.com/MILVLG/mcan-vqa">https://github.com/MILVLG/mcan-vqa</a></p> 
<h2>二、文章导读</h2> 
<p style="text-indent:50px;">网上目前还是没有看到这篇文章的介绍，所以还是自己来记录。</p> 
<p style="text-indent:50px;">首先给出文章的摘要：</p> 
<blockquote> 
 <p>Visual Question Answering (VQA) requires a finegrained and simultaneous understanding of both the visual content of images and the textual content of questions. Therefore, <strong>designing an effective ‘co-attention’ model to associate key words in questions with key objects in images is central to VQA performance</strong>. So far, most successful attempts at <strong>co-attention learning have been achieved by using shallow models</strong>, and deep co-attention models show little improvement over their shallow counterparts. <span style="color:#f33b45;"><strong>In this paper, we propose a deep Modular Co-Attention Network (MCAN) that consists of Modular Co-Attention (MCA) layers cascaded in depth</strong></span>. Each MCA layer models the self-attention of questions and images, as well as the question-guided-attention of images jointly using a modular composition of two basic attention units. We quantitatively and qualitatively evaluate MCAN on the benchmark VQA-v2 dataset and conduct extensive ablation studies to explore the reasons behind MCAN’s effectiveness. Experimental results demonstrate that MCAN significantly outperforms the previous state-of-the-art. Our best single model delivers 70.63% overall accuracy on the test-dev set.</p> 
</blockquote> 
<p style="text-indent:50px;">首先作者介绍了他的动机，VQA需要精细化以及同事理解视觉内容和文本内容，因此设计了协同注意力模型来联合问题中的关键词和图像中的关键目标。但是目前的协同注意力学习只用在了浅层模型，而深层模型在浅层问题上只有微弱的提升。这篇文章作者提出了深层协同注意力网络MCAN（由协同注意力模块MCA层串联构成）。每一个MCA层都能够对图像和问题的注意力进行建模。最后作者还对该模型进行了评估。</p> 
<h2>三、文献详细介绍</h2> 
<p style="text-indent:50px;">注意力机制目前表现出了非常好的效果，目前协同注意力在图像和文本表示中也有着不错的表现。但是，这种<strong>协同注意力机制只能够学习到多模态之间粗糙的交互，协同注意力也不够进行图像和问题关键词之间的关系推断</strong>。为了解决协同注意力不能够进行多模态之间的充分交互问题，目前提出了两个密集协同注意力模型（dense co-attention model）——BAN和DCN。有趣的是，这两个模型是可以串联在一起解决更深层次的视觉推理。但是这两个模型相较于浅层模型却仅有很小的性能提升，作者认为原因在于这两个模型没有同时在每一个模态中对密度自注意力建模。</p> 
<p style="text-indent:50px;">灵感来自于Transformer模型，作者设置了两个注意力单元（general attention units）：一个自注意力单元（self-attention (SA) unit）进行模态内部交互和一个导向注意力单元（guided-attention (GA) unit）进行模态之间交互。之后再用一个协同注意力模块层（Modular Co-Attention (MCA) layers）将两个单元串联起来，最后将多个模块层串联起来，组成MCAN网络（Modular Co-Attention Network (MCAN)）。模型基于VQAv2数据集的精度如下图所示：</p> 
<p style="text-align:center;"><img alt="" class="has" height="419" src="https://images2.imgbox.com/0e/6d/AGxpG4TG_o.png" width="400"></p> 
<h4>1. 相关工作</h4> 
<p style="text-indent:33px;"><strong>（1）VQA</strong></p> 
<p style="text-indent:33px;">VQA是近几年发展起来的新方向。一般处理思路就是将图像和问题先表示为全局特征，然后再用多模态融合模型进行答案的概率预测。目前问题的表示主要用LSTM，多模态融合主要用残差网络。目前融合造成的问题在于，对一张图进行全局特征表示也许会损失一些关键信息，而这些信息可能涉及到问题中的图像局部区域，解决办法大多还是用的注意力机制。</p> 
<p style="text-indent:33px;"><strong>（2）协同注意力模型（Co-Attention Models）</strong></p> 
<p style="text-indent:33px;">同时学习问题的文本注意力和图像的诗句注意力是有必要的，目前协同注意力网络网络是在每个模态中分别学习其注意力分布，且忽视了图像和文本的密集交互（dense attention）。这对多模态特征之间精细关系的理解造成了瓶颈。解决这一问题目前主要办法则是用密集协同注意力网络。</p> 
<h4>2. 模块化协同注意力层MCA（Modular Co-Attention Layer）</h4> 
<p style="text-indent:33px;">MCA层是将两个注意力单元结合起来的一个模块，灵感来自于scaled dot-product attention，使用三种结合方式，将能够获得三种MCA变体（variants）。</p> 
<p style="text-indent:33px;"><strong>（1）SA和GA单元（Self-Attention and Guided-Attention Units）：</strong></p> 
<p style="text-indent:33px;">scaled dot-product attention的输入由问题、关键词的维度<img alt="d_{key}" class="mathcode" src="https://images2.imgbox.com/30/05/I2AMqMOV_o.gif">、值的维度<img alt="d_{value}" class="mathcode" src="https://images2.imgbox.com/6e/8d/iUOsiBhd_o.gif">组成。为了方便，将这两个维度值都设为d，再计算问题和所有关键词的点积，除以<img alt="\sqrt{d}" class="mathcode" src="https://images2.imgbox.com/f2/12/HHNlcfLW_o.gif">后再用softmax来获得attention的权重值:</p> 
<p style="text-align:center;"><img alt="" class="has" height="175" src="https://images2.imgbox.com/bb/52/iR3mSuMO_o.png" width="400"></p> 
<p style="text-indent:33px;">之后再引入multi-head attention，它由h个平行的“head”组成：</p> 
<p style="text-align:center;"><img alt="" class="has" height="186" src="https://images2.imgbox.com/5a/b3/XSMtqggn_o.png" width="400"></p> 
<p style="text-indent:33px;">为了减小模型的体量，常用<img alt="d_{h}=d/h" class="mathcode" src="https://images2.imgbox.com/77/f0/h4HfEqxz_o.gif">。在multi-head attention之上先建立SA（self-attention）和GA（guided-attention）单元。SA是由multi-head attention层和pointwise feed-forward层组成，假设输入特征集X，那multi-head attention层就是学习特征集中逐点之间的关系，pointwise feed-forward层（(FC(4d)-ReLU-Dropout(0.1)-FC(d)）则是对输出进行进一步转化。GA则是由两个输入的特征集合X和Y，可对两个集合中逐点计算关系，因此适用于多模态信息交互。两个单元的构成如下图所示：</p> 
<p style="text-align:center;"><img alt="" class="has" height="387" src="https://images2.imgbox.com/3c/3e/AhIGKLPE_o.png" width="400"></p> 
<p style="text-indent:33px;"><strong>（2）模块合成（Modular Composition for VQA）：</strong></p> 
<p style="text-indent:33px;">前面提到两个模态的结合有三种方式，可以用三种MCA层来处理VQA问题，这三个MCA层都可以串联起来，结构如下图所示：</p> 
<p style="text-align:center;"><img alt="" class="has" height="340" src="https://images2.imgbox.com/b4/25/Q4tLhfCU_o.png" width="400"></p> 
<p style="text-indent:33px;">Fig 3a中，输入问题特征，通过一个identity mapping到输出特征；Fig 3b中多增加了一个SA层来进行问题模态的内部交互；Fig 3c中，增加了另一个SA层进行图像模态的内部交互。以上的三种模态并未覆盖所有的MCA组合方式，作者对其他的组合方式也进行了探究，但是并没有达到理想的效果，所以这里就不多做介绍了。</p> 
<h4>3. 模块化协同注意力网络（Modular Co-Attention Networks）</h4> 
<p style="text-indent:33px;">首先来看一下作者设计的网络结构：</p> 
<p style="text-align:center;"><img alt="" class="has" height="304" src="https://images2.imgbox.com/16/7d/dbat6Iny_o.png" width="800"></p> 
<p style="text-indent:33px;">整个网络分为了三个模块：①对输入的图像和问题进行特征表示（representation）；②提出两个协同注意力模型，即stacking和encoder-decoder，这两个模型都是由多个MCA层串联而成，能够对之前的问题特征和图像特征进行进一步提炼；③利用一个简单的多模态融合模型对两个特征进行融合，最后再将其输入到一个多标签类别分类器中进行正确答案的预测。</p> 
<p style="text-indent:33px;"><strong>（1）问题和图像的表示（Question and Image Representations）</strong>：</p> 
<p style="text-indent:33px;">图像表示是一个区域视觉特征的集合，采用自下而上（bottom-up）的方式，这些特征是用在Visual Genome数据上训练好的Faster R-CNN（里面核心是ResNet-101）。作者设置了一个置信阈值来判断是否为活动目标，且目标的数量介于[10, 100]，对于第i个目标，他是由卷积层进行平均池化（mean-pooling）得到的特征，记为<img alt="x_{i}" class="mathcode" src="https://images2.imgbox.com/0a/60/XkmasvMx_o.gif">，最终就可以将图像表示为一个特征矩阵X。</p> 
<p style="text-indent:33px;">问题表示是将输入的问题先划分为单词，最多为14个单词，之后再用300维的GloVe word embeddings方法（在大规模语料库上预训练而成）将每一个单词转化为一个向量。然后，词嵌入再输入一个单层的LSTM网络（有<img alt="d_{y}" class="mathcode" src="https://images2.imgbox.com/4e/34/u9HpKdqG_o.gif">个隐藏单元）。最后输出一个问题的特征矩阵Y。</p> 
<p style="text-indent:33px;">最后为了处理目标的数量变化m和问题的长度变化n，使用0进行填充（X填充为100，Y填充为14）。</p> 
<p style="text-indent:33px;"><strong>（2）深度协同注意力学习（Deep Co-Attention Learning）：</strong></p> 
<p style="text-indent:33px;">将前面得到的图像特征X和问题特征Y作为输入，传入到一个由L个MCA层串联而成的注意力网络中进行协同注意力学习。对于MCA层，每一层的输入都是前一层的输出：</p> 
<p style="text-align:center;"><img alt="" class="has" height="189" src="https://images2.imgbox.com/6b/1f/QOLv544x_o.png" width="400"></p> 
<p style="text-indent:33px;">两个深度协同注意力网络可以用下图表示：</p> 
<p style="text-align:center;"><img alt="" class="has" height="329" src="https://images2.imgbox.com/63/4c/FBuCYkOj_o.png" width="400"></p> 
<p style="text-indent:33px;">stacking模型是由多个MCA层串联成的，输出的是最终的图像特征和问题特征。encoder-decoder模型思路来自于Transformer模型，编码器是由L个SA单元来学习问题特征，解码器是用SGA单元，根据问题特征来学习图像特征。这里作者设置L的值为1.</p> 
<p style="text-indent:33px;"><strong>（3）多模态融合和分类输出（Multimodal Fusion and Output Classifier）：</strong></p> 
<p style="text-indent:33px;">在深度协同注意力学习后，输出的问题特征和图像特征包含了丰富的问题单词和图像区域的注意力权重信息，因此这里采用了两层的MLP（(FC(d)-ReLU-Dropout(0.1)-FC(1))）分别获得问题特征和图像特征。如果我们以图像特征X为例，最终获得的特征<img alt="x_{i}" class="mathcode" src="https://images2.imgbox.com/2f/ca/qPc2JAqC_o.gif">为：</p> 
<p style="text-align:center;"><img alt="" class="has" height="154" src="https://images2.imgbox.com/9f/d5/R6TdN0Gj_o.png" width="400"></p> 
<p style="text-indent:33px;">分别获得的两种特征用线性多模态融合函数进行融合：</p> 
<p style="text-align:center;"><img alt="" class="has" height="100" src="https://images2.imgbox.com/71/4f/g0OIm7mu_o.png" width="400"></p> 
<p style="text-indent:33px;">融合后的特征z用sigmoid函数映射到一个向量s中，最后loss函数用二值交叉熵（binary cross-entropy）计算N类答案的分类结果。</p> 
<h4>4. 实验</h4> 
<p style="text-indent:33px;">作者将MCAN模型用在了VQAv2数据集上，下面直接展示作者做的一些ablation study。</p> 
<p style="text-indent:33px;">首先是对于不同模块的实验：</p> 
<p style="text-align:center;"><img alt="" class="has" height="277" src="https://images2.imgbox.com/ad/6f/fCqaDYRK_o.png" width="800"></p> 
<p style="text-indent:33px;">然后是探究MCA层数L的多少对VQA精度的影响（<strong><em>这里是我自己的一个疑问，从图中可以看到，大致上随着L层数的增多，精度都有一定程度提升，但是作者在文中介绍自己的L设置为1，其实这里不是很理解</em></strong>）：</p> 
<p style="text-align:center;"><img alt="" class="has" height="233" src="https://images2.imgbox.com/11/33/YTKtmVWA_o.png" width="800"></p> 
<p style="text-indent:33px;">然后是不同阶段的注意力结果：</p> 
<p style="text-align:center;"><img alt="" class="has" height="554" src="https://images2.imgbox.com/52/5f/bytEPtZm_o.png" width="800"></p> 
<p style="text-indent:33px;">最后还有一张注意力的权重体现图：</p> 
<p style="text-align:center;"><img alt="" class="has" height="217" src="https://images2.imgbox.com/01/2a/SyantLDc_o.png" width="800"></p> 
<h2>四、结论</h2>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/24db548284480b06ebe2b4d79ec4c2e0/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">logstash-启动</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/839ef1401791fd16ae3b63f8c8ea4df8/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">android studio 可视化设计 design edit unavalible</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>