<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ICCV2019：基于Anchor point的手势及人体3D姿态估计方法：A2J - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="ICCV2019：基于Anchor point的手势及人体3D姿态估计方法：A2J" />
<meta property="og:description" content="A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation from a Single Depth Image 简介： 这是我们在ICCV 2019的一篇论文，文章主要解决的是基于单张深度图像的手势以及人体的3D姿态估计问题，我们首次提出了将经典目标检测的anchor的概念应用到姿态估计任务中来，设计了一种简洁有效的方法，最终在包括三个手势姿态估计数据集（HANDS2019，ICVL，NYU）以及两个人体姿态估计数据集（ITOP，K2HPD）上取得了state-of-the-art的效果。
这里对文章做一个简要的介绍，欢迎大家讨论，相互学习，同时，如果你对我们的工作有兴趣，可以下载阅读我们的论文：https://arxiv.org/abs/1908.09999。同时，代码也即将给出：https://github.com/zhangboshen/A2J。
Anchor-to-Joint： 首先给出两张paper中的示意图。左图是我们技术方案的一个流程示意，对于一张给定的输入深度图片（这里假设输入仅包含单个手或人体，前面需要一个检测器的预处理步骤，类似于human pose的Top-down思路），我们密集的在图像上设立anchor point，如右图所示，每隔 s t r i d e = 4 stride=4 stride=4个像素点就会设立一个anchor point，最终的关节点预测将会通过这些anchor point去完成。具体而言，每个锚点需要针对所有关节点预测一个图像坐标下的偏移量（左图中的绿色箭头）以及一个深度值，最终的关节点坐标将由所有锚点加权投票得出（权重是可学习的，左图中红色的anchor point就是权重值比较大的点，我们称之为informative anchor point）。
技术实现： 为了实现上面的思路，我们采取的技术流程如下图所示。Backbone我们在实验中主要基于ResNet50，也对比进行了其他的网络结构。输出包含三个分支，一个是图像坐标系下面的偏移量输出，一个是深度值输出，最后一个是对anchor point进行加权的anchor proposal分支输出。第三支的输出结果经过softmax之后直接乘在前面两支的输出结果上对anchor point投票进行权重赋值。
上面的技术方案有几个特点：
1）Anchor proposal分支可学习，对于不同的关节点，Informative anchor point的分布是不一样的，因此，A2J具有针对不同关节点的自适应性；
2）网络中不包含Deconv层，这里区别于目前主流的Heatmap的做法，不需要上采样使得我们的算法速度更快，结构更简单；
3）End-to-end training，我们最终的loss来自于anchor point加权投票之后的预测输出以及GT之间的差异，不需要对GT进行任何形式的变换。
4）快。网络结构简单，前向推理的速度很快，我们在NVIDIA 1080Ti的实测速度超过100fps。
网络结构 上面提到Backbone基于ResNet来提取特征，事实上我们对ResNet的结构进行了一些小的调整，使得最终编码得到的特征是经过16倍下采样的（而不是32倍）。对于三个输出分支，我们的结构选择如下所示，即4个3×3的卷积就直接进行输出。最终的输出是对于每一个anchor point都会有上面提到的offset、depth以及置信度输出。
损失函数 对于损失函数的选择，我们使用SmoothL1作为距离衡量标准，最终的损失包括 l o s s 1 loss_1 loss1​和 l o s s 2 loss_2 loss2​两项， l o s s 1 loss_1 loss1​用于衡量预测值与GT之间的差异：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/62a05ca46b18a4c768d8585ddd3a5d6c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-09-17T16:08:27+08:00" />
<meta property="article:modified_time" content="2019-09-17T16:08:27+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ICCV2019：基于Anchor point的手势及人体3D姿态估计方法：A2J</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h4><a id="A2J_AnchortoJoint_Regression_Network_for_3D_Articulated_Pose_Estimation_from_a_Single_Depth_Image_0"></a><em>A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation from a Single Depth Image</em></h4> 
<h4><a id="_1"></a>简介：</h4> 
<p>这是我们在ICCV 2019的一篇论文，文章主要解决的是基于单张深度图像的手势以及人体的3D姿态估计问题，我们首次提出了将经典目标检测的<strong>anchor</strong>的概念应用到姿态估计任务中来，设计了一种简洁有效的方法，最终在包括三个手势姿态估计数据集（HANDS2019，ICVL，NYU）以及两个人体姿态估计数据集（ITOP，K2HPD）上取得了<strong>state-of-the-art</strong>的效果。</p> 
<p>这里对文章做一个简要的介绍，欢迎大家讨论，相互学习，同时，如果你对我们的工作有兴趣，可以下载阅读我们的论文：<a href="https://arxiv.org/abs/1908.09999" rel="nofollow">https://arxiv.org/abs/1908.09999</a>。同时，代码也即将给出：<a href="https://github.com/zhangboshen/A2J">https://github.com/zhangboshen/A2J</a>。</p> 
<h4><a id="AnchortoJoint_7"></a>Anchor-to-Joint：</h4> 
<p>首先给出两张paper中的示意图。左图是我们技术方案的一个流程示意，对于一张给定的输入深度图片（这里假设输入仅包含单个手或人体，前面需要一个检测器的预处理步骤，类似于human pose的Top-down思路），我们密集的在图像上设立anchor point，如右图所示，每隔<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         s 
        
       
         t 
        
       
         r 
        
       
         i 
        
       
         d 
        
       
         e 
        
       
         = 
        
       
         4 
        
       
      
        stride=4 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">t</span><span class="mord mathdefault" style="margin-right: 0.02778em;">r</span><span class="mord mathdefault">i</span><span class="mord mathdefault">d</span><span class="mord mathdefault">e</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">4</span></span></span></span></span>个像素点就会设立一个anchor point，最终的关节点预测将会通过这些anchor point去完成。具体而言，每个锚点需要针对所有关节点预测一个图像坐标下的偏移量（左图中的绿色箭头）以及一个深度值，最终的关节点坐标将由所有锚点加权投票得出（权重是可学习的，左图中红色的anchor point就是权重值比较大的点，我们称之为informative anchor point）。<br> <img src="https://images2.imgbox.com/18/be/EJKPN0NO_o.png" alt="A2J"></p> 
<h4><a id="_10"></a>技术实现：</h4> 
<p>为了实现上面的思路，我们采取的技术流程如下图所示。Backbone我们在实验中主要基于ResNet50，也对比进行了其他的网络结构。输出包含三个分支，一个是图像坐标系下面的偏移量输出，一个是深度值输出，最后一个是对anchor point进行加权的anchor proposal分支输出。第三支的输出结果经过softmax之后直接乘在前面两支的输出结果上对anchor point投票进行权重赋值。<br> <img src="https://images2.imgbox.com/ed/5a/sljvTTCp_o.png" alt="pipeline"><br> 上面的技术方案有几个特点：<br> 1）Anchor proposal分支可学习，对于不同的关节点，Informative anchor point的分布是不一样的，因此，A2J具有针对不同关节点的<strong>自适应性</strong>；<br> 2）网络中<strong>不包含Deconv层</strong>，这里区别于目前主流的Heatmap的做法，不需要上采样使得我们的算法速度更快，结构更简单；<br> 3）<strong>End-to-end training</strong>，我们最终的loss来自于anchor point加权投票之后的预测输出以及GT之间的差异，不需要对GT进行任何形式的变换。<br> 4）<strong>快</strong>。网络结构简单，前向推理的速度很快，我们在NVIDIA 1080Ti的实测速度超过100fps。</p> 
<h6><a id="_19"></a>网络结构</h6> 
<p>上面提到Backbone基于ResNet来提取特征，事实上我们对ResNet的结构进行了一些小的调整，使得最终编码得到的特征是经过16倍下采样的（而不是32倍）。对于三个输出分支，我们的结构选择如下所示，即4个3×3的卷积就直接进行输出。最终的输出是对于每一个anchor point都会有上面提到的offset、depth以及置信度输出。<br> <img src="https://images2.imgbox.com/b2/7e/kB8NiuDX_o.png" alt="network"></p> 
<h6><a id="_22"></a>损失函数</h6> 
<p>对于损失函数的选择，我们使用SmoothL1作为距离衡量标准，最终的损失包括<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         l 
        
       
         o 
        
       
         s 
        
        
        
          s 
         
        
          1 
         
        
       
      
        loss_1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault">s</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         l 
        
       
         o 
        
       
         s 
        
        
        
          s 
         
        
          2 
         
        
       
      
        loss_2 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault">s</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>两项，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         l 
        
       
         o 
        
       
         s 
        
        
        
          s 
         
        
          1 
         
        
       
      
        loss_1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault">s</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>用于衡量预测值与GT之间的差异：<br> <img src="https://images2.imgbox.com/90/3c/4yRCIZ5n_o.png" alt="loss1"><br> <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         l 
        
       
         o 
        
       
         s 
        
        
        
          s 
         
        
          2 
         
        
       
      
        loss_2 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault">s</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>我们称之为Informative anchor point surrounding loss，这个loss的意义在于控制Informative anchor point的空间分布，我们希望最终的Informative anchor point可以从关节点的各个角度去观察预测，进而加以下面的约束：<br> <img src="https://images2.imgbox.com/a2/d9/R63AaKpX_o.png" alt="loss2"><br> <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         l 
        
       
         o 
        
       
         s 
        
        
        
          s 
         
        
          2 
         
        
       
      
        loss_2 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault">s</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>实际是上是一个正则项，可以有效地缓解过拟合现象，最终的效果也非常显著，如下图所示，有无<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         l 
        
       
         o 
        
       
         s 
        
        
        
          s 
         
        
          2 
         
        
       
      
        loss_2 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault">s</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>的时候Informative anchor point的位置分布差异很大：<br> <img src="https://images2.imgbox.com/a5/6e/5KNeRYUA_o.png" alt="loss2_vis"></p> 
<h4><a id="_29"></a>实验：</h4> 
<p>实验部分我们在包括三个手势姿态估计数据集（HANDS2019，ICVL，NYU）以及两个人体姿态估计数据集（ITOP，K2HPD）上进行，最终的结果都已经达到或接近了现在的state-of-the-art:<br> <img src="https://images2.imgbox.com/99/a0/LkcnvUri_o.png" alt="resultHand"><br> <img src="https://images2.imgbox.com/dc/a9/f7dzm7Cr_o.png" alt="resultHuman"><br> Ablation study依次验证了我们对于每一个component的选择的有效性（详细结果请参见我们的论文）。</p> 
<h4><a id="_34"></a>总结：</h4> 
<p>我们的工作创新点在于以一种简单的方式来使得anchor point的概念在姿态估计这个任务上得以实现。方法简单有效。A2J区别于现在主流的基于Heatmap的2D人体姿态估计方法以及手势3D姿态估计的方法（3D CNN，PointNet），拥有以上提到的许多优点。同时，A2J可以很容易的拓展到许多类似的任务中去，比如基于RGB图的2D/3D人体以及手势的姿态估计。此外，A2J的速度优势明显，未来可以考虑应用到一些耗时要求较高的嵌入式设备中。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/dc8545b75bda2e363d06644298400754/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">CentOS 升级 Camke</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/bf5c2e2d0cec744f3cf9149b6c322765/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Python基础教程：教你制作一个汇率换算程序</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>