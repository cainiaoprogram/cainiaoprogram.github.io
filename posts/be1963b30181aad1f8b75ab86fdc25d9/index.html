<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>82.长短期记忆网络（LSTM）以及代码实现 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="82.长短期记忆网络（LSTM）以及代码实现" />
<meta property="og:description" content="1. 长短期记忆网络 忘记门：将值朝0减少输入门：决定不是忽略掉输入数据输出门：决定是不是使用隐状态 2. 门 3. 候选记忆单元 4. 记忆单元 5. 隐状态 6. 总结 7. 从零实现的代码 我们首先加载时光机器数据集。
import torch from torch import nn from d2l import torch as d2l batch_size, num_steps = 32, 35 train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps) 7.1 初始化模型参数 接下来，我们需要定义和初始化模型参数。 如前所述，超参数num_hiddens定义隐藏单元的数量。 我们按照标准差 0.01 的高斯分布初始化权重，并将偏置项设为 0 。
def get_lstm_params(vocab_size, num_hiddens, device): num_inputs = num_outputs = vocab_size def normal(shape): return torch.randn(size=shape, device=device)*0.01 def three(): return (normal((num_inputs, num_hiddens)), normal((num_hiddens, num_hiddens)), torch.zeros(num_hiddens, device=device)) W_xi, W_hi, b_i = three() # 输入门参数 W_xf, W_hf, b_f = three() # 遗忘门参数 W_xo, W_ho, b_o = three() # 输出门参数 W_xc, W_hc, b_c = three() # 候选记忆元参数 # 输出层参数 W_hq = normal((num_hiddens, num_outputs)) b_q = torch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/be1963b30181aad1f8b75ab86fdc25d9/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-01-18T14:57:05+08:00" />
<meta property="article:modified_time" content="2023-01-18T14:57:05+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">82.长短期记忆网络（LSTM）以及代码实现</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="1__0"></a>1. 长短期记忆网络</h2> 
<ul><li>忘记门：将值朝0减少</li><li>输入门：决定不是忽略掉输入数据</li><li>输出门：决定是不是使用隐状态</li></ul> 
<h2><a id="2__7"></a>2. 门</h2> 
<p><img src="https://images2.imgbox.com/bc/a5/Zui5KsK7_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="3__10"></a>3. 候选记忆单元</h2> 
<p><img src="https://images2.imgbox.com/ad/d6/pINTRBDt_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="4__15"></a>4. 记忆单元</h2> 
<p><img src="https://images2.imgbox.com/9a/a4/MvNVCtVN_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="5__19"></a>5. 隐状态</h2> 
<p><img src="https://images2.imgbox.com/cb/c9/hJfqYEGJ_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="6__23"></a>6. 总结</h2> 
<p><img src="https://images2.imgbox.com/f8/54/1xj9pvhY_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="7__26"></a>7. 从零实现的代码</h2> 
<p>我们首先加载时光机器数据集。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l

batch_size<span class="token punctuation">,</span> num_steps <span class="token operator">=</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">35</span>
train_iter<span class="token punctuation">,</span> vocab <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_data_time_machine<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> num_steps<span class="token punctuation">)</span>
</code></pre> 
<h2><a id="71__39"></a>7.1 初始化模型参数</h2> 
<p>接下来，我们需要定义和初始化模型参数。 如前所述，超参数<code>num_hiddens</code>定义隐藏单元的数量。 我们按照标准差 0.01 的高斯分布初始化权重，并将偏置项设为 0 。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">get_lstm_params</span><span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>
    num_inputs <span class="token operator">=</span> num_outputs <span class="token operator">=</span> vocab_size

    <span class="token keyword">def</span> <span class="token function">normal</span><span class="token punctuation">(</span>shape<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>size<span class="token operator">=</span>shape<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">0.01</span>

    <span class="token keyword">def</span> <span class="token function">three</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token punctuation">(</span>normal<span class="token punctuation">(</span><span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                normal<span class="token punctuation">(</span><span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">)</span>

    W_xi<span class="token punctuation">,</span> W_hi<span class="token punctuation">,</span> b_i <span class="token operator">=</span> three<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 输入门参数</span>
    W_xf<span class="token punctuation">,</span> W_hf<span class="token punctuation">,</span> b_f <span class="token operator">=</span> three<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 遗忘门参数</span>
    W_xo<span class="token punctuation">,</span> W_ho<span class="token punctuation">,</span> b_o <span class="token operator">=</span> three<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 输出门参数</span>
    W_xc<span class="token punctuation">,</span> W_hc<span class="token punctuation">,</span> b_c <span class="token operator">=</span> three<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 候选记忆元参数</span>
    <span class="token comment"># 输出层参数</span>
    W_hq <span class="token operator">=</span> normal<span class="token punctuation">(</span><span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> num_outputs<span class="token punctuation">)</span><span class="token punctuation">)</span>
    b_q <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>num_outputs<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>
    <span class="token comment"># 附加梯度</span>
    params <span class="token operator">=</span> <span class="token punctuation">[</span>W_xi<span class="token punctuation">,</span> W_hi<span class="token punctuation">,</span> b_i<span class="token punctuation">,</span> W_xf<span class="token punctuation">,</span> W_hf<span class="token punctuation">,</span> b_f<span class="token punctuation">,</span> W_xo<span class="token punctuation">,</span> W_ho<span class="token punctuation">,</span> b_o<span class="token punctuation">,</span> W_xc<span class="token punctuation">,</span> W_hc<span class="token punctuation">,</span>
              b_c<span class="token punctuation">,</span> W_hq<span class="token punctuation">,</span> b_q<span class="token punctuation">]</span>
    <span class="token keyword">for</span> param <span class="token keyword">in</span> params<span class="token punctuation">:</span>
        param<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> params
</code></pre> 
<h2><a id="72__70"></a>7.2 定义模型</h2> 
<p>在<strong>初始化函数</strong>中， 长短期记忆网络的隐状态需要返回一个额外的<strong>记忆元</strong>， 单元的值为0，形状为（批量大小，隐藏单元数）。 因此，我们得到以下的状态初始化。</p> 
<pre><code class="prism language-python"><span class="token comment"># C和H都要初始化</span>
<span class="token keyword">def</span> <span class="token function">init_lstm_state</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span>
            torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>实际模型的定义与我们前面讨论的一样： <strong>提供三个门和一个额外的记忆元</strong>。 请注意，<code>只有隐状态才会传递到输出层， 而记忆元 𝐂𝑡 不直接参与输出计算</code>。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">lstm</span><span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> state<span class="token punctuation">,</span> params<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token punctuation">[</span>W_xi<span class="token punctuation">,</span> W_hi<span class="token punctuation">,</span> b_i<span class="token punctuation">,</span> W_xf<span class="token punctuation">,</span> W_hf<span class="token punctuation">,</span> b_f<span class="token punctuation">,</span> W_xo<span class="token punctuation">,</span> W_ho<span class="token punctuation">,</span> b_o<span class="token punctuation">,</span> W_xc<span class="token punctuation">,</span> W_hc<span class="token punctuation">,</span> b_c<span class="token punctuation">,</span>
     W_hq<span class="token punctuation">,</span> b_q<span class="token punctuation">]</span> <span class="token operator">=</span> params
    <span class="token punctuation">(</span>H<span class="token punctuation">,</span> C<span class="token punctuation">)</span> <span class="token operator">=</span> state
    outputs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> X <span class="token keyword">in</span> inputs<span class="token punctuation">:</span>
        I <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">(</span>X @ W_xi<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>H @ W_hi<span class="token punctuation">)</span> <span class="token operator">+</span> b_i<span class="token punctuation">)</span>
        F <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">(</span>X @ W_xf<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>H @ W_hf<span class="token punctuation">)</span> <span class="token operator">+</span> b_f<span class="token punctuation">)</span>
        O <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">(</span>X @ W_xo<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>H @ W_ho<span class="token punctuation">)</span> <span class="token operator">+</span> b_o<span class="token punctuation">)</span>
        C_tilda <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span><span class="token punctuation">(</span>X @ W_xc<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>H @ W_hc<span class="token punctuation">)</span> <span class="token operator">+</span> b_c<span class="token punctuation">)</span>
        C <span class="token operator">=</span> F <span class="token operator">*</span> C <span class="token operator">+</span> I <span class="token operator">*</span> C_tilda
        H <span class="token operator">=</span> O <span class="token operator">*</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>C<span class="token punctuation">)</span>
        Y <span class="token operator">=</span> <span class="token punctuation">(</span>H @ W_hq<span class="token punctuation">)</span> <span class="token operator">+</span> b_q
        outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>Y<span class="token punctuation">)</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>H<span class="token punctuation">,</span> C<span class="token punctuation">)</span>
</code></pre> 
<h2><a id="73__101"></a>7.3 训练和预测</h2> 
<p>让我们通过实例化rnn_scratch中 引入的<code>RNNModelScratch类</code>来训练一个长短期记忆网络， 就如我们在gru中所做的一样。</p> 
<pre><code class="prism language-python">vocab_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> device <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> d2l<span class="token punctuation">.</span>try_gpu<span class="token punctuation">(</span><span class="token punctuation">)</span>
num_epochs<span class="token punctuation">,</span> lr <span class="token operator">=</span> <span class="token number">500</span><span class="token punctuation">,</span> <span class="token number">1</span>
model <span class="token operator">=</span> d2l<span class="token punctuation">.</span>RNNModelScratch<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> device<span class="token punctuation">,</span> get_lstm_params<span class="token punctuation">,</span>
                            init_lstm_state<span class="token punctuation">,</span> lstm<span class="token punctuation">)</span>
d2l<span class="token punctuation">.</span>train_ch8<span class="token punctuation">(</span>model<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> vocab<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> device<span class="token punctuation">)</span>
</code></pre> 
<p>运行结果：</p> 
<p><img src="https://images2.imgbox.com/2b/f4/BDTidF0I_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="8__119"></a>8. 简洁实现</h2> 
<p>使用高级API，我们可以直接实例化<code>LSTM模型</code>。 高级API封装了前文介绍的所有配置细节。 这段代码的运行速度要快得多， 因为它使用的是编译好的运算符而不是Python来处理之前阐述的许多细节。</p> 
<pre><code class="prism language-python">num_inputs <span class="token operator">=</span> vocab_size
lstm_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span>
model <span class="token operator">=</span> d2l<span class="token punctuation">.</span>RNNModel<span class="token punctuation">(</span>lstm_layer<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
d2l<span class="token punctuation">.</span>train_ch8<span class="token punctuation">(</span>model<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> vocab<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> device<span class="token punctuation">)</span>
</code></pre> 
<p>运行结果：</p> 
<p><img src="https://images2.imgbox.com/02/17/zYlHhbhY_o.png" alt="在这里插入图片描述"></p> 
<p>实际情况下，LSTM和GRU用哪个都可以，性能差不多。</p> 
<blockquote> 
 <p>长短期记忆网络是典型的具有重要状态控制的<strong>隐变量自回归模型</strong>。 多年来已经提出了其许多变体，例如，多层、残差连接、不同类型的正则化。 然而，由于序列的<code>长距离依赖性</code>，训练长短期记忆网络 和其他序列模型（例如门控循环单元）的成本是相当高的。 在后面的内容中，我们将讲述更高级的替代模型，如<strong>Transformer</strong>。</p> 
</blockquote> 
<h2><a id="9_QA_144"></a>9. Q&amp;A</h2>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/1521d200c0420e20f5460d4b6f5bbab9/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">super-jacoco环境部署和使用--支持环境全量</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/70100fdcb118470cd1fb74674a2202dc/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">神经网络模型训练及验证套路</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>