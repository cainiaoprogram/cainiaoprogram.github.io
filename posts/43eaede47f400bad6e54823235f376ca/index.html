<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>异常监测②——lstm时间序列预测&amp;lstm简易原理 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="异常监测②——lstm时间序列预测&amp;lstm简易原理" />
<meta property="og:description" content="目录
利用LSTM进行时间序列预测流程
lstm滞后性
理解lstm原理
一、预测
二、遗忘
三、筛选
四、忽视
202305补充：从公式角度理解，
原理示例
lstm参数
各参数的定义解释：
batch_size
如何选择batch_size的大小：
epoch
预测滞后问题
损失函数的选择
1、Mean Squared Error Loss（MSE）
2、Mean Squared Logarithmic Error Loss（MSLE）
3、Mean Absolute Error Loss (MAE)
4、Binary Classification Loss Functions
5、Binary Cross-Entropy Loss
6、Hinge Loss
7、Squared HInge Loss
8、Multi-Class Classification Loss Functions
如何选择优化器Optimizer
dropout层
1、什么时候应该加dropout层？
2、放在哪几个层之间？
利用LSTM进行时间序列预测流程 在用统计学方法（3σ原则、四分位距法）等对异常数据进行监测后 （异常监测①——统计学方法判断）
由于业务特点，导致时序数据的规律趋势会变化。下方为举例：
时间点原来的count现在的count2020-01-01 13:00:0026710002020-01-01 13:10:0030002020-01-01 13:20:0026102020-01-01 13:30:0028910232020-01-01 13:40:0023502020-01-01 13:50:0027302020-01-01 14:00:003091014 此时原统计学方法可能不再适用，可以重新观察新的数据分布规律，手动修改代码逻辑。
但考虑到后续规律仍可能发生变动，为避免后续需要继续改异常监测逻辑，选择使用深度学习模型——lstm 对时序数据的分布进行预测。并且增添 模型更新 逻辑，使得模型不需要再人工更改。
参考以下资料：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/43eaede47f400bad6e54823235f376ca/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-23T15:28:08+08:00" />
<meta property="article:modified_time" content="2023-05-23T15:28:08+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">异常监测②——lstm时间序列预测&amp;lstm简易原理</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E5%88%A9%E7%94%A8LSTM%E8%BF%9B%E8%A1%8C%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E9%A2%84%E6%B5%8B-toc" style="margin-left:0px;"><a href="#%E5%88%A9%E7%94%A8LSTM%E8%BF%9B%E8%A1%8C%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E9%A2%84%E6%B5%8B" rel="nofollow">利用LSTM进行时间序列预测流程</a></p> 
<p id="lstm%E6%BB%9E%E5%90%8E%E6%80%A7-toc" style="margin-left:0px;"><a href="#lstm%E6%BB%9E%E5%90%8E%E6%80%A7" rel="nofollow">lstm滞后性</a></p> 
<p id="%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3lstm%E5%8E%9F%E7%90%86-toc" style="margin-left:0px;"><a href="#%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3lstm%E5%8E%9F%E7%90%86" rel="nofollow">理解lstm原理</a></p> 
<p id="%E4%B8%80%E3%80%81%E9%A2%84%E6%B5%8B-toc" style="margin-left:80px;"><a href="#%E4%B8%80%E3%80%81%E9%A2%84%E6%B5%8B" rel="nofollow">一、预测</a></p> 
<p id="%E4%BA%8C%E3%80%81%E9%81%97%E5%BF%98-toc" style="margin-left:80px;"><a href="#%E4%BA%8C%E3%80%81%E9%81%97%E5%BF%98" rel="nofollow">二、遗忘</a></p> 
<p id="%E4%B8%89%E3%80%81%E7%AD%9B%E9%80%89-toc" style="margin-left:80px;"><a href="#%E4%B8%89%E3%80%81%E7%AD%9B%E9%80%89" rel="nofollow">三、筛选</a></p> 
<p id="%E5%9B%9B%E3%80%81%E5%BF%BD%E8%A7%86-toc" style="margin-left:80px;"><a href="#%E5%9B%9B%E3%80%81%E5%BF%BD%E8%A7%86" rel="nofollow">四、忽视</a></p> 
<p id="202305%E8%A1%A5%E5%85%85%EF%BC%9A%E4%BB%8E%E5%85%AC%E5%BC%8F%E8%A7%92%E5%BA%A6%E7%90%86%E8%A7%A3%EF%BC%8C-toc" style="margin-left:40px;"><a href="#202305%E8%A1%A5%E5%85%85%EF%BC%9A%E4%BB%8E%E5%85%AC%E5%BC%8F%E8%A7%92%E5%BA%A6%E7%90%86%E8%A7%A3%EF%BC%8C" rel="nofollow">202305补充：从公式角度理解，</a></p> 
<p id="%E7%A4%BA%E4%BE%8B%EF%BC%9A-toc" style="margin-left:40px;"><a href="#%E7%A4%BA%E4%BE%8B%EF%BC%9A" rel="nofollow">原理示例</a></p> 
<p id="lstm%E5%8F%82%E6%95%B0-toc" style="margin-left:0px;"><a href="#lstm%E5%8F%82%E6%95%B0" rel="nofollow">lstm参数</a></p> 
<p id="%E5%90%84%E5%8F%82%E6%95%B0%E7%9A%84%E5%AE%9A%E4%B9%89%E8%A7%A3%E9%87%8A%EF%BC%9A-toc" style="margin-left:80px;"><a href="#%E5%90%84%E5%8F%82%E6%95%B0%E7%9A%84%E5%AE%9A%E4%B9%89%E8%A7%A3%E9%87%8A%EF%BC%9A" rel="nofollow">各参数的定义解释：</a></p> 
<p id="batch_size-toc" style="margin-left:80px;"><a href="#batch_size" rel="nofollow">batch_size</a></p> 
<p id="%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9batch_size%E7%9A%84%E5%A4%A7%E5%B0%8F%EF%BC%9A-toc" style="margin-left:80px;"><a href="#%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9batch_size%E7%9A%84%E5%A4%A7%E5%B0%8F%EF%BC%9A" rel="nofollow">如何选择batch_size的大小：</a></p> 
<p id="epoch-toc" style="margin-left:80px;"><a href="#epoch" rel="nofollow">epoch</a></p> 
<p id="%E9%A2%84%E6%B5%8B%E6%BB%9E%E5%90%8E%E9%97%AE%E9%A2%98-toc" style="margin-left:0px;"><a href="#%E9%A2%84%E6%B5%8B%E6%BB%9E%E5%90%8E%E9%97%AE%E9%A2%98" rel="nofollow">预测滞后问题</a></p> 
<p id="%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E9%80%89%E6%8B%A9-toc" style="margin-left:0px;"><a href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E9%80%89%E6%8B%A9" rel="nofollow">损失函数的选择</a></p> 
<p id="1%E3%80%81Mean%20Squared%20Error%20Loss%EF%BC%88MSE%EF%BC%89-toc" style="margin-left:80px;"><a href="#1%E3%80%81Mean%20Squared%20Error%20Loss%EF%BC%88MSE%EF%BC%89" rel="nofollow">1、Mean Squared Error Loss（MSE）</a></p> 
<p id="2%E3%80%81Mean%20Squared%20Logarithmic%20Error%20Loss%EF%BC%88MSLE%EF%BC%89-toc" style="margin-left:80px;"><a href="#2%E3%80%81Mean%20Squared%20Logarithmic%20Error%20Loss%EF%BC%88MSLE%EF%BC%89" rel="nofollow">2、Mean Squared Logarithmic Error Loss（MSLE）</a></p> 
<p id="3%E3%80%81Mean%20Absolute%20Error%20Loss%20(MAE)-toc" style="margin-left:80px;"><a href="#3%E3%80%81Mean%20Absolute%20Error%20Loss%20%28MAE%29" rel="nofollow">3、Mean Absolute Error Loss (MAE)</a></p> 
<p id="4%E3%80%81Binary%20Classification%20Loss%20Functions-toc" style="margin-left:80px;"><a href="#4%E3%80%81Binary%20Classification%20Loss%20Functions" rel="nofollow">4、Binary Classification Loss Functions</a></p> 
<p id="5%E3%80%81Binary%20Cross-Entropy%20Loss-toc" style="margin-left:80px;"><a href="#5%E3%80%81Binary%20Cross-Entropy%20Loss" rel="nofollow">5、Binary Cross-Entropy Loss</a></p> 
<p id="6%E3%80%81Hinge%20Loss-toc" style="margin-left:80px;"><a href="#6%E3%80%81Hinge%20Loss" rel="nofollow">6、Hinge Loss</a></p> 
<p id="7%E3%80%81Squared%20HInge%20Loss-toc" style="margin-left:80px;"><a href="#7%E3%80%81Squared%20HInge%20Loss" rel="nofollow">7、Squared HInge Loss</a></p> 
<p id="8%E3%80%81Multi-Class%20Classification%20Loss%20Functions-toc" style="margin-left:80px;"><a href="#8%E3%80%81Multi-Class%20Classification%20Loss%20Functions" rel="nofollow">8、Multi-Class Classification Loss Functions</a></p> 
<p id="%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E4%BC%98%E5%8C%96%E5%99%A8Optimizer-toc" style="margin-left:0px;"><a href="#%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E4%BC%98%E5%8C%96%E5%99%A8Optimizer" rel="nofollow">如何选择优化器Optimizer</a></p> 
<p id="dropout%E5%B1%82-toc" style="margin-left:0px;"><a href="#dropout%E5%B1%82" rel="nofollow">dropout层</a></p> 
<p id="1%E3%80%81%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E5%BA%94%E8%AF%A5%E5%8A%A0dropout%E5%B1%82%EF%BC%9F-toc" style="margin-left:40px;"><a href="#1%E3%80%81%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E5%BA%94%E8%AF%A5%E5%8A%A0dropout%E5%B1%82%EF%BC%9F" rel="nofollow">1、什么时候应该加dropout层？</a></p> 
<p id="2%E3%80%81%E6%94%BE%E5%9C%A8%E5%93%AA%E5%87%A0%E4%B8%AA%E5%B1%82%E4%B9%8B%E9%97%B4%EF%BC%9F-toc" style="margin-left:40px;"><a href="#2%E3%80%81%E6%94%BE%E5%9C%A8%E5%93%AA%E5%87%A0%E4%B8%AA%E5%B1%82%E4%B9%8B%E9%97%B4%EF%BC%9F" rel="nofollow">2、放在哪几个层之间？</a></p> 
<p></p> 
<hr id="hr-toc"> 
<h4></h4> 
<h2 id="%E5%88%A9%E7%94%A8LSTM%E8%BF%9B%E8%A1%8C%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E9%A2%84%E6%B5%8B">利用LSTM进行时间序列预测流程</h2> 
<p>在用统计学方法（3σ原则、四分位距法）等对异常数据进行监测后 （<a href="https://blog.csdn.net/qq_33936417/article/details/100634377" title="异常监测①——统计学方法判断">异常监测①——统计学方法判断</a>）</p> 
<p>由于业务特点，导致时序数据的规律趋势会变化。下方为举例：</p> 
<table border="1" cellpadding="1" cellspacing="1" style="width:500px;"><tbody><tr><td>时间点</td><td>原来的count</td><td>现在的count</td></tr><tr><td>2020-01-01 13:00:00</td><td>267</td><td>1000</td></tr><tr><td>2020-01-01 13:10:00</td><td>300</td><td>0</td></tr><tr><td>2020-01-01 13:20:00</td><td>261</td><td>0</td></tr><tr><td>2020-01-01 13:30:00</td><td>289</td><td>1023</td></tr><tr><td>2020-01-01 13:40:00</td><td>235</td><td>0</td></tr><tr><td>2020-01-01 13:50:00</td><td>273</td><td>0</td></tr><tr><td>2020-01-01 14:00:00</td><td>309</td><td>1014</td></tr></tbody></table> 
<p>此时原统计学方法可能不再适用，可以重新观察新的数据分布规律，手动修改代码逻辑。</p> 
<p>但考虑到后续规律仍可能发生变动，为避免后续需要继续改异常监测逻辑，选择使用深度学习模型——<strong>lstm</strong> 对时序数据的分布进行预测。并且增添 模型更新 逻辑，使得模型不需要再人工更改。</p> 
<blockquote> 
 <p>参考以下资料：</p> 
 <p><a href="https://cloud.tencent.com/developer/article/1416353" rel="nofollow" title="干货 | 携程实时智能异常检测平台的算法及工程实现">干货 | 携程实时智能异常检测平台的算法及工程实现</a>  （<a href="http://www.itdks.com/Course/detail?id=17581" rel="nofollow" title="视频链接">视频链接</a>）</p> 
 <p><a href="https://blog.csdn.net/qq_35649669/article/details/84990183" title="简单粗暴LSTM：LSTM进行时间序列预测">简单粗暴LSTM：LSTM进行时间序列预测</a></p> 
 <p><a href="https://www.slideshare.net/ssuserbefd12/ss-164777085" rel="nofollow" title="异常检测在苏宁的实践">异常检测在苏宁的实践</a></p> 
 <p><a href="https://zhuanlan.zhihu.com/p/23018343" rel="nofollow" title="基于TensorFlow一次简单的RNN实现">基于TensorFlow一次简单的RNN实现</a></p> 
</blockquote> 
<p></p> 
<p>确定逻辑：</p> 
<blockquote> 
 <p><strong>1、原始数据存储</strong></p> 
 <p>        mongodb</p> 
 <p><strong>2、数据预处理</strong></p> 
 <p>        去除节假日数据，异常点替换、缺失值填充</p> 
 <p><strong>3、模型设计及训练</strong></p> 
 <p>        训练集：近十天数据（不含近三天）。   预测集：近三天</p> 
 <p>        多轮超参优化，确定最优超参，及预测范围（用历史多长数据预测未来多长）</p> 
 <p>        保存模型</p> 
 <p><strong>4、异常判断</strong></p> 
 <p>     ①<a href="https://www.slideshare.net/ssuserbefd12/ss-164777085" rel="nofollow" title="https://www.slideshare.net/ssuserbefd12/ss-164777085">https://www.slideshare.net/ssuserbefd12/ss-164777085</a>    异常边界取90%分位数，持续几次异常产生告警（邮件、微信）</p> 
 <p>     ② 3σ原则</p> 
 <p><strong>5、模型更新</strong></p> 
 <p>        数据监控：通过均值、标准差、中位数、IQR等统计指标以天为粒度监控数据漂移，数据漂移超过阈值</p> 
 <p>        模型监控：每天自动统计模型输出的全部异常点占比的概率，概率分布变化超出阈值后模型自动提取最新数据重新训练</p> 
</blockquote> 
<hr> 
<blockquote> 
 <p>注：关于训练集和测试集 该一起归一化还是分开归一化参考：</p> 
 <p><a href="https://www.zhihu.com/question/312639136" rel="nofollow" title="机器学习中，对于数据的预处理是否是测试集和训练集一起进行？">机器学习中，对于数据的预处理是否是测试集和训练集一起进行？</a> <a href="https://blog.csdn.net/qq_35649669/article/details/86756343" title="浅谈归一化对于LSTM进行时间序列预测的影响">浅谈归一化对于LSTM进行时间序列预测的影响</a></p> 
</blockquote> 
<p><strong>待补充：</strong></p> 
<p>捕捉过多的非异常 VS  漏掉真正的异常，怎么权衡</p> 
<p>为什么选择lstm而不是别的算法？</p> 
<p>lstm原理理解（各超参数的定义）</p> 
<p>算法如何调参（看什么指标<a href="https://www.cnblogs.com/kamekin/p/10163743.html" rel="nofollow" title="https://www.cnblogs.com/kamekin/p/10163743.html">https://www.cnblogs.com/kamekin/p/10163743.html</a>）</p> 
<p></p> 
<hr> 
<h2 id="lstm%E6%BB%9E%E5%90%8E%E6%80%A7">lstm滞后性</h2> 
<p>1、摘自stackoverflow ——预测值滞后问题是过拟合现象，那么就按过拟合的解决方法来尝试，例如：增加dropout 层等。</p> 
<p style="text-align:center;"><img alt="" height="232" src="https://images2.imgbox.com/12/a4/snAWoU7b_o.jpg" width="431"></p> 
<p>2、摘自CSDN——“滞后性”的原因是LSTM过于重视时间步长中最后一个t-1时刻的数据，导致预测结果就是t-1时刻数据的复现。 </p> 
<p></p> 
<hr> 
<h2 id="%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3lstm%E5%8E%9F%E7%90%86">理解lstm原理</h2> 
<blockquote> 
 <p>参考的文章以及油管视频：（这个文章所在的网站似乎没了，视频还可以看） </p> 
 <p><a href="https://brohrer.mcknote.com/zh-Hans/how_machine_learning_works/how_rnns_lstm_work.html" rel="nofollow" title="递归神经网路和长短期记忆模型 RNN &amp; LSTM · 資料科學・機器・人">递归神经网路和长短期记忆模型 RNN &amp; LSTM · 資料科學・機器・人</a></p> 
 <p><a href="https://www.youtube.com/watch?v=WCUNPb-5EYI" rel="nofollow" title="​​​​​​Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM)">​​​​​​Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM)</a></p> 
</blockquote> 
<p>下面是我的总结以及理解：</p> 
<p>模型包含 <span style="color:#ff2600;">预测、忽视、遗忘、筛选</span> 四条路径。</p> 
<blockquote> 
 <div> 
  <p>注释：激活函数在本示例中的符号表示 （见下图右上角：</p> 
  <div></div> 
  <div> 
   <span style="color:#ff2600;">双曲正切函数tanh</span> 
  </div> 
  <div> 
   <img alt="" height="137" src="https://images2.imgbox.com/28/e2/Ea7IAJVb_o.jpg" width="300"> 
  </div> 
  <div></div> 
  <div></div> 
  <div> 
   <span style="color:#ff2600;">逻辑函数sigmoid</span>（和双曲正切函数类似，只是它的输出值介于0和1之间） 
  </div> 
  <div> 
   <img alt="" height="152" src="https://images2.imgbox.com/e3/c1/OPIWTtCG_o.jpg" width="306"> 
  </div> 
 </div> 
 <div></div> 
</blockquote> 
<h4></h4> 
<h4 id="%E4%B8%80%E3%80%81%E9%A2%84%E6%B5%8B">一、预测</h4> 
<div> 
 <span style="color:#333333;">传统RNN中，利用先前的预测（昨天的预测）以及新的信息（昨天的结果）。我们会利用两者作出新的预测，而这些新的预测会用于下一时间步的输出的运算。</span> 
</div> 
<div> 
 <figure class="image"> 
  <img alt="" height="313" src="https://images2.imgbox.com/df/0c/H1WYglgC_o.png" width="557"> 
  <figcaption>
    图1 
  </figcaption> 
 </figure> 
</div> 
<div></div> 
<div></div> 
<hr> 
<h4 id="%E4%BA%8C%E3%80%81%E9%81%97%E5%BF%98">二、遗忘</h4> 
<div>
  可能出现的错误： 模型只有很短期的记忆，只会参考前一步的结果，不会参考更早之前的信息，为解决这个问题，需要加入 
 <span style="color:#ff2600;">记忆/遗忘 路径</span>； 
</div> 
<div>
  （这里的原因展开讲是  ‘ 当时间步数T较大时，目标函数L关于隐藏状态  
 <img alt="h_t" class="mathcode" src="https://images2.imgbox.com/f8/7f/DFXuKoDh_o.png"> 的梯度容易出现梯度衰减 ’，涉及的公式推导可参考 
 <a class="link-info" href="https://book.douban.com/subject/33450010/" rel="nofollow" title="李沐书">李沐书</a> P233） 
</div> 
<div></div> 
<div> 
 <span style="color:#333333;">①在这条路径里，我们会通过sigmoid函数，创建一个<strong>记忆</strong></span> 
 <span style="color:#333333;"><strong>或遗忘特定信息</strong></span> 
 <span style="color:#333333;">的闸门（即图中的「圈叉」符号），</span> 
</div> 
<div> 
 <span style="color:#333333;">②此圈叉闸门的输入是上一轮的预测结果。 </span> 
</div> 
<div> 
 <span style="color:#333333;">③此闸门的输出会加到本次的预测当中 （即图中的「圈加」符号） 。</span> 
</div> 
<div></div> 
<div> 
 <span style="color:#333333;">注：圈圈里包含十字的符号是（矩阵）</span> 
 <strong><span style="color:#ff2600;">逐元素加法</span></strong> 
 <span style="color:#333333;">；</span> 
 <span style="color:#333333;">圈圈里有个叉的符号是（矩阵）</span> 
 <strong><span style="color:#ff2600;">逐元素乘法</span></strong> 
</div> 
<div></div> 
<div> 
 <figure class="image"> 
  <img alt="" height="316" src="https://images2.imgbox.com/ce/cf/35NE8kX8_o.png" width="562"> 
  <figcaption>
    图2 
  </figcaption> 
 </figure> 
</div> 
<div></div> 
<div></div> 
<hr> 
<h4 id="%E4%B8%89%E3%80%81%E7%AD%9B%E9%80%89"><span style="color:#333333;">三、筛选</span></h4> 
<div> 
 <span style="color:#333333;">现在不一定会直接将这组结果当做最终的预测。所以模型中还要加一个</span> 
 <span style="color:#ff2600;">筛选路径</span> 
 <span style="color:#333333;">（selection)，将一部分的预测结果保留在模型中（从预测结果中选出特定几项作为该次循环的预测结果）。</span> 
</div> 
<div> 
 <figure class="image"> 
  <img alt="" height="311" src="https://images2.imgbox.com/63/74/PcIu7VL9_o.png" width="554"> 
  <figcaption>
    图3 
  </figcaption> 
 </figure> 
</div> 
<div></div> 
<div>
  ①这个筛选路径也自成一个神经网络，每次输入的新信息 和 旧预测 都会影响筛选路径中 阀门的大小； 
</div> 
<div>
  ②筛选路径也有一个sigmoid挤压函数（中文里一般叫激活函数），因为在之前我们做了一次逐元素相加，预测结果可能会比1大、或比-1小，所以这个压缩函数是用来确保数值大小仍在（-1,1）区间内。 
</div> 
<div></div> 
<div></div> 
<div></div> 
<hr> 
<h4 id="%E5%9B%9B%E3%80%81%E5%BF%BD%E8%A7%86">四、忽视</h4> 
<div>
  在完成整个模型前，还需要加上一个 
 <span style="color:#ff2600;">忽视路径（ignoring）</span> 
</div> 
<div>
  忽视路径可以让近期内不是 
 <span style="color:#333333;">很相关的结果先被忽视，避免它们影响之后的路径。和其他路径一样，忽视路径有自己的神经网络、激活函数和矩阵运算 （即图中右下角的「圈叉」符号） 。</span> 
 <figure class="image"> 
  <img alt="" height="346" src="https://images2.imgbox.com/bf/10/H6orQUB3_o.jpg" width="573"> 
  <figcaption>
    图4 
  </figcaption> 
 </figure> 
</div> 
<div></div> 
<div></div> 
<div></div> 
<div></div> 
<p></p> 
<div> 
 <h3 id="202305%E8%A1%A5%E5%85%85%EF%BC%9A%E4%BB%8E%E5%85%AC%E5%BC%8F%E8%A7%92%E5%BA%A6%E7%90%86%E8%A7%A3%EF%BC%8C"><span style="color:#fe2c24;">202305补充：</span><span style="color:#0d0016;">从</span>公式角度理解，</h3> 
 <div>
   （图4 和下面的 图6.10搭配理解） 
 </div> 
 <div> 
  <div>
    1、forgetting这条路径通常叫作 
   <span style="color:#956fe7;">遗忘门</span>，这条路径上sigmoid函数的输出为： 
  </div> 
  <div>
                                     
   <img alt="" height="38" src="https://images2.imgbox.com/43/9f/xepmR7QE_o.png" width="310"> 
  </div> 
  <div></div> 
 </div> 
 <div>
   2、最上面这条selecting路径，常见的说法叫 
  <span style="color:#956fe7;">输出门</span>，这条路径是输出门控制 从记忆细胞 
  <img alt="C_t" class="mathcode" src="https://images2.imgbox.com/83/92/U2zj9aF0_o.png">到隐藏状态 
  <img alt="H_t" class="mathcode" src="https://images2.imgbox.com/0a/78/qZ0FDJZx_o.png">的信息流动。而这条路径的sigmoid函数输出为： 
 </div> 
 <div>
                                   
  <img alt="" height="38" src="https://images2.imgbox.com/6b/c1/9vmW6web_o.png" width="324"> 
 </div> 
 <div>
   右上角的prediction可看做当前时刻的隐藏状态（实际上不算是最后的预测值，最终预测值还需要经过全连接层），当前时刻的隐藏状态：                                ​​​​​​​  
 </div> 
 <div>
           ​​​​​​​        ​​​​​​​        ​​​​​​​         
  <img alt="" height="56" src="https://images2.imgbox.com/d0/d4/4DCcjCFH_o.png" width="232">  
 </div> 
 <div>
           当输出门 
  <img alt="O_t" class="mathcode" src="https://images2.imgbox.com/82/27/Ii0WU7va_o.png">近似1时，记忆细胞信息将传递到隐藏状态供输出层使用。 
 </div> 
 <div>
           当输出门近似0时，记忆细胞信息只自己保留。 
 </div> 
 <div></div> 
 <div></div> 
</div> 
<div></div> 
<div>
  3、ignoring路径通常叫作 
 <span style="color:#956fe7;">输入门</span>， 
</div> 
<div>
          ​​​​​​​        ​​​​​​​         
 <img alt="" height="36" src="https://images2.imgbox.com/6f/8e/g6OwioxM_o.png" width="305"> 
</div> 
<div></div> 
<div>
  4、最下方这条是生成 候选记忆细胞（也可以看做一种特殊的隐藏状态）的路径。 
</div> 
<div>
          ​​​​​​​        ​​​​​​​       
 <img alt="" height="49" src="https://images2.imgbox.com/1f/85/jQEM1KBT_o.png" width="323"> 
</div> 
<div></div> 
<div>
  而当前时间步的记忆细胞： 
</div> 
<div>
          ​​​​​​​        ​​​​​​​         
 <img alt="" height="36" src="https://images2.imgbox.com/5d/88/50aVfbO9_o.png" width="226"> 
</div> 
<p>        遗忘门控制上一时间步的记忆细胞<img alt="C_{t-1}" class="mathcode" src="https://images2.imgbox.com/75/f9/gKaHXqSP_o.png">中的信息是否传递到当前时间步；</p> 
<p>        输入门控制当前时间步的输入<img alt="X_t" class="mathcode" src="https://images2.imgbox.com/9a/45/HmMjkx59_o.png">通过<img alt="\tilde{C}_t" class="mathcode" src="https://images2.imgbox.com/0d/0d/UxH5xPEG_o.png">如何流入当前时间步的记忆细胞<img alt="C_t" class="mathcode" src="https://images2.imgbox.com/43/59/g3cuKc96_o.png"></p> 
<div></div> 
<blockquote> 
 <div>
   总结：下图给出整个LSTM中，三个门的工作流程。（原图见李沐书的P245 ） 
 </div> 
 <div>
   sigmoid函数输出值在0,1之间，可以被用作一种控制阀门。 
  <img alt="F_t" class="mathcode" height="17" src="https://images2.imgbox.com/1c/70/ciCh6uvD_o.png" width="18">、 
  <img alt="I_t" class="mathcode" src="https://images2.imgbox.com/8a/df/iiVIilUe_o.png">、 
  <img alt="O_t" class="mathcode" src="https://images2.imgbox.com/91/b9/SEZxcNwW_o.png">都是由sigmoid函数得出， 
 </div> 
 <div> 
  <img alt="F_t" class="mathcode" height="17" src="https://images2.imgbox.com/c3/39/kHerIH2x_o.png" width="18">控制的是上一时间步的记忆细胞 
  <img alt="C_{t-1}" class="mathcode" src="https://images2.imgbox.com/eb/8a/LNR5Gato_o.png">中留下多少，所以叫遗忘门。 
 </div> 
 <div> 
  <img alt="I_t" class="mathcode" src="https://images2.imgbox.com/a7/b3/yKXhXnft_o.png">控制 
  <img alt="\tilde{C}_t" class="mathcode" src="https://images2.imgbox.com/58/e2/9w2HJAjT_o.png">，实质是控制 
  <img alt="X_t" class="mathcode" src="https://images2.imgbox.com/f9/59/pFCZrelW_o.png">有多少信息流入当前时间步记忆细胞，所以叫输入门（因为本质控制的是输入数据X。 
 </div> 
 <div> 
  <img alt="O_t" class="mathcode" src="https://images2.imgbox.com/53/af/j19IItc7_o.png">控制 
  <img alt="C_t" class="mathcode" src="https://images2.imgbox.com/3a/17/8RhqKGNw_o.png">中有多少信息输出到 
  <img alt="H_t" class="mathcode" src="https://images2.imgbox.com/29/09/uxPzd4Md_o.png">，所以叫输出门。 
 </div> 
 <div>
   而决定3个门输出值的权重参数W是在训练中学到的，即LSTM能自适应地调整参数，从而不断优化3个门的策略，学习到序列规律。 
 </div> 
 <div> 
  <img alt="" height="397" src="https://images2.imgbox.com/9f/75/CNgwedbe_o.png" width="612"> 
 </div> 
 <div></div> 
 <div></div> 
 <div></div> 
</blockquote> 
<hr> 
<h3 id="%E7%A4%BA%E4%BE%8B%EF%BC%9A">原理示例</h3> 
<div> 
 <span style="color:#333333;">童书里只有三种句子：「道格看见珍（句号）」、「珍看见小点（句号）」、以及「小点看见道格（句号）」。</span> 
</div> 
<div> 
 <img alt="" height="258" src="https://images2.imgbox.com/d5/1d/a8LsXJTs_o.jpg" width="396"> 
</div> 
<div></div> 
<div></div> 
<div>
  one-hot 编码应用： 
</div> 
<div> 
 <span style="color:#333333;">如果道格是我最后读到的单字，那我的信息矢量中，就只有道格的数值为 1，其他的数值都为 0。我们也可以按前面的方法，利用昨天（前一次）的预测结果，继续预测明天（下一次）的结果。经过一定的训练后，我们应该能从模型中看出一些特定的规律。</span> 
</div> 
<div></div> 
<div>
  进行预测： 
</div> 
<div>
  如果截止目前的故事为  
 <span style="color:#333333;">「珍看见小点（句号），道格⋯⋯」</span> 
</div> 
<div></div> 
<div> 
 <span style="color:#ff2600;">1、本次输入=道格，上次预测包含 「道格、珍、小点」</span> （因为前一次是“句号”为输入，所以前一次预测包含所有人的名字） 
</div> 
<div>
  将两个向量输入模型中： 
</div> 
<div></div> 
<p> <img alt="" height="318" src="https://images2.imgbox.com/6b/47/0Dn76WkP_o.jpg" width="535"></p> 
<div>
  初步预测结果： 
 <span style="color:#ff2600;">看见、非道格</span>（分别为正预测 
 <span style="color:#333333;">positive prediction </span>和负预测 
 <span style="color:#333333;">negative prediction</span>） 
</div> 
<div></div> 
<div>
  2、本例子不需考虑忽视路径，于是进入遗忘路径，初次预测当做此模型没有历史memory，所以进入筛选路径 
</div> 
<div> 
 <img alt="" height="315" src="https://images2.imgbox.com/55/ba/tSQjRTwL_o.jpg" width="544"> 
</div> 
<div></div> 
<div>
  3、假设筛选路径学习到的是  
 <span style="color:#333333;">「在前一个单字是名字的状况下，接下来的结果只能是『看见』或句号」这项规则，于是「非道格」这项预测就被挡掉了，</span> 
 <span style="color:#ff2600;">剩下「</span> 
 <span style="color:#ff2600;">看见</span> 
 <span style="color:#ff2600;">」成为最终预测</span> 
 <span style="color:#333333;">。</span> 
</div> 
<div></div> 
<div></div> 
<div>
  新的一轮循环： 
</div> 
<div></div> 
<div>
  1、本次输入=看见   ，上次预测=看见， 
 <span style="color:#333333;">「看见」同时是新信息和旧预测。</span> 
</div> 
<div>
  初步预测结果：「 
 <span style="color:#333333;">道格、珍、小点」 其中之一</span> 
</div> 
<div></div> 
<div>
  2、跳过忽视路径 
</div> 
<div>
  3、进入遗忘路径，前一轮预测结果是  
 <span style="color:#333333;">「非道格」和「看见」，</span> 
</div> 
<div> 
 <span style="color:#333333;">遗忘闸门学习到的是 ： </span> 
 <span style="color:#333333;">既然前一个单字是『看见』，根据经验我可以将回忆中的『看见』忘掉，保留名字就好。</span> 
</div> 
<div> 
 <span style="color:#333333;">于是遗忘路径输出的是  「非道格」</span> 
</div> 
<div></div> 
<div> 
 <span style="color:#333333;">4、</span> 
 <span style="color:#333333;">遗忘路径输出的「非道格」 和初步预测的 </span>「 
 <span style="color:#333333;">道格、珍、小点」进行逐元素加法，</span> 
 <span style="color:#333333;">「道格」的预测正负抵消，只留下了「珍」和「小点」继续前往下条路径。</span> 
</div> 
<div> 
 <img alt="" height="331" src="https://images2.imgbox.com/97/21/3GnLdWUX_o.jpg" width="555"> 
</div> 
<div></div> 
<div></div> 
<div></div> 
<div>
  5、筛选路径学习到 ： 
 <span style="color:#333333;">当「看见」是前一个单字时，下个出现的单字应该是一个名字，所以它让「珍」和「小点」双双通过。</span> 
</div> 
<div> 
 <span style="color:#333333;">于是，在最后的预测结果中，我们得到了 「珍」和「小点」。</span> 
</div> 
<div></div> 
<p><img alt="" height="335" src="https://images2.imgbox.com/78/ba/4NAO1oD9_o.jpg" width="557"></p> 
<p></p> 
<h2 id="lstm%E5%8F%82%E6%95%B0">lstm参数</h2> 
<h4 id="%E5%90%84%E5%8F%82%E6%95%B0%E7%9A%84%E5%AE%9A%E4%B9%89%E8%A7%A3%E9%87%8A%EF%BC%9A">各参数的定义解释：</h4> 
<div> 
 <div></div> 
 <div> 
  <pre><code class="language-python">model.add(LSTM(neurons,input_shape=(None,1))) #输入数据的形状
model.add(Dropout(dropout_value)) #dropout层
model.add(Dense(1))  
model.compile(loss='mean_squared_error', optimizer='adam')  # 编译（损失函数、优化器） 
model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size,verbose=verbose)
</code></pre> 
  <p><span style="color:#333333;"><span style="color:#333333;">参数解释：</span></span></p> 
 </div> 
 <div> 
  <span style="color:#333333;">1、neurons:LSTM神经元数量</span> 
 </div> 
 <div> 
  <span style="color:#333333;">2、input_shape=(timesteps,input_dim)中， input_dim ：features/embeddings 的维度</span> 
 </div> 
 <div> 
  <span style="color:#333333;">3、dropout_value:为了抑制过拟合，需要添加dropout层，详细见《dropout层》</span> 
 </div> 
 <div> 
  <span style="color:#333333;">    float between 0 and 1.DEfault:0. </span> 
 </div> 
 <div> 
  <span style="color:#333333;">    This value is the percentage of the considered network connections per epoch/batch.</span> 
 </div> 
 <div> 
  <span style="color:#333333;"><span style="color:#333333;">4、verbose:</span></span> 
 </div> 
 <div> 
  <span style="color:#333333;"><strong><span style="color:#333333;">    fit中的verbose:</span>日志显示</strong></span> 
 </div> 
 <div> 
  <span style="color:#333333;">    verbose = 0 为不在标准输出流输出日志信息</span> 
 </div> 
 <div> 
  <span style="color:#333333;">    verbose = 1 为输出进度条记录</span> 
 </div> 
 <div> 
  <span style="color:#333333;">    verbose = 2 为每个epoch输出一行记录</span> 
 </div> 
 <div> 
  <span style="color:#333333;">    注意： 默认为 1</span> 
 </div> 
 <div></div> 
 <div> 
  <span style="color:#333333;"><span style="color:#333333;"><strong>    evaluate中的verbose:</strong></span></span> 
 </div> 
 <div> 
  <span style="color:#333333;">    verbose：日志显示</span> 
 </div> 
 <div> 
  <span style="color:#333333;">    verbose = 0 为不在标准输出流输出日志信息</span> 
 </div> 
 <div> 
  <span style="color:#333333;">    verbose = 1 为输出进度条记录</span> 
 </div> 
 <div> 
  <span style="color:#333333;">    注意： 只能取 0 和 1；默认为 1</span> 
 </div> 
</div> 
<div></div> 
<div></div> 
<blockquote> 
 <ul><li> 
   <div> 
    <span style="color:#242729;">one </span> 
    <span style="color:#242729;"><strong>epoch</strong></span> 
    <span style="color:#242729;"> = one forward pass and one backward pass of </span> 
    <span style="color:#242729;"><em>all</em></span> 
    <span style="color:#242729;"> the training examples</span> 
   </div> </li><li> 
   <div> 
    <span style="color:#242729;"><strong>batch size</strong></span> 
    <span style="color:#242729;"> = the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need.</span> 
   </div> </li><li> 
   <div> 
    <span style="color:#242729;">number of </span> 
    <span style="color:#242729;"><strong>iterations</strong></span> 
    <span style="color:#242729;"> = number of passes, each pass using [batch size] number of examples. To be clear, one pass = one forward pass + one backward pass (we do not count the forward pass and backward pass as two different passes).</span> 
   </div> </li></ul> 
</blockquote> 
<div> 
 <span style="color:#242729;">Example: if you have 1000 training examples, and your batch size is 500, then it will take 2 iterations to complete 1 epoch.</span> 
</div> 
<div> 
 <div> 
  <span style="color:#333333;">（为便于理解，将一次forward/backward 称作 一次pass）</span> 
 </div> 
 <div> 
  <span style="color:#333333;">一次epoch= 所有训练数据都经历一次 pass 后更新参数的过程。</span> 
 </div> 
 <div> 
  <span style="color:#333333;">batch_size= 一次 pass 用的训练数据的数量</span> 
 </div> 
 <div> 
  <span style="color:#333333;">iteration= pass 的次数。 一个iteration=1个pass=batch_size个训练数据</span> 
 </div> 
 <div> 
  <span style="color:#333333;">另：一般是iteration译成“迭代”</span> 
 </div> 
 <div></div> 
 <div> 
  <span style="color:#333333;"><span style="color:#008f00;">1个iteration * batch_size= 训练样本的数量</span></span> 
 </div> 
 <div></div> 
</div> 
<div></div> 
<div></div> 
<div></div> 
<h4 id="batch_size"><span style="color:#ff2600;"><span style="color:#ff2600;">batch_size</span></span></h4> 
<div> 
 <a href="https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network" rel="nofollow" title="python - What is batch size in neural network? - Cross Validated">python - What is batch size in neural network? - Cross Validated</a> 
</div> 
<div>
  若training samples有1050个， batch_size =100。则模型第一次取第 1-100的samples 训练网络，第二次取第 101-200 的样本去训练网络。最后一次用最后50个样本去训练网络。 
</div> 
<div> 
 <span style="color:#1a1a1a;">Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。且减少训练时间。</span> 
 <span style="color:#1a1a1a;">但容易陷入局部最小（如鞍点）。</span> 
 <span style="color:#1a1a1a;">小的batch_size 收敛速度慢</span> 
</div> 
<div></div> 
<div> 
 <a href="https://zhuanlan.zhihu.com/p/77235541" rel="nofollow" title="keras中epochs、batch_size、iterations详解 - 知乎">keras中epochs、batch_size、iterations详解 - 知乎</a> 
</div> 
<div> 
 <a href="https://www.zhihu.com/question/32673260" rel="nofollow" title="深度学习中的batch的大小对学习效果有何影响？ - 知乎">深度学习中的batch的大小对学习效果有何影响？ - 知乎</a> 
</div> 
<div> 
 <span style="color:#4d4d4d;">①batch_size=训练数据集的长度时，为批量梯度下降算法——&gt;</span> 
 <span style="color:#242729;">Gradient is more general, but intractable for huge datasets.</span> 
</div> 
<div> 
 <span style="color:#4d4d4d;">②batch_size=1时就是随机梯度下降算法（在线学习）——&gt;</span> 
 <span style="color:#242729;">Gradient can be noisy.</span> 
</div> 
<div> 
 <span style="color:#242729;">③batch_size介于上述两者之间</span> 
</div> 
<div></div> 
<div></div> 
<div></div> 
<div></div> 
<h4 id="%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9batch_size%E7%9A%84%E5%A4%A7%E5%B0%8F%EF%BC%9A"><span style="color:#ff2600;">如何选择batch_size的大小：</span></h4> 
<div>
  对于训练数据数量&lt;1000 的，batch_size默认值是32的： 
 <a href="https://stackoverflow.com/questions/35050753/how-big-should-batch-size-and-number-of-epochs-be-when-fitting-a-model-in-keras" rel="nofollow" title="python - How big should batch size and number of epochs be when fitting a model? - Stack Overflow">python - How big should batch size and number of epochs be when fitting a model? - Stack Overflow</a> 
</div> 
<div>
  可以选择8的倍数，然后是稍微大一点 
 <a href="http://www.yebaochen.com/2018/02/05/batch-size-epochs-setting" rel="nofollow" title="http://www.yebaochen.com/2018/02/05/batch-size-epochs-setting">http://www.yebaochen.com/2018/02/05/batch-size-epochs-setting</a> 
</div> 
<div></div> 
<div></div> 
<div></div> 
<div></div> 
<h4 id="epoch"><span style="color:#ff2600;">epoch</span></h4> 
<div> 
 <span style="color:#1a1a1a;">epoch过大：过拟合；训练时间长。</span> 
</div> 
<div> 
 <span style="color:#1a1a1a;">epochs过小：欠拟合</span> 
</div> 
<div> 
 <span style="color:#1a1a1a;">需要根据损失函数的减小速率来选定epoch，当loss减小速率趋于平缓时即为合适的epoch</span> 
</div> 
<div></div> 
<div></div> 
<hr> 
<h2 id="%E9%A2%84%E6%B5%8B%E6%BB%9E%E5%90%8E%E9%97%AE%E9%A2%98">预测滞后问题</h2> 
<div> 
 <span style="color:#2f2f2f;">模型有些参数可以自己手动调一下,看看模型在不同参数下的效果(虽然我估计数据量太少,可能调参带来的变化不是很大,但是可以体验调参的过程),下面我就可以调的参数说明:</span> 
</div> 
<blockquote> 
 <p><span style="color:#2f2f2f;">(1)损失函数现在使用的是</span><span style="color:#2c3e50;">mean_squared_error</span><span style="color:#2f2f2f;">,可以调成别的</span></p> 
 <div></div> 
 <div> 
  <span style="color:#2f2f2f;">(2)优化器是</span> 
  <span style="color:#2c3e50;">adam</span> 
  <span style="color:#2f2f2f;">,也可以调,甚至对优化器内的参数进行调整(比如学习率)</span> 
 </div> 
 <div></div> 
 <div> 
  <span style="color:#2f2f2f;">(3)训练次数是50,可以调低点(因为我看后面模型的损失不下降了)</span> 
 </div> 
 <div></div> 
 <div> 
  <span style="color:#2f2f2f;">(4)基于历史多少数据的参数</span> 
  <span style="color:#2c3e50;">look_back</span> 
  <span style="color:#2f2f2f;">可调,你可以设置为3,5.....</span> 
 </div> 
</blockquote> 
<div></div> 
<hr> 
<h2 id="%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E9%80%89%E6%8B%A9">损失函数的选择</h2> 
<div> 
 <a href="https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/" rel="nofollow" title="How to Choose Loss Functions When Training Deep Learning Neural Networks - MachineLearningMastery.com">How to Choose Loss Functions When Training Deep Learning Neural Networks - MachineLearningMastery.com</a> 
</div> 
<div></div> 
<h4 id="1%E3%80%81Mean%20Squared%20Error%20Loss%EF%BC%88MSE%EF%BC%89">1、Mean Squared Error Loss（MSE）</h4> 
<div>
          是回归类问题的默认损失函数； 
</div> 
<div>
  从数学角度看，当目标变量符合高斯分布时，此损失函数是最佳的、最应该首先考虑的。只有当有足够理由时才选别的损失函数。 
</div> 
<div></div> 
<div> 
 <span style="color:#555555;">Mean squared error is calculated as the average of the squared differences between the predicted and actual values. </span> 
</div> 
<div> 
 <div> 
  <span style="color:#333333;">model.compile(loss=‘mean_squared_error')</span> 
 </div> 
</div> 
<div></div> 
<h4 id="2%E3%80%81Mean%20Squared%20Logarithmic%20Error%20Loss%EF%BC%88MSLE%EF%BC%89">2、<span style="color:#222222;">Mean Squared Logarithmic Error Loss（MSLE）</span></h4> 
<div> 
 <span style="color:#222222;">        当目标值较分散，且预测值很大；此时可以先计算每个预测值的自然对数，再计算此对数 和 actual values的mean squared error </span> 
</div> 
<div> 
 <div> 
  <span style="color:#333333;">model.compile(loss=‘mean_squared_logarithmic_error’)</span> 
 </div> 
</div> 
<div></div> 
<h4 id="3%E3%80%81Mean%20Absolute%20Error%20Loss%20(MAE)">3、Mean Absolute Error Loss (MAE)</h4> 
<div>
          目标值大部分符合高斯分布，但有一些界外点（原理均值的过大或过小值） 
</div> 
<div> 
 <span style="color:#555555;">‘</span> 
 <span style="color:#555555;"><em>mean_absolute_error</em></span> 
 <span style="color:#555555;">’</span> 
</div> 
<div></div> 
<h4 id="4%E3%80%81Binary%20Classification%20Loss%20Functions"><span style="color:#555555;">4、</span><span style="color:#222222;">Binary Classification Loss Functions</span></h4> 
<div>
          二分，预测值是两种标签之一的问题。 
</div> 
<div></div> 
<h4 id="5%E3%80%81Binary%20Cross-Entropy%20Loss">5、<span style="color:#222222;">Binary Cross-Entropy Loss</span></h4> 
<div>
          交叉熵是用于二元分类问题的默认损失函数。适用于目标值在 {0,1}集合中的二分类问题。 
</div> 
<div>
  loss=‘binary_crossentropy’ 
</div> 
<div></div> 
<h4 id="6%E3%80%81Hinge%20Loss">6、Hinge Loss</h4> 
<div>
          对于二分类问题，交叉熵的另一种替代方法是Hinge Loss(铰链损失函数)，它主要用于支持向量机(SVM)模型。 
</div> 
<div>
  适用于目标值在 {-1,1}集合中的二分类问题。 
</div> 
<div>
  loss=‘hinge’ 
</div> 
<div></div> 
<h4 id="7%E3%80%81Squared%20HInge%20Loss">7、Squared HInge Loss</h4> 
<h4 id="8%E3%80%81Multi-Class%20Classification%20Loss%20Functions">8、<span style="color:#222222;">Multi-Class Classification Loss Functions</span></h4> 
<div>
  多类分类是指那些示例被指定为两个以上类中的一个的预测建模问题。 
</div> 
<div></div> 
<div></div> 
<div></div> 
<h2 id="%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E4%BC%98%E5%8C%96%E5%99%A8Optimizer">如何选择优化器Optimizer</h2> 
<div> 
 <a href="https://www.dlology.com/blog/quick-notes-on-how-to-choose-optimizer-in-keras/" rel="nofollow" title="Quick Notes on How to choose Optimizer In Keras | DLology">Quick Notes on How to choose Optimizer In Keras | DLology</a> 
</div> 
<div></div> 
<div></div> 
<div></div> 
<h2 id="dropout%E5%B1%82">dropout层</h2> 
<h3 id="1%E3%80%81%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E5%BA%94%E8%AF%A5%E5%8A%A0dropout%E5%B1%82%EF%BC%9F"><strong>1、什么时候应该加dropout层？</strong></h3> 
<p>过拟合的表现：</p> 
<blockquote> 
 <p>在训练集下的表现过于优越，导致在验证集（测试集）上的表现不佳。</p> 
 <p>在神经网络中，有一个普遍现象可以说明是出现了过拟合，验证集的准确率回载训练了多个epoch后达到当val_loss达到一个最低值的时候突然回升，val_loss又不断升高。</p> 
</blockquote> 
<p>过拟合原因：模型复杂度过高，模型将训练数据中的噪声或随机误差视为真实模式，可以理解为模型只是死记硬背住了训练数据，而不能很好地泛化到新数据上。而模型复杂度由模型中的 <u>参数数量</u> 和 <u>参数值大小</u> 决定。</p> 
<p><span style="color:#f33b45;">过拟合的解决方法：</span><span style="color:#0d0016;">dropout 或 L1/L2正则化</span></p> 
<p>简单理解：L1、L2正则化是在损失函数中加入正则化项，相当于加入了约束条件，从而可以约束参数w的大小。而dropout是在训练时按一定概率弃用某些隐藏层神经元，减少神经元之间的相互依赖性，从而降低模型复杂度。</p> 
<p></p> 
<p>判断过拟合的代码：</p> 
<blockquote> 
 <p>通过model.evaluate计算测试精度判断 模型是否在训练集上性能过优：</p> 
 <pre><code class="language-python">train_acc = model.evaluate(trainX, trainy, verbose=0)
test_acc = model.evaluate(testX, testy, verbose=0)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))</code></pre> 
 <p>模型在训练数据集上的性能优于测试数据集，这是过度拟合的一个可能标志。</p> 
 <pre><code class="language-python">如：Train: 1.000, Test: 0.757 ，则可能为过拟合</code></pre> 
</blockquote> 
<h3></h3> 
<h3 id="2%E3%80%81%E6%94%BE%E5%9C%A8%E5%93%AA%E5%87%A0%E4%B8%AA%E5%B1%82%E4%B9%8B%E9%97%B4%EF%BC%9F"><strong>2、放在哪几个层之间？</strong></h3> 
<p><span style="color:#f33b45;">可以放任意层之间，但在不同位置，选取的dropout_value值应该采用不同的大小。</span></p> 
<p>经验法则是，当dropout应用于全连接层时，将保留概率(1 - drop概率)设置为0.5，而当应用于卷积层时，将其设置为更大的数字(通常为0.8、0.9)，即（1-dropout_value）=0.8或0.9 （dropout_value=0.1或0.2）。</p> 
<p style="text-align:center;"><img alt="" height="172" src="https://images2.imgbox.com/d7/a0/Hqe85frJ_o.jpg" width="555"></p> 
<p></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6f7b3748de74b9ea41b60e70c49a1924/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">docker常用命令汇总</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c7944e1e551c98af8ab1494c07fd3fd8/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">7-5 选择法排序 (只排三轮) (10 分)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>