<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>深度学习笔记（七）--ResNet（残差网络） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="深度学习笔记（七）--ResNet（残差网络）" />
<meta property="og:description" content="内容来自吴恩达老师视频，网易云课堂有哦 ResNets 非常非常深的神经网络是很难训练的，因为存在梯度消失和梯度爆炸问题。ResNets是由残差块（Residual block）构建的，首先解释一下什么是残差块。
这是一个两层神经网络，在 层进行激活，得到 ，再次进行激活，两层之后得到 。计算过程是从 开始，首先进行线性激活，根据这个公式： ，通过 算出 ，即 乘以权重矩阵，再加上偏差因子。然后通过ReLU非线性激活函数得到 ， 计算得出。接着再次进行线性激活，依据等式 ，最后根据这个等式再次进行ReLu非线性激活，即 ，这里的 是指ReLU非线性函数，得到的结果就是 。换句话说，信息流从 到 需要经过以上所有步骤，即这组网络层的主路径。 在残差网络中有一点变化，我们将 直接向后，拷贝到神经网络的深层，在ReLU非线性激活函数前加上 ，这是一条捷径。] 的信息直接到达神经网络的深层，不再沿着主路径传递，这就意味着最后这个等式 )去掉了，取而代之的是另一个ReLU非线性函数，仍然对 进行 函数处理，但这次要加上 ，即： ，也就是加上的这个 产生了一个残差块。 在上面这个图中，我们也可以画一条捷径，直达第二层。实际上这条捷径是在进行ReLU非线性激活函数之前加上的，而这里的每一个节点都执行了线性函数和ReLU激活函数。所以 插入的时机是在线性激活之后，ReLU激活之前。除了捷径，你还会听到另一个术语“跳跃连接”，就是指 跳过一层或者好几层，从而将信息传递到神经网络的更深层。
ResNet的发明者是何恺明（Kaiming He）、张翔宇（Xiangyu Zhang）、任少卿（Shaoqing Ren）和孙剑（Jiangxi Sun），他们发现使用残差块能够训练更深的神经网络。所以构建一个ResNet网络就是通过将很多这样的残差块堆积在一起，形成一个很深神经网络，我们来看看这个网络。
这并不是一个残差网络，而是一个普通网络（Plain network），这个术语来自ResNet论文。
把它变成ResNet的方法是加上所有跳跃连接，每两层增加一个捷径，构成一个残差块。如图所示，5个残差块连接在一起构成一个残差网络。
如果我们使用标准优化算法训练一个普通网络，比如说梯度下降法，或者其它热门的优化算法。如果没有残差，没有这些捷径或者跳跃连接，凭经验你会发现随着网络深度的加深，训练错误会先减少，然后增多。而理论上，随着网络深度的加深，应该训练得越来越好才对。也就是说，理论上网络深度越深越好。但实际上，如果没有残差网络，对于一个普通网络来说，深度越深意味着用优化算法越难训练。实际上，随着网络深度的加深，训练错误会越来越多。
但有了ResNets就不一样了，即使网络再深，训练的表现却不错，比如说训练误差减少，就算是训练深达100层的网络也不例外。有人甚至在1000多层的神经网络中做过实验，尽管目前我还没有看到太多实际应用。但是对 的激活，或者这些中间的激活能够到达网络的更深层。这种方式确实有助于解决梯度消失和梯度爆炸问题，让我们在训练更深网络的同时，又能保证良好的性能。也许从另外一个角度来看，随着网络越来深，网络连接会变得臃肿，但是ResNet确实在训练深度网络方面非常有效。 微信公众号：任冬学编程
tx工作的后端开发仔，分享后端技术、机器学习、数据结构与算法、计算机基础、程序员面试等话题。欢迎关注。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/0be58c28990c2f791b8f0d824f0c5af4/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-07-25T17:54:23+08:00" />
<meta property="article:modified_time" content="2018-07-25T17:54:23+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深度学习笔记（七）--ResNet（残差网络）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2 style="margin-left:0cm;"></h2> 
<h2 style="margin-left:0cm;">内容来自吴恩达老师视频，网易云课堂有哦</h2> 
<h2 style="margin-left:0cm;"><span style="color:#f33b45;"><strong>ResNets</strong></span></h2> 
<p style="margin-left:0cm;">非常非常深的神经网络是很难训练的，因为存在梯度消失和梯度爆炸问题。<strong>ResNets</strong>是由<span style="color:#f33b45;">残差块（<strong>Residual block</strong>）</span>构建的，首先解释一下什么是残差块。</p> 
<p style="margin-left:0cm;"><img alt="" class="has" height="256" src="https://images2.imgbox.com/92/24/UC19iBmk_o.png" width="560"></p> 
<p>这是一个两层神经网络，在<img alt="" class="has" height="25" src="https://images2.imgbox.com/9f/fd/UCxQdk4Y_o.png" width="8"> 层进行激活，得到<img alt="" class="has" height="26" src="https://images2.imgbox.com/95/0c/YFtTWPWt_o.png" width="33"> ，再次进行激活，两层之后得到<img alt="" class="has" height="26" src="https://images2.imgbox.com/2e/5c/DvewpCdE_o.png" width="33"> 。计算过程是从<img alt="" class="has" height="26" src="https://images2.imgbox.com/8e/f2/F39qnpDh_o.png" width="20"> 开始，首先进行线性激活，根据这个公式：<img alt="" class="has" height="26" src="https://images2.imgbox.com/9c/aa/hVpNXh6r_o.png" width="158"> ，通过<img alt="" class="has" height="26" src="https://images2.imgbox.com/3e/55/v3c8qAAy_o.png" width="20"> 算出<img alt="" class="has" height="26" src="https://images2.imgbox.com/69/ff/6unZkdh1_o.png" width="32"> ，即<img alt="" class="has" height="26" src="https://images2.imgbox.com/78/e2/nqE5EOwo_o.png" width="20"> 乘以权重矩阵，再加上偏差因子。然后通过<strong>ReLU</strong>非线性激活函数得到<img alt="" class="has" height="26" src="https://images2.imgbox.com/f2/89/TQigonPx_o.png" width="33"> ，<img alt="" class="has" height="26" src="https://images2.imgbox.com/56/38/KYhfKfpA_o.png" width="103"> 计算得出。接着再次进行线性激活，依据等式<img alt="" class="has" height="26" src="https://images2.imgbox.com/13/94/L218odci_o.png" width="173"> ，最后根据这个等式再次进行<strong>ReLu</strong>非线性激活，即<img alt="" class="has" height="26" src="https://images2.imgbox.com/b8/08/HsIbir0e_o.png" width="103"> ，这里的<img alt="" class="has" height="25" src="https://images2.imgbox.com/d6/b8/km1B161T_o.png" width="9"> 是指<strong>ReLU</strong>非线性函数，得到的结果就是<img alt="" class="has" height="26" src="https://images2.imgbox.com/73/73/tTFNm4T9_o.png" width="33"> 。换句话说，信息流从<img alt="" class="has" height="26" src="https://images2.imgbox.com/c3/80/Qv1wdn8l_o.png" width="20"> 到<img alt="" class="has" height="26" src="https://images2.imgbox.com/ae/2c/6n5B7Lp6_o.png" width="33"> 需要经过以上所有步骤，即这组网络层的主路径。 </p> 
<p><img alt="" class="has" height="277" src="https://images2.imgbox.com/d2/33/SvNAEy5X_o.png" width="560"></p> 
<p>在残差网络中有一点变化，我们将<img alt="" class="has" height="26" src="https://images2.imgbox.com/aa/c6/eV2NlidG_o.png" width="20"> 直接向后，拷贝到神经网络的深层，在<strong>ReLU</strong>非线性激活函数前加上<img alt="" class="has" height="26" src="https://images2.imgbox.com/8f/63/pgZTZCNB_o.png" width="20"> ，这是一条捷径。<em>]</em><img alt="" class="has" height="26" src="https://images2.imgbox.com/56/1c/ibPlNv9Z_o.png" width="20"> 的信息直接到达神经网络的深层，不再沿着主路径传递，这就意味着最后这个等式<img alt="" class="has" height="26" src="https://images2.imgbox.com/50/71/LoBHUyfq_o.png" width="103"> )去掉了，取而代之的是另一个<strong>ReLU</strong>非线性函数，仍然对<img alt="" class="has" height="26" src="https://images2.imgbox.com/50/fe/jbKHpYIy_o.png" width="32"> 进行<img alt="" class="has" height="25" src="https://images2.imgbox.com/d8/25/ujIQaeat_o.png" width="12"> 函数处理，但这次要加上<img alt="" class="has" height="26" src="https://images2.imgbox.com/4f/de/hjVNvQvp_o.png" width="20"> ，<span style="color:#f33b45;"><strong>即：<em> </em><img alt="" class="has" height="27" src="https://images2.imgbox.com/95/69/Wa0ZMLWw_o.png" width="144"></strong></span> ，<span style="color:#f33b45;">也就是加上的这个<img alt="" class="has" height="26" src="https://images2.imgbox.com/05/d6/l8tGuKCR_o.png" width="20"> 产生了一个残差块。 </span></p> 
<p><img alt="" class="has" height="279" src="https://images2.imgbox.com/65/3a/h6r9bgia_o.png" width="560"></p> 
<p style="margin-left:0cm;">在上面这个图中，我们也可以画一条捷径，直达第二层。实际上这条捷径是在进行<strong>ReLU</strong>非线性激活函数之前加上的，而这里的每一个节点都执行了线性函数和<strong>ReLU</strong>激活函数。所以<img alt="" class="has" height="26" src="https://images2.imgbox.com/e7/1f/9ph894ph_o.png" width="20"> 插入的时机是在线性激活之后，<strong>ReLU</strong>激活之前。除了捷径，你还会听到另一个术语<span style="color:#f33b45;">“跳跃连接”，就是指<img alt="" class="has" height="26" src="https://images2.imgbox.com/98/b0/kbyR7iaK_o.png" width="20"> 跳过一层或者好几层，从而将信息传递到神经网络的更深层</span>。</p> 
<p><strong>ResNet</strong>的发明者是<strong>何恺明</strong>（<strong>Kaiming He</strong>）、<strong>张翔宇</strong>（<strong>Xiangyu Zhang</strong>）、<strong>任少卿</strong>（<strong>Shaoqing Ren</strong>）和<strong>孙剑</strong>（<strong>Jiangxi Sun</strong>），他们发现使用残差块能够训练更深的神经网络。所以构建一个<strong>ResNet</strong>网络就是通过将很多这样的残差块堆积在一起，形成一个很深神经网络，我们来看看这个网络。</p> 
<p><img alt="" class="has" height="71" src="https://images2.imgbox.com/cb/13/DkmrnHf6_o.png" width="532"></p> 
<p>这并不是一个残差网络，而是一个<span style="color:#f33b45;">普通网络（<strong>Plain network</strong>）</span>，这个术语来自<strong>ResNet</strong>论文。</p> 
<p><img alt="" class="has" height="134" src="https://images2.imgbox.com/f4/81/1ANyITcQ_o.png" width="560"></p> 
<p> 把它变成<span style="color:#f33b45;"><strong>ResNet</strong>的方法是加上所有跳跃连接，每两层增加一个捷径，构成一个残差块。如图所示，5个残差块连接在一起构成一个残差网络。</span></p> 
<p> <img alt="" class="has" height="312" src="https://images2.imgbox.com/51/b5/KuUHm8Ny_o.png" width="560"></p> 
<p>       如果我们使用标准优化算法训练一个普通网络，比如说梯度下降法，或者其它热门的优化算法。如果没有残差，没有这些捷径或者跳跃连接，凭经验你会发现随着网络深度的加深，训练错误会先减少，然后增多。而理论上，随着网络深度的加深，应该训练得越来越好才对。也就是说，理论上网络深度越深越好。但实际上，如果没有残差网络，对于一个普通网络来说，深度越深意味着用优化算法越难训练。<span style="color:#f33b45;">实际上，随着网络深度的加深，训练错误会越来越多</span>。</p> 
<p>        但有了<strong>ResNets</strong>就不一样了，即使网络再深，训练的表现却不错，比如说训练误差减少，就算是训练深达100层的网络也不例外。有人甚至在1000多层的神经网络中做过实验，尽管目前我还没有看到太多实际应用。但是对<img alt="" class="has" height="25" src="https://images2.imgbox.com/1b/8f/bDY38byV_o.png" width="8"> 的激活，或者这些中间的激活能够到达网络的更深层。这种方式<span style="color:#f33b45;">确实有助于解决梯度消失和梯度爆炸问题，让我们在训练更深网络的同时，又能保证良好的性能</span>。也许从另外一个角度来看，随着网络越来深，网络连接会变得臃肿，但是<strong>ResNet</strong>确实在训练深度网络方面非常有效。 </p> 
<p></p> 
<p><span style="color:#fe2c24;"><strong>微信公众号：任冬学编程</strong></span><br> tx工作的后端开发仔，分享后端技术、机器学习、数据结构与算法、计算机基础、程序员面试等话题。欢迎关注。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a9be4cbd4ac8ce82c5afe90f9b784522/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【转】区块链是什么，如何简单易懂地介绍区块链？</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b59426796391206e7ba1edb584a7dcd5/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">android.view.InflateException: Binary XML file line #0: Error inflating class &lt;unknown&gt; 解决办法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>