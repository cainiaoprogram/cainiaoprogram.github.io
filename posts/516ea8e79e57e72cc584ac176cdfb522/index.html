<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Python 实现一个简单的神经网络（附代码） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Python 实现一个简单的神经网络（附代码）" />
<meta property="og:description" content="目录
⭐前言⭐
⭐砖块：神经元⭐
📌一个简单的例子📌
📌编码一个神经元 📌
📌把神经元组装成网络 📌
📌例子：前馈📌
📌编码神经网络：前馈📌
📌训练神经网络 第一部分📌
📌损失📌
📌损失计算例子📌
📌代码：MSE损失📌
📌训练神经网络 第二部分📌
📌例子：计算偏导数📌
📌代码：一个完整的神经网络📌
📌后话📌
⭐前言⭐ 以下内容我们用Python从头实现一个神经网络来理解神经网络的原理。
首先让我们看看神经网络的基本单位，神经元。神经元接受输入，对其做一些数据操作，然后产生输出。例如，这是一个2-输入神经元：
这里发生了三个事情。首先，每个输入都跟一个权重相乘（红色）：
然后，加权后的输入求和，加上一个偏差b（绿色）：
最后，这个结果传递给一个激活函数f：
激活函数的用途是将一个无边界的输入，转变成一个可预测的形式。常用的激活函数就就是S型函数：
S型函数的值域是(0, 1)。简单来说，就是把(−∞, &#43;∞)压缩到(0, 1) ，很大的负数约等于0，很大的正数约等于1。
📌一个简单的例子📌 假设我们有一个神经元，激活函数就是S型函数，其参数如下：
w=[0,1]，b=4
w=[0,1]就是以向量的形式表示w1=0，w2=1。现在，我们给这个神经元一个输入[2,3]。我们用点积来表示：
当输入是[2, 3]时，这个神经元的输出是0.999。给定输入，得到输出的过程被称为前馈（feedforward）。
📌编码一个神经元 📌 让我们来实现一个神经元！用Python的NumPy库来完成其中的数学计算：
import numpy as np def sigmoid(x): # 我们的激活函数: f(x) = 1 / (1 &#43; e^(-x)) return 1 / (1 &#43; np.exp(-x)) class Neuron: def __init__(self, weights, bias): self.weights = weights self." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/516ea8e79e57e72cc584ac176cdfb522/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-04T15:43:22+08:00" />
<meta property="article:modified_time" content="2022-07-04T15:43:22+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Python 实现一个简单的神经网络（附代码）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-dracula">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <hr> 
<p><strong>目录</strong></p> 
<p><a href="https://blog.csdn.net/weixin_43734080/article/details/122212654#t0">⭐前言⭐</a></p> 
<p><a href="https://blog.csdn.net/weixin_43734080/article/details/122212654#t1">⭐砖块：神经元⭐</a></p> 
<p><a href="https://blog.csdn.net/weixin_43734080/article/details/122212654#t2">📌一个简单的例子📌</a></p> 
<p><a href="https://blog.csdn.net/weixin_43734080/article/details/122212654#t3">📌编码一个神经元 📌</a></p> 
<p><a href="https://blog.csdn.net/weixin_43734080/article/details/122212654#t4">📌把神经元组装成网络 📌</a></p> 
<p><a href="https://blog.csdn.net/weixin_43734080/article/details/122212654#t5">📌例子：前馈📌</a></p> 
<p><a href="https://blog.csdn.net/weixin_43734080/article/details/122212654#t6">📌编码神经网络：前馈📌</a></p> 
<p><a href="https://blog.csdn.net/weixin_43734080/article/details/122212654#t7">📌训练神经网络 第一部分📌</a></p> 
<p><a href="https://blog.csdn.net/weixin_43734080/article/details/122212654#t8">📌损失📌</a></p> 
<p><a href="https://blog.csdn.net/weixin_43734080/article/details/122212654#t9">📌损失计算例子📌</a></p> 
<p><a href="https://blog.csdn.net/weixin_43734080/article/details/122212654#t10">📌代码：MSE损失📌</a></p> 
<p><a href="https://blog.csdn.net/weixin_43734080/article/details/122212654#t11">📌训练神经网络 第二部分📌</a></p> 
<p><a href="https://blog.csdn.net/weixin_43734080/article/details/122212654#t12">📌例子：计算偏导数📌</a></p> 
<p><a href="https://blog.csdn.net/weixin_43734080/article/details/122212654#t13">📌代码：一个完整的神经网络📌</a></p> 
<p><a href="https://blog.csdn.net/weixin_43734080/article/details/122212654#t14">📌后话📌</a></p> 
<hr> 
<h3><a id="_37"></a>⭐前言⭐</h3> 
<p>以下内容我们用Python从头实现一个<a href="https://so.csdn.net/so/search?q=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;spm=1001.2101.3001.7020">神经网络</a>来理解神经网络的原理。</p> 
<p>首先让我们看看神经网络的基本单位，<a href="https://so.csdn.net/so/search?q=%E7%A5%9E%E7%BB%8F%E5%85%83&amp;spm=1001.2101.3001.7020">神经元</a>。神经元接受输入，对其做一些数据操作，然后产生输出。例如，这是一个2-输入神经元：</p> 
<p><img src="https://images2.imgbox.com/9a/02/OOhvpVqP_o.png" alt="图片"></p> 
<p>这里发生了三个事情。首先，每个输入都跟一个权重相乘（红色）：</p> 
<p><img src="https://images2.imgbox.com/c6/3f/MZE2lqKW_o.png" alt="x_1\leftarrow x_1\times w_1"></p> 
<p><img src="https://images2.imgbox.com/76/11/Wf20cgaO_o.png" alt="x_2\leftarrow x_2\times w_2"></p> 
<p>然后，加权后的输入求和，加上一个偏差b（绿色）：</p> 
<p><img src="https://images2.imgbox.com/a4/3d/86hVsxXe_o.png" alt="(x_1\times w_1)+(x_2\times w_2)+b"></p> 
<p>最后，这个结果传递给一个激活函数f：</p> 
<p><img src="https://images2.imgbox.com/e2/60/R9p3sp8o_o.png" alt="y=f(x_1\times w_1+x_2\times w_2+b)"></p> 
<p>激活函数的用途是将一个无边界的输入，转变成一个可预测的形式。常用的激活函数就就是S型函数：</p> 
<p><img src="https://images2.imgbox.com/92/ef/4l1grvnQ_o.png" alt="图片"></p> 
<p>S型函数的值域是(0, 1)。简单来说，就是把(−∞, +∞)压缩到(0, 1) ，很大的负数约等于0，很大的正数约等于1。</p> 
<h3><a id="_65"></a>📌一个简单的例子📌</h3> 
<p>假设我们有一个神经元，激活函数就是S型函数，其参数如下：</p> 
<p>w=[0,1]，b=4</p> 
<p>w=[0,1]就是以向量的形式表示w1=0，w2=1。现在，我们给这个神经元一个输入[2,3]。我们用点积来表示：</p> 
<p><img src="https://images2.imgbox.com/35/46/xd5q0Jjr_o.png" alt="y=f(w\cdot x+b)=f(7)=0.999"></p> 
<p>当输入是[2, 3]时，这个神经元的输出是0.999。给定输入，得到输出的过程被称为前馈（feedforward）。</p> 
<h3><a id="__77"></a>📌编码一个神经元 📌</h3> 
<p>让我们来实现一个神经元！用Python的NumPy库来完成其中的数学计算：</p> 
<pre><code class="prism language-py"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
 
<span class="token keyword">def</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token comment"># 我们的激活函数: f(x) = 1 / (1 + e^(-x))</span>
  <span class="token keyword">return</span> <span class="token number">1</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
 
<span class="token keyword">class</span> <span class="token class-name">Neuron</span><span class="token punctuation">:</span>
  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> weights<span class="token punctuation">,</span> bias<span class="token punctuation">)</span><span class="token punctuation">:</span>
    self<span class="token punctuation">.</span>weights <span class="token operator">=</span> weights
    self<span class="token punctuation">.</span>bias <span class="token operator">=</span> bias
 
  <span class="token keyword">def</span> <span class="token function">feedforward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 加权输入，加入偏置，然后使用激活函数</span>
    total <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>self<span class="token punctuation">.</span>weights<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>bias
    <span class="token keyword">return</span> sigmoid<span class="token punctuation">(</span>total<span class="token punctuation">)</span>
 
weights <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># w1 = 0, w2 = 1</span>
bias <span class="token operator">=</span> <span class="token number">4</span>                   <span class="token comment"># b = 4</span>
n <span class="token operator">=</span> Neuron<span class="token punctuation">(</span>weights<span class="token punctuation">,</span> bias<span class="token punctuation">)</span>
 
x <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>       <span class="token comment"># x1 = 2, x2 = 3</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>n<span class="token punctuation">.</span>feedforward<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment"># 0.9990889488055994</span>

</code></pre> 
<h3><a id="__107"></a>📌把神经元组装成网络 📌</h3> 
<p>所谓的神经网络就是一堆神经元。这就是一个简单的神经网络：</p> 
<p><img src="https://images2.imgbox.com/40/3a/PiFDTgwe_o.png" alt="图片"></p> 
<p>这个网络有两个输入，一个有两个神经元（ <img src="https://images2.imgbox.com/6f/fb/wsIXbO9V_o.png" alt="h_1">和 <img src="https://images2.imgbox.com/97/64/LUSOBBjT_o.png" alt="h_2">）的隐藏层，以及一个有一个神经元（<img src="https://images2.imgbox.com/2e/a8/fGnKpt4q_o.png" alt="o_1"> ) ）的输出层。要注意，<img src="https://images2.imgbox.com/7b/1b/8q9eV1ds_o.png" alt="o_1">的输入就是<img src="https://images2.imgbox.com/07/d5/v4oYjA28_o.png" alt="h_1">和<img src="https://images2.imgbox.com/66/a0/iLfMsUYt_o.png" alt="h_2">的输出，这样就组成了一个网络。</p> 
<blockquote> 
 <p>隐藏层就是输入层和输出层之间的层，隐藏层可以是多层的。</p> 
</blockquote> 
<h3><a id="_117"></a>📌例子：前馈📌</h3> 
<p>我们继续用前面图中的网络，假设每个神经元的权重都是 ，截距项也相同 ，激活函数也都是S型函数。分别用 表示相应的神经元的输出。</p> 
<p>当输入 x=[2,3]时，会得到什么结果？</p> 
<p>这个神经网络对输入[2,3]的输出是0.7216，很简单。</p> 
<h3><a id="_125"></a>📌编码神经网络：前馈📌</h3> 
<p><img src="https://images2.imgbox.com/ed/44/2wQCZhGu_o.png" alt="图片"></p> 
<pre><code class="prism language-py"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
 
<span class="token comment"># ... code from previous section here</span>
 
<span class="token keyword">class</span> <span class="token class-name">OurNeuralNetwork</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">'''
  A neural network with:
    - 2 inputs
    - a hidden layer with 2 neurons (h1, h2)
    - an output layer with 1 neuron (o1)
  Each neuron has the same weights and bias:
    - w = [0, 1]
    - b = 0
  '''</span>
  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    weights <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    bias <span class="token operator">=</span> <span class="token number">0</span>
 
    <span class="token comment"># 这里是来自前一节的神经元类</span>
    self<span class="token punctuation">.</span>h1 <span class="token operator">=</span> Neuron<span class="token punctuation">(</span>weights<span class="token punctuation">,</span> bias<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>h2 <span class="token operator">=</span> Neuron<span class="token punctuation">(</span>weights<span class="token punctuation">,</span> bias<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>o1 <span class="token operator">=</span> Neuron<span class="token punctuation">(</span>weights<span class="token punctuation">,</span> bias<span class="token punctuation">)</span>
 
  <span class="token keyword">def</span> <span class="token function">feedforward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    out_h1 <span class="token operator">=</span> self<span class="token punctuation">.</span>h1<span class="token punctuation">.</span>feedforward<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    out_h2 <span class="token operator">=</span> self<span class="token punctuation">.</span>h2<span class="token punctuation">.</span>feedforward<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
 
    <span class="token comment"># o1的输入是h1和h2的输出</span>
    out_o1 <span class="token operator">=</span> self<span class="token punctuation">.</span>o1<span class="token punctuation">.</span>feedforward<span class="token punctuation">(</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>out_h1<span class="token punctuation">,</span> out_h2<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
 
    <span class="token keyword">return</span> out_o1
 
network <span class="token operator">=</span> OurNeuralNetwork<span class="token punctuation">(</span><span class="token punctuation">)</span>
x <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>network<span class="token punctuation">.</span>feedforward<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 0.7216325609518421</span>
</code></pre> 
<p>结果正确，看上去没问题。</p> 
<h3><a id="__169"></a>📌训练神经网络 第一部分📌</h3> 
<p>现在有这样的数据：</p> 
<table><thead><tr><th>姓名</th><th>体重(磅)</th><th>身高 (英寸)</th><th>性别</th></tr></thead><tbody><tr><td>Alice</td><td>133</td><td>65</td><td>F</td></tr><tr><td>Bob</td><td>160</td><td>72</td><td>M</td></tr><tr><td>Charlie</td><td>152</td><td>70</td><td>M</td></tr><tr><td>Diana</td><td>120</td><td>60</td><td>F</td></tr></tbody></table> 
<p>接下来我们用这个数据来训练神经网络的权重和截距项，从而可以根据身高体重预测性别：</p> 
<p><img src="https://images2.imgbox.com/ae/88/uucj2ppz_o.png" alt="图片"></p> 
<p>我们用0和1分别表示男性（M）和女性（F），并对数值做了转化：</p> 
<table><thead><tr><th>姓名</th><th>体重 (减 135)</th><th>身高 (减 66)</th><th>性别</th></tr></thead><tbody><tr><td>Alice</td><td>-2</td><td>-1</td><td>1</td></tr><tr><td>Bob</td><td>25</td><td>6</td><td>0</td></tr><tr><td>Charlie</td><td>17</td><td>4</td><td>0</td></tr><tr><td>Diana</td><td>-15</td><td>-6</td><td>1</td></tr></tbody></table> 
<blockquote> 
 <p>我这里是随意选取了135和66来标准化数据，通常会使用平均值。</p> 
</blockquote> 
<h3><a id="_195"></a>📌损失📌</h3> 
<p>在训练网络之前，我们需要量化当前的网络是『好』还是『坏』，从而可以寻找更好的网络。这就是定义损失的目的。</p> 
<p>我们在这里用平均方差（MSE）损失： ，让我们仔细看看：</p> 
<ul><li> <p>n是样品数，这里等于4（Alice、Bob、Charlie和Diana）。</p> </li><li> <p>y表示要预测的变量，这里是性别。</p> </li><li> <p><img src="https://images2.imgbox.com/0f/c8/yiwCAaxV_o.png" alt="y_{true}">是变量的真实值（『正确答案』）。例如，Alice的<img src="https://images2.imgbox.com/1a/76/lc8N5QFj_o.png" alt="y_{true}">就是1（男性)。</p> </li><li> <p><img src="https://images2.imgbox.com/76/e2/R8UzDRpp_o.png" alt="y_{pred}">变量的预测值。这就是我们网络的输出。</p> </li></ul> 
<p><img src="https://images2.imgbox.com/7a/14/IoDWoggv_o.png" alt="(y_{true}-y_{pred})^{2}">被称为方差（squared error）。我们的损失函数就是所有方差的平均值。预测效果于浩，损失就越少。</p> 
<p>更好的预测 = 更少的损失！</p> 
<p>训练网络 = 最小化它的损失。</p> 
<h3><a id="_216"></a>📌损失计算例子📌</h3> 
<p>假设我们的网络总是输出0，换言之就是认为所有人都是男性。损失如何？</p> 
<table><thead><tr><th>Name</th><th>y_true</th><th>y_pred</th><th>(y_true - y_pred)^2</th></tr></thead><tbody><tr><td>Alice</td><td>1</td><td>0</td><td>1</td></tr><tr><td>Bob</td><td>0</td><td>0</td><td>0</td></tr><tr><td>Charlie</td><td>0</td><td>0</td><td>0</td></tr><tr><td>Diana</td><td>1</td><td>0</td><td>1</td></tr></tbody></table> 
<p><img src="https://images2.imgbox.com/f2/d1/XhUud9jw_o.png" alt="MSE=\frac{1}{4}(1+0+0+1)=0.5"></p> 
<h3><a id="MSE_229"></a>📌代码：MSE损失📌</h3> 
<p>下面是计算MSE损失的代码：</p> 
<pre><code class="prism language-py"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
 
<span class="token keyword">def</span> <span class="token function">mse_loss</span><span class="token punctuation">(</span>y_true<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token comment"># y_true and y_pred are numpy arrays of the same length.</span>
  <span class="token keyword">return</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>y_true <span class="token operator">-</span> y_pred<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
 
y_true <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
y_pred <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
 
<span class="token keyword">print</span><span class="token punctuation">(</span>mse_loss<span class="token punctuation">(</span>y_true<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 0.5</span>

</code></pre> 
<blockquote> 
 <p>如果你不理解这段代码，可以看看NumPy的快速入门中关于数组的操作。</p> 
</blockquote> 
<h3><a id="__249"></a>📌训练神经网络 第二部分📌</h3> 
<p>现在我们有了一个明确的目标：<strong>最小化神经网络的损失</strong>。通过调整网络的权重和截距项，我们可以改变其预测结果，但如何才能逐步地减少损失？</p> 
<blockquote> 
 <p>这一段内容涉及到多元微积分，如果不熟悉微积分的话，可以跳过这些数学内容。</p> 
</blockquote> 
<p>这一段内容涉及到多元微积分，如果不熟悉微积分的话，可以跳过这些数学内容。</p> 
<p>为了简化问题，假设数据集中只有Alice，那均方差损失就只是Alice的方差：</p> 
<p><img src="https://images2.imgbox.com/d6/d5/PtS3Vjxh_o.png" alt="MSE=\frac{1}{1}\sum_{i=1}{1}(y_{true}-y_{pred}){2}=1"></p> 
<p>也可以把损失看成是权重和截距项的函数。让我们给网络标上权重和截距项：</p> 
<p><img src="https://images2.imgbox.com/39/73/921IHGth_o.png" alt="图片"></p> 
<p>这样我们就可以把网络的损失表示为：</p> 
<p><img src="https://images2.imgbox.com/f3/f4/wVYmn7z8_o.png" alt="L(w_1,w_2,w_3,w_4,w_5,w_6,b_1,b_2,b_3)"></p> 
<p>假设我们要优化 <img src="https://images2.imgbox.com/ae/a1/e3yyXXlO_o.png" alt="w_1">，当我们改变 <img src="https://images2.imgbox.com/e1/50/0h2aDeb1_o.png" alt="w_1">时，损失<img src="https://images2.imgbox.com/fd/ce/GBXcUczX_o.png" alt="L">会怎么变化？可以用<img src="https://images2.imgbox.com/11/81/eAn7luYB_o.png" alt="\frac{\partial L}{\partial w_1}">来回答这个问题，怎么计算？</p> 
<p>首先，让我们用<img src="https://images2.imgbox.com/9a/92/xTCZvBN6_o.png" alt="\frac{\partial y_{pred}}{\partial w_1}">来改写这个<a href="https://so.csdn.net/so/search?q=%E5%81%8F%E5%AF%BC%E6%95%B0&amp;spm=1001.2101.3001.7020">偏导数</a>：</p> 
<p><img src="https://images2.imgbox.com/ac/4d/8hQJAv3F_o.png" alt="\frac{\partial L}{\partial w_1}=\frac{\partial L}{\partial y_{pred}}\ast \frac{\partial y_{pred}}{\partial w_1}"></p> 
<p>因为我们已经知道 <img src="https://images2.imgbox.com/7d/d7/DljiUsTE_o.png" alt="L=(1-y_{pred})^{2}"> ，所以我们可以计算 <img src="https://images2.imgbox.com/1d/c9/XFsNFN4k_o.png" alt="\frac{\partial L}{\partial y_{pred}}"></p> 
<p><img src="https://images2.imgbox.com/54/94/3RrOEaiC_o.png" alt="\frac{\partial L}{\partial y_{pred}}=\frac{\partial (1-y_{pred})^{2}}{\partial y_{pred}}=-2(1-y_{pred})"></p> 
<p>现在让我们来搞定 <img src="https://images2.imgbox.com/6e/7e/uCe72C5c_o.png" alt="\frac{\partial y_{pred}}{\partial w_1}"> 。<img src="https://images2.imgbox.com/e0/5a/LyuY5cK9_o.png" alt="h_1,h_2,o_1">分别是其所表示的神经元的输出，我们有：</p> 
<p><img src="https://images2.imgbox.com/3a/9c/NgrdR4zi_o.png" alt="y_{pred}=o_1=f(w_5h_1+w_6h_2)+b_3"></p> 
<p>由于 <img src="https://images2.imgbox.com/da/73/PqOWfVx2_o.png" alt="w_1"> 只会影响 <img src="https://images2.imgbox.com/22/68/ebTLbhUS_o.png" alt="h_1"> （不会影响 <img src="https://images2.imgbox.com/ea/fc/JXmpcJsG_o.png" alt="h_2">)，所以：</p> 
<p><img src="https://images2.imgbox.com/14/7b/QWeDMGIk_o.png" alt="\frac{\partial y_{pred}}{\partial w_1}=\frac{\partial y_{pred}}{\partial h_1}\ast \frac{\partial h_1}{\partial w_1}"></p> 
<p><img src="https://images2.imgbox.com/33/ca/Rh3TNHbb_o.png" alt="\frac{\partial y_{pred}}{\partial h_1}=w_5\ast f'{(w_5h_1+w_6h_2+b_3)}"></p> 
<p>对 <img src="https://images2.imgbox.com/ad/32/ru4OneNa_o.png" alt="\frac{\partial h_1}{\partial w_1}"> ，我们也可以这么做：</p> 
<p><img src="https://images2.imgbox.com/16/3d/gUeb02Sp_o.png" alt="h_1=f(w_1x_1+w_2h_2+b_1)"></p> 
<p><img src="https://images2.imgbox.com/76/a6/7TpomHyA_o.png" alt="\frac{\partial h_1}{\partial w_1}=x_1\ast f'({w_1x_1+w_2x_2+b_1})"></p> 
<p>在这里，<img src="https://images2.imgbox.com/d3/53/gBYySuB6_o.png" alt="x_1"> 是身高，<img src="https://images2.imgbox.com/05/7d/S1c64ZIh_o.png" alt="x_2"> 是体重。这是我们第二次看到 <img src="https://images2.imgbox.com/91/fa/gISXDVBA_o.png" alt="f{}'(x)"> （S型函数的导数)了。求解：</p> 
<p><img src="https://images2.imgbox.com/d9/b4/k8lSJs0m_o.png" alt="f(x)=\frac{1}{1-e^{-x}}"></p> 
<p><img src="https://images2.imgbox.com/a1/e3/igAkHZy9_o.png" alt="f{}'(x)=\frac{e{-x}}{(1-e{-x})^{2}}=f(x)\ast (1-f(x))"></p> 
<p>我们已经把 <img src="https://images2.imgbox.com/52/b8/fDIRSEs6_o.png" alt="\frac{\partial L}{\partial w_1}">分解成了几个我们能计算的部分：</p> 
<p><img src="https://images2.imgbox.com/0b/07/xLDFaUVV_o.png" alt="\frac{\partial L}{\partial w_1}=\frac{\partial L}{\partial y_{pred}}\frac{\partial y_{pred}}{\partial h_1}\frac{\partial h_1}{\partial w_1}"></p> 
<p>这种计算偏导的方法叫『反向传播算法』(backpropagation)。</p> 
<p>好多数学符号，如果你还没搞明白的话，我们来看一个实际例子。</p> 
<h3><a id="_309"></a>📌例子：计算偏导数📌</h3> 
<p>我们还是看数据集中只有Alice的情况：</p> 
<p>把所有的权重和截距项都分别初始化为1和0。在网络中做前馈计算：</p> 
<p><img src="https://images2.imgbox.com/e3/38/onTjvd3Z_o.png" alt="h_1=f(w_1x_1+w_2x_2+b_1)=f(-2+-1+0)=0.0474"></p> 
<p><img src="https://images2.imgbox.com/2d/32/aiDHRLIf_o.png" alt="h_1=f(w_3x_1+w_4x_2+b_2)=f(-2+-1+0)=0.0474"></p> 
<p><img src="https://images2.imgbox.com/e0/1d/kwJ2zQkd_o.png" alt="o_1=f(w_5h_1+w_6h_2+b_3)=f(0.0474+0.0474+0)=0.524"></p> 
<p>网络的输出是 <img src="https://images2.imgbox.com/aa/95/3w3LVI7y_o.png" alt="pred=0.524"> ，对于Male(0)或者Female(1)都没有太强的倾向性。算一下 <img src="https://images2.imgbox.com/9c/b1/2FTZrHLs_o.png" alt="\frac{\partial L}{\partial w_1}"></p> 
<p><img src="https://images2.imgbox.com/6a/f5/4RgtNDjV_o.png" alt="\frac{\partial L}{\partial w_1}=\frac{\partial L}{\partial y_{pred}}\frac{\partial y_{pred}}{\partial h_1}\frac{\partial h_1}{\partial w_1}"></p> 
<p><img src="https://images2.imgbox.com/d4/dd/9uW0MzdZ_o.png" alt="\frac{\partial L}{\partial y_{pred}}=\frac{\partial (1-y_{pred})^{2}}{\partial y_{pred}}=-2(1-y_{pred})=-2(1-0.524)=-0.952"></p> 
<p><img src="https://images2.imgbox.com/e7/15/SdVyoNpN_o.png" alt="\frac{\partial y_{pred}}{\partial h_1}=1\ast f'{(w_5h_1+w_6h_2+b_3)}=w_5\ast f'{(0.0474+0.0474+0)}"></p> 
<p><img src="https://images2.imgbox.com/d1/9e/vRhQNBqu_o.png" alt="=f(0.0948)f(1-f(0.0948))=0.249"></p> 
<p><img src="https://images2.imgbox.com/b7/5d/NHlbIunG_o.png" alt="\frac{\partial h_1}{\partial w_1}=x_1\ast f'({w_1x_1+w_2x_2+b_1})=-2f{}'({-2+-1+0})"></p> 
<p><img src="https://images2.imgbox.com/85/85/7E0OfGHt_o.png" alt="=-2f(-3)(1-f(-3))=-0.0904"></p> 
<p><img src="https://images2.imgbox.com/32/e5/UNSQsSh5_o.png" alt="\frac{\partial L}{\partial w_1}=-0.952\ast 0.249\ast (-0.0904)=0.0214"></p> 
<blockquote> 
 <p>提示：前面已经得到了S型激活函数的导数 <img src="https://images2.imgbox.com/6c/29/lr8ChNRd_o.png" alt="f{}'(x)=f(x)\cdot (1-f(x))"> 。</p> 
</blockquote> 
<p>搞定！这个结果的意思就是增加<img src="https://images2.imgbox.com/cc/a6/bTtGWRnC_o.png" alt="w_1">，<img src="https://images2.imgbox.com/11/bf/ONR67ra2_o.png" alt="L">也会随之轻微上升。</p> 
<p>现在训练神经网络已经万事俱备了！我们会使用名为随机梯度下降法的优化算法来优化网络的权重和截距项，实现损失的最小化。核心就是这个更新公式：</p> 
<p><img src="https://images2.imgbox.com/fd/01/81FoH6UL_o.png" alt="w_1\leftarrow w_1-\eta \frac{\partial L}{\partial w_1}"></p> 
<p><img src="https://images2.imgbox.com/17/58/4AttTvxP_o.png" alt="\eta">是一个常数，被称为学习率，用于调整训练的速度。我们要做的就是用 <img src="https://images2.imgbox.com/af/3f/obzFNBI5_o.png" alt="w_1">减去<img src="https://images2.imgbox.com/a5/58/Q6PHkZiA_o.png" alt="\eta \frac{\partial L}{\partial w_1}"></p> 
<p>我们的训练过程是这样的：</p> 
<ol><li> <p>从我们的数据集中选择一个样本，用随机梯度下降法进行优化——每次我们都只针对一个样本进行优化；</p> </li><li> <p>计算每个权重或截距项对损失的偏导（例如<img src="https://images2.imgbox.com/aa/49/Qg8kbAfA_o.png" alt="\frac{\partial L}{\partial w_1}"> 、<img src="https://images2.imgbox.com/a2/47/6QfJMah8_o.png" alt="\frac{\partial L}{\partial w_2}">等)；</p> </li><li> <p>用更新等式更新每个权重和截距项；</p> </li><li> <p>重复第一步；</p> </li></ol> 
<h3><a id="_358"></a>📌代码：一个完整的神经网络📌</h3> 
<p>我们终于可以实现一个完整的神经网络了：</p> 
<table><thead><tr><th>姓名</th><th>身高 (减 135)</th><th>体重 (减 66)</th><th>Gender</th></tr></thead><tbody><tr><td>Alice</td><td>-2</td><td>-1</td><td>1</td></tr><tr><td>Bob</td><td>25</td><td>6</td><td>0</td></tr><tr><td>Charlie</td><td>17</td><td>4</td><td>0</td></tr><tr><td>Diana</td><td>-15</td><td>-6</td><td>1</td></tr></tbody></table> 
<p><img src="https://images2.imgbox.com/23/a6/vMb9V75b_o.png" alt="图片"></p> 
<p>总体代码</p> 
<pre><code class="prism language-py"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
 
<span class="token keyword">def</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token comment"># Sigmoid activation function: f(x) = 1 / (1 + e^(-x))</span>
  <span class="token keyword">return</span> <span class="token number">1</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
 
<span class="token keyword">def</span> <span class="token function">deriv_sigmoid</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token comment"># Derivative of sigmoid: f'(x) = f(x) * (1 - f(x))</span>
  fx <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
  <span class="token keyword">return</span> fx <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> fx<span class="token punctuation">)</span>
 
<span class="token keyword">def</span> <span class="token function">mse_loss</span><span class="token punctuation">(</span>y_true<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token comment"># y_true和y_pred是相同长度的numpy数组。</span>
  <span class="token keyword">return</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>y_true <span class="token operator">-</span> y_pred<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
 
<span class="token keyword">class</span> <span class="token class-name">OurNeuralNetwork</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">'''
  A neural network with:
    - 2 inputs
    - a hidden layer with 2 neurons (h1, h2)
    - an output layer with 1 neuron (o1)
  *** 免责声明 ***:
    下面的代码是为了简单和演示，而不是最佳的。
    真正的神经网络代码与此完全不同。不要使用此代码。
    相反，读/运行它来理解这个特定的网络是如何工作的。
  '''</span>
  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 权重，Weights</span>
    self<span class="token punctuation">.</span>w1 <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>w2 <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>w3 <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>w4 <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>w5 <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>w6 <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token punctuation">)</span>
 
    <span class="token comment"># 截距项，Biases</span>
    self<span class="token punctuation">.</span>b1 <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>b2 <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>b3 <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token punctuation">)</span>
 
  <span class="token keyword">def</span> <span class="token function">feedforward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># X是一个有2个元素的数字数组。</span>
    h1 <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>self<span class="token punctuation">.</span>w1 <span class="token operator">*</span> x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>w2 <span class="token operator">*</span> x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>b1<span class="token punctuation">)</span>
    h2 <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>self<span class="token punctuation">.</span>w3 <span class="token operator">*</span> x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>w4 <span class="token operator">*</span> x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>b2<span class="token punctuation">)</span>
    o1 <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>self<span class="token punctuation">.</span>w5 <span class="token operator">*</span> h1 <span class="token operator">+</span> self<span class="token punctuation">.</span>w6 <span class="token operator">*</span> h2 <span class="token operator">+</span> self<span class="token punctuation">.</span>b3<span class="token punctuation">)</span>
    <span class="token keyword">return</span> o1
 
  <span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data<span class="token punctuation">,</span> all_y_trues<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">'''
    - data is a (n x 2) numpy array, n = # of samples in the dataset.
    - all_y_trues is a numpy array with n elements.
      Elements in all_y_trues correspond to those in data.
    '''</span>
    learn_rate <span class="token operator">=</span> <span class="token number">0.1</span>
    epochs <span class="token operator">=</span> <span class="token number">1000</span> <span class="token comment"># 遍历整个数据集的次数</span>
 
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
      <span class="token keyword">for</span> x<span class="token punctuation">,</span> y_true <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>data<span class="token punctuation">,</span> all_y_trues<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># --- 做一个前馈(稍后我们将需要这些值)</span>
        sum_h1 <span class="token operator">=</span> self<span class="token punctuation">.</span>w1 <span class="token operator">*</span> x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>w2 <span class="token operator">*</span> x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>b1
        h1 <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>sum_h1<span class="token punctuation">)</span>
 
        sum_h2 <span class="token operator">=</span> self<span class="token punctuation">.</span>w3 <span class="token operator">*</span> x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>w4 <span class="token operator">*</span> x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>b2
        h2 <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>sum_h2<span class="token punctuation">)</span>
 
        sum_o1 <span class="token operator">=</span> self<span class="token punctuation">.</span>w5 <span class="token operator">*</span> h1 <span class="token operator">+</span> self<span class="token punctuation">.</span>w6 <span class="token operator">*</span> h2 <span class="token operator">+</span> self<span class="token punctuation">.</span>b3
        o1 <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>sum_o1<span class="token punctuation">)</span>
        y_pred <span class="token operator">=</span> o1
 
        <span class="token comment"># --- 计算偏导数。</span>
        <span class="token comment"># --- Naming: d_L_d_w1 represents "partial L / partial w1"</span>
        d_L_d_ypred <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">2</span> <span class="token operator">*</span> <span class="token punctuation">(</span>y_true <span class="token operator">-</span> y_pred<span class="token punctuation">)</span>
 
        <span class="token comment"># Neuron o1</span>
        d_ypred_d_w5 <span class="token operator">=</span> h1 <span class="token operator">*</span> deriv_sigmoid<span class="token punctuation">(</span>sum_o1<span class="token punctuation">)</span>
        d_ypred_d_w6 <span class="token operator">=</span> h2 <span class="token operator">*</span> deriv_sigmoid<span class="token punctuation">(</span>sum_o1<span class="token punctuation">)</span>
        d_ypred_d_b3 <span class="token operator">=</span> deriv_sigmoid<span class="token punctuation">(</span>sum_o1<span class="token punctuation">)</span>
 
        d_ypred_d_h1 <span class="token operator">=</span> self<span class="token punctuation">.</span>w5 <span class="token operator">*</span> deriv_sigmoid<span class="token punctuation">(</span>sum_o1<span class="token punctuation">)</span>
        d_ypred_d_h2 <span class="token operator">=</span> self<span class="token punctuation">.</span>w6 <span class="token operator">*</span> deriv_sigmoid<span class="token punctuation">(</span>sum_o1<span class="token punctuation">)</span>
 
        <span class="token comment"># Neuron h1</span>
        d_h1_d_w1 <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> deriv_sigmoid<span class="token punctuation">(</span>sum_h1<span class="token punctuation">)</span>
        d_h1_d_w2 <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> deriv_sigmoid<span class="token punctuation">(</span>sum_h1<span class="token punctuation">)</span>
        d_h1_d_b1 <span class="token operator">=</span> deriv_sigmoid<span class="token punctuation">(</span>sum_h1<span class="token punctuation">)</span>
 
        <span class="token comment"># Neuron h2</span>
        d_h2_d_w3 <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> deriv_sigmoid<span class="token punctuation">(</span>sum_h2<span class="token punctuation">)</span>
        d_h2_d_w4 <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> deriv_sigmoid<span class="token punctuation">(</span>sum_h2<span class="token punctuation">)</span>
        d_h2_d_b2 <span class="token operator">=</span> deriv_sigmoid<span class="token punctuation">(</span>sum_h2<span class="token punctuation">)</span>
 
        <span class="token comment"># --- 更新权重和偏差</span>
        <span class="token comment"># Neuron h1</span>
        self<span class="token punctuation">.</span>w1 <span class="token operator">-=</span> learn_rate <span class="token operator">*</span> d_L_d_ypred <span class="token operator">*</span> d_ypred_d_h1 <span class="token operator">*</span> d_h1_d_w1
        self<span class="token punctuation">.</span>w2 <span class="token operator">-=</span> learn_rate <span class="token operator">*</span> d_L_d_ypred <span class="token operator">*</span> d_ypred_d_h1 <span class="token operator">*</span> d_h1_d_w2
        self<span class="token punctuation">.</span>b1 <span class="token operator">-=</span> learn_rate <span class="token operator">*</span> d_L_d_ypred <span class="token operator">*</span> d_ypred_d_h1 <span class="token operator">*</span> d_h1_d_b1
 
        <span class="token comment"># Neuron h2</span>
        self<span class="token punctuation">.</span>w3 <span class="token operator">-=</span> learn_rate <span class="token operator">*</span> d_L_d_ypred <span class="token operator">*</span> d_ypred_d_h2 <span class="token operator">*</span> d_h2_d_w3
        self<span class="token punctuation">.</span>w4 <span class="token operator">-=</span> learn_rate <span class="token operator">*</span> d_L_d_ypred <span class="token operator">*</span> d_ypred_d_h2 <span class="token operator">*</span> d_h2_d_w4
        self<span class="token punctuation">.</span>b2 <span class="token operator">-=</span> learn_rate <span class="token operator">*</span> d_L_d_ypred <span class="token operator">*</span> d_ypred_d_h2 <span class="token operator">*</span> d_h2_d_b2
 
        <span class="token comment"># Neuron o1</span>
        self<span class="token punctuation">.</span>w5 <span class="token operator">-=</span> learn_rate <span class="token operator">*</span> d_L_d_ypred <span class="token operator">*</span> d_ypred_d_w5
        self<span class="token punctuation">.</span>w6 <span class="token operator">-=</span> learn_rate <span class="token operator">*</span> d_L_d_ypred <span class="token operator">*</span> d_ypred_d_w6
        self<span class="token punctuation">.</span>b3 <span class="token operator">-=</span> learn_rate <span class="token operator">*</span> d_L_d_ypred <span class="token operator">*</span> d_ypred_d_b3
 
      <span class="token comment"># --- 在每次epoch结束时计算总损失 </span>
      <span class="token keyword">if</span> epoch <span class="token operator">%</span> <span class="token number">10</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        y_preds <span class="token operator">=</span> np<span class="token punctuation">.</span>apply_along_axis<span class="token punctuation">(</span>self<span class="token punctuation">.</span>feedforward<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> data<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> mse_loss<span class="token punctuation">(</span>all_y_trues<span class="token punctuation">,</span> y_preds<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Epoch %d loss: %.3f"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>epoch<span class="token punctuation">,</span> loss<span class="token punctuation">)</span><span class="token punctuation">)</span>
 
<span class="token comment"># 定义数据集</span>
data <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>
  <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  <span class="token comment"># Alice</span>
  <span class="token punctuation">[</span><span class="token number">25</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span>   <span class="token comment"># Bob</span>
  <span class="token punctuation">[</span><span class="token number">17</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>   <span class="token comment"># Charlie</span>
  <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">15</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token comment"># Diana</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span>
all_y_trues <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>
  <span class="token number">1</span><span class="token punctuation">,</span> <span class="token comment"># Alice</span>
  <span class="token number">0</span><span class="token punctuation">,</span> <span class="token comment"># Bob</span>
  <span class="token number">0</span><span class="token punctuation">,</span> <span class="token comment"># Charlie</span>
  <span class="token number">1</span><span class="token punctuation">,</span> <span class="token comment"># Diana</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span>
 
<span class="token comment"># 训练我们的神经网络!</span>
network <span class="token operator">=</span> OurNeuralNetwork<span class="token punctuation">(</span><span class="token punctuation">)</span>
network<span class="token punctuation">.</span>train<span class="token punctuation">(</span>data<span class="token punctuation">,</span> all_y_trues<span class="token punctuation">)</span>
</code></pre> 
<p>随着网络的学习，损失在稳步下降。</p> 
<p><img src="https://images2.imgbox.com/56/50/PlbxwVHD_o.png" alt="图片"></p> 
<p>现在我们可以用这个网络来预测性别了：</p> 
<pre><code class="prism language-py"><span class="token comment"># 做一些预测</span>
emily <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># 128 磅, 63 英寸</span>
frank <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 155 磅, 68 英寸</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Emily: %.3f"</span> <span class="token operator">%</span> network<span class="token punctuation">.</span>feedforward<span class="token punctuation">(</span>emily<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 0.951 - F</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Frank: %.3f"</span> <span class="token operator">%</span> network<span class="token punctuation">.</span>feedforward<span class="token punctuation">(</span>frank<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 0.039 - M</span>

</code></pre> 
<h3><a id="_521"></a>📌后话📌</h3> 
<p>搞定了一个简单的神经网络，快速回顾一下：</p> 
<ul><li> <p>介绍了神经网络的基本结构——神经元；</p> </li><li> <p>在神经元中使用S型激活函数；</p> </li><li> <p>神经网络就是连接在一起的神经元；</p> </li><li> <p>构建了一个数据集，输入（或特征）是体重和身高，输出（或标签）是性别；</p> </li><li> <p>学习了损失函数和均方差损失；</p> </li><li> <p>训练网络就是最小化其损失；</p> </li><li> <p>用反向传播方法计算偏导；</p> </li><li> <p>用随机梯度下降法训练网络；</p> </li></ul> 
<p>接下来你还可以：</p> 
<ul><li> <p>用机器学习库实现更大更好的神经网络，例如TensorFlow、Keras和PyTorch；</p> </li><li> <p>在浏览器中实现神经网络；</p> </li><li> <p>其他类型的激活函数；</p> </li><li> <p>其他类型的优化器；</p> </li><li> <p>学习卷积神经网络，这给计算机视觉领域带来了革命；</p> </li><li> <p>学习递归神经网络，常用语自然语言处理；</p> </li></ul> 
<p>附加一段代码</p> 
<pre><code class="prism language-py"><span class="token comment">#!/usr/bin/pyton3</span>

<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
 
 
<span class="token comment"># Activation Function</span>
<span class="token keyword">def</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token number">1</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
 
 
<span class="token comment"># Neural Unit</span>
<span class="token keyword">class</span> <span class="token class-name">Perceptron</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_size<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> initializer<span class="token punctuation">:</span> <span class="token builtin">list</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span> activation_func<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># W[0] is the bias</span>
        self<span class="token punctuation">.</span>input_size <span class="token operator">=</span> input_size
        <span class="token comment"># W is parameters of Perceptron</span>
        self<span class="token punctuation">.</span>n_W <span class="token operator">=</span> self<span class="token punctuation">.</span>input_size <span class="token operator">+</span> <span class="token number">1</span>
        self<span class="token punctuation">.</span>W <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span>low<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> high<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">,</span> size<span class="token operator">=</span>self<span class="token punctuation">.</span>n_W<span class="token punctuation">)</span>
        <span class="token comment"># X is the input vector of Perceptron</span>
        self<span class="token punctuation">.</span>X <span class="token operator">=</span> <span class="token boolean">None</span>
 
        self<span class="token punctuation">.</span>output <span class="token operator">=</span> <span class="token number">0.0</span>
        self<span class="token punctuation">.</span>delta_W <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>n_W<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>delta_X <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> input_size<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>activation_func <span class="token operator">=</span> activation_func
        <span class="token keyword">if</span> initializer<span class="token punctuation">:</span>
            <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>initializer<span class="token punctuation">)</span> <span class="token operator">==</span> self<span class="token punctuation">.</span>n_W
            self<span class="token punctuation">.</span>W <span class="token operator">=</span> initializer
 
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span> <span class="token operator">==</span> self<span class="token punctuation">.</span>input_size
        self<span class="token punctuation">.</span>X <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>X<span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token builtin">float</span><span class="token punctuation">)</span>
        y <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>W<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>X<span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>W<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>activation_func <span class="token operator">==</span> <span class="token string">'sigmoid'</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>output <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>y<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>output <span class="token operator">=</span> y
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>output
 
    <span class="token keyword">def</span> <span class="token function">update</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> lr<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>W <span class="token operator">=</span> self<span class="token punctuation">.</span>W <span class="token operator">+</span> lr <span class="token operator">*</span> self<span class="token punctuation">.</span>delta_W
        self<span class="token punctuation">.</span>delta_W <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>n_W<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>
 
    <span class="token keyword">def</span> <span class="token function">__call__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
 
 
<span class="token comment"># Neural Layer</span>
<span class="token keyword">class</span> <span class="token class-name">Layer</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_size<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> output_size<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> activation_func<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>input_size <span class="token operator">=</span> input_size
        self<span class="token punctuation">.</span>output_size <span class="token operator">=</span> output_size
        self<span class="token punctuation">.</span>net <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>
            <span class="token punctuation">[</span>Perceptron<span class="token punctuation">(</span>input_size<span class="token operator">=</span>input_size<span class="token punctuation">,</span> activation_func<span class="token operator">=</span>activation_func<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>output_size<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>activation_func <span class="token operator">=</span> activation_func
        self<span class="token punctuation">.</span>inputs <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> input_size<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>lr <span class="token operator">=</span> lr
        self<span class="token punctuation">.</span>outputs <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> output_size<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>
 
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>inputs <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>X<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>outputs <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>p<span class="token punctuation">(</span>X<span class="token punctuation">)</span> <span class="token keyword">for</span> p <span class="token keyword">in</span> self<span class="token punctuation">.</span>net<span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>outputs
 
    <span class="token keyword">def</span> <span class="token function">__call__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
 
    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> delta_outputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>delta_outputs<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>net<span class="token punctuation">)</span>
        <span class="token keyword">for</span> idx <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>output_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
            delta_output <span class="token operator">=</span> delta_outputs<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>
            p <span class="token operator">=</span> self<span class="token punctuation">.</span>net<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>
            o <span class="token operator">=</span> self<span class="token punctuation">.</span>outputs<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>
            <span class="token keyword">if</span> self<span class="token punctuation">.</span>activation_func <span class="token operator">==</span> <span class="token string">'sigmoid'</span><span class="token punctuation">:</span>
                <span class="token comment"># W0 is the bias</span>
                p<span class="token punctuation">.</span>delta_W <span class="token operator">=</span> delta_output <span class="token operator">*</span> o <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> o<span class="token punctuation">)</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token builtin">list</span><span class="token punctuation">(</span>p<span class="token punctuation">.</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># expand X for W_0</span>
                p<span class="token punctuation">.</span>delta_X <span class="token operator">=</span> delta_output <span class="token operator">*</span> o <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> o<span class="token punctuation">)</span> <span class="token operator">*</span> p<span class="token punctuation">.</span>W<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                <span class="token comment"># linear</span>
                p<span class="token punctuation">.</span>delta_W <span class="token operator">=</span> delta_output <span class="token operator">*</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token builtin">list</span><span class="token punctuation">(</span>p<span class="token punctuation">.</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span>
                <span class="token comment"># W0 is the bias</span>
                p<span class="token punctuation">.</span>delta_X <span class="token operator">=</span> delta_output <span class="token operator">*</span> p<span class="token punctuation">.</span>W<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
 
    <span class="token keyword">def</span> <span class="token function">update</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> p <span class="token keyword">in</span> self<span class="token punctuation">.</span>net<span class="token punctuation">:</span>
            p<span class="token punctuation">.</span>update<span class="token punctuation">(</span>self<span class="token punctuation">.</span>lr<span class="token punctuation">)</span>
 
 
<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    <span class="token comment"># standard version ============================</span>
    samples <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
               <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">25</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
               <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">17</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
               <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">15</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
 
    <span class="token comment"># training</span>
    layer1 <span class="token operator">=</span> Layer<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
    layer2 <span class="token operator">=</span> Layer<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> activation_func<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># iteration</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'iteration </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>i<span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
        text_X <span class="token operator">=</span> samples<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        text_y_d <span class="token operator">=</span> samples<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
        test_y <span class="token operator">=</span> layer2<span class="token punctuation">(</span>layer1<span class="token punctuation">(</span><span class="token punctuation">(</span>samples<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'X:</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>text_X<span class="token punctuation">}</span></span><span class="token string">, y_d:</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>text_y_d<span class="token punctuation">}</span></span><span class="token string">, y:</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>test_y<span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> X<span class="token punctuation">,</span> y_d <span class="token keyword">in</span> samples<span class="token punctuation">:</span>
            <span class="token comment"># forward</span>
            y <span class="token operator">=</span> layer2<span class="token punctuation">(</span>layer1<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
            <span class="token comment"># backward layer2 -&gt; layer1</span>
            err <span class="token operator">=</span> y_d <span class="token operator">-</span> y   <span class="token comment"># delta_outputs of layer 2</span>
            layer2<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">[</span>err<span class="token punctuation">]</span><span class="token punctuation">)</span>
 
            delta_outputs <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">]</span> <span class="token operator">*</span> layer2<span class="token punctuation">.</span>input_size<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>  <span class="token comment"># delta_outputs of layer 1</span>
            <span class="token keyword">for</span> p <span class="token keyword">in</span> layer2<span class="token punctuation">.</span>net<span class="token punctuation">:</span>
                delta_outputs <span class="token operator">+=</span> p<span class="token punctuation">.</span>delta_X
            layer1<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>delta_outputs<span class="token punctuation">)</span>
 
            <span class="token comment"># update gradient</span>
            layer2<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token punctuation">)</span>
            layer1<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token punctuation">)</span>
 
    <span class="token comment"># result</span>
    <span class="token keyword">for</span> X<span class="token punctuation">,</span> y_d <span class="token keyword">in</span> samples<span class="token punctuation">:</span>
        y <span class="token operator">=</span> layer2<span class="token punctuation">(</span>layer1<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'The final result: X:</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>X<span class="token punctuation">}</span></span><span class="token string">, y_d:</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>y_d<span class="token punctuation">}</span></span><span class="token string">, y:</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>y<span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
 
    <span class="token comment"># # simple version ============================</span>
    <span class="token comment"># samples = [[[0, 1], 1],</span>
    <span class="token comment">#            [[1, 0], -1]]</span>
    <span class="token comment"># # training</span>
    <span class="token comment"># layer1 = Layer(2, 1, activation_func='')</span>
    <span class="token comment"># for i in range(100):</span>
    <span class="token comment">#     # iteration</span>
    <span class="token comment">#     print(f'iteration {i}')</span>
    <span class="token comment">#     for X, y_d in samples:</span>
    <span class="token comment">#         # forward</span>
    <span class="token comment">#         y = layer1(X)[0]</span>
    <span class="token comment">#         # backward</span>
    <span class="token comment">#         err = y_d - y</span>
    <span class="token comment">#         layer1.backward([err])</span>
    <span class="token comment">#         print(f'W：{layer1.net[0].W}')</span>
    <span class="token comment">#         print(f'delta_W：{layer1.net[0].delta_W}')</span>
    <span class="token comment">#         layer1.update()</span>
    <span class="token comment">#         print(f'X:{X}, y_d:{y_d}, y:{y}')</span>
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/3786b082f265a193eb5c5e7b343049fa/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">ADC Driver test</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/4b8fc1469a4b9dab19a523cd4dbab141/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">docker 安装rocketmq服务</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>