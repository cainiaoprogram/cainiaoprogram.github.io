<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>GPT4限制被破解！ChatGPT实现超长文本处理的新方法 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="GPT4限制被破解！ChatGPT实现超长文本处理的新方法" />
<meta property="og:description" content="目录
前言
使用chat-gpt过程中有哪些痛点
1.无法理解人类情感和主观性 2.上下文丢失
3.约定被打断
那如何去解决这个痛点
Transformer（RMT）怎么去实现的
1.Transformer 模型
2.RMT模型
3.计算推理速率
4.渐进学习能力
总结
写到最后
大家好，我是AI大侠，AI领域的专业博主
前言 ChatGPT已经成为了一款备受欢迎的工具，它可以帮助用户解答问题、写代码、翻译，甚至可以通过它学习更多行业的知识。然而，博主在使用ChatGPT时会发现它还不够智能，有时候不能够完全理解用户的意思，答非所问，下面是博主在使用中遇到的痛点
使用chat-gpt过程中有哪些痛点 1.无法理解人类情感和主观性 尽管ChatGPT可以根据上下文理解用户的输入，但它仍然无法真正了解用户的意图，ChatGPT只能根据输入数据和算法进行分析和回答，无法真正理解人类的情感和主观性。这种局限性可能导致一些误解和问题。
2.上下文丢失 与ChatGPT进行对话时，它能够记住上下文，并在后续回答中考虑之前的内容。但是，博主在使用过程中经常会出现ChatGPT忘记之前的对话，这可能是由于单次请求中Token数量的限制或是ChatGPT会话长度的限制所导致的。
3.约定被打断 如果在会话中如果有很多其他的问答，ChatGPT可能会在继续下一步时忘记之前的约定，需要再次约定才会保持下去
那如何去解决这个痛点 这几个痛点我想使用过gpt的小伙伴都深有体会，那如何去解决这些问题呢。其实openAI已经给出了答案。
在发布gpt4的时候，最大的变化除了新数据模型的发布，还有一个重要的技术点更新：上下文token默认为8K 最长32k(约50页文本) 这代表可以可以处理更长的对话 以及 更深层次的语义分析。这也是gpt4更智能好用的原因。
但如果把这个token提升到200万个，那又会发生什么，
AI 模型使用的是非结构化文本，常用 Token 表示，以 GPT 模型为例，1000 个 Token 约等于 750 个英文单词
一篇在AI界热论的论文给出了答案，《Scaling Transformer to 1M tokens and beyond with RMT》它可以把Transformer 的 Token 上限扩展至 100 万，甚至更多。
Transformer（RMT）怎么去实现的 1.Transformer 模型 Transformer 是一种神经网络模型，是迄今为止最新和最强大的模型之一，常用于处理上下文学习语义含义。
我们来看看gpt4的上下文处理模型为什么只能达到8-32k，因为transformer 的可输入长度取决于内存大小，这意味着实现太长的token不现实，Transformer 存在一个关键问题，即其注意力操作的二次复杂度，这导致将大模型应用于处理较长序列变得越来越困难。然而，通过利用特殊的记忆 token 实现记忆机制的 Recurrent Memory Transformer（RMT）模型，有效上下文长度能够增长到百万级，这带来了新的发展前景。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/58169928cc0a1d2489e5d6315f3e9a9c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-24T11:27:47+08:00" />
<meta property="article:modified_time" content="2023-05-24T11:27:47+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">GPT4限制被破解！ChatGPT实现超长文本处理的新方法</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E5%89%8D%E8%A8%80-toc" style="margin-left:0px;"><a href="#%E5%89%8D%E8%A8%80" rel="nofollow">前言</a></p> 
<p id="%E4%BD%BF%E7%94%A8chat-gpt%E8%BF%87%E7%A8%8B%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E7%97%9B%E7%82%B9-toc" style="margin-left:0px;"><a href="#%E4%BD%BF%E7%94%A8chat-gpt%E8%BF%87%E7%A8%8B%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E7%97%9B%E7%82%B9" rel="nofollow">使用chat-gpt过程中有哪些痛点</a></p> 
<p id="1.%E6%97%A0%E6%B3%95%E7%90%86%E8%A7%A3%E4%BA%BA%E7%B1%BB%E6%83%85%E6%84%9F%E5%92%8C%E4%B8%BB%E8%A7%82%E6%80%A7%C2%A0-toc" style="margin-left:40px;"><a href="#1.%E6%97%A0%E6%B3%95%E7%90%86%E8%A7%A3%E4%BA%BA%E7%B1%BB%E6%83%85%E6%84%9F%E5%92%8C%E4%B8%BB%E8%A7%82%E6%80%A7%C2%A0" rel="nofollow">1.无法理解人类情感和主观性 </a></p> 
<p id="2.%E4%B8%8A%E4%B8%8B%E6%96%87%E4%B8%A2%E5%A4%B1-toc" style="margin-left:40px;"><a href="#2.%E4%B8%8A%E4%B8%8B%E6%96%87%E4%B8%A2%E5%A4%B1" rel="nofollow">2.上下文丢失</a></p> 
<p id="3.%E7%BA%A6%E5%AE%9A%E8%A2%AB%E6%89%93%E6%96%AD-toc" style="margin-left:40px;"><a href="#3.%E7%BA%A6%E5%AE%9A%E8%A2%AB%E6%89%93%E6%96%AD" rel="nofollow">3.约定被打断</a></p> 
<p id="%E9%82%A3%E5%A6%82%E4%BD%95%E5%8E%BB%E8%A7%A3%E5%86%B3%E8%BF%99%E4%B8%AA%E7%97%9B%E7%82%B9-toc" style="margin-left:0px;"><a href="#%E9%82%A3%E5%A6%82%E4%BD%95%E5%8E%BB%E8%A7%A3%E5%86%B3%E8%BF%99%E4%B8%AA%E7%97%9B%E7%82%B9" rel="nofollow">那如何去解决这个痛点</a></p> 
<p id="Transformer%EF%BC%88RMT%EF%BC%89%E6%80%8E%E4%B9%88%E5%8E%BB%E5%AE%9E%E7%8E%B0%E7%9A%84-toc" style="margin-left:0px;"><a href="#Transformer%EF%BC%88RMT%EF%BC%89%E6%80%8E%E4%B9%88%E5%8E%BB%E5%AE%9E%E7%8E%B0%E7%9A%84" rel="nofollow">Transformer（RMT）怎么去实现的</a></p> 
<p id="1.Transformer%20%E6%A8%A1%E5%9E%8B-toc" style="margin-left:40px;"><a href="#1.Transformer%20%E6%A8%A1%E5%9E%8B" rel="nofollow">1.Transformer 模型</a></p> 
<p id="2.RMT%E6%A8%A1%E5%9E%8B-toc" style="margin-left:40px;"><a href="#2.RMT%E6%A8%A1%E5%9E%8B" rel="nofollow">2.RMT模型</a></p> 
<p id="%C2%A03.%E8%AE%A1%E7%AE%97%E6%8E%A8%E7%90%86%E9%80%9F%E7%8E%87-toc" style="margin-left:40px;"><a href="#%C2%A03.%E8%AE%A1%E7%AE%97%E6%8E%A8%E7%90%86%E9%80%9F%E7%8E%87" rel="nofollow"> 3.计算推理速率</a></p> 
<p id="4.%E6%B8%90%E8%BF%9B%E5%AD%A6%E4%B9%A0%E8%83%BD%E5%8A%9B-toc" style="margin-left:40px;"><a href="#4.%E6%B8%90%E8%BF%9B%E5%AD%A6%E4%B9%A0%E8%83%BD%E5%8A%9B" rel="nofollow">4.渐进学习能力</a></p> 
<p id="%E6%80%BB%E7%BB%93-toc" style="margin-left:0px;"><a href="#%E6%80%BB%E7%BB%93" rel="nofollow">总结</a></p> 
<p id="%E5%86%99%E5%88%B0%E6%9C%80%E5%90%8E-toc" style="margin-left:0px;"><a href="#%E5%86%99%E5%88%B0%E6%9C%80%E5%90%8E" rel="nofollow">写到最后</a></p> 
<hr id="hr-toc"> 
<p></p> 
<p>大家好，我是AI大侠，AI领域的专业博主</p> 
<h2 id="%E5%89%8D%E8%A8%80">前言</h2> 
<p>ChatGPT已经成为了一款备受欢迎的工具，它可以帮助用户<strong>解答问题、写代码、翻译</strong>，甚至可以通过它学习更多行业的知识。然而，博主在使用ChatGPT时会发现它还不够智能，有时候不能够完全理解用户的意思，<strong>答非所问</strong>，下面是博主在使用中遇到的痛点</p> 
<h2 id="%E4%BD%BF%E7%94%A8chat-gpt%E8%BF%87%E7%A8%8B%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E7%97%9B%E7%82%B9">使用chat-gpt过程中有哪些痛点</h2> 
<p><img alt="" height="445" src="https://images2.imgbox.com/25/ba/XaNHKOIX_o.jpg" width="1030"></p> 
<h3 id="1.%E6%97%A0%E6%B3%95%E7%90%86%E8%A7%A3%E4%BA%BA%E7%B1%BB%E6%83%85%E6%84%9F%E5%92%8C%E4%B8%BB%E8%A7%82%E6%80%A7%C2%A0">1.无法理解人类情感和主观性 </h3> 
<p>尽管ChatGPT可以根据上下文理解用户的输入，但它仍然无法真正了解用户的意图，ChatGPT<strong>只能根据输入数据和算法进行分析和回答</strong>，无法真正理解人类的情感和主观性。这种局限性可能导致一些误解和问题。</p> 
<h3 id="2.%E4%B8%8A%E4%B8%8B%E6%96%87%E4%B8%A2%E5%A4%B1">2.上下文丢失</h3> 
<p>与ChatGPT进行对话时，它能够记住<strong>上下文</strong>，并在后续回答中考虑之前的内容。但是，博主在使用过程中经常会出现ChatGPT忘记之前的对话，这可能是由于单次请求中<strong>Token数量的限制</strong>或是ChatGPT<strong>会话长度的限制</strong>所导致的。</p> 
<h3 id="3.%E7%BA%A6%E5%AE%9A%E8%A2%AB%E6%89%93%E6%96%AD">3.约定被打断</h3> 
<p>如果在会话中如果有很多其他的问答，ChatGPT可能会在继续下一步时忘记之前的<strong>约定</strong>，需要再次约定才会保持下去</p> 
<h2 id="%E9%82%A3%E5%A6%82%E4%BD%95%E5%8E%BB%E8%A7%A3%E5%86%B3%E8%BF%99%E4%B8%AA%E7%97%9B%E7%82%B9">那如何去解决这个痛点</h2> 
<p>这几个痛点我想使用过gpt的小伙伴都深有体会，那如何去解决这些问题呢。其实openAI已经给出了答案。</p> 
<p>在发布gpt4的时候，最大的变化除了新数据模型的发布，还有一个重要的技术点更新：<strong>上下文token默认为8K 最长32k(约50页文本) </strong>这代表可以可以处理更长的对话 以及 更深层次的语义分析。这也是gpt4更智能好用的原因。</p> 
<p>但如果把这个token提升到<strong>200万</strong>个，那又会发生什么，</p> 
<blockquote> 
 <p>AI 模型使用的是非结构化文本，常用 Token 表示，以 GPT 模型为例，1000 个 Token 约等于 750 个英文单词</p> 
</blockquote> 
<p>一篇在AI界热论的论文给出了答案，《<strong>Scaling Transformer to 1M tokens and beyond with RMT</strong>》它可以把Transformer 的 Token <strong>上限扩展至 100 万，甚至更多</strong>。</p> 
<p><img alt="" height="317" src="https://images2.imgbox.com/1d/f7/mqQPllQa_o.jpg" width="1094"></p> 
<h2 id="Transformer%EF%BC%88RMT%EF%BC%89%E6%80%8E%E4%B9%88%E5%8E%BB%E5%AE%9E%E7%8E%B0%E7%9A%84">Transformer（RMT）怎么去实现的</h2> 
<h3 id="1.Transformer%20%E6%A8%A1%E5%9E%8B">1.Transformer 模型</h3> 
<p><img alt="" height="431" src="https://images2.imgbox.com/8d/04/sysN9cAU_o.jpg" width="1018"></p> 
<p></p> 
<p>Transformer 是一种<strong>神经网络模型</strong>，是迄今为止最新和最强大的模型之一，常用于处理<strong>上下文学习语义含义</strong>。</p> 
<p>我们来看看gpt4的上下文处理模型为什么只能达到8-32k，因为transformer 的<strong>可输入长度取决于内存大小</strong>，这意味着实现太长的token不现实，Transformer 存在一个关键问题，即其注意力操作的二次复杂度，这导致将大模型应用于<strong>处理较长序列变得越来越困难</strong>。然而，通过利用特殊的记忆 token 实现记忆机制的 Recurrent Memory Transformer（RMT）模型，有效上下文长度能够增长到百万级，这带来了新的发展前景。</p> 
<h3 id="2.RMT%E6%A8%A1%E5%9E%8B">2.RMT模型</h3> 
<p><strong>RMT 全称Recurrent Memory Transformer（递归记忆Transformer）</strong></p> 
<figure class="image"> 
 <img alt="" height="239" src="https://images2.imgbox.com/00/7e/IlYzzkJM_o.jpg" width="552"> 
 <figcaption>
   RMT结构图 
 </figcaption> 
</figure> 
<p></p> 
<p></p> 
<p>递归记忆Transformer（RMT）是一种<strong>基于记忆机制的序列建模架构</strong>，用于存储和处理序列数据中的局部和全局信息，并通过递归传递信息来处理长序列中的段之间的依赖关系。</p> 
<p>相较于标准Transformer模型的实施，RMT仅通过对输入和输出序列进行修改而<strong>无需修改底层模型架构</strong>。模型通过训练过程中的记忆操作和序列表示处理来掌控记忆机制的行为。</p> 
<p>具体而言，RMT采用记忆token的方式将记忆信息添加到输入序列中，从而为模型提供额外的容量，以处理与输入序列中任意元素无直接关联的信息。为了应对长序列的挑战，<strong>RMT将序列分割为不同的段，并通过记忆传递机制将上一段的记忆状态传递到当前段</strong>。在训练过程中，梯度通过记忆传递的路径从当前段向前一段流动，从而实现信息的回传和更新记忆状态的目的。</p> 
<p></p> 
<blockquote> 
 <p>这意味着扩展了token的数量，如果达到理想的200万，我们可以将整部小说甚至更多内容输入到GPT中，而<strong>无需依赖上下文来理解用户的信息</strong>。这种改进使得GPT能够更准确地处理输入，并提供更精准的回复。现在，试想一下，如果我将整篇《红楼梦》输入到GPT中，是否可以让它帮我续写这个经典作品呢？</p> 
</blockquote> 
<h3 id="%C2%A03.%E8%AE%A1%E7%AE%97%E6%8E%A8%E7%90%86%E9%80%9F%E7%8E%87"> 3.计算推理速率</h3> 
<p><img alt="" height="393" src="https://images2.imgbox.com/88/51/dGsnYvpQ_o.jpg" width="1103"></p> 
<p></p> 
<p>从论文的计算结果中可以很直观地观察到，<strong>推理时间与输入序列长度呈线性关系</strong>。</p> 
<p>在处理包含多个片段的大型序列时，<strong>递归记忆Transformer（RMT）模型可能比非循环模型更有效率。</strong></p> 
<p>这意味着在GPT模型中输入更多内容，可以让模型更深入地理解用户的意图，从而提供更准确的答复。</p> 
<blockquote> 
 <p>如果将自己的聊天信息和朋友圈动态等数据导入GPT模型，并让它进行理解和吸收，是否能够快速生成一个完整的虚拟人格呢？如果token达到这个量级 完全是可实现的，这就有些恐怖了</p> 
</blockquote> 
<h3 id="4.%E6%B8%90%E8%BF%9B%E5%AD%A6%E4%B9%A0%E8%83%BD%E5%8A%9B">4.渐进学习能力</h3> 
<p>论文中还指出，<strong>随着输入数量的增加，机器学习模型学习到的结果也变得更加准确。</strong></p> 
<p><img alt="" height="530" src="https://images2.imgbox.com/7f/be/OqM3OMjd_o.jpg" width="759"></p> 
<p></p> 
<p>这意味着输入更多的数据可以<strong>显著提升模型的性能和预测准确度。</strong></p> 
<h2 id="%E6%80%BB%E7%BB%93">总结</h2> 
<p>这项技术将使得ChatGPT的能力上限被<strong>突破</strong>。这也让ChatGPT的痛点得以解决，使得它更完美。</p> 
<blockquote> 
 <p>我们甚至可以将整个项目的代码交给GPT，并明确告诉它我们的需求，它将能够直接开始处理后续需求、修改代码并进行优化以及后面的需求迭代。</p> 
</blockquote> 
<p></p> 
<h2 id="%E5%86%99%E5%88%B0%E6%9C%80%E5%90%8E">写到最后</h2> 
<p>每天在AI领域都有令人震撼的进展，各种新技术层出不穷。有幸生活在这个充满创新的时代，你准备好了吗</p> 
<p></p> 
<p>AI是一个充满机遇和挑战的领域，</p> 
<p>AI时代已经到来，AI真的会取代我们吗？</p> 
<p>你还不主动了解AI？</p> 
<p>你还在为跟同事聊AI插不上话吗？</p> 
<p>那请关注大侠，带你了解AI行业第一动态。</p> 
<p><img alt="" height="383" src="https://images2.imgbox.com/8a/05/VT4MBvca_o.jpg" width="900"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/261e8b5c8e393695745bfb258afd533a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【CentOS】Linux CentOS7 配置LAMP环境</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/372cf8ea86ff968bfcb7cb6151223c62/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">ubuntu20.4配置自动启动root用户并开启向日葵</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>