<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>网络爬虫 | 京东全站数据采集（类目、店铺、商品、评论）——基于Python中Scrapy框架 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="网络爬虫 | 京东全站数据采集（类目、店铺、商品、评论）——基于Python中Scrapy框架" />
<meta property="og:description" content="目录
1.定义采集数据的存储结构
2.定义管道文件
3.定义中间件文件
4.scrapy爬虫设置文件修改
5.商品类目抓取
6.商品信息抓取
7.店铺信息抓取
8.评论信息抓取
9.抓取过程
10.基本数据展示
1.定义采集数据的存储结构 【存储结构说明】class CategoriesItem(Item)：存储京东类目信息class ProductsItem(Item)：存储京东商品信息class ShopItem(Item)：存储京东店铺信息class CommentSummaryItem(Item)：存储京东每个商品的评论概况信息class CommentItem(Item)：存储京东每个商品的评论基本信息class CommentImageItem(Item)：存储京东每个商品中每条评论的图像信息说明：类中所定义字段可依据具体采集要求或response内容进行调整 【items.py程序】
# -*- coding: utf-8 -*- # Define here the models for your scraped items # # See documentation in: # http://doc.scrapy.org/en/latest/topics/items.html from scrapy import Item, Field class CategoriesItem(Item): &#34;&#34;&#34; 存储京东类目信息 &#34;&#34;&#34; name = Field() # 商品三级类目名称 url = Field() # 商品三级类目对应url _id = Field() # 商品类目对应id[一级id,二级id,三级id] class ProductsItem(Item): &#34;&#34;&#34; 存储京东商品信息 &#34;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/0e45ec4bfb1de0e29c19802f2717a694/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-05-11T18:12:53+08:00" />
<meta property="article:modified_time" content="2020-05-11T18:12:53+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">网络爬虫 | 京东全站数据采集（类目、店铺、商品、评论）——基于Python中Scrapy框架</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="1.%E5%AE%9A%E4%B9%89%E9%87%87%E9%9B%86%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84-toc" style="margin-left:40px;"><a href="#1.%E5%AE%9A%E4%B9%89%E9%87%87%E9%9B%86%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84" rel="nofollow">1.定义采集数据的存储结构</a></p> 
<p id="2.%E5%AE%9A%E4%B9%89%E7%AE%A1%E9%81%93%E6%96%87%E4%BB%B6-toc" style="margin-left:40px;"><a href="#2.%E5%AE%9A%E4%B9%89%E7%AE%A1%E9%81%93%E6%96%87%E4%BB%B6" rel="nofollow">2.定义管道文件</a></p> 
<p id="3.%E5%AE%9A%E4%B9%89%E4%B8%AD%E9%97%B4%E4%BB%B6%E6%96%87%E4%BB%B6-toc" style="margin-left:40px;"><a href="#3.%E5%AE%9A%E4%B9%89%E4%B8%AD%E9%97%B4%E4%BB%B6%E6%96%87%E4%BB%B6" rel="nofollow">3.定义中间件文件</a></p> 
<p id="4.scrapy%E7%88%AC%E8%99%AB%E8%AE%BE%E7%BD%AE%E6%96%87%E4%BB%B6%E4%BF%AE%E6%94%B9-toc" style="margin-left:40px;"><a href="#4.scrapy%E7%88%AC%E8%99%AB%E8%AE%BE%E7%BD%AE%E6%96%87%E4%BB%B6%E4%BF%AE%E6%94%B9" rel="nofollow">4.scrapy爬虫设置文件修改</a></p> 
<p id="5.%E5%95%86%E5%93%81%E7%B1%BB%E7%9B%AE%E6%8A%93%E5%8F%96-toc" style="margin-left:40px;"><a href="#5.%E5%95%86%E5%93%81%E7%B1%BB%E7%9B%AE%E6%8A%93%E5%8F%96" rel="nofollow">5.商品类目抓取</a></p> 
<p id="6.%E5%95%86%E5%93%81%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96-toc" style="margin-left:40px;"><a href="#6.%E5%95%86%E5%93%81%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96" rel="nofollow">6.商品信息抓取</a></p> 
<p id="7.%E5%BA%97%E9%93%BA%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96-toc" style="margin-left:40px;"><a href="#7.%E5%BA%97%E9%93%BA%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96" rel="nofollow">7.店铺信息抓取</a></p> 
<p id="8.%E8%AF%84%E8%AE%BA%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96-toc" style="margin-left:40px;"><a href="#8.%E8%AF%84%E8%AE%BA%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96" rel="nofollow">8.评论信息抓取</a></p> 
<p id="9.%E6%8A%93%E5%8F%96%E8%BF%87%E7%A8%8B-toc" style="margin-left:40px;"><a href="#9.%E6%8A%93%E5%8F%96%E8%BF%87%E7%A8%8B" rel="nofollow">9.抓取过程</a></p> 
<p id="10.%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%B1%95%E7%A4%BA-toc" style="margin-left:40px;"><a href="#10.%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%B1%95%E7%A4%BA" rel="nofollow">10.基本数据展示</a></p> 
<hr id="hr-toc"> 
<h3 id="1.%E5%AE%9A%E4%B9%89%E9%87%87%E9%9B%86%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84">1.定义采集数据的存储结构</h3> 
<blockquote> 
 <ul><li><em><u><strong>【存储结构说明】</strong></u></em></li><li>class CategoriesItem(Item)：存储京东类目信息</li><li>class ProductsItem(Item)：存储京东商品信息</li><li>class ShopItem(Item)：存储京东店铺信息</li><li>class CommentSummaryItem(Item)：存储京东每个商品的评论概况信息</li><li>class CommentItem(Item)：存储京东每个商品的评论基本信息</li><li>class CommentImageItem(Item)：存储京东每个商品中每条评论的图像信息</li><li>说明：类中所定义字段可依据具体采集要求或response内容进行调整</li></ul> 
</blockquote> 
<p><u><span style="color:#f33b45;"><strong>【items.py程序】</strong></span></u></p> 
<pre><code class="language-python"># -*- coding: utf-8 -*-

# Define here the models for your scraped items
#
# See documentation in:
# http://doc.scrapy.org/en/latest/topics/items.html
from scrapy import Item, Field


class CategoriesItem(Item):
    """
    存储京东类目信息
    """
    name = Field()    # 商品三级类目名称
    url = Field()     # 商品三级类目对应url
    _id = Field()     # 商品类目对应id[一级id,二级id,三级id]


class ProductsItem(Item):
    """
    存储京东商品信息
    """
    name = Field()                  # 商品名称
    url = Field()                   # 商品url[用于商品主图提取]
    _id = Field()                   # 商品sku
    category = Field()              # 商品三级类目
    description = Field()           # 商品描述
    shopId = Field()                # 商品所在店铺id(名称)
    commentCount = Field()          # 商品评价总数=CommentSummaryItem.commentCount
    # goodComment = Field()           # 商品好评数
    # generalComment = Field()        # 商品中评数
    # poolComment = Field()           # 商品差评数
    # favourableDesc1 = Field()       # 商品优惠描述1
    # favourableDesc2 = Field()       # 商品优惠描述2
    # venderId = Field()              # 供应商id
    # reallyPrice = Field()           # 商品现价
    # originalPrice = Field()         # 商品原价


class ShopItem(Item):
    _id = Field()                   # 店铺url
    shopName = Field()              # 店铺名称
    shopItemScore = Field()         # 店铺[商品评价]
    shopLgcScore = Field()          # 店铺[物流履约]
    shopAfterSale = Field()         # 店铺[售后服务]


class CommentItem(Item):
    _id = Field()                   # 评论id
    productId = Field()             # 商品id=sku
    guid = Field()                  # 评论全局唯一标识符
    firstCategory = Field()         # 商品一级类目
    secondCategory = Field()        # 商品二级类目
    thirdCategory = Field()         # 商品三级类目
    score = Field()                 # 用户评分
    nickname = Field()              # 用户昵称
    plusAvailable = Field()         # 用户账户等级(201：PLUS, 103:普通用户，0：无价值用户)
    content = Field()               # 评论内容
    creationTime = Field()          # 评论时间
    replyCount = Field()            # 评论的评论数
    usefulVoteCount = Field()       # 用户评论的被点赞数
    imageCount = Field()            # 评论中图片的数量


class CommentImageItem(Item):
    _id = Field()                   # 晒图对应id(1张图对应1个id)
    commentGuid = Field()           # 晒图所在评论的全局唯一标识符guid
    imgId = Field()                 # 晒图对应id
    imgUrl = Field()                # 晒图url
    imgTitle = Field()              # 晒图标题
    imgStatus = Field()             # 晒图状态


class CommentSummaryItem(Item):
    """商品评论总结"""
    _id = Field()                   # 商品sku
    productId = Field()             # 商品pid
    commentCount = Field()          # 商品累计评论数
    score1Count = Field()           # 用户评分为1的数量
    score2Count = Field()           # 用户评分为2的数量
    score3Count = Field()           # 用户评分为3的数量
    score4Count = Field()           # 用户评分为3的数量
    score5Count = Field()           # 用户评分为5的数量</code></pre> 
<h3 id="2.%E5%AE%9A%E4%B9%89%E7%AE%A1%E9%81%93%E6%96%87%E4%BB%B6">2.定义管道文件</h3> 
<blockquote> 
 <ul><li><em><u><strong>【管道文件说明】</strong></u></em></li><li>数据库：MongoDB</li><li>数据库名称：JD</li><li>数据库集合：Categories、Products、Shop、CommentSummary、Comment和CommentImage</li><li>处理过程：先判断待插入数据库集合类型是否匹配，然后插入，并为重复数据插入抛出异常</li></ul> 
</blockquote> 
<p><u><span style="color:#f33b45;"><strong>【pipelines.py】</strong></span></u></p> 
<pre><code class="language-python"># -*- coding: utf-8 -*-

# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html

import pymongo
from JDSpider.items import *


class MongoDBPipeline(object):
    def __init__(self):
        clinet = pymongo.MongoClient("localhost", 27017)
        db = clinet["JD"]
        self.Categories = db["Categories"]
        self.Products = db["Products"]
        self.Shop = db["Shop"]
        self.Comment = db["Comment"]
        self.CommentImage = db["CommentImage"]
        self.CommentSummary = db["CommentSummary"]

    def process_item(self, item, spider):
        """ 判断item的类型，并作相应的处理，再入数据库 """
        if isinstance(item, CategoriesItem):
            try:
                self.Categories.insert(dict(item))
            except Exception as e:
                print('get failed:', e)
        elif isinstance(item, ProductsItem):
            try:
                self.Products.insert(dict(item))
            except Exception as e:
                print('get failed:', e)
        elif isinstance(item, ShopItem):
            try:
                self.Shop.insert(dict(item))
            except Exception as e:
                print('get failed:', e)
        elif isinstance(item, CommentItem):
            try:
                self.Comment.insert(dict(item))
            except Exception as e:
                print('get failed:', e)
        elif isinstance(item, CommentImageItem):
            try:
                self.CommentImage.insert(dict(item))
            except Exception as e:
                print('get failed:', e)
        elif isinstance(item, CommentSummaryItem):
            try:
                self.CommentSummary.insert(dict(item))
            except Exception as e:
                print('get failed:', e)
        elif isinstance(item, ShopItem):
            try:
                self.Shop.insert(dict(item))
            except Exception as e:
                print('get failed:', e)
        return item</code></pre> 
<h3 id="3.%E5%AE%9A%E4%B9%89%E4%B8%AD%E9%97%B4%E4%BB%B6%E6%96%87%E4%BB%B6">3.定义中间件文件</h3> 
<blockquote> 
 <ul><li><em><u><strong>【中间件文件说明】</strong></u></em></li><li>包括“爬虫代理中间件”和“缓存中间件”</li><li>爬虫代理中间件：防止连续请求被京东后台发现并拉黑</li><li>缓存中间件：判断京东后台服务器响应情况，并作出针对性处理</li></ul> 
</blockquote> 
<p><u><span style="color:#f33b45;"><strong>【middlewares.py】</strong></span></u></p> 
<pre><code class="language-python"># -*- coding: utf-8 -*-

# Define here the models for your spider middleware
#
# See documentation in:
# http://doc.scrapy.org/en/latest/topics/spider-middleware.html

import os
import logging
from scrapy.exceptions import IgnoreRequest
from scrapy.utils.response import response_status_message
from scrapy.downloadermiddlewares.retry import RetryMiddleware
import random

logger = logging.getLogger(__name__)


class UserAgentMiddleware(object):
    """ 换User-Agent """

    def process_request(self, request, spider):
        """设置爬虫代理"""
        with open("E://proxy.txt", "r") as f:
            PROXIES = f.readlines()
            agent = random.choice(PROXIES)
            agent = agent.strip()
            request.headers["User-Agent"] = agent


class CookiesMiddleware(RetryMiddleware):
    """ 维护Cookie """

    def process_request(self, request, spider):
        pass

    def process_response(self, request, response, spider):
        if response.status in [300, 301, 302, 303]:
            try:
                reason = response_status_message(response.status)
                return self._retry(request, reason, spider) or response  # 重试
            except Exception as e:
                raise IgnoreRequest
        elif response.status in [403, 414]:
            logger.error("%s! Stopping..." % response.status)
            os.system("pause")
        else:
            return response</code></pre> 
<h3 id="4.scrapy%E7%88%AC%E8%99%AB%E8%AE%BE%E7%BD%AE%E6%96%87%E4%BB%B6%E4%BF%AE%E6%94%B9">4.scrapy爬虫设置文件修改</h3> 
<blockquote> 
 <ul><li><em><u><strong>【修改说明】</strong></u></em></li><li>robot协议：置位False，防止京东网站不允许爬虫抓取数据</li><li>爬虫最大并发请求：可依据电脑实际性能进行设置</li><li>下载中间件优先级：值越小，优先级越高</li><li>管道文件优先级：值越小，优先级越高</li><li>说明：代码文件过长，故不再展示</li></ul> 
</blockquote> 
<h3 id="5.%E5%95%86%E5%93%81%E7%B1%BB%E7%9B%AE%E6%8A%93%E5%8F%96">5.商品类目抓取</h3> 
<blockquote> 
 <ul><li><em><u><strong>【商品类目抓取说明】</strong></u></em></li><li>有些类别里面包含有很多子类别，所以对于这样的url，需要再次yield并进行抓取</li></ul> 
</blockquote> 
<pre><code class="language-python">texts = selector.xpath('//div[@class="category-item m"]/div[@class="mc"]/div[@class="items"]/dl/dd/a').extract()
            for text in texts:
                # 获取全部三级类目链接+三级类目名称
                items = re.findall(r'&lt;a href="(.*?)" target="_blank"&gt;(.*?)&lt;/a&gt;', text)
                for item in items:
                    # 判断“商品链接”是否需要继续请求
                    if item[0].split('.')[0][2:] in key_word:
                        if item[0].split('.')[0][2:] != 'list':
                            yield Request(url='https:' + item[0], callback=self.parse_category)
                        else:
                            # 记录一级类目：名称/可提数URL/id编码
                            categoriesItem = CategoriesItem()
                            categoriesItem['name'] = item[1]
                            categoriesItem['url'] = 'https:' + item[0]
                            categoriesItem['_id'] = item[0].split('=')[1].split('&amp;')[0]
                            yield categoriesItem
                            meta = dict()
                            meta["category"] = item[0].split("=")[1]
                            yield Request(url='https:' + item[0], callback=self.parse_list, meta=meta)</code></pre> 
<h3 id="6.%E5%95%86%E5%93%81%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96">6.商品信息抓取</h3> 
<blockquote> 
 <ul><li>【店铺信息抓取说明】</li><li>流程：访问每个类别的url，在产品列表中获取每个商品对应的url,进入详情页面抓取产品的详情</li><li>注意：此处要通过分析得出翻页请求对应的response地址，并解析规律进行翻页</li></ul> 
</blockquote> 
<p><u><span style="color:#f33b45;"><strong>【获取商品链接】</strong></span></u></p> 
<pre><code class="language-python">selector = Selector(response)
        texts = selector.xpath('//*[@id="J_goodsList"]/ul/li/div/div[@class="p-img"]/a').extract()
        for text in texts:
            items = text.split("=")[3].split('"')[1]
            yield Request(url='https:' + items, callback=self.parse_product, meta=meta)

        # 翻页[仅翻前50页]
        maxPage = int(response.xpath('//div[@id="J_filter"]/div/div/span/i/text()').extract()[0])
        if maxPage &gt; 1:
            if maxPage &gt; 50:
                maxPage = 50
            for i in range(2, maxPage):
                num = 2*i - 1
                caterory = meta["category"].split(",")[0]+'%2C' + meta["category"].split(",")[1] + '%2C' + meta["category"].split(",")[2]
                url = list_url % (caterory, num, 30*num)
                print('products next page:', url)
                yield Request(url=url, callback=self.parse_list2, meta=meta)</code></pre> 
<h3 id="7.%E5%BA%97%E9%93%BA%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96">7.店铺信息抓取</h3> 
<blockquote> 
 <ul><li><u><em><strong>【店铺信息抓取说明】</strong></em></u></li><li>店铺信息在抓取商品信息的页面便可以获取</li><li>但是，要区分自营和非自营，因为自营缺少一些内容</li></ul> 
</blockquote> 
<pre><code class="language-python"># 商品在售店铺id+店铺信息获取
        shopItem["shopName"] = response.xpath('//div[@class="m m-aside popbox"]/div/div/h3/a/text()').extract()[0]
        shopItem["_id"] = "https:" + response.xpath('//div[@class="m m-aside popbox"]/div/div/h3/a/@href').extract()[0]
        productsItem['shopId'] = shopItem["_id"]
        # 区分是否自营
        res = response.xpath('//div[@class="score-parts"]/div/span/em/@title').extract()
        if len(res) == 0:
            shopItem["shopItemScore"] = "京东自营"
            shopItem["shopLgcScore"] = "京东自营"
            shopItem["shopAfterSale"] = "京东自营"
        else:
            shopItem["shopItemScore"] = res[0]
            shopItem["shopLgcScore"] = res[1]
            shopItem["shopAfterSale"] = res[2]
            # shopItem["_id"] = response.xpath('//div[@class="m m-aside popbox"]/div/div/h3/a/@href').extract()[0].split("-")[1].split(".")[0]
        yield shopItem</code></pre> 
<h3 id="8.%E8%AF%84%E8%AE%BA%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96">8.评论信息抓取</h3> 
<blockquote> 
 <ul><li><em><u><strong>【评论信息抓取说明】</strong></u></em></li><li>评论的信息也是动态加载，返回的格式也是json，且会不定期进行更新，访问格式如下：</li><li> <pre><code class="language-python">comment_url = 'https://club.jd.com/comment/productPageComments.action?productId=%s&amp;score=0&amp;sortType=5&amp;page=%s&amp;pageSize=10'</code></pre> <p></p> </li></ul> 
</blockquote> 
<pre><code class="language-python">    def parse_comments(self, response):
        """
        获取商品评论
        :param response: 评论相应的json脚本
        :return:
        """
        try:
            data = json.loads(response.text)
        except Exception as e:
            print('get comment failed:', e)
            return None

        product_id = response.meta['product_id']

        # 商品评论概况获取[仅导入一次]
        commentSummaryItem = CommentSummaryItem()
        commentSummary = data.get('productCommentSummary')
        commentSummaryItem['_id'] = commentSummary.get('skuId')
        commentSummaryItem['productId'] = commentSummary.get('productId')
        commentSummaryItem['commentCount'] = commentSummary.get('commentCount')
        commentSummaryItem['score1Count'] = commentSummary.get('score1Count')
        commentSummaryItem['score2Count'] = commentSummary.get('score2Count')
        commentSummaryItem['score3Count'] = commentSummary.get('score3Count')
        commentSummaryItem['score4Count'] = commentSummary.get('score4Count')
        commentSummaryItem['score5Count'] = commentSummary.get('score5Count')

        # 判断commentSummaryItem类型
        yield commentSummaryItem

        # 商品评论[第一页，剩余页面评论由，parse_comments2]
        for comment_item in data['comments']:
            comment = CommentItem()
            comment['_id'] = str(product_id)+","+str(comment_item.get("id"))
            comment['productId'] = product_id
            comment["guid"] = comment_item.get('guid')
            comment['firstCategory'] = comment_item.get('firstCategory')
            comment['secondCategory'] = comment_item.get('secondCategory')
            comment['thirdCategory'] = comment_item.get('thirdCategory')
            comment['score'] = comment_item.get('score')
            comment['nickname'] = comment_item.get('nickname')
            comment['plusAvailable'] = comment_item.get('plusAvailable')
            comment['content'] = comment_item.get('content')
            comment['creationTime'] = comment_item.get('creationTime')
            comment['replyCount'] = comment_item.get('replyCount')
            comment['usefulVoteCount'] = comment_item.get('usefulVoteCount')
            comment['imageCount'] = comment_item.get('imageCount')
            yield comment

            # 存储当前用户评论中的图片
            if 'images' in comment_item:
                for image in comment_item['images']:
                    commentImageItem = CommentImageItem()
                    commentImageItem['commentGuid'] = comment_item.get('guid')
                    commentImageItem['imgId'] = image.get('id')
                    commentImageItem['_id'] = str(product_id)+","+str(comment_item.get('id'))+","+str(image.get('id'))
                    commentImageItem['imgUrl'] = 'http:' + image.get('imgUrl')
                    commentImageItem['imgTitle'] = image.get('imgTitle')
                    commentImageItem['imgStatus'] = image.get('status')
                    yield commentImageItem

        # 评论翻页[尽量保证评分充足]
        max_page = int(data.get('maxPage', '1'))
        # if max_page &gt; 60:
        #     # 设置评论的最大翻页数
        #     max_page = 60
        for i in range(1, max_page):
            url = comment_url % (product_id, str(i))
            meta = dict()
            meta['product_id'] = product_id
            yield Request(url=url, callback=self.parse_comments2, meta=meta)</code></pre> 
<h3 id="9.%E6%8A%93%E5%8F%96%E8%BF%87%E7%A8%8B">9.抓取过程</h3> 
<p style="text-align:center;"><img alt="" height="349" src="https://images2.imgbox.com/87/57/CrrFvIR8_o.png" width="696"></p> 
<h3 id="10.%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%B1%95%E7%A4%BA">10.基本数据展示</h3> 
<blockquote> 
 <p>有数据需要的可以联系，数据非常大</p> 
</blockquote> 
<p style="text-align:center;"><img alt="" height="264" src="https://images2.imgbox.com/23/cf/0hI8qjpH_o.png" width="218"></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/31e72ebf5832ea458cc9b474ba063d58/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Element-ui 遇到的坑之返回顶部Backtop组件的target如何设置正确的值的问题。</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/fd0bdd4d48309ef00e77b4a343a4c101/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Hadoop完全分布式安装的心酸历程</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>