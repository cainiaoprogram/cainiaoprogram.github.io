<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ViLT:Vision-and-Language Transformer Withoout Convolution or Region Supervision - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="ViLT:Vision-and-Language Transformer Withoout Convolution or Region Supervision" />
<meta property="og:description" content=" ViLT:Vision-and-Language Transformer Withoout Convolution or Region Supervision 如今，在多模态领域，同样采取先预训练，再进行微调的方式。
解决问题 视觉和语言预训练 (VLP) 提高了各种联合视觉和语言下游任务的性能。之前的工作发现，在视觉方面网络的模型越复杂越好，最终的结果就会越好。即当前的 VLP 方法严重依赖图像特征提取过程，其中大部分涉及区域监督（例如，对象检测）和卷积架构（例如，ResNet）。
作者认为此前的工作存在以下两个问题：
(1) 效率/速度方面存在问题，简单地提取输入特征需要比多模式交互步骤更多的计算；
(2) 表达能力，因为它是视觉嵌入器及其预定义视觉词汇表达能力的上限。仅仅用一个预训练好的模型去抽取特征，模型的表达能力是受限的，由于不是端到端的学习，可能抽取的特征非最优解。
为了解决上述问题，作者提出了一个极简化的模型。
为什么要选择目标检测？ 目标检测是天然的离散化的过程，并且有明确的语义信息，这正是transformer所需要的。下游任务往往和物体有直接的联系，即对物体有依赖性。
虽然大部分数据集可以事先通过目标检测抽取特征。事实上，速率是个十分严重的问题，因为当你在真实世界中，去做这种应用的时候，数据是每时每秒在实时生成的，对于新数据，再做推理的时候，就没有那么多时间来做这种目标检测。
所以，重心就转移到怎么设计一个更轻量更简单的图像特征抽取的方法。 研究方法和创新点 ViLT 是迄今为止最简单的视觉和语言模型架构，因为它委托转换器模块提取和处理视觉特征，而不是单独的深度视觉嵌入器。这种设计本质上会带来显着的运行时间和参数效率。第一次，我们在不使用区域特征或一般的深度卷积视觉嵌入器的情况下，在视觉和语言任务上取得了出色的表现。此外，我们首次凭经验表明，在 VLP 训练方案中前所未有的全词屏蔽和图像增强进一步推动了下游性能。 模态融合方法 single-stream
将image和text输入直接concatenation起来。
dual-stream
对于image和text，各自先各自将自己的输入进行一些处理，充分挖掘单独模态里包含的信息，然后再去在之后的某一个时间点做一个融合。
特征抽取 region featuresgrid featurespatch projection 流程和模块 single-stream
损失函数 Image Text Matching——负对数似然损失&#43;图像文本对齐分数
Masked Language Modeling
预训练数据集 实验结果 " />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/65ca0b83914258bf268aeeefecb3da99/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-01-06T11:11:22+08:00" />
<meta property="article:modified_time" content="2023-01-06T11:11:22+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ViLT:Vision-and-Language Transformer Withoout Convolution or Region Supervision</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="ViLTVisionandLanguage_Transformer_Withoout_Convolution_or_Region_Supervision_0"></a>ViLT:Vision-and-Language Transformer Withoout Convolution or Region Supervision</h2> 
<p>如今，在多模态领域，同样采取先预训练，再进行微调的方式。</p> 
<h5><a id="_2"></a>解决问题</h5> 
<p>视觉和语言预训练 (VLP) 提高了各种联合视觉和语言下游任务的性能。之前的工作发现，在视觉方面网络的模型越复杂越好，最终的结果就会越好。即当前的 VLP 方法严重依赖图像特征提取过程，其中大部分涉及区域监督（例如，对象检测）和卷积架构（例如，ResNet）。<br> 作者认为此前的工作存在以下两个问题：<br> (1) 效率/速度方面存在问题，简单地提取输入特征需要比多模式交互步骤更多的计算；<br> (2) 表达能力，因为它是视觉嵌入器及其预定义视觉词汇表达能力的上限。仅仅用一个预训练好的模型去抽取特征，模型的表达能力是受限的，由于不是端到端的学习，可能抽取的特征非最优解。<br> 为了解决上述问题，作者提出了一个极简化的模型。<br> <img src="https://images2.imgbox.com/0d/07/pvY125Of_o.png" alt="在这里插入图片描述"></p> 
<h6><a id="_9"></a>为什么要选择目标检测？</h6> 
<ul><li>目标检测是天然的离散化的过程，并且有明确的语义信息，这正是transformer所需要的。</li><li>下游任务往往和物体有直接的联系，即对物体有依赖性。<br> 虽然大部分数据集可以事先通过目标检测抽取特征。事实上，速率是个十分严重的问题，因为当你在真实世界中，去做这种应用的时候，数据是每时每秒在实时生成的，对于新数据，再做推理的时候，就没有那么多时间来做这种目标检测。<br> 所以，重心就转移到怎么设计一个<strong>更轻量更简单的图像特征抽取</strong>的方法。</li></ul> 
<h5><a id="_14"></a>研究方法和创新点</h5> 
<p><img src="https://images2.imgbox.com/93/5b/uVDKk0RD_o.png" alt="在这里插入图片描述"></p> 
<ul><li>ViLT 是迄今为止最简单的视觉和语言模型架构，因为它委托转换器模块提取和处理视觉特征，而不是单独的深度视觉嵌入器。这种设计本质上会带来显着的运行时间和参数效率。</li><li>第一次，我们在不使用区域特征或一般的深度卷积视觉嵌入器的情况下，在视觉和语言任务上取得了出色的表现。</li><li>此外，我们首次凭经验表明，在 VLP 训练方案中前所未有的全词屏蔽和图像增强进一步推动了下游性能。</li></ul> 
<h6><a id="_19"></a>模态融合方法</h6> 
<p>single-stream<br> 将image和text输入直接concatenation起来。<br> dual-stream<br> 对于image和text，各自先各自将自己的输入进行一些处理，充分挖掘单独模态里包含的信息，然后再去在之后的某一个时间点做一个融合。</p> 
<h6><a id="_24"></a>特征抽取</h6> 
<ul><li>region features</li><li>grid features</li><li>patch projection</li></ul> 
<h5><a id="_28"></a>流程和模块</h5> 
<p><img src="https://images2.imgbox.com/08/74/6xW1tnpO_o.png" alt="在这里插入图片描述"><br> single-stream<br> <img src="https://images2.imgbox.com/74/80/OYG06yft_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="_32"></a>损失函数</h5> 
<p>Image Text Matching——负对数似然损失+图像文本对齐分数<br> Masked Language Modeling</p> 
<h5><a id="_35"></a>预训练数据集</h5> 
<p><img src="https://images2.imgbox.com/0d/9e/p3SkwgKp_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="_38"></a>实验结果</h5> 
<p><img src="https://images2.imgbox.com/f6/8c/2I9KecNx_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/9937cae4ab5a1b2d0b3562f25087fe66/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">LXMERT:Learning Cross-Modality Encoder Representations from Transformers</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/019fd48e5dabf6a695b2cdd0afd37bb4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">RabbitMQ 消息确认机制</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>