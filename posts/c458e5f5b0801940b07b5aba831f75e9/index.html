<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>回归模型 第5篇：knn回归 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="回归模型 第5篇：knn回归" />
<meta property="og:description" content="基于最邻近算法的分类，本质上是对离散的数据标签进行预测，实际上，最邻近算法也可以用于对连续的数据标签进行预测，这种方法叫做基于最邻近数据的回归，预测的值(即数据的标签)是连续值，通过计算数据点最临近数据点平均值而获得预测值。
一，sklearn的knn回归 scikit-learn实现了两个不同的最邻近回归模型：
KNeighborsRegressor：根据每个查询点的最邻近的k个数据点的均值作为预测值，其中，k是用户指定的整数。RadiusNeighborsRegressor：基于查询点的固定半径内的数据点的均值作为预测值，其中r是用户指定的浮点值。 回归模拟器的定义如下，该定义只列出最重要的参数，详细参数请参考sicikit-learn 官网：
sklearn.neighbors.KNeighborsRegressor(n_neighbors=5, weights=&#39;uniform&#39;, algorithm=&#39;auto&#39;, metric=&#39;minkowski&#39;,...) sklearn.neighbors.RadiusNeighborsRegressor(radius=1.0, weights=&#39;uniform&#39;, algorithm=&#39;auto&#39;, metric=&#39;minkowski&#39;,...) 参数注释：
radius：寻找最邻近数据点的半径n_neighbors：最邻近的邻居数量algorithm：寻找最邻近的数据点的算法，有效值是[&#39;auto&#39;，&#39;ball_tree&#39;，&#39;kd_tree&#39;，&#39;brute&#39;]metric：计算距离的度量，详细信息请查看：DistanceMetric weights：权重，默认值weights =&#39;uniform&#39;，为每个邻居分配统一的权重。 weights =&#39;distance&#39;分配的权重与距查询点的距离成反比。用于也可以提供定义函数来计算权重。在某些情况下，最好对邻居加权，以使较近的邻居对拟合的贡献更大，这可以通过weights关键字完成。 最基本的最邻近回归使用统一的权重，也就是说，在特定范围中的每个数据点对查询点的分类(回归)的作用是相同的。在某些情况下，对权重点进行加权可能会比较有利，以使邻近的点比远离的点对回归的贡献更大，这可以通过weights关键字完成。默认值weights =&#39;uniform&#39;，为所有点分配相等的权重。 weights =&#39;distance&#39;分配的权重与距查询点的距离成反比。
二，基于最邻近的数据点的数量来预测 当使用knn计算某个数据点的预测值时，模型会从训练数据集中选择离该数据点最近的k个数据点，并且把它们的y值取均值，把该均值作为新数据点的预测值：
from sklearn.neighbors import KNeighborsRegressor 对于knn分类，使用score方法评估模型，对于回归的问题，返回的是R^2分数，R^2分数也叫做决定系数，是回归模型预测的优度度量，位于0到1之间，R^2等于1对应完美预测，R^2等于0对应于常数模型，即总是预测训练集响应(y_train)的均值。
from sklearn.datasets import make_regression from sklearn.neighbors import KNeighborsRegressor from sklearn.model_selection import train_test_split kng=KNeighborsRegressor(n_neighbors=5) x_data,y_data=make_regression(n_features=1,n_informative=1,noise=50,random_state=1) x_train,x_test,y_train,y_test=train_test_split(x_data,y_data,random_state=1) kng.fit(x_train,y_train) prediction=kng.predict(x_test) kng_test_score=kng.score(x_test,y_test) kng_train_score=kng.score(x_train,y_train)
print(&#39;test data score:{:.2f}&#39;.format(kng_test_score)) 三，knn回归模型的优缺点 knn回归有两个重要的参数：最邻近数据点的数量k，数据点之间距离的度量方法。
在实践中，通常使用较小的k值，在knn分类中通常把k值设置为奇数，便于找到多数邻居的标签。默认的距离度量是欧式距离，它在多数情况下的效果都很好，除此之外，还有曼哈顿距离等，详细信息，请阅读《Scipy 学习第3篇：数字向量的距离计算》。
在确定knn回归或knn分类的k值时，可以通过折叠交叉验证来寻找最佳的k值，示例代码如下：
from sklearn import datasets from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import GridSearchCV #通过网络方式来获取参数 # 导入iris数据集 iris2=datasets." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/c458e5f5b0801940b07b5aba831f75e9/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-11-02T22:37:00+08:00" />
<meta property="article:modified_time" content="2020-11-02T22:37:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">回归模型 第5篇：knn回归</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="cnblogs_post_body" class="blogpost-body blogpost-body-html" style="font-size: 16px;"> 
 <p>基于最邻近算法的分类，本质上是对离散的数据标签进行预测，实际上，最邻近算法也可以用于对连续的数据标签进行预测，这种方法叫做基于最邻近数据的回归，预测的值(即数据的标签)是连续值，通过计算数据点最临近数据点平均值而获得预测值。</p> 
 <h3>一，sklearn的knn回归</h3> 
 <p>scikit-learn实现了两个不同的最邻近回归模型：</p> 
 <ul><li>KNeighborsRegressor：根据每个查询点的最邻近的k个数据点的均值作为预测值，其中，k是用户指定的整数。</li><li>RadiusNeighborsRegressor：基于查询点的固定半径内的数据点的均值作为预测值，其中r是用户指定的浮点值。</li></ul> 
 <p>回归模拟器的定义如下，该定义只列出最重要的参数，详细参数请参考sicikit-learn 官网：</p> 
 <div class="cnblogs_code"> 
  <pre><code class="has">sklearn.neighbors.KNeighborsRegressor(n_neighbors=5, weights='uniform', algorithm='auto', metric='minkowski',...)
sklearn.neighbors.RadiusNeighborsRegressor(radius=1.0, weights='uniform', algorithm='auto', metric='minkowski',...)</code>
</pre> 
 </div> 
 <p>参数注释：</p> 
 <ul><li>radius：寻找最邻近数据点的半径</li><li>n_neighbors：最邻近的邻居数量</li><li>algorithm：寻找最邻近的数据点的算法，有效值是['auto'，'ball_tree'，'kd_tree'，'brute']</li><li>metric：计算距离的度量，详细信息请查看：<a class="reference internal" title="sklearn.neighbors.DistanceMetric" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html#sklearn.neighbors.DistanceMetric" rel="nofollow"><code class="xref py py-class docutils literal notranslate">DistanceMetric</code></a> </li><li>weights：权重，默认值weights ='uniform'，为每个邻居分配统一的权重。 weights ='distance'分配的权重与距查询点的距离成反比。用于也可以提供定义函数来计算权重。在某些情况下，最好对邻居加权，以使较近的邻居对拟合的贡献更大，这可以通过weights关键字完成。</li></ul> 
 <p>最基本的最邻近回归使用统一的权重，也就是说，在特定范围中的每个数据点对查询点的分类(回归)的作用是相同的。在某些情况下，对权重点进行加权可能会比较有利，以使邻近的点比远离的点对回归的贡献更大，这可以通过weights关键字完成。默认值weights ='uniform'，为所有点分配相等的权重。 weights ='distance'分配的权重与距查询点的距离成反比。</p> 
 <h3>二，基于最邻近的数据点的数量来预测</h3> 
 <p>当使用knn计算某个数据点的预测值时，模型会从训练数据集中选择离该数据点最近的k个数据点，并且把它们的y值取均值，把该均值作为新数据点的预测值：</p> 
 <div class="cnblogs_code"> 
  <pre><code class="has">from sklearn.neighbors import KNeighborsRegressor</code>
</pre> 
 </div> 
 <p>对于knn分类，使用score方法评估模型，对于回归的问题，返回的是R^2分数，R^2分数也叫做决定系数，是回归模型预测的优度度量，位于0到1之间，R^2等于1对应完美预测，R^2等于0对应于常数模型，即总是预测训练集响应(y_train)的均值。</p> 
 <div class="cnblogs_code"> 
  <pre><code class="has">from sklearn.datasets import make_regression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import train_test_split

kng=KNeighborsRegressor(n_neighbors=5)

x_data,y_data=make_regression(n_features=1,n_informative=1,noise=50,random_state=1)
x_train,x_test,y_train,y_test=train_test_split(x_data,y_data,random_state=1)

kng.fit(x_train,y_train)
prediction=kng.predict(x_test)

kng_test_score=kng.score(x_test,y_test)
kng_train_score=kng.score(x_train,y_train)<br>
print('test data score:{:.2f}'.format(kng_test_score))</code>
</pre> 
 </div> 
 <h3>三，knn回归模型的优缺点</h3> 
 <p>knn回归有两个重要的参数：最邻近数据点的数量k，数据点之间距离的度量方法。</p> 
 <p>在实践中，通常使用较小的k值，在knn分类中通常把k值设置为奇数，便于找到多数邻居的标签。默认的距离度量是欧式距离，它在多数情况下的效果都很好，除此之外，还有曼哈顿距离等，详细信息，请阅读《<a href="https://www.cnblogs.com/ljhdo/p/13691232.html" rel="nofollow noopener noreferrer" target="_blank">Scipy 学习第3篇：数字向量的距离计算</a>》。</p> 
 <p>在确定knn回归或knn分类的k值时，可以通过折叠交叉验证来寻找最佳的k值，示例代码如下：</p> 
 <div class="cnblogs_code"> 
  <pre><code class="has">from sklearn import datasets
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV  #通过网络方式来获取参数

# 导入iris数据集
iris2=datasets.load_iris()
X2=iris2.data
y2=iris2.target
print(X2.shape,y2.shape)

# 设置需要搜索的K值，'n_neightbors'是sklearn中KNN的参数
parameters={<!-- -->'n_neightbors':[1,3,5,7,9,11,13,15]}
knn=KNeighborsClassifier()#注意：这里不用指定参数

# 通过GridSearchCV来搜索最好的K值。这个模块的内部其实就是对每一个K值进行评估
clf=GridSearchCV(knn,parameters,cv=5)  #5折
clf.fit(X2,y2)

# 输出最好的参数以及对应的准确率
print("最终最佳准确率：%.2f"%clf.<strong>best_score_</strong>,"最终的最佳K值",clf.<strong>best_params_</strong>)</code>
</pre> 
 </div> 
 <p>knn回归模型的优点之一是模型很容易理解，通常不需要过多的调参就可以得到不错的性能，并且构建模型的速度通常很快。但是使用knn算法时，对数据进行预处理是很重要的，对特征很多的数据集、对于大多数特征值都为0的数据集，效果往往不是很好。</p> 
 <p>虽然k邻近算法很容易理解，但是由于预测速度慢，且不能处理具有很多特征的数据集，所以，在实践中往往不会用到。</p> 
 <p>参考文档：</p> 
 <p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor" rel="nofollow noopener noreferrer" target="_blank">sklearn.neighbors.KNeighborsRegressor</a></p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8b5cd64bfb0d30b69407130572e296e1/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">类和对象-----引用和指向</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/73ea358a04f53c8ce9ce7882bd3a1efd/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">K8s中Pod的三种健康检查探针</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>