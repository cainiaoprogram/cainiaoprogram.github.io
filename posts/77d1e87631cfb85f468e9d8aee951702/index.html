<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision内容理解 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision内容理解" />
<meta property="og:description" content="ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision内容理解 一、Abstract二、引言三、背景介绍1、目前VLP模型框架介绍2、多模态交互框架3、Visual Embedding 框架 四、VILT模型1、模型总览2、预训练计划3、Whole Word Masking4、Image Augmentation5、模型结构图 五、实验1、总览：2、实施细节3、VLP模型的复杂度分析4、分类任务5、检索任务6、Ablation Study7. 常规的Visualization操作 六、结论及展望 写在前面 这篇文章作为一系列多模态理解的第一篇，从这里记录本人研究内容相关话题，期待后来者共同学习，共同进步，也顺便作为笔记备份。 文章链接：ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision代码：https://github.com/dandelin/vilt
发表于ICML2021, v2版本更精彩一些。本人看的是第一版，且第一版代码未放出。第二版多了代码部分&#43;region feature提取部分，推荐看一下~ 2022年7月29日 更新: 今天沐神和搭档朱老师在 B 站上面讲解了这篇论文，
链接：https://www.bilibili.com/video/BV14r4y1j74y 一、Abstract 摘要部分首先指出：当前大多数模型极度依赖于目标检测模型提取出的Feature进行VLP建模（不得不说，类似Faster-rcnn这种框架提取出的特征确实用起来精度高了很多），同时点明
缺点一：效率/速度；当然，肯定会慢很多，毋庸置疑，因为这种方法是将提取出的特征缓存到磁盘里面，再进行VLP训练。
缺点二：由于类似RPN这种候选区域生成器，也就是文中所说的visual enconde的出现&#43;监督训练过程中label 词汇的有限，就导致了其上界问题。
后面就是作者对自己模型ViLT的介绍了：完全去除掉conv操作，加速加速再加速。同时提及精度和某些模型相当或超过一些模型。
评语：点名缺点很正常，速度慢，但是精度高呀，你没法打败；
上界问题哪个模型都离不开image&#43;text呀
作者提出的这个模型，咋说，速度快，确实；但这个accuracy着实拉胯，和19年的MCAN比较额。
二、引言 引言部分描述了当前VLP模型的结构以及预训练的方法：MLM，ITM；
借势引出了Visual enocde的方法，基于region(传统)或者grid(2020年整活了)，这些需要conv，而作者提出conv-free的方法，在速度上更快。
最后作者提出文章的三点贡献：
1、运行速度快，参数少；
2、第一个不使用卷积就能达到相当不错的精度;
3、首次用实验表明了whole word masking and image augmentations针对下游任务的有效性。
三、背景介绍 1、目前VLP模型框架介绍 喏，就是下面这张图了点出了目前VLP的框架结构：
a图是19年及之前的方法，后来多模态交互部分都改成transformer了；
b图暂时还没认真看到过，因为大多数的Text Embed都是直接调用nn.embedding模块实现；
c图是目前自transformer出来之后的主流方法，精度贼高，但速度慢；
d图是目前作者提出的方法。速度快，但精度差。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/77d1e87631cfb85f468e9d8aee951702/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-29T09:31:53+08:00" />
<meta property="article:modified_time" content="2022-07-29T09:31:53+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision内容理解</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night-eighties">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision内容理解</h4> 
 <ul><li><a href="#Abstract_9" rel="nofollow">一、Abstract</a></li><li><a href="#_22" rel="nofollow">二、引言</a></li><li><a href="#_30" rel="nofollow">三、背景介绍</a></li><li><ul><li><a href="#1VLP_31" rel="nofollow">1、目前VLP模型框架介绍</a></li><li><a href="#2_43" rel="nofollow">2、多模态交互框架</a></li><li><a href="#3Visual_Embedding__45" rel="nofollow">3、Visual Embedding 框架</a></li></ul> 
  </li><li><a href="#VILT_51" rel="nofollow">四、VILT模型</a></li><li><ul><li><a href="#1_52" rel="nofollow">1、模型总览</a></li><li><a href="#2_56" rel="nofollow">2、预训练计划</a></li><li><a href="#3Whole_Word_Masking_58" rel="nofollow">3、Whole Word Masking</a></li><li><a href="#4Image_Augmentation_60" rel="nofollow">4、Image Augmentation</a></li><li><a href="#5_62" rel="nofollow">5、模型结构图</a></li></ul> 
  </li><li><a href="#_64" rel="nofollow">五、实验</a></li><li><ul><li><a href="#1_65" rel="nofollow">1、总览：</a></li><li><a href="#2_70" rel="nofollow">2、实施细节</a></li><li><a href="#3VLP_78" rel="nofollow">3、VLP模型的复杂度分析</a></li><li><a href="#4_80" rel="nofollow">4、分类任务</a></li><li><a href="#5_85" rel="nofollow">5、检索任务</a></li><li><a href="#6Ablation_Study_88" rel="nofollow">6、Ablation Study</a></li><li><a href="#7_Visualization_99" rel="nofollow">7. 常规的Visualization操作</a></li></ul> 
  </li><li><a href="#_101" rel="nofollow">六、结论及展望</a></li></ul> 
</div> 
<br> 
<strong>写在前面</strong> 
<br> 这篇文章作为一系列多模态理解的第一篇，从这里记录本人研究内容相关话题，期待后来者共同学习，共同进步，也顺便作为笔记备份。 
<p></p> 
<ul><li>文章链接：<a href="https://arxiv.org/abs/2102.03334" rel="nofollow">ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision</a></li><li>代码：<a href="https://github.com/dandelin/vilt">https://github.com/dandelin/vilt</a><br> 发表于ICML2021, v2版本更精彩一些。本人看的是第一版，且第一版代码未放出。第二版多了代码部分+region feature提取部分，推荐看一下~</li></ul> 
<p><strong>2022年7月29日 更新</strong>: 今天沐神和搭档朱老师在 B 站上面讲解了这篇论文，</p> 
<ul><li>链接：<a href="https://www.bilibili.com/video/BV14r4y1j74y" rel="nofollow">https://www.bilibili.com/video/BV14r4y1j74y</a></li></ul> 
<h2><a id="Abstract_9"></a>一、Abstract</h2> 
<p><img src="https://images2.imgbox.com/84/8d/HMyD9oJi_o.png" alt="在这里插入图片描述"><br> 摘要部分首先指出：当前大多数模型极度依赖于目标检测模型提取出的Feature进行VLP建模（不得不说，类似Faster-rcnn这种框架提取出的特征确实用起来精度高了很多），同时点明</p> 
<p><mark>缺点一</mark>：效率/速度；当然，肯定会慢很多，毋庸置疑，因为这种方法是将提取出的特征缓存到磁盘里面，再进行VLP训练。</p> 
<p><mark>缺点二</mark>：由于类似RPN这种候选区域生成器，也就是文中所说的visual enconde的出现+监督训练过程中label 词汇的有限，就导致了其上界问题。</p> 
<p>后面就是作者对自己模型ViLT的介绍了：完全去除掉conv操作，加速加速再加速。同时提及精度和某些模型相当或超过一些模型。</p> 
<p><strong>评语</strong>：点名缺点很正常，速度慢，但是精度高呀，你没法打败；<br> 上界问题哪个模型都离不开image+text呀<br> 作者提出的这个模型，咋说，速度快，确实；但这个accuracy着实拉胯，和19年的MCAN比较额。</p> 
<h2><a id="_22"></a>二、引言</h2> 
<p>引言部分描述了当前VLP模型的结构以及预训练的方法：MLM，ITM；<br> 借势引出了Visual enocde的方法，基于region(传统)或者grid(2020年整活了)，这些需要conv，而作者提出conv-free的方法，在速度上更快。</p> 
<p>最后作者提出文章的<strong>三点贡献</strong>：<br> 1、运行速度快，参数少；<br> 2、第一个不使用卷积就能达到相当不错的精度;<br> 3、首次用实验表明了<mark>whole word masking and image augmentations</mark>针对下游任务的有效性。</p> 
<h2><a id="_30"></a>三、背景介绍</h2> 
<h3><a id="1VLP_31"></a>1、目前VLP模型框架介绍</h3> 
<p>喏，就是下面这张图了点出了目前VLP的框架结构：<br> <img src="https://images2.imgbox.com/2e/7f/aiMbOYat_o.png" alt="在这里插入图片描述"><br> a图是19年及之前的方法，后来多模态交互部分都改成transformer了；</p> 
<p>b图暂时还没认真看到过，因为大多数的Text Embed都是直接调用nn.embedding模块实现；</p> 
<p>c图是目前自transformer出来之后的主流方法，精度贼高，但速度慢；</p> 
<p>d图是目前作者提出的方法。速度快，但精度差。</p> 
<p>但是这几种方法都没考虑transformer需要数据的问题，文章作者采用64块GPU，一个batch4096，我等一般人搞不起。</p> 
<h3><a id="2_43"></a>2、多模态交互框架</h3> 
<p>目前主要分为<mark>单流和双流</mark>模型，由于transformer的出现，双流模型以及快被淘汰了，主要是单流模型，作者也采用的是单流模型。</p> 
<h3><a id="3Visual_Embedding__45"></a>3、Visual Embedding 框架</h3> 
<p>三种：<br> 1、基于<mark>region</mark>特征的,采用Resnet backbone提取特征，然后NMS操作，最后ROIhead；<br> 2、基于<mark>grid</mark>特征的，类似于直接采用resnet-152这种直接提取特征的<br> 3、基于<mark>patch</mark>特征的，将图像切成块，然后flatten，之后全连接层，变成VLP的输入。<br> 作者采用的模型结构就是这种基于patch的。</p> 
<h2><a id="VILT_51"></a>四、VILT模型</h2> 
<h3><a id="1_52"></a>1、模型总览</h3> 
<p>这一部分作者概述了自己模型的一个流程，并用公式总结出来：<br> <img src="https://images2.imgbox.com/b1/cb/vFrrmkbU_o.png" alt="在这里插入图片描述"><br> 作者还提及了一些参数设置啥的，基本上和bert模型差不多，有一些区别在于LN的位置。</p> 
<h3><a id="2_56"></a>2、预训练计划</h3> 
<p>与bert模型类似，作者采用MLM和ITM这两种预训练方式</p> 
<h3><a id="3Whole_Word_Masking_58"></a>3、Whole Word Masking</h3> 
<p>这个是作者强调使用的另外一种，与MLM不同的是，直接MASK整个词。</p> 
<h3><a id="4Image_Augmentation_60"></a>4、Image Augmentation</h3> 
<p>这是作者说明的另一种图像增强方式，采用高斯模糊这种操作来增强模型的泛化能力(现在有很多文章专门在针对这个泛化搞事情了)。</p> 
<h3><a id="5_62"></a>5、模型结构图</h3> 
<p><img src="https://images2.imgbox.com/e3/c5/nyeJ1HRw_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_64"></a>五、实验</h2> 
<h3><a id="1_65"></a>1、总览：</h3> 
<p>4个数据集：Microsoft COCO(MSCOCO), Visual Genome(VG), SBU Captions(SBU), Google Conceptual Caption<br> 2种验证：<br> VQAv2+NLVR2<br> (MSCOCO+Flick30K)</p> 
<h3><a id="2_70"></a>2、实施细节</h3> 
<p>Adam+lr warmed up<br> 图像尺寸的调整：<br> 一般的VLP模型：800x1333，作者将其限定为384x640,产生12x20=240个patches, 另外将patches填充到length=200用于batch training.<br> 需要注意的是相比于800x1333小了4倍。<br> 在预训练时并未使用bert的权重，而是scratch；<br> 迭代100K，64块V100，3天~，batch=4096，一般人哪能这么干[抠鼻]<br> 在下游任务中训练10个epoch，batch=256/158</p> 
<h3><a id="3VLP_78"></a>3、VLP模型的复杂度分析</h3> 
<p>主要强调一些参数量以及运行时间+输入bert模型的tokens长度</p> 
<h3><a id="4_80"></a>4、分类任务</h3> 
<p>VQAv2.0+NLVRv2<br> VQAv2:3129个vocabulary,仅仅采用VQAv2数据集进行微调，而其他模型有的采用了VG-QA模型<br> NLVR:Natural Language for Visual Reasoning<br> 给两幅图片和一句话，判断哪个符合。</p> 
<h3><a id="5_85"></a>5、检索任务</h3> 
<p>MSCOCO+F30K<br> Image-to-text, text-to-image retrieval</p> 
<h3><a id="6Ablation_Study_88"></a>6、Ablation Study</h3> 
<p><img src="https://images2.imgbox.com/f8/5c/bYnZT5cF_o.png" alt="在这里插入图片描述"><br> 主要验证：<br> 1、训练steps，也就是传说中的迭代次数；<br> 越多越好<br> 2、有无WM<br> 有肯定好呀<br> 3、有无MPP<br> 没啥用<br> 4、有无RandAugment<br> 可以，很强</p> 
<h3><a id="7_Visualization_99"></a>7. 常规的Visualization操作</h3> 
<p><img src="https://images2.imgbox.com/f4/ef/XV2pEcxi_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_101"></a>六、结论及展望</h2> 
<p>总结一下本文说了啥，以及未来可能要干啥<br> 离谱的是批评了搞unimodal embedders就像是在搞军备竞赛，呼吁大家不要这样干，🐮<br> 展望：<br> 1、在拓展性上要增强，采用大尺度的数据集，建立ViLT的变体模型+ 大量vision-and-language datasets；<br> 2、Mask Modeling操作<br> 由于MPP效果不咋地，可以在这方面努力；<br> 3、Augmentation Strategies策略<br> 突出了高斯模糊的重要性。</p> 
<p>推荐另一篇zhihu文章：<a href="https://zhuanlan.zhihu.com/p/369733979?utm_medium=social&amp;utm_oi=710944035610562560" rel="nofollow">ViLT：最简单的多模态Transformer</a></p> 
<p>Ok，完美结束，吃完兰州牛肉面，晚上撸代码~</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/07693e64cf0ceba56feceda5a35f1ec4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">视觉学习笔记4——ORB-SLAM3的地图保存与使用</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7026139a86d3386366e5bc9f52091791/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">隐马尔可夫模型基础介绍</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>