<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>特征工程之特征选择 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="特征工程之特征选择" />
<meta property="og:description" content="在前一篇文章中我介绍了一些数据预处理的方法，原始数据在经过预处理之后可以被算法处理了，但是实际中可能有一些特征是没有必要的，比如在中国采集的一些数据，那么国籍就都是中国，其实也就没有意义了，反映在统计量上就是方差过小，也就是样本在这个特征上变化很小。还有一种情况是特征和最后的结果相关性很小，也就是这个特征不起作用，衡量这种相关性我们可以用卡方检验，F-检验以及互信息等。其实很多sklearn的算法本身带有coef_和feature_importance_属性，而这个属性就可以被利用来筛选特征。前面说过的方法其实都是在已有特征的基础上排除特征的方法，但是在实际中我们很多时候需要自己构造特征，构造出好的特征可以大大提升模型的性能，对于这方面我就不是很了解了，毕竟我也是个新手，没有太多经验。
根据方差移除特征 sklearn中VarianceThreshold可以起到这个作用
from sklearn.feature_selection import VarianceThreshold sel = VarianceThreshold(0.2) sel.fit_transform(data) 上述代码起到移除方差小于0.2的特征的作用（只起到示意作用）。
单一变量特征选择 单一变量选择就是通过某种得分来度量相关性，进而选择特征，sklearn中有两个比较常用 . SelectKBest ：选择前k个最好的特征，也就是得分最高的K个 . SelectPercentile :选择前百分之几的特征，这个百分比由用户指定
所以从上面就可以看出必须要有一个方法来衡量这种相关性，来传给上面的两个方法，才能够做出选择。衡量方法也就是上面提到的，卡方检验，F-检验以及互信息。这几种方法在sklearn中是有专门的实现的，再进行单一变量特征选择的时候将他们作为参数传递进去。下面以卡方检验为例
&gt;&gt;&gt; from sklearn.datasets import load_iris &gt;&gt;&gt; from sklearn.feature_selection import SelectKBest &gt;&gt;&gt; from sklearn.feature_selection import chi2 &gt;&gt;&gt; iris = load_iris() &gt;&gt;&gt; X, y = iris.data, iris.target &gt;&gt;&gt; X.shape (150, 4) &gt;&gt;&gt; X_new = SelectKBest(chi2, k=2).fit_transform(X, y) &gt;&gt;&gt; X_new.shape (150, 2) 作为参数的打分的函数对于回归和分类是不同的： -回归：f_regression, mutual_info_regression -分类：chi2, f_classif, mutual_info_classif
值得注意的是互信息（mutual_info_regression， mutual_info_classif）可以得到特征和最后的结果之间的非线性的相关性，而卡方检验和F-检验应该只能够判断线性性。互信息的公式是 I(X;Y)=∑x∈X∑y∈Yp(x,y)p(x,y)p(x)p(y) I ( X ; Y ) = ∑ x ∈ X ∑ y ∈ Y p ( x , y ) p ( x , y ) p ( x ) p ( y ) 递归特征消除 递归特征消除（RFE）很好理解，给定一个模型，要求这个模型要能够给出coef_或者feature_importance_，然后我们就能够根据训练的这些相关性特征删除得分最差的特征，然后再一次训练，重复这个过程，直到最后满足我们预设的特征个数。还可以用RFECV通过交叉验证找出最佳的特征个数。下面是一个示例。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/22635042b6ed6519200b057b3947f2b0/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-01-27T22:00:43+08:00" />
<meta property="article:modified_time" content="2018-01-27T22:00:43+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">特征工程之特征选择</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>在前一篇文章中我介绍了一些数据预处理的方法，原始数据在经过预处理之后可以被算法处理了，但是实际中可能有一些特征是没有必要的，比如在中国采集的一些数据，那么国籍就都是中国，其实也就没有意义了，反映在统计量上就是方差过小，也就是样本在这个特征上变化很小。还有一种情况是特征和最后的结果相关性很小，也就是这个特征不起作用，衡量这种相关性我们可以用卡方检验，F-检验以及互信息等。其实很多sklearn的算法本身带有coef_和feature_importance_属性，而这个属性就可以被利用来筛选特征。前面说过的方法其实都是在已有特征的基础上排除特征的方法，但是在实际中我们很多时候需要自己构造特征，构造出好的特征可以大大提升模型的性能，对于这方面我就不是很了解了，毕竟我也是个新手，没有太多经验。</p> 
<h2 id="根据方差移除特征">根据方差移除特征</h2> 
<p>sklearn中VarianceThreshold可以起到这个作用</p> 
<pre class="prettyprint"><code class=" hljs haskell"><span class="hljs-title">from</span> sklearn.feature_selection <span class="hljs-import"><span class="hljs-keyword">import</span> VarianceThreshold</span>
<span class="hljs-title">sel</span> = <span class="hljs-type">VarianceThreshold</span>(<span class="hljs-number">0.2</span>)
<span class="hljs-title">sel</span>.fit_transform(<span class="hljs-typedef"><span class="hljs-keyword">data</span>)</span></code></pre> 
<p>上述代码起到移除方差小于0.2的特征的作用（只起到示意作用）。</p> 
<h2 id="单一变量特征选择">单一变量特征选择</h2> 
<p>单一变量选择就是通过某种得分来度量相关性，进而选择特征，sklearn中有两个比较常用 <br> . SelectKBest ：选择前k个最好的特征，也就是得分最高的K个 <br> . SelectPercentile :选择前百分之几的特征，这个百分比由用户指定</p> 
<p>所以从上面就可以看出必须要有一个方法来衡量这种相关性，来传给上面的两个方法，才能够做出选择。衡量方法也就是上面提到的，卡方检验，F-检验以及互信息。这几种方法在sklearn中是有专门的实现的，再进行单一变量特征选择的时候将他们作为参数传递进去。下面以卡方检验为例</p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-prompt">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris
<span class="hljs-prompt">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectKBest
<span class="hljs-prompt">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> chi2
<span class="hljs-prompt">&gt;&gt;&gt; </span>iris = load_iris()
<span class="hljs-prompt">&gt;&gt;&gt; </span>X, y = iris.data, iris.target
<span class="hljs-prompt">&gt;&gt;&gt; </span>X.shape
(<span class="hljs-number">150</span>, <span class="hljs-number">4</span>)
<span class="hljs-prompt">&gt;&gt;&gt; </span>X_new = SelectKBest(chi2, k=<span class="hljs-number">2</span>).fit_transform(X, y)
<span class="hljs-prompt">&gt;&gt;&gt; </span>X_new.shape
(<span class="hljs-number">150</span>, <span class="hljs-number">2</span>)
</code></pre> 
<p>作为参数的打分的函数对于回归和分类是不同的： <br> -回归：f_regression, mutual_info_regression <br> -分类：chi2, f_classif, mutual_info_classif</p> 
<p>值得注意的是互信息（mutual_info_regression， mutual_info_classif）可以得到特征和最后的结果之间的非线性的相关性，而卡方检验和F-检验应该只能够判断线性性。互信息的公式是 <br> <span class="MathJax_Preview" style="color: inherit; display: none;"></span></p> 
<div class="MathJax_Display" style="text-align: center;"> 
 <span tabindex="0" class="MathJax" id="MathJax-Element-37-Frame" style="text-align: center; position: relative;"> 
   
   <span class="math" id="MathJax-Span-818" style="width: 18.35em; display: inline-block;"><span style="width: 14.65em; height: 0px; font-size: 125%; display: inline-block; position: relative;"><span style="left: 0em; top: -2.35em; position: absolute; clip: rect(0.73em, 1014.65em, 3.84em, -1000em);"><span class="mrow" id="MathJax-Span-819"><span class="mi" id="MathJax-Span-820" style="font-family: MathJax_Math; font-style: italic;">I<span style="width: 0.06em; height: 1px; overflow: hidden; display: inline-block;"></span></span><span class="mo" id="MathJax-Span-821" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-822" style="font-family: MathJax_Math; font-style: italic;">X<span style="width: 0.02em; height: 1px; overflow: hidden; display: inline-block;"></span></span><span class="mo" id="MathJax-Span-823" style="font-family: MathJax_Main;">;</span><span class="mi" id="MathJax-Span-824" style="padding-left: 0.16em; font-family: MathJax_Math; font-style: italic;">Y<span style="width: 0.18em; height: 1px; overflow: hidden; display: inline-block;"></span></span><span class="mo" id="MathJax-Span-825" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-826" style="padding-left: 0.27em; font-family: MathJax_Main;">=</span><span class="munderover" id="MathJax-Span-827" style="padding-left: 0.27em;"><span style="width: 1.47em; height: 0px; display: inline-block; position: relative;"><span style="left: 0.01em; top: -4em; position: absolute; clip: rect(2.9em, 1001.38em, 4.6em, -1000em);"><span class="mo" id="MathJax-Span-828" style="font-family: MathJax_Size2; vertical-align: 0em;">∑</span><span style="width: 0px; height: 4em; display: inline-block;"></span></span><span style="left: 0em; top: -2.9em; position: absolute; clip: rect(3.36em, 1001.48em, 4.27em, -1000em);"><span class="texatom" id="MathJax-Span-829"><span class="mrow" id="MathJax-Span-830"><span class="mi" id="MathJax-Span-831" style="font-family: MathJax_Math; font-size: 70.7%; font-style: italic;">x</span><span class="mo" id="MathJax-Span-832" style="font-family: MathJax_Main; font-size: 70.7%;">∈</span><span class="mi" id="MathJax-Span-833" style="font-family: MathJax_Math; font-size: 70.7%; font-style: italic;">X<span style="width: 0.01em; height: 1px; overflow: hidden; display: inline-block;"></span></span></span></span><span style="width: 0px; height: 4em; display: inline-block;"></span></span></span></span><span class="munderover" id="MathJax-Span-834" style="padding-left: 0.16em;"><span style="width: 1.44em; height: 0px; display: inline-block; position: relative;"><span style="left: 0em; top: -4em; position: absolute; clip: rect(2.9em, 1001.38em, 4.6em, -1000em);"><span class="mo" id="MathJax-Span-835" style="font-family: MathJax_Size2; vertical-align: 0em;">∑</span><span style="width: 0px; height: 4em; display: inline-block;"></span></span><span style="left: 0.04em; top: -2.9em; position: absolute; clip: rect(3.36em, 1001.36em, 4.39em, -1000em);"><span class="texatom" id="MathJax-Span-836"><span class="mrow" id="MathJax-Span-837"><span class="mi" id="MathJax-Span-838" style="font-family: MathJax_Math; font-size: 70.7%; font-style: italic;">y<span style="width: 0em; height: 1px; overflow: hidden; display: inline-block;"></span></span><span class="mo" id="MathJax-Span-839" style="font-family: MathJax_Main; font-size: 70.7%;">∈</span><span class="mi" id="MathJax-Span-840" style="font-family: MathJax_Math; font-size: 70.7%; font-style: italic;">Y<span style="width: 0.12em; height: 1px; overflow: hidden; display: inline-block;"></span></span></span></span><span style="width: 0px; height: 4em; display: inline-block;"></span></span></span></span><span class="mi" id="MathJax-Span-841" style="padding-left: 0.16em; font-family: MathJax_Math; font-style: italic;">p</span><span class="mo" id="MathJax-Span-842" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-843" style="font-family: MathJax_Math; font-style: italic;">x</span><span class="mo" id="MathJax-Span-844" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-845" style="padding-left: 0.16em; font-family: MathJax_Math; font-style: italic;">y<span style="width: 0em; height: 1px; overflow: hidden; display: inline-block;"></span></span><span class="mo" id="MathJax-Span-846" style="font-family: MathJax_Main;">)</span><span class="mfrac" id="MathJax-Span-847"><span style="width: 3.75em; height: 0px; margin-right: 0.12em; margin-left: 0.12em; display: inline-block; position: relative;"><span style="left: 50%; top: -4.71em; margin-left: -1.39em; position: absolute; clip: rect(3.1em, 1002.69em, 4.4em, -1000em);"><span class="mrow" id="MathJax-Span-848"><span class="mi" id="MathJax-Span-849" style="font-family: MathJax_Math; font-style: italic;">p</span><span class="mo" id="MathJax-Span-850" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-851" style="font-family: MathJax_Math; font-style: italic;">x</span><span class="mo" id="MathJax-Span-852" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-853" style="padding-left: 0.16em; font-family: MathJax_Math; font-style: italic;">y<span style="width: 0em; height: 1px; overflow: hidden; display: inline-block;"></span></span><span class="mo" id="MathJax-Span-854" style="font-family: MathJax_Main;">)</span></span><span style="width: 0px; height: 4em; display: inline-block;"></span></span><span style="left: 50%; top: -3.28em; margin-left: -1.81em; position: absolute; clip: rect(3.1em, 1003.53em, 4.4em, -1000em);"><span class="mrow" id="MathJax-Span-855"><span class="mi" id="MathJax-Span-856" style="font-family: MathJax_Math; font-style: italic;">p</span><span class="mo" id="MathJax-Span-857" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-858" style="font-family: MathJax_Math; font-style: italic;">x</span><span class="mo" id="MathJax-Span-859" style="font-family: MathJax_Main;">)</span><span class="mi" id="MathJax-Span-860" style="font-family: MathJax_Math; font-style: italic;">p</span><span class="mo" id="MathJax-Span-861" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-862" style="font-family: MathJax_Math; font-style: italic;">y<span style="width: 0em; height: 1px; overflow: hidden; display: inline-block;"></span></span><span class="mo" id="MathJax-Span-863" style="font-family: MathJax_Main;">)</span></span><span style="width: 0px; height: 4em; display: inline-block;"></span></span><span style="left: 0em; top: -1.27em; position: absolute; clip: rect(0.83em, 1003.75em, 1.2em, -1000em);"><span style="width: 3.75em; height: 0px; overflow: hidden; vertical-align: 0em; border-top-color: currentColor; border-top-width: 1.3px; border-top-style: solid; display: inline-block;"></span><span style="width: 0px; height: 1.05em; display: inline-block;"></span></span></span></span></span><span style="width: 0px; height: 2.35em; display: inline-block;"></span></span></span><span style="width: 0px; height: 3.63em; overflow: hidden; vertical-align: -1.74em; border-left-color: currentColor; border-left-width: 0px; border-left-style: solid; display: inline-block;"></span></span> 
  <span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block"> 
    
    
      I 
     
    
      ( 
     
    
      X 
     
    
      ; 
     
    
      Y 
     
    
      ) 
     
    
      = 
     
     
     
       ∑ 
      
      
      
        x 
       
      
        ∈ 
       
      
        X 
       
      
     
     
     
       ∑ 
      
      
      
        y 
       
      
        ∈ 
       
      
        Y 
       
      
     
    
      p 
     
    
      ( 
     
    
      x 
     
    
      , 
     
    
      y 
     
    
      ) 
     
     
      
      
        p 
       
      
        ( 
       
      
        x 
       
      
        , 
       
      
        y 
       
      
        ) 
       
      
      
      
        p 
       
      
        ( 
       
      
        x 
       
      
        ) 
       
      
        p 
       
      
        ( 
       
      
        y 
       
      
        ) 
       
      
     
   </span></span> 
</div><script id="MathJax-Element-37" type="math/tex; mode=display">I(X;Y) = \sum_{x\in X}\sum_{y \in Y} p(x,y) \frac{p(x,y)}{p(x)p(y)}</script> 
<p></p> 
<h2 id="递归特征消除">递归特征消除</h2> 
<p>递归特征消除（RFE）很好理解，给定一个模型，要求这个模型要能够给出coef_或者feature_importance_，然后我们就能够根据训练的这些相关性特征删除得分最差的特征，然后再一次训练，重复这个过程，直到最后满足我们预设的特征个数。还可以用RFECV通过交叉验证找出最佳的特征个数。下面是一个示例。</p> 
<pre class="prettyprint"><code class=" hljs avrasm">from sklearn<span class="hljs-preprocessor">.feature</span>_selection import RFE
from sklearn<span class="hljs-preprocessor">.linear</span>_model import LogisticRegression

<span class="hljs-preprocessor">#递归特征消除法，返回特征选择后的数据</span>
<span class="hljs-preprocessor">#参数estimator为基模型</span>
<span class="hljs-preprocessor">#参数n_features_to_select为选择的特征个数</span>
RFE(estimator=LogisticRegression(), n_features_to_select=<span class="hljs-number">2</span>)<span class="hljs-preprocessor">.fit</span>_transform(iris<span class="hljs-preprocessor">.data</span>, iris<span class="hljs-preprocessor">.target</span>)
</code></pre> 
<h2 id="selectfrommodel">SelectFromModel</h2> 
<p>SlectFormModel也就是从模型中选择，感觉上和上面的递归特征消除有点相似，它是一个“元转换器”，可以和任何带有coef_或者feature_importance_的模型一起使用。另外我们会指定一个参数“阈值”，小于这个阈值的特征被认为是不重要的，并且会被移除，不像是RFE会反复地拟合模型每次移除一个。大致用法如下</p> 
<pre class="prettyprint"><code class=" hljs avrasm">clf = LassoCV()

<span class="hljs-preprocessor"># Set a minimum threshold of 0.25</span>
sfm = SelectFromModel(clf, threshold=<span class="hljs-number">0.25</span>)
sfm<span class="hljs-preprocessor">.fit</span>(<span class="hljs-built_in">X</span>, <span class="hljs-built_in">y</span>)
n_features = sfm<span class="hljs-preprocessor">.transform</span>(<span class="hljs-built_in">X</span>)<span class="hljs-preprocessor">.shape</span>[<span class="hljs-number">1</span>]</code></pre> 
<p>上面用的是LassoCV结合SelectFromModel进行特征选择。</p> 
<h2 id="基于l1范数的特征选择">基于L1范数的特征选择</h2> 
<p>带有L1惩罚项的线性模型容易产生稀疏解，那么我们可以让SelectFromModel和这种模型相结合，最后选择相关系数不为0的特征就行。一般的线性模型有Lasso,LogisiticRegression以及LinearSVC</p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-prompt">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> LinearSVC
<span class="hljs-prompt">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris
<span class="hljs-prompt">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectFromModel
<span class="hljs-prompt">&gt;&gt;&gt; </span>iris = load_iris()
<span class="hljs-prompt">&gt;&gt;&gt; </span>X, y = iris.data, iris.target
<span class="hljs-prompt">&gt;&gt;&gt; </span>X.shape
(<span class="hljs-number">150</span>, <span class="hljs-number">4</span>)
<span class="hljs-prompt">&gt;&gt;&gt; </span>lsvc = LinearSVC(C=<span class="hljs-number">0.01</span>, penalty=<span class="hljs-string">"l1"</span>, dual=<span class="hljs-keyword">False</span>).fit(X, y)
<span class="hljs-prompt">&gt;&gt;&gt; </span>model = SelectFromModel(lsvc, prefit=<span class="hljs-keyword">True</span>)
<span class="hljs-prompt">&gt;&gt;&gt; </span>X_new = model.transform(X)
<span class="hljs-prompt">&gt;&gt;&gt; </span>X_new.shape
(<span class="hljs-number">150</span>, <span class="hljs-number">3</span>)</code></pre> 
<p>对于SVC和LogisticRegression越小的C，越少的特征被选择。对于Lasso，越大的alpha越少的特征被选择。具体是什么原因，我也不知道。</p> 
<h2 id="基于树的特征选择">基于树的特征选择</h2> 
<p>这个方法的原理也是类似的，因为树在生成过程中一定也会选择特征。回忆一下决策树，不论是ID3，C4.5还是CART开始的时候都是先找一个最能够区分数据集的特征，CART是二叉树，第一步要找出最优特征和最优切分点，之后不断循环。这个过程不就是筛选出最优特征的过程吗？不过在应用中，代码还是和上面类似，和SelectFromModel已启用。</p> 
<pre class="prettyprint"><code class=" hljs r">&gt;&gt;&gt; from sklearn.ensemble import ExtraTreesClassifier
&gt;&gt;&gt; from sklearn.datasets import load_iris
&gt;&gt;&gt; from sklearn.feature_selection import SelectFromModel
&gt;&gt;&gt; iris = load_iris()
&gt;&gt;&gt; X, y = iris.data, iris.target
&gt;&gt;&gt; X.shape
&gt;(<span class="hljs-number">150</span>, <span class="hljs-number">4</span>)
&gt;&gt;&gt; clf = ExtraTreesClassifier()
&gt;&gt;&gt; clf = clf.fit(X, y)
&gt;&gt;&gt; clf.feature_importances_
array([ <span class="hljs-number">0.04</span><span class="hljs-keyword">...</span>, <span class="hljs-number">0.05</span><span class="hljs-keyword">...</span>, <span class="hljs-number">0.4</span><span class="hljs-keyword">...</span>, <span class="hljs-number">0.4</span><span class="hljs-keyword">...</span>])
&gt;&gt;&gt; model = SelectFromModel(clf, prefit=True)
&gt;&gt;&gt; X_new = model.transform(X)
&gt;&gt;&gt; X_new.shape
(<span class="hljs-number">150</span>, <span class="hljs-number">2</span>)</code></pre> 
<p>以上是一些特征筛选的方法，另外还有特征的构造，但本人经验不足，在此就略去这方面的论述了。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/320eeb0ba88a72bc6dcd473050a6b937/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【LeetCode】696. Count Binary Substrings 解题报告（Python）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0ec9eab1f9e591c8ad2f3fbcac2342f0/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">python opencv-批量调整图片的曝光率</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>