<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>YOLO v3网络结构分析 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="YOLO v3网络结构分析" />
<meta property="og:description" content="1.Darknet-53 模型结构
在论文中虽然有给网络的图，但我还是简单说一下。这个网络主要是由一系列的1x1和3x3的卷积层组成（每个卷积层后都会跟一个BN层和一个LeakyReLU)层，作者说因为网络中有53个convolutional layers，所以叫做Darknet-53（2 &#43; 1*2 &#43; 1 &#43; 2*2 &#43; 1 &#43; 8*2 &#43; 1 &#43; 8*2 &#43; 1 &#43; 4*2 &#43; 1 = 53 按照顺序数，最后的Connected是全连接层也算卷积层，一共53个）。下图就是Darknet-53的结构图，在右侧标注了一些信息方便理解。（卷积的strides默认为（1，1），padding默认为same，当strides为（2，2）时padding为valid）
其网络结构能分为三部分。
第一部分：图片的输入
第二部分：基础网络。作者原文使用的是 Darkjnet-53 without FC layer。
第三部分：yolo v3的三个分支。y1（13x13x255）,y2（26x26x255）,y3(52x52x255)。
Y1从上面结点获取。是最高层的，最抽象的特征。适合比较大的目标检测；
Y2是在Y1的基础上做一个上采样&#43;一个低层的一个特征进行连接（concat）。
适合中等的目标检测；
Y3是在Y2的基础上做一个上采样&#43;一个更低层的一个特征进行连接（concat）。适合小 目标的检测。
三个尺度的输出皆是奇数，使得网格会有个中心位置。
通道数255：{[80(coco数据集共有80类)&#43;[X&#43;Y&#43;W&#43;H&#43;Confiderce]}×3(anchor) = 255
CBL为卷积块：由conv2d，Batch Normalization，Leaky relu 这三个网络层组成。
Res为残差块：包含两个卷积块和一个add层，add层只是将相同维度的张量进行相加。
ResX为多个残差块,X代表残差块的个数。
上采样；将特征图的宽和高变成原来的两倍，通道数不变。
Concat张量拼接：相同宽和高的通道数相加。
具体来说（基于YOLOV3）
1：输入一张任意大小图片，保持长宽比不变的情况下，缩放至 w 或 h 达到416，再覆盖在416*416的新图上，作为网络的输入。即网络的输入是一张416*416，3通道的RGB图。
2：运行网络。YOLO的CNN网络把图片分成 S*S 个网格（yolov3多尺度预测，输出3层，每层 S * S个网格，分别为 13*13 ，26 *26 ，52*52），然后每个单元格负责去检测那些中心点落在该格子内的目标，如图二所示。 每个单元格需要预测 3*（4&#43;1&#43;B）个值。如果将输入图片划分为 S*S 网格，那么每层最终预测值为 S*S*3*(4&#43;1&#43;B) 大小的张量。 B：类别数（coco集为80类），即B=80." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/6696935f29492c41b28cba1c6a8d7294/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-10-22T01:03:28+08:00" />
<meta property="article:modified_time" content="2021-10-22T01:03:28+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">YOLO v3网络结构分析</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>1.Darknet-53 模型结构<br> 在论文中虽然有给网络的图，但我还是简单说一下。这个网络主要是由一系列的1x1和3x3的卷积层组成（每个卷积层后都会跟一个BN层和一个LeakyReLU)层，作者说因为网络中有53个convolutional layers，所以叫做Darknet-53（2 + 1*2 + 1 + 2*2 + 1 + 8*2 + 1 + 8*2 + 1 + 4*2 + 1 = 53 按照顺序数，最后的Connected是全连接层也算卷积层，一共53个）。下图就是Darknet-53的结构图，在右侧标注了一些信息方便理解。（卷积的strides默认为（1，1），padding默认为same，当strides为（2，2）时padding为valid）</p> 
<p> <img alt="" src="https://images2.imgbox.com/f7/55/xLdEgNnI_o.png"></p> 
<p></p> 
<p><img alt="" src="https://images2.imgbox.com/9b/31/RNUu3hcC_o.jpg"> </p> 
<p> </p> 
<p>其网络结构能分为三部分。<br> 第一部分：图片的输入<br> 第二部分：基础网络。作者原文使用的是 Darkjnet-53 without FC layer。<br> 第三部分：yolo v3的三个分支。y1（13x13x255）,y2（26x26x255）,y3(52x52x255)。</p> 
<p>Y1从上面结点获取。是最高层的，最抽象的特征。适合比较大的目标检测；<br> Y2是在Y1的基础上做一个上采样+一个低层的一个特征进行连接（concat）。<br> 适合中等的目标检测；<br> Y3是在Y2的基础上做一个上采样+一个更低层的一个特征进行连接（concat）。适合小 目标的检测。</p> 
<p>三个尺度的输出皆是奇数，使得网格会有个中心位置。</p> 
<p>通道数255：{[80(coco数据集共有80类)+[X+Y+W+H+Confiderce]}×3(anchor) = 255</p> 
<p></p> 
<p><img alt="在这里插入图片描述" height="756" src="https://images2.imgbox.com/1e/fe/Wj76uBtW_o.png" width="800"></p> 
<p> </p> 
<p> CBL为卷积块：由conv2d，Batch Normalization，Leaky relu 这三个网络层组成。<br> Res为残差块：包含两个卷积块和一个add层，add层只是将相同维度的张量进行相加。<br> ResX为多个残差块,X代表残差块的个数。<br> 上采样；将特征图的宽和高变成原来的两倍，通道数不变。<br> Concat张量拼接：相同宽和高的通道数相加。</p> 
<p>具体来说（基于YOLOV3）</p> 
<p>1：输入一张任意大小图片，保持长宽比不变的情况下，缩放至 w 或 h 达到416，再覆盖在416*416的新图上，作为网络的输入。即网络的输入是一张416*416，3通道的RGB图。</p> 
<p>2：运行网络。YOLO的CNN网络把图片分成 S*S 个网格（yolov3多尺度预测，输出3层，每层 S * S个网格，分别为 13*13 ，26 *26 ，52*52），然后每个单元格负责去检测那些中心点落在该格子内的目标，如图二所示。  每个单元格需要预测 3*（4+1+B）个值。如果将输入图片划分为 S*S 网格，那么每层最终预测值为 S*S*3*(4+1+B) 大小的张量。 B：类别数（coco集为80类），即B=80.  3 为每层anchorbox数量，4 为边界框大小和位置（x , y , w , h ）1 为置信度。 </p> 
<p> 3: 通过NMS，非极大值抑制，筛选出框boxes,输出框class_boxes和置信度class_box_scores，再生成类别信息classes，生成最终的检测数据框，并返回<br>  </p> 
<p>多尺度：</p> 
<p>yolov3采用多尺度预测。【（13*13）（26*26）（52*52）】</p> 
<p>•小尺度：（13*13的feature map）</p> 
<p>网络接收一张（416*416）的图，经过5个步长为2的卷积来进行降采样（416 / 2ˆ5 = 13）.输出（13*13）。<br> •中尺度 ： （26*26的feature map）</p> 
<p>从小尺度中的倒数第二层的卷积层上采样(x2，up sampling)再与最后一个13x13大小的特征图相加，输出（26*26）。<br> •大尺度： （52*52的feature map）</p> 
<p>操作同中尺度输出（52*52）<br>  </p> 
<p>anchor box:</p> 
<p>yolov3 anchor box一共有9个，由k-means聚类得到。在COCO数据集上，9个聚类是：（10*13）;（16*30）;（33*23）;（30*61）;（62*45）; （59*119）; （116*90）; （156*198）; （373*326）。</p> 
<p>不同尺寸特征图对应不同大小的先验框。</p> 
<p>13*13feature map对应【（116*90），（156*198），（373*326）】<br> 26*26feature map对应【（30*61），（62*45），（59*119）】<br> 52*52feature map对应【（10*13），（16*30），（33*23）】<br> 原因：特征图越大，感受野越小。对小目标越敏感，所以选用小的anchor box。</p> 
<p>          特征图越小，感受野越大。对大目标越敏感，所以选用大的anchor box。</p> 
<p></p> 
<p>边框预测：</p> 
<p>预测tx ty tw th</p> 
<p>对tx和ty进行sigmoid，并加上对应的offset（下图Cx, Cy）<br> 对th和tw进行exp，并乘以对应的锚点值<br> 对tx,ty,th,tw乘以对应的步幅，即：416/13, 416 ⁄ 26, 416 ⁄ 52<br> 最后，使用sigmoid对Objectness和Classes confidence进行sigmoid得到0~1的概率，之所以用sigmoid取代之前版本的softmax，原因是softmax会扩大最大类别概率值而抑制其他类别概率值</p> 
<p><img alt="" height="273" src="https://images2.imgbox.com/97/f2/JKJwoxws_o.png" width="300"><br>        </p> 
<p></p> 
<p>(tx,ty) :目标中心点相对于该点所在网格左上角的偏移量，经过sigmoid归一化。即值属于【0,1】。如图约（0.3 , 0.4）</p> 
<p>(cx,cy):该点所在网格的左上角距离最左上角相差的格子数。如图（1,1）</p> 
<p>(pw,ph):anchor box 的边长</p> 
<p>(tw,th):预测边框的宽和高</p> 
<p>PS：最终得到的边框坐标值是bx,by,bw,bh.而网络学习目标是tx,ty,tw,th</p> 
<p> </p> 
<p><em><strong>损失函数LOSS</strong></em></p> 
<ul><li>YOLO V3把YOLOV2中的Softmax loss变成Logistic loss</li></ul> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/5b/63/PEFU0P2q_o.png"></p> 
<p> </p> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f8dff3df8190502be1646e0b2f65619f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">『图解』深度可分离卷积</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b934bc46db5602bc533e494014c3cb72/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Arduino Uno &#43; APDS9930 实现手势控制LED灯亮灭、调光等</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>