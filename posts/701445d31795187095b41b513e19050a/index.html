<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【论文笔记】基于聚类特征深度LSTM的语音情感识别 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【论文笔记】基于聚类特征深度LSTM的语音情感识别" />
<meta property="og:description" content="Clustering-Based Speech Emotion Recognition by Incorporating Learned Features and Deep BiLSTM IEEE Access - South Korea, Pakistan 关键词：语音情感识别、深度双向LSTM、关键片段、序列选择、CNN特征归一化、RBFN
摘要 传统SER主要注重手工特征和使用传统CNN模型来提取高阶特征，增加识别准确率和整个模型的复杂度。本文提出了新的框架：
采用基于RBFN的聚类方法选择一个关键序列，目的是：减少计算复杂度STFT得到频谱图，经过CNN模型提取显著特征，后对其进行归一化保证识别准确率，目的是：特征提取后进行处理保证更加容易提取时空信息归一化后传入Deep BiLSTM学习时序信息并预测最终情感标签 SER简介 SER背景传统方法用的是手工提取的特征2-D CNN本来是用在图像处理领域，现在也应用于SERCNN-LSTM提取空间-时间特征但是CNN的使用会增加计算复杂度和网络参数所以使用K-means聚类，采用RBF（为什么非要用RBF不用其他的？）作为相似度度量，从每个簇抽取一个片段后将抽取的片段经过STFT算法得到频谱图，随后放入CNN（Resnet模型的FC-1000层）CNN的输出通过均值方差正则化，并输入深度BiLSTM网络提取时序信息并通过softmax层输出预测结果本文的创新点： 采用CNN-BiLSTM框架捕捉时序信息，所用的CNN模型是第一次被应用在这个领域，采用ResNet101特征结合序列学习机制提出一个用RBFN作为相似度量的抽取和选择序列的方法，选择距离簇心最近的作为代表，能够减少处理时间均值方差归一化能够提升性能，这也是框架的主要贡献之一实验表明结果比较不错，适合识别实时情感。 SER调研 SER系统分为两个部分：特征选择和分类，特征提取有CNN或者高斯混合模型等等。CNN最近在特征提取兴起，有采用预训练模型做迁移学习提取特征的，LSTM-RNN用来学习时序信息，还有不需要手工特征输入的端到端的方法。
CNN-LSTM结合的方法用来捕捉高阶特征和时序信息，有文章采用预训练CNN和SVM结合做情感分类。
提出的SER方法 框架分为三个模块，第一个模块有两个部分：
第一个，将音频文件分成多个片段并找出连续片段只差，获得的差值通过一个阈值确保相似度并通过shot边界检测（这里好像是图像处理的相关内容，这里是怎么用的呢？）找到聚类所使用的K值。若两帧之差大于阈值，K值加一。每一簇都找到一个距离中心最近的关键片段，我们采用RBF作为聚类算法的相似度估计。
第二部分，用STFT画出所选择关键序列的频谱图，后采用预训练的Resnet101中的FC-1000层提取特征。具体的网络结构在表1中给出：
学习到的特征通过均值和标准差进行归一化，最后通过深度双向LSTM学习时序信息，获得序列信息，并预测最终情感分类。网络结构如下：
A. 预处理和序列选择 首先将语音分帧，窗长为500ms，每个片段的标签是整个语段的标签，后通过K均值聚类。采用RBF代替K-Means中的欧几里得距离矩阵，K值的选择并不是随机的，而是通过shot边界检测动态的估计相似性。（这里需要注意，每个文件的K值是不同的）
B. 基于RBF的相似度度量 RBF是计算片段之间相似性的非线性方法，人类大脑通过非线性过程识别和分辨模式。本文模型是基于RBFN的非线性模型，我们使用映射函数来找到两语音片段间的相似性，其中也用到归一化的概念。一维高斯模型是一个很好的选择，因为其可以平滑映射函数。
函数中心为z，宽度参数是 σ \sigma σ，这个函数用来度量x和中心z之间的相似程度，RBFN中有不同的RBF，用来进行非线性估计：
拓展的映射函数如下：
其有N个RBF，说明有多个中心z，为了减少网络计算量，这里每一个片段仅仅采用了一维的高斯RBF：
x表示语音信号片段，z表示每个片段的RBF中心， σ \sigma σ表示每个片段的RBF宽度。其中宽度是可变的，共有P个RBF，参数调整、非线性加权和样本方差估计如下：
如果特定语音信号片段更相关，标准差会很小，如果标准差很大意味着片段不想管，较小的 σ \sigma σ值对距离的变化更加敏感。
C. CNN特征提取和RNN 本文采用CNN提取语音片段特征，RNN提取时序特征，后采用预训练的CNN提取特征，提取的每个片段特征作为RNN的一个时间步，RNN最后一个时间步的输出作为情感分类的最终结果。训练大量复杂序列信息只用LSTM是不能正确识别的，本文采用多层深度Bi-LSTM进行序列识别，内部结构和记忆模块信息如图：
D. 双向LSTM 本文采用多层LSTM的概念，采用前向和后向结合的Bi-LSTM网络进行训练，其中20%数据作为验证集，从训练数据中分离，并且通过交叉验证计算错误率，优化器采用Adam，学习率为0.001。
实验设置和结果 实验数据集： IEMOCAP、Emo-DB、RAVDESS三个。IEMOCAP数据集包含10个人，有五个session，每个session是两两对话，每个语音片段长度约3~15s，共使用了4种情绪，采样率16KHz；Emo-DB有10人，5男5女，每个语音片段约2~3s，包含7种情绪，采样率16KHz；RAVDESS包含24人，12男12女，共有8种情绪，训练数据分布除了neutral数量相同，采样率48000Hz。这三个数据库在speaker independent的实验中均采用五折交叉验证，80%用于训练其余用来测试。
实验评估： 本文通过speaker independent和speaker dependent两种方式评估，将每一个语段按照时间t分成片段，有25%重叠部分（具体这个时间t应该取多少？）。基于RBF的相似度度量采用K均值聚类在每一簇选择关键片段（距离聚类中心最近的一个片段）。选择好关键片段后，通过Resnet101模型的FC-1000进行高阶特征提取，选择全局平均值和标准差来归一化提高整个模型的准确率。归一化的特征输入到BiLATM中，经过softmax层后，输出预测概率。系统使用MATLAB 2019b中的神经网络工具箱进行特征提取、模型训练和评估。数据被分为80%用于训练和20%用于测试。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/701445d31795187095b41b513e19050a/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-11-22T18:01:46+08:00" />
<meta property="article:modified_time" content="2021-11-22T18:01:46+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【论文笔记】基于聚类特征深度LSTM的语音情感识别</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="centerfont_colorgreenClusteringBased_Speech_Emotion_Recognition_by_Incorporating_Learned_Features_and_Deep_BiLSTM_0"></a> 
 <center> 
  <font color="green">Clustering-Based Speech Emotion Recognition by Incorporating Learned Features and Deep BiLSTM</font> 
 </center></h2> 
<h6><a id="centerIEEE_Access__South_Korea_Pakistan_1"></a> 
 <center>
   IEEE Access - South Korea, Pakistan 
 </center></h6> 
<p><font size="2">关键词：语音情感识别、深度双向LSTM、关键片段、序列选择、CNN特征归一化、RBFN</font></p> 
<hr> 
<h4><a id="_4"></a>摘要</h4> 
<p>传统SER主要注重手工特征和使用传统CNN模型来提取高阶特征，增加识别准确率和整个模型的复杂度。本文提出了新的框架：</p> 
<ol><li>采用<strong>基于RBFN的聚类</strong>方法选择一个关键序列，目的是：<em>减少计算复杂度</em></li><li>STFT得到频谱图，经过CNN模型提取显著特征，后对其进行归一化保证识别准确率，目的是：特征提取后进行处理保证更加容易提取时空信息</li><li>归一化后传入Deep BiLSTM学习时序信息并预测最终情感标签</li></ol> 
<hr> 
<h4><a id="SER_10"></a>SER简介</h4> 
<ul><li>SER背景</li><li>传统方法用的是手工提取的特征</li><li>2-D CNN本来是用在图像处理领域，现在也应用于SER</li><li>CNN-LSTM提取空间-时间特征</li><li>但是CNN的使用会增加计算复杂度和网络参数</li><li>所以使用K-means聚类，采用RBF（<font color="red">为什么非要用RBF不用其他的？</font>）作为相似度度量，从每个簇抽取<strong>一个</strong>片段</li><li>后将抽取的片段经过STFT算法得到频谱图，随后放入CNN（<strong>Resnet模型的FC-1000层</strong>）</li><li>CNN的输出通过均值方差正则化，并输入深度BiLSTM网络提取时序信息并通过softmax层输出预测结果</li><li>本文的创新点：</li></ul> 
<ol><li>采用CNN-BiLSTM框架捕捉时序信息，所用的CNN模型是第一次被应用在这个领域，采用ResNet101特征结合序列学习机制</li><li>提出一个用RBFN作为相似度量的抽取和选择序列的方法，选择距离簇心最近的作为代表，能够减少处理时间</li><li>均值方差归一化能够提升性能，这也是框架的主要贡献之一</li><li>实验表明结果比较不错，适合识别实时情感。</li></ol> 
<hr> 
<h4><a id="SER_25"></a>SER调研</h4> 
<p>SER系统分为两个部分：特征选择和分类，特征提取有CNN或者高斯混合模型等等。CNN最近在特征提取兴起，有采用预训练模型做迁移学习提取特征的，LSTM-RNN用来学习时序信息，还有不需要手工特征输入的端到端的方法。<br> CNN-LSTM结合的方法用来捕捉高阶特征和时序信息，有文章采用预训练CNN和SVM结合做情感分类。</p> 
<hr> 
<h4><a id="SER_29"></a>提出的SER方法</h4> 
<p>框架分为三个模块，第一个模块有两个部分：<br> 第一个，将音频文件分成多个片段并找出连续片段只差，获得的差值通过一个阈值确保相似度并通过shot边界检测（这里好像是图像处理的相关内容，这里是怎么用的呢？）找到聚类所使用的K值。若两帧之差大于阈值，K值加一。每一簇都找到一个距离中心最近的关键片段，我们采用RBF作为聚类算法的相似度估计。<br> 第二部分，用STFT画出所选择关键序列的频谱图，后采用<strong>预训练</strong>的Resnet101中的FC-1000层提取特征。具体的网络结构在表1中给出：<br> <img src="https://images2.imgbox.com/ef/88/JFyidpsp_o.png" alt="在这里插入图片描述"><br> 学习到的特征通过均值和标准差进行归一化，最后通过深度双向LSTM学习时序信息，获得序列信息，并预测最终情感分类。网络结构如下：<br> <img src="https://images2.imgbox.com/5f/c9/pkJLHkb2_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="A__36"></a>A. 预处理和序列选择</h5> 
<p>首先将语音分帧，窗长为500ms，每个片段的标签是整个语段的标签，后通过K均值聚类。采用RBF代替K-Means中的欧几里得距离矩阵，K值的选择并不是随机的，而是通过shot边界检测动态的估计相似性。（这里需要注意，每个文件的K值是不同的）</p> 
<h5><a id="B_RBF_38"></a>B. 基于RBF的相似度度量</h5> 
<p>RBF是计算片段之间相似性的非线性方法，人类大脑通过非线性过程识别和分辨模式。本文模型是基于RBFN的非线性模型，我们使用映射函数来找到两语音片段间的相似性，其中也用到归一化的概念。一维高斯模型是一个很好的选择，因为其可以平滑映射函数。<br> <img src="https://images2.imgbox.com/e6/cf/YCKvnUH1_o.png" alt="在这里插入图片描述"><br> 函数中心为z，宽度参数是<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         σ 
        
       
      
        \sigma 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">σ</span></span></span></span></span>，这个函数用来度量x和中心z之间的相似程度，RBFN中有不同的RBF，用来进行非线性估计：<br> <img src="https://images2.imgbox.com/ff/4e/fqBPwMRR_o.png" alt="在这里插入图片描述"><br> 拓展的映射函数如下：<br> <img src="https://images2.imgbox.com/8e/a7/sLcPT7KB_o.png" alt="在这里插入图片描述"><br> 其有N个RBF，说明有多个中心z，为了减少网络计算量，这里每一个片段仅仅采用了一维的高斯RBF：<br> <img src="https://images2.imgbox.com/95/75/PDjPq6ki_o.png" alt="在这里插入图片描述"><br> x表示语音信号片段，z表示每个片段的RBF中心，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         σ 
        
       
      
        \sigma 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">σ</span></span></span></span></span>表示每个片段的RBF宽度。其中宽度是可变的，共有P个RBF，参数调整、非线性加权和样本方差估计如下：<br> <img src="https://images2.imgbox.com/94/35/HIRNMg6Z_o.png" alt="在这里插入图片描述"><br> 如果特定语音信号片段更相关，标准差会很小，如果标准差很大意味着片段不想管，较小的<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         σ 
        
       
      
        \sigma 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">σ</span></span></span></span></span>值对距离的变化更加敏感。</p> 
<h5><a id="C_CNNRNN_50"></a>C. CNN特征提取和RNN</h5> 
<p>本文采用CNN提取语音片段特征，RNN提取时序特征，后采用预训练的CNN提取特征，提取的每个片段特征作为RNN的一个时间步，RNN<strong>最后一个时间步的输出</strong>作为情感分类的最终结果。训练大量复杂序列信息只用LSTM是不能正确识别的，本文采用多层深度Bi-LSTM进行序列识别，内部结构和记忆模块信息如图：<br> <img src="https://images2.imgbox.com/e3/a9/opn7FgmS_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="D_LSTM_53"></a>D. 双向LSTM</h5> 
<p>本文采用多层LSTM的概念，采用前向和后向结合的Bi-LSTM网络进行训练，其中20%数据作为验证集，从训练数据中分离，并且通过交叉验证计算错误率，优化器采用Adam，学习率为0.001。</p> 
<hr> 
<h4><a id="_56"></a>实验设置和结果</h4> 
<p><strong>实验数据集：</strong> IEMOCAP、Emo-DB、RAVDESS三个。IEMOCAP数据集包含10个人，有五个session，每个session是两两对话，每个语音片段长度约3~15s，共使用了4种情绪，采样率16KHz；Emo-DB有10人，5男5女，每个语音片段约2~3s，包含7种情绪，采样率16KHz；RAVDESS包含24人，12男12女，共有8种情绪，训练数据分布除了neutral数量相同，采样率48000Hz。这三个数据库在speaker independent的实验中均采用五折交叉验证，80%用于训练其余用来测试。<br> <strong>实验评估：</strong> 本文通过speaker independent和speaker dependent两种方式评估，将每一个语段按照时间t分成片段，有25%重叠部分（<font color="red">具体这个时间t应该取多少？</font>）。基于RBF的相似度度量采用K均值聚类在每一簇选择关键片段（距离聚类中心最近的一个片段）。选择好关键片段后，通过Resnet101模型的FC-1000进行高阶特征提取，选择全局平均值和标准差来归一化提高整个模型的准确率。归一化的特征输入到BiLATM中，经过softmax层后，输出预测概率。系统使用MATLAB 2019b中的神经网络工具箱进行特征提取、模型训练和评估。数据被分为80%用于训练和20%用于测试。<br> <strong>模型优化：</strong> 选择Adam作为优化器，实验中比较了特征归一化和没有归一化结果的差异，选择了512作为batch-size，0.001作为学习率，实验表明，归一化使得预测准确率有所提升。（文中是三个数据库的结果，这里只给出了一个IEMOCAP的），SD和SI分别代表speaker dependent和speaker independent。<br> <img src="https://images2.imgbox.com/d5/ed/T4owPk0U_o.png" alt="在这里插入图片描述"><br> 实验还对数据的训练和测试时间进行了比较，该模型因为从数据中选取关键片段作为训练数据，大大减少了数据处理时间。<br> <strong>SI实验：</strong> 五折交叉验证，80%的speaker作为训练数据，20%的speaker作为测试数据，结果如下：<br> <img src="https://images2.imgbox.com/a6/8c/ObUsZKtW_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/fe/c7/oEUz14zW_o.png" alt="在这里插入图片描述"><br> <font color="red">貌似这里没有和baseline进行相应的比较</font>，其中happy、neutral和sad情绪容易和其他情绪进行混淆。<br> <strong>SD实验：</strong> 将所有文件混合为一个集合， 随机选取80%的数据进行训练，20%用于测试和验证。结果如下：<br> <img src="https://images2.imgbox.com/64/87/xJ1B5mmF_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/84/c7/Haa2Bzq5_o.png" alt="在这里插入图片描述"><br> 同样，似乎happy情绪是最容易进行混淆的情绪，可能是因为训练数据数量受限，语音数据较图像、视频等数据少得多，容易识别错误。</p> 
<hr> 
<h4><a id="_71"></a>讨论</h4> 
<p>和其他很多baseline进行比较后，我们会发现，该模型能够大幅度提高识别准确率：<br> <img src="https://images2.imgbox.com/43/c9/JFxTtw1b_o.png" alt="在这里插入图片描述"><br> 同时还将该模型和其他预训练的CNN模型进行比较，结果如下：<br> <img src="https://images2.imgbox.com/26/a9/UVwlHEf9_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/c1/5f/Yp1fDy4H_o.png" alt="在这里插入图片描述"><br> 上面结果可以看出本文模型的效果远大于其它CNN模型的效果。</p> 
<hr> 
<p>我想对本文进行复现，复现的内容在以后的博客中给出，也希望大家阅读后指出我阅读过程中的错误，多多交流，谢谢大家阅读。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/1a5830596d4862e9317469eb0ef0b795/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">系统安全及应用</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f061f38a1096d8e60c35e5de4bb98968/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【leetcode刷题笔记】01.两数之和</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>