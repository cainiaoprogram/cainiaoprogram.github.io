<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>《数学基础》-4.凸优化-4.1.无约束优化 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="《数学基础》-4.凸优化-4.1.无约束优化" />
<meta property="og:description" content="4.1.无约束优化 4.1.1.无约束优化问题 无约束优化问题是机器学习中最普遍、最简单的优化问题。
求最大值也可以 在前面加上负号，变成上面求最小的形式。
求一个函数f(x)的最小值可以对函数f(x)求导并使其等于0(或者说使得梯度▽f(x)等于0)，但是很多复杂的函数求导后没法求出解，所以这种方法实际上很少用。
常用梯度下降法、牛顿法或者拟牛顿法求解。
4.1.2.梯度下降法 基于迭代的方法，从某个点开始找很多点，使得这些点满足：，且有，这里表示单位梯度，经常写作，λ表示步长，所以通项是：
实际上λ也不会取很大，一般是
其过程为：
梯度下降法的种类：
①批量梯度下降法（BGD）
更新系数时，所有样本都参与计算
优点：需要个很少的迭代次数就可以收敛
缺点：当样本量很大时，更新一次的时间很长
②随机梯度下降法（SGD）
更新系数时，从n个样本中随机选择一个样本参与计算，
优点：更新一次的时间很短，所以大样本时有优势
缺点：会受到每一个样本的影响会很大，不稳定，需要更多的迭代次数才能收敛
③小批量梯度下降法（MBGD）
结合了批量梯度下降法和随机梯度下降法，选择一小部分样本参与计算
例如：
所有的样本都算完，就是一个epoch
4.1.3.牛顿法 求一个函数的最小值可以对函数求导并使其等于0(或者说使得梯度等于0)：，把函数的导数看做一个函数，令
牛顿法求的过程也是迭代过程
假设的函数曲线是这个样子，要找到那个的点，先做某个的切线，然后找到切线与x轴相交的点然后再做的切线，以此类推，不断逼近的点。
先来求第一条切线的方程：
令y=0（就是上图中的点）得：
再把带入得：
这是二维的情况，如果是多维的情况：
其中H是海森矩阵，除以海森矩阵就是乘以它的逆矩阵。
为什么这里是海森矩阵？因为是的n维向量，是n维向量，二次求导就是海森矩阵。
在机器学习中，要算海森矩阵的逆矩阵很麻烦，于是就引申出了很多种拟牛顿法BFGS（用另外一个矩阵来逼近海森矩阵的逆矩阵）。
牛顿法收敛速度：
按这个迭代原理，就应该是函数的局部最优点，也就是有最小值，且有要弄明白这个收敛速度，就是要比较下到的距离和到的距离的区别，由上述结论得：
由于，所以分子加上得：
根据中值定理f(b)−f(a)=(b−a)f′(ξ),a&lt;ξ&lt;b，得：
再利用拉格朗日中值定理得：
ξ是在之间的，所以
由于M的分子分母都是导数，导数都是有界的，所以M是有界的，用表示其上界。
即：
当和的距离小于1：，则，这里是按照平方的速度进行收敛的，收敛速度更快，注意这里有条件：x和的距离小于1，如果距离大于1，上界会越来越大，没法收敛。
综上，牛顿法要拟合，不能离最小值太远的地方拟合，越接近极小值再拟合收敛的效果越好。因此可以先用梯度下降，到了局部极小值附近后再用牛顿法。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/955f18a3dc5865e735dee12a7692ec1d/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-07-30T13:04:54+08:00" />
<meta property="article:modified_time" content="2020-07-30T13:04:54+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">《数学基础》-4.凸优化-4.1.无约束优化</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3><strong>4.1.无约束优化</strong></h3> 
<h4><a name="_Toc23792"></a><strong>4.1.1.无约束优化问题</strong></h4> 
<p>无约束优化问题是机器学习中最普遍、最简单的优化问题。</p> 
<p><img alt="" height="38" src="https://images2.imgbox.com/a8/95/ACOhexHF_o.png" width="308"></p> 
<p>求最大值也可以 在前面加上负号，变成上面求最小的形式。</p> 
<p>求一个函数f(x)的最小值可以对函数f(x)求导并使其等于0(或者说使得梯度▽f(x)等于0)，但是很多复杂的函数求导后没法求出解，所以这种方法实际上很少用。</p> 
<p>常用梯度下降法、牛顿法或者拟牛顿法求解。</p> 
<h4><a name="_Toc17704"></a><strong>4.1.2.梯度下降法</strong></h4> 
<p>基于迭代的方法，从某个点<img alt="" height="30" src="https://images2.imgbox.com/e7/c5/HTtMd5to_o.png" width="22">开始找很多点<img alt="" height="30" src="https://images2.imgbox.com/3a/e2/QTYTkBn3_o.png" width="127">，使得这些点满足：<img alt="" height="30" src="https://images2.imgbox.com/fa/df/n4NgrAHP_o.png" width="215">，且有<img alt="" height="59" src="https://images2.imgbox.com/43/2d/xqqeJFLn_o.png" width="157">，这里<img alt="" height="59" src="https://images2.imgbox.com/19/3e/Jloi0FHE_o.png" width="72">表示单位梯度，经常写作<img alt="" height="30" src="https://images2.imgbox.com/e8/08/0HaxN1eL_o.png" width="62">，λ表示步长，所以通项是：</p> 
<p><img alt="" height="37" src="https://images2.imgbox.com/79/84/OPHjVfrf_o.png" width="256"></p> 
<p>实际上λ也不会取很大，一般是<img alt="" height="27" src="https://images2.imgbox.com/a6/6d/lvW6FeaI_o.png" width="85"></p> 
<p>其过程为：</p> 
<p>  <img alt="" height="233" src="https://images2.imgbox.com/d3/b1/xeNr2mcO_o.png" width="426"></p> 
<p>梯度下降法的种类：</p> 
<p>①批量梯度下降法（BGD）</p> 
<p>更新系数时，所有样本都参与计算</p> 
<p>优点：需要个很少的迭代次数就可以收敛</p> 
<p>缺点：当样本量很大时，更新一次的时间很长</p> 
<p>②随机梯度下降法（SGD）</p> 
<p>更新系数时，从n个样本中随机选择一个样本参与计算，</p> 
<p>优点：更新一次的时间很短，所以大样本时有优势</p> 
<p>缺点：会受到每一个样本的影响会很大，不稳定，需要更多的迭代次数才能收敛</p> 
<p>③小批量梯度下降法（MBGD）</p> 
<p>结合了批量梯度下降法和随机梯度下降法，选择一小部分样本参与计算</p> 
<p>例如：</p> 
<p><img alt="" height="290" src="https://images2.imgbox.com/ad/1b/Vkgxgc6M_o.png" width="581"></p> 
<p>所有的样本都算完，就是一个epoch</p> 
<h4><a name="_Toc5625"></a><strong>4.1.3.牛顿法</strong></h4> 
<p>求一个函数<img alt="" height="27" src="https://images2.imgbox.com/c7/5b/tW9QOyBB_o.png" width="45">的最小值可以对函数<img alt="" height="27" src="https://images2.imgbox.com/76/03/v7WElybe_o.png" width="45">求导并使其等于0(或者说使得梯度<img alt="" height="27" src="https://images2.imgbox.com/80/1c/abJ8j7KL_o.png" width="55">等于0)：<img alt="" height="30" src="https://images2.imgbox.com/21/1b/YmvElkib_o.png" width="79">，把函数<img alt="" height="27" src="https://images2.imgbox.com/9c/26/de5LUAUF_o.png" width="45">的导数看做一个函数，令<img alt="" height="30" src="https://images2.imgbox.com/91/b6/wlPyNwm1_o.png" width="202"></p> 
<p>牛顿法求<img alt="" height="27" src="https://images2.imgbox.com/c8/96/OfTAkJwe_o.png" width="74">的过程也是迭代过程</p> 
<p><img alt="" height="309" src="https://images2.imgbox.com/f7/31/x9upFKcj_o.png" width="494"></p> 
<p>假设<img alt="" height="27" src="https://images2.imgbox.com/38/42/VpUGeOZo_o.png" width="44">的函数曲线是这个样子，要找到那个<img alt="" height="27" src="https://images2.imgbox.com/29/c3/xnFnECv3_o.png" width="74">的点，先做某个<img alt="" height="30" src="https://images2.imgbox.com/92/aa/3zDSyom0_o.png" width="24">的切线，然后找到切线与x轴相交的点<img alt="" height="30" src="https://images2.imgbox.com/27/1e/XgzAJbaR_o.png" width="34">然后再做<img alt="" height="30" src="https://images2.imgbox.com/2c/a7/J1NmESwQ_o.png" width="34">的切线，以此类推，不断逼近<img alt="" height="27" src="https://images2.imgbox.com/b0/3a/JtS7h0EQ_o.png" width="74">的点。</p> 
<p>先来求第一条切线的方程：</p> 
<p><img alt="" height="45" src="https://images2.imgbox.com/ba/0d/R6TXudP0_o.png" width="364"></p> 
<p>令y=0（就是上图中的<img alt="" height="30" src="https://images2.imgbox.com/d8/e8/cGBKn8cE_o.png" width="34">点）得：</p> 
<p><img alt="" height="159" src="https://images2.imgbox.com/e4/22/SfJgYj6N_o.png" width="311"></p> 
<p>再把<img alt="" height="30" src="https://images2.imgbox.com/8c/71/gREt7xjX_o.png" width="107">带入得：</p> 
<p><img alt="" height="60" src="https://images2.imgbox.com/25/0c/rFL49Gk8_o.png" width="207"></p> 
<p>这是二维的情况，如果是多维的情况：</p> 
<p><img alt="" height="37" src="https://images2.imgbox.com/9a/0e/HJEgA5y2_o.png" width="270"></p> 
<p>其中H是海森矩阵，除以海森矩阵就是乘以它的逆矩阵。</p> 
<p>为什么这里是海森矩阵？因为<img alt="" height="30" src="https://images2.imgbox.com/e4/5f/rlxYCJGt_o.png" width="24">是<img alt="" height="25" src="https://images2.imgbox.com/88/d8/qYDHWgkn_o.png" width="27">的n维向量，<img alt="" height="30" src="https://images2.imgbox.com/c0/4b/vNlTAAYu_o.png" width="64">是n维向量，二次求导就是海森矩阵。</p> 
<p>在机器学习中，要算海森矩阵的逆矩阵很麻烦，于是就引申出了很多种拟牛顿法BFGS（用另外一个矩阵来逼近海森矩阵的逆矩阵）。</p> 
<p> </p> 
<p><strong>牛顿法收敛速度</strong><strong>：</strong></p> 
<p>按这个迭代原理，<img alt="" height="30" src="https://images2.imgbox.com/28/ae/dHd5eEZe_o.png" width="22">就应该是函数的局部最优点，也就是<img alt="" height="30" src="https://images2.imgbox.com/b6/8c/D4AFzUwL_o.png" width="52">有最小值，且有<img alt="" height="32" src="https://images2.imgbox.com/d7/fc/RC5bQFqE_o.png" width="87">要弄明白这个收敛速度，就是要比较下<img alt="" height="30" src="https://images2.imgbox.com/dd/e1/vlWHmjDM_o.png" width="34">到<img alt="" height="30" src="https://images2.imgbox.com/14/df/zsXtQNEk_o.png" width="22">的距离和<img alt="" height="30" src="https://images2.imgbox.com/a5/a2/qTC8pCeA_o.png" width="24">到<img alt="" height="30" src="https://images2.imgbox.com/c1/db/HC7EuzDq_o.png" width="22">的距离的区别，由上述结论得：</p> 
<p><img alt="" height="67" src="https://images2.imgbox.com/db/b3/GKipdPui_o.png" width="339"></p> 
<p>由于<img alt="" height="32" src="https://images2.imgbox.com/06/1f/GYsG6tKL_o.png" width="87">，所以分子加上<img alt="" height="32" src="https://images2.imgbox.com/12/d0/08LyllWy_o.png" width="57">得：</p> 
<p><img alt="" height="63" src="https://images2.imgbox.com/80/0b/2NYMYp1i_o.png" width="296"></p> 
<p>根据中值定理f(b)−f(a)=(b−a)f′(ξ),a&lt;ξ&lt;b，得：</p> 
<p><img alt="" height="67" src="https://images2.imgbox.com/de/78/5VdpVtql_o.png" width="298"></p> 
<p><img alt="" height="66" src="https://images2.imgbox.com/22/c2/eLzmKQ6B_o.png" width="248"></p> 
<p><img alt="" height="68" src="https://images2.imgbox.com/ec/b1/NVbTgnGc_o.png" width="286"></p> 
<p>再利用拉格朗日中值定理得：</p> 
<p><img alt="" height="64" src="https://images2.imgbox.com/cb/8d/8yTpaP0r_o.png" width="282"></p> 
<p><img alt="" height="62" src="https://images2.imgbox.com/7a/9a/UZb4Fb5C_o.png" width="166"></p> 
<p><img alt="" height="32" src="https://images2.imgbox.com/a2/49/VRa4dvHj_o.png" width="239"></p> 
<p>ξ是在<img alt="" height="30" src="https://images2.imgbox.com/9c/23/XiOO97ku_o.png" width="60">之间的，所以</p> 
<p><img alt="" height="33" src="https://images2.imgbox.com/34/fd/H6DcZzlF_o.png" width="388"></p> 
<p>由于M的分子分母都是导数，导数都是有界的，所以M是有界的，用<img alt="" height="27" src="https://images2.imgbox.com/01/ec/Ie82vaAN_o.png" width="27">表示其上界。</p> 
<p><img alt="" height="29" src="https://images2.imgbox.com/10/7c/uh47mKiQ_o.png" width="508"></p> 
<p>即：</p> 
<p><img alt="" height="34" src="https://images2.imgbox.com/ac/80/3KCJtCve_o.png" width="282"></p> 
<p>当<img alt="" height="30" src="https://images2.imgbox.com/08/05/JbD0viEF_o.png" width="24">和<img alt="" height="30" src="https://images2.imgbox.com/65/9a/pj9PR9IU_o.png" width="22">的距离小于1：<img alt="" height="30" src="https://images2.imgbox.com/3c/3d/LQ91BxeL_o.png" width="100">，则<img alt="" height="32" src="https://images2.imgbox.com/6d/c6/gZN1M0iv_o.png" width="120">，这里是按照平方的速度进行收敛的，收敛速度更快，注意这里有条件：x<img alt="" height="30" src="https://images2.imgbox.com/4e/11/1IdcBhjt_o.png" width="24">和<img alt="" height="30" src="https://images2.imgbox.com/d1/9b/Jdkc6Uwg_o.png" width="22">的距离小于1，如果距离大于1，上界会越来越大，没法收敛。</p> 
<p> </p> 
<p>综上，牛顿法要拟合，不能离最小值太远的地方拟合，越接近极小值再拟合收敛的效果越好。因此可以先用梯度下降，到了局部极小值附近后再用牛顿法。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/54aaadfb6086d867cb304eafe494846d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Docker 与 kafka</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/111f5975f1bf3ad88e16277bd88a3697/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">jvm最全详解-06-JVM调优实战及常量池详解</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>