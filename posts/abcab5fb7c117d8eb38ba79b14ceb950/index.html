<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Task05 编写BERT 模型 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Task05 编写BERT 模型" />
<meta property="og:description" content="文章目录 前言1 Tokenization分词-BertTokenizer2 Model BertModel2.1 BertEmberddings2.2 BertEncoder2.2.1 BertAttention2.2.1.1 BertSelfAttention2.2.1.2 BertSelfOutput 2.2.3 BertOutput 2.3 BertPooler 总结参考 前言 1 Tokenization分词-BertTokenizer 接下来的代码我感觉太复杂了
import collections import os import unicodedata from typing import List, Optional, Tuple from transformers.tokenization_utils import PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace from transformers.utils import logging logger = logging.get_logger(__name__) VOCAB_FILES_NAMES = {&#34;vocab_file&#34;: &#34;vocab.txt&#34;} PRETRAINED_VOCAB_FILES_MAP = { &#34;vocab_file&#34;: { &#34;bert-base-uncased&#34;: &#34;https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt&#34;, } } PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = { &#34;bert-base-uncased&#34;: 512, } PRETRAINED_INIT_CONFIGURATION = { &#34;bert-base-uncased&#34;: {&#34;do_lower_case&#34;: True}, } def load_vocab(vocab_file): &#34;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/abcab5fb7c117d8eb38ba79b14ceb950/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-09-22T00:06:09+08:00" />
<meta property="article:modified_time" content="2021-09-22T00:06:09+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Task05 编写BERT 模型</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#_1" rel="nofollow">前言</a></li><li><a href="#1_TokenizationBertTokenizer_4" rel="nofollow">1 Tokenization分词-BertTokenizer</a></li><li><a href="#2_Model_BertModel_432" rel="nofollow">2 Model BertModel</a></li><li><ul><li><a href="#21_BertEmberddings_619" rel="nofollow">2.1 BertEmberddings</a></li><li><a href="#22_BertEncoder_683" rel="nofollow">2.2 BertEncoder</a></li><li><ul><li><ul><li><a href="#221_BertAttention_884" rel="nofollow">2.2.1 BertAttention</a></li><li><a href="#2211_BertSelfAttention_984" rel="nofollow">2.2.1.1 BertSelfAttention</a></li><li><a href="#2212_BertSelfOutput_986" rel="nofollow">2.2.1.2 BertSelfOutput</a></li></ul> 
    </li><li><a href="#223_BertOutput_988" rel="nofollow">2.2.3 BertOutput</a></li></ul> 
   </li><li><a href="#23_BertPooler_990" rel="nofollow">2.3 BertPooler</a></li></ul> 
  </li><li><a href="#_992" rel="nofollow">总结</a></li><li><a href="#_996" rel="nofollow">参考</a></li></ul> 
</div> 
<p></p> 
<h2><a id="_1"></a>前言</h2> 
<p><img src="https://images2.imgbox.com/8d/fa/9UPGYVpF_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="1_TokenizationBertTokenizer_4"></a>1 Tokenization分词-BertTokenizer</h2> 
<p>接下来的代码我感觉太复杂了</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> collections
<span class="token keyword">import</span> os
<span class="token keyword">import</span> unicodedata
<span class="token keyword">from</span> typing <span class="token keyword">import</span> List<span class="token punctuation">,</span> Optional<span class="token punctuation">,</span> Tuple

<span class="token keyword">from</span> transformers<span class="token punctuation">.</span>tokenization_utils <span class="token keyword">import</span> PreTrainedTokenizer<span class="token punctuation">,</span> _is_control<span class="token punctuation">,</span> _is_punctuation<span class="token punctuation">,</span> _is_whitespace
<span class="token keyword">from</span> transformers<span class="token punctuation">.</span>utils <span class="token keyword">import</span> logging


logger <span class="token operator">=</span> logging<span class="token punctuation">.</span>get_logger<span class="token punctuation">(</span>__name__<span class="token punctuation">)</span>

VOCAB_FILES_NAMES <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token string">"vocab_file"</span><span class="token punctuation">:</span> <span class="token string">"vocab.txt"</span><span class="token punctuation">}</span>

PRETRAINED_VOCAB_FILES_MAP <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"vocab_file"</span><span class="token punctuation">:</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string">"bert-base-uncased"</span><span class="token punctuation">:</span> <span class="token string">"https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt"</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>

PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"bert-base-uncased"</span><span class="token punctuation">:</span> <span class="token number">512</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>

PRETRAINED_INIT_CONFIGURATION <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"bert-base-uncased"</span><span class="token punctuation">:</span> <span class="token punctuation">{<!-- --></span><span class="token string">"do_lower_case"</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>


<span class="token keyword">def</span> <span class="token function">load_vocab</span><span class="token punctuation">(</span>vocab_file<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Loads a vocabulary file into a dictionary."""</span>
    vocab <span class="token operator">=</span> collections<span class="token punctuation">.</span>OrderedDict<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>vocab_file<span class="token punctuation">,</span> <span class="token string">"r"</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> reader<span class="token punctuation">:</span>
        tokens <span class="token operator">=</span> reader<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> index<span class="token punctuation">,</span> token <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>tokens<span class="token punctuation">)</span><span class="token punctuation">:</span>
        token <span class="token operator">=</span> token<span class="token punctuation">.</span>rstrip<span class="token punctuation">(</span><span class="token string">"\n"</span><span class="token punctuation">)</span>
        vocab<span class="token punctuation">[</span>token<span class="token punctuation">]</span> <span class="token operator">=</span> index
    <span class="token keyword">return</span> vocab


<span class="token keyword">def</span> <span class="token function">whitespace_tokenize</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Runs basic whitespace cleaning and splitting on a piece of text."""</span>
    text <span class="token operator">=</span> text<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> <span class="token keyword">not</span> text<span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    tokens <span class="token operator">=</span> text<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> tokens


<span class="token keyword">class</span> <span class="token class-name">BertTokenizer</span><span class="token punctuation">(</span>PreTrainedTokenizer<span class="token punctuation">)</span><span class="token punctuation">:</span>

    vocab_files_names <span class="token operator">=</span> VOCAB_FILES_NAMES
    pretrained_vocab_files_map <span class="token operator">=</span> PRETRAINED_VOCAB_FILES_MAP
    pretrained_init_configuration <span class="token operator">=</span> PRETRAINED_INIT_CONFIGURATION
    max_model_input_sizes <span class="token operator">=</span> PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        vocab_file<span class="token punctuation">,</span>
        do_lower_case<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
        do_basic_tokenize<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
        never_split<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        unk_token<span class="token operator">=</span><span class="token string">"[UNK]"</span><span class="token punctuation">,</span>
        sep_token<span class="token operator">=</span><span class="token string">"[SEP]"</span><span class="token punctuation">,</span>
        pad_token<span class="token operator">=</span><span class="token string">"[PAD]"</span><span class="token punctuation">,</span>
        cls_token<span class="token operator">=</span><span class="token string">"[CLS]"</span><span class="token punctuation">,</span>
        mask_token<span class="token operator">=</span><span class="token string">"[MASK]"</span><span class="token punctuation">,</span>
        tokenize_chinese_chars<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
        strip_accents<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        <span class="token operator">**</span>kwargs
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>
            do_lower_case<span class="token operator">=</span>do_lower_case<span class="token punctuation">,</span>
            do_basic_tokenize<span class="token operator">=</span>do_basic_tokenize<span class="token punctuation">,</span>
            never_split<span class="token operator">=</span>never_split<span class="token punctuation">,</span>
            unk_token<span class="token operator">=</span>unk_token<span class="token punctuation">,</span>
            sep_token<span class="token operator">=</span>sep_token<span class="token punctuation">,</span>
            pad_token<span class="token operator">=</span>pad_token<span class="token punctuation">,</span>
            cls_token<span class="token operator">=</span>cls_token<span class="token punctuation">,</span>
            mask_token<span class="token operator">=</span>mask_token<span class="token punctuation">,</span>
            tokenize_chinese_chars<span class="token operator">=</span>tokenize_chinese_chars<span class="token punctuation">,</span>
            strip_accents<span class="token operator">=</span>strip_accents<span class="token punctuation">,</span>
            <span class="token operator">**</span>kwargs<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>

        <span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isfile<span class="token punctuation">(</span>vocab_file<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>
                <span class="token string-interpolation"><span class="token string">f"Can't find a vocabulary file at path '</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>vocab_file<span class="token punctuation">}</span></span><span class="token string">'. To load the vocabulary from a Google pretrained "</span></span>
                <span class="token string">"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`"</span>
            <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>vocab <span class="token operator">=</span> load_vocab<span class="token punctuation">(</span>vocab_file<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ids_to_tokens <span class="token operator">=</span> collections<span class="token punctuation">.</span>OrderedDict<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span>ids<span class="token punctuation">,</span> tok<span class="token punctuation">)</span> <span class="token keyword">for</span> tok<span class="token punctuation">,</span> ids <span class="token keyword">in</span> self<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>do_basic_tokenize <span class="token operator">=</span> do_basic_tokenize
        <span class="token keyword">if</span> do_basic_tokenize<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>basic_tokenizer <span class="token operator">=</span> BasicTokenizer<span class="token punctuation">(</span>
                do_lower_case<span class="token operator">=</span>do_lower_case<span class="token punctuation">,</span>
                never_split<span class="token operator">=</span>never_split<span class="token punctuation">,</span>
                tokenize_chinese_chars<span class="token operator">=</span>tokenize_chinese_chars<span class="token punctuation">,</span>
                strip_accents<span class="token operator">=</span>strip_accents<span class="token punctuation">,</span>
            <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>wordpiece_tokenizer <span class="token operator">=</span> WordpieceTokenizer<span class="token punctuation">(</span>vocab<span class="token operator">=</span>self<span class="token punctuation">.</span>vocab<span class="token punctuation">,</span> unk_token<span class="token operator">=</span>self<span class="token punctuation">.</span>unk_token<span class="token punctuation">)</span>

    <span class="token decorator annotation punctuation">@property</span>
    <span class="token keyword">def</span> <span class="token function">do_lower_case</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>basic_tokenizer<span class="token punctuation">.</span>do_lower_case

    <span class="token decorator annotation punctuation">@property</span>
    <span class="token keyword">def</span> <span class="token function">vocab_size</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">get_vocab</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>vocab<span class="token punctuation">,</span> <span class="token operator">**</span>self<span class="token punctuation">.</span>added_tokens_encoder<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">_tokenize</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>
        split_tokens <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>do_basic_tokenize<span class="token punctuation">:</span>
            <span class="token keyword">for</span> token <span class="token keyword">in</span> self<span class="token punctuation">.</span>basic_tokenizer<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span>text<span class="token punctuation">,</span> never_split<span class="token operator">=</span>self<span class="token punctuation">.</span>all_special_tokens<span class="token punctuation">)</span><span class="token punctuation">:</span>

                <span class="token comment"># If the token is part of the never_split set</span>
                <span class="token keyword">if</span> token <span class="token keyword">in</span> self<span class="token punctuation">.</span>basic_tokenizer<span class="token punctuation">.</span>never_split<span class="token punctuation">:</span>
                    split_tokens<span class="token punctuation">.</span>append<span class="token punctuation">(</span>token<span class="token punctuation">)</span>
                <span class="token keyword">else</span><span class="token punctuation">:</span>
                    split_tokens <span class="token operator">+=</span> self<span class="token punctuation">.</span>wordpiece_tokenizer<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span>token<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            split_tokens <span class="token operator">=</span> self<span class="token punctuation">.</span>wordpiece_tokenizer<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
        <span class="token keyword">return</span> split_tokens

    <span class="token keyword">def</span> <span class="token function">_convert_token_to_id</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> token<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Converts a token (str) in an id using the vocab."""</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>get<span class="token punctuation">(</span>token<span class="token punctuation">,</span> self<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>get<span class="token punctuation">(</span>self<span class="token punctuation">.</span>unk_token<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">_convert_id_to_token</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Converts an index (integer) in a token (str) using the vocab."""</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>ids_to_tokens<span class="token punctuation">.</span>get<span class="token punctuation">(</span>index<span class="token punctuation">,</span> self<span class="token punctuation">.</span>unk_token<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">convert_tokens_to_string</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tokens<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Converts a sequence of tokens (string) in a single string."""</span>
        out_string <span class="token operator">=</span> <span class="token string">" "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>tokens<span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">" ##"</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> out_string

    <span class="token keyword">def</span> <span class="token function">build_inputs_with_special_tokens</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span> token_ids_0<span class="token punctuation">:</span> List<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">,</span> token_ids_1<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>List<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> List<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
        adding special tokens. A BERT sequence has the following format:
        - single sequence: ``[CLS] X [SEP]``
        - pair of sequences: ``[CLS] A [SEP] B [SEP]``
        Args:
            token_ids_0 (:obj:`List[int]`):
                List of IDs to which the special tokens will be added.
            token_ids_1 (:obj:`List[int]`, `optional`):
                Optional second list of IDs for sequence pairs.
        Returns:
            :obj:`List[int]`: List of `input IDs &lt;../glossary.html#input-ids&gt;`__ with the appropriate special tokens.
        """</span>
        <span class="token keyword">if</span> token_ids_1 <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>cls_token_id<span class="token punctuation">]</span> <span class="token operator">+</span> token_ids_0 <span class="token operator">+</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>sep_token_id<span class="token punctuation">]</span>
        cls <span class="token operator">=</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>cls_token_id<span class="token punctuation">]</span>
        sep <span class="token operator">=</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>sep_token_id<span class="token punctuation">]</span>
        <span class="token keyword">return</span> cls <span class="token operator">+</span> token_ids_0 <span class="token operator">+</span> sep <span class="token operator">+</span> token_ids_1 <span class="token operator">+</span> sep

    <span class="token keyword">def</span> <span class="token function">get_special_tokens_mask</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span> token_ids_0<span class="token punctuation">:</span> List<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">,</span> token_ids_1<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>List<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span> already_has_special_tokens<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> List<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
        special tokens using the tokenizer ``prepare_for_model`` method.
        Args:
            token_ids_0 (:obj:`List[int]`):
                List of IDs.
            token_ids_1 (:obj:`List[int]`, `optional`):
                Optional second list of IDs for sequence pairs.
            already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):
                Whether or not the token list is already formatted with special tokens for the model.
        Returns:
            :obj:`List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.
        """</span>

        <span class="token keyword">if</span> already_has_special_tokens<span class="token punctuation">:</span>
            <span class="token keyword">return</span> <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>get_special_tokens_mask<span class="token punctuation">(</span>
                token_ids_0<span class="token operator">=</span>token_ids_0<span class="token punctuation">,</span> token_ids_1<span class="token operator">=</span>token_ids_1<span class="token punctuation">,</span> already_has_special_tokens<span class="token operator">=</span><span class="token boolean">True</span>
            <span class="token punctuation">)</span>

        <span class="token keyword">if</span> token_ids_1 <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>token_ids_0<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>token_ids_1<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
        <span class="token keyword">return</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>token_ids_0<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">create_token_type_ids_from_sequences</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span> token_ids_0<span class="token punctuation">:</span> List<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">,</span> token_ids_1<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>List<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> List<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence
        pair mask has the following format:
        ::
            0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
            | first sequence    | second sequence |
        If :obj:`token_ids_1` is :obj:`None`, this method only returns the first portion of the mask (0s).
        Args:
            token_ids_0 (:obj:`List[int]`):
                List of IDs.
            token_ids_1 (:obj:`List[int]`, `optional`):
                Optional second list of IDs for sequence pairs.
        Returns:
            :obj:`List[int]`: List of `token type IDs &lt;../glossary.html#token-type-ids&gt;`_ according to the given
            sequence(s).
        """</span>
        sep <span class="token operator">=</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>sep_token_id<span class="token punctuation">]</span>
        cls <span class="token operator">=</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>cls_token_id<span class="token punctuation">]</span>
        <span class="token keyword">if</span> token_ids_1 <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>cls <span class="token operator">+</span> token_ids_0 <span class="token operator">+</span> sep<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>cls <span class="token operator">+</span> token_ids_0 <span class="token operator">+</span> sep<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token builtin">len</span><span class="token punctuation">(</span>token_ids_1 <span class="token operator">+</span> sep<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">save_vocabulary</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> save_directory<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span> filename_prefix<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Tuple<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
        index <span class="token operator">=</span> <span class="token number">0</span>
        <span class="token keyword">if</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isdir<span class="token punctuation">(</span>save_directory<span class="token punctuation">)</span><span class="token punctuation">:</span>
            vocab_file <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>
                save_directory<span class="token punctuation">,</span> <span class="token punctuation">(</span>filename_prefix <span class="token operator">+</span> <span class="token string">"-"</span> <span class="token keyword">if</span> filename_prefix <span class="token keyword">else</span> <span class="token string">""</span><span class="token punctuation">)</span> <span class="token operator">+</span> VOCAB_FILES_NAMES<span class="token punctuation">[</span><span class="token string">"vocab_file"</span><span class="token punctuation">]</span>
            <span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            vocab_file <span class="token operator">=</span> <span class="token punctuation">(</span>filename_prefix <span class="token operator">+</span> <span class="token string">"-"</span> <span class="token keyword">if</span> filename_prefix <span class="token keyword">else</span> <span class="token string">""</span><span class="token punctuation">)</span> <span class="token operator">+</span> save_directory
        <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>vocab_file<span class="token punctuation">,</span> <span class="token string">"w"</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> writer<span class="token punctuation">:</span>
            <span class="token keyword">for</span> token<span class="token punctuation">,</span> token_index <span class="token keyword">in</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> key<span class="token operator">=</span><span class="token keyword">lambda</span> kv<span class="token punctuation">:</span> kv<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token keyword">if</span> index <span class="token operator">!=</span> token_index<span class="token punctuation">:</span>
                    logger<span class="token punctuation">.</span>warning<span class="token punctuation">(</span>
                        <span class="token string-interpolation"><span class="token string">f"Saving vocabulary to </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>vocab_file<span class="token punctuation">}</span></span><span class="token string">: vocabulary indices are not consecutive."</span></span>
                        <span class="token string">" Please check that the vocabulary is not corrupted!"</span>
                    <span class="token punctuation">)</span>
                    index <span class="token operator">=</span> token_index
                writer<span class="token punctuation">.</span>write<span class="token punctuation">(</span>token <span class="token operator">+</span> <span class="token string">"\n"</span><span class="token punctuation">)</span>
                index <span class="token operator">+=</span> <span class="token number">1</span>
        <span class="token keyword">return</span> <span class="token punctuation">(</span>vocab_file<span class="token punctuation">,</span><span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">BasicTokenizer</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> do_lower_case<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> never_split<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> tokenize_chinese_chars<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> strip_accents<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> never_split <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            never_split <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>do_lower_case <span class="token operator">=</span> do_lower_case
        self<span class="token punctuation">.</span>never_split <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span>never_split<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>tokenize_chinese_chars <span class="token operator">=</span> tokenize_chinese_chars
        self<span class="token punctuation">.</span>strip_accents <span class="token operator">=</span> strip_accents

    <span class="token keyword">def</span> <span class="token function">tokenize</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">,</span> never_split<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Basic Tokenization of a piece of text. Split on "white spaces" only, for sub-word tokenization, see
        WordPieceTokenizer.
        Args:
            **never_split**: (`optional`) list of str
                Kept for backward compatibility purposes. Now implemented directly at the base class level (see
                :func:`PreTrainedTokenizer.tokenize`) List of token not to split.
        """</span>
        <span class="token comment"># union() returns a new set by concatenating the two sets.</span>
        never_split <span class="token operator">=</span> self<span class="token punctuation">.</span>never_split<span class="token punctuation">.</span>union<span class="token punctuation">(</span><span class="token builtin">set</span><span class="token punctuation">(</span>never_split<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">if</span> never_split <span class="token keyword">else</span> self<span class="token punctuation">.</span>never_split
        text <span class="token operator">=</span> self<span class="token punctuation">.</span>_clean_text<span class="token punctuation">(</span>text<span class="token punctuation">)</span>

        <span class="token comment"># This was added on November 1st, 2018 for the multilingual and Chinese</span>
        <span class="token comment"># models. This is also applied to the English models now, but it doesn't</span>
        <span class="token comment"># matter since the English models were not trained on any Chinese data</span>
        <span class="token comment"># and generally don't have any Chinese data in them (there are Chinese</span>
        <span class="token comment"># characters in the vocabulary because Wikipedia does have some Chinese</span>
        <span class="token comment"># words in the English Wikipedia.).</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>tokenize_chinese_chars<span class="token punctuation">:</span>
            text <span class="token operator">=</span> self<span class="token punctuation">.</span>_tokenize_chinese_chars<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
        orig_tokens <span class="token operator">=</span> whitespace_tokenize<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
        split_tokens <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> token <span class="token keyword">in</span> orig_tokens<span class="token punctuation">:</span>
            <span class="token keyword">if</span> token <span class="token keyword">not</span> <span class="token keyword">in</span> never_split<span class="token punctuation">:</span>
                <span class="token keyword">if</span> self<span class="token punctuation">.</span>do_lower_case<span class="token punctuation">:</span>
                    token <span class="token operator">=</span> token<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span>
                    <span class="token keyword">if</span> self<span class="token punctuation">.</span>strip_accents <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">False</span><span class="token punctuation">:</span>
                        token <span class="token operator">=</span> self<span class="token punctuation">.</span>_run_strip_accents<span class="token punctuation">(</span>token<span class="token punctuation">)</span>
                <span class="token keyword">elif</span> self<span class="token punctuation">.</span>strip_accents<span class="token punctuation">:</span>
                    token <span class="token operator">=</span> self<span class="token punctuation">.</span>_run_strip_accents<span class="token punctuation">(</span>token<span class="token punctuation">)</span>
            split_tokens<span class="token punctuation">.</span>extend<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_run_split_on_punc<span class="token punctuation">(</span>token<span class="token punctuation">,</span> never_split<span class="token punctuation">)</span><span class="token punctuation">)</span>

        output_tokens <span class="token operator">=</span> whitespace_tokenize<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>split_tokens<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> output_tokens

    <span class="token keyword">def</span> <span class="token function">_run_strip_accents</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Strips accents from a piece of text."""</span>
        text <span class="token operator">=</span> unicodedata<span class="token punctuation">.</span>normalize<span class="token punctuation">(</span><span class="token string">"NFD"</span><span class="token punctuation">,</span> text<span class="token punctuation">)</span>
        output <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> char <span class="token keyword">in</span> text<span class="token punctuation">:</span>
            cat <span class="token operator">=</span> unicodedata<span class="token punctuation">.</span>category<span class="token punctuation">(</span>char<span class="token punctuation">)</span>
            <span class="token keyword">if</span> cat <span class="token operator">==</span> <span class="token string">"Mn"</span><span class="token punctuation">:</span>
                <span class="token keyword">continue</span>
            output<span class="token punctuation">.</span>append<span class="token punctuation">(</span>char<span class="token punctuation">)</span>
        <span class="token keyword">return</span> <span class="token string">""</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>output<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">_run_split_on_punc</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">,</span> never_split<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Splits punctuation on a piece of text."""</span>
        <span class="token keyword">if</span> never_split <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">and</span> text <span class="token keyword">in</span> never_split<span class="token punctuation">:</span>
            <span class="token keyword">return</span> <span class="token punctuation">[</span>text<span class="token punctuation">]</span>
        chars <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span>
        i <span class="token operator">=</span> <span class="token number">0</span>
        start_new_word <span class="token operator">=</span> <span class="token boolean">True</span>
        output <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">while</span> i <span class="token operator">&lt;</span> <span class="token builtin">len</span><span class="token punctuation">(</span>chars<span class="token punctuation">)</span><span class="token punctuation">:</span>
            char <span class="token operator">=</span> chars<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
            <span class="token keyword">if</span> _is_punctuation<span class="token punctuation">(</span>char<span class="token punctuation">)</span><span class="token punctuation">:</span>
                output<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">[</span>char<span class="token punctuation">]</span><span class="token punctuation">)</span>
                start_new_word <span class="token operator">=</span> <span class="token boolean">True</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                <span class="token keyword">if</span> start_new_word<span class="token punctuation">:</span>
                    output<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
                start_new_word <span class="token operator">=</span> <span class="token boolean">False</span>
                output<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>char<span class="token punctuation">)</span>
            i <span class="token operator">+=</span> <span class="token number">1</span>

        <span class="token keyword">return</span> <span class="token punctuation">[</span><span class="token string">""</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> output<span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">_tokenize_chinese_chars</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Adds whitespace around any CJK character."""</span>
        output <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> char <span class="token keyword">in</span> text<span class="token punctuation">:</span>
            cp <span class="token operator">=</span> <span class="token builtin">ord</span><span class="token punctuation">(</span>char<span class="token punctuation">)</span>
            <span class="token keyword">if</span> self<span class="token punctuation">.</span>_is_chinese_char<span class="token punctuation">(</span>cp<span class="token punctuation">)</span><span class="token punctuation">:</span>
                output<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span>
                output<span class="token punctuation">.</span>append<span class="token punctuation">(</span>char<span class="token punctuation">)</span>
                output<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                output<span class="token punctuation">.</span>append<span class="token punctuation">(</span>char<span class="token punctuation">)</span>
        <span class="token keyword">return</span> <span class="token string">""</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>output<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">_is_chinese_char</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> cp<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Checks whether CP is the codepoint of a CJK character."""</span>
        <span class="token comment"># This defines a "chinese character" as anything in the CJK Unicode block:</span>
        <span class="token comment">#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)</span>
        <span class="token comment">#</span>
        <span class="token comment"># Note that the CJK Unicode block is NOT all Japanese and Korean characters,</span>
        <span class="token comment"># despite its name. The modern Korean Hangul alphabet is a different block,</span>
        <span class="token comment"># as is Japanese Hiragana and Katakana. Those alphabets are used to write</span>
        <span class="token comment"># space-separated words, so they are not treated specially and handled</span>
        <span class="token comment"># like the all of the other languages.</span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span>
            <span class="token punctuation">(</span>cp <span class="token operator">&gt;=</span> <span class="token number">0x4E00</span> <span class="token keyword">and</span> cp <span class="token operator">&lt;=</span> <span class="token number">0x9FFF</span><span class="token punctuation">)</span>
            <span class="token keyword">or</span> <span class="token punctuation">(</span>cp <span class="token operator">&gt;=</span> <span class="token number">0x3400</span> <span class="token keyword">and</span> cp <span class="token operator">&lt;=</span> <span class="token number">0x4DBF</span><span class="token punctuation">)</span>  <span class="token comment">#</span>
            <span class="token keyword">or</span> <span class="token punctuation">(</span>cp <span class="token operator">&gt;=</span> <span class="token number">0x20000</span> <span class="token keyword">and</span> cp <span class="token operator">&lt;=</span> <span class="token number">0x2A6DF</span><span class="token punctuation">)</span>  <span class="token comment">#</span>
            <span class="token keyword">or</span> <span class="token punctuation">(</span>cp <span class="token operator">&gt;=</span> <span class="token number">0x2A700</span> <span class="token keyword">and</span> cp <span class="token operator">&lt;=</span> <span class="token number">0x2B73F</span><span class="token punctuation">)</span>  <span class="token comment">#</span>
            <span class="token keyword">or</span> <span class="token punctuation">(</span>cp <span class="token operator">&gt;=</span> <span class="token number">0x2B740</span> <span class="token keyword">and</span> cp <span class="token operator">&lt;=</span> <span class="token number">0x2B81F</span><span class="token punctuation">)</span>  <span class="token comment">#</span>
            <span class="token keyword">or</span> <span class="token punctuation">(</span>cp <span class="token operator">&gt;=</span> <span class="token number">0x2B820</span> <span class="token keyword">and</span> cp <span class="token operator">&lt;=</span> <span class="token number">0x2CEAF</span><span class="token punctuation">)</span>  <span class="token comment">#</span>
            <span class="token keyword">or</span> <span class="token punctuation">(</span>cp <span class="token operator">&gt;=</span> <span class="token number">0xF900</span> <span class="token keyword">and</span> cp <span class="token operator">&lt;=</span> <span class="token number">0xFAFF</span><span class="token punctuation">)</span>
            <span class="token keyword">or</span> <span class="token punctuation">(</span>cp <span class="token operator">&gt;=</span> <span class="token number">0x2F800</span> <span class="token keyword">and</span> cp <span class="token operator">&lt;=</span> <span class="token number">0x2FA1F</span><span class="token punctuation">)</span>  <span class="token comment">#</span>
        <span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#</span>
            <span class="token keyword">return</span> <span class="token boolean">True</span>

        <span class="token keyword">return</span> <span class="token boolean">False</span>

    <span class="token keyword">def</span> <span class="token function">_clean_text</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Performs invalid character removal and whitespace cleanup on text."""</span>
        output <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> char <span class="token keyword">in</span> text<span class="token punctuation">:</span>
            cp <span class="token operator">=</span> <span class="token builtin">ord</span><span class="token punctuation">(</span>char<span class="token punctuation">)</span>
            <span class="token keyword">if</span> cp <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">or</span> cp <span class="token operator">==</span> <span class="token number">0xFFFD</span> <span class="token keyword">or</span> _is_control<span class="token punctuation">(</span>char<span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token keyword">continue</span>
            <span class="token keyword">if</span> _is_whitespace<span class="token punctuation">(</span>char<span class="token punctuation">)</span><span class="token punctuation">:</span>
                output<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                output<span class="token punctuation">.</span>append<span class="token punctuation">(</span>char<span class="token punctuation">)</span>
        <span class="token keyword">return</span> <span class="token string">""</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>output<span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">WordpieceTokenizer</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Runs WordPiece tokenization."""</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab<span class="token punctuation">,</span> unk_token<span class="token punctuation">,</span> max_input_chars_per_word<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>vocab <span class="token operator">=</span> vocab
        self<span class="token punctuation">.</span>unk_token <span class="token operator">=</span> unk_token
        self<span class="token punctuation">.</span>max_input_chars_per_word <span class="token operator">=</span> max_input_chars_per_word

    <span class="token keyword">def</span> <span class="token function">tokenize</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform
        tokenization using the given vocabulary.
        For example, :obj:`input = "unaffable"` wil return as output :obj:`["un", "##aff", "##able"]`.
        Args:
          text: A single token or whitespace separated tokens. This should have
            already been passed through `BasicTokenizer`.
        Returns:
          A list of wordpiece tokens.
        """</span>

        output_tokens <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> token <span class="token keyword">in</span> whitespace_tokenize<span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>
            chars <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>token<span class="token punctuation">)</span>
            <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>chars<span class="token punctuation">)</span> <span class="token operator">&gt;</span> self<span class="token punctuation">.</span>max_input_chars_per_word<span class="token punctuation">:</span>
                output_tokens<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>unk_token<span class="token punctuation">)</span>
                <span class="token keyword">continue</span>

            is_bad <span class="token operator">=</span> <span class="token boolean">False</span>
            start <span class="token operator">=</span> <span class="token number">0</span>
            sub_tokens <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
            <span class="token keyword">while</span> start <span class="token operator">&lt;</span> <span class="token builtin">len</span><span class="token punctuation">(</span>chars<span class="token punctuation">)</span><span class="token punctuation">:</span>
                end <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>chars<span class="token punctuation">)</span>
                cur_substr <span class="token operator">=</span> <span class="token boolean">None</span>
                <span class="token keyword">while</span> start <span class="token operator">&lt;</span> end<span class="token punctuation">:</span>
                    substr <span class="token operator">=</span> <span class="token string">""</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>chars<span class="token punctuation">[</span>start<span class="token punctuation">:</span>end<span class="token punctuation">]</span><span class="token punctuation">)</span>
                    <span class="token keyword">if</span> start <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
                        substr <span class="token operator">=</span> <span class="token string">"##"</span> <span class="token operator">+</span> substr
                    <span class="token keyword">if</span> substr <span class="token keyword">in</span> self<span class="token punctuation">.</span>vocab<span class="token punctuation">:</span>
                        cur_substr <span class="token operator">=</span> substr
                        <span class="token keyword">break</span>
                    end <span class="token operator">-=</span> <span class="token number">1</span>
                <span class="token keyword">if</span> cur_substr <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                    is_bad <span class="token operator">=</span> <span class="token boolean">True</span>
                    <span class="token keyword">break</span>
                sub_tokens<span class="token punctuation">.</span>append<span class="token punctuation">(</span>cur_substr<span class="token punctuation">)</span>
                start <span class="token operator">=</span> end

            <span class="token keyword">if</span> is_bad<span class="token punctuation">:</span>
                output_tokens<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>unk_token<span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                output_tokens<span class="token punctuation">.</span>extend<span class="token punctuation">(</span>sub_tokens<span class="token punctuation">)</span>
        <span class="token keyword">return</span> output_tokens
</code></pre> 
<pre><code class="prism language-python">bt <span class="token operator">=</span> BertTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'bert-base-uncased'</span><span class="token punctuation">)</span>
bt<span class="token punctuation">(</span><span class="token string">'I like natural language progressing!'</span><span class="token punctuation">)</span>
<span class="token comment"># {'input_ids': [101, 1045, 2066, 3019, 2653, 27673, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}</span>
</code></pre> 
<h2><a id="2_Model_BertModel_432"></a>2 Model BertModel</h2> 
<p>BertModel 主要为 transformer encoder 结构，包含三个部分：</p> 
<p>1 embeddings，即BertEmbeddings类的实体， 根据单词符号获取对应的向量表示；<br> 2 encoder，即BertEncoder类的实体；<br> 3 pooler，即BertPooler类的实体，这一部分是可选的。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> transformers<span class="token punctuation">.</span>models<span class="token punctuation">.</span>bert<span class="token punctuation">.</span>modeling_bert <span class="token keyword">import</span> <span class="token operator">*</span>
<span class="token keyword">class</span> <span class="token class-name">BertModel</span><span class="token punctuation">(</span>BertPreTrainedModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of
    cross-attention is added between the self-attention layers, following the architecture described in `Attention is
    all you need &lt;https://arxiv.org/abs/1706.03762&gt;`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.
    To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration
    set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder`
    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an
    input to the forward pass.
    """</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">,</span> add_pooling_layer<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>config <span class="token operator">=</span> config

        self<span class="token punctuation">.</span>embeddings <span class="token operator">=</span> BertEmbeddings<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> BertEncoder<span class="token punctuation">(</span>config<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>pooler <span class="token operator">=</span> BertPooler<span class="token punctuation">(</span>config<span class="token punctuation">)</span> <span class="token keyword">if</span> add_pooling_layer <span class="token keyword">else</span> <span class="token boolean">None</span>

        self<span class="token punctuation">.</span>init_weights<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">get_input_embeddings</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>embeddings<span class="token punctuation">.</span>word_embeddings

    <span class="token keyword">def</span> <span class="token function">set_input_embeddings</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> value<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>embeddings<span class="token punctuation">.</span>word_embeddings <span class="token operator">=</span> value

    <span class="token keyword">def</span> <span class="token function">_prune_heads</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> heads_to_prune<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base
        class PreTrainedModel
        """</span>
        <span class="token keyword">for</span> layer<span class="token punctuation">,</span> heads <span class="token keyword">in</span> heads_to_prune<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layer<span class="token punctuation">[</span>layer<span class="token punctuation">]</span><span class="token punctuation">.</span>attention<span class="token punctuation">.</span>prune_heads<span class="token punctuation">(</span>heads<span class="token punctuation">)</span>

    <span class="token decorator annotation punctuation">@add_start_docstrings_to_model_forward</span><span class="token punctuation">(</span>BERT_INPUTS_DOCSTRING<span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token string">"batch_size, sequence_length"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token decorator annotation punctuation">@add_code_sample_docstrings</span><span class="token punctuation">(</span>
        tokenizer_class<span class="token operator">=</span>_TOKENIZER_FOR_DOC<span class="token punctuation">,</span>
        checkpoint<span class="token operator">=</span>_CHECKPOINT_FOR_DOC<span class="token punctuation">,</span>
        output_type<span class="token operator">=</span>BaseModelOutputWithPoolingAndCrossAttentions<span class="token punctuation">,</span>
        config_class<span class="token operator">=</span>_CONFIG_FOR_DOC<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        input_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        attention_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        token_type_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        position_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        head_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        inputs_embeds<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        encoder_hidden_states<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        encoder_attention_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        past_key_values<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        use_cache<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        output_attentions<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        output_hidden_states<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        return_dict<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">r"""
        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):
            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if
            the model is configured as a decoder.
        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):
            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in
            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:
            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.
        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):
            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.
            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`
            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`
            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.
        use_cache (:obj:`bool`, `optional`):
            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up
            decoding (see :obj:`past_key_values`).
        """</span>
        output_attentions <span class="token operator">=</span> output_attentions <span class="token keyword">if</span> output_attentions <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>output_attentions
        output_hidden_states <span class="token operator">=</span> <span class="token punctuation">(</span>
            output_hidden_states <span class="token keyword">if</span> output_hidden_states <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>output_hidden_states
        <span class="token punctuation">)</span>
        return_dict <span class="token operator">=</span> return_dict <span class="token keyword">if</span> return_dict <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>use_return_dict

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>is_decoder<span class="token punctuation">:</span>
            use_cache <span class="token operator">=</span> use_cache <span class="token keyword">if</span> use_cache <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>use_cache
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            use_cache <span class="token operator">=</span> <span class="token boolean">False</span>

        <span class="token keyword">if</span> input_ids <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">and</span> inputs_embeds <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"You cannot specify both input_ids and inputs_embeds at the same time"</span><span class="token punctuation">)</span>
        <span class="token keyword">elif</span> input_ids <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            input_shape <span class="token operator">=</span> input_ids<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
            batch_size<span class="token punctuation">,</span> seq_length <span class="token operator">=</span> input_shape
        <span class="token keyword">elif</span> inputs_embeds <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            input_shape <span class="token operator">=</span> inputs_embeds<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
            batch_size<span class="token punctuation">,</span> seq_length <span class="token operator">=</span> input_shape
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"You have to specify either input_ids or inputs_embeds"</span><span class="token punctuation">)</span>

        device <span class="token operator">=</span> input_ids<span class="token punctuation">.</span>device <span class="token keyword">if</span> input_ids <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> inputs_embeds<span class="token punctuation">.</span>device

        <span class="token comment"># past_key_values_length</span>
        past_key_values_length <span class="token operator">=</span> past_key_values<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token keyword">if</span> past_key_values <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> <span class="token number">0</span>

        <span class="token keyword">if</span> attention_mask <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            attention_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_length <span class="token operator">+</span> past_key_values_length<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>

        <span class="token keyword">if</span> token_type_ids <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>embeddings<span class="token punctuation">,</span> <span class="token string">"token_type_ids"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                buffered_token_type_ids <span class="token operator">=</span> self<span class="token punctuation">.</span>embeddings<span class="token punctuation">.</span>token_type_ids<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>seq_length<span class="token punctuation">]</span>
                buffered_token_type_ids_expanded <span class="token operator">=</span> buffered_token_type_ids<span class="token punctuation">.</span>expand<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_length<span class="token punctuation">)</span>
                token_type_ids <span class="token operator">=</span> buffered_token_type_ids_expanded
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                token_type_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>input_shape<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>

        <span class="token comment"># We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]</span>
        <span class="token comment"># ourselves in which case we just need to make it broadcastable to all heads.</span>
        extended_attention_mask<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor <span class="token operator">=</span> self<span class="token punctuation">.</span>get_extended_attention_mask<span class="token punctuation">(</span>attention_mask<span class="token punctuation">,</span> input_shape<span class="token punctuation">,</span> device<span class="token punctuation">)</span>

        <span class="token comment"># If a 2D or 3D attention mask is provided for the cross-attention</span>
        <span class="token comment"># we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>is_decoder <span class="token keyword">and</span> encoder_hidden_states <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            encoder_batch_size<span class="token punctuation">,</span> encoder_sequence_length<span class="token punctuation">,</span> _ <span class="token operator">=</span> encoder_hidden_states<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
            encoder_hidden_shape <span class="token operator">=</span> <span class="token punctuation">(</span>encoder_batch_size<span class="token punctuation">,</span> encoder_sequence_length<span class="token punctuation">)</span>
            <span class="token keyword">if</span> encoder_attention_mask <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                encoder_attention_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>encoder_hidden_shape<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>
            encoder_extended_attention_mask <span class="token operator">=</span> self<span class="token punctuation">.</span>invert_attention_mask<span class="token punctuation">(</span>encoder_attention_mask<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            encoder_extended_attention_mask <span class="token operator">=</span> <span class="token boolean">None</span>

        <span class="token comment"># Prepare head mask if needed</span>
        <span class="token comment"># 1.0 in head_mask indicate we keep the head</span>
        <span class="token comment"># attention_probs has shape bsz x n_heads x N x N</span>
        <span class="token comment"># input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]</span>
        <span class="token comment"># and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]</span>
        head_mask <span class="token operator">=</span> self<span class="token punctuation">.</span>get_head_mask<span class="token punctuation">(</span>head_mask<span class="token punctuation">,</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>num_hidden_layers<span class="token punctuation">)</span>

        embedding_output <span class="token operator">=</span> self<span class="token punctuation">.</span>embeddings<span class="token punctuation">(</span>
            input_ids<span class="token operator">=</span>input_ids<span class="token punctuation">,</span>
            position_ids<span class="token operator">=</span>position_ids<span class="token punctuation">,</span>
            token_type_ids<span class="token operator">=</span>token_type_ids<span class="token punctuation">,</span>
            inputs_embeds<span class="token operator">=</span>inputs_embeds<span class="token punctuation">,</span>
            past_key_values_length<span class="token operator">=</span>past_key_values_length<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        encoder_outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>
            embedding_output<span class="token punctuation">,</span>
            attention_mask<span class="token operator">=</span>extended_attention_mask<span class="token punctuation">,</span>
            head_mask<span class="token operator">=</span>head_mask<span class="token punctuation">,</span>
            encoder_hidden_states<span class="token operator">=</span>encoder_hidden_states<span class="token punctuation">,</span>
            encoder_attention_mask<span class="token operator">=</span>encoder_extended_attention_mask<span class="token punctuation">,</span>
            past_key_values<span class="token operator">=</span>past_key_values<span class="token punctuation">,</span>
            use_cache<span class="token operator">=</span>use_cache<span class="token punctuation">,</span>
            output_attentions<span class="token operator">=</span>output_attentions<span class="token punctuation">,</span>
            output_hidden_states<span class="token operator">=</span>output_hidden_states<span class="token punctuation">,</span>
            return_dict<span class="token operator">=</span>return_dict<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        sequence_output <span class="token operator">=</span> encoder_outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        pooled_output <span class="token operator">=</span> self<span class="token punctuation">.</span>pooler<span class="token punctuation">(</span>sequence_output<span class="token punctuation">)</span> <span class="token keyword">if</span> self<span class="token punctuation">.</span>pooler <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> <span class="token boolean">None</span>

        <span class="token keyword">if</span> <span class="token keyword">not</span> return_dict<span class="token punctuation">:</span>
            <span class="token keyword">return</span> <span class="token punctuation">(</span>sequence_output<span class="token punctuation">,</span> pooled_output<span class="token punctuation">)</span> <span class="token operator">+</span> encoder_outputs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>

        <span class="token keyword">return</span> BaseModelOutputWithPoolingAndCrossAttentions<span class="token punctuation">(</span>
            last_hidden_state<span class="token operator">=</span>sequence_output<span class="token punctuation">,</span>
            pooler_output<span class="token operator">=</span>pooled_output<span class="token punctuation">,</span>
            past_key_values<span class="token operator">=</span>encoder_outputs<span class="token punctuation">.</span>past_key_values<span class="token punctuation">,</span>
            hidden_states<span class="token operator">=</span>encoder_outputs<span class="token punctuation">.</span>hidden_states<span class="token punctuation">,</span>
            attentions<span class="token operator">=</span>encoder_outputs<span class="token punctuation">.</span>attentions<span class="token punctuation">,</span>
            cross_attentions<span class="token operator">=</span>encoder_outputs<span class="token punctuation">.</span>cross_attentions<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>





</code></pre> 
<h3><a id="21_BertEmberddings_619"></a>2.1 BertEmberddings</h3> 
<p><img src="https://images2.imgbox.com/db/05/F4ATeMir_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">BertEmbeddings</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Construct the embeddings from word, position and token_type embeddings."""</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>word_embeddings <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>config<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> padding_idx<span class="token operator">=</span>config<span class="token punctuation">.</span>pad_token_id<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>position_embeddings <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>config<span class="token punctuation">.</span>max_position_embeddings<span class="token punctuation">,</span> config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>token_type_embeddings <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>config<span class="token punctuation">.</span>type_vocab_size<span class="token punctuation">,</span> config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>

        <span class="token comment"># self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load</span>
        <span class="token comment"># any TensorFlow checkpoint file</span>
        self<span class="token punctuation">.</span>LayerNorm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> eps<span class="token operator">=</span>config<span class="token punctuation">.</span>layer_norm_eps<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_dropout_prob<span class="token punctuation">)</span>
        <span class="token comment"># position_ids (1, len position emb) is contiguous in memory and exported when serialized</span>
        self<span class="token punctuation">.</span>position_embedding_type <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>config<span class="token punctuation">,</span> <span class="token string">"position_embedding_type"</span><span class="token punctuation">,</span> <span class="token string">"absolute"</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"position_ids"</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>config<span class="token punctuation">.</span>max_position_embeddings<span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> version<span class="token punctuation">.</span>parse<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>__version__<span class="token punctuation">)</span> <span class="token operator">&gt;</span> version<span class="token punctuation">.</span>parse<span class="token punctuation">(</span><span class="token string">"1.6.0"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span>
                <span class="token string">"token_type_ids"</span><span class="token punctuation">,</span>
                torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>self<span class="token punctuation">.</span>position_ids<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">,</span> device<span class="token operator">=</span>self<span class="token punctuation">.</span>position_ids<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span>
                persistent<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
            <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span> input_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> token_type_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> position_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> inputs_embeds<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> past_key_values_length<span class="token operator">=</span><span class="token number">0</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> input_ids <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            input_shape <span class="token operator">=</span> input_ids<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            input_shape <span class="token operator">=</span> inputs_embeds<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>

        seq_length <span class="token operator">=</span> input_shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>

        <span class="token keyword">if</span> position_ids <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            position_ids <span class="token operator">=</span> self<span class="token punctuation">.</span>position_ids<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> past_key_values_length <span class="token punctuation">:</span> seq_length <span class="token operator">+</span> past_key_values_length<span class="token punctuation">]</span>

        <span class="token comment"># Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs</span>
        <span class="token comment"># when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves</span>
        <span class="token comment"># issue #5664</span>
        <span class="token keyword">if</span> token_type_ids <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token string">"token_type_ids"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                buffered_token_type_ids <span class="token operator">=</span> self<span class="token punctuation">.</span>token_type_ids<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>seq_length<span class="token punctuation">]</span>
                buffered_token_type_ids_expanded <span class="token operator">=</span> buffered_token_type_ids<span class="token punctuation">.</span>expand<span class="token punctuation">(</span>input_shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> seq_length<span class="token punctuation">)</span>
                token_type_ids <span class="token operator">=</span> buffered_token_type_ids_expanded
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                token_type_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>input_shape<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">,</span> device<span class="token operator">=</span>self<span class="token punctuation">.</span>position_ids<span class="token punctuation">.</span>device<span class="token punctuation">)</span>

        <span class="token keyword">if</span> inputs_embeds <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            inputs_embeds <span class="token operator">=</span> self<span class="token punctuation">.</span>word_embeddings<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>
        token_type_embeddings <span class="token operator">=</span> self<span class="token punctuation">.</span>token_type_embeddings<span class="token punctuation">(</span>token_type_ids<span class="token punctuation">)</span>

        embeddings <span class="token operator">=</span> inputs_embeds <span class="token operator">+</span> token_type_embeddings
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>position_embedding_type <span class="token operator">==</span> <span class="token string">"absolute"</span><span class="token punctuation">:</span>
            position_embeddings <span class="token operator">=</span> self<span class="token punctuation">.</span>position_embeddings<span class="token punctuation">(</span>position_ids<span class="token punctuation">)</span>
            embeddings <span class="token operator">+=</span> position_embeddings
        embeddings <span class="token operator">=</span> self<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>embeddings<span class="token punctuation">)</span>
        embeddings <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>embeddings<span class="token punctuation">)</span>
        <span class="token keyword">return</span> embeddings
</code></pre> 
<h3><a id="22_BertEncoder_683"></a>2.2 BertEncoder</h3> 
<p>包含多层BertLayer</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">BertEncoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>config <span class="token operator">=</span> config
        self<span class="token punctuation">.</span>layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>BertLayer<span class="token punctuation">(</span>config<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>config<span class="token punctuation">.</span>num_hidden_layers<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        hidden_states<span class="token punctuation">,</span>
        attention_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        head_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        encoder_hidden_states<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        encoder_attention_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        past_key_values<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        use_cache<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        output_attentions<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
        output_hidden_states<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
        return_dict<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        all_hidden_states <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> output_hidden_states <span class="token keyword">else</span> <span class="token boolean">None</span>
        all_self_attentions <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> output_attentions <span class="token keyword">else</span> <span class="token boolean">None</span>
        all_cross_attentions <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> output_attentions <span class="token keyword">and</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>add_cross_attention <span class="token keyword">else</span> <span class="token boolean">None</span>

        next_decoder_cache <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> use_cache <span class="token keyword">else</span> <span class="token boolean">None</span>
        <span class="token keyword">for</span> i<span class="token punctuation">,</span> layer_module <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>layer<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> output_hidden_states<span class="token punctuation">:</span>
                all_hidden_states <span class="token operator">=</span> all_hidden_states <span class="token operator">+</span> <span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span><span class="token punctuation">)</span>

            layer_head_mask <span class="token operator">=</span> head_mask<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">if</span> head_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> <span class="token boolean">None</span>
            past_key_value <span class="token operator">=</span> past_key_values<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">if</span> past_key_values <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> <span class="token boolean">None</span>

            <span class="token keyword">if</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">,</span> <span class="token string">"gradient_checkpointing"</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">)</span> <span class="token keyword">and</span> self<span class="token punctuation">.</span>training<span class="token punctuation">:</span>

                <span class="token keyword">if</span> use_cache<span class="token punctuation">:</span>
                    logger<span class="token punctuation">.</span>warning<span class="token punctuation">(</span>
                        <span class="token string">"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting "</span>
                        <span class="token string">"`use_cache=False`..."</span>
                    <span class="token punctuation">)</span>
                    use_cache <span class="token operator">=</span> <span class="token boolean">False</span>

                <span class="token keyword">def</span> <span class="token function">create_custom_forward</span><span class="token punctuation">(</span>module<span class="token punctuation">)</span><span class="token punctuation">:</span>
                    <span class="token keyword">def</span> <span class="token function">custom_forward</span><span class="token punctuation">(</span><span class="token operator">*</span>inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
                        <span class="token keyword">return</span> module<span class="token punctuation">(</span><span class="token operator">*</span>inputs<span class="token punctuation">,</span> past_key_value<span class="token punctuation">,</span> output_attentions<span class="token punctuation">)</span>

                    <span class="token keyword">return</span> custom_forward

                layer_outputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>checkpoint<span class="token punctuation">.</span>checkpoint<span class="token punctuation">(</span>
                    create_custom_forward<span class="token punctuation">(</span>layer_module<span class="token punctuation">)</span><span class="token punctuation">,</span>
                    hidden_states<span class="token punctuation">,</span>
                    attention_mask<span class="token punctuation">,</span>
                    layer_head_mask<span class="token punctuation">,</span>
                    encoder_hidden_states<span class="token punctuation">,</span>
                    encoder_attention_mask<span class="token punctuation">,</span>
                <span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                layer_outputs <span class="token operator">=</span> layer_module<span class="token punctuation">(</span>
                    hidden_states<span class="token punctuation">,</span>
                    attention_mask<span class="token punctuation">,</span>
                    layer_head_mask<span class="token punctuation">,</span>
                    encoder_hidden_states<span class="token punctuation">,</span>
                    encoder_attention_mask<span class="token punctuation">,</span>
                    past_key_value<span class="token punctuation">,</span>
                    output_attentions<span class="token punctuation">,</span>
                <span class="token punctuation">)</span>

            hidden_states <span class="token operator">=</span> layer_outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
            <span class="token keyword">if</span> use_cache<span class="token punctuation">:</span>
                next_decoder_cache <span class="token operator">+=</span> <span class="token punctuation">(</span>layer_outputs<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> output_attentions<span class="token punctuation">:</span>
                all_self_attentions <span class="token operator">=</span> all_self_attentions <span class="token operator">+</span> <span class="token punctuation">(</span>layer_outputs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
                <span class="token keyword">if</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>add_cross_attention<span class="token punctuation">:</span>
                    all_cross_attentions <span class="token operator">=</span> all_cross_attentions <span class="token operator">+</span> <span class="token punctuation">(</span>layer_outputs<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> output_hidden_states<span class="token punctuation">:</span>
            all_hidden_states <span class="token operator">=</span> all_hidden_states <span class="token operator">+</span> <span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> <span class="token keyword">not</span> return_dict<span class="token punctuation">:</span>
            <span class="token keyword">return</span> <span class="token builtin">tuple</span><span class="token punctuation">(</span>
                v
                <span class="token keyword">for</span> v <span class="token keyword">in</span> <span class="token punctuation">[</span>
                    hidden_states<span class="token punctuation">,</span>
                    next_decoder_cache<span class="token punctuation">,</span>
                    all_hidden_states<span class="token punctuation">,</span>
                    all_self_attentions<span class="token punctuation">,</span>
                    all_cross_attentions<span class="token punctuation">,</span>
                <span class="token punctuation">]</span>
                <span class="token keyword">if</span> v <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span>
            <span class="token punctuation">)</span>
        <span class="token keyword">return</span> BaseModelOutputWithPastAndCrossAttentions<span class="token punctuation">(</span>
            last_hidden_state<span class="token operator">=</span>hidden_states<span class="token punctuation">,</span>
            past_key_values<span class="token operator">=</span>next_decoder_cache<span class="token punctuation">,</span>
            hidden_states<span class="token operator">=</span>all_hidden_states<span class="token punctuation">,</span>
            attentions<span class="token operator">=</span>all_self_attentions<span class="token punctuation">,</span>
            cross_attentions<span class="token operator">=</span>all_cross_attentions<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
</code></pre> 
<p>attention在下面还有一层</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">BertEncoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>config <span class="token operator">=</span> config
        self<span class="token punctuation">.</span>layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>BertLayer<span class="token punctuation">(</span>config<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>config<span class="token punctuation">.</span>num_hidden_layers<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        hidden_states<span class="token punctuation">,</span>
        attention_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        head_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        encoder_hidden_states<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        encoder_attention_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        past_key_values<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        use_cache<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        output_attentions<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
        output_hidden_states<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
        return_dict<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        all_hidden_states <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> output_hidden_states <span class="token keyword">else</span> <span class="token boolean">None</span>
        all_self_attentions <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> output_attentions <span class="token keyword">else</span> <span class="token boolean">None</span>
        all_cross_attentions <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> output_attentions <span class="token keyword">and</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>add_cross_attention <span class="token keyword">else</span> <span class="token boolean">None</span>

        next_decoder_cache <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> use_cache <span class="token keyword">else</span> <span class="token boolean">None</span>
        <span class="token keyword">for</span> i<span class="token punctuation">,</span> layer_module <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>layer<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> output_hidden_states<span class="token punctuation">:</span>
                all_hidden_states <span class="token operator">=</span> all_hidden_states <span class="token operator">+</span> <span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span><span class="token punctuation">)</span>

            layer_head_mask <span class="token operator">=</span> head_mask<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">if</span> head_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> <span class="token boolean">None</span>
            past_key_value <span class="token operator">=</span> past_key_values<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">if</span> past_key_values <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> <span class="token boolean">None</span>

            <span class="token keyword">if</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">,</span> <span class="token string">"gradient_checkpointing"</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">)</span> <span class="token keyword">and</span> self<span class="token punctuation">.</span>training<span class="token punctuation">:</span>

                <span class="token keyword">if</span> use_cache<span class="token punctuation">:</span>
                    logger<span class="token punctuation">.</span>warning<span class="token punctuation">(</span>
                        <span class="token string">"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting "</span>
                        <span class="token string">"`use_cache=False`..."</span>
                    <span class="token punctuation">)</span>
                    use_cache <span class="token operator">=</span> <span class="token boolean">False</span>

                <span class="token keyword">def</span> <span class="token function">create_custom_forward</span><span class="token punctuation">(</span>module<span class="token punctuation">)</span><span class="token punctuation">:</span>
                    <span class="token keyword">def</span> <span class="token function">custom_forward</span><span class="token punctuation">(</span><span class="token operator">*</span>inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
                        <span class="token keyword">return</span> module<span class="token punctuation">(</span><span class="token operator">*</span>inputs<span class="token punctuation">,</span> past_key_value<span class="token punctuation">,</span> output_attentions<span class="token punctuation">)</span>

                    <span class="token keyword">return</span> custom_forward

                layer_outputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>checkpoint<span class="token punctuation">.</span>checkpoint<span class="token punctuation">(</span>
                    create_custom_forward<span class="token punctuation">(</span>layer_module<span class="token punctuation">)</span><span class="token punctuation">,</span>
                    hidden_states<span class="token punctuation">,</span>
                    attention_mask<span class="token punctuation">,</span>
                    layer_head_mask<span class="token punctuation">,</span>
                    encoder_hidden_states<span class="token punctuation">,</span>
                    encoder_attention_mask<span class="token punctuation">,</span>
                <span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                layer_outputs <span class="token operator">=</span> layer_module<span class="token punctuation">(</span>
                    hidden_states<span class="token punctuation">,</span>
                    attention_mask<span class="token punctuation">,</span>
                    layer_head_mask<span class="token punctuation">,</span>
                    encoder_hidden_states<span class="token punctuation">,</span>
                    encoder_attention_mask<span class="token punctuation">,</span>
                    past_key_value<span class="token punctuation">,</span>
                    output_attentions<span class="token punctuation">,</span>
                <span class="token punctuation">)</span>

            hidden_states <span class="token operator">=</span> layer_outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
            <span class="token keyword">if</span> use_cache<span class="token punctuation">:</span>
                next_decoder_cache <span class="token operator">+=</span> <span class="token punctuation">(</span>layer_outputs<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> output_attentions<span class="token punctuation">:</span>
                all_self_attentions <span class="token operator">=</span> all_self_attentions <span class="token operator">+</span> <span class="token punctuation">(</span>layer_outputs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
                <span class="token keyword">if</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>add_cross_attention<span class="token punctuation">:</span>
                    all_cross_attentions <span class="token operator">=</span> all_cross_attentions <span class="token operator">+</span> <span class="token punctuation">(</span>layer_outputs<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> output_hidden_states<span class="token punctuation">:</span>
            all_hidden_states <span class="token operator">=</span> all_hidden_states <span class="token operator">+</span> <span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> <span class="token keyword">not</span> return_dict<span class="token punctuation">:</span>
            <span class="token keyword">return</span> <span class="token builtin">tuple</span><span class="token punctuation">(</span>
                v
                <span class="token keyword">for</span> v <span class="token keyword">in</span> <span class="token punctuation">[</span>
                    hidden_states<span class="token punctuation">,</span>
                    next_decoder_cache<span class="token punctuation">,</span>
                    all_hidden_states<span class="token punctuation">,</span>
                    all_self_attentions<span class="token punctuation">,</span>
                    all_cross_attentions<span class="token punctuation">,</span>
                <span class="token punctuation">]</span>
                <span class="token keyword">if</span> v <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span>
            <span class="token punctuation">)</span>
        <span class="token keyword">return</span> BaseModelOutputWithPastAndCrossAttentions<span class="token punctuation">(</span>
            last_hidden_state<span class="token operator">=</span>hidden_states<span class="token punctuation">,</span>
            past_key_values<span class="token operator">=</span>next_decoder_cache<span class="token punctuation">,</span>
            hidden_states<span class="token operator">=</span>all_hidden_states<span class="token punctuation">,</span>
            attentions<span class="token operator">=</span>all_self_attentions<span class="token punctuation">,</span>
            cross_attentions<span class="token operator">=</span>all_cross_attentions<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
</code></pre> 
<h5><a id="221_BertAttention_884"></a>2.2.1 BertAttention</h5> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">BertEncoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>config <span class="token operator">=</span> config
        self<span class="token punctuation">.</span>layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>BertLayer<span class="token punctuation">(</span>config<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>config<span class="token punctuation">.</span>num_hidden_layers<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        hidden_states<span class="token punctuation">,</span>
        attention_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        head_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        encoder_hidden_states<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        encoder_attention_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        past_key_values<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        use_cache<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        output_attentions<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
        output_hidden_states<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
        return_dict<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        all_hidden_states <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> output_hidden_states <span class="token keyword">else</span> <span class="token boolean">None</span>
        all_self_attentions <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> output_attentions <span class="token keyword">else</span> <span class="token boolean">None</span>
        all_cross_attentions <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> output_attentions <span class="token keyword">and</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>add_cross_attention <span class="token keyword">else</span> <span class="token boolean">None</span>

        next_decoder_cache <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> use_cache <span class="token keyword">else</span> <span class="token boolean">None</span>
        <span class="token keyword">for</span> i<span class="token punctuation">,</span> layer_module <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>layer<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> output_hidden_states<span class="token punctuation">:</span>
                all_hidden_states <span class="token operator">=</span> all_hidden_states <span class="token operator">+</span> <span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span><span class="token punctuation">)</span>

            layer_head_mask <span class="token operator">=</span> head_mask<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">if</span> head_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> <span class="token boolean">None</span>
            past_key_value <span class="token operator">=</span> past_key_values<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">if</span> past_key_values <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> <span class="token boolean">None</span>

            <span class="token keyword">if</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">,</span> <span class="token string">"gradient_checkpointing"</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">)</span> <span class="token keyword">and</span> self<span class="token punctuation">.</span>training<span class="token punctuation">:</span>

                <span class="token keyword">if</span> use_cache<span class="token punctuation">:</span>
                    logger<span class="token punctuation">.</span>warning<span class="token punctuation">(</span>
                        <span class="token string">"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting "</span>
                        <span class="token string">"`use_cache=False`..."</span>
                    <span class="token punctuation">)</span>
                    use_cache <span class="token operator">=</span> <span class="token boolean">False</span>

                <span class="token keyword">def</span> <span class="token function">create_custom_forward</span><span class="token punctuation">(</span>module<span class="token punctuation">)</span><span class="token punctuation">:</span>
                    <span class="token keyword">def</span> <span class="token function">custom_forward</span><span class="token punctuation">(</span><span class="token operator">*</span>inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
                        <span class="token keyword">return</span> module<span class="token punctuation">(</span><span class="token operator">*</span>inputs<span class="token punctuation">,</span> past_key_value<span class="token punctuation">,</span> output_attentions<span class="token punctuation">)</span>

                    <span class="token keyword">return</span> custom_forward

                layer_outputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>checkpoint<span class="token punctuation">.</span>checkpoint<span class="token punctuation">(</span>
                    create_custom_forward<span class="token punctuation">(</span>layer_module<span class="token punctuation">)</span><span class="token punctuation">,</span>
                    hidden_states<span class="token punctuation">,</span>
                    attention_mask<span class="token punctuation">,</span>
                    layer_head_mask<span class="token punctuation">,</span>
                    encoder_hidden_states<span class="token punctuation">,</span>
                    encoder_attention_mask<span class="token punctuation">,</span>
                <span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                layer_outputs <span class="token operator">=</span> layer_module<span class="token punctuation">(</span>
                    hidden_states<span class="token punctuation">,</span>
                    attention_mask<span class="token punctuation">,</span>
                    layer_head_mask<span class="token punctuation">,</span>
                    encoder_hidden_states<span class="token punctuation">,</span>
                    encoder_attention_mask<span class="token punctuation">,</span>
                    past_key_value<span class="token punctuation">,</span>
                    output_attentions<span class="token punctuation">,</span>
                <span class="token punctuation">)</span>

            hidden_states <span class="token operator">=</span> layer_outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
            <span class="token keyword">if</span> use_cache<span class="token punctuation">:</span>
                next_decoder_cache <span class="token operator">+=</span> <span class="token punctuation">(</span>layer_outputs<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> output_attentions<span class="token punctuation">:</span>
                all_self_attentions <span class="token operator">=</span> all_self_attentions <span class="token operator">+</span> <span class="token punctuation">(</span>layer_outputs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
                <span class="token keyword">if</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>add_cross_attention<span class="token punctuation">:</span>
                    all_cross_attentions <span class="token operator">=</span> all_cross_attentions <span class="token operator">+</span> <span class="token punctuation">(</span>layer_outputs<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> output_hidden_states<span class="token punctuation">:</span>
            all_hidden_states <span class="token operator">=</span> all_hidden_states <span class="token operator">+</span> <span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> <span class="token keyword">not</span> return_dict<span class="token punctuation">:</span>
            <span class="token keyword">return</span> <span class="token builtin">tuple</span><span class="token punctuation">(</span>
                v
                <span class="token keyword">for</span> v <span class="token keyword">in</span> <span class="token punctuation">[</span>
                    hidden_states<span class="token punctuation">,</span>
                    next_decoder_cache<span class="token punctuation">,</span>
                    all_hidden_states<span class="token punctuation">,</span>
                    all_self_attentions<span class="token punctuation">,</span>
                    all_cross_attentions<span class="token punctuation">,</span>
                <span class="token punctuation">]</span>
                <span class="token keyword">if</span> v <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span>
            <span class="token punctuation">)</span>
        <span class="token keyword">return</span> BaseModelOutputWithPastAndCrossAttentions<span class="token punctuation">(</span>
            last_hidden_state<span class="token operator">=</span>hidden_states<span class="token punctuation">,</span>
            past_key_values<span class="token operator">=</span>next_decoder_cache<span class="token punctuation">,</span>
            hidden_states<span class="token operator">=</span>all_hidden_states<span class="token punctuation">,</span>
            attentions<span class="token operator">=</span>all_self_attentions<span class="token punctuation">,</span>
            cross_attentions<span class="token operator">=</span>all_cross_attentions<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
</code></pre> 
<h5><a id="2211_BertSelfAttention_984"></a>2.2.1.1 BertSelfAttention</h5> 
<p>这一块可以说是模型的核心区域</p> 
<h5><a id="2212_BertSelfOutput_986"></a>2.2.1.2 BertSelfOutput</h5> 
<p>最直接的目的就是降低网络层数过深带来的训练难度</p> 
<h4><a id="223_BertOutput_988"></a>2.2.3 BertOutput</h4> 
<p>这里是一个全连接+dropout+LayerNorm</p> 
<h3><a id="23_BertPooler_990"></a>2.3 BertPooler</h3> 
<p>知识简单地取出了句子的第一个token</p> 
<h2><a id="_992"></a>总结</h2> 
<p>对Bert模型的实现进行了分析学习<br> 感觉自己太菜了 这里看啥啥不会哈哈哈哈,真的是太深了这对于我来说</p> 
<h2><a id="_996"></a>参考</h2> 
<p><a href="https://github.com/datawhalechina/Learn-NLP-with-Transformers">Datawhale基于transformers的自然语言处理(NLP入门)</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/841fa5de93982dcac54aff91336080cc/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">mybatis-plus详解</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/5c7088883032164deb5c5fe19c4100cd/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">云迁移常见工具介绍</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>