<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>CBAM: Convolutional Block Attention Module—— channel attention &#43; spatial attention - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="CBAM: Convolutional Block Attention Module—— channel attention &#43; spatial attention" />
<meta property="og:description" content="影响卷积神经网络的几大因素： Depth: VGG, ResNetWidth: GoogLeNetCardinality: Xception, ResNeXtAttention：channel attention, spatial attention Attention在人类感知系统中扮演了重要角色，人类视觉系统的一大重要性质是人类并不是试图一次处理完整个场景，与此相反，为了更好地捕捉视觉结构，人类利用一系列的局部瞥见，选择性地聚焦于突出的部分。 CBAM其实就是顺序进行channel attention和spatial attention：
Channel attention: focus on what feature map is meaningful; 全连接层是使用卷积核=1的卷积实现的Spatial attention：focus on where is an informative part；沿channel 轴的求均值操作 Attention和fature map是元素级别的相乘，相乘时会自动进行broadcast(copy)操作，即channel attention沿着spatial维度广播，spatial attention沿着channel维度广播
class ChannelAttention(nn.Module): def __init__(self, in_planes, ratio=16): super(ChannelAttention, self).__init__() self.avg_pool = nn.AdaptiveAvgPool2d(1) self.max_pool = nn.AdaptiveMaxPool2d(1) self.fc1 = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False) self.relu1 = nn.ReLU() self.fc2 = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False) self." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/c1207ed375b49a9c593ddfbafe0cebf7/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-07-31T17:24:01+08:00" />
<meta property="article:modified_time" content="2019-07-31T17:24:01+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">CBAM: Convolutional Block Attention Module—— channel attention &#43; spatial attention</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <ol><li>影响卷积神经网络的几大因素：</li></ol> 
<ul><li>Depth: VGG, ResNet</li><li>Width: GoogLeNet</li><li>Cardinality: Xception, ResNeXt</li><li>Attention：channel attention, spatial attention</li></ul> 
<ol><li>Attention在人类感知系统中扮演了重要角色，人类视觉系统的一大重要性质是人类并不是试图一次处理完整个场景，与此相反，为了更好地捕捉视觉结构，人类利用一系列的局部瞥见，选择性地聚焦于突出的部分。</li></ol> 
<p style="margin-left:0cm;"><img alt="" class="has" height="422" src="https://images2.imgbox.com/c1/0e/Gs9wf8yT_o.png" width="937"></p> 
<p style="margin-left:0cm;"><img alt="" class="has" height="653" src="https://images2.imgbox.com/31/ed/720ikMTx_o.png" width="959"></p> 
<p style="margin-left:0cm;"><img alt="" class="has" height="341" src="https://images2.imgbox.com/a9/ec/wwmJvt1U_o.png" width="974"></p> 
<p>CBAM其实就是顺序进行channel attention和spatial attention：</p> 
<ul><li>Channel attention: focus on <span style="color:#FF0000;">what</span> feature map is meaningful; 全连接层是使用卷积核=1的卷积实现的</li><li>Spatial attention：focus on <span style="color:#FF0000;">where </span>is an informative part；沿channel 轴的求均值操作</li></ul> 
<p style="margin-left:0cm;"><img alt="" class="has" height="94" src="https://images2.imgbox.com/76/3c/ZZ0nhkIF_o.png" width="242"></p> 
<p style="margin-left:0cm;">Attention和fature map是元素级别的相乘，相乘时会自动进行broadcast(copy)操作，即channel attention沿着spatial维度广播，spatial attention沿着channel维度广播</p> 
<pre class="has"><code class="language-python">class ChannelAttention(nn.Module):
    def __init__(self, in_planes, ratio=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        self.fc1   = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False)
        self.relu1 = nn.ReLU()
        self.fc2   = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)

        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))
        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))
        out = avg_out + max_out
        return self.sigmoid(out)

class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()

        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'
        padding = 3 if kernel_size == 7 else 1

        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x = torch.cat([avg_out, max_out], dim=1)
        x = self.conv1(x)
        return self.sigmoid(x)
</code></pre> 
<p> </p> 
<p>参考代码：<a href="https://github.com/luuuyi/CBAM.PyTorch/blob/master/model/resnet_cbam.py">https://github.com/luuuyi/CBAM.PyTorch/blob/master/model/resnet_cbam.py</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/973980f373ac73d2432bf4ad6438608a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">使用python操作mysql数据库</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/dca56523524f32e7215ed494caa13448/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">浅谈图像分割算法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>