<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models" />
<meta property="og:description" content="Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models 基本信息 博客贡献人 燕青
作者 Yue Zhang, Yafu Li, Leyang Cui, et al.
标签 LLM, Hallucinations
摘要 虽然大型语言模型在一系列下游任务中表现出卓越的能力，但一个重要的问题是它们表现出幻觉的倾向：LLMs偶尔会产生偏离用户输入、与先前生成的上下文相矛盾或与既定的世界知识相抵触的内容。这一现象对LLMs在真实场景中的可靠性提出了实质性的挑战。本文回顾了近年来在幻觉的检测、解释和缓解方面所做的努力，并强调了LLMs所带来的独特挑战。本文给出了LLM幻觉现象的分类和评估基准，分析了现有的旨在缓解LLM幻觉的方法，并讨论了潜在的发展方向。
问题定义 幻觉定义 LLMs产生无意义或谬误输出内容的现象通常被称为幻觉。这大大降低了LLMs在真实世界场景中的可靠性。例如，LLMs可能会制造错误的医疗诊断或治疗计划，从而导致有形的实际生活风险。
输入冲突幻觉：LLMs产生偏离用户提供的源输入的内容，其与用户任务指令之间的矛盾反映了LLMs对用户意图的误解。上下文冲突幻觉：LLMs生成的内容与先前生成的信息本身冲突。LLMs在产生长距离或多跳响应时可能会表现出自相矛盾，其产生于LLMs失去对语境的追踪或无法在整个会话过程中保持一致，这可能是由于LLMs在保持长时记忆或识别相关语境方面存在局限性。事实冲突幻觉：LLM产生偏离既定世界知识的内容。事实冲突幻觉的来源可以是多方面的，可以在LLM生命周期的不同阶段引入。 图1. 大模型响应中的三种幻觉 尽管其他两种类型的幻觉也很重要，但最近在LLMs中的幻觉研究的焦点主要集中在事实冲突幻觉上。可能的原因包括但不限于：
( 1 )在传统的NLG领域中，输入和语境冲突的幻觉已经被广泛研究。然而，由于缺乏权威的知识来源作为参考，事实冲突幻觉在LLMs中提出了更为复杂的挑战；
( 2 )事实冲突幻觉往往会对LLMs的实际应用产生更多的副作用，因此在最近的研究中得到了更多的重视。鉴于这种研究现状，本文的以下部分将主要集中于事实冲突的幻觉。
独特挑战 虽然传统自然语言生成(NLG)环境中的幻觉已被广泛研究，但在LLMs领域中理解和解决幻觉问题具有独特的挑战性：
海量训练数据：与为特定任务精心整理的数据相反，LLM的预训练使用从网络上获得的数万亿token，难以消除捏造、过时或有偏见的信息。谬误的不易察觉性：LLMs产生的虚假信息幻觉往往具有很强的拟真性，以至于连人类都难以察觉。需要考虑利用更多的知识来源进行验证。大语言模型的多样性：相比于通常应用于单一任务的传统NLG模型，大模型在多任务、多语言和多领域环境中表现更好。在评估方面，LLMs更常用于自由形式的文本生成，而这种缺乏确定性参考的任务使幻觉的自动检测变得复杂。因此，建立一个全面、可靠、自动化的评价基准至关重要。在缓解方面，所提出的方法应该是稳健有效的，在应用于各种场景时保持了良好的性能。 本文还列出一些除幻觉外的常见问题，以帮助读者区分它们和幻觉。
歧义性：答案可能并不一定是不正确的，但是对于用户问题却没有给出一个有用的答案。不完整性：不完整性问题发生在生成的响应不完整或碎片化的情况下，如下图例子中只给出了换轮胎4个步骤中的2步。偏见：LLMs中的偏见是指生成文本中不公平或偏见态度的表现，这些偏见可能来源于训练数据。如下图例子中将教师描述为女性是一种偏见。信息不足：指LLMs逃避回答某些问题的倾向，即使其有能力这样做。例如，由于奖励模型的不完善，RLHF可能会导致LLMs的过度优化，从而可能导致模型输出信息不足。 图2. LLMs除了幻觉外可能暴露的其它问题 方法 总体框架 本文首先给出对LLMs中幻觉的定义( § 2 )。接下来介绍相关的基准和度量指标( § 3 )。随后，本文讨论了LLM幻觉的潜在来源( § 4 )，并对最近解决该问题的工作进行了深入的回顾( § 5 )。
图3. 本文总体结构 LLM幻觉评估基准 基准数据集 针对LLMs中幻觉的评估，已经提出了多种基准。本文在图6中呈现了具有代表性的基准，并根据它们的评价方式、任务形式和构造方法进行讨论。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/b621c58a567213c8f3346c6eadf73307/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-16T19:47:05+08:00" />
<meta property="article:modified_time" content="2023-10-16T19:47:05+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="Sirens_Song_in_the_AI_Ocean_A_Survey_on_Hallucination_in_Large_Language_Models_0"></a>Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models</h2> 
<h3><a id="_2"></a>基本信息</h3> 
<h4><a id="_4"></a>博客贡献人</h4> 
<p>燕青</p> 
<h4><a id="_8"></a>作者</h4> 
<p>Yue Zhang, Yafu Li, Leyang Cui, et al.</p> 
<h4><a id="_12"></a>标签</h4> 
<p>LLM, Hallucinations</p> 
<h3><a id="_16"></a>摘要</h3> 
<p>虽然大型语言模型在一系列下游任务中表现出卓越的能力，但一个重要的问题是它们表现出幻觉的倾向：LLMs偶尔会产生偏离用户输入、与先前生成的上下文相矛盾或与既定的世界知识相抵触的内容。这一现象对LLMs在真实场景中的可靠性提出了实质性的挑战。本文回顾了近年来在幻觉的检测、解释和缓解方面所做的努力，并强调了LLMs所带来的独特挑战。本文给出了LLM幻觉现象的分类和评估基准，分析了现有的旨在缓解LLM幻觉的方法，并讨论了潜在的发展方向。</p> 
<h3><a id="_20"></a>问题定义</h3> 
<h4><a id="_22"></a>幻觉定义</h4> 
<p>LLMs产生无意义或谬误输出内容的现象通常被称为幻觉。这大大降低了LLMs在真实世界场景中的可靠性。例如，LLMs可能会制造错误的医疗诊断或治疗计划，从而导致有形的实际生活风险。</p> 
<ul><li><strong>输入冲突幻觉</strong>：LLMs产生偏离用户提供的源输入的内容，其与用户任务指令之间的矛盾反映了LLMs对用户意图的误解。</li><li><strong>上下文冲突幻觉</strong>：LLMs生成的内容与先前生成的信息本身冲突。LLMs在产生<u>长距离</u>或<u>多跳响应</u>时可能会表现出<u>自相矛盾</u>，其产生于LLMs失去对语境的追踪或无法在整个会话过程中保持一致，这可能是由于LLMs在保持长时记忆或识别相关语境方面存在局限性。</li><li><strong>事实冲突幻觉</strong>：LLM产生偏离既定世界知识的内容。事实冲突幻觉的来源可以是多方面的，可以在LLM生命周期的不同阶段引入。</li></ul> 
<div> 
 <center> 
  <img src="https://images2.imgbox.com/40/f5/OO2JjGWl_o.png"> 
 </center> 
 <center>
   图1. 大模型响应中的三种幻觉 
 </center> 
 <br> 
</div> 
<p>尽管其他两种类型的幻觉也很重要，但最近在LLMs中的幻觉研究的焦点主要集中在<strong>事实冲突幻觉</strong>上。可能的原因包括但不限于：</p> 
<p>( 1 )在传统的NLG领域中，输入和语境冲突的幻觉已经被广泛研究。然而，由于缺乏权威的知识来源作为参考，事实冲突幻觉在LLMs中提出了更为复杂的挑战；</p> 
<p>( 2 )事实冲突幻觉往往会对LLMs的实际应用产生更多的副作用，因此在最近的研究中得到了更多的重视。鉴于这种研究现状，本文的以下部分将主要集中于事实冲突的幻觉。</p> 
<h4><a id="_45"></a>独特挑战</h4> 
<p>虽然传统自然语言生成(NLG)环境中的幻觉已被广泛研究，但在LLMs领域中理解和解决幻觉问题具有独特的挑战性：</p> 
<ul><li><strong>海量训练数据</strong>：与为特定任务精心整理的数据相反，LLM的预训练使用从网络上获得的数万亿token，难以消除<u>捏造、过时或有偏见</u>的信息。</li><li><strong>谬误的不易察觉性</strong>：LLMs产生的虚假信息幻觉往往具有很强的拟真性，以至于连人类都难以察觉。需要考虑利用更多的知识来源进行验证。</li><li><strong>大语言模型的多样性</strong>：相比于通常应用于单一任务的传统NLG模型，大模型在多任务、多语言和多领域环境中表现更好。在<strong>评估</strong>方面，LLMs更常用于自由形式的文本生成，而这种<u>缺乏确定性参考</u>的任务使幻觉的自动检测变得复杂。因此，建立一个全面、可靠、自动化的评价基准至关重要。在<strong>缓解</strong>方面，所提出的方法应该是稳健有效的，在应用于<u>各种场景</u>时保持了良好的性能。</li></ul> 
<p>本文还列出一些除幻觉外的常见问题，以帮助读者区分它们和幻觉。</p> 
<ul><li><strong>歧义性</strong>：答案可能并不一定是不正确的，但是对于用户问题却没有给出一个有用的答案。</li><li><strong>不完整性</strong>：不完整性问题发生在生成的响应不完整或碎片化的情况下，如下图例子中只给出了换轮胎4个步骤中的2步。</li><li><strong>偏见</strong>：LLMs中的偏见是指生成文本中不公平或偏见态度的表现，这些偏见可能来源于训练数据。如下图例子中将教师描述为女性是一种偏见。</li><li><strong>信息不足</strong>：指LLMs逃避回答某些问题的倾向，即使其有能力这样做。例如，由于奖励模型的不完善，RLHF可能会导致LLMs的过度优化，从而可能导致模型输出信息不足。</li></ul> 
<div> 
 <center> 
  <img src="https://images2.imgbox.com/36/b8/GoOD6j8x_o.png"> 
 </center> 
 <center>
   图2. LLMs除了幻觉外可能暴露的其它问题 
 </center> 
 <br> 
</div> 
<h3><a id="_69"></a>方法</h3> 
<h4><a id="_71"></a>总体框架</h4> 
<p>本文首先给出对LLMs中幻觉的定义( § 2 )。接下来介绍相关的基准和度量指标( § 3 )。随后，本文讨论了LLM幻觉的潜在来源( § 4 )，并对最近解决该问题的工作进行了深入的回顾( § 5 )。</p> 
<div> 
 <center> 
  <img src="https://images2.imgbox.com/e1/cc/xBjInx9N_o.png"> 
 </center> 
 <center>
   图3. 本文总体结构 
 </center> 
 <br> 
</div> 
<h4><a id="LLM_83"></a>LLM幻觉评估基准</h4> 
<h5><a id="_85"></a>基准数据集</h5> 
<p>针对LLMs中幻觉的评估，已经提出了多种基准。本文在图6中呈现了具有代表性的基准，并根据它们的评价方式、任务形式和构造方法进行讨论。</p> 
<p><strong>a. 评价方式</strong>：现有基准主要基于LLM的两种不同能力来评估幻觉：</p> 
<p>（1）<strong>生成</strong>事实陈述的能力</p> 
<p>（2）<strong>判别</strong>事实与谬误的能力。</p> 
<div> 
 <center> 
  <img src="https://images2.imgbox.com/14/f0/AFCXTHEt_o.png"> 
 </center> 
 <center>
   图4. 幻觉的两种评价方式 
 </center> 
 <br> 
</div> 
<p><strong>b. 任务形式</strong>：</p> 
<p>（1）探讨<strong>问答</strong>情境中的幻觉问题，评估了LLMs为知识密集型问题提供真实答案的能力</p> 
<p>（2）使用<strong>任务指令</strong>，提示LLMs产生响应，然后对这些响应的真实性进行评估。</p> 
<p>（3）使LLMs在给定前缀的情况下<strong>补全文本</strong>，并在事实陈述的生成过程中诊断潜在的幻觉。</p> 
<div> 
 <center> 
  <img src="https://images2.imgbox.com/d8/c7/XT6Pc7t4_o.png"> 
 </center> 
 <center>
   图5. 幻觉评价任务的三种形式 
 </center> 
 <br> 
</div> 
<p><strong>c. 构造方法</strong>：图6展示了针对LLMs幻觉具有代表性的评估基准，大多数上述评估基准的数据集创建或质量保证都涉及到<u>人类注释员</u>的参与。</p> 
<div> 
 <center> 
  <img src="https://images2.imgbox.com/1e/08/CitJ2yiD_o.png"> 
 </center> 
 <center>
   图6. 针对LLMs幻觉具有代表性的评估基准 
 </center> 
 <br> 
</div> 
<h5><a id="_131"></a>评估指标</h5> 
<p>语言生成的自由性和开放性使得对LLMs产生的幻觉进行评估变得困难。最常用和最可靠的评估幻觉的方法依赖于<strong>人类</strong>专家遵循特定的原则进行评估。值得注意的是，尽管现有基准使用人为评估来确保可靠性，但它们也寻求支持<strong>自动化</strong>的方法，以促进高效和一致的评估。</p> 
<p><strong>a. 人工评估</strong></p> 
<p>为了确保精确和可靠的评估，现有的基准侧重于设计专门的人工评估原则，其中包括用于评估每个模型生成文本的人工标注。虽然人类评价提供了可靠性和可解释性，但由于注释者的<strong>主观性</strong>，它可能是<strong>不一致</strong>的。由于每次需要对新模型进行评估时都需要进行劳动密集型的标注过程，因此它也是非常<strong>昂贵</strong>的。</p> 
<p><strong>b. 基于模型的自动评估</strong></p> 
<p>为了实现自动化评估，可以设计基于模型的方法作为人类评估的代理。（1）例如Align Score建立了一个统一的函数来评估两个文本之间的事实一致性。该<strong>对齐函数</strong>在一个包含自然语言推理(NLI)、问答(QA)和释义等7个任务的大型数据集上进行训练。（2）Mündler等人设计了专门的提示语，询问<strong>评价者LLM</strong> (例如, ChatGPT )在同一语境下<strong>主观LLM</strong>是否自相矛盾，并报告精确率、召回率和F1分数等分类指标。</p> 
<p><strong>c. 基于规则的自动评估</strong></p> 
<p>对于评估方式为判别的基准，常见的基于规则的分类指标，如准确率可以直接用于评估LLMs区分事实陈述和非事实陈述的能力。相比之下，另一类研究则专注于设计专门用于评估幻觉的启发式方法：真实性Prompt结合了基于命名实体的度量和基于文本导出的度量来捕获真实性的不同方面。</p> 
<div> 
 <br> 
</div> 
<h4><a id="LLM_151"></a>LLM幻觉来源</h4> 
<h5><a id="LLMs_153"></a>LLMs缺乏相关知识或内化虚假知识</h5> 
<p>在预训练阶段，LLMs从大量的训练数据中积累了大量的知识，存储在模型参数中。当被要求回答问题或完成任务时，如果LLMs缺乏相关知识或从训练语料中内化了错误知识，他们通常会表现出幻觉。LLMs有时会将虚假关联，如位置相近或高度共现的关联，误解为事实性知识。最近的研究发现，<u>LLM幻觉与训练数据分布之间存在很强的相关性</u>。此外，与知识记忆相关的两种额外能力使LLMs能够提供真实的答案：<strong>知识回忆</strong>和<strong>知识推理</strong>。其中任何一种能力的缺陷都可能导致幻觉。</p> 
<h5><a id="LLMs_157"></a>LLMs有时高估自身能力</h5> 
<p>为了了解语言模型是否能够评估其反应的准确性并识别其<strong>知识边界</strong>，近期的研究表明LLMs能够评估自己回答的正确性(自我评估)。然而，对于非常大的LLMs，正确答案和错误答案的分布熵可能是相似的，即LLMs在生成错误答案时与生成正确答案时一样有信心。推理<u>准确性和置信度</u>之间存在相关关系，但这种置信度往往超过了LLMs的实际能力，即过度自信。一般而言，LLMs对事实性知识边界的理解可能是不精确的，他们经常表现出过度自信。这种过度自信会误导LLM伪造无确定性依据的答案。</p> 
<h5><a id="LLMs_161"></a>对齐过程可能会误导LLMs产生幻觉</h5> 
<p>LLMs通常在预训练之后会经历一个对齐过程，在这个过程中，LLMs接受进一步的规范化训练，以使他们的反应符合人类的偏好。然而，当使用LLMs<u>没有从预训练阶段获得先验知识</u>的指令进行训练时，这实际上是一个使LLMs产生幻觉的失调过程。另一个潜在的问题是，LLMs可能会产生<u>有利于用户视角</u>的响应，而不是提供正确或真实的答案，这可能导致幻觉。</p> 
<h5><a id="LLMs_165"></a>LLMs采用的生成策略存在潜在风险</h5> 
<p>当今最先进的LLMs依次产生响应，一次输出一个token。近期研究发现，LLMs有时会重复他们早期的错误，即使他们认识到他们是错误的。换句话说，LLMs可能更喜欢自我一致性的滚雪球幻觉，而不是从错误中恢复。这种现象被称为<strong>幻觉滚雪球</strong>。局部优化(token预测)并不一定能保证全局优化(序列预测)，早期的局部预测可能会导致LLMs无法做出正确的全局预测。此外，基于采样的生成策略所引入的随机性，如<strong>top-p</strong>和<strong>top-k</strong>，也可能是产生幻觉的潜在来源。</p> 
<div> 
 <br> 
</div> 
<h4><a id="LLM_173"></a>缓解LLM幻觉</h4> 
<p>在这一部分中，本文对最近关于减轻LLM幻觉的研究进行了广泛的回顾。为了使结构清晰，将现有的缓解方法按照其在LLM生命周期内的阶段进行分类。</p> 
<h5><a id="_177"></a>预训练阶段</h5> 
<p>LLMs的知识大多是在预训练阶段获得的，在预训练语料中存在的误导信息，可能会破坏LLMs的参数知识，这是导致幻觉的一个重要因素。因此，减轻幻觉的直观方法可能涉及手动或自动地对预训练语料进行过滤，以尽可能减少不可验证或不可靠的数据。为了缓解幻觉，当前的LLMs倾向于从可信的文本源收集预训练数据。例如LLaMA 2在构建预训练语料库时，策略性地从高度可靠来源(如维基百科)上采样数据。</p> 
<blockquote> 
 <p>一个潜在的研究方向可能是设计更有效的选择或过滤策略。</p> 
</blockquote> 
<h5><a id="SFT_183"></a>监督微调(SFT)阶段</h5> 
<p>LLMs普遍经历监督微调( SFT )的过程，以激发从预训练中获得的知识，并学习如何与用户进行交互。SFT通常包括首先标注或收集大规模任务的指令跟随数据，然后使用最大似然估计( MLE )在该数据上微调经过预训练的基础LLMs。</p> 
<p>鉴于SFT数据的体积相对较小，手动和自动过滤都是可行的选择。</p> 
<p>尽管LLMs的参数中编码了大量的先验知识，但仍然存在超过其先验知识容量的知识。通过克隆人类在SFT过程中的行为，LLMs学会以积极的语气回应所有问题，而不用评估这些问题是否超出其知识边界(见图7)。</p> 
<div> 
 <center> 
  <img src="https://images2.imgbox.com/b5/31/pKGPZ1mb_o.png"> 
 </center> 
 <center>
   图7. SFT数据通常包含超过LLMs参数知识的样本，可能导致幻觉 
 </center> 
 <br> 
</div> 
<p>因此，在推理过程中，如果提示被试回答与未学知识有关的问题，他们很可能会自信地产生幻觉。缓解这一问题的方法之一可以是<strong>基于诚信的SFT</strong>，即在SFT数据中引入一些<u>诚信样本</u>。诚信样本是指承认自己不称职的回答，如"对不起，我不知道"。</p> 
<blockquote> 
 <p>基于诚信的SFT存在的问题是，（1）分布外(OOD)泛化能力不理想（2）被标注的诚实样本只是反映了标注者的无能和不确定性，而非LLM的无能和不确定性，因为<u>标注者并不知道LLM的知识边界</u>。</p> 
</blockquote> 
<h5><a id="RLHF_205"></a>基于人类反馈的强化学习(RLHF)阶段</h5> 
<p>目前，许多研究人员试图通过基于人类反馈的强化学习( RLHF )来进一步改进有监督的微调LLMs。该过程包括两个步骤：</p> 
<ul><li> <p>训练一个奖励模型( RW )作为人类偏好的代理，为每个LLM响应分配一个合适的奖励值。</p> </li><li> <p>使用RL算法，在奖励模型的反馈下对SFT模型进行优化。</p> </li></ul> 
<p>利用人类反馈不仅可以缩小机器生成内容与人类偏好之间的差距，而且可以帮助LLMs与期望的标准或目标保持一致。目前常用的一个标准是" <strong>3H</strong> "，即有用(<strong>helpful</strong>)、诚实(<strong>honest</strong>)、无害(<strong>harmless</strong>)。其中诚实指的就是LLM响应中幻觉的最小化。</p> 
<p>鉴于在SFT阶段不是缓解幻觉的最优解，<a href="https://www.youtube.com/watch?v=hhiLw5Q_UFg" rel="nofollow">Schulman( 2023 )</a>提出在RLHF期间解决这一问题。他们设计了一个特殊的奖励函数，仅用于减轻幻觉，如表8所示。这里的"Unhedged/Hedged Correct/Wrong"是指LLM以积极或犹豫的语气提供正确或错误的答案。"“无信息”“表示”“我不知道”"这样的安全回答。其核心思想是通过向专门设计的奖励学习，鼓励LLM挑战假设、表达不确定性和承认自己的无能。这种方法，我们称之为<strong>基于诚信的RL</strong>，它比基于诚信的SFT有许多优点。这样做的主要好处是可以让LLM自由探索其知识边界，从而增强其对OOD案例的<u>泛化能力</u>。此外，它减少了对大量人工标注的需求，并消除了对标注者猜测LLMs<u>知识边界</u>的要求。</p> 
<div> 
 <center> 
  <img src="https://images2.imgbox.com/6e/48/HFwufCpT_o.png"> 
 </center> 
 <center>
   图8. 通过RL缓解LLM幻觉的奖励设计实例 
 </center> 
 <br> 
</div> 
<h5><a id="_225"></a>推理阶段</h5> 
<p>与前述在训练时缓解幻觉的方法相比，在推理时缓解幻觉可能更具有成本效益和可控性。因此，大多数现有的研究都集中在这个方向上，本文在以下几个部分进行详细介绍。</p> 
<h6><a id="a__229"></a>a. 设计解码策略</h6> 
<p>解码策略，如贪心解码和波束搜索解码，决定了如何从模型产生的概率分布中选择输出Token。</p> 
<p><a href="https://arxiv.org/abs/2206.04624" rel="nofollow">Lee等人(2022)</a>发现核采样（如top-p）采样通过引入随机性来增强多样性，这可能会无意中导致幻觉，因为LLMs倾向于伪造信息以产生多样化的响应。因此，他们引入了一种名为事实-核采样的解码算法，旨在利用top-p和贪心解码的优势，在多样性和真实性之间取得更有效的平衡。</p> 
<p><a href="https://arxiv.org/abs/2309.11495" rel="nofollow">Dhuliawala等人( 2023 )</a>开发了一个被称为"验证链" (Chain-of-Verification, COVE )的解码框架。该框架基于这样的观察：独立的验证问题通常比长形式答案中提出的问题产生更准确的事实。COVE框架首先规划验证问题，然后回答这些问题，最终产生一个强化的、修正的回应。在多种任务上的实验结果表明，COVE能够有效缓解幻觉。</p> 
<blockquote> 
 <p>缓解幻觉的解码策略通常以即插即用的方式设计，因此该方法易于部署，具有实际应用前景。然而，对于这种方法，大多数现有的工作都需要访问Token级别的输出概率，而相当数量的当前LLM只能通过<u>有限的API</u> (如ChatGPT)返回生成的内容。因此，应该鼓励未来该方向的研究在更严格的<strong>黑盒</strong>环境下进行探索。</p> 
</blockquote> 
<h6><a id="b__239"></a>b. 依靠外部知识</h6> 
<p>利用外部知识作为补充证据，协助LLM提供准确回答的方法通常包括两个步骤：</p> 
<ul><li>首先需要准确获取与用户指令相关的知识</li><li>利用获得的知识来指导答案的生成</li></ul> 
<p><strong>b.1 知识获取</strong></p> 
<p>（1）<strong>外部知识库</strong>：现有的大多数工作是从外部知识库中获取信息，例如大规模非结构化语料库、结构化数据库、特定网站如维基百科，甚至整个互联网。知识检索过程通常使用各种稀疏或密集检索器。搜索引擎如Google，也可以看作是一种特殊的知识检索器。</p> 
<p>（2）<strong>外部工具</strong>*：除了仅仅从知识库中检索信息外，还有许多其他工具可以提供有价值的证据，以增强LLM生成内容的真实性。例如，Fac Tool针对特定的下游任务，使用不同的工具来帮助检测LLM中的幻觉，如用于基于<u>知识问答的搜索引擎API</u>、用于<u>代码生成的代码执行器</u>、用于<u>科学文献综述的Google Scholar API</u>等。</p> 
<p><strong>b.2 知识利用</strong></p> 
<p>（1）<strong>生成时补充</strong>：利用检索到的知识或工具反馈的最直接的方法是在LLMs的<u>prompt中加入外部知识查询结果</u>。该方法既有效又易于实现，这样获取的知识也被称为上下文知识。现有研究表明，LLMs具有很强的上下文学习的能力，这使得他们能够从上下文知识中提取和利用有价值的信息来纠正他们先前生成的错误响应。</p> 
<p>（2）<strong>事后纠正</strong>：另一种常见的做法是在后处理阶段构建辅助纠正器来纠正幻觉。纠正器可以是另一个LLM，也可以是一个特定的小模型。这种纠正器首先与外部知识源进行互动以收集足够的证据，然后纠正幻觉。</p> 
<div> 
 <center> 
  <img src="https://images2.imgbox.com/cc/a8/tuGWA7R5_o.png"> 
 </center> 
 <center>
   图9. 两种不同的利用外部知识减少LLM幻觉的方法 
 </center> 
 <br> 
</div> 
<h6><a id="c__266"></a>c. 利用不确定性</h6> 
<p>不确定性是推理过程中检测和缓解幻觉的一个有价值的指标。通常，它是指模型输出的置信水平，可以帮助用户决定何时信任LLM。在能够准确刻画LLM响应不确定性的前提下，用户可以过滤或纠正LLM不确定性较高的声明，因为这些声明更倾向于是虚假和伪造的。</p> 
<p>一般来说，估计LLMs不确定性的方法可以分为三类，如图10所示。</p> 
<ul><li><strong>基于Logit的方法</strong>：该方法需要访问模型logit，通常通过计算Token级别的概率或熵来度量不确定性。</li><li><strong>基于口语描述的方法</strong>：它涉及直接请求LLMs来表达他们的不确定性，例如使用以下提示："请回答并提供你的信心分数(从0到100) "。这种方法是有效的，因为LLMs的语言和指令执行能力十分强大。值得注意的是，也可以使用<a href="https://arxiv.org/abs/2201.11903v5" rel="nofollow">思维链提示(2022)</a>来增强这种方法。</li><li><strong>基于一致性的方法</strong>：该方法基于这样的假设：当LLMs犹豫不决时，它们很可能为同一个问题提供逻辑上不一致的响应。</li></ul> 
<div> 
 <center> 
  <img src="https://images2.imgbox.com/fa/8e/me84FiFs_o.png"> 
 </center> 
 <center>
   图10. 3种典型的LLM不确定性估计方法 
 </center> 
 <center>
   在基于logit方法的例子中，使用红色/绿色背景来区分低/高生成概率的Token 
 </center> 
 <center>
   在基于一致性方法的例子中，响应是通过多次采样获得的 
 </center> 
 <br> 
</div> 
<h5><a id="_286"></a>其他方法</h5> 
<p>最近的一些研究试图从多代理的角度来解决LLMs中的幻觉问题，其中多个LLMs (又称代理)独立地提出并协作地辩论他们的反应，以达到单个共识，如图11所示。</p> 
<div> 
 <center> 
  <img src="https://images2.imgbox.com/f5/8e/KUVCsirw_o.png"> 
 </center> 
 <center>
   图11. 缓解LLM幻觉的多代理交互过程示例 
 </center> 
 <br> 
</div> 
<h3><a id="_298"></a>相关知识链接</h3> 
<h4><a id="_300"></a>论文原文</h4> 
<p><a href="https://arxiv.org/abs/2309.01219" rel="nofollow">Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models</a></p> 
<h4><a id="_304"></a>相关工作涉及的论文</h4> 
<p>基于诚信的RL：<a href="https://www.youtube.com/watch?v=hhiLw5Q_UFg" rel="nofollow"> Reinforcement Learning from Human Feedback: Progress and Challenges</a></p> 
<p>事实-核采样解码算法：<a href="https://arxiv.org/abs/2206.04624" rel="nofollow">Factuality Enhanced Language Models for Open-Ended Text Generation</a></p> 
<p>验证链：<a href="https://arxiv.org/abs/2309.11495" rel="nofollow">Chain-of-Verification Reduces Hallucination in Large Language Models</a></p> 
<p>思维链提示：<a href="https://arxiv.org/abs/2201.11903v5" rel="nofollow">Chain of Thought Prompting Elicits Reasoning in Large Language Models</a></p> 
<h3><a id="_314"></a>总结</h3> 
<h4><a id="_316"></a>幻觉定义</h4> 
<ul><li>输入冲突</li><li>上下文冲突</li><li>事实冲突</li></ul> 
<h4><a id="_322"></a>幻觉来源</h4> 
<ul><li> <p>LLMs缺乏相关知识或内化虚假知识</p> </li><li> <p>LLMs有时高估自身能力</p> </li><li> <p>对齐过程可能会误导LLMs产生幻觉</p> </li><li> <p>LLMs采用的生成策略存在潜在风险</p> </li></ul> 
<h4><a id="_332"></a>缓解幻觉方法</h4> 
<ul><li>预训练阶段 
  <ul><li>设计有效的选择或过滤策略来筛选预训练数据</li></ul> </li><li>监督微调(SFT)阶段 
  <ul><li>基于诚信的SFT</li></ul> </li><li>基于人类反馈的强化学习(RLHF)阶段 
  <ul><li><strong>3H</strong>：有用(<strong>helpful</strong>)、诚实(<strong>honest</strong>)、无害的(<strong>harmless</strong>)</li><li>基于诚信的RL</li></ul> </li><li>推理阶段 
  <ul><li>设计解码策略</li><li>依靠外部知识</li><li>利用不确定性</li></ul> </li></ul> 
<h4><a id="_346"></a>[启发]</h4> 
<ul><li>在获取外部知识部分，本文提到“外部工具”的使用，这是实际应用中容易忽略的点，合理使用外部工具也能提高模型的性能。</li><li>对于缓解LLM幻觉的方法，本文从LLM生命周期的4个不同阶段进行论述。这样的方法也可以推广到研究中，对LLM某种能力的改进可以先从这几个不同阶段入手。</li></ul> 
<h3><a id="BibTex_351"></a>BibTex</h3> 
<pre><code>@misc{zhang2023sirens,
      title={Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models}, 
      author={Yue Zhang and Yafu Li and Leyang Cui and Deng Cai and Lemao Liu and Tingchen Fu and Xinting Huang and Enbo Zhao and Yu Zhang and Yulong Chen and Longyue Wang and Anh Tuan Luu and Wei Bi and Freda Shi and Shuming Shi},
      year={2023},
      eprint={2309.01219},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/34a8b9eb663008e8beffc3b69ce7fa8a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">51单片机音乐喷泉频谱彩灯多功能音乐盒播放器</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7f32469c0abe484f8560d217eb8767d5/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">卷积神经网络之“浅层特征”与“深层特征”</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>