<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>我分析了ACL21论文列表，发现对比学习已经... - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="我分析了ACL21论文列表，发现对比学习已经..." />
<meta property="og:description" content="文 | 花小花Posy
小伙伴们，好久不见呀，小花又回来了！
最近关注对比学习，所以ACL21的论文列表出来后，小花就搜罗了一波，好奇NLPers们都用对比学习干了什么？都是怎么用的呀？效果怎样呀？
接收列表中有21篇论文题目包含了关键词“contrastive”。下图是题目的词云，其中最显著的是使用对比学习去学习表示或者帮助语义理解，还有机翻、摘要、关系抽取。
小花选择了10篇有意思的论文跟大家分享，方向包括句子表示[1-3]，自然语言理解[4-6], 生成式摘要[7]，意图检测[8]，多模态理解[9], 机器翻译[10]。当然还有其它的，比如用于事件抽取[12]、QA[13]等等，大家感兴趣可以自行补充！
为了大家快速get到跟对比学习最相关的部分，主要涉及论文中哪里用了对比，对比的对象是谁。
对比学习最重要的原料就是正例和负例的对比，以及在不同的应用场景下应该如何构造合理的正例和负例，如何设计对比损失函数。正负例的构造，可以分为利用显式的数据增强方式构造正负例[3-5,8-10]，或者通过在语义空间/模型层面采样/生成正负例[1-2,6]。从对比损失函数的使用上来讲，可以分为与原始的MLM损失加和一起进行joint训练[1,5,10],或者进行pipeline训练[7-8]。
下面有请今天的主角们登场，大家开心食用！
从BERT中提取出句子向量的easy模式，想必大家都超熟了，使用[CLS]的表示或者使用不同的pooling操作。但这就够了嘛？当然不够！
从预训练语言模型中提取出句子表示的最优方法是什么，仍是研究者们在不断探索的问题。除了之前推送过的强者SimCSE以外，下面前三篇的主题都是如何利用对比学习去学习到更好的句子/文本片段表示。
[1] 自我引导的对比学习（一个BERT不够，那就两个） Self-Guided Contrastive Learning for BERT Sentence Representations
https://arxiv.org/pdf/2106.07345.pdf
来自首尔大学，讨论的问题是如何在不引入外部资源或者显示的数据增强的情况下，利用BERT自身的信息去进行对比，从而获得更高质量的句子表示？
文中对比的是：BERT的中间层表示和最后的CLS的表示。模型包含两个BERT，一个BERT的参数是固定的，用于计算中间层的表示，其计算分两步：(1) 使用MAX-pooling获取每一层的句子向量表示 （2）使用均匀采样的方式从N层中采样一个表示；另一个BERT是要fine-tune的，用于计算句子CLS的表示。同一个句子的通过两个BERT获得两个表示，从而形成正例，负例则是另一个句子的中间层的表示或者最后的CLS的表示。
文中还对比了不同负例组合的方式，最后发现只保留CLS的表示 和隐藏层的表示 之间的对比，忽略CLS和CLS以及中间层和中间层之间的对比是最优的，即保留(1)(3)。
这篇论文没有选择直接从底层数据增强角度出发，是稍微偏模型方法的改进的，侧重挖掘模型内部的信息。主实验是在STS和SentEval任务上测试的，从结果来看的话，仍然是SimCSE要好很多，而且SimCSE操作起来是更简单的。不过本文也是提供了一个不一样的思路。
[2] 花式数据增强 ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer
https://arxiv.org/pdf/2105.11741.pdf
来自北邮的工作，也是研究如何在无监督的模式下，学习更好的句子表示。该工作主要对比了使用4种不同的数据增强方式进行对比对句子表示的作用。
模型是在STS任务上进行评估的。和SimCSE一样也用了NLI做监督，整体性能比SimCSE低1-2个点。
[3] 无监督文本表示 DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations
https://arxiv.org/pdf/2006.03659.pdf
DeCLUTR来自多伦多大学，是NLP领域使用对比学习中较早的一篇，去年6月份就放到arxiv上面了。
文章研究的问题同样是：如何利对比学习从大量无标注数据学习更好的通用句子表示？文中的对比体现在两个方面：
对比来自不同文档的文本片段(span)的语义。如果两个文本片段(span)来自同一个文档，那么他们的语义表示的距离应该相对较近，否则距离远；
对比来自同一文档的文本span。当两个文本片段都来自同一个文档，如果他们在文档中的位置距离比较近，他们的语义表示距离近，否则远。
在采样正例的时候有些讲究。具体来讲是先从一个文档中采样N(&gt;=1)个原始文本片段 (锚点span)，然后从每个锚点span周围采样，作为正例 span。采样规则是正例span可以与锚点span交叠、相邻、从属。负例是从一个batch中随机采样得到的。对比学习的损失函数是InfoNCE。模型整体的损失函数是InfoNCE和MLM的加和。
实验是在SenEval benchmark(28个数据集）上进行测试的，包含有/半监督任务和无监督任务。有/半监督任务的baseline有InferSent，Universal Sentence Encoder和Sentence Transformers；无监督任务的baseline有QuickThoughts。最显著的实验结果是DeCLUTR在大部分的数据集上取得了SOTA，并且在无监督任务上取得了和有监督任务相当的结果。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/c35c41ba29cde30407c0730ecfbe9e6f/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-08-05T11:05:00+08:00" />
<meta property="article:modified_time" content="2021-08-05T11:05:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">我分析了ACL21论文列表，发现对比学习已经...</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p><img src="https://images2.imgbox.com/b4/c7/bjhS9pt8_o.png"></p> 
 <blockquote> 
  <blockquote> 
   <blockquote> 
    <blockquote> 
     <p style="text-align: center">文 | 花小花Posy<br></p> 
    </blockquote> 
   </blockquote> 
  </blockquote> 
 </blockquote> 
 <p>小伙伴们，好久不见呀，小花又回来了！</p> 
 <p>最近关注对比学习，所以ACL21的论文列表出来后，小花就搜罗了一波，好奇NLPers们都用对比学习干了什么？都是怎么用的呀？效果怎样呀？</p> 
 <p>接收列表中有21篇论文题目包含了关键词“contrastive”。下图是题目的词云，其中最显著的是使用对比学习去学习表示或者帮助语义理解，还有机翻、摘要、关系抽取。</p> 
 <img src="https://images2.imgbox.com/7a/15/9vmhfY0B_o.png"> 
 <p>小花选择了10篇有意思的论文跟大家分享，方向包括句子表示[1-3]，自然语言理解[4-6], 生成式摘要[7]，意图检测[8]，多模态理解[9], 机器翻译[10]。当然还有其它的，比如用于事件抽取[12]、QA[13]等等，大家感兴趣可以自行补充！</p> 
 <p>为了大家快速get到跟对比学习最相关的部分，主要涉及<strong>论文中哪里用了对比，对比的对象是谁</strong>。</p> 
 <p>对比学习最重要的原料就是正例和负例的对比，以及在不同的应用场景下应该如何构造合理的正例和负例，如何设计对比损失函数。正负例的构造，可以分为利用显式的数据增强方式构造正负例[3-5,8-10]，或者通过在语义空间/模型层面采样/生成正负例[1-2,6]。从对比损失函数的使用上来讲，可以分为与原始的MLM损失加和一起进行joint训练[1,5,10],或者进行pipeline训练[7-8]。</p> 
 <p>下面有请今天的主角们登场，大家开心食用！</p> 
 <img src="https://images2.imgbox.com/59/5f/pmlUWjmd_o.png"> 
 <p>从BERT中提取出句子向量的easy模式，想必大家都超熟了，使用[CLS]的表示或者使用不同的pooling操作。但这就够了嘛？当然不够！</p> 
 <p>从预训练语言模型中提取出句子表示的最优方法是什么，仍是研究者们在不断探索的问题。除了之前推送过的强者SimCSE以外，下面前三篇的主题都是如何利用对比学习去学习到更好的句子/文本片段表示。</p> 
 <h3>[1] 自我引导的对比学习（一个BERT不够，那就两个）</h3> 
 <p><em><strong>Self-Guided Contrastive Learning for BERT Sentence Representations</strong></em><br><em>https://arxiv.org/pdf/2106.07345.pdf</em></p> 
 <p>来自首尔大学，讨论的问题是<strong>如何在不引入外部资源或者显示的数据增强的情况下，利用BERT自身的信息去进行对比，从而获得更高质量的句子表示？</strong></p> 
 <p>文中对比的是：<strong>BERT的中间层表示和最后的CLS的表示</strong>。模型包含两个BERT，一个BERT的参数是固定的，用于计算中间层的表示，其计算分两步：(1) 使用MAX-pooling获取每一层的句子向量表示 （2）使用均匀采样的方式从N层中采样一个表示；另一个BERT是要fine-tune的，用于计算句子CLS的表示。同一个句子的通过两个BERT获得两个表示，从而形成正例，负例则是另一个句子的中间层的表示或者最后的CLS的表示。</p> 
 <img src="https://images2.imgbox.com/bb/fc/3QLA7nrD_o.png"> 
 <p>文中还对比了不同负例组合的方式，最后发现只保留CLS的表示 
    <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 -442 727 599.8" style="vertical-align: -0.357ex;width: 1.645ex;height: 1.357ex;"> 
     <g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"> 
      <g> 
       <g> 
        <g> 
         <path d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path> 
        </g> 
        <g transform="translate(433, -150) scale(0.707)"> 
         <path d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path> 
        </g> 
       </g> 
      </g> 
     </g> 
    </svg>和隐藏层的表示 
    <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 -694 870 851.8" style="vertical-align: -0.357ex;width: 1.968ex;height: 1.927ex;"> 
     <g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"> 
      <g> 
       <g> 
        <g> 
         <path d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path> 
        </g> 
        <g transform="translate(576, -150) scale(0.707)"> 
         <path d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path> 
        </g> 
       </g> 
      </g> 
     </g> 
    </svg>之间的对比，忽略CLS和CLS以及中间层和中间层之间的对比是最优的，即保留(1)(3)。<img src="https://images2.imgbox.com/20/19/TwgRscOm_o.png"></p> 
 <p>这篇论文没有选择直接从底层数据增强角度出发，是稍微偏模型方法的改进的，侧重挖掘模型内部的信息。主实验是在STS和SentEval任务上测试的，从结果来看的话，仍然是SimCSE要好很多，而且SimCSE操作起来是更简单的。不过本文也是提供了一个不一样的思路。</p> 
 <h3>[2] 花式数据增强</h3> 
 <p><em><strong>ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer<br></strong></em><em>https://arxiv.org/pdf/2105.11741.pdf</em></p> 
 <p>来自北邮的工作，也是研究如何在无监督的模式下，学习更好的句子表示。该工作主要对比了使用4种不同的数据增强方式进行对比对句子表示的作用。<img src="https://images2.imgbox.com/72/0f/nNnJI4pH_o.png"></p> 
 <p>模型是在STS任务上进行评估的。和SimCSE一样也用了NLI做监督，整体性能比SimCSE低1-2个点。</p> 
 <h3>[3] 无监督文本表示</h3> 
 <p><em><strong>DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations</strong></em><br><em>https://arxiv.org/pdf/2006.03659.pdf</em></p> 
 <p>DeCLUTR来自多伦多大学，是NLP领域使用对比学习中较早的一篇，去年6月份就放到arxiv上面了。</p> 
 <p>文章研究的问题同样是：如何利对比学习从大量无标注数据学习更好的通用句子表示？文中的对比体现在两个方面：</p> 
 <blockquote> 
  <ol><li><p style="text-align: left">对比来自不同文档的文本片段(span)的语义。如果两个文本片段(span)来自同一个文档，那么他们的语义表示的距离应该相对较近，否则距离远；</p></li><li><p style="text-align: left">对比来自同一文档的文本span。当两个文本片段都来自同一个文档，如果他们在文档中的位置距离比较近，他们的语义表示距离近，否则远。</p></li></ol> 
 </blockquote> 
 <p>在采样正例的时候有些讲究。具体来讲是先从一个文档中采样N(&gt;=1)个原始文本片段 (锚点span)，然后从每个锚点span周围采样，作为正例 span。采样规则是正例span可以与锚点span交叠、相邻、从属。负例是从一个batch中随机采样得到的。对比学习的损失函数是InfoNCE。模型整体的损失函数是InfoNCE和MLM的加和。</p> 
 <p>实验是在SenEval benchmark(28个数据集）上进行测试的，包含有/半监督任务和无监督任务。有/半监督任务的baseline有InferSent，Universal Sentence Encoder和Sentence Transformers；无监督任务的baseline有QuickThoughts。最显著的实验结果是DeCLUTR在大部分的数据集上取得了SOTA，并且在无监督任务上取得了和有监督任务相当的结果。</p> 
 <p>接下来两篇文章是关于如何利用对比学习提升自然语言理解任务的性能。</p> 
 <h3>[4] 论负例对对比学习的重要性</h3> 
 <p><em><strong>CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding<br></strong></em><em>https://arxiv.org/pdf/2107.00440.pdf</em></p> 
 <p>来自清华大学，文章探讨的是如何利用对比学习提高模型的鲁棒性。在初步实验中发现用探针对句子语义进行轻微扰动，模型就会预测错误。之前的对抗训练确实能够从扰动的样本中学习，但是主要侧重于语义相似的扰动，<strong>忽略了语义不同或者相反的扰动。这样的语义改变无法被对抗学习无法检测到</strong>。本文提出CLINE，使用无监督的方法构造负样本。通过同时利用语义相似和相反的样例和原始样例进行对比，模型可以侦测到扰动导致的语义的改变。</p> 
 <p>正负例的构造：</p> 
 <blockquote> 
  <p>正例是将句子中的词(名词、动词、形容词）替换为其同义词. 负例是将句子中的词替换为其反义词或者随机选择的词。</p> 
 </blockquote> 
 <p>文中的损失函数由三部分构成：掩码语言模型的MLM损失 + 检测当前词是否是被替换的词 的损失RTD + InfoNCE对比正例和负例。有个小细节不太一样的是对比InfoNCE中并没有引入温度参数τ。</p> 
 <p><img src="https://images2.imgbox.com/2e/46/X6W1prrj_o.png">实验是在NLU任务上进行的，包括NLI(SNLI, PERSPECTRUM,) 情感分析(IMDB,MB) 阅读理解 （BoolQ）, 新闻分类(AG)。实验结果表明使用CLINE训练的模型可以同时在对抗测试集和对比测试集上提升性能。</p> 
 <h3>[5] 对比实例学习+远距离监督关系抽取</h3> 
 <p><em><strong>CIL: Contrastive Instance Learning Framework for Distantly Supervised Relation Extraction</strong></em><br><em>https://arxiv.org/pdf/2106.10855.pdf</em></p> 
 <p>来自阿里巴巴-浙江大学前沿技术联合研究中心，研究如何利用对比学习提高远距离监督的关系抽取任务的性能。</p> 
 <p>从对比角度讲，正例是同一关系下的实例对，负例是不同关系的实例对。文中的重点是<strong>在有噪声的情况下，如何构造正负例</strong>。CIL的baseline是多实例对比学习，是将多个属于同一关系的实例放在一个bag中，一起训练得到一个关系的表示。每个实例都被假设是表达了一个实体对之间的关系。</p> 
 <blockquote> 
  <p>正例：直觉上讲，对于一个实例 
     <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 -442 763 599.8" style="vertical-align: -0.357ex;width: 1.726ex;height: 1.357ex;"> 
      <g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"> 
       <g> 
        <g> 
         <g> 
          <path d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path> 
         </g> 
         <g transform="translate(469, -150) scale(0.707)"> 
          <path d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path> 
         </g> 
        </g> 
       </g> 
      </g> 
     </svg>的正例只要从同一个bag中随机sample一个就好，或者使用bag的整体表示。但因为是远距离监督，无法保证任意两个实例之间都一定表达了同一种关系，同样也无法保证样例和bag的整体表示一定关系相同。如果这样强行构造正负例的话，必然会引入噪声。文中采用的一种方式是，对于插入/替换掉 
     <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 -442 763 599.8" style="vertical-align: -0.357ex;width: 1.726ex;height: 1.357ex;"> 
      <g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"> 
       <g> 
        <g> 
         <g> 
          <path d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path> 
         </g> 
         <g transform="translate(469, -150) scale(0.707)"> 
          <path d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path> 
         </g> 
        </g> 
       </g> 
      </g> 
     </svg>中不重要的词语（还是数据增强）。</p> 
 </blockquote> 
 <blockquote> 
  <p>负例：同样因为是远距离监督，不能随便从一个别的bag中采样一个实例 
     <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 -442 810.3 736.2" style="vertical-align: -0.666ex;width: 1.833ex;height: 1.666ex;"> 
      <g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"> 
       <g> 
        <g> 
         <g> 
          <path d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path> 
         </g> 
         <g transform="translate(469, -150) scale(0.707)"> 
          <path d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path> 
         </g> 
        </g> 
       </g> 
      </g> 
     </svg>作为 
     <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 -442 763 599.8" style="vertical-align: -0.357ex;width: 1.726ex;height: 1.357ex;"> 
      <g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"> 
       <g> 
        <g> 
         <g> 
          <path d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path> 
         </g> 
         <g transform="translate(469, -150) scale(0.707)"> 
          <path d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path> 
         </g> 
        </g> 
       </g> 
      </g> 
     </svg>的负例，那样噪声会比较大。因此文中采用了使用整个别的bag的表示作为负例，能相对更好地降噪。</p> 
 </blockquote> 
 <p>模型的损失函数是InfoNCE对比损失和MLM损失的加权和。CIL在NYT10，GDS和KBP三个数据集上取得较大提升。</p> 
 <h3>[6] Post-training中使用对比学习</h3> 
 <p><em><strong>Bi-Granularity Contrastive Learning for Post-Training in Few-Shot Scene</strong></em><br><em>https://arxiv.org/pdf/2106.02327</em></p> 
 <p>来自中山大学，本文主要针对样本量稀少的场景，如何使用对比学习先在无标注数据集进行post-training， 然后再在有标注数据集上fine-tuning。</p> 
 <p>对比方法：互补的mask方法，将一个输入进行多次mask，第一次的mask的比例是 
     <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 -442 859.2 636" style="vertical-align: -0.439ex;width: 1.944ex;height: 1.439ex;"> 
      <g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"> 
       <g> 
        <g> 
         <g> 
          <path d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path> 
         </g> 
         <g transform="translate(503, -150) scale(0.707)"> 
          <path d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path> 
         </g> 
        </g> 
       </g> 
      </g> 
     </svg>, 第二次mask的时候只针对第一次mask中没被选择的token以 
     <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 -442 1173.8 636" style="vertical-align: -0.439ex;width: 2.656ex;height: 1.439ex;"> 
      <g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"> 
       <g> 
        <g> 
         <g> 
          <path d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path> 
         </g> 
         <g transform="translate(503, -150) scale(0.707)"> 
          <path d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path> 
         </g> 
        </g> 
       </g> 
      </g> 
     </svg>的比例进行mask,所以两个句子被mask的部分是互补的，第三次以此类推。 <strong>对比是在多个被mask的输入上进行的</strong>。这样做的好处是既可以避免 
    <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 -442 1173.8 636" style="vertical-align: -0.439ex;width: 2.656ex;height: 1.439ex;"> 
     <g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"> 
      <g> 
       <g> 
        <g> 
         <path d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path> 
        </g> 
        <g transform="translate(503, -150) scale(0.707)"> 
         <path d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path> 
        </g> 
       </g> 
      </g> 
     </g> 
    </svg>太小时，两个句子太相似导致对比损失迅速降到0，也可以避免 
    <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 -442 1173.8 636" style="vertical-align: -0.439ex;width: 2.656ex;height: 1.439ex;"> 
     <g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"> 
      <g> 
       <g> 
        <g> 
         <path d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path> 
        </g> 
        <g transform="translate(503, -150) scale(0.707)"> 
         <path d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path> 
        </g> 
       </g> 
      </g> 
     </g> 
    </svg>太大而导致模型无法恢复mask的内容。（和SimCSE的直接两次dropout相比复杂了点，但有异曲同工之妙）。</p> 
 <img src="https://images2.imgbox.com/08/73/WJ9rWGpj_o.png"> 
 <p>实验是在少样本GLUE上进行的，只有20个样例的时候提升不是很明显，样本100和1000的时候相比之前SOTA有轻微提升。</p> 
 <h3>[7] 对比学习+生成式摘要</h3> 
 <p><em><strong>SimCLS: A Simple Framework for Contrastive Learning of Abstractive Summarization</strong></em><br><em>https://arxiv.org/pdf/2106.01890</em></p> 
 <p>来自CMU，蛮有意思的一篇文章。核心点是利用对比学习将文本生成看作是reference-free的评价问题。</p> 
 <p>生成式摘要典型的框架是Seq2Seq，之前也有工作将对比学习损失作为MLE损失的增强。不同的是，这篇文章将两个损失用在了不同的阶段。文中将摘要生成分解为两个过程：生成 和 评分+选择。从而提出了two-stage的框架，stage1是Seq2Seq模型，仍然利用MLE损失生成候选摘要，stage2引入对比学习，利用参数化的评估模型对stage1中生成的候选进行排序。两个阶段是分开优化的，都是有监督的。<strong>这里对比的是生成的候选摘要和原始文档</strong>。引入了一个raking loss， 希望预测值和真实值接近；希望每个候选值之间有差距。</p> 
 <h3>[8] 对比学习 + 意图检测</h3> 
 <p><em><strong>Modeling Discriminative Representations for Out-of-Domain Detection with Supervised Contrastive Learning</strong></em> <br><em>https://arxiv.org/pdf/2105.14289.pdf</em></p> 
 <p>来自北邮模式识别实验室，研究的问题是：在Task-oriented的对话系统中，如何检测用户query中的跟task/domain不相关的问题。比如你问一个银行的app语音助手，我有多少余额，它检测该问题为in-domain (IND) 的问题，并给出回答；但你如果问它，我们一起健身的小伙伴都怎样呀？我们不希望模型“不懂装懂”，而是希望它可以检测该问题为out-of-domain (OOD)，并引导用户提出domain相关的问题。下图是来自数据集[11]中的一个样例。</p> 
 <img src="https://images2.imgbox.com/5f/05/Pc372rPj_o.png"> 
 <p>OOD的检测方法分为有监督和无监督的两种。有监督的方式在训练时已知哪些数据OOD的，所以在训练时可以将OOD的数据当成一个类型；无监督方式训练的时候只有标注的IND数据。常用的方法是先利用IND数据学习类别的特征 (分类器），然后使用检测算法计算IND样本和OOD样本的相似度。</p> 
 <p>本文的先验假设是：</p> 
 <blockquote> 
  <p>一个OOD检测模型依赖于高质量IND类别表示模型。之前的IND分类器虽然在IND数据上表现好，应用到OOD时性能不高，原因是类别间的间隔很模糊。所以该工作的核心是利用对比学习减小类内距离，增大类间距离。更好的IND聚类促使更好的OOD分类。</p> 
 </blockquote> 
 <p>本文主要针对的是无监督OOD进行训练，策略是先用有监督对比学习在IND数据上训练，然后用cross-entropy损失对分类器fine-tune，有监督对比学习的目标是拉近IND中属于拉近同一类别的意图，推远不同类别的意图。因此:</p> 
 <blockquote> 
  <p>正例对来自同一个类别的数据 负例是不同类别的数据</p> 
 </blockquote> 
 <p>文中也使用了对抗攻击生成hard正例来做数据增强。文中的实验是比较全面的，对比了不同scale的数据集，不同的encoder，不同的OOD检测算法。</p> 
 <p>小花觉得这篇有意思主要是因为OOD检测的思路，不仅可以用在意图检测领域，还可以直接扩展到别的领域，比如用于关系抽取中检测新的关系。</p> 
 <p>接下来的两篇论文都利用了任务本身的属性将对比扩展到了多对多上，同时包含单个模态/语言的对比和跨模态/跨语言的对比。</p> 
 <h3>[9] 对比学习 + 多模态学习</h3> 
 <p><em><strong>UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning</strong></em><br><em>https://arxiv.org/pdf/2012.15409.pdf</em></p> 
 <p>来自百度的UNIMO，利用跨模态的对比学习将文本和视觉信息对齐到一个统一的语义空间。之前也有统一训练文本和视觉的模型，比如ViLBERT, VisualBERT，但是它们只能利用有限的文本-图像对齐的数据，且无法有效的适应到单模态的场景。本文要解决的是问题是：如何通过对比学习同时利用单模态和多模态的数据来实现更好的图像-文本对齐？</p> 
 <p>对比部分核心的点是，通过花式重写原始的caption来生成正例和负例。<strong>对于一对对齐的图像-文本数据，通过文本重写的方式构造多模态的正负例样本，同时通过文本/图像检索的方式构造单模态的正例样本。</strong> 正负例样本又分为多个level，包括句子级别、短语级别、词级别。比如句子级别的多模态的正例是通过back-translation生成的，负例是利用当前图片的字幕从其它图片的字幕中找相似的得到的。</p> 
 <img src="https://images2.imgbox.com/cc/87/lETIfLX2_o.png"> 
 <p>UNIMO的优势在于可以同时利用单模态数据和多模态对齐数据进行训练，测试时在单模态的理解和生成任务上都表现很好。</p> 
 <h3>[10] 对比学习 + 机器翻译</h3> 
 <p><em><strong>Contrastive Learning for Many-to-many Multilingual Neural Machine Translation</strong></em> <br><em>https://arxiv.org/pdf/2105.09501.pdf</em></p> 
 <p>看到这篇文章的时候，脑子里想的是，“一生二，二生三，三生万物”。在对比学习中，只要存在一个对象，我们就可以给它找到或者造一个对比对象，让它们去自我对比，自己进化；当多个对象成立的时候，我们都不需要造了，只需要利用就好。</p> 
 <p>多对多机翻就是典型的例子。这篇文章来自字节跳动AI Lab，研究的问题是：如何学习更好的通用跨语言表示，来获得更好的多语言翻译效果？尤其是当源语言或者目标语言不是English的时候。</p> 
 <p>本文的先验假设是，如果两句话说的是同一个意思，即使它们使用的语言不相同，那么它们在语义空间中的表示也应该接近。所以本文的训练目标是：<strong>减少相似句子表示之间的距离，增大不相关句子表示之间的距离</strong>。文中使用了fancy的数据增强，同时使用单语和多语的数据进行对比。<img src="https://images2.imgbox.com/91/2c/mEka1iOO_o.png"></p> 
 <p>方法简单，效果好，实验solid，值得细品。</p> 
 <h3>插播一个有趣的游戏</h3> 
 <p>你玩过词联想的游戏嘛？什么？还没有？</p> 
 <p>来来来，奉上链接！<br><em>https://smallworldofwords.org/zh</em></p> 
 <h3>一起交流</h3> 
 <p style="text-align: left">想和你一起学习进步！『<strong>NewBeeNLP』</strong>目前已经建立了多个不同方向交流群（<strong>机器学习 / 深度学习 / 自然语言处理 / 搜索推荐 / 图网络 / 面试交流 / </strong>等），名额有限，赶紧添加下方微信加入一起讨论交流吧！（注意一定要<strong>备注信息</strong>才能通过）</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/a2/8a/FpV8dwTt_o.png"></p> 
 <blockquote> 
  <blockquote> 
   <p><img src="https://images2.imgbox.com/51/78/WdaAUTHP_o.png"></p> 
   <h5>[1] DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations https://arxiv.org/pdf/2006.03659.pdf</h5> 
   <h5>[2] Self-Guided Contrastive Learning for BERT Sentence Representations https://arxiv.org/pdf/2106.07345.pdf</h5> 
   <h5>[3] ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer https://arxiv.org/pdf/2105.11741.pdf</h5> 
   <h5>[4] CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding https://arxiv.org/pdf/2107.00440.pdf</h5> 
   <h5>[5] CIL: Contrastive Instance Learning Framework for Distantly Supervised Relation Extraction https://arxiv.org/pdf/2106.10855.pdf</h5> 
   <h5>[6] Bi-Granularity Contrastive Learning for Post-Training in Few-Shot Scene https://arxiv.org/pdf/2106.02327</h5> 
   <h5>[7] SimCLS: A Simple Framework for Contrastive Learning of Abstractive Summarization https://arxiv.org/pdf/2106.01890</h5> 
   <h5>[8] Modeling Discriminative Representations for Out-of-Domain Detection with Supervised Contrastive Learning  https://arxiv.org/pdf/2105.14289</h5> 
   <h5>[9] UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning https://arxiv.org/pdf/2012.15409.pdf</h5> 
   <h5>[10] Contrastive Learning for Many-to-many Multilingual Neural Machine Translation https://arxiv.org/pdf/2105.09501.pdf</h5> 
   <h5>[11] An Evaluation Dataset for Intent Classificationand Out-of-Scope Prediction https://aclanthology.org/D19-1131.pdf</h5> 
   <h5>[12] CLEVE: Contrastive Pre-training for Event Extraction https://arxiv.org/pdf/2105.14485.pdf</h5> 
   <h5>[13] KACE: Generating Knowledge Aware Contrastive Explanations for Natural Language Inference (not public yet)</h5> 
   <h5>[14] xMoCo: Cross Momentum Contrastive Learning for Open-Domain Question Answering (not public yet)</h5> 
   <p style="text-align: center">- <strong>END </strong>-</p> 
   <p><img src="https://images2.imgbox.com/ca/2a/jcnEIzVb_o.png"><br></p> 
   <p style="text-align: center"><img src="https://images2.imgbox.com/12/12/dVG73Lh2_o.png"><br></p> 
   <p><a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ%3D%3D&amp;chksm=97ad36fda0dabfebf2e7b956796168b057bf98a4dc3a169e9220fe7d48040e0e54d53a366eca&amp;idx=1&amp;mid=2247503919&amp;scene=21&amp;sn=43cf6dc023b2479d7d5c9e6810281805#wechat_redirect" rel="nofollow">近代自然语言处理技术发展的『第四范式』</a></p> 
   <p><a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ%3D%3D&amp;chksm=97ad36fda0dabfebf2e7b956796168b057bf98a4dc3a169e9220fe7d48040e0e54d53a366eca&amp;idx=1&amp;mid=2247503919&amp;scene=21&amp;sn=43cf6dc023b2479d7d5c9e6810281805#wechat_redirect" rel="nofollow">2021-08-04</a></p> 
   <a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ%3D%3D&amp;chksm=97ad36fda0dabfebf2e7b956796168b057bf98a4dc3a169e9220fe7d48040e0e54d53a366eca&amp;idx=1&amp;mid=2247503919&amp;scene=21&amp;sn=43cf6dc023b2479d7d5c9e6810281805#wechat_redirect" rel="nofollow"><img src="https://images2.imgbox.com/aa/1c/hGMwQqYc_o.png"> </a> 
   <p><a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ%3D%3D&amp;chksm=97ad293ca0daa02aaaee30a9512a612c9779d8f7bcd6826f64d584df9917dfda7ed2cb1dd9d7&amp;idx=1&amp;mid=2247503854&amp;scene=21&amp;sn=0f9689ad05644888f02f9e389d1d5760#wechat_redirect" rel="nofollow">[旧文新读] 深度学习在Airbnb搜索的应用实践</a></p> 
   <p><a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ%3D%3D&amp;chksm=97ad293ca0daa02aaaee30a9512a612c9779d8f7bcd6826f64d584df9917dfda7ed2cb1dd9d7&amp;idx=1&amp;mid=2247503854&amp;scene=21&amp;sn=0f9689ad05644888f02f9e389d1d5760#wechat_redirect" rel="nofollow">2021-08-02</a></p> 
   <a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ%3D%3D&amp;chksm=97ad293ca0daa02aaaee30a9512a612c9779d8f7bcd6826f64d584df9917dfda7ed2cb1dd9d7&amp;idx=1&amp;mid=2247503854&amp;scene=21&amp;sn=0f9689ad05644888f02f9e389d1d5760#wechat_redirect" rel="nofollow"><img src="https://images2.imgbox.com/5f/b2/vmMn92Tm_o.png"> </a> 
   <p><a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ%3D%3D&amp;chksm=97ad293ea0daa0281710df91b9b44ff39bef4e539ad49938d7e1f438518c3660af77cd9e2fa3&amp;idx=1&amp;mid=2247503852&amp;scene=21&amp;sn=fe4310dd40e7994ff418f4a6bdb28a5a#wechat_redirect" rel="nofollow">Embedding 技术在推荐系统中的应用实践</a></p> 
   <p><a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ%3D%3D&amp;chksm=97ad293ea0daa0281710df91b9b44ff39bef4e539ad49938d7e1f438518c3660af77cd9e2fa3&amp;idx=1&amp;mid=2247503852&amp;scene=21&amp;sn=fe4310dd40e7994ff418f4a6bdb28a5a#wechat_redirect" rel="nofollow">2021-08-01</a></p> 
   <a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ%3D%3D&amp;chksm=97ad293ea0daa0281710df91b9b44ff39bef4e539ad49938d7e1f438518c3660af77cd9e2fa3&amp;idx=1&amp;mid=2247503852&amp;scene=21&amp;sn=fe4310dd40e7994ff418f4a6bdb28a5a#wechat_redirect" rel="nofollow"><img src="https://images2.imgbox.com/03/2e/X1DYIsmJ_o.png"> </a> 
   <p><a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ%3D%3D&amp;chksm=97ad2957a0daa0411a9374ea1f8ea7d4ba2fe07158cbc403b9ff88157fe3a2740c4427b96223&amp;idx=1&amp;mid=2247503749&amp;scene=21&amp;sn=9348fd08b584e817f62a66dced2f43b1#wechat_redirect" rel="nofollow">NLP预训练家族 | Transformer-XL及其进化XLNet</a></p> 
   <p><a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ%3D%3D&amp;chksm=97ad2957a0daa0411a9374ea1f8ea7d4ba2fe07158cbc403b9ff88157fe3a2740c4427b96223&amp;idx=1&amp;mid=2247503749&amp;scene=21&amp;sn=9348fd08b584e817f62a66dced2f43b1#wechat_redirect" rel="nofollow">2021-07-30</a></p> 
   <a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ%3D%3D&amp;chksm=97ad2957a0daa0411a9374ea1f8ea7d4ba2fe07158cbc403b9ff88157fe3a2740c4427b96223&amp;idx=1&amp;mid=2247503749&amp;scene=21&amp;sn=9348fd08b584e817f62a66dced2f43b1#wechat_redirect" rel="nofollow"><img src="https://images2.imgbox.com/6b/60/EKXCcJ53_o.png"> </a> 
   <p style="text-align: center"><img src="https://images2.imgbox.com/2b/a2/24mvuJV6_o.gif" height="792" width="1068"></p> 
  </blockquote> 
 </blockquote> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d0974f00198845cc4fc4f33814e67576/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">AI训练与推理芯片</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/48a20b0753f7924cdf215ce78eee54fc/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">普通类，抽象类，接口，内部类</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>