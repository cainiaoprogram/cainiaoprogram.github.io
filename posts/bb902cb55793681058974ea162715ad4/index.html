<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>何恺明团队提出探索用于目标检测的不分层ViT Backbone - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="何恺明团队提出探索用于目标检测的不分层ViT Backbone" />
<meta property="og:description" content="【写在前面】 作者探索了普通的、非分层的视觉Transformer（ViT）作为目标检测的骨干网络。这种设计使原始 ViT 架构能够针对对象检测进行微调，而无需重新设计用于预训练的分层主干。通过对微调的最小调整，本文的普通骨干检测器可以获得具有竞争力的结果。令人惊讶的是，作者观察到:(1)从单尺度特征图(没有常见的FPN设计)构建一个简单的特征金字塔是足够的，(2)在很少的跨窗口传播块的辅助下，使用窗口注意(不移动)是足够的。通过使用预训练的纯ViT主干mask自动编码器(MAE)，本文的检测器名为ViTDet，可以与之前所有基于分层主干的SOTA方法取得竞争性的结果，仅使用ImageNet-1K预训练就可以在COCO数据集上达到61.3 APbox。
1. 论文和代码地址 Exploring Plain Vision Transformer Backbones
for Object Detection
论文地址：https://arxiv.org/abs/2203.16527
代码地址：https://github.com/facebookresearch/detectron2/tree/main/projects/ViTDet
2. Motivation 现代目标检测器通常由一个与检测任务无关的主干特征提取器和一组包含特定检测先验知识的颈部（neck）和头部（head）组成。颈部/头部中的常见组件可能包括RoI 操作 、区域建议网络 (RPN) 或锚点（Anchor）、特征金字塔网络 (FPN) 等。如果特定任务的颈部/头部的设计与骨干的设计脱钩，它们可能会并行发展。根据实验，目标检测研究受益于对通用主干和检测特定模块的很大程度上独立探索。长期以来，由于卷积网络 (ConvNet) 的实际设计，这些主干一直是多尺度、分层架构，这严重影响了用于检测多尺度对象的颈部/头部设计（例如，FPN ）。
在过去的一年中，视觉Transformer (ViT) 已被确立为视觉识别的强大支柱。与典型的 ConvNets 不同，原始的 ViT 是一个简单的、非分层的架构，它始终保持单尺度特征图。它的“极简主义”追求在应用于对象检测时遇到了挑战——例如，**如何通过上游预训练的简单主干处理下游任务中的多尺度对象？是否普通的 ViT在使用高分辨率检测图像效率低下？**放弃这种追求的一种解决方案是将分层设计重新引入主干。该解决方案，例如 Swin Transformers，可以继承基于 ConvNet 的检测器设计并显示出成功的结果。
在这项工作中，作者追求不同的方向：探索仅使用普通、非分层主干的目标检测器。如果这个方向成功，它将能够使用原始 ViT 主干进行目标检测；这将使预训练设计与微调需求脱钩，保持上游与下游任务的独立性，就像基于 ConvNet 的研究一样。这个方向也部分遵循了 ViT 在追求通用特征时“减少归纳偏置”的哲学。由于非局部自注意力计算可以学习平移等变特征，它们还可以从某些形式的监督或自监督预训练中学习尺度等变特征。
在本文的研究中，作者的目标不是开发新组件。相反，作者进行了足以克服上述挑战的最小调整。特别是，本文的检测器仅从普通 ViT 主干的最后一个特征图构建一个简单的特征金字塔（见上图）。这放弃了 FPN 设计并放弃了分层主干的要求。为了有效地从高分辨率图像中提取特征，本文的检测器使用简单的非重叠窗口注意力。少量的跨窗口块（例如 4 个）可能是全局注意力或卷积，用于传播信息。这些调整仅在微调期间进行，不会改变预训练。
事实证明，本文简单的设计取得了令人惊讶的结果。作者发现在普通的 ViT 主干的情况下，FPN 设计不是必需的，它的好处可以通过从大步幅、单比例特征图构建的简单金字塔有效地获得。作者还发现，只要信息在少量层的窗口中很好地传播，窗口注意力就足够了。
更令人惊讶的是，在某些情况下，本文名为 ViTDet 的普通骨干检测器可以与领先的分层骨干检测器（例如 Swin 、MViT ）竞争。使用 Masked Autoencoder (MAE) 预训练，本文的普通骨干检测器可以胜过在 ImageNet-1K/21K 上通过监督预训练的分层检测器（见上图）。对于较大的模型尺寸，收益更为突出。本文的检测器的竞争力是在不同的目标检测器框架下观察到的，包括 Mask R-CNN 、Cascade Mask R-CNN 及其增强模型。本文基于普通的 ViT-Huge 主干，在 COCO 数据集上报告了 61." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/bb902cb55793681058974ea162715ad4/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-08-12T00:30:00+08:00" />
<meta property="article:modified_time" content="2022-08-12T00:30:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">何恺明团队提出探索用于目标检测的不分层ViT Backbone</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_2"></a>【写在前面】</h2> 
<p>作者探索了普通的、非分层的视觉Transformer（ViT）作为目标检测的骨干网络。这种设计使原始 ViT 架构能够针对对象检测进行微调，而无需重新设计用于预训练的分层主干。通过对微调的最小调整，本文的普通骨干检测器可以获得具有竞争力的结果。令人惊讶的是，作者观察到:(1)<strong>从单尺度特征图(没有常见的FPN设计)构建一个简单的特征金字塔是足够的</strong>，(2)<strong>在很少的跨窗口传播块的辅助下，使用窗口注意(不移动)是足够的</strong>。通过使用预训练的纯ViT主干mask自动编码器(MAE)，本文的检测器名为ViTDet，可以与之前所有基于分层主干的SOTA方法取得竞争性的结果，仅使用ImageNet-1K预训练就可以在COCO数据集上达到61.3 APbox。</p> 
<h2><a id="1__6"></a>1. 论文和代码地址</h2> 
<p><img src="https://images2.imgbox.com/e9/c8/HPXyNSkc_o.png" alt=""></p> 
<p>Exploring Plain Vision Transformer Backbones<br> for Object Detection</p> 
<p>论文地址：<a href="https://arxiv.org/abs/2203.16527" rel="nofollow" title="https://arxiv.org/abs/2203.16527">https://arxiv.org/abs/2203.16527</a></p> 
<p>代码地址：<a href="https://github.com/facebookresearch/detectron2/tree/main/projects/ViTDet" title="https://github.com/facebookresearch/detectron2/tree/main/projects/ViTDet">https://github.com/facebookresearch/detectron2/tree/main/projects/ViTDet</a></p> 
<h2><a id="2_Motivation_19"></a>2. Motivation</h2> 
<p>现代目标检测器通常由一个与检测任务无关的主干特征提取器和一组包含特定检测先验知识的颈部（neck）和头部（head）组成。颈部/头部中的常见组件可能包括RoI 操作 、区域建议网络 (RPN) 或锚点（Anchor）、特征金字塔网络 (FPN) 等。如果特定任务的颈部/头部的设计与骨干的设计脱钩，它们可能会并行发展。根据实验，目标检测研究受益于对通用主干和检测特定模块的很大程度上独立探索。长期以来，由于卷积网络 (ConvNet) 的实际设计，这些主干一直是多尺度、分层架构，这严重影响了用于检测多尺度对象的颈部/头部设计（例如，FPN ）。<br> 在过去的一年中，视觉Transformer (ViT) 已被确立为视觉识别的强大支柱。与典型的 ConvNets 不同，原始的 ViT 是一个简单的、非分层的架构，它始终保持单尺度特征图。它的“极简主义”追求在应用于对象检测时遇到了挑战——例如，**如何通过上游预训练的简单主干处理下游任务中的多尺度对象？是否普通的 ViT在使用高分辨率检测图像效率低下？**放弃这种追求的一种解决方案是将分层设计重新引入主干。该解决方案，例如 Swin Transformers，可以继承基于 ConvNet 的检测器设计并显示出成功的结果。</p> 
<p>在这项工作中，作者追求不同的方向：<strong>探索仅使用普通、非分层主干的目标检测器</strong>。如果这个方向成功，它将能够使用原始 ViT 主干进行目标检测；这将使预训练设计与微调需求脱钩，保持上游与下游任务的独立性，就像基于 ConvNet 的研究一样。这个方向也部分遵循了 ViT 在追求通用特征时“减少归纳偏置”的哲学。由于非局部自注意力计算可以学习平移等变特征，它们还可以从某些形式的监督或自监督预训练中学习尺度等变特征。</p> 
<p><img src="https://images2.imgbox.com/32/ac/LEcZieaS_o.png" alt=""></p> 
<p>在本文的研究中，作者的目标不是开发新组件。相反，作者进行了足以克服上述挑战的最小调整。特别是，本文的检测器仅从普通 ViT 主干的最后一个特征图构建一个简单的特征金字塔（见上图）。这放弃了 FPN 设计并放弃了分层主干的要求。为了有效地从高分辨率图像中提取特征，本文的检测器使用简单的非重叠窗口注意力。少量的跨窗口块（例如 4 个）可能是全局注意力或卷积，用于传播信息。这些调整仅在微调期间进行，不会改变预训练。</p> 
<p>事实证明，本文简单的设计取得了令人惊讶的结果。作者发现在普通的 ViT 主干的情况下，FPN 设计不是必需的，它的好处可以通过从大步幅、单比例特征图构建的简单金字塔有效地获得。作者还发现，只要信息在少量层的窗口中很好地传播，窗口注意力就足够了。</p> 
<p><img src="https://images2.imgbox.com/95/94/tINiFtlt_o.png" alt=""></p> 
<p>更令人惊讶的是，在某些情况下，本文名为 ViTDet 的普通骨干检测器可以与领先的分层骨干检测器（例如 Swin 、MViT ）竞争。使用 Masked Autoencoder (MAE) 预训练，本文的普通骨干检测器可以胜过在 ImageNet-1K/21K 上通过监督预训练的分层检测器（见上图）。对于较大的模型尺寸，收益更为突出。本文的检测器的竞争力是在不同的目标检测器框架下观察到的，包括 Mask R-CNN 、Cascade Mask R-CNN 及其增强模型。本文基于普通的 ViT-Huge 主干，在 COCO 数据集上报告了 61.3 APbox，仅使用没有标签的 ImageNet-1K 预训练。作者还在长尾 L VIS 检测数据集上展示了具有竞争力的结果。虽然这些强有力的结果可能部分归因于 MAE 预训练的有效性，但本文的研究表明，普通主干检测器可能很有前途，挑战了用于目标检测的分层主干的根深蒂固的位置。</p> 
<h2><a id="3__40"></a>3. 方法</h2> 
<p>本文的目标是消除对骨干网的分层约束，并启用对普通骨干网目标检测的探索。为此，本文的目标是进行最少的修改，以仅在微调期间使简单的主干适应目标检测任务。在这些适应之后，原则上可以应用任何检测器头，作者选择使用 Mask R-CNN及其扩展。</p> 
<h3><a id="Simple_feature_pyramid_44"></a>Simple feature pyramid</h3> 
<p>FPN是构建用于目标检测的网络内金字塔的常见解决方案。如果主干是分层的，FPN 的动机是结合早期阶段的高分辨率特征和后期阶段的更强特征。这是在 FPN 中通过自上而下和横向连接实现的。</p> 
<p>如果主干是非分层的，那么 FPN 动机的基础就会丢失，因为主干中的所有特征图都具有相同的分辨率。在本文的场景中，作者仅使用主干中的最后一个特征图，它应该具有最强的特征。在这张特征图上，作者并行应用一组卷积或反卷积来生成多尺度特征图。具体来说，使用尺度为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          1 
         
        
          16 
         
        
       
      
        \frac{1}{16} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.1901em; vertical-align: -0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8451em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">16</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>（步幅 = 16 ）的默认 ViT 特征图，作者使用步幅为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         { 
        
       
         2 
        
       
         , 
        
       
         1 
        
       
         , 
        
        
        
          1 
         
        
          2 
         
        
       
         , 
        
        
        
          1 
         
        
          4 
         
        
       
         } 
        
       
      
        \left\{2,1, \frac{1}{2}, \frac{1}{4}\right\} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.2em; vertical-align: -0.35em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;"><span class="delimsizing size1">{<!-- --></span></span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8451em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8451em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top: 0em;"><span class="delimsizing size1">}</span></span></span></span></span></span></span>的卷积生成尺度为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         { 
        
        
        
          1 
         
        
          32 
         
        
       
         , 
        
        
        
          1 
         
        
          16 
         
        
       
         , 
        
        
        
          1 
         
        
          8 
         
        
       
         , 
        
        
        
          1 
         
        
          4 
         
        
       
         } 
        
       
      
        \left\{\frac{1}{32}, \frac{1}{16}, \frac{1}{8}, \frac{1}{4}\right\} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.2em; vertical-align: -0.35em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;"><span class="delimsizing size1">{<!-- --></span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8451em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">32</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8451em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">16</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8451em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">8</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8451em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top: 0em;"><span class="delimsizing size1">}</span></span></span></span></span></span></span>的特征图，其中小数步幅表示反卷积。作者将其称为“简单特征金字塔”。</p> 
<p>从单个特征图构建多尺度特征图的策略与 SSD的策略有关。然而，本文的场景涉及从深度、低分辨率的特征图进行上采样，而SSD利用了较浅的特征图。在分层主干中，上采样通常由横向连接辅助；在普通的 ViT 主干中，作者凭实验发现这不是必需的，简单的反卷积就足够了。作者认为这是因为 ViT 可以依赖位置嵌入来编码位置，并且还因为高维 ViT 补丁嵌入不一定会丢弃信息。</p> 
<p><img src="https://images2.imgbox.com/ef/b5/zsinsbJ9_o.png" alt=""></p> 
<p>作者将与同样构建在普通主干上的两个 FPN 变体进行比较（上图）。在第一个变体中，主干被人为地分为多个阶段，以模仿分层主干的阶段，应用横向和自上而下的连接（上图 （a））。第二个变体与第一个变体类似，但仅使用最后一个映射而不是划分的阶段（上图 (b)）。</p> 
<h3><a id="Backbone_adaptation_58"></a>Backbone adaptation</h3> 
<p>目标检测器受益于高分辨率输入图像，但在整个主干中计算全局自注意力在显存中是令人望而却步的，而且速度很慢。在这项研究中，作者关注预训练的主干执行全局自注意的场景，然后在微调期间适应更高分辨率的输入。这与最近的通过主干预训练直接修改注意力计算的方法相反。本文的场景能够使用原始的 ViT 主干进行检测，而无需重新设计预训练架构。</p> 
<p>作者探索使用带有几个跨窗口块的窗口注意力。在微调期间，给定一个高分辨率特征图，作者将其划分为常规的非重叠窗口。在每个窗口内计算自注意力。这在原始Transformer 中被称为“受限”自注意力。</p> 
<p>与 Swin 不同，本文不会跨层“移动”窗口。为了允许信息传播，作者使用了极少数（默认情况下，4 个）可以跨窗口的块。作者将预训练的主干平均分成 4 个块子集（例如，对于 24 块 ViT-L，每个子集中有 6 个）。作者在每个子集的最后一个块中应用传​​播策略。在本文中，作者研究了这两种策略：</p> 
<p>(1) <strong>全局传播（Global propagation）</strong>。作者在每个子集的最后一个块中执行全局自注意。由于全局块的数量很少，显存和计算成本是可行的。这</p> 
<p>(2) <strong>卷积传播（Convolutional propagation）</strong>。作为替代方案，作者在每个子集之后添加一个额外的卷积块。卷积块是一个残差块，由一个或多个卷积和一个恒等连接组成。该块中的最后一层被初始化为零，因此该块的初始状态是一个identity。将块初始化为identity允许将其插入到预训练主干中的任何位置，而不会破坏主干的初始状态。</p> 
<p>本文的骨干适应很简单，并且使检测微调与全局自注意力预训练兼容。如前所述，没有必要重新设计预训练架构。</p> 
<h3><a id="Discussion_73"></a>Discussion</h3> 
<p>对象检测器包含可以与任务无关的组件，例如主干，以及其他特定于任务的组件，例如 RoI 头。这种模型分解使与任务无关的组件能够使用非检测数据（例如 ImageNet）进行预训练，这可能会提供一个优势，因为检测训练数据相对稀缺。</p> 
<p>从这个角度来看，追求包含较少归纳偏差的主干变得合理，因为可以使用大规模数据和/或自监督来有效地训练主干。相比之下，检测任务特定组件的可用数据相对较少，并且仍可能受益于额外的归纳偏差。虽然追求具有较少感应偏差的检测头是一个活跃的工作领域，但像 DETR这样的领先方法难以训练并且仍然受益于特定于检测的先验知识。在这些观察的推动下，本文的工作在检测器的主干方面遵循了原始普通 ViT 论文的精神。虽然 ViT 论文的讨论侧重于减少平移等变的归纳偏差，但在本文的案例中，它是关于在主干中的尺度等变上减少甚至没有归纳偏差。作者认为普通骨干实现的尺度等变性是从数据中学习先验知识，类似于它如何在没有卷积的情况下学习平移等方差和局部性。</p> 
<p>本文的目标是证明这种方法的可行性。因此，作者选择使用标准检测特定组件（即，Mask R-CNN 及其扩展）来实现本文的方法。在检测头中探索更少的感应偏差是未来工作的一个开放且有趣的方向。</p> 
<h3><a id="Implementation_81"></a>Implementation</h3> 
<p>作者使用 vanilla ViT-B、ViT-L、ViT-H作为预训练主干。将patch大小设置为 16，因此特征图比例为 1/16，即步幅 = 16。检测头遵循 Mask R-CNN 或 Cascade Mask R-CNN。输入图像为 1024×1024，在训练期间通过大规模抖动进行了增强。由于这种重度正则化，作者在 COCO 中微调了多达 100 个 epoch。使用 AdamW 优化器并使用基线版本搜索最佳超参数。</p> 
<h2><a id="4_85"></a>4.实验</h2> 
<p><img src="https://images2.imgbox.com/4f/1f/x8uOnl7O_o.png" alt=""></p> 
<p>上表展示了不同特征金字塔的实验结果。</p> 
<p><img src="https://images2.imgbox.com/0c/4c/XgFzknqq_o.png" alt=""></p> 
<p>上表消融了本文的骨干适应方法。简而言之，在具有纯窗口注意力且没有跨窗口传播块的基线之上，各种传播方式都可以显示出不错的收益。</p> 
<p><img src="https://images2.imgbox.com/28/9d/mggKXClc_o.png" alt=""></p> 
<p>上表展示了主干适应策略的实际表现。</p> 
<p><img src="https://images2.imgbox.com/11/f1/aMoiWlcP_o.png" alt=""></p> 
<p>上表展示了使用在 COCO 上评估的 Mask R-CNN 对具有普通 ViT 主干的预训练策略进行消融。</p> 
<p><img src="https://images2.imgbox.com/0a/c5/CDR4JIde_o.png" alt=""></p> 
<p>上表展示了在 COCO 上使用 Mask RCNN和 Cascade Mask R-CNN 比较普通与分层主干。</p> 
<p><img src="https://images2.imgbox.com/c3/55/pOedjMjx_o.png" alt=""></p> 
<p>上表展示了与原始论文报告的 COCO SOTA结果的系统级比较。</p> 
<p><img src="https://images2.imgbox.com/66/44/3XgpGHm2_o.png" alt=""></p> 
<p>上表展示了与原始论文报告的 LVIS SOTA结果的系统级比较。</p> 
<h2><a id="5__129"></a>5. 总结</h2> 
<p>本文的探索表明，plain-backbone 检测是一个很有前途的研究方向。这种方法在很大程度上保持了通用主干和下游特定任务设计的独立性——这在基于 ConvNet 的研究中是这种情况，但在基于 Transformer 的研究中却不是。作者希望将预训练与微调分离是一种普遍有益于社区的方法。例如，在自然语言处理（NLP）中，通用预训练（GPT、BERT）极大地推动了该领域的发展，并一直支持各种下游任务。在这项研究中，本文的普通骨干检测器受益于 MAE 中现成的预训练模型。作者希望这种方法也有助于拉近计算机视觉和 NLP 领域的距离。</p> 
<p><img src="https://images2.imgbox.com/df/a9/86yYOvL1_o.png" alt=""></p> 
<hr> 
<p>已建立深度学习公众号——<strong>FightingCV</strong>，欢迎大家关注！！！</p> 
<p><strong>ICCV、CVPR、NeurIPS、ICML论文解析汇总：https://github.com/xmu-xiaoma666/FightingCV-Paper-Reading</strong></p> 
<p><strong>面向小白的Attention、重参数、MLP、卷积核心代码学习：https://github.com/xmu-xiaoma666/External-Attention-pytorch</strong></p> 
<p>加入交流群，请添加小助手wx：<strong>FightngCV666</strong></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/39abdcbc5fce4062b7da0249ccd73566/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Ubuntu 安装 tbb 步骤详解</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a12c5d7972b3ec3d8aad248bfbc90eee/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">目标检测mAP（Mean Average Precision）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>