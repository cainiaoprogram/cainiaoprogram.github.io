<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>基于DeepSpeed训练ChatGPT - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="基于DeepSpeed训练ChatGPT" />
<meta property="og:description" content="基于DeepSpeed训练ChatGPT 最少只需一张32G GPU，自己也可以训练一个ChatGPT！ 最近微软发布了一个基于DeepSpeed的训练优化框架来完成ChatGPT类模型的训练，博主对其进行了研究并通过此博文分享相关技术细节。
一、配置预览 1、开源仓库：DeepSpeed-Chat
2、配置要求：
● cuda：11.0以上
● torch：1.12.1&#43;cu113
● deepspeed：0.9.0
● transformers：4.29.0.dev0
3、开源语料（Hugging face Dataset）：
● Dahoas/rm-static
● Dahoas/full-hh-rlhf
● Dahoas/synthetic-instruct-gptj-pairwise
● yitingxie/rlhf-reward-datasets
● openai/webgpt_comparisons
● stanfordnlp/SHP
4、数据格式样例：
需要包含三个字段，分别为：
● prompt：instruction-prompt，当前的输入；
● chosen：人来反馈选中的回复，或当前pair得分最高的回复；
● rejected：人类反馈未选中的回复，或当前pair得分最低的回复；
个人也可以按照这个格式设计自己的训练数据。
5、数据处理函数（样例）：
针对训练数据，可以设计如下几个数据处理函数。
# The prompt should be in the format of: &#34; Human: &#34; &#43; actual_prompt_sentence &#43; &#34; Assistant:&#34; # 只获取prompt字段的数据 def get_prompt(self, sample): return &#34; Human: &#34; &#43; sample[&#39;prompt&#39;] &#43; &#34;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/86cd1ee746afe1d6d2ea1d4141cb8b8c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-19T10:38:49+08:00" />
<meta property="article:modified_time" content="2023-04-19T10:38:49+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">基于DeepSpeed训练ChatGPT</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="DeepSpeedChatGPT_0"></a>基于DeepSpeed训练ChatGPT</h2> 
<h3><a id="32G_GPUChatGPT_1"></a>最少只需一张32G GPU，自己也可以训练一个ChatGPT！</h3> 
<p>  最近微软发布了一个基于DeepSpeed的训练优化框架来完成ChatGPT类模型的训练，博主对其进行了研究并通过此博文分享相关技术细节。</p> 
<h3><a id="_5"></a>一、配置预览</h3> 
<p><strong>1、开源仓库</strong>：<a href="https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat">DeepSpeed-Chat</a></p> 
<p><strong>2、配置要求：</strong><br> ● cuda：11.0以上<br> ● torch：1.12.1+cu113<br> ● deepspeed：0.9.0<br> ● transformers：4.29.0.dev0</p> 
<p><strong>3、开源语料（Hugging face Dataset）：</strong><br> ● Dahoas/rm-static<br> ● Dahoas/full-hh-rlhf<br> ● Dahoas/synthetic-instruct-gptj-pairwise<br> ● yitingxie/rlhf-reward-datasets<br> ● openai/webgpt_comparisons<br> ● stanfordnlp/SHP</p> 
<p><strong>4、数据格式样例：</strong><br> <img src="https://images2.imgbox.com/18/ff/IHKgwToI_o.png" alt="在这里插入图片描述"><br> 需要包含三个字段，分别为：<br> ● prompt：instruction-prompt，当前的输入；<br> ● chosen：人来反馈选中的回复，或当前pair得分最高的回复；<br> ● rejected：人类反馈未选中的回复，或当前pair得分最低的回复；<br> 个人也可以按照这个格式设计自己的训练数据。</p> 
<p><strong>5、数据处理函数（样例）：</strong><br>   针对训练数据，可以设计如下几个数据处理函数。</p> 
<pre><code class="prism language-python"><span class="token comment"># The prompt should be in the format of: " Human: " + actual_prompt_sentence + " Assistant:"</span>
<span class="token comment"># 只获取prompt字段的数据</span>
<span class="token keyword">def</span> <span class="token function">get_prompt</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sample<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token string">" Human: "</span> <span class="token operator">+</span> sample<span class="token punctuation">[</span><span class="token string">'prompt'</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token string">" Assistant:"</span>

<span class="token comment"># The chosen response should be in the format of: " " + actual_response_sentence</span>
<span class="token comment"># 只获取chosen字段的数据</span>
<span class="token keyword">def</span> <span class="token function">get_chosen</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sample<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token string">" "</span> <span class="token operator">+</span> sample<span class="token punctuation">[</span><span class="token string">'chosen'</span><span class="token punctuation">]</span>

<span class="token comment"># The rejected response should be in the format of: " " + actual_response_sentence</span>
<span class="token comment"># If the dataset does not have rejected response, return None</span>
<span class="token comment"># 只获取rejected字段的数据</span>
<span class="token keyword">def</span> <span class="token function">get_rejected</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sample<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token string">" "</span> <span class="token operator">+</span> sample<span class="token punctuation">[</span><span class="token string">'rejected'</span><span class="token punctuation">]</span>
<span class="token comment"># 同时获取prompt和chosen的数据</span>
<span class="token comment"># 这两段数据一一拼接后可以训练SFT</span>
<span class="token keyword">def</span> <span class="token function">get_prompt_and_chosen</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sample<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token string">" Human: "</span> <span class="token operator">+</span> sample<span class="token punctuation">[</span><span class="token string">'prompt'</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token string">" Assistant: "</span> <span class="token operator">+</span> sample<span class="token punctuation">[</span><span class="token string">'chosen'</span><span class="token punctuation">]</span>
<span class="token comment"># 同时获取prompt和rejected数据</span>
<span class="token keyword">def</span> <span class="token function">get_prompt_and_rejected</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sample<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token string">" Human: "</span> <span class="token operator">+</span> sample<span class="token punctuation">[</span><span class="token string">'prompt'</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token string">" Assistant: "</span> <span class="token operator">+</span> sample<span class="token punctuation">[</span>
            <span class="token string">'rejected'</span><span class="token punctuation">]</span>
</code></pre> 
<p><strong>6、InstructGPT基本流程：</strong><br>   InstructGPT是训练ChatGPT的核心思路，其融入了大量的对话数据，并按照如下三个步骤进行训练，如图所示：<br> <img src="https://images2.imgbox.com/6b/a0/TiXBi6uO_o.png" alt="在这里插入图片描述"><br> 在后面将会结合代码和相关知识介绍这三个步骤。</p> 
<p><strong>7、显存优化相关知识：</strong><br>   ZeRo-Stage和ZeRo-offload显存优化：<a href="https://zhuanlan.zhihu.com/p/619429610" rel="nofollow">https://zhuanlan.zhihu.com/p/619429610</a></p> 
<h3><a id="Step1_Supervised_FinetuningSFT_67"></a>二、Step1: Supervised Fine-tuning（SFT）</h3> 
<p>  第一阶段主要为监督训练。在InstructGPT中，通过设计Instruction Prompt，收集各式各样的数据集，并构建为对话模式，例如下面的就是一个prompt：</p> 
<blockquote> 
 <p>Human: How can I find out what types of butterflies are in my area?<br> Assistant: Which location are you in?<br> Human: I am in Oregon.<br> Assistant: There are about 175 species of butterflies in Oregon, of which 100 are long-distance migrants from southern California. Also, some of the common butterflies in Oregon<br> Human: Great. What are some common species then?<br> Assistant:</p> 
</blockquote> 
<p>对应人工打标的回复为：</p> 
<blockquote> 
 <p>About 150 species of butterflies live in Oregon, with about 100 species are moths, and about 20 species are common here year-round, and another 10 species are seen here year-round. I suggest you keep an eye out for skippers, gossamer wings, and red admirals.</p> 
</blockquote> 
<p>在这一过程，我们可以搜集50万到1000万不等的监督数据来构建此类监督数据。</p> 
<blockquote> 
 <p>一般的，这类对话式数据的来源有如下几种：</p> 
 <ul><li>搜集公开的benchmark，通过启发式方法将这些相互独立的样本构建成多轮对话模式；</li><li>互联网开源的一些对话数据集；</li><li>自行设计prompt，调用OpenAI gpt3.5-turbo，进行模型蒸馏。目前最近很多大厂或组织发布的ChatGPT类大模型中，在SFT阶段使用的数据大多采用从OpenAI中套取数据的方法来实现的。博主也自行整理了此类数据，详见：[<a href="https://github.com/wjn1996/HugNLP/blob/main/documents/instruction_prompting/generative_instruction_tuning.md">Click Me</a>]</li></ul> 
</blockquote> 
<h4><a id="21__84"></a>2.1 数据处理：</h4> 
<p>● 只需要获得训练集和验证集即可，也可以进行采样；<br> ● 接着，读取的数据中，获取prompt和chosen两个字段：</p> 
<pre><code class="prism language-python"><span class="token keyword">for</span> i<span class="token punctuation">,</span> tmp_data <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>current_dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># tokenize the text</span>
    chosen_sentence <span class="token operator">=</span> raw_dataset<span class="token punctuation">.</span>get_prompt_and_chosen<span class="token punctuation">(</span>
        tmp_data<span class="token punctuation">)</span>  <span class="token comment"># the accept response</span>
    <span class="token keyword">if</span> chosen_sentence <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token comment"># end_of_conversation_token表示每个对话的终止符，可以用“&lt;|endoftext|&gt;”表示</span>
        chosen_sentence <span class="token operator">+=</span> end_of_conversation_token
        chosen_token <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>chosen_sentence<span class="token punctuation">,</span>
                                 max_length<span class="token operator">=</span>max_seq_len<span class="token punctuation">,</span>
                                 padding<span class="token operator">=</span><span class="token string">"max_length"</span><span class="token punctuation">,</span>
                                 truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                 return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span>
        chosen_token<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span> <span class="token operator">=</span> chosen_token<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>
            <span class="token number">0</span><span class="token punctuation">)</span>
        chosen_token<span class="token punctuation">[</span><span class="token string">"attention_mask"</span><span class="token punctuation">]</span> <span class="token operator">=</span> chosen_token<span class="token punctuation">[</span>
            <span class="token string">"attention_mask"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
        chosen_dataset<span class="token punctuation">.</span>append<span class="token punctuation">(</span>chosen_token<span class="token punctuation">)</span>
</code></pre> 
<p>● 此时，一条样本可以表示为prompt+chosen，中间会插入一些用于对话的标记，例如“Human: ”、“Assistant: ”、“&lt;|endoftext|&gt;”等。</p> 
<h4><a id="22__108"></a>2.2 模型训练</h4> 
<p>  构建一个用于SFT训练的模型，模型可以指定为<code>AutoModelForCausalLM</code>类</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">create_hf_model</span><span class="token punctuation">(</span>model_class<span class="token punctuation">,</span>
                    model_name_or_path<span class="token punctuation">,</span>
                    tokenizer<span class="token punctuation">,</span>
                    ds_config<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                    rlhf_training<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    model_config <span class="token operator">=</span> AutoConfig<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name_or_path<span class="token punctuation">)</span>
    model_config<span class="token punctuation">.</span>dropout <span class="token operator">=</span> <span class="token number">0.0</span>
    <span class="token comment"># Note: dschf is defined in function scope to avoid global effects</span>
    <span class="token comment"># https://huggingface.co/docs/transformers/main_classes/deepspeed#nontrainer-deepspeed-integration</span>
    <span class="token keyword">if</span> ds_config <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">and</span> ds_config<span class="token punctuation">[</span><span class="token string">"zero_optimization"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"stage"</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">3</span><span class="token punctuation">:</span>
        dschf <span class="token operator">=</span> HfDeepSpeedConfig<span class="token punctuation">(</span>ds_config<span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        dschf <span class="token operator">=</span> <span class="token boolean">None</span>
    <span class="token keyword">if</span> rlhf_training<span class="token punctuation">:</span>
        <span class="token comment"># the weight loading is handled by create critic model</span>
        model <span class="token operator">=</span> model_class<span class="token punctuation">.</span>from_config<span class="token punctuation">(</span>model_config<span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        model <span class="token operator">=</span> model_class<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
            model_name_or_path<span class="token punctuation">,</span>
            from_tf<span class="token operator">=</span><span class="token builtin">bool</span><span class="token punctuation">(</span><span class="token string">".ckpt"</span> <span class="token keyword">in</span> model_name_or_path<span class="token punctuation">)</span><span class="token punctuation">,</span>
            config<span class="token operator">=</span>model_config<span class="token punctuation">)</span>
    model<span class="token punctuation">.</span>config<span class="token punctuation">.</span>end_token_id <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>eos_token_id
    model<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pad_token_id <span class="token operator">=</span> model<span class="token punctuation">.</span>config<span class="token punctuation">.</span>eos_token_id
    model<span class="token punctuation">.</span>resize_token_embeddings<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token number">8</span> <span class="token operator">*</span> math<span class="token punctuation">.</span>ceil<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">8.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># make the vocab size multiple of 8</span>

    <span class="token keyword">return</span> model
</code></pre> 
<p>  按照Causal Language Modeling进行训练，例如GPT、OPT、LLaMA、BLOOM等。</p> 
<h3><a id="Step2_Training_Pairwise_Reward_FunctionRW_142"></a>三、Step2: Training Pairwise Reward Function（RW）</h3> 
<p>  在此阶段，我们需要训练一个Reward函数，来为模型的输出进行评分。在InstructGPT原文中，采用的方法是对于同一个prompt，让大模型生成4～7个回复，然后让经过培训的标注人员为这些回复进行打分。因而可以得到若干个pair。而此过程需要借助人工标注来完成对齐。</p> 
<p>  在DeepSpeed-Chat中，我们直接获取已经打标好的开源的Reward训练数据。对于每一条数据，除了prompt以外，包括一对回复：</p> 
<ul><li>chosen：表示较好的回复，可以作为正样本；</li><li>rejected：表示较差的回复，可以作为负样本。</li></ul> 
<h4><a id="31__148"></a>3.1 数据处理：</h4> 
<p>● 读取训练集和验证集用来训练偏好模型；<br> ● 此时需要读取prompt、chosen和rejected三个字段数据，每一条数据是一个pairwise</p> 
<pre><code class="prism language-python"><span class="token keyword">for</span> i<span class="token punctuation">,</span> tmp_data <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>current_dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># tokenize the text</span>
    chosen_sentence <span class="token operator">=</span> raw_dataset<span class="token punctuation">.</span>get_prompt_and_chosen<span class="token punctuation">(</span>
        tmp_data<span class="token punctuation">)</span>  <span class="token comment"># the accept response</span>
    reject_sentence <span class="token operator">=</span> raw_dataset<span class="token punctuation">.</span>get_prompt_and_rejected<span class="token punctuation">(</span>
        tmp_data<span class="token punctuation">)</span>  <span class="token comment"># the accept response</span>
    <span class="token keyword">if</span> chosen_sentence <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">and</span> reject_sentence <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        chosen_sentence <span class="token operator">+=</span> end_of_conversation_token  <span class="token comment"># the accept response</span>
        reject_sentence <span class="token operator">+=</span> end_of_conversation_token
        chosen_token <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>chosen_sentence<span class="token punctuation">,</span>
                                 max_length<span class="token operator">=</span>max_seq_len<span class="token punctuation">,</span>
                                 padding<span class="token operator">=</span><span class="token string">"max_length"</span><span class="token punctuation">,</span>
                                 truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                 return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span>
        reject_token <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>reject_sentence<span class="token punctuation">,</span>
                                 max_length<span class="token operator">=</span>max_seq_len<span class="token punctuation">,</span>
                                 padding<span class="token operator">=</span><span class="token string">"max_length"</span><span class="token punctuation">,</span>
                                 truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                 return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span>
        chosen_token<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span> <span class="token operator">=</span> chosen_token<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span>
        chosen_token<span class="token punctuation">[</span><span class="token string">"attention_mask"</span><span class="token punctuation">]</span> <span class="token operator">=</span> chosen_token<span class="token punctuation">[</span><span class="token string">"attention_mask"</span><span class="token punctuation">]</span>
        chosen_dataset<span class="token punctuation">.</span>append<span class="token punctuation">(</span>chosen_token<span class="token punctuation">)</span>

        reject_token<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span> <span class="token operator">=</span> reject_token<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span>
        reject_token<span class="token punctuation">[</span><span class="token string">"attention_mask"</span><span class="token punctuation">]</span> <span class="token operator">=</span> reject_token<span class="token punctuation">[</span><span class="token string">"attention_mask"</span><span class="token punctuation">]</span>
        reject_dataset<span class="token punctuation">.</span>append<span class="token punctuation">(</span>reject_token<span class="token punctuation">)</span>
</code></pre> 
<h4><a id="32_DataCollator_181"></a>3.2 DataCollator</h4> 
<p>  给定一个batch，其包含batch_size个chosen examples和rejected examples，将其进行拆分，具体操作如下：</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">DataCollatorReward</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__call__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data<span class="token punctuation">)</span><span class="token punctuation">:</span>
        batch <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>
        <span class="token comment"># f[0]是chosen input ids，f[2]是rrejected input ids</span>
        <span class="token comment"># 该操作是指，先给定N个examples，转换为2*N个样本。</span>
        <span class="token comment"># 前N个为chosen input ids，后N个为rejected input ids</span>
        batch<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>f<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">for</span> f <span class="token keyword">in</span> data<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span>f<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token keyword">for</span> f <span class="token keyword">in</span> data<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        batch<span class="token punctuation">[</span><span class="token string">"attention_mask"</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>f<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token keyword">for</span> f <span class="token keyword">in</span> data<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span>f<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token keyword">for</span> f <span class="token keyword">in</span> data<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> batch
</code></pre> 
<h4><a id="33_Reward_195"></a>3.3 定义Reward模型：</h4> 
<p>  定义reward模型：选择<code>OPT-350M</code>模型作为backbone，并定义一个linear层用于分类。</p> 
<ul><li>OPT模型中，需要定义–num_padding_at_beginning=1，OPT默认首个字符为PAD token;</li><li>对于每个chosen或rejected tokens，取第一个padding token的前一个token的得分作为当前chosen或rejected input的得分</li></ul> 
<blockquote> 
 <p>For RW, the training objective is the pairwise ranking score, i.e., for the two query-answer pairs, RM is supposed to give a higher score to the better answer. There are multiple ways to achieve this. In our implementation, we use either the end token of the sequence or the first padding token as the aggregated score and compare them. Others may also use the average score for the entire answer as an alternative.</p> 
</blockquote> 
<p>Reward函数细节详见代码和注释：</p> 
<pre><code class="prism language-python"><span class="token comment"># Copyright (c) Microsoft Corporation.</span>
<span class="token comment"># SPDX-License-Identifier: Apache-2.0</span>
<span class="token comment"># DeepSpeed Team</span>
<span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token comment">## Note that the following code is modified from</span>
<span class="token comment">## https://github.com/CarperAI/trlx/blob/main/examples/summarize_rlhf/reward_model/reward_model.py</span>
<span class="token keyword">class</span> <span class="token class-name">RewardModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> base_model<span class="token punctuation">,</span> tokenizer<span class="token punctuation">,</span> num_padding_at_beginning<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>config <span class="token operator">=</span> base_model<span class="token punctuation">.</span>config
        self<span class="token punctuation">.</span>num_padding_at_beginning <span class="token operator">=</span> num_padding_at_beginning
        <span class="token keyword">if</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">,</span> <span class="token string">"word_embed_proj_dim"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># `OPT` models use word_embed_proj_dim as final output</span>
            <span class="token comment"># https://github.com/huggingface/transformers/blob/main/src/transformers/models/opt/modeling_opt.py#L497</span>
            self<span class="token punctuation">.</span>v_head <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>word_embed_proj_dim<span class="token punctuation">,</span>
                                    <span class="token number">1</span><span class="token punctuation">,</span>
                                    bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment"># `gpt-neo(x)` models use `hidden_size` attribute names instead of `n_embd``</span>
            self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>n_embd <span class="token operator">=</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>hidden_size <span class="token keyword">if</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>
                self<span class="token punctuation">.</span>config<span class="token punctuation">,</span> <span class="token string">"hidden_size"</span><span class="token punctuation">)</span> <span class="token keyword">else</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>n_embd
            self<span class="token punctuation">.</span>v_head <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>n_embd<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>rwtranrsformer <span class="token operator">=</span> base_model
        self<span class="token punctuation">.</span>PAD_ID <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>pad_token_id

    <span class="token keyword">def</span> <span class="token function">gradient_checkpointing_enable</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>rwtranrsformer<span class="token punctuation">.</span>gradient_checkpointing_enable<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">gradient_checkpointing_disable</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>rwtranrsformer<span class="token punctuation">.</span>gradient_checkpointing_disable<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>
                input_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                past_key_values<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                attention_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                position_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                head_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                inputs_embeds<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                use_cache<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        假设默认设置的batch_size为N，那么len(input_ids)=2*N
        其中前N个为chosen input ids（正样本），后N个为rejected input ids（负样本）
        """</span>
        loss <span class="token operator">=</span> <span class="token boolean">None</span>

        transformer_outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>rwtranrsformer<span class="token punctuation">(</span>
            input_ids<span class="token punctuation">,</span>
            past_key_values<span class="token operator">=</span>past_key_values<span class="token punctuation">,</span>
            attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">,</span>
            head_mask<span class="token operator">=</span>head_mask<span class="token punctuation">,</span>
            inputs_embeds<span class="token operator">=</span>inputs_embeds<span class="token punctuation">,</span>
            use_cache<span class="token operator">=</span>use_cache<span class="token punctuation">)</span>

        hidden_states <span class="token operator">=</span> transformer_outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        rewards <span class="token operator">=</span> self<span class="token punctuation">.</span>v_head<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        chosen_mean_scores <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        rejected_mean_scores <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

        <span class="token comment"># Split the inputs and rewards into two parts, chosen and rejected</span>
        <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>input_ids<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">2</span>
        bs <span class="token operator">=</span> input_ids<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">//</span> <span class="token number">2</span>
        seq_len <span class="token operator">=</span> input_ids<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>

        chosen_ids <span class="token operator">=</span> input_ids<span class="token punctuation">[</span><span class="token punctuation">:</span>bs<span class="token punctuation">]</span>  <span class="token comment"># N x seq x 1 前N个为正样本</span>
        rejected_ids <span class="token operator">=</span> input_ids<span class="token punctuation">[</span>bs<span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token comment"># 后N个为负样本</span>
        chosen_rewards <span class="token operator">=</span> rewards<span class="token punctuation">[</span><span class="token punctuation">:</span>bs<span class="token punctuation">]</span> <span class="token comment"># 获得前N个正样本的预测的reward</span>
        rejected_rewards <span class="token operator">=</span> rewards<span class="token punctuation">[</span>bs<span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token comment"># 获得后N个负样本的预测的reward</span>

        <span class="token comment"># Compute pairwise loss. Only backprop on the different tokens before padding</span>
        loss <span class="token operator">=</span> <span class="token number">0</span>
        <span class="token comment"># 遍历每个样本</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>bs<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># 获得一个chosen样本（正样本）</span>
            chosen_id <span class="token operator">=</span> chosen_ids<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token comment"># seq</span>
            <span class="token comment"># 获得一个rejected样本（负样本）</span>
            rejected_id <span class="token operator">=</span> rejected_ids<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
            <span class="token comment"># 当前正样本的得分</span>
            chosen_reward <span class="token operator">=</span> chosen_rewards<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
            <span class="token comment"># 当前负样本的得分</span>
            rejected_reward <span class="token operator">=</span> rejected_rewards<span class="token punctuation">[</span>i<span class="token punctuation">]</span>

            c_inds <span class="token operator">=</span> <span class="token punctuation">(</span>chosen_id <span class="token operator">==</span> self<span class="token punctuation">.</span>PAD_ID<span class="token punctuation">)</span><span class="token punctuation">.</span>nonzero<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 获得所有padding token的索引</span>
            c_ind <span class="token operator">=</span> c_inds<span class="token punctuation">[</span>self<span class="token punctuation">.</span>num_padding_at_beginning<span class="token punctuation">]</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span> <span class="token comment"># 如果是OPT，那么第0个一定是OPT模型默认在input最前面的padding token，不予考虑</span>
                c_inds
            <span class="token punctuation">)</span> <span class="token operator">&gt;</span> self<span class="token punctuation">.</span>num_padding_at_beginning <span class="token keyword">else</span> seq_len  <span class="token comment"># OPT model pads the first token, so we need to use the second padding token as the end of the sequence</span>
            check_divergence <span class="token operator">=</span> <span class="token punctuation">(</span>chosen_id <span class="token operator">!=</span> rejected_id<span class="token punctuation">)</span><span class="token punctuation">.</span>nonzero<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># [[0, 0], [1, 0], ..., [seq_len, 0]]</span>

            <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>check_divergence<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span> <span class="token comment"># 说明不存在相等的padding token</span>
                end_ind <span class="token operator">=</span> rejected_reward<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
                divergence_ind <span class="token operator">=</span> end_ind <span class="token operator">-</span> <span class="token number">1</span>
                r_ind <span class="token operator">=</span> c_ind
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                <span class="token comment"># Check if there is any padding otherwise take length of sequence</span>
                r_inds <span class="token operator">=</span> <span class="token punctuation">(</span>rejected_id <span class="token operator">==</span> self<span class="token punctuation">.</span>PAD_ID<span class="token punctuation">)</span><span class="token punctuation">.</span>nonzero<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 找出所有与padding token相等的token索引</span>
                r_ind <span class="token operator">=</span> r_inds<span class="token punctuation">[</span>self<span class="token punctuation">.</span>num_padding_at_beginning<span class="token punctuation">]</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>r_inds<span class="token punctuation">)</span> <span class="token operator">&gt;</span> self<span class="token punctuation">.</span>num_padding_at_beginning <span class="token keyword">else</span> seq_len
                end_ind <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span>c_ind<span class="token punctuation">,</span> r_ind<span class="token punctuation">)</span>
                divergence_ind <span class="token operator">=</span> check_divergence<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
            <span class="token keyword">assert</span> divergence_ind <span class="token operator">&gt;</span> <span class="token number">0</span>
            c_truncated_reward <span class="token operator">=</span> chosen_reward<span class="token punctuation">[</span>divergence_ind<span class="token punctuation">:</span>end_ind<span class="token punctuation">]</span>
            r_truncated_reward <span class="token operator">=</span> rejected_reward<span class="token punctuation">[</span>divergence_ind<span class="token punctuation">:</span>end_ind<span class="token punctuation">]</span>
            chosen_mean_scores<span class="token punctuation">.</span>append<span class="token punctuation">(</span>
                chosen_reward<span class="token punctuation">[</span>c_ind <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment">#use the end score for reference</span>
            rejected_mean_scores<span class="token punctuation">.</span>append<span class="token punctuation">(</span>rejected_reward<span class="token punctuation">[</span>r_ind <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
            <span class="token comment"># pair-wise loss</span>
            loss <span class="token operator">+=</span> <span class="token operator">-</span>torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>
                torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>c_truncated_reward <span class="token operator">-</span> r_truncated_reward<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>

        loss <span class="token operator">=</span> loss <span class="token operator">/</span> bs
        chosen_mean_scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>chosen_mean_scores<span class="token punctuation">)</span>
        rejected_mean_scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>rejected_mean_scores<span class="token punctuation">)</span>
        <span class="token keyword">return</span> <span class="token punctuation">{<!-- --></span>
            <span class="token string">"loss"</span><span class="token punctuation">:</span> loss<span class="token punctuation">,</span>
            <span class="token string">"chosen_mean_scores"</span><span class="token punctuation">:</span> chosen_mean_scores<span class="token punctuation">,</span>
            <span class="token string">"rejected_mean_scores"</span><span class="token punctuation">:</span> rejected_mean_scores<span class="token punctuation">,</span>
        <span class="token punctuation">}</span>

    <span class="token keyword">def</span> <span class="token function">forward_value</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>
                      input_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                      attention_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                      past_key_values<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                      position_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                      head_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                      inputs_embeds<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                      return_value_only<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
                      prompt_length<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
                      use_cache<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>

        transformer_outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>rwtranrsformer<span class="token punctuation">(</span>
            input_ids<span class="token punctuation">,</span>
            past_key_values<span class="token operator">=</span>past_key_values<span class="token punctuation">,</span>
            attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">,</span>
            head_mask<span class="token operator">=</span>head_mask<span class="token punctuation">,</span>
            inputs_embeds<span class="token operator">=</span>inputs_embeds<span class="token punctuation">,</span>
            use_cache<span class="token operator">=</span>use_cache<span class="token punctuation">)</span>
        hidden_states <span class="token operator">=</span> transformer_outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        values <span class="token operator">=</span> self<span class="token punctuation">.</span>v_head<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> return_value_only<span class="token punctuation">:</span>
            <span class="token keyword">return</span> values
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment"># [0 0 0 0 prompt, answer, 0 0 0 0 ] for step 3, we have padding at the beginning</span>
            <span class="token comment"># [prompt, answer, 0, 0, 0, 0] this is normal</span>
            <span class="token keyword">assert</span> prompt_length <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">"prompt_length must be greater than 1 to help select the end score"</span>
            bs <span class="token operator">=</span> values<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
            seq_len <span class="token operator">=</span> input_ids<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
            chosen_end_scores <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment"># we use this name for consistency with the original forward function</span>
            <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>bs<span class="token punctuation">)</span><span class="token punctuation">:</span>
                input_id <span class="token operator">=</span> input_ids<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
                value <span class="token operator">=</span> values<span class="token punctuation">[</span>i<span class="token punctuation">]</span>

                c_inds <span class="token operator">=</span> <span class="token punctuation">(</span>input_id<span class="token punctuation">[</span>prompt_length<span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">==</span> self<span class="token punctuation">.</span>PAD_ID<span class="token punctuation">)</span><span class="token punctuation">.</span>nonzero<span class="token punctuation">(</span><span class="token punctuation">)</span>
                <span class="token comment"># here we only use the answer part of the sequence so we do not need to care about the padding at the beginning</span>
                c_ind <span class="token operator">=</span> c_inds<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> prompt_length <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>
                    c_inds<span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">0</span> <span class="token keyword">else</span> seq_len
                chosen_end_scores<span class="token punctuation">.</span>append<span class="token punctuation">(</span>value<span class="token punctuation">[</span>c_ind <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
            <span class="token keyword">return</span> <span class="token punctuation">{<!-- --></span>
                <span class="token string">"values"</span><span class="token punctuation">:</span> values<span class="token punctuation">,</span>
                <span class="token string">"chosen_end_scores"</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>chosen_end_scores<span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token punctuation">}</span>
</code></pre> 
<p>上面的Reward函数可以认为是一个分类器，需要基于Causal LM（例如OPT）作为Backbone，获得完整的Reward模型：</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">create_critic_model</span><span class="token punctuation">(</span>model_name_or_path<span class="token punctuation">,</span>
                        tokenizer<span class="token punctuation">,</span>
                        ds_config<span class="token punctuation">,</span>
                        num_padding_at_beginning<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
                        rlhf_training<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># OPT model family always put a padding token at the beginning of the sequence,</span>
    <span class="token comment"># we did not see this in other models but not sure if it is a general rule</span>
    critic_model <span class="token operator">=</span> create_hf_model<span class="token punctuation">(</span>AutoModel<span class="token punctuation">,</span> model_name_or_path<span class="token punctuation">,</span> tokenizer<span class="token punctuation">,</span>
                                   ds_config<span class="token punctuation">,</span> rlhf_training<span class="token punctuation">)</span>
    critic_model <span class="token operator">=</span> RewardModel<span class="token punctuation">(</span>
        critic_model<span class="token punctuation">,</span>
        tokenizer<span class="token punctuation">,</span>
        num_padding_at_beginning<span class="token operator">=</span>num_padding_at_beginning<span class="token punctuation">)</span>

    <span class="token keyword">if</span> rlhf_training<span class="token punctuation">:</span>
        <span class="token comment"># critic model needs to load the weight here</span>
        model_ckpt_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>model_name_or_path<span class="token punctuation">,</span> <span class="token string">'pytorch_model.bin'</span><span class="token punctuation">)</span>
        <span class="token keyword">assert</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>
            model_ckpt_path
        <span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string-interpolation"><span class="token string">f"Cannot find model checkpoint at </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>model_ckpt_path<span class="token punctuation">}</span></span><span class="token string">"</span></span>
        critic_model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>
            torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>model_ckpt_path<span class="token punctuation">,</span> map_location<span class="token operator">=</span><span class="token string">'cpu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> critic_model
</code></pre> 
<h3><a id="Step3RLHF_TuningPPO_393"></a>四、Step3：RLHF Tuning——PPO算法</h3> 
<p>  PPO算法是一种Actor-Critic强化学习架构。相关解读如下所示：<br> <a href="https://zhuanlan.zhihu.com/p/110998399" rel="nofollow">https://zhuanlan.zhihu.com/p/110998399</a><br> <a href="https://www.zhihu.com/question/56692640/answer/152930557" rel="nofollow">https://www.zhihu.com/question/56692640/answer/152930557</a></p> 
<h4><a id="41__398"></a>4.1 数据处理</h4> 
<p>在第三阶段，可以选择监督训练数据和无监督数据。<br> ● 监督数据：此时只有prompt，没有chosen和rejected input。</p> 
<pre><code class="prism language-python"><span class="token keyword">for</span> i<span class="token punctuation">,</span> tmp_data <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>current_dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># tokenize the text</span>
    prompt <span class="token operator">=</span> raw_dataset<span class="token punctuation">.</span>get_prompt<span class="token punctuation">(</span>tmp_data<span class="token punctuation">)</span>
    <span class="token keyword">if</span> prompt <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        prompt_token <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>prompt<span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span>
        prompt_token<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span> <span class="token operator">=</span> prompt_token<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span>
        prompt_token<span class="token punctuation">[</span><span class="token string">"attention_mask"</span><span class="token punctuation">]</span> <span class="token operator">=</span> prompt_token<span class="token punctuation">[</span><span class="token string">"attention_mask"</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> key_word <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">,</span> <span class="token string">"attention_mask"</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
            length <span class="token operator">=</span> prompt_token<span class="token punctuation">[</span>key_word<span class="token punctuation">]</span><span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
            <span class="token keyword">if</span> length <span class="token operator">&gt;</span> max_seq_len<span class="token punctuation">:</span>
                <span class="token comment"># 先将正常的token序列的顺序倒序排列，（会在datacollator中再次倒序恢复原始排列）</span>
                y <span class="token operator">=</span> prompt_token<span class="token punctuation">[</span>key_word<span class="token punctuation">]</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">[</span>length <span class="token operator">-</span> <span class="token punctuation">(</span>max_seq_len <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>flip<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                <span class="token comment"># 先将正常的token序列的顺序倒序排列，（会在datacollator中再次倒序恢复原始排列）</span>
                y <span class="token operator">=</span> prompt_token<span class="token punctuation">[</span>key_word<span class="token punctuation">]</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>flip<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
            prompt_token<span class="token punctuation">[</span>key_word<span class="token punctuation">]</span> <span class="token operator">=</span> y
        prompt_dataset<span class="token punctuation">.</span>append<span class="token punctuation">(</span>prompt_token<span class="token punctuation">)</span>  
</code></pre> 
<p>● 无监督数据：只有文本，并进行group：</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">get_unsupervised_data</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> tokenizer<span class="token punctuation">)</span><span class="token punctuation">:</span>
    unsupervised_raw_datasets <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span>
        args<span class="token punctuation">.</span>unsupervised_dataset_name<span class="token punctuation">,</span> args<span class="token punctuation">.</span>unsupervised_dataset_config_name<span class="token punctuation">)</span>
    column_names <span class="token operator">=</span> unsupervised_raw_datasets<span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>column_names
    text_column_name <span class="token operator">=</span> <span class="token string">"text"</span> <span class="token keyword">if</span> <span class="token string">"text"</span> <span class="token keyword">in</span> column_names <span class="token keyword">else</span> column_names<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">tokenize_function</span><span class="token punctuation">(</span>examples<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> tokenizer<span class="token punctuation">(</span>examples<span class="token punctuation">[</span>text_column_name<span class="token punctuation">]</span><span class="token punctuation">)</span>

    tokenized_datasets <span class="token operator">=</span> unsupervised_raw_datasets<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>
        tokenize_function<span class="token punctuation">,</span>
        batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
        num_proc<span class="token operator">=</span>args<span class="token punctuation">.</span>preprocessing_num_workers<span class="token punctuation">,</span>
        remove_columns<span class="token operator">=</span>column_names<span class="token punctuation">,</span>
        load_from_cache_file<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
        desc<span class="token operator">=</span><span class="token string">"Running tokenizer on dataset"</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span>

    block_size <span class="token operator">=</span> args<span class="token punctuation">.</span>max_prompt_seq_len <span class="token operator">+</span> args<span class="token punctuation">.</span>max_answer_seq_len

    <span class="token keyword">def</span> <span class="token function">group_texts</span><span class="token punctuation">(</span>examples<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Concatenate all texts.</span>
        concatenated_examples <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
            k<span class="token punctuation">:</span> <span class="token builtin">list</span><span class="token punctuation">(</span>chain<span class="token punctuation">(</span><span class="token operator">*</span>examples<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token keyword">for</span> k <span class="token keyword">in</span> examples<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token punctuation">}</span>
        total_length <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>concatenated_examples<span class="token punctuation">[</span><span class="token builtin">list</span><span class="token punctuation">(</span>examples<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token comment"># We drop the small remainder, we could add padding if the model supported it instead of this drop, you can</span>
        <span class="token comment"># customize this part to your needs.</span>
        <span class="token keyword">if</span> total_length <span class="token operator">&gt;=</span> block_size<span class="token punctuation">:</span>
            total_length <span class="token operator">=</span> <span class="token punctuation">(</span>total_length <span class="token operator">//</span> block_size<span class="token punctuation">)</span> <span class="token operator">*</span> block_size
        <span class="token comment"># Split by chunks of max_len.</span>
        result <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
            k<span class="token punctuation">:</span>
            <span class="token punctuation">[</span>t<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i <span class="token operator">+</span> block_size<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> total_length<span class="token punctuation">,</span> block_size<span class="token punctuation">)</span><span class="token punctuation">]</span>
            <span class="token keyword">for</span> k<span class="token punctuation">,</span> t <span class="token keyword">in</span> concatenated_examples<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token punctuation">}</span>
        result<span class="token punctuation">[</span><span class="token string">"labels"</span><span class="token punctuation">]</span> <span class="token operator">=</span> result<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> result

    lm_datasets <span class="token operator">=</span> tokenized_datasets<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>
        group_texts<span class="token punctuation">,</span>
        batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
        num_proc<span class="token operator">=</span>args<span class="token punctuation">.</span>preprocessing_num_workers<span class="token punctuation">,</span>
        load_from_cache_file<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
        desc<span class="token operator">=</span><span class="token string-interpolation"><span class="token string">f"Grouping texts in chunks of </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>block_size<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span>

    train_dataset <span class="token operator">=</span> lm_datasets<span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">]</span>

    <span class="token keyword">return</span> train_dataset
</code></pre> 
<h4><a id="42_DataCollator_475"></a>4.2 DataCollator</h4> 
<p>针对监督数据，需要进行处理：</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">DataCollatorRLHF</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> max_token_len<span class="token punctuation">,</span> inference_tp_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>max_token_len <span class="token operator">=</span> max_token_len
        self<span class="token punctuation">.</span>inference_tp_size <span class="token operator">=</span> inference_tp_size

    <span class="token keyword">def</span> <span class="token function">__call__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data<span class="token punctuation">)</span><span class="token punctuation">:</span>
        batch <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>
        pad_token_id <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>

        prompt <span class="token operator">=</span> pad_sequence<span class="token punctuation">(</span><span class="token punctuation">[</span>f<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">for</span> f <span class="token keyword">in</span> data<span class="token punctuation">]</span><span class="token punctuation">,</span>
                              padding_value<span class="token operator">=</span>pad_token_id<span class="token punctuation">,</span>
                              batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        prompt_mask <span class="token operator">=</span> pad_sequence<span class="token punctuation">(</span><span class="token punctuation">[</span>f<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token keyword">for</span> f <span class="token keyword">in</span> data<span class="token punctuation">]</span><span class="token punctuation">,</span>
                                   padding_value<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
                                   batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

        <span class="token comment">### make sure the final ouput is a seqence of 2**?</span>
        length <span class="token operator">=</span> prompt<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
        pad_length <span class="token operator">=</span> self<span class="token punctuation">.</span>max_token_len <span class="token operator">-</span> length
        <span class="token keyword">if</span> pad_length <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
            batch<span class="token punctuation">[</span><span class="token string">"prompt"</span><span class="token punctuation">]</span> <span class="token operator">=</span> F<span class="token punctuation">.</span>pad<span class="token punctuation">(</span>prompt<span class="token punctuation">,</span>
                                    pad<span class="token operator">=</span><span class="token punctuation">(</span>pad_length<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                    mode<span class="token operator">=</span><span class="token string">'constant'</span><span class="token punctuation">,</span>
                                    value<span class="token operator">=</span>pad_token_id<span class="token punctuation">)</span>
            batch<span class="token punctuation">[</span><span class="token string">"prompt_att_mask"</span><span class="token punctuation">]</span> <span class="token operator">=</span> F<span class="token punctuation">.</span>pad<span class="token punctuation">(</span>prompt_mask<span class="token punctuation">,</span>
                                             pad<span class="token operator">=</span><span class="token punctuation">(</span>pad_length<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                             mode<span class="token operator">=</span><span class="token string">'constant'</span><span class="token punctuation">,</span>
                                             value<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            batch<span class="token punctuation">[</span><span class="token string">"prompt"</span><span class="token punctuation">]</span> <span class="token operator">=</span> prompt
            batch<span class="token punctuation">[</span><span class="token string">"prompt_att_mask"</span><span class="token punctuation">]</span> <span class="token operator">=</span> prompt_mask
        batch<span class="token punctuation">[</span><span class="token string">"prompt"</span><span class="token punctuation">]</span> <span class="token operator">=</span> batch<span class="token punctuation">[</span><span class="token string">"prompt"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>flip<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        batch<span class="token punctuation">[</span><span class="token string">"prompt_att_mask"</span><span class="token punctuation">]</span> <span class="token operator">=</span> batch<span class="token punctuation">[</span><span class="token string">"prompt_att_mask"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>flip<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> batch
</code></pre> 
<h4><a id="43__515"></a>4.3 模型</h4> 
<p>  在RLHF阶段，需要加载前两个阶段训练得到的SFT模型和reward，用于初始化RLHF引擎。下面展示具体细节。</p> 
<h5><a id="431_DeepSpeedRLHFEngine_517"></a>4.3.1 初始化DeepSpeedRLHFEngine：</h5> 
<p>  获得一个<code>DeepSpeedRLHFEngine</code>对象，用于初始化一系列模型，包括Actor、Critic、Reference和Reward。</p> 
<pre><code class="prism language-python">rlhf_engine <span class="token operator">=</span> DeepSpeedRLHFEngine<span class="token punctuation">(</span>
        actor_model_name_or_path<span class="token operator">=</span>args<span class="token punctuation">.</span>actor_model_name_or_path<span class="token punctuation">,</span>
        critic_model_name_or_path<span class="token operator">=</span>args<span class="token punctuation">.</span>critic_model_name_or_path<span class="token punctuation">,</span>
        tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span>
        num_total_iters<span class="token operator">=</span>num_total_iters<span class="token punctuation">,</span>
        args<span class="token operator">=</span>args<span class="token punctuation">)</span>
</code></pre> 
<p><strong>（1）初始化Actor、Reference模型：</strong><br> ● 因为Actor模型是Stage1训练的SFT，其参数量很大，因此需要配置ZeRO-Stage和ZeRO-Offload进行显存优化：</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">get_train_ds_config</span><span class="token punctuation">(</span>offload<span class="token punctuation">,</span>
                        stage<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
                        enable_hybrid_engine<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
                        inference_tp_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
                        release_inference_cache<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
                        pin_parameters<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                        tp_gather_partition_size<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">:</span>

    device <span class="token operator">=</span> <span class="token string">"cpu"</span> <span class="token keyword">if</span> offload <span class="token keyword">else</span> <span class="token string">"none"</span>
    zero_opt_dict <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string">"stage"</span><span class="token punctuation">:</span> stage<span class="token punctuation">,</span>
        <span class="token string">"offload_param"</span><span class="token punctuation">:</span> <span class="token punctuation">{<!-- --></span>
            <span class="token string">"device"</span><span class="token punctuation">:</span> device
        <span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token string">"offload_optimizer"</span><span class="token punctuation">:</span> <span class="token punctuation">{<!-- --></span>
            <span class="token string">"device"</span><span class="token punctuation">:</span> device
        <span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token string">"stage3_param_persistence_threshold"</span><span class="token punctuation">:</span> <span class="token number">1e4</span><span class="token punctuation">,</span>
        <span class="token string">"stage3_max_live_parameters"</span><span class="token punctuation">:</span> <span class="token number">3e7</span><span class="token punctuation">,</span>
        <span class="token string">"stage3_prefetch_bucket_size"</span><span class="token punctuation">:</span> <span class="token number">3e7</span><span class="token punctuation">,</span>
        <span class="token string">"memory_efficient_linear"</span><span class="token punctuation">:</span> <span class="token boolean">False</span>
    <span class="token punctuation">}</span>
    <span class="token keyword">return</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string">"train_batch_size"</span><span class="token punctuation">:</span> GLOBAL_BATCH_SIZE<span class="token punctuation">,</span>
        <span class="token string">"train_micro_batch_size_per_gpu"</span><span class="token punctuation">:</span> MICRO_BATCH_SIZE<span class="token punctuation">,</span>
        <span class="token string">"steps_per_print"</span><span class="token punctuation">:</span> <span class="token number">10</span><span class="token punctuation">,</span>
        <span class="token string">"zero_optimization"</span><span class="token punctuation">:</span> zero_opt_dict<span class="token punctuation">,</span>
        <span class="token string">"fp16"</span><span class="token punctuation">:</span> <span class="token punctuation">{<!-- --></span>
            <span class="token string">"enabled"</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">,</span>
            <span class="token string">"loss_scale_window"</span><span class="token punctuation">:</span> <span class="token number">100</span>
        <span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token string">"gradient_clipping"</span><span class="token punctuation">:</span> <span class="token number">1.0</span><span class="token punctuation">,</span>
        <span class="token string">"prescale_gradients"</span><span class="token punctuation">:</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
        <span class="token string">"wall_clock_breakdown"</span><span class="token punctuation">:</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
        <span class="token string">"hybrid_engine"</span><span class="token punctuation">:</span> <span class="token punctuation">{<!-- --></span>
            <span class="token string">"enabled"</span><span class="token punctuation">:</span> enable_hybrid_engine<span class="token punctuation">,</span>
            <span class="token string">"inference_tp_size"</span><span class="token punctuation">:</span> inference_tp_size<span class="token punctuation">,</span>
            <span class="token string">"release_inference_cache"</span><span class="token punctuation">:</span> release_inference_cache<span class="token punctuation">,</span>
            <span class="token string">"pin_parameters"</span><span class="token punctuation">:</span> pin_parameters<span class="token punctuation">,</span>
            <span class="token string">"tp_gather_partition_size"</span><span class="token punctuation">:</span> tp_gather_partition_size<span class="token punctuation">,</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
</code></pre> 
<blockquote> 
 <p>ZeRo-stage一共有三个：<br> <img src="https://images2.imgbox.com/ae/7f/NgmEkpUz_o.png" alt="在这里插入图片描述">如果设置为3，则为最优状态，包括参数、梯度和优化状态全部进行并行化处理。</p> 
</blockquote> 
<p>● 初始化Actor模型，加载预训练SFT的参数（以及LoRA）<br> ● deepspeed engine封装：</p> 
<pre><code class="prism language-python">actor_engine<span class="token punctuation">,</span> <span class="token operator">*</span>_ <span class="token operator">=</span> deepspeed<span class="token punctuation">.</span>initialize<span class="token punctuation">(</span>model<span class="token operator">=</span>actor_model<span class="token punctuation">,</span>
                                            optimizer<span class="token operator">=</span>optim<span class="token punctuation">,</span>
                                            lr_scheduler<span class="token operator">=</span>lr_scheduler<span class="token punctuation">,</span>
                                            config<span class="token operator">=</span>ds_config<span class="token punctuation">)</span>
</code></pre> 
<p><strong>（2）初始化Critic、Reward模型</strong><br> ● 配置ZeRO-Stage和ZeRO-offload</p> 
<pre><code class="prism language-python">ds_config <span class="token operator">=</span> get_train_ds_config<span class="token punctuation">(</span>offload<span class="token operator">=</span>self<span class="token punctuation">.</span>args<span class="token punctuation">.</span>offload<span class="token punctuation">,</span> stage<span class="token operator">=</span>self<span class="token punctuation">.</span>args<span class="token punctuation">.</span>critic_zero_stage<span class="token punctuation">)</span>
</code></pre> 
<p>● 用预训练的RW模型，初始化Critic参数；<br> ● 封装DeepSpeed Engine。</p> 
<h5><a id="432_PPO_592"></a>4.3.2 PPO训练+无监督预训练</h5> 
<p>  在InstructGPT中，第三阶段的训练通常需要结合无监督的预训练目标和PPO训练目标联合训练，以确保模型在AC框架下强化学习训练过程中不会忘记原始的预训练任务，因此无监督的预训练可以作为一种正则化。</p> 
<p>  因此定义<code>DeepSpeedPPOTrainerUnsupervised</code>类用于完成PPO+无监督预训练。训练关键细节如下：</p> 
<p><strong>（1）Generate Experience</strong><br>   在Actor-Critic框架下，首先需要优化Critic，用于评价在状态 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         s 
        
       
      
        s 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">s</span></span></span></span></span>下的得分。因此需要采样一系列的状态-动作序列（即Experience），在GPT模型中，状态 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         s 
        
       
      
        s 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">s</span></span></span></span></span>可以视为已经生成的文本。<br>   给定一个prompt（例如in-context examples和query），生成相应的文本，并进行打分。细节如下代码和注释：</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">generate_experience</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> prompts<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 给定prompt，生成response text</span>
    self<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 调用model.generate()生成序列，由actor模型生成。</span>
    seq <span class="token operator">=</span> self<span class="token punctuation">.</span>_generate_sequence<span class="token punctuation">(</span>prompts<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>

    pad_token_id <span class="token operator">=</span> self<span class="token punctuation">.</span>tokenizer<span class="token punctuation">.</span>pad_token_id
    attention_mask <span class="token operator">=</span> seq<span class="token punctuation">.</span>not_equal<span class="token punctuation">(</span>pad_token_id<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 将生成的序列喂入actor模型中，得到输出的概率分布</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>actor_model<span class="token punctuation">(</span>seq<span class="token punctuation">,</span> attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">)</span>
        output_ref <span class="token operator">=</span> self<span class="token punctuation">.</span>ref_model<span class="token punctuation">(</span>seq<span class="token punctuation">,</span> attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">)</span>
        <span class="token comment"># 将生成的序列喂入critic和reward模型中，获得奖励和状态价值</span>
        reward_score <span class="token operator">=</span> self<span class="token punctuation">.</span>reward_model<span class="token punctuation">.</span>forward_value<span class="token punctuation">(</span>
            seq<span class="token punctuation">,</span> attention_mask<span class="token punctuation">,</span>
            prompt_length<span class="token operator">=</span>self<span class="token punctuation">.</span>prompt_length<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'chosen_end_scores'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span>
            <span class="token punctuation">)</span>
        values <span class="token operator">=</span> self<span class="token punctuation">.</span>critic_model<span class="token punctuation">.</span>forward_value<span class="token punctuation">(</span>
            seq<span class="token punctuation">,</span> attention_mask<span class="token punctuation">,</span> return_value_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>

    logits <span class="token operator">=</span> output<span class="token punctuation">.</span>logits
    logits_ref <span class="token operator">=</span> output_ref<span class="token punctuation">.</span>logits
	<span class="token comment"># 获得生成的文本seq、以及对应的概率、状态价值和奖励等信息</span>
    <span class="token keyword">return</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string">'prompts'</span><span class="token punctuation">:</span> prompts<span class="token punctuation">,</span>
        <span class="token string">'logprobs'</span><span class="token punctuation">:</span> gather_log_probs<span class="token punctuation">(</span>logits<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> seq<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token string">'ref_logprobs'</span><span class="token punctuation">:</span> gather_log_probs<span class="token punctuation">(</span>logits_ref<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> seq<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token string">'value'</span><span class="token punctuation">:</span> values<span class="token punctuation">,</span>
        <span class="token string">'rewards'</span><span class="token punctuation">:</span> reward_score<span class="token punctuation">,</span>
        <span class="token string">'input_ids'</span><span class="token punctuation">:</span> seq<span class="token punctuation">,</span>
        <span class="token string">"attention_mask"</span><span class="token punctuation">:</span> attention_mask
    <span class="token punctuation">}</span>
</code></pre> 
<p><strong>（2）保存Experience到经验池</strong><br>   经验池包含一系列根据prompt生成的文本和一系列奖励信息，其可以用于训练Critic模型。</p> 
<p><strong>（3）RLHF训练</strong><br> Actor-Critic的基本流程为：<br> 采样 → 更新Critic参数 → 根据Critic计算Advantage Function → 更新Actor参数</p> 
<blockquote> 
 <p>Advantage计算：<br> <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           Q 
          
         
           ^ 
          
         
        
          ( 
         
         
         
           s 
          
         
           t 
          
         
        
          , 
         
         
         
           a 
          
         
           t 
          
         
        
          ) 
         
        
          − 
         
         
         
           V 
          
         
           ^ 
          
         
        
          ( 
         
         
         
           s 
          
         
           t 
          
         
        
          ) 
         
        
          = 
         
         
         
           A 
          
         
           ^ 
          
         
        
          ( 
         
         
         
           s 
          
         
           t 
          
         
        
          , 
         
         
         
           a 
          
         
           t 
          
         
        
          ) 
         
        
       
         \hat{Q}(s_t, a_t) - \hat{V}(s_t) = \hat{A}(s_t, a_t) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.1968em; vertical-align: -0.25em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathnormal">Q</span></span><span class="" style="top: -3.2523em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.1667em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1944em;"><span class=""></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1.1968em; vertical-align: -0.25em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathnormal" style="margin-right: 0.2222em;">V</span></span><span class="" style="top: -3.2523em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.25em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.1968em; vertical-align: -0.25em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathnormal">A</span></span><span class="" style="top: -3.2523em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.1111em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span><br> <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          Q 
         
        
          ( 
         
         
         
           s 
          
         
           t 
          
         
        
          , 
         
         
         
           a 
          
         
           t 
          
         
        
          ) 
         
        
          = 
         
        
          r 
         
        
          ( 
         
         
         
           s 
          
         
           t 
          
         
        
          , 
         
         
         
           a 
          
         
           t 
          
         
        
          ) 
         
        
          + 
         
         
         
           ∑ 
          
          
          
            s 
           
           
           
             t 
            
           
             + 
            
           
             1 
            
           
          
         
        
          P 
         
        
          ( 
         
         
         
           s 
          
          
          
            t 
           
          
            + 
           
          
            1 
           
          
         
        
          ∣ 
         
         
         
           s 
          
         
           t 
          
         
        
          , 
         
         
         
           a 
          
         
           t 
          
         
        
          ) 
         
        
          [ 
         
        
          V 
         
        
          ( 
         
         
         
           s 
          
          
          
            t 
           
          
            + 
           
          
            1 
           
          
         
        
          ) 
         
        
          ] 
         
        
       
         Q(s_t, a_t) = r(s_t, a_t) + \sum_{s_{t+1}}P(s_{t+1}|s_t, a_t)[V(s_{t+1})] 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">r</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1.1915em; vertical-align: -0.4415em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position: relative; top: 0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.0017em;"><span class="" style="top: -2.4003em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3173em;"><span class="" style="top: -2.357em; margin-left: 0em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2025em;"><span class=""></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.4415em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.1389em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em;"><span class=""></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right: 0.2222em;">V</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em;"><span class=""></span></span></span></span></span></span><span class="mclose">)]</span></span></span></span></span><br> 我们可以用一个神经网络Critic模型来表示 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           ∑ 
          
          
          
            s 
           
           
           
             t 
            
           
             + 
            
           
             1 
            
           
          
         
        
          P 
         
        
          ( 
         
         
         
           s 
          
          
          
            t 
           
          
            + 
           
          
            1 
           
          
         
        
          ∣ 
         
         
         
           s 
          
         
           t 
          
         
        
          , 
         
         
         
           a 
          
         
           t 
          
         
        
          ) 
         
        
          [ 
         
        
          V 
         
        
          ( 
         
         
         
           s 
          
          
          
            t 
           
          
            + 
           
          
            1 
           
          
         
        
          ) 
         
        
          ] 
         
        
       
         \sum_{s_{t+1}}P(s_{t+1}|s_t, a_t)[V(s_{t+1})] 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.1915em; vertical-align: -0.4415em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position: relative; top: 0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.0017em;"><span class="" style="top: -2.4003em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3173em;"><span class="" style="top: -2.357em; margin-left: 0em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2025em;"><span class=""></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.4415em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.1389em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em;"><span class=""></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right: 0.2222em;">V</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em;"><span class=""></span></span></span></span></span></span><span class="mclose">)]</span></span></span></span></span>，即给定一个状态 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          s 
         
        
       
         s 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">s</span></span></span></span></span>，可以根据Critic模型预测一个得分 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           Q 
          
         
           ^ 
          
         
        
       
         \hat{Q} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.1412em; vertical-align: -0.1944em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathnormal">Q</span></span><span class="" style="top: -3.2523em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.1667em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1944em;"><span class=""></span></span></span></span></span></span></span></span></span>。而 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           V 
          
         
           ^ 
          
         
        
       
         \hat{V} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.9468em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9468em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathnormal" style="margin-right: 0.2222em;">V</span></span><span class="" style="top: -3.2523em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.25em;"><span class="mord">^</span></span></span></span></span></span></span></span></span></span></span>是已知的，所以可以构造监督信号来训练Critic模型。</p> 
</blockquote> 
<p>  因此RLHF训练为关键部分，也是InstructGPT第三步的优化过程。细节详见代码和注释：</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">train_rlhf</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># train the rlhf mode here</span>
    <span class="token comment">### process the old outputs</span>
    prompts <span class="token operator">=</span> inputs<span class="token punctuation">[</span><span class="token string">'prompts'</span><span class="token punctuation">]</span> <span class="token comment"># 输入的prompt（例如in-context exemplar + query）</span>
    log_probs <span class="token operator">=</span> inputs<span class="token punctuation">[</span><span class="token string">'logprobs'</span><span class="token punctuation">]</span> <span class="token comment"># 根据prompt，actor模型生成的文本的概率</span>
    ref_log_probs <span class="token operator">=</span> inputs<span class="token punctuation">[</span><span class="token string">'ref_logprobs'</span><span class="token punctuation">]</span> <span class="token comment"># 根据prompt，reference生成模型的文本的概率</span>
    reward_score <span class="token operator">=</span> inputs<span class="token punctuation">[</span><span class="token string">'rewards'</span><span class="token punctuation">]</span> <span class="token comment"># 根据prompt生成的seq，reward模型得到的奖励</span>
    values <span class="token operator">=</span> inputs<span class="token punctuation">[</span><span class="token string">'value'</span><span class="token punctuation">]</span> <span class="token comment"># 根据prompt生成的seq，critic模型得到的状态价值函数值</span>
    attention_mask <span class="token operator">=</span> inputs<span class="token punctuation">[</span><span class="token string">'attention_mask'</span><span class="token punctuation">]</span> <span class="token comment"># actor生成的文本的attention mask</span>
    seq <span class="token operator">=</span> inputs<span class="token punctuation">[</span><span class="token string">'input_ids'</span><span class="token punctuation">]</span> <span class="token comment"># 根据prompt，actor生成的文本</span>

    start <span class="token operator">=</span> prompts<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">1</span> <span class="token comment"># 记prompt文本最后一个位置</span>
    action_mask <span class="token operator">=</span> attention_mask<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>

    old_values <span class="token operator">=</span> values
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 获得prompt文本本身的奖励</span>
        <span class="token comment"># 由于prompt本身已存在文本，相当于整个决策序列中中已有的状态动作序列，</span>
        <span class="token comment"># 因此我们需要计算一下prompt文本对应的奖励</span>
        old_rewards <span class="token operator">=</span> self<span class="token punctuation">.</span>compute_rewards<span class="token punctuation">(</span>prompts<span class="token punctuation">,</span> log_probs<span class="token punctuation">,</span>
                                           ref_log_probs<span class="token punctuation">,</span> reward_score<span class="token punctuation">,</span>
                                           action_mask<span class="token punctuation">)</span>
        <span class="token triple-quoted-string string">"""
        compute_rewards函数：
    	def compute_rewards(self, prompts, log_probs, ref_log_probs, reward_score,
                        action_mask):
            kl_divergence_estimate = -self.kl_ctl * (log_probs - ref_log_probs)
            rewards = kl_divergence_estimate
            start = prompts.shape[1] - 1
            ends = start + action_mask[:, start:].sum(1)
            reward_clip = torch.clamp(reward_score, -self.clip_reward_value,
                                      self.clip_reward_value)
            batch_size = log_probs.shape[0]
            for j in range(batch_size):
                rewards[j, start:ends[j]][-1] += reward_clip[j]
    
            return rewards
        """</span>
        <span class="token comment"># 获得advantage值（v + r - v'）</span>
        advantages<span class="token punctuation">,</span> returns <span class="token operator">=</span> self<span class="token punctuation">.</span>get_advantages_and_returns<span class="token punctuation">(</span>
            old_values<span class="token punctuation">,</span> old_rewards<span class="token punctuation">,</span> start<span class="token punctuation">)</span>
        <span class="token triple-quoted-string string">"""
        def get_advantages_and_returns(self, values, rewards, start):
            # Adopted from https://github.com/CarperAI/trlx/blob/main/trlx/models/modeling_ppo.py#L134
            lastgaelam = 0
            advantages_reversed = []
            length = rewards.size()[-1]
            for t in reversed(range(start, length)):
                nextvalues = values[:, t + 1] if t &lt; length - 1 else 0.0
                delta = rewards[:, t] + self.gamma * nextvalues - values[:, t]
                lastgaelam = delta + self.gamma * self.lam * lastgaelam
                advantages_reversed.append(lastgaelam)
            advantages = torch.stack(advantages_reversed[::-1], dim=1)
            returns = advantages + values[:, start:]
            return advantages.detach(), returns
        """</span>

    <span class="token comment"># 下面则是获得生成部分seq的奖励等信息</span>
    batch <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token string">'input_ids'</span><span class="token punctuation">:</span> seq<span class="token punctuation">,</span> <span class="token string">"attention_mask"</span><span class="token punctuation">:</span> attention_mask<span class="token punctuation">}</span>
    <span class="token comment"># 获得seq的的概率</span>
    actor_prob <span class="token operator">=</span> self<span class="token punctuation">.</span>actor_model<span class="token punctuation">(</span><span class="token operator">**</span>batch<span class="token punctuation">,</span> use_cache<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">.</span>logits
    actor_log_prob <span class="token operator">=</span> gather_log_probs<span class="token punctuation">(</span>actor_prob<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                                      inputs<span class="token punctuation">[</span><span class="token string">'input_ids'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token comment"># 根据seq的概率logits，advantage作为权重，优化actor模型参数</span>
    actor_loss <span class="token operator">=</span> self<span class="token punctuation">.</span>actor_loss_fn<span class="token punctuation">(</span>actor_log_prob<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> start<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                                    log_probs<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> start<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> advantages<span class="token punctuation">,</span>
                                    action_mask<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> start<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token triple-quoted-string string">"""
	def actor_loss_fn(self, logprobs, old_logprobs, advantages, mask):
        ## policy gradient loss
        log_ratio = (logprobs - old_logprobs) * mask
        ratio = torch.exp(log_ratio)
        pg_loss1 = -advantages * ratio
        pg_loss2 = -advantages * torch.clamp(ratio, 1.0 - self.cliprange,
                                             1.0 + self.cliprange)
        pg_loss = torch.sum(torch.max(pg_loss1, pg_loss2) * mask) / mask.sum()
        return pg_loss
    """</span>
    <span class="token comment"># 更新actor参数</span>
    self<span class="token punctuation">.</span>actor_model<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>actor_loss<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>actor_model<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 获得seq的critic得分</span>
    value <span class="token operator">=</span> self<span class="token punctuation">.</span>critic_model<span class="token punctuation">.</span>forward_value<span class="token punctuation">(</span><span class="token operator">**</span>batch<span class="token punctuation">,</span>
                                            return_value_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                            use_cache<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
    <span class="token comment"># 计算Critic loss</span>
    critic_loss <span class="token operator">=</span> self<span class="token punctuation">.</span>critic_loss_fn<span class="token punctuation">(</span>value<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> start<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> old_values<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> start<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                                      returns<span class="token punctuation">,</span> action_mask<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> start<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token triple-quoted-string string">"""
	def critic_loss_fn(self, values, old_values, returns, mask):
        ## value loss
        values_clipped = torch.clamp(
            values,
            old_values - self.cliprange_value,
            old_values + self.cliprange_value,
        )
        vf_loss1 = (values - returns)**2
        vf_loss2 = (values_clipped - returns)**2
        vf_loss = 0.5 * torch.sum(
            torch.max(vf_loss1, vf_loss2) * mask) / mask.sum()
        return vf_loss
    """</span>
    <span class="token comment"># 更新Critic模型参数</span>
    self<span class="token punctuation">.</span>critic_model<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>critic_loss<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>critic_model<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> actor_loss<span class="token punctuation">,</span> critic_loss
</code></pre> 
<p><strong>（4）无监督预训练</strong><br>   在无监督数据集上，按照Causal Language Modeling进行预训练，更新actor模型参数。其为最原始的GPT类模型的预训练目标。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">train_unsupervised</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> unsup_coef<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># Train the unsupervised model here</span>
    self<span class="token punctuation">.</span>_validate_training_mode<span class="token punctuation">(</span><span class="token punctuation">)</span>

    outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>actor_model<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">,</span> use_cache<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
    loss <span class="token operator">=</span> outputs<span class="token punctuation">.</span>loss
    self<span class="token punctuation">.</span>actor_model<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>unsup_coef <span class="token operator">*</span> loss<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>actor_model<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>（5）EMA（指数移动平均）</strong><br>   额外引入EMA优化模型的参数，详见：</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">moving_average</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> model_ema<span class="token punctuation">,</span> beta<span class="token operator">=</span><span class="token number">0.992</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> zero_stage<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    zero_stage_3 <span class="token operator">=</span> <span class="token punctuation">(</span>zero_stage <span class="token operator">==</span> <span class="token number">3</span><span class="token punctuation">)</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> param<span class="token punctuation">,</span> param_ema <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                    model_ema<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># TODO: use prefiltering for efficiency</span>
            params_to_fetch <span class="token operator">=</span> _z3_params_to_fetch<span class="token punctuation">(</span><span class="token punctuation">[</span>param<span class="token punctuation">,</span> param_ema
                                                   <span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">if</span> zero_stage_3 <span class="token keyword">else</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
            should_gather_param <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>params_to_fetch<span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">0</span>
            <span class="token keyword">with</span> deepspeed<span class="token punctuation">.</span>zero<span class="token punctuation">.</span>GatheredParameters<span class="token punctuation">(</span>
                    params_to_fetch<span class="token punctuation">,</span> enabled<span class="token operator">=</span>should_gather_param<span class="token punctuation">)</span><span class="token punctuation">:</span>
                data <span class="token operator">=</span> param<span class="token punctuation">.</span>data
                <span class="token keyword">if</span> device <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                    data <span class="token operator">=</span> data<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
                param_ema<span class="token punctuation">.</span>data<span class="token punctuation">.</span>copy_<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>lerp<span class="token punctuation">(</span>data<span class="token punctuation">,</span> param_ema<span class="token punctuation">.</span>data<span class="token punctuation">,</span> beta<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<h5><a id="433_RLHF_792"></a>4.3.3 RLHF整体训练过程</h5> 
<p>  下面展示第三步的训练过程：</p> 
<ul><li>for 每一个epoch： 
  <ul><li>for 遍历每个batch，得到小批量的prompt和无监督语料： 
    <ul><li>对于所有prompt，调用<code>trainer.generate_experience(prompts)</code>获得经验数据，包括生成的seq、logits、奖励、状态价值等；</li><li>将这一组prompt的经验数据加入经验池；</li><li>for 每一个ppo_epoch: 
      <ul><li>for 遍历经验池中的每一个batch经验数据，以及无监督语料： 
        <ul><li>调用<code>trainer.train_rlhf(exp_data)</code>，更新Actor和Critic模型；</li><li>调用<code>trainer.train_unsupervised(unsup_data)</code>在无监督语料上预训练，更新Actor模型；</li><li>调用<code>moving_average()</code>进行指数移动平均</li></ul> </li><li>每一轮ppo_epoch时，打乱经验池和无监督语料的顺序。</li></ul> </li></ul> </li></ul> </li></ul> 
<hr> 
<p>  基于DeepSpeed训练可以实现在普通的GPU上训练超大规模语言模型，对照表如下所示：<br> <img src="https://images2.imgbox.com/22/de/4nzkJVbK_o.png" alt="在这里插入图片描述"></p> 
<hr>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8d67f0f5e61c5daeb934ea8ae42453a0/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">OpenCV 图像处理学习手册：6~7</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b1ca5604716eafc4e69101059f56e88a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">k8s1.26安装（kubeadm containerd）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>