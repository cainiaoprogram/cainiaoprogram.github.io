<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>scrapy中间件的使用 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="scrapy中间件的使用" />
<meta property="og:description" content="scrapy中间件的使用 学习目标： 应用 scrapy中使用间件使用随机UA的方法应用 scrapy中使用代理ip的的方法应用 scrapy与selenium配合使用 1. scrapy中间件的分类和作用 1.1 scrapy中间件的分类 根据scrapy运行流程中所在位置不同分为：
下载中间件爬虫中间件 1.2 scrapy中间的作用：预处理request和response对象 对header以及cookie进行更换和处理使用代理ip等对请求进行定制化操作， 但在scrapy默认的情况下 两种中间件都在middlewares.py一个文件中
爬虫中间件使用方法和下载中间件相同，且功能重复，通常使用下载中间件
2. 下载中间件的使用方法： 接下来我们对腾讯招聘爬虫进行修改完善，通过下载中间件来学习如何使用中间件
编写一个Downloader Middlewares和我们编写一个pipeline一样，定义一个类，然后在setting中开启
Downloader Middlewares默认的方法：
process_request(self, request, spider)：
当每个request通过下载中间件时，该方法被调用。返回None值：没有return也是返回None，该request对象传递给下载器，或通过引擎传递给其他权重低的process_request方法返回Response对象：不再请求，把response返回给引擎返回Request对象：把request对象通过引擎交给调度器，此时将不通过其他权重低的process_request方法 process_response(self, request, response, spider)：
当下载器完成http请求，传递响应给引擎的时候调用返回Resposne：通过引擎交给爬虫处理或交给权重更低的其他下载中间件的process_response方法返回Request对象：通过引擎交给调取器继续请求，此时将不通过其他权重低的process_request方法 在settings.py中配置开启中间件，权重值越小越优先执行
3. 定义实现随机User-Agent的下载中间件 3.1 在middlewares.py中完善代码 import random from Tencent.settings import USER_AGENTS_LIST # 注意导入路径,请忽视pycharm的错误提示 class UserAgentMiddleware(object): def process_request(self, request, spider): user_agent = random.choice(USER_AGENTS_LIST) request.headers[&#39;User-Agent&#39;] = user_agent # 不写return class CheckUA: def process_response(self,request,response,spider): print(request.headers[&#39;User-Agent&#39;]) return response # 不能少！ 3." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/5fbff4bad470012af321e34e865667ef/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-02-02T07:57:02+08:00" />
<meta property="article:modified_time" content="2023-02-02T07:57:02+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">scrapy中间件的使用</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="scrapy_0"></a>scrapy中间件的使用</h3> 
<h6><a id="_1"></a>学习目标：</h6> 
<ol><li>应用 scrapy中使用间件使用随机UA的方法</li><li>应用 scrapy中使用代理ip的的方法</li><li>应用 scrapy与selenium配合使用</li></ol> 
<hr> 
<h4><a id="1_scrapy_8"></a>1. scrapy中间件的分类和作用</h4> 
<h6><a id="11_scrapy_10"></a>1.1 scrapy中间件的分类</h6> 
<p>根据scrapy运行流程中所在位置不同分为：</p> 
<ol><li>下载中间件</li><li>爬虫中间件</li></ol> 
<h6><a id="12_scrapyrequestresponse_15"></a>1.2 scrapy中间的作用：预处理request和response对象</h6> 
<ol><li>对header以及cookie进行更换和处理</li><li>使用代理ip等</li><li>对请求进行定制化操作，</li></ol> 
<p>但在scrapy默认的情况下 两种中间件都在middlewares.py一个文件中</p> 
<p>爬虫中间件使用方法和下载中间件相同，且功能重复，通常使用下载中间件</p> 
<h4><a id="2__24"></a>2. 下载中间件的使用方法：</h4> 
<blockquote> 
 <p>接下来我们对腾讯招聘爬虫进行修改完善，通过下载中间件来学习如何使用中间件<br> 编写一个Downloader Middlewares和我们编写一个pipeline一样，定义一个类，然后在setting中开启</p> 
</blockquote> 
<p>Downloader Middlewares默认的方法：</p> 
<ul><li> <p>process_request(self, request, spider)：</p> 
  <ol><li>当每个request通过下载中间件时，该方法被调用。</li><li>返回None值：没有return也是返回None，该request对象传递给下载器，或通过引擎传递给其他权重低的process_request方法</li><li>返回Response对象：不再请求，把response返回给引擎</li><li>返回Request对象：把request对象通过引擎交给调度器，此时将不通过其他权重低的process_request方法</li></ol> </li><li> <p>process_response(self, request, response, spider)：</p> 
  <ol><li>当下载器完成http请求，传递响应给引擎的时候调用</li><li>返回Resposne：通过引擎交给爬虫处理或交给权重更低的其他下载中间件的process_response方法</li><li>返回Request对象：通过引擎交给调取器继续请求，此时将不通过其他权重低的process_request方法</li></ol> </li><li> <p>在settings.py中配置开启中间件，权重值越小越优先执行</p> </li></ul> 
<h4><a id="3_UserAgent_42"></a>3. 定义实现随机User-Agent的下载中间件</h4> 
<h5><a id="31_middlewarespy_44"></a>3.1 在middlewares.py中完善代码</h5> 
<pre><code>import random
from Tencent.settings import USER_AGENTS_LIST # 注意导入路径,请忽视pycharm的错误提示

class UserAgentMiddleware(object):
    def process_request(self, request, spider):
        user_agent = random.choice(USER_AGENTS_LIST)
        request.headers['User-Agent'] = user_agent
        # 不写return

class CheckUA:
    def process_response(self,request,response,spider):
        print(request.headers['User-Agent'])
        return response # 不能少！
</code></pre> 
<h5><a id="32_settings_62"></a>3.2 在settings中设置开启自定义的下载中间件，设置方法同管道</h5> 
<pre><code class="prism language-python">DOWNLOADER_MIDDLEWARES <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
   <span class="token string">'Tencent.middlewares.UserAgentMiddleware'</span><span class="token punctuation">:</span> <span class="token number">543</span><span class="token punctuation">,</span> <span class="token comment"># 543是权重值</span>
   <span class="token string">'Tencent.middlewares.CheckUA'</span><span class="token punctuation">:</span> <span class="token number">600</span><span class="token punctuation">,</span> <span class="token comment"># 先执行543权重的中间件，再执行600的中间件</span>
<span class="token punctuation">}</span>
</code></pre> 
<h5><a id="33_settingsUA_71"></a>3.3 在settings中添加UA的列表</h5> 
<pre><code>USER_AGENTS_LIST = [
    "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)",
    "Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)",
    "Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)",
    "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)",
    "Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6",
    "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1",
    "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0",
    "Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5"
]
</code></pre> 
<h5><a id="_86"></a>运行爬虫观察现象</h5> 
<h4><a id="4_ip_89"></a>4. 代理ip的使用</h4> 
<h5><a id="41__91"></a>4.1 思路分析</h5> 
<ol><li>代理添加的位置：request.meta中增加<code>proxy</code>字段</li><li>获取一个代理ip，赋值给<code>request.meta['proxy']</code> 
  <ul><li>代理池中随机选择代理ip</li><li>代理ip的webapi发送请求获取一个代理ip</li></ul> </li></ol> 
<h5><a id="42__97"></a>4.2 具体实现</h5> 
<p>免费代理ip：</p> 
<pre><code>class ProxyMiddleware(object):
    def process_request(self,request,spider):
        # proxies可以在settings.py中，也可以来源于代理ip的webapi
        # proxy = random.choice(proxies) 

        # 免费的会失效，报 111 connection refused 信息！重找一个代理ip再试
        proxy = 'https://1.71.188.37:3128' 

        request.meta['proxy'] = proxy
        return None # 可以不写return
</code></pre> 
<p>收费代理ip：</p> 
<pre><code># 人民币玩家的代码(使用abuyun提供的代理ip)
import base64

# 代理隧道验证信息  这个是在那个网站上申请的
proxyServer = 'http://proxy.abuyun.com:9010' # 收费的代理ip服务器地址，这里是abuyun
proxyUser = 用户名
proxyPass = 密码
proxyAuth = "Basic " + base64.b64encode(proxyUser + ":" + proxyPass)

class ProxyMiddleware(object):
    def process_request(self, request, spider):
        # 设置代理
        request.meta["proxy"] = proxyServer
        # 设置认证
        request.headers["Proxy-Authorization"] = proxyAuth
</code></pre> 
<h5><a id="43_ip_134"></a>4.3 检测代理ip是否可用</h5> 
<p>在使用了代理ip的情况下可以在下载中间件的process_response()方法中处理代理ip的使用情况，如果该代理ip不能使用可以替换其他代理ip</p> 
<pre><code>class ProxyMiddleware(object):
    ......
    def process_response(self, request, response, spider):
        if response.status != '200':
            request.dont_filter = True # 重新发送的请求对象能够再次进入队列
            return requst
</code></pre> 
<h6><a id="settingspy_146"></a>在settings.py中开启该中间件</h6> 
<h4><a id="5_selenium_148"></a>5. 在中间件中使用selenium</h4> 
<blockquote> 
 <p>以github登陆为例</p> 
</blockquote> 
<h5><a id="51__152"></a>5.1 完成爬虫代码</h5> 
<pre><code>import scrapy

class Login4Spider(scrapy.Spider):
    name = 'login4'
    allowed_domains = ['github.com']
    start_urls = ['https://github.com/1596930226'] # 直接对验证的url发送请求

    def parse(self, response):
        with open('check.html', 'w') as f:
            f.write(response.body.decode())
</code></pre> 
<h5><a id="52_middlewarespyselenium_167"></a>5.2 在middlewares.py中使用selenium</h5> 
<pre><code>import time
from selenium import webdriver


def getCookies():
    # 使用selenium模拟登陆，获取并返回cookie
    username = input('输入github账号:')
    password = input('输入github密码:')
    options = webdriver.ChromeOptions()
    options.add_argument('--headless')
    options.add_argument('--disable-gpu')
    driver = webdriver.Chrome('/home/worker/Desktop/driver/chromedriver',
                              chrome_options=options)
    driver.get('https://github.com/login')
    time.sleep(1)
    driver.find_element_by_xpath('//*[@id="login_field"]').send_keys(username)
    time.sleep(1)
    driver.find_element_by_xpath('//*[@id="password"]').send_keys(password)
    time.sleep(1)
    driver.find_element_by_xpath('//*[@id="login"]/form/div[3]/input[3]').click()
    time.sleep(2)
    cookies_dict = {cookie['name']: cookie['value'] for cookie in driver.get_cookies()}
    driver.quit()
    return cookies_dict

class LoginDownloaderMiddleware(object):

    def process_request(self, request, spider):
        cookies_dict = getCookies()
        print(cookies_dict)
        request.cookies = cookies_dict # 对请求对象的cookies属性进行替换
</code></pre> 
<h6><a id="selenium_203"></a>配置文件中设置开启该中间件后，运行爬虫可以在日志信息中看到selenium相关内容</h6> 
<hr> 
<h3><a id="_207"></a>小结</h3> 
<p>中间件的使用：</p> 
<ol><li>完善中间件代码：</li></ol> 
<ul><li> <p>process_request(self, request, spider)：</p> 
  <ol><li>当每个request通过下载中间件时，该方法被调用。</li><li>返回None值：没有return也是返回None，该request对象传递给下载器，或通过引擎传递给其他权重低的process_request方法</li><li>返回Response对象：不再请求，把response返回给引擎</li><li>返回Request对象：把request对象通过引擎交给调度器，此时将不通过其他权重低的process_request方法</li></ol> </li><li> <p>process_response(self, request, response, spider)：</p> 
  <ol><li>当下载器完成http请求，传递响应给引擎的时候调用</li><li>返回Resposne：通过引擎交给爬虫处理或交给权重更低的其他下载中间件的process_response方法</li><li>返回Request对象：通过引擎交给调取器继续请求，此时将不通过其他权重低的process_request方法</li></ol> </li></ul> 
<ol start="2"><li>需要在settings.py中开启中间件<br> DOWNLOADER_MIDDLEWARES = {<!-- --><br> ‘myspider.middlewares.UserAgentMiddleware’: 543,<br> }</li></ol> 
<hr> 
<p>引擎交给爬虫处理或交给权重更低的其他下载中间件的process_response方法<br> 3. 返回Request对象：通过引擎交给调取器继续请求，此时将不通过其他权重低的process_request方法</p> 
<ol start="2"><li>需要在settings.py中开启中间件<br> DOWNLOADER_MIDDLEWARES = {<!-- --><br> ‘myspider.middlewares.UserAgentMiddleware’: 543,<br> }</li></ol> 
<hr>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/0bbabe6f0b5fd988fffc022b62cde06c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">CUDA编程笔记（9）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d90595119a00c673aa8739238a118218/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">python的items()函数的用法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>