<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>43期《深入浅出Pytorch》课程 - Task01:PyTorch的安装和基础知识&#43;前置知识打卡 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="43期《深入浅出Pytorch》课程 - Task01:PyTorch的安装和基础知识&#43;前置知识打卡" />
<meta property="og:description" content="Task01 1、Pytorch安装2、基础知识2.1 张量(Tensor)2.2 自动求导2.3 梯度2.4 并行计算 3、前置知识打卡 1、Pytorch安装 由于之前使用过Pytorch，所以说不需要再重新下载，直接开始后续的基础知识
2、基础知识 由于之前学习过numpy系列，所以说对于Pytorch的理解会轻松很多。
2.1 张量(Tensor) ①创建tensor
import torch x = torch.rand(4, 3) print(x) 和numpy差不多，随机生成4X3的矩阵
②全0矩阵
x = torch.zeros(4, 3, dtype=torch.long) print(x) 和numpy里面的zeros也是一样的
由于之前系统学习过numpy并且参加过一些比赛，于是不在此赘述关于张量部分的知识，和numpy的基础很像，换个包名字就行(具体关于numpy的可以参考此链接https://blog.csdn.net/weixin_42198265/category_11063560.html?spm=1001.2014.3001.5482)
2.2 自动求导 在pytorch中，所有神经网络的核心是 autograd 包
①.backward()
torch.Tensor 是这个包的核心类。如果设置它的属性 .requires_grad 为 True，那么它将会追踪对于该张量的所有操作。
-当完成计算后可以通过调用 .backward()，来自动计算所有的梯度。这个张量的所有梯度将会自动累加到.grad属性。 注意：在 y.backward() 时，如果 y 是标量，则不需要为 backward() 传入任何参数；否则，需要传入一个与 y 同形的Tensor
②Function
Tensor 和 Function 互相连接生成了一个无环图 (acyclic graph)，它编码了完整的计算历史。
每个张量都有一个.grad_fn属性，该属性引用了创建 Tensor 自身的Function(除非这个张量是用户手动创建的，即这个张量的grad_fn是 None )。 from __future__ import print_function import torch # 手动创建张量-&gt;grad_fn返回结果是None x = torch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/bac5fb1c6d488c49df9b7cfc89ac37c1/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-11-16T00:11:53+08:00" />
<meta property="article:modified_time" content="2022-11-16T00:11:53+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">43期《深入浅出Pytorch》课程 - Task01:PyTorch的安装和基础知识&#43;前置知识打卡</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><img src="https://images2.imgbox.com/15/48/kjb2VH6G_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ab/e3/KGRmIXc5_o.png" alt="在这里插入图片描述"></p> 
<p></p> 
<div class="toc"> 
 <h4>Task01</h4> 
 <ul><li><a href="#1Pytorch_4" rel="nofollow">1、Pytorch安装</a></li><li><a href="#2_7" rel="nofollow">2、基础知识</a></li><li><ul><li><a href="#21_Tensor_9" rel="nofollow">2.1 张量(Tensor)</a></li><li><a href="#22__30" rel="nofollow">2.2 自动求导</a></li><li><a href="#23__66" rel="nofollow">2.3 梯度</a></li><li><a href="#24__135" rel="nofollow">2.4 并行计算</a></li></ul> 
  </li><li><a href="#3_142" rel="nofollow">3、前置知识打卡</a></li></ul> 
</div> 
<p></p> 
<h2><a id="1Pytorch_4"></a>1、Pytorch安装</h2> 
<p>由于之前使用过Pytorch，所以说不需要再重新下载，直接开始后续的基础知识<br> <img src="https://images2.imgbox.com/c0/3d/mTqbJJIh_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="2_7"></a>2、基础知识</h2> 
<p>由于之前学习过numpy系列，所以说对于Pytorch的理解会轻松很多。</p> 
<h3><a id="21_Tensor_9"></a>2.1 张量(Tensor)</h3> 
<p><strong>①创建tensor</strong></p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span> 
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre> 
<p>和numpy差不多，随机生成4X3的矩阵</p> 
<p><img src="https://images2.imgbox.com/ed/25/IfDLjrmw_o.png" alt="在这里插入图片描述"><br> <strong>②全0矩阵</strong></p> 
<pre><code class="prism language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre> 
<p>和numpy里面的zeros也是一样的<br> <img src="https://images2.imgbox.com/a4/9a/Fvw7ER6i_o.png" alt="在这里插入图片描述"><br> 由于之前系统学习过numpy并且参加过一些比赛，于是不在此赘述关于张量部分的知识，和numpy的基础很像，换个包名字就行(具体关于numpy的可以参考此链接<a href="https://blog.csdn.net/weixin_42198265/category_11063560.html?spm=1001.2014.3001.5482">https://blog.csdn.net/weixin_42198265/category_11063560.html?spm=1001.2014.3001.5482</a>)<br> <img src="https://images2.imgbox.com/66/79/aPH4GJpw_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="22__30"></a>2.2 自动求导</h3> 
<p><strong>在pytorch中，所有神经网络的核心是 autograd 包</strong><br> <strong>①.backward()</strong></p> 
<ul><li>torch.Tensor 是这个包的核心类。</li><li>如果设置它的属性 <code>.requires_grad</code> 为 True，那么它将会追踪对于该张量的所有操作。<br> -当完成计算后可以通过<strong>调用 .backward()</strong>，来自动计算<strong>所有的梯度</strong>。</li><li>这个张量的所有梯度将会自动累加到<code>.grad</code>属性。</li></ul> 
<p><strong>注意：在 y.backward() 时，如果 y 是标量，则不需要为 backward() 传入任何参数；否则，需要传入一个与 y 同形的Tensor</strong></p> 
<p><strong>②Function</strong><br> Tensor 和 Function 互相连接生成了一个无环图 (acyclic graph)，它编码了完整的计算历史。</p> 
<ul><li>每个张量都有一个.grad_fn属性，该属性引用了创建 Tensor 自身的Function(除非这个张量是用户手动创建的，即这个张量的grad_fn是 None )。</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">from</span> __future__ <span class="token keyword">import</span> print_function
<span class="token keyword">import</span> torch
<span class="token comment"># 手动创建张量-&gt;grad_fn返回结果是None</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad_fn<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/f3/19/nAZgeYBG_o.png" alt="在这里插入图片描述"></p> 
<p><strong>③requires_grad</strong><br> .requires_grad - 默认的是False，如果不打开，就是False；如果打开就会显示True；最后的计算记录可以在.grad_fn中显示。</p> 
<pre><code class="prism language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment"># 缺失情况下默认 requires_grad = False</span>
a <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>a <span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>a <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span>
a<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span>
b <span class="token operator">=</span> <span class="token punctuation">(</span>a <span class="token operator">*</span> a<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">.</span>grad_fn<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/72/40/PaWdtCC6_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="23__66"></a>2.3 梯度</h3> 
<p>反向传播：</p> 
<ul><li>因为 out 是一个标量，因此<code>out.backward()</code>和 <code>out.backward(torch.tensor(1.))</code> 等价</li></ul> 
<p><strong>out.backward()</strong> -&gt; 输出导数 d(out)/dx【微积分】</p> 
<p>grad在反向传播过程中是<strong>累加</strong>的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零</p> 
<pre><code class="prism language-python"><span class="token comment"># 设置tensor</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> x<span class="token operator">**</span><span class="token number">2</span>
z <span class="token operator">=</span> y <span class="token operator">*</span> y <span class="token operator">*</span> <span class="token number">3</span>
out <span class="token operator">=</span> z<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 第一次梯度下降</span>
out<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>

<span class="token comment"># 不归0的梯度下降 - 梯度累加</span>
out2 <span class="token operator">=</span> x<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
out2<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span> <span class="token comment"># 进行累加</span>

<span class="token comment"># 归0后的梯度下降 - 梯度从0开始</span>
out3 <span class="token operator">=</span> x<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
x<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 将梯度归0</span>
out3<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/29/f0/Rv5FM69Q_o.png" alt="在这里插入图片描述"><br> 由于如果是<strong>向量</strong>，是<strong>无法直接计算其梯度的</strong>，故而有一种方法是将向量传给 backward，再使用.grad方法。</p> 
<p>先确定向量</p> 
<pre><code class="prism language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment"># 随机得到一个1X3的矩阵</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>

y <span class="token operator">=</span> x <span class="token operator">*</span> <span class="token number">2</span> <span class="token comment"># 将矩阵进行两倍相乘</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span>
i <span class="token operator">=</span> <span class="token number">0</span> <span class="token comment"># 设置梯度参数-记录次数</span>
<span class="token keyword">while</span> y<span class="token punctuation">.</span>data<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">1000</span><span class="token punctuation">:</span>
    y <span class="token operator">=</span> y <span class="token operator">*</span> <span class="token number">2</span>
    i <span class="token operator">=</span> i <span class="token operator">+</span> <span class="token number">1</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span>

输出<span class="token punctuation">:</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token number">0.1789</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.4232</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.9889</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token number">0.3577</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.8463</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.9778</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>MulBackward0<span class="token operator">&gt;</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>  <span class="token number">183.1484</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1457.3232</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1012.6375</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>MulBackward0<span class="token operator">&gt;</span><span class="token punctuation">)</span>
<span class="token number">9</span>
</code></pre> 
<p>torch.autograd 不能直接计算完整的雅可比矩阵，但是如果我们只想要雅可比向量积，<strong>只需将这个向量作为参数传给 backward</strong></p> 
<pre><code class="prism language-python">v <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">0.0001</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>
y<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>v<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>
输入：
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0240e+02</span><span class="token punctuation">,</span> <span class="token number">1.0240e+03</span><span class="token punctuation">,</span> <span class="token number">1.0240e-01</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

修改梯度参数
v <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">8.0</span><span class="token punctuation">,</span> <span class="token number">0.00001</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>
y<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>v<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>
输出：
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0240e+02</span><span class="token punctuation">,</span> <span class="token number">8.1920e+03</span><span class="token punctuation">,</span> <span class="token number">1.0240e-02</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="24__135"></a>2.4 并行计算</h3> 
<p>之前李弘毅老师的视频中说过，深度学习再一次爆火的一部分原因是，可以用GPU进行并行运算了，大大减少训练模型时间，但是由于自己电脑配置太低，cuda不太能用，之前试过没有成功。所以说一般使用GPU都在kaggle的平台里。<br> 一周还有30多小时，是在不行还能用算力资源，一小时也比较便宜<br> <img src="https://images2.imgbox.com/c5/a5/xUc2BZJq_o.png" alt="在这里插入图片描述"><br> 对于并行运算，有一个李弘毅老师视频讲过，就类似下方这种（<a href="https://blog.csdn.net/weixin_42198265/article/details/126349359">https://blog.csdn.net/weixin_42198265/article/details/126349359</a>）<br> <img src="https://images2.imgbox.com/91/2f/HtqnaqvI_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="3_142"></a>3、前置知识打卡</h2> 
<p>对于前置知识这一块，由于之前接触过numpy和pytorch，所以说有些会的就不再记录了。<br> <strong>①!cat demo.py</strong><br> 这种就是可以直接在jupyter中运行py文件<br> <strong>②%timeit</strong><br> 计算时间-评估运行效率</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">choose_sort</span><span class="token punctuation">(</span>arr<span class="token punctuation">:</span> <span class="token builtin">list</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">list</span><span class="token punctuation">:</span>
    length <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>arr<span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>length<span class="token punctuation">)</span><span class="token punctuation">:</span>
        min_idx <span class="token operator">=</span> i
        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span> length<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> arr<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">&lt;</span> arr<span class="token punctuation">[</span>min_idx<span class="token punctuation">]</span><span class="token punctuation">:</span>
                min_idx <span class="token operator">=</span> j
        <span class="token keyword">if</span> min_idx <span class="token operator">!=</span> i<span class="token punctuation">:</span>
            arr<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> arr<span class="token punctuation">[</span>min_idx<span class="token punctuation">]</span> <span class="token operator">=</span> arr<span class="token punctuation">[</span>min_idx<span class="token punctuation">]</span><span class="token punctuation">,</span> arr<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
    <span class="token keyword">return</span> arr

<span class="token keyword">import</span> random
lst <span class="token operator">=</span> <span class="token punctuation">[</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token operator">%</span>timeit choose_sort<span class="token punctuation">(</span>lst<span class="token punctuation">)</span>

输出：
<span class="token number">32.6</span> ms ± <span class="token number">575</span> µs per loop <span class="token punctuation">(</span>mean ± std<span class="token punctuation">.</span> dev<span class="token punctuation">.</span> of <span class="token number">7</span> runs<span class="token punctuation">,</span> <span class="token number">10</span> loops each<span class="token punctuation">)</span>
</code></pre> 
<p>学习文档里面还写了一些对于文件的保存和读取，不过我一般都是用pd.read_csv那些，对于txt文件读取也是用的with open</p> 
<p>以上就是task01的全部打卡，希望可以在datawhale的学习中，学的更好！</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/1a3862577a5b1c11a823a0765b3d085f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">uniapp生成推送证书和推送打包证书</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/9e4c5be5377c78a293ac74aa3667cc15/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">matlab图像的放缩</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>