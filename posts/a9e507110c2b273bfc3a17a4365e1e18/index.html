<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>SqueezeNet论文详解 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="SqueezeNet论文详解" />
<meta property="og:description" content="原文地址： https://arxiv.org/pdf/1602.07360.pdf
网络介绍： 本文提出SqeezeNet。它在ImageNet上实现了和AlexNet相同的正确率，但是只使用了1/50的参数。更进一步，使用模型压缩技术，可以将SqueezeNet压缩到0.5MB，这是AlexNet的1/510。
架构的提出动机（轻量化模型的好处）： 1.提高分布式训练的效率：
对于分布式数据并行训练，通信开销与模型中的参数数量成正比。所以服务器之间的通信是分布式CNN训练的可扩展性的限制因素。因为是轻量化模型，训练过程中的沟通更少，数据量更少，训练更快。
2.小模型导出到客户端的开销更少：
较小的模型需要较少的通信，使频繁更新更可行。
3.更可行的FPGA和嵌入式部署：
FPGA通常具有小于10MB的片上存储器并且没有片外存储器，足够小的模型可以直接存储在芯片上。
模型设计过程： 1.模型压缩的探索过程
压缩的目的：
减小参数的同时保证一定的精度。
一种方法是使用现有的CNN模型并有损压缩它。主要方式有模型奇异值分解(SVD)，网络剪枝，深度压缩、硬件加速器。
2.神经网络空间设计的探索
神经网络（包括深度卷积神经网络）具有大的设计空间，具有用于微架构，宏架构，解算器和其它超参数的许多选项。
通常进行设计空间探索的方法有：
（1）贝叶斯优化
（2）模拟退火
（3）随机搜索
（4）遗传算法
3.squeezeNet设计过程：
结构设计策略：
1.使用1*1滤波器代替3*3滤波器，1x1滤波器具有比3x3滤波器少9倍的参数。
2.将输入到3x3过滤器的通道的数量减少。考虑一个完全由3x3滤波器组成的卷积层。该层中的参数的总量是（输入通道的数量）∗（滤波器的数量）∗（3 * 3）。 因此，为了在CNN中保持小的参数总数，不仅要减少3×3滤波器的数量（见上面的策略1），而且要减少3×3滤波器的输入通道的数量。我们使用挤压层将输入到3x3个过滤器的通道数量减少。
3.在网络后期降采样(将欠采样操作延后)，以使卷积层具有大的激活图.更大的激活图保留了更多的信息，可以提供更高的分类准确率 .（可以在参数数量受限的情况下提高准确率）
Fire模块的构建：
其中包括：挤压卷积层(1*1滤波器)、扩展层(1*1和3*3卷积的混合)。我们在Fire模块中公开了三个可调整维度（超级参数）：s1X1，e1X1和e3X3。在Fire模块中，s1X1是挤压层（所有1x1）中的滤波器数，e1X1是扩展层中1x1滤波器的数量，e3X3是扩展层中的3x3滤波器的数量。当我们使用Fire模块时，我们将s1X1设置为小于（e1X1 &#43; e3X3）。
SqueezeNet结构：
SqueezeNet开始于一个独立的卷积层（conv1），然后是8个fire模块（fire2-9），最后是一个最终的转换层（conv10）。 我们从网络的开始到结束逐渐增加每个fire模块的过滤器数量。polling安排尽量靠后，所以 SqueezeNet在层conv1，fire4，fire8和conv10之后以步长为2来执行max-pooling。如图是SqueezeNet架构的宏体系结构视图。 左：SqueezeNet; 中间：SqueezeNet与简单的旁路; 右：具有复杂旁路的SqueezeNet。
SqueezeNet实现细节： 1.Fire的扩展块中，1*1卷积核3*3卷积块输出特征图维度的保证。为了使来自1x1和3x3滤波器的输出激活具有相同的高度和宽度，我们在输入数据中向扩展模块的3x3滤波器添加零填充的1像素边界。
2.激活函数选用Relu
3.在fire9模块之后应用比率为50％的dropout
4.去掉全连接层
模型性能评估 SqueezeNet体系结构尺寸：
基于SVD的方法能够将预训练的AlexNet模型压缩5倍，同时将顶1精度降低到56.0％（Denton等人，2014）。网络修剪实现了9倍的模型大小减少，同时在ImageNet上保持了57.2％的top-1和80.3％的top-5精度的基线（Han et al。，2015b）。深度压缩使模型尺寸减小了35倍，同时仍然保持了基线精度水平（Han 等人，2015a）。现在，使用SqueezeNet，与AlexNet相比，我们实现了模型尺寸减少50倍，同时满足或超过了AlexNet的top-1和前top-5的精度。
即使使用未压缩的32位值来表示模型，SqueezeNet具有比模型压缩社区的最好结果小1.4倍的模型大小，同时保持或超过基线精度。
SqueezeNet模型适合被压缩：
我们使用33％稀疏度和8-bits，对SqueezeNet应用深度压缩（Han等，2015a）。 这产生了与AlexNet相当的精度的0.66MB模型（363×小于32位AlexNet）。此外，在SqueezeNet上应用具有6-bits和33％稀疏性的深度压缩，我们产生具有等效精度的0.47MB型号（510x小于32位AlexNet）。
总之：通过将CNN架构创新（SqueezeNet）与最先进的压缩技术（深度压缩）相结合，我们实现了模型尺寸减少510×，与基准相比精度没有降低。
SqueezeNet空间结构设计探索 上图分别是SR和3*3滤波器在扩展层比例对模型大小和精度的影响
已有的参数：
s1X1，e1X1和e3X3。 SqueezeNet有8个fire模块，总共24维超参数。
再定义几个参数(为了对SqueezeNet这样样架构的设计空间进行广泛扫描)：
1.basee：
CNN中第一个fire模块中的扩展过滤器数
2.pct3X3：
3X3的扩展过滤器的百分比，ei3X3=ei∗pct3X3" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/a9e507110c2b273bfc3a17a4365e1e18/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-11-27T00:19:36+08:00" />
<meta property="article:modified_time" content="2018-11-27T00:19:36+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">SqueezeNet论文详解</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h4 style="margin-left:0in;">原文地址：</h4> 
<p style="margin-left:0in;"><a href="https://arxiv.org/pdf/1602.07360.pdf" rel="nofollow">https://arxiv.org/pdf/1602.07360.pdf</a></p> 
<p style="margin-left:0in;"> </p> 
<h4 style="margin-left:0in;">网络介绍：</h4> 
<p style="margin-left:0in;">本文提出SqeezeNet。它在ImageNet上实现了和AlexNet相同的正确率，但是只使用了1/50的参数。更进一步，使用模型压缩技术，可以将SqueezeNet压缩到0.5MB，这是AlexNet的1/510。</p> 
<p style="margin-left:0in;"> </p> 
<h4 style="margin-left:0in;">架构的提出动机（轻量化模型的好处）：</h4> 
<p style="margin-left:0in;"><strong>1.提高分布式训练的效率：</strong></p> 
<p style="margin-left:0in;">对于分布式数据并行训练，通信开销与模型中的参数数量成正比。所以服务器之间的通信是分布式CNN训练的可扩展性的限制因素。因为是轻量化模型，训练过程中的沟通更少，数据量更少，训练更快。</p> 
<p style="margin-left:0in;"><strong>2.小模型导出到客户端的开销更少：</strong></p> 
<p style="margin-left:0in;">较小的模型需要较少的通信，使频繁更新更可行。</p> 
<p style="margin-left:0in;"><strong>3.更可行的FPGA和嵌入式部署：</strong></p> 
<p style="margin-left:0in;">FPGA通常具有小于10MB的片上存储器并且没有片外存储器，足够小的模型可以直接存储在芯片上。</p> 
<p style="margin-left:0in;"> </p> 
<h4 style="margin-left:0in;">模型设计过程：</h4> 
<p style="margin-left:0in;"><strong>1.模型压缩的探索过程</strong></p> 
<p style="margin-left:0in;">压缩的目的：</p> 
<p style="margin-left:0in;">减小参数的同时保证一定的精度。</p> 
<p style="margin-left:0in;">一种方法是使用现有的CNN模型并有损压缩它。主要方式有模型奇异值分解(SVD)，网络剪枝，深度压缩、硬件加速器。</p> 
<p style="margin-left:0in;"><strong>2.神经网络空间设计的探索</strong></p> 
<p style="margin-left:0in;">神经网络（包括深度卷积神经网络）具有大的设计空间，具有用于微架构，宏架构，解算器和其它超参数的许多选项。</p> 
<p style="margin-left:0in;">通常进行设计空间探索的方法有：</p> 
<p style="margin-left:0in;">（1）贝叶斯优化</p> 
<p style="margin-left:0in;">（2）模拟退火</p> 
<p style="margin-left:0in;">（3）随机搜索</p> 
<p style="margin-left:0in;">（4）遗传算法</p> 
<p style="margin-left:0in;"><strong>3.squeezeNet设计过程：</strong></p> 
<p style="margin-left:0in;">结构设计策略：</p> 
<p style="margin-left:0in;">1.使用1*1滤波器代替3*3滤波器，1x1滤波器具有比3x3滤波器少9倍的参数。</p> 
<p style="margin-left:0in;">2.将输入到3x3过滤器的通道的数量减少。考虑一个完全由3x3滤波器组成的卷积层。该层中的参数的总量是（输入通道的数量）∗（滤波器的数量）∗（3 * 3）。 因此，为了在CNN中保持小的参数总数，不仅要减少3×3滤波器的数量（见上面的策略1），而且要减少3×3滤波器的输入通道的数量。我们使用挤压层将输入到3x3个过滤器的通道数量减少。</p> 
<p style="margin-left:0in;">3.在网络后期降采样(将欠采样操作延后)，以使卷积层具有大的激活图.更大的激活图保留了更多的信息，可以提供更高的分类准确率 .（可以在参数数量受限的情况下提高准确率）</p> 
<p style="margin-left:0in;">Fire模块的构建：</p> 
<p style="text-align:center;"><img alt="" class="has" height="343" src="https://images2.imgbox.com/85/ef/iSb9cg18_o.png" width="535"></p> 
<p style="margin-left:0in;">其中包括：挤压卷积层(1*1滤波器)、扩展层(1*1和3*3卷积的混合)。我们在Fire模块中公开了三个可调整维度（超级参数）：s1X1，e1X1和e3X3。在Fire模块中，s1X1是挤压层（所有1x1）中的滤波器数，e1X1是扩展层中1x1滤波器的数量，e3X3是扩展层中的3x3滤波器的数量。当我们使用Fire模块时，我们将s1X1设置为小于（e1X1 + e3X3）。</p> 
<p style="margin-left:0in;">SqueezeNet结构：</p> 
<p style="text-align:center;"><img alt="" class="has" height="564" src="https://images2.imgbox.com/5f/8c/8zJJsDAB_o.png" width="772"></p> 
<p style="margin-left:0in;">SqueezeNet开始于一个独立的卷积层（conv1），然后是8个fire模块（fire2-9），最后是一个最终的转换层（conv10）。 我们从网络的开始到结束逐渐增加每个fire模块的过滤器数量。polling安排尽量<strong>靠后，</strong>所以 SqueezeNet在层conv1，fire4，fire8和conv10之后以步长为2来执行max-pooling。如图是SqueezeNet架构的宏体系结构视图。 左：SqueezeNet; 中间：SqueezeNet与简单的旁路; 右：具有复杂旁路的SqueezeNet。</p> 
<p style="margin-left:0in;"> </p> 
<h4 style="margin-left:0in;">SqueezeNet实现细节：</h4> 
<p style="margin-left:0in;">1.Fire的扩展块中，1*1卷积核3*3卷积块输出特征图维度的保证。为了使来自1x1和3x3滤波器的输出激活具有相同的高度和宽度，我们在输入数据中向扩展模块的3x3滤波器添加零填充的1像素边界。</p> 
<p style="margin-left:0in;">2.激活函数选用Relu</p> 
<p style="margin-left:0in;">3.在fire9模块之后应用比率为50％的dropout</p> 
<p style="margin-left:0in;">4.去掉全连接层</p> 
<p style="margin-left:0in;"> </p> 
<h4 style="margin-left:0in;">模型性能评估</h4> 
<p style="margin-left:0in;"><strong>SqueezeNet体系结构尺寸：</strong></p> 
<p style="text-align:center;"><img alt="" class="has" height="495" src="https://images2.imgbox.com/45/22/fPPe36Ul_o.png" width="831"></p> 
<p style="margin-left:0in;"> 基于SVD的方法能够将预训练的AlexNet模型压缩<strong>5倍</strong>，同时将顶1精度降低到56.0％（Denton等人，2014）。网络修剪实现了<strong>9倍</strong>的模型大小减少，同时在ImageNet上保持了57.2％的top-1和80.3％的top-5精度的基线（Han et al。，2015b）。深度压缩使模型尺寸减小了<strong>35倍</strong>，同时仍然保持了基线精度水平（Han 等人，2015a）。现在，使用SqueezeNet，与AlexNet相比，我们实现了模型尺寸减少<strong>50倍</strong>，同时满足或超过了AlexNet的top-1和前top-5的精度。</p> 
<p style="text-align:center;"><img alt="" class="has" height="221" src="https://images2.imgbox.com/39/25/JW3KxRve_o.png" width="732"></p> 
<p style="margin-left:0in;">即使使用未压缩的32位值来表示模型，SqueezeNet具有比模型压缩社区的最好结果小1.4倍的模型大小，同时保持或超过基线精度。</p> 
<p style="margin-left:0in;"><strong>SqueezeNet模型适合被压缩：</strong></p> 
<p style="margin-left:0in;">我们使用33％稀疏度和8-bits，对SqueezeNet应用深度压缩（Han等，2015a）。 这产生了与AlexNet相当的精度的0.66MB模型（363×小于32位AlexNet）。此外，在SqueezeNet上应用具有6-bits和33％稀疏性的深度压缩，我们产生具有等效精度的0.47MB型号（510x小于32位AlexNet）。</p> 
<p style="margin-left:0in;">总之：通过将CNN架构创新（SqueezeNet）与最先进的压缩技术（深度压缩）相结合，我们实现了模型尺寸减少510×，与基准相比精度没有降低。</p> 
<p style="margin-left:0in;"> </p> 
<h4 style="margin-left:0in;">SqueezeNet空间结构设计探索</h4> 
<p style="text-align:center;"><img alt="" class="has" height="290" src="https://images2.imgbox.com/13/42/QRhUYyIJ_o.png" width="771"></p> 
<p style="margin-left:0in;">上图分别是SR和3*3滤波器在扩展层比例对模型大小和精度的影响</p> 
<p style="margin-left:0in;"><strong>已有的参数：</strong></p> 
<p style="margin-left:0in;"><em>s</em>1<em>X</em>1，<em>e</em>1<em>X</em>1和<em>e</em>3<em>X</em>3。 SqueezeNet有8个fire模块，总共24维超参数。</p> 
<p style="margin-left:0in;">再定义几个参数(为了对SqueezeNet这样样架构的设计空间进行广泛扫描)：</p> 
<p style="margin-left:0in;">1.basee：</p> 
<p style="margin-left:0in;">CNN中第一个fire模块中的扩展过滤器数</p> 
<p style="margin-left:0in;">2.pct3X3：</p> 
<p style="margin-left:0in;">3X3的扩展过滤器的百分比，ei3X3=ei∗pct3X3</p> 
<p style="margin-left:0in;">3.freq：</p> 
<p style="margin-left:0in;">Fire module的个数，当经过freq个fire block后，增加扩展过滤器的数量incre，所以，对于某freq个fire某块中的某个fire模块，扩展滤波器的数量为：ei=basee+(incre∗j/freq)，由基本定义：ei=ei1X1+ei3X3</p> 
<p style="margin-left:0in;">4.挤压比SR</p> 
<p style="margin-left:0in;">范围是0~1，si1X1=SR∗ei，挤压层滤波器个数占扩展层滤波器的比例。<br>  </p> 
<p style="margin-left:0in;"><strong>模型的宏观设计空间探索：</strong></p> 
<p style="margin-left:0in;">之前探索了微架构级别的设计空间(单个fire模块的内容)，现在探索宏观层面上关于Fire模块高级连接的设计策略。受ResNet启发，我们也探究旁路结构的作用。</p> 
<p style="margin-left:0in;">我们探索了三种结构：不加旁路的Squeeze，加简单旁路的Squeeze，和加复杂旁路的Squeeze.</p> 
<p style="margin-left:0in;">下面的数据可以看书，加旁路对模型确实有效果(必然吧，借鉴了后面的Inception的思想)。</p> 
<p style="text-align:center;"><img alt="" class="has" height="106" src="https://images2.imgbox.com/c8/ac/GmzSUxQ6_o.png" width="668"></p> 
<p style="margin-left:0in;">细节：</p> 
<p style="margin-left:0in;">加旁路只能加一半的fire模块，因为在简单的情况下，输入通道的数量和输出通道的数量必须相同; 因此，只有一半的Fire模块可以具有简单的旁路连接。当不能满足“相同数量的通道”要求时，我们使用复杂的旁路连接，如图所示 在图2的右侧。虽然简单的旁路是“仅有线”，但是我们将复杂旁路定义为包括1x1卷积层的旁路，滤波器的数量等于所需的输出通道的数量。 请注意，复杂的旁路连接会为模型添加额外的参数，而简单的旁路连接则不会。</p> 
<p style="margin-left:0in;"><strong>增加旁路的作用分析：</strong></p> 
<p style="margin-left:0in;">1.改变模型的正则化程度</p> 
<p style="margin-left:0in;">2.挤压层性能有限，旁路可以减轻挤压层的信息表示性瓶颈。在SqueezeNet中，挤压比（SR）为0.125，意味着每个挤压层比相应的膨胀层具有8倍少的输出通道。 由于这种严重的维度减小，可以通过挤压层的信息有限。然而，通过添加旁路连接到SqueezeNet，我们打开了信息绕挤压层流动的途径。</p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;">欢迎批评指正，讨论学习~<br> 最近在github放了两份分类的代码，分别是用Tensorflow和Pytorch实现的，主要用于深度学习入门，学习Tensorflow和Pytorch搭建网络基本的操作。打算将各网络实现一下放入这两份代码中，有兴趣可以看一看，期待和大家一起维护更新。<br> 代码地址：<br><a href="https://github.com/jiawen9611/classification_Tensorflow_Proj">Tensorflow实现分类网络</a><br><a href="https://github.com/jiawen9611/classification_Pytorch_Proj">Pytorch实现分类网络</a></p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p> 
<p style="margin-left:0in;"> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/2f6d7e59ad73f888b399eb79ebc60c25/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">数据库第六章</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/94646d2e7bb3777184c7e217a6595762/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Python subprocess.Popen 实时输出 stdout（正确管道写法）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>