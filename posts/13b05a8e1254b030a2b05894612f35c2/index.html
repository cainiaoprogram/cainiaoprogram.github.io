<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>我的一个Tensorflow模板 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="我的一个Tensorflow模板" />
<meta property="og:description" content="前段时间就开始学习Tensorflow了。虽然大致看了两本书，也没怎么上手实践过，但是学习期间就觉得Tensorflow有点乱，这两天试着写了个网络，主要目的就是给自己弄个初级的模板出来，以后有什么新的想法或者任务，就在这个模板的基础上添添改改就行，不必要每次都重新写很多东西。
为了以后的方便我目前主要想实现以下三个功能： 1. 使用tf.slim来构建网络的结构，因为看书上还有很多老一点的代码都还是一步步地做的，定义一个卷积层至少需要四五行代码，用slim的话一行就行，并且我看到一些新的网络的实现都是使用的slim。 2. 加上tensorboard 的功能，能够可视化的话，对理解网络有很大帮助，并且也很方便训练（不得不说tensorboard可是让我吃了个大亏） 3. 加上保存的功能，也就是持久化，持久化的部分我看书的时候就没怎么看懂，后来么，，，就不管原理了，能用就行
最后经过一番调试，算是完成了上述几个“简单”的目标，但是还是在参考别人代码的情况下完成的，，，我只想说Tensorflow确实对新手不太友好，也可能是我学习路径不太对。
tf.slim模块 slim模块是对tensorflow底层代码的一个高级封装，可以大大简化构造网络的代码量，比如实现一个卷积层，传统的tensorflow层可能需要
input = ... with tf.name_scope(&#39;conv1_1&#39;) as scope: kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32, stddev=1e-1), name=&#39;weights&#39;) conv = tf.nn.conv2d(input, kernel, [1, 1, 1, 1], padding=&#39;SAME&#39;) biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32), trainable=True, name=&#39;biases&#39;) bias = tf.nn.bias_add(conv, biases) conv1 = tf.nn.relu(bias, name=scope) 但是用slim的话
input = ... net = slim.conv2d(input, 128, [3, 3], scope=&#39;conv1_1&#39;) conv2d的变量作用分别是 input：代表输入的张量 128：代表生成的张量的depth，其实就是在该层使用多少卷积核，也就是能够生成多少feature map scope: 用于命名" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/13b05a8e1254b030a2b05894612f35c2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-03-23T21:07:43+08:00" />
<meta property="article:modified_time" content="2018-03-23T21:07:43+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">我的一个Tensorflow模板</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>前段时间就开始学习Tensorflow了。虽然大致看了两本书，也没怎么上手实践过，但是学习期间就觉得Tensorflow有点乱，这两天试着写了个网络，主要目的就是给自己弄个初级的模板出来，以后有什么新的想法或者任务，就在这个模板的基础上添添改改就行，不必要每次都重新写很多东西。</p> 
<p>为了以后的方便我目前主要想实现以下三个功能： <br> 1. 使用tf.slim来构建网络的结构，因为看书上还有很多老一点的代码都还是一步步地做的，定义一个卷积层至少需要四五行代码，用slim的话一行就行，并且我看到一些新的网络的实现都是使用的slim。 <br> 2. 加上tensorboard 的功能，能够可视化的话，对理解网络有很大帮助，并且也很方便训练（不得不说tensorboard可是让我吃了个大亏） <br> 3. 加上保存的功能，也就是持久化，持久化的部分我看书的时候就没怎么看懂，后来么，，，就不管原理了，能用就行</p> 
<p>最后经过一番调试，算是完成了上述几个“简单”的目标，但是还是在参考别人代码的情况下完成的，，，我只想说Tensorflow确实对新手不太友好，也可能是我学习路径不太对。</p> 
<h2 id="tfslim模块">tf.slim模块</h2> 
<p>slim模块是对tensorflow底层代码的一个高级封装，可以大大简化构造网络的代码量，比如实现一个卷积层，传统的tensorflow层可能需要</p> 
<pre class="prettyprint"><code class=" hljs r">input = <span class="hljs-keyword">...</span>
with tf.name_scope(<span class="hljs-string">'conv1_1'</span>) as scope:
  kernel = tf.Variable(tf.truncated_normal([<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">64</span>, <span class="hljs-number">128</span>], dtype=tf.float32,
                                           stddev=<span class="hljs-number">1e-1</span>), name=<span class="hljs-string">'weights'</span>)
  conv = tf.nn.conv2d(input, kernel, [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], padding=<span class="hljs-string">'SAME'</span>)
  biases = tf.Variable(tf.constant(<span class="hljs-number">0.0</span>, shape=[<span class="hljs-number">128</span>], dtype=tf.float32),
                       trainable=True, name=<span class="hljs-string">'biases'</span>)
  bias = tf.nn.bias_add(conv, biases)
  conv1 = tf.nn.relu(bias, name=scope)</code></pre> 
<p>但是用slim的话</p> 
<pre class="prettyprint"><code class=" hljs r">input = <span class="hljs-keyword">...</span>
net = slim.conv2d(input, <span class="hljs-number">128</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>], scope=<span class="hljs-string">'conv1_1'</span>)</code></pre> 
<p>conv2d的变量作用分别是 <br> input：代表输入的张量 <br> 128：代表生成的张量的depth，其实就是在该层使用多少卷积核，也就是能够生成多少feature map <br> scope: 用于命名</p> 
<p>还有一个很好用的功能是</p> 
<pre class="prettyprint"><code class=" hljs javascript"><span class="hljs-keyword">with</span> slim.arg_scope([slim.conv2d], padding=<span class="hljs-string">'SAME'</span>,
                      weights_initializer=tf.truncated_normal_initializer(stddev=<span class="hljs-number">0.01</span>)
                      weights_regularizer=slim.l2_regularizer(<span class="hljs-number">0.0005</span>)):</code></pre> 
<p>使用 slim.arg_scope时，对于右边中括号中的所有函数，后面的参数设置都作为他们的默认参数</p> 
<p>所以我的网络是</p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">model2</span><span class="hljs-params">(inputs)</span>:</span>
    <span class="hljs-keyword">with</span> slim.arg_scope([layers.conv2d, slim.fully_connected], 
                       activation_fn = tf.nn.relu,
                       weights_initializer = tf.truncated_normal_initializer(<span class="hljs-number">0.0</span>, <span class="hljs-number">0.001</span>),
                       reuse = tf.AUTO_REUSE,
                       normalizer_fn = slim.batch_norm):
        <span class="hljs-keyword">with</span> slim.arg_scope([slim.conv2d, layers_lib.max_pool2d], padding = <span class="hljs-string">'SAME'</span>):
            net = layers.conv2d(inputs, <span class="hljs-number">32</span>, [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], stride = <span class="hljs-number">1</span>, scope = <span class="hljs-string">'conv2d_1_1x1'</span>)
            net = layers.conv2d(net, <span class="hljs-number">64</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>], stride = <span class="hljs-number">2</span> , scope = <span class="hljs-string">'conv2d_2_3x3'</span>)
            net = layers.max_pool2d(net, [<span class="hljs-number">3</span>,<span class="hljs-number">3</span>], stride = <span class="hljs-number">2</span>, scope = <span class="hljs-string">'max_pool2d_3_3x3'</span>)
            net = layers.conv2d(net, <span class="hljs-number">128</span>, [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], stride = <span class="hljs-number">1</span>, scope = <span class="hljs-string">'conv2d_4_1x1'</span>)
            net = layers.conv2d(net, <span class="hljs-number">256</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>], stride = <span class="hljs-number">2</span> , scope = <span class="hljs-string">'conv2d_5_3x3'</span>)
            net = layers.conv2d(net, <span class="hljs-number">256</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>], stride = <span class="hljs-number">1</span> , scope = <span class="hljs-string">'conv2d_5a_3x3'</span>)
            net = layers.max_pool2d(net, [<span class="hljs-number">3</span>,<span class="hljs-number">3</span>], stride = <span class="hljs-number">2</span>, scope = <span class="hljs-string">'max_pool2d_6_3x3'</span>)
            net = layers.conv2d(net, <span class="hljs-number">512</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>], stride = <span class="hljs-number">2</span> , scope = <span class="hljs-string">'conv2d_7_3x3'</span>)
            <span class="hljs-comment"># net = tf.reshape(net, [batch_size, -1])</span>
            <span class="hljs-comment"># net = layers.conv2d(net, 1, [1, 1], stride = 2, scope = 'conv2d_7_1x1')</span>
        net = slim.fully_connected(net, <span class="hljs-number">256</span>, scope = <span class="hljs-string">'fc_7'</span>)
        net = slim.fully_connected(net, <span class="hljs-number">10</span>, scope = <span class="hljs-string">'fc_8'</span>)
    <span class="hljs-keyword">return</span> net</code></pre> 
<p>结构上是比较简单的毕竟只是第一次写，第一次训练，就用个简单的网络就好。输入inputs是一个张量，输出net也是，大小应该是[batch_size, 1, 1, 10],，这样就把网络看做一个函数，很多网络都是这么实现的，看起来很简洁，而不是把各种结构的声明定义全部放在一起。</p> 
<p>这里要说明两点， <br> 1） 我在刚开始运行的时候总是被提醒某个变量要么已经存在了，要么没有声明，我也不知道原因，只是根据提示添加了reuse = tf.AUTO_REUSE这个参数，还有在代码中添加了</p> 
<pre class="prettyprint"><code class=" hljs avrasm">tf<span class="hljs-preprocessor">.reset</span>_default_graph() </code></pre> 
<p>这一行，具体原因我也不是很清楚，或许路过的哪位大神可以告诉我 <br> 2） 刚开始训练的时候没过多久，损失loss就完全不动了，按理说至少会有个波动，我就猜到是不是发生了梯度消失，所以就在代码中加了一个参数</p> 
<pre class="prettyprint"><code class=" hljs fix"><span class="hljs-attribute">normalizer_fn </span>=<span class="hljs-string"> slim.batch_norm</span></code></pre> 
<p>只是加了这么一句话，使用了BN，还都是默认参数，但是完全可以训练了，而且速度还挺快。 <br> 我用的是cifar-10的数据集 <br> <img title="" alt="这里写图片描述" src="https://images2.imgbox.com/3a/f6/oObeKTxU_o.png"> <br> 上图是没有BN的情况下，可以看到，才几百步（一步是一个batch），loss就完全不变了，我猜是因为梯度消失。 <br> <img title="" alt="这里写图片描述" src="https://images2.imgbox.com/b0/ed/RMpQTWTK_o.png"></p> 
<p>上图是加了BN的情况，可以看到将近5个小时的训练，将近10k步，accuracy 已经有92%了。可见BN确实是一个很牛逼的算法。 <br> 只是开始设置的时候训练的epoch数相对于我的渣电脑有点大，所以没有训练完就停了，没有测试在测试集上的准确度，不过无所谓，反正我只是想得到一个模板。</p> 
<h2 id="tensorboard-可视化">tensorboard 可视化</h2> 
<p>我一开始知道tensorflow有可视化的功能的时候就很想了解，对任何学科，任何问题，能够可视化都是可以加强自己的理解也帮助别人理解的好方法，但是我一开始也走了些弯路。 <br> 首先<strong>要想可视化需要在代码中添加一些代码，指出想要可视化哪些量</strong>，然而开始的时候我还以为可以直接可视化的，谁知道还要添加代码。 <br> 然后就是盘符的问题，我按照教程启动tensorboard之后总显示没有活动的事件（好像是这么说的，反正就是空白，没有数据），找了很多方法，反正又是Windows的问题，默认的是反斜杠，在写地址的时候最好写绝对地址，<strong>Windows下输入地址要用斜杠，不要用反斜杠</strong></p> 
<pre class="prettyprint"><code class=" hljs ruby">tensorboard --logdir =<span class="hljs-constant">F</span><span class="hljs-symbol">:/TensorFlow_exp/mnist/log1</span></code></pre> 
<p>都是小问题，但是解决不了的话就很烦。</p> 
<p>我们想要使用tensorboard的时候，首先需要进行<strong>summary operation</strong>，一般主要用的就是<strong>tf.summary.scalar()</strong>就是相当于将某个标量记录下来，用于在tensorboard中展示，用法如下</p> 
<pre class="prettyprint"><code class=" hljs perl">tf.summary.<span class="hljs-keyword">scalar</span>(<span class="hljs-string">'accuracy'</span>, accuracy)
<span class="hljs-comment"># 或者</span>
tf.summary.<span class="hljs-keyword">scalar</span>(<span class="hljs-string">"cost_function"</span>, cost)</code></pre> 
<p>就可以将最常用的accuracy 和loss 记录下来，另外还有<strong>tf.summary.histgram()</strong>用来记录数据的分布等，具体可以参见tensorboard的文档</p> 
<p>我们可以定义很多这种summary操作，之后可以把他们merge一下，再定一个FileWriter就可以往一个event文件中添加数据了。tensorboard 是读取这个文件来进行可视化的。和定义网络结构的过程类似，以上几步也只是定义了一个步骤，在训练的时候还要往event文件中不断添加数据。这时候就要用到<strong>add_summary()</strong>这个方法来向event文件正式写入数据的，这一步是要在训练的时候进行的。总体的框架基本如下</p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-comment"># 定义损失，并将其加入到scalar中，之后loss就会在tensorboard面板中的scalar中出现</span>
<span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'cross_entropy'</span>):
        total_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=y))
        tf.summary.scalar(<span class="hljs-string">"loss"</span>, total_loss)
<span class="hljs-comment"># 定义accuracy，也将其加入summary</span>
<span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'accuracy'</span>):
    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'correct_prediction'</span>):
        correct_pred = tf.equal(tf.argmax(y_, <span class="hljs-number">1</span>), tf.argmax(y, <span class="hljs-number">1</span>))
    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'accuracy'</span>):
        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
tf.summary.scalar(<span class="hljs-string">'accuracy'</span>, accuracy)

<span class="hljs-comment"># 将所有的summary 合并在一起，方便处理</span>
merged_summary_op = tf.summary.merge_all()  
<span class="hljs-comment"># 定义两个FileWriter，分别储存训练数据和测试数据，这一步已经在 with tf.Session() as sess 里面了</span>
    train_writer = tf.summary.FileWriter(log_dir + <span class="hljs-string">'/train'</span>, sess.graph)  
    test_writer = tf.summary.FileWriter(log_dir + <span class="hljs-string">'/test'</span>)  
<span class="hljs-comment"># 对每个batch的处理,这里面每80个batch在测试集上测试，并将summary写入test_writer，注意这里面测试集和训练集的数据获取没有写</span>
<span class="hljs-comment">#每100个batch保存模型，以及一些运行相关的metadata，实测这个保存的操作挺浪费时间，所以才要间隔一定的batch</span>
<span class="hljs-comment"># 其他的正常步数就正常运行训练</span>
    <span class="hljs-keyword">if</span> batch % <span class="hljs-number">80</span> ==<span class="hljs-number">0</span>:
                summary, acc = sess.run([merged_summary_op, accuracy], feed_dict={x:X_test, y:y_test })  
                test_writer.add_summary(summary, i * total_batch + batch)  
                print(<span class="hljs-string">'Accuracy at step %s: %s'</span> % (i * total_batch + batch, acc)) 
            <span class="hljs-keyword">else</span> :
                <span class="hljs-keyword">if</span> batch % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:
                    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
                    run_metadata = tf.RunMetadata()
                    _, summary = sess.run([optimizer,  merged_summary_op], feed_dict={x: batch_x, y: batch_y})
                    train_writer.add_run_metadata(run_metadata, <span class="hljs-string">'step%d'</span> % (i * total_batch + batch))
                    train_writer.add_summary(summary, i * total_batch + batch)
                    saver.save(sess, <span class="hljs-string">'F:/TensorFlow_exp/cifar10/model.ckpt'</span>, i * total_batch + batch)
                <span class="hljs-keyword">else</span>:
                    _, summary = sess.run([optimizer,  merged_summary_op], feed_dict={x: batch_x, y: batch_y})
                    train_writer.add_summary(summary, i * total_batch + batch)</code></pre> 
<p>有上面这个框架应该够了，开始的时候我没有管test的准确率，后来有看到别人的代码感觉挺好的，就又借鉴了一下，这样在tensorboard上可以同时看到训练和测试的accuracy，很方便。再重复一遍</p> 
<h2 id="保存持久化">保存（持久化）</h2> 
<p>我们的模型训练完了是要保存下来用于inference的，或者有时候不方便，训练一半需要停下来，如果不进行持久化的话，在内存里面的好不容易训练的模型就没了。还有一个很重要的点就是我们很多时候没必要自己去训练网络，拿在ImageNet 上训练过的fine tune一下就好。但是说实话这种finetune我还不太清楚，回头搞清楚了可以写下来。所以持久化是很重要的，这里先写一下保存为ckpt格式的简单的保存。 <br> 其实很简单，其实主要就两句话</p> 
<pre class="prettyprint"><code class=" hljs vala">saver = tf.train.Saver()
<span class="hljs-preprocessor"># 声明一个saver()，然后用他就可以进行持久化</span>

<span class="hljs-preprocessor">#然后在一个session中就可以</span>
saver.save(sess, <span class="hljs-string">'F:/TensorFlow_exp/cifar10_reg/model.ckpt'</span>)</code></pre> 
<p>保存后的文件是有四个，可以用于之后用的时候的恢复，但是对于模型的回复我还不是非常清楚，搞清楚了的话，我在记录一下</p> 
<p>以上是主要的内容，比较简略，毕竟我也不是非常熟悉Tensorflow，之后再继续学习。不过这里要说明一下，上面的网络我加了测试集之后发现我的网络有很严重的过拟合现象 <br> <img title="" alt="这里写图片描述" src="https://images2.imgbox.com/cb/77/RZg5p2iM_o.png"> <br> 可以看到，训练集上面的准确度（橙色）在6k步之后就超过了0.9，但是在测试集上面的准确度却始终没有超过0.7，还有缓慢下降的趋势，可以说是很典型的过拟合了，在网络参数里面加入了正则化也没有用，总之目前原因还不太清楚，以后能够找出来的话再更新吧。</p> 
<p>下面附上代码供参考</p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-comment"># from tensorflow.contrib.framework.python.ops import arg_scope</span>
<span class="hljs-keyword">from</span> tensorflow.contrib.layers.python.layers <span class="hljs-keyword">import</span> layers <span class="hljs-keyword">as</span> layers_lib
<span class="hljs-keyword">from</span> tensorflow.contrib.layers.python.layers <span class="hljs-keyword">import</span> regularizers
<span class="hljs-keyword">from</span> tensorflow.examples.tutorials.mnist <span class="hljs-keyword">import</span> input_data
<span class="hljs-keyword">import</span> tensorflow.contrib.slim <span class="hljs-keyword">as</span> slim
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> _pickle <span class="hljs-keyword">as</span> pickle
<span class="hljs-keyword">import</span> seaborn
<span class="hljs-keyword">from</span> tensorflow.contrib.framework.python.ops <span class="hljs-keyword">import</span> arg_scope
<span class="hljs-keyword">from</span> tensorflow.contrib.layers.python.layers <span class="hljs-keyword">import</span> layers 
<span class="hljs-keyword">from</span> tensorflow.contrib.layers.python.layers <span class="hljs-keyword">import</span> regularizers
<span class="hljs-keyword">from</span> tensorflow.contrib.slim.nets <span class="hljs-keyword">import</span> resnet_v2
<span class="hljs-keyword">from</span> tensorflow.python.layers.core <span class="hljs-keyword">import</span> Flatten

tf.reset_default_graph() 
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">model</span><span class="hljs-params">(inputs)</span>:</span>
    <span class="hljs-keyword">with</span> slim.arg_scope([slim.conv2d, slim.fully_connected],
                      activation_fn=tf.nn.relu,
                      weights_initializer=tf.truncated_normal_initializer(<span class="hljs-number">0.0</span>, <span class="hljs-number">0.01</span>),
                      weights_regularizer=slim.l2_regularizer(<span class="hljs-number">0.0005</span>)):
        net = slim.repeat(inputs, <span class="hljs-number">2</span>, slim.conv2d, <span class="hljs-number">64</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>], scope=<span class="hljs-string">'conv1'</span>)
        net = slim.max_pool2d(net, [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], scope=<span class="hljs-string">'pool1'</span>)
        net = slim.repeat(net, <span class="hljs-number">2</span>, slim.conv2d, <span class="hljs-number">128</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>], scope=<span class="hljs-string">'conv2'</span>)
        net = slim.max_pool2d(net, [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], scope=<span class="hljs-string">'pool2'</span>)
    <span class="hljs-keyword">return</span> net

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">model2</span><span class="hljs-params">(inputs)</span>:</span>
    <span class="hljs-keyword">with</span> slim.arg_scope([layers.conv2d, slim.fully_connected], 
                       activation_fn = tf.nn.relu,
                       weights_initializer = tf.truncated_normal_initializer(<span class="hljs-number">0.0</span>, <span class="hljs-number">0.001</span>),
                       reuse = tf.AUTO_REUSE,
                       normalizer_fn = slim.batch_norm,
                       weights_regularizer=slim.l2_regularizer(<span class="hljs-number">0.0005</span>)):
        <span class="hljs-keyword">with</span> slim.arg_scope([slim.conv2d, layers_lib.max_pool2d], padding = <span class="hljs-string">'SAME'</span>):
            net = layers.conv2d(inputs, <span class="hljs-number">32</span>, [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], stride = <span class="hljs-number">1</span>, scope = <span class="hljs-string">'conv2d_1_1x1'</span>)
            net = layers.conv2d(net, <span class="hljs-number">64</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>], stride = <span class="hljs-number">2</span> , scope = <span class="hljs-string">'conv2d_2_3x3'</span>)
            net = layers.max_pool2d(net, [<span class="hljs-number">3</span>,<span class="hljs-number">3</span>], stride = <span class="hljs-number">2</span>, scope = <span class="hljs-string">'max_pool2d_3_3x3'</span>)
            net = layers.conv2d(net, <span class="hljs-number">128</span>, [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], stride = <span class="hljs-number">1</span>, scope = <span class="hljs-string">'conv2d_4_1x1'</span>)
            net = layers.conv2d(net, <span class="hljs-number">256</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>], stride = <span class="hljs-number">2</span> , scope = <span class="hljs-string">'conv2d_5_3x3'</span>)
            net = layers.conv2d(net, <span class="hljs-number">256</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>], stride = <span class="hljs-number">1</span> , scope = <span class="hljs-string">'conv2d_5a_3x3'</span>)
            net = layers.max_pool2d(net, [<span class="hljs-number">3</span>,<span class="hljs-number">3</span>], stride = <span class="hljs-number">2</span>, scope = <span class="hljs-string">'max_pool2d_6_3x3'</span>)
            net = layers.conv2d(net, <span class="hljs-number">512</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>], stride = <span class="hljs-number">2</span> , scope = <span class="hljs-string">'conv2d_7_3x3'</span>)
            <span class="hljs-comment"># net = tf.reshape(net, [batch_size, -1])</span>
            <span class="hljs-comment"># net = layers.conv2d(net, 1, [1, 1], stride = 2, scope = 'conv2d_7_1x1')</span>
        net = slim.fully_connected(net, <span class="hljs-number">256</span>, scope = <span class="hljs-string">'fc_7'</span>)
        net = slim.fully_connected(net, <span class="hljs-number">10</span>, scope = <span class="hljs-string">'fc_8'</span>)
    <span class="hljs-keyword">return</span> net

<span class="hljs-comment"># g = tf.Graph()</span>

learning_rate = <span class="hljs-number">1e-3</span>
training_iters = <span class="hljs-number">100</span>
batch_size = <span class="hljs-number">100</span>
display_step = <span class="hljs-number">5</span>
n_features = <span class="hljs-number">3072</span>  <span class="hljs-comment"># 32*32*3</span>
n_classes = <span class="hljs-number">10</span>

<span class="hljs-comment">#tf.reset_default_graph() </span>

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">unpickle</span><span class="hljs-params">(filename)</span>:</span>
    <span class="hljs-string">'''解压数据'''</span>
    <span class="hljs-keyword">with</span> open(filename, <span class="hljs-string">'rb'</span>) <span class="hljs-keyword">as</span> f:
        d = pickle.load(f, encoding=<span class="hljs-string">'latin1'</span>)
        <span class="hljs-keyword">return</span> d


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">onehot</span><span class="hljs-params">(labels)</span>:</span>
    <span class="hljs-string">'''one-hot 编码'''</span>
    n_sample = len(labels)
    n_class = max(labels) + <span class="hljs-number">1</span>
    onehot_labels = np.zeros((n_sample, n_class))
    onehot_labels[np.arange(n_sample), labels] = <span class="hljs-number">1</span>
    <span class="hljs-keyword">return</span> onehot_labels


<span class="hljs-comment"># 训练数据集</span>
data1 = unpickle(<span class="hljs-string">'F:/TensorFlow_exp/cifar10-TensorFlow-tensorboard/cifar10-TensorFlow-tensorboard/cifar-10-batches-py/data_batch_1'</span>)
data2 = unpickle(<span class="hljs-string">'F:/TensorFlow_exp/cifar10-TensorFlow-tensorboard/cifar10-TensorFlow-tensorboard/cifar-10-batches-py/data_batch_2'</span>)
data3 = unpickle(<span class="hljs-string">'F:/TensorFlow_exp/cifar10-TensorFlow-tensorboard/cifar10-TensorFlow-tensorboard/cifar-10-batches-py/data_batch_3'</span>)
data4 = unpickle(<span class="hljs-string">'F:/TensorFlow_exp/cifar10-TensorFlow-tensorboard/cifar10-TensorFlow-tensorboard/cifar-10-batches-py/data_batch_4'</span>)
data5 = unpickle(<span class="hljs-string">'F:/TensorFlow_exp/cifar10-TensorFlow-tensorboard/cifar10-TensorFlow-tensorboard/cifar-10-batches-py/data_batch_5'</span>)
X_train = np.concatenate((data1[<span class="hljs-string">'data'</span>], data2[<span class="hljs-string">'data'</span>], data3[<span class="hljs-string">'data'</span>], data4[<span class="hljs-string">'data'</span>], data5[<span class="hljs-string">'data'</span>]), axis=<span class="hljs-number">0</span>)
y_train = np.concatenate((data1[<span class="hljs-string">'labels'</span>], data2[<span class="hljs-string">'labels'</span>], data3[<span class="hljs-string">'labels'</span>], data4[<span class="hljs-string">'labels'</span>], data5[<span class="hljs-string">'labels'</span>]), axis=<span class="hljs-number">0</span>)
y_train = onehot(y_train)
<span class="hljs-comment"># 测试数据集</span>
test = unpickle(<span class="hljs-string">'F:/TensorFlow_exp/cifar10-TensorFlow-tensorboard/cifar10-TensorFlow-tensorboard/cifar-10-batches-py/test_batch'</span>)
X_test = test[<span class="hljs-string">'data'</span>][:<span class="hljs-number">5000</span>, :]
y_test = onehot(test[<span class="hljs-string">'labels'</span>])[:<span class="hljs-number">5000</span>, :]
<span class="hljs-keyword">del</span> test

print(<span class="hljs-string">'Training dataset shape:'</span>, X_train.shape)
print(<span class="hljs-string">'Training labels shape:'</span>, y_train.shape)
print(<span class="hljs-string">'Testing dataset shape:'</span>, X_test.shape)
print(<span class="hljs-string">'Testing labels shape:'</span>, y_test.shape)

<span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'input'</span>):
        x = tf.placeholder(tf.float32, [<span class="hljs-keyword">None</span>, n_features])
        y = tf.placeholder(tf.float32, [<span class="hljs-keyword">None</span>, n_classes])

<span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'input_reshape'</span>):
        x4d = tf.reshape(x, [-<span class="hljs-number">1</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>])

y_ = model2(x4d)
<span class="hljs-comment">#y_  = resnet_v2(x4d)</span>

<span class="hljs-comment">#loss = slim.losses.softmax_cross_entropy(y_,y)</span>
<span class="hljs-comment"># reg_loss = slim.add_n(slim.losses.get_regularization_losses())</span>

<span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'cross_entropy'</span>):
        total_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=y))
        tf.summary.scalar(<span class="hljs-string">"loss"</span>, total_loss)
<span class="hljs-comment"># total_loss = slim.losses.get_total_loss()</span>
tf.summary.scalar(<span class="hljs-string">'losses/total_loss'</span>, total_loss)
<span class="hljs-comment"># train_op = slim.learning(total_loss, tf.train.GradientDescentOptimizer(learning = 0.001))</span>
<span class="hljs-comment"># 评估模型</span>
<span class="hljs-comment">#y_cur = tf.reshape(y_, [batch_size, -1])</span>
y_cur = Flatten()(y_)
<span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'accuracy'</span>):
    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'correct_prediction'</span>):
        correct_pred = tf.equal(tf.argmax(y_cur, <span class="hljs-number">1</span>), tf.argmax(y, <span class="hljs-number">1</span>))
    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'accuracy'</span>):
        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
tf.summary.scalar(<span class="hljs-string">'accuracy'</span>, accuracy)
<span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'train'</span>):
        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(total_loss)
merged_summary_op = tf.summary.merge_all()
init = tf.global_variables_initializer()
log_dir = <span class="hljs-string">'F:/TensorFlow_exp/cifar10_reg/log'</span>
saver = tf.train.Saver()

<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:
    sess.run(init)
    <span class="hljs-comment"># summary_writer = tf.summary.FileWriter(log_dir, graph=tf.get_default_graph())</span>
    train_writer = tf.summary.FileWriter(log_dir + <span class="hljs-string">'/train'</span>, sess.graph)  
    test_writer = tf.summary.FileWriter(log_dir + <span class="hljs-string">'/test'</span>) 
<span class="hljs-comment"># slim.learning.train(train_op, log_dir, number_of_steps = 1000,</span>
 <span class="hljs-comment">#                  save_summaries_secs = 30, save_interval_secs = 60)</span>
    c = []
    total_batch = int(X_train.shape[<span class="hljs-number">0</span>] / batch_size)
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(training_iters):

        <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> range(total_batch):
            batch_x = X_train[batch*batch_size : (batch+<span class="hljs-number">1</span>)*batch_size, :]
            batch_y = y_train[batch*batch_size : (batch+<span class="hljs-number">1</span>)*batch_size, :]
            <span class="hljs-keyword">if</span> batch % <span class="hljs-number">80</span> ==<span class="hljs-number">0</span>:
                summary, acc = sess.run([merged_summary_op, accuracy], feed_dict={x:X_test, y:y_test })  
                test_writer.add_summary(summary, i * total_batch + batch)  
                print(<span class="hljs-string">'Accuracy at step %s: %s'</span> % (i * total_batch + batch, acc)) 
            <span class="hljs-keyword">else</span> :
                <span class="hljs-keyword">if</span> batch % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:
                    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
                    run_metadata = tf.RunMetadata()
                    _, summary = sess.run([optimizer,  merged_summary_op], feed_dict={x: batch_x, y: batch_y})
                    train_writer.add_run_metadata(run_metadata, <span class="hljs-string">'step%d'</span> % (i * total_batch + batch))
                    train_writer.add_summary(summary, i * total_batch + batch)
                    saver.save(sess, <span class="hljs-string">'F:/TensorFlow_exp/cifar10_reg/model.ckpt'</span>, i * total_batch + batch)
                <span class="hljs-keyword">else</span>:
                    _, summary = sess.run([optimizer,  merged_summary_op], feed_dict={x: batch_x, y: batch_y})
                    train_writer.add_summary(summary, i * total_batch + batch)



    train_writer.close()
    test_writer.close()
    print(<span class="hljs-string">"Optimization Finished!"</span>)
    <span class="hljs-comment"># Test</span>
    test_acc = sess.run(accuracy, feed_dict={x: X_test, y: y_test})
    print(<span class="hljs-string">"Testing Accuracy:"</span>, test_acc)
    plt.plot(c)
    plt.xlabel(<span class="hljs-string">'Iter'</span>)
    plt.ylabel(<span class="hljs-string">'Cost'</span>)
    plt.title(<span class="hljs-string">'lr=%f, ti=%d, bs=%d, acc=%f'</span> % (learning_rate, training_iters, batch_size, test_acc))
    plt.tight_layout()
    plt.savefig(<span class="hljs-string">'cnn-tf-cifar10-%s.png'</span> % test_acc, dpi=<span class="hljs-number">200</span>)</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4cb5ae3f9349959fc1a25aaedfed0607/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">linux 服务器配置java环境</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/07de6450916ca5d6a3f7bd4d52d397cc/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">springboot feign不能注入</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>