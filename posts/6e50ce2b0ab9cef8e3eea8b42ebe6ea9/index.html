<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>PyTorch学习笔记01——Tensor张量 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="PyTorch学习笔记01——Tensor张量" />
<meta property="og:description" content="Tensor张量 什么是TensorTensor的创建1. 直接创建2. 通过numpy创建3. 依据数值创建4. 依据概率分布创建 Tensor的属性Tensor的操作1. 张量拼接与切分2. 张量索引3. 张量变换4. 将张量移动到GPU Tensor的数学运算1. 加减乘除2. 对、指、幂函数3. 三角函数 什么是Tensor 张量是一种特殊的数据结构，与数组和矩阵非常相似。在 PyTorch 中，我们使用张量对模型的输入和输出以及模型的参数进行编码。
张量相当于一个多维数组。
torch.Tensor(dtype, shape, device, data, requires_grad, grad, grad_fn, is_leaf) requires_grad：指示是否需要梯度
grad：data的梯度
grad_fn：创建Tensor的function
is_leaf：是否是叶子节点（张量）
Tensor的创建 1. 直接创建 张量可以直接从数据中创建。数据类型是自动推断的。
从data创建，可以是list，numpy。
torch.tensor(data, dtype=None, # 默认与data一致 device=None, # cuda/cpu requires_grad=False, # 是否需要梯度 pin_memory=False) # 是否存于锁页内存，通常为False 例如：
data = [[1, 2],[3, 4]] x_data = torch.tensor(data) 2. 通过numpy创建 从numpy创建的tensor源于ndarray共享内存，当修改其中一个的数据，另一个也会被改动。
np_array = np.array(data) x_np = torch.from_numpy(np_array) 让tensor和numpy共享底层内存 t = torch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/6e50ce2b0ab9cef8e3eea8b42ebe6ea9/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-04T18:41:28+08:00" />
<meta property="article:modified_time" content="2023-04-04T18:41:28+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">PyTorch学习笔记01——Tensor张量</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>Tensor张量</h4> 
 <ul><li><a href="#Tensor_1" rel="nofollow">什么是Tensor</a></li><li><a href="#Tensor_13" rel="nofollow">Tensor的创建</a></li><li><ul><li><a href="#1__14" rel="nofollow">1. 直接创建</a></li><li><a href="#2_numpy_32" rel="nofollow">2. 通过numpy创建</a></li><li><a href="#3__51" rel="nofollow">3. 依据数值创建</a></li><li><a href="#4__170" rel="nofollow">4. 依据概率分布创建</a></li></ul> 
  </li><li><a href="#Tensor_241" rel="nofollow">Tensor的属性</a></li><li><a href="#Tensor_247" rel="nofollow">Tensor的操作</a></li><li><ul><li><a href="#1__248" rel="nofollow">1. 张量拼接与切分</a></li><li><a href="#2__313" rel="nofollow">2. 张量索引</a></li><li><a href="#3__358" rel="nofollow">3. 张量变换</a></li><li><a href="#4_GPU_432" rel="nofollow">4. 将张量移动到GPU</a></li></ul> 
  </li><li><a href="#Tensor_439" rel="nofollow">Tensor的数学运算</a></li><li><ul><li><a href="#1__440" rel="nofollow">1. 加减乘除</a></li><li><a href="#2__500" rel="nofollow">2. 对、指、幂函数</a></li><li><a href="#3__510" rel="nofollow">3. 三角函数</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="Tensor_1"></a>什么是Tensor</h2> 
<p>张量是一种特殊的数据结构，与数组和矩阵非常相似。在 PyTorch 中，我们使用张量对模型的输入和输出以及模型的参数进行编码。<br> 张量相当于一个多维数组。</p> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>dtype<span class="token punctuation">,</span> shape<span class="token punctuation">,</span> device<span class="token punctuation">,</span> data<span class="token punctuation">,</span> requires_grad<span class="token punctuation">,</span> 
			grad<span class="token punctuation">,</span> grad_fn<span class="token punctuation">,</span> is_leaf<span class="token punctuation">)</span>
</code></pre> 
<p>requires_grad：指示是否需要梯度<br> grad：data的梯度<br> grad_fn：创建Tensor的function<br> is_leaf：是否是叶子节点（张量）</p> 
<h2><a id="Tensor_13"></a>Tensor的创建</h2> 
<h3><a id="1__14"></a>1. 直接创建</h3> 
<p>张量可以直接从数据中创建。数据类型是自动推断的。<br> 从data创建，可以是list，numpy。</p> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>data<span class="token punctuation">,</span>
			dtype<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>  <span class="token comment"># 默认与data一致</span>
			device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>  <span class="token comment"># cuda/cpu</span>
			requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>  <span class="token comment"># 是否需要梯度</span>
			pin_memory<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token comment"># 是否存于锁页内存，通常为False</span>
</code></pre> 
<p>例如：</p> 
<pre><code class="prism language-python">data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
x_data <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="2_numpy_32"></a>2. 通过numpy创建</h3> 
<p>从numpy创建的tensor源于ndarray共享内存，当修改其中一个的数据，另一个也会被改动。</p> 
<pre><code class="prism language-python">np_array <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
x_np <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>np_array<span class="token punctuation">)</span>
</code></pre> 
<ul><li>让tensor和numpy共享底层内存</li></ul> 
<pre><code class="prism language-python">t <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"t: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>t<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
n <span class="token operator">=</span> t<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"n: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>n<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>  <span class="token comment"># n和t是一样的</span>
t<span class="token punctuation">.</span>add_<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"t: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>t<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"n: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>n<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>  <span class="token comment"># t更改后，n的值依然跟t一样</span>
</code></pre> 
<h3><a id="3__51"></a>3. 依据数值创建</h3> 
<ol><li>依size创建全0张量</li></ol> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token operator">*</span>size<span class="token punctuation">,</span>  <span class="token comment"># 如(3,3)、(3,224,224)</span>
			out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>  <span class="token comment"># 输出的张量，将生成张量赋值给一个张量</span>
			dtype<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
			layout<span class="token operator">=</span>torch<span class="token punctuation">.</span>strided<span class="token punctuation">,</span>  <span class="token comment"># 内存中的布局形式，有strided、sparse_coo等</span>
			device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
			requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> 
<ol start="2"><li>依input形状创建全0张量</li></ol> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span>
				dtype<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
				layout<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
				device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
				requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> 
<ol start="3"><li>依size创建全1张量</li></ol> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token operator">*</span>size<span class="token punctuation">,</span>
			out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
			dtype<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
			layout<span class="token operator">=</span>torch<span class="token punctuation">.</span>strided<span class="token punctuation">,</span>
			device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
			requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> 
<ol start="4"><li>依input形状创建全1张量</li></ol> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span>
				dtype<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
				layout<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
				device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
				requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> 
<ol start="5"><li>依size创建全为fill_value的张量</li></ol> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span><span class="token operator">*</span>size<span class="token punctuation">,</span>
			fill_value<span class="token punctuation">,</span>
			out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
			dtype<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
			layout<span class="token operator">=</span>torch<span class="token punctuation">.</span>strided<span class="token punctuation">,</span>
			device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
			requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
<span class="token comment"># 比如：</span>
torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
</code></pre> 
<ol start="6"><li>依input形状创建全为fill_value的张量</li></ol> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span>
				fill_value<span class="token punctuation">,</span>
				dtype<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
				layout<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
				device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
				requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> 
<ol start="7"><li>创建等值数列</li></ol> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>start<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>  <span class="token comment"># 区间为[start, end)</span>
			end<span class="token punctuation">,</span>
			step<span class="token punctuation">,</span>
			out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
			dtype<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
			layout<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
			device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
			requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> 
<ol start="8"><li>创建均分数列</li></ol> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span>start<span class="token punctuation">,</span>  <span class="token comment"># 区间为[start, end]</span>
				end<span class="token punctuation">,</span>
				steps<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span>  <span class="token comment"># 数列长度，而不是步长，步长=(end-start)/(steps-1)</span>
				out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
				dtype<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
				layout<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
				device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
				requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>		
</code></pre> 
<ol start="9"><li>创建对数数列</li></ol> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>logspace<span class="token punctuation">(</span>start<span class="token punctuation">,</span>
				end<span class="token punctuation">,</span>
				steps<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span>
				base<span class="token operator">=</span><span class="token number">10.0</span><span class="token punctuation">,</span>
				out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
				dtype<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
				layout<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
				device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
				requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> 
<ol start="10"><li>创建单位对角矩阵（2维张量）</li></ol> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>eye<span class="token punctuation">(</span>n<span class="token punctuation">,</span>  <span class="token comment"># 矩阵行数</span>
		m<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>  <span class="token comment"># 矩阵列数（默认等于行数）</span>
		out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
		dtype<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
		layout<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
		device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
		requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="4__170"></a>4. 依据概率分布创建</h3> 
<ol><li>生成正态分布</li></ol> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>normal<span class="token punctuation">(</span>mean<span class="token punctuation">,</span> std<span class="token punctuation">,</span> out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
<span class="token comment"># 当mean和std全为标量时，要加一个size</span>
torch<span class="token punctuation">.</span>normal<span class="token punctuation">(</span>mean<span class="token punctuation">,</span> std<span class="token punctuation">,</span> size<span class="token punctuation">,</span> out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
<span class="token comment"># 比如：</span>
torch<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<ol start="2"><li>生成标准正态分布</li></ol> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token operator">*</span>size<span class="token punctuation">,</span> 
			out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
			dtype<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
			layout<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
			device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
			requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>randn_like<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># (mean=0 std=1的)</span>
</code></pre> 
<ol start="3"><li>在区间[0, 1)上生成均匀分布</li></ol> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token operator">*</span>size<span class="token punctuation">,</span>
			out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
			dtype<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
			layout<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
			device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
			requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

torch<span class="token punctuation">.</span>rand_like<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<ol start="4"><li>在区间[low, high)上生成整数均匀分布</li></ol> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span>low<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>
			high<span class="token punctuation">,</span>
			size<span class="token punctuation">,</span>
			out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
			dtype<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
			layout<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
			device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
			requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

torch<span class="token punctuation">.</span>randint_like<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<ol start="5"><li>生成从0到n-1的随机排列</li></ol> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>randperm<span class="token punctuation">(</span>n<span class="token punctuation">,</span>  <span class="token comment"># n为张量长度，常用来生成索引</span>
			out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
			dtype<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
			layout<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
			device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
			requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> 
<ol start="6"><li>以input为概率，生成伯努利分布（0-1分布/两点分布）</li></ol> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>bernoulli<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span>  <span class="token comment"># 概率值</span>
				<span class="token operator">*</span><span class="token punctuation">,</span> 
				generator<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> 
				out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> 
<h2><a id="Tensor_241"></a>Tensor的属性</h2> 
<p>获取张量的形状：tensor.shape<br> 获取张量的数据类型：tensor.dtype<br> 获取存储张量的设备：tensor.device</p> 
<p>关于张量的属性更细致的内容参考 <a href="https://pytorch.org/docs/stable/tensor_attributes.html" rel="nofollow">官方文档 Tensor Attributes API</a></p> 
<h2><a id="Tensor_247"></a>Tensor的操作</h2> 
<h3><a id="1__248"></a>1. 张量拼接与切分</h3> 
<ol><li>将张量按维度dim拼接</li></ol> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>tensors<span class="token punctuation">,</span>  <span class="token comment"># 张量序列</span>
		dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>  <span class="token comment"># 要拼接的维度</span>
		out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> 
<p>比如：</p> 
<pre><code class="prism language-python">t <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
t_0 <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>t<span class="token punctuation">,</span> t<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># ([4, 3]) 在2这个维度上拼接，2+2=4</span>
t_1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>t<span class="token punctuation">,</span> t<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># ([2, 6]) 3+3=6</span>
t_2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>t<span class="token punctuation">,</span> t<span class="token punctuation">,</span> t<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># ([2, 9]) 3+3+3=9</span>
</code></pre> 
<ol start="2"><li>在新创建的维度dim上进行拼接<br> 所有张量大小必须相同</li></ol> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>tensors<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> 
<p>比如：</p> 
<pre><code class="prism language-python">t <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
t_3 <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>t<span class="token punctuation">,</span> t<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># ([2, 3, 2]) 在后面加了一个维度</span>
t_4 <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>t<span class="token punctuation">,</span> t<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># ([2, 2, 3]) 因为已有0维了，原维度往后的全部后移了一维</span>
</code></pre> 
<ol start="3"><li>将张量按维度dim进行平均切分</li></ol> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>chunk<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span>  <span class="token comment"># 要切分的张量</span>
			chunks<span class="token punctuation">,</span>  <span class="token comment"># 要切分的份数</span>
			dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># 要切分的维度</span>
<span class="token comment"># 返回值：张量列表（若不能整除，最后一份张量小于其他张量）</span>
</code></pre> 
<p>比如：</p> 
<pre><code class="prism language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 先把7/3向上取整得到3，前几份每份为3</span>
list_of_tensors <span class="token operator">=</span> torch<span class="token punctuation">.</span>chunk<span class="token punctuation">(</span>a<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> chunks<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment"># ([2, 3]), ([2, 3]), ([2, 1])</span>
<span class="token keyword">for</span> idx<span class="token punctuation">,</span> t <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>list_of_tensors<span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"第{}个张量：{}, shape is {}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>idx<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> t<span class="token punctuation">,</span> t<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<ol start="4"><li>将张量按dim维度切分</li></ol> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span>  <span class="token comment"># 要切分的张量</span>
			split_size_or_sections<span class="token punctuation">,</span>  <span class="token comment"># 为int时，表示每一份的长度；为list时，按list元素切分</span>
			dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
</code></pre> 
<p>比如：</p> 
<pre><code class="prism language-python">t <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
list_of_tensors <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>t<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># ([2, 2]), ([2, 1]), ([2, 2])</span>
<span class="token keyword">for</span> idx<span class="token punctuation">,</span> t <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>list_of_tensors<span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"第{}个张量：{}, shape is {}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>idx<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> t<span class="token punctuation">,</span> t<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span>
list_of_tensors <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>t<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 2+1+1 != 5，会报错</span>
</code></pre> 
<h3><a id="2__313"></a>2. 张量索引</h3> 
<ol><li>在维度dim上，按index索引数据</li></ol> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>index_select<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span>  <span class="token comment"># 要索引的张量</span>
				dim<span class="token punctuation">,</span>  <span class="token comment"># 要索引的维度</span>
				index<span class="token punctuation">,</span>  <span class="token comment"># 要索引数据的序号</span>
				out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
<span class="token comment"># 返回值：依index索引数据拼接的张量</span>
</code></pre> 
<p>比如：</p> 
<pre><code class="prism language-python">t <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># tensor([[4, 5, 0],</span>
<span class="token comment">#												[5, 7, 1],</span>
<span class="token comment">#												[2, 5, 8]])</span>
idx <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">)</span>    <span class="token comment"># 64位整型，必须这样设定</span>
t_select <span class="token operator">=</span> torch<span class="token punctuation">.</span>index_select<span class="token punctuation">(</span>t<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> index<span class="token operator">=</span>idx<span class="token punctuation">)</span>  <span class="token comment"># tensor([[4, 5, 0],															#															  [2, 5, 8]])</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"t:\n{}\nt_select:\n{}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>t<span class="token punctuation">,</span> t_select<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<ol start="2"><li>按mask中的True索引</li></ol> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>masked_select<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span>  <span class="token comment"># 要索引的张量</span>
					mask<span class="token punctuation">,</span>  <span class="token comment"># 与input同形状的布尔类型张量</span>
					out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
<span class="token comment"># 返回值：一维张量</span>
<span class="token comment"># 因为不能确定有多少元素是True，所以不能确定返回的形状，就统一返回一维张量</span>

</code></pre> 
<p>比如：</p> 
<pre><code class="prism language-python">t <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
mask <span class="token operator">=</span> t<span class="token punctuation">.</span>ge<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>  <span class="token comment"># 生成&gt;=5的布尔型张量（形状不变）</span>
<span class="token comment"># ge：大于等于&gt;=   gt: 大于&gt;  le：小于等于&lt;=  lt：小于&lt;</span>
t_select <span class="token operator">=</span> torch<span class="token punctuation">.</span>masked_select<span class="token punctuation">(</span>t<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>  <span class="token comment"># tensor([5, 5, 7, 5, 8])</span>
</code></pre> 
<ol start="3"><li>类似 numpy 的索引<br> 选择第一行：tensor[0]<br> 选择第一列：tensor[:, 0]<br> 选择最后一列：tensor[…, -1]<br> 将第二列赋值为0：tensor[:, 1] = 0</li></ol> 
<h3><a id="3__358"></a>3. 张量变换</h3> 
<ol><li>torch.reshape()</li></ol> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span>   <span class="token comment"># input：要变换的张量</span>
			  shape<span class="token punctuation">)</span>  <span class="token comment"># shape：新张量的形状</span>
</code></pre> 
<p>功能：变换张量形状<br> 注意事项：当张量在内存中是连续时，新张量与input共享数据内存</p> 
<pre><code class="prism language-python">    t <span class="token operator">=</span> torch<span class="token punctuation">.</span>randperm<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">)</span>
    t_reshape <span class="token operator">=</span> torch<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>t<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment"># 大小要与之前的匹配，否则会报错</span>
    <span class="token comment"># t_reshape = torch.reshape(t, (-1, 2, 2))  # -1的意思是不需要关心这个维度，根据别的维度计算而来</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"t:{}\nt_reshape:\n{}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>t<span class="token punctuation">,</span> t_reshape<span class="token punctuation">)</span><span class="token punctuation">)</span>

    t<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1024</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"t:{}\nt_reshape:\n{}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>t<span class="token punctuation">,</span> t_reshape<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"t.data 内存地址:{}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token builtin">id</span><span class="token punctuation">(</span>t<span class="token punctuation">.</span>data<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># id()是获取内存地址</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"t_reshape.data 内存地址:{}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token builtin">id</span><span class="token punctuation">(</span>t_reshape<span class="token punctuation">.</span>data<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<ol start="2"><li>torch.transpose()</li></ol> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span>  <span class="token comment"># input：要变换的张量</span>
				dim0<span class="token punctuation">,</span>  <span class="token comment"># dim0、dim1：要交换的维度</span>
				dim1<span class="token punctuation">)</span>
</code></pre> 
<p>功能：交换张量的两个维度<br> 例如：</p> 
<pre><code class="prism language-python">    <span class="token comment"># torch.transpose</span>
    t <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    t_transpose <span class="token operator">=</span> torch<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>t<span class="token punctuation">,</span> dim0<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> dim1<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>    <span class="token comment"># c*h*w     h*w*c</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"t shape:{}\nt_transpose shape: {}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>t<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> t_transpose<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>经常在图像前期的预处理中使用，比如读取进来的图像可能是c<em>h</em>w的，把它变换成h<em>w</em>c，把channel放到最后<br> 3. torch.t()</p> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
</code></pre> 
<p>功能：二维张量的转置，对矩阵而言，等价于</p> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
</code></pre> 
<ol start="4"><li>torch.squeeze()</li></ol> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span>
			  dim<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
			  out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> 
<p>功能：压缩长度为1的维度（轴）<br> dim若为None，移除所有长度为1的轴；若指定维度，当且仅当该轴长度为1时，可以被移除<br> 例如：</p> 
<pre><code class="prism language-python">    t <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    t_sq <span class="token operator">=</span> torch<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>t<span class="token punctuation">)</span>  <span class="token comment"># 移除后形状变为([2, 3])</span>
    t_0 <span class="token operator">=</span> torch<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>t<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># ([2, 3, 1])</span>
    t_1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>t<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># ([1, 2, 3, 1])，由于第一个轴的长度不是1，所以不会被移除</span>
</code></pre> 
<ol start="6"><li>torch.unsqueeze()</li></ol> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span>
				dim<span class="token punctuation">,</span>  <span class="token comment"># 扩展的维度</span>
				out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> 
<p>功能：依据dim扩展维度，dim上的长度为1<br> 比如要扩展形状为([2, 3])的张量，dim的取值范围使[-3, 2]，不能在3及以上的维度扩展。</p> 
<h3><a id="4_GPU_432"></a>4. 将张量移动到GPU</h3> 
<pre><code class="prism language-python"><span class="token comment"># We move our tensor to the GPU if available</span>
<span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    tensor <span class="token operator">=</span> tensor<span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>
</code></pre> 
<h2><a id="Tensor_439"></a>Tensor的数学运算</h2> 
<h3><a id="1__440"></a>1. 加减乘除</h3> 
<ul><li>有如下这些常用函数：</li></ul> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>add<span class="token punctuation">(</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>addcdiv<span class="token punctuation">(</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>addcmul<span class="token punctuation">(</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>sub<span class="token punctuation">(</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>div<span class="token punctuation">(</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>mul<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<ul><li>前三个有什么区别呢？</li></ul> 
<ol><li>torch.add()</li></ol> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>add<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span>  <span class="token comment"># input：第一个张量</span>
		  alpha<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>  <span class="token comment"># alpha：乘项因子</span>
		  other<span class="token punctuation">,</span>  <span class="token comment"># other：第二个张量</span>
		  out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> 
<p>功能：逐元素计算<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         i 
        
       
         n 
        
       
         p 
        
       
         u 
        
       
         t 
        
       
         + 
        
       
         a 
        
       
         l 
        
       
         p 
        
       
         h 
        
       
         a 
        
       
         ∗ 
        
       
         o 
        
       
         t 
        
       
         h 
        
       
         e 
        
       
         r 
        
       
      
        input+alpha*other 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.854em; vertical-align: -0.1944em;"></span><span class="mord mathnormal">in</span><span class="mord mathnormal">p</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;"></span><span class="mord mathnormal">a</span><span class="mord mathnormal">lp</span><span class="mord mathnormal">ha</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal">o</span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal" style="margin-right: 0.0278em;">er</span></span></span></span></span>，计算的是一个先乘后加的运算</p> 
<ol start="2"><li>torch.addcdiv() 实现<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          o 
         
        
          u 
         
         
         
           t 
          
         
           i 
          
         
        
          = 
         
        
          i 
         
        
          n 
         
        
          p 
         
        
          u 
         
         
         
           t 
          
         
           i 
          
         
        
          + 
         
        
          v 
         
        
          a 
         
        
          l 
         
        
          u 
         
        
          e 
         
        
          ∗ 
         
         
          
          
            t 
           
          
            e 
           
          
            n 
           
          
            s 
           
          
            o 
           
          
            r 
           
           
           
             1 
            
           
             i 
            
           
          
          
          
            t 
           
          
            e 
           
          
            n 
           
          
            s 
           
          
            o 
           
          
            r 
           
           
           
             2 
            
           
             i 
            
           
          
         
        
       
         out_i = input_i + value * \frac{tensor1_i}{tensor2_i} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7651em; vertical-align: -0.15em;"></span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.854em; vertical-align: -0.1944em;"></span><span class="mord mathnormal">in</span><span class="mord mathnormal">p</span><span class="mord mathnormal">u</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">v</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right: 0.0197em;">l</span><span class="mord mathnormal">u</span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1.3063em; vertical-align: -0.4451em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8612em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight" style="margin-right: 0.0278em;">sor</span><span class="mord mtight"><span class="mord mtight">2</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3281em;"><span class="" style="top: -2.357em; margin-left: 0em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em;"><span class=""></span></span></span></span></span></span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.4101em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight" style="margin-right: 0.0278em;">sor</span><span class="mord mtight"><span class="mord mtight">1</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3281em;"><span class="" style="top: -2.357em; margin-left: 0em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em;"><span class=""></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.4451em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></li><li>torch.addcmul() 实现<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          o 
         
        
          u 
         
         
         
           t 
          
         
           i 
          
         
        
          = 
         
        
          i 
         
        
          n 
         
        
          p 
         
        
          u 
         
         
         
           t 
          
         
           i 
          
         
        
          + 
         
        
          v 
         
        
          a 
         
        
          l 
         
        
          u 
         
        
          e 
         
        
          ∗ 
         
        
          t 
         
        
          e 
         
        
          n 
         
        
          s 
         
        
          o 
         
        
          r 
         
         
         
           1 
          
         
           i 
          
         
        
          ∗ 
         
        
          t 
         
        
          e 
         
        
          n 
         
        
          s 
         
        
          o 
         
        
          r 
         
         
         
           2 
          
         
           i 
          
         
        
       
         out_i = input_i + value*tensor1_i*tensor2_i 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7651em; vertical-align: -0.15em;"></span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.854em; vertical-align: -0.1944em;"></span><span class="mord mathnormal">in</span><span class="mord mathnormal">p</span><span class="mord mathnormal">u</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">v</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right: 0.0197em;">l</span><span class="mord mathnormal">u</span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.7944em; vertical-align: -0.15em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal" style="margin-right: 0.0278em;">sor</span><span class="mord"><span class="mord">1</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.7944em; vertical-align: -0.15em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal" style="margin-right: 0.0278em;">sor</span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span></li></ol> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>addcmul<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span>
			  value<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
			  tensor1<span class="token punctuation">,</span>
			  tensor2<span class="token punctuation">,</span>
			  out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> 
<ul><li>关于乘法运算，@和torch.matmul()功能一样，*和torch.mul()功能一样</li></ul> 
<pre><code class="prism language-python"><span class="token comment"># This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value</span>
<span class="token comment"># ``tensor.T`` returns the transpose of a tensor</span>
y1 <span class="token operator">=</span> tensor @ tensor<span class="token punctuation">.</span>T
y2 <span class="token operator">=</span> tensor<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>tensor<span class="token punctuation">.</span>T<span class="token punctuation">)</span>

y3 <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand_like<span class="token punctuation">(</span>y1<span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> tensor<span class="token punctuation">.</span>T<span class="token punctuation">,</span> out<span class="token operator">=</span>y3<span class="token punctuation">)</span>


<span class="token comment"># This computes the element-wise product. z1, z2, z3 will have the same value</span>
z1 <span class="token operator">=</span> tensor <span class="token operator">*</span> tensor
z2 <span class="token operator">=</span> tensor<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>

z3 <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand_like<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> tensor<span class="token punctuation">,</span> out<span class="token operator">=</span>z3<span class="token punctuation">)</span>
</code></pre> 
<ul><li>单元素张量可以通过item()转化成Python数值</li></ul> 
<pre><code class="prism language-python">tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
agg <span class="token operator">=</span> tensor<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
agg_item <span class="token operator">=</span> agg<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>agg_item<span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token punctuation">(</span>agg_item<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<ul><li>In-place操作<br> 函数的后缀是下划线_，表示这个函数是就地操作的（结果存储到操作数中）<br> 例如：<code>x.t_()</code>、<code>x.copy_()</code>、<code>x.add_()</code><br> 就地操作可以节省一些内存，但在计算导数时可能会出现问题，因为会立即丢失历史计算数据，所以不推荐使用。</li></ul> 
<h3><a id="2__500"></a>2. 对、指、幂函数</h3> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>log10<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>log2<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="3__510"></a>3. 三角函数</h3> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>acos<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>cosh<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>cos<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>asin<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>atan<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>atan2<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> other<span class="token punctuation">,</span> out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> 
<p>关于张量更细致的内容参考 <a href="https://pytorch.org/docs/stable/tensors.html" rel="nofollow">官方文档 torch.Tensor API</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/58704cf0a7cf616d6c3192865b0a07e8/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">常用快捷键整理（centos7、Notepad&#43;&#43;、Idea、Excel）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/aabd70063dfbc886d110221ea10eb843/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">windows中解压Linux中的tar.gz.part分卷压缩包</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>