<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>利用python版tensorRT导出engine【以yolov5为例】 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="利用python版tensorRT导出engine【以yolov5为例】" />
<meta property="og:description" content="环境说明：
tensorRT:8.2.4.2
CUDA:10.2
pytorch:1.7
显卡：NVIDIA 1650
Windows10
python 3.7
另一篇文章中写过C&#43;&#43;版的trt推理。本篇文章是python版本的trt yolov5推理。
构建engine一般有两种方式。
方式1：torch模型-&gt;wts(序列化网络)-&gt;engine-&gt;推理
方式2：torch模型-&gt;onnx-&gt;engine-&gt;推理
第一种方式如果网络结构简单，在定义网络构建engine的时候还可以，但网络复杂的情况就麻烦了，写网络的时候还容易出错。
第二种方式也是很多人常用的方法，转onnx再转engine。转onnx就比较容易了，而转engine一般有两种方式，第一种是trt官方自带的方式，在你trt文件下的bin目录下有个trtexec.exe的文件，执行命令就可以将onnx转engine。而第二种python版trt自带工具，这也是本文要介绍的。
我这里的代码是用的v5 6.1代码，因为6.1以及之后版本的export.py中有engine格式的导出。
git clone https://github.com/ultralytics/yolov5
我们可以看一下官方提供的yolov5s.pt中都包含什么内容：
dict_keys([&#39;epoch&#39;, &#39;best_fitness&#39;, &#39;model&#39;, &#39;ema&#39;, &#39;updates&#39;, &#39;optimizer&#39;, &#39;wandb_id&#39;, &#39;date&#39;])
可以看到上述pt文件中包含了这些key值，其中的model就是我们要的，而且需要注意的是这个model不仅含有网络权重信息，还包含了整个网络结构【如果你想把其他网络转onnx，也需要主要必须torch保存的是整个网络】 目录
导出onnx
导出engine
导出onnx 执行下面的命令就可以得到我们的onnx模型。
python export.py --weights yolov5s.pt --include onnx 这里附上导出onnx的代码。
@try_export def export_onnx(model, im, file, opset, dynamic, simplify, prefix=colorstr(&#39;ONNX:&#39;)): # YOLOv5 ONNX export check_requirements(&#39;onnx&#39;) import onnx LOGGER.info(f&#39;\n{prefix} starting export with onnx {onnx.__version__}...&#39;) f = file.with_suffix(&#39;.onnx&#39;) output_names = [&#39;output0&#39;, &#39;output1&#39;] if isinstance(model, SegmentationModel) else [&#39;output0&#39;] if dynamic: dynamic = {&#39;images&#39;: {0: &#39;batch&#39;, 2: &#39;height&#39;, 3: &#39;width&#39;}} # shape(1,3,640,640) if isinstance(model, SegmentationModel): dynamic[&#39;output0&#39;] = {0: &#39;batch&#39;, 1: &#39;anchors&#39;} # shape(1,25200,85) dynamic[&#39;output1&#39;] = {0: &#39;batch&#39;, 2: &#39;mask_height&#39;, 3: &#39;mask_width&#39;} # shape(1,32,160,160) elif isinstance(model, DetectionModel): dynamic[&#39;output0&#39;] = {0: &#39;batch&#39;, 1: &#39;anchors&#39;} # shape(1,25200,85) torch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/c54010aacae7351457249cf97808e533/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-10-13T19:13:03+08:00" />
<meta property="article:modified_time" content="2022-10-13T19:13:03+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">利用python版tensorRT导出engine【以yolov5为例】</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>环境说明：</p> 
<p>tensorRT:8.2.4.2</p> 
<p>CUDA:10.2</p> 
<p>pytorch:1.7</p> 
<p>显卡：NVIDIA 1650</p> 
<p>Windows10</p> 
<p>python 3.7</p> 
<hr> 
<p></p> 
<p>另一篇文章中写过C++版的trt推理。本篇文章是python版本的trt yolov5推理。</p> 
<p>构建engine一般有两种方式。</p> 
<p><strong>方式1：torch模型-&gt;wts(序列化网络)-&gt;engine-&gt;推理</strong></p> 
<p><strong>方式2：torch模型-&gt;onnx-&gt;engine-&gt;推理</strong></p> 
<p>第一种方式如果网络结构简单，在定义网络构建engine的时候还可以，但网络复杂的情况就麻烦了，写网络的时候还容易出错。</p> 
<p>第二种方式也是很多人常用的方法，转onnx再转engine。转onnx就比较容易了，而转engine一般有两种方式，第一种是trt官方自带的方式，在你trt文件下的bin目录下有个trtexec.exe的文件，执行命令就可以将onnx转engine。而第二种python版trt自带工具，这也是本文要介绍的。</p> 
<hr> 
<p>我这里的代码是用的v5 6.1代码，因为6.1以及之后版本的export.py中有engine格式的导出。</p> 
<blockquote> 
 <p>git clone https://github.com/ultralytics/yolov5</p> 
</blockquote> 
<p>我们可以看一下官方提供的yolov5s.pt中都包含什么内容：</p> 
<blockquote> 
 <p>dict_keys(['epoch', 'best_fitness', 'model', 'ema', 'updates', 'optimizer', 'wandb_id', 'date'])</p> 
</blockquote> 
<p>可以看到上述pt文件中包含了这些key值，其中的model就是我们要的，而且需要注意的是这个model不仅含有网络权重信息，还包含了整个网络结构【如果你想把其他网络转onnx，也需要主要必须torch保存的是整个网络】 </p> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="%E5%AF%BC%E5%87%BAonnx-toc" style="margin-left:0px;"><a href="#%E5%AF%BC%E5%87%BAonnx" rel="nofollow">导出onnx</a></p> 
<p id="%C2%A0%E5%AF%BC%E5%87%BAengine-toc" style="margin-left:0px;"><a href="#%C2%A0%E5%AF%BC%E5%87%BAengine" rel="nofollow"> 导出engine</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="%E5%AF%BC%E5%87%BAonnx">导出onnx</h2> 
<p>执行下面的命令就可以得到我们的onnx模型。</p> 
<pre><code class="language-bash">python export.py --weights yolov5s.pt --include onnx</code></pre> 
<p>这里附上导出onnx的代码。</p> 
<pre><code class="language-python">@try_export
def export_onnx(model, im, file, opset, dynamic, simplify, prefix=colorstr('ONNX:')):
    # YOLOv5 ONNX export
    check_requirements('onnx')
    import onnx

    LOGGER.info(f'\n{prefix} starting export with onnx {onnx.__version__}...')
    f = file.with_suffix('.onnx')

    output_names = ['output0', 'output1'] if isinstance(model, SegmentationModel) else ['output0']
    if dynamic:
        dynamic = {'images': {0: 'batch', 2: 'height', 3: 'width'}}  # shape(1,3,640,640)
        if isinstance(model, SegmentationModel):
            dynamic['output0'] = {0: 'batch', 1: 'anchors'}  # shape(1,25200,85)
            dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)
        elif isinstance(model, DetectionModel):
            dynamic['output0'] = {0: 'batch', 1: 'anchors'}  # shape(1,25200,85)

    torch.onnx.export(
        model.cpu() if dynamic else model,  # --dynamic only compatible with cpu
        im.cpu() if dynamic else im,
        f,
        verbose=False,
        opset_version=opset,
        do_constant_folding=True,
        input_names=['images'],
        output_names=output_names,
        dynamic_axes=dynamic or None)

    # Checks
    model_onnx = onnx.load(f)  # load onnx model
    onnx.checker.check_model(model_onnx)  # check onnx model

    # Metadata
    d = {'stride': int(max(model.stride)), 'names': model.names}
    for k, v in d.items():
        meta = model_onnx.metadata_props.add()
        meta.key, meta.value = k, str(v)
    onnx.save(model_onnx, f)

    # Simplify
    if simplify:
        try:
            cuda = torch.cuda.is_available()
            check_requirements(('onnxruntime-gpu' if cuda else 'onnxruntime', 'onnx-simplifier&gt;=0.4.1'))
            import onnxsim

            LOGGER.info(f'{prefix} simplifying with onnx-simplifier {onnxsim.__version__}...')
            model_onnx, check = onnxsim.simplify(model_onnx)
            assert check, 'assert check failed'
            onnx.save(model_onnx, f)
        except Exception as e:
            LOGGER.info(f'{prefix} simplifier failure: {e}')
    return f, model_onnx</code></pre> 
<p>export_onnx函数中，model就是我们加载的torch网络，im是一个输入样例 ，file为yolov5s.pt的路径[我这里为F:/yolov5/yolov5s.pt]。opset就是版本这里是12，dynamic就说动态输入【我这里没开启】。</p> 
<p>output_names是获取model的结点名字，由于这里是目标检测不是图像分割，因此仅有一个output，取名为output0。</p> 
<p>这里需要注意一点的是，明明v5有三个head，为什么这里仅一个输出，如果你去看<span style="color:#ed7976;">models/yolo.py</span>中的Detect可以看到下面的代码，<strong>在export模型下会把三个输出拼接为1个</strong>。</p> 
<pre><code class="language-python"># 如果export 为True，返回的输出是三个head合并为1个。
return x if self.training else (torch.cat(z, 1),) if self.export else (torch.cat(z, 1), x)</code></pre> 
<pre><code class="language-python">output_names = ['output0', 'output1'] if isinstance(model, SegmentationModel) else ['output0']</code></pre> 
<p> 下面这部分代码是onnx导出的核心代码，这里需要注意一下这里需要传入<strong>输入(input_names)输出结点(output_names)</strong>。【这里的结点名不要随意更改，因为后面还会用到】</p> 
<pre><code class="language-python">torch.onnx.export(
        model.cpu() if dynamic else model,  # --dynamic only compatible with cpu
        im.cpu() if dynamic else im,
        f,
        verbose=False,
        opset_version=opset,
        do_constant_folding=True,
        input_names=['images'],
        output_names=output_names,
        dynamic_axes=dynamic or None)</code></pre> 
<p>下面的图就onnx可视化，images就是我们前面定义好的输入结点。 </p> 
<p class="img-center"><img alt="" height="512" src="https://images2.imgbox.com/55/ed/HZA9SmQs_o.png" width="219"></p> 
<p>下面这一部分就是输出部分，输出是三个头进行了整合，结点为output0，shape[1,25200,85]。这里的25200=80 * 80 *3 + 40 * 40 *3 + 20* 20 *3【3是anchors】，85就是80个类+(center_x,cente_y,w,h,conf) </p> 
<p class="img-center"><img alt="" height="357" src="https://images2.imgbox.com/ac/9a/6gexFkJk_o.png" width="609"></p> 
<hr> 
<p>在介绍导出engine过程需要先介绍一下会遇到的相关术语。</p> 
<p><strong>1.建立logger:日志记录器</strong></p> 
<p><strong>2.建立Builder:网络元数据</strong></p> 
<p>        用于<strong>搭建网络的入口</strong>，网络的TRT内部表示以及可执行程序引擎都是由该对象的成员方法生成   </p> 
<blockquote> 
 <p>常用成员函数：</p> 
 <p>builder.max_batch_size = 256    # 指定最大batch_size(static shape模型下使用)</p> 
 <p><s>builder.max_workspace_size = 1 &lt;&lt;30</s>  # 指定最大可用显存</p> 
 <p><s>builder.fp16_model = True/Flase</s>  # 开启或者关闭FP16模式</p> 
 <p><s>builder.int8_model = True/Flase</s>  # # 开启或者关闭int8模式</p> 
 <p>注意：(上面的成员变量即将弃用，上面这些设置将被放置在Config中，而<strong>builder仅仅是作为一个构建引擎的入口</strong>)</p> 
</blockquote> 
<p><strong>3.建立BuilderConfig:网络元数据的选项</strong></p> 
<p>        <strong>负责配置模型的一些参数</strong>，比如是否开启FP16，int8模型等。</p> 
<p>        通常的语句为：config = builder.create_builder_config()</p> 
<blockquote> 
 <p>常用的成员函数：</p> 
 <p>config.max_workspace_size = 1&lt;&lt;30  # <strong>指定构建期间可用显存</strong>(单位：Byte)</p> 
 <p>config.flag = ..  # 设置标志位，如1&lt;&lt;int(trt.BuilderFlag.FP16)</p> 
</blockquote> 
<p><strong>4.创建Network:计算图内容</strong></p> 
<p>        网络主体，使用api搭建网络过程中，将不断的向其中添加一些层，并标记网络的输入输出节点(这个可能大家在使用C++构建engine的时候遇到过，也就是wts-&gt;engine的过程)。不过这里还提供了其他的方法，可以<strong>采用解析器Parser加载来自onnx文件中的网络(推荐使用)</strong>，就不用一层一层手工添加。</p> 
<p>        语法：network = builder.create_network()</p> 
<blockquote> 
 <p>常用方法：</p> 
 <p>network.add_input('tensor',trt.float32,(3,4,5))  # 标记网络输入张量</p> 
 <p>convLayer = network.add_convolution_nd(XXX)  # 添加各种网络层</p> 
 <p>network.mark_output(convLayer.get_output(0)) # 标记网络输出张量</p> 
 <p></p> 
 <p>常用获取网络信息的成员：</p> 
 <p>network.name/network.num_layers/network.num_inputs/network.num_outputs</p> 
</blockquote> 
<p>network是计算图在TRT中的具体描述，由builder生成，在使用TRT原生api搭建网络的workflow中，我们需要不断地想network中添加一些层，并标记network的输入输出张量，而在<strong>使用parser导入onnx模型的workflow中，一旦模型解析完成，network的内容就会被自动填入</strong>。<br>  </p> 
<p><strong>5.生成SerializedNetwork:网络的TRT内部表示 </strong></p> 
<p>        模型网络在TRT中的内部表示，可用它<strong>生成可执行的推理引擎</strong>或者把它序列化保存为文件，方便以后读取和使用</p> 
<p></p> 
<hr> 
<h2 id="%C2%A0%E5%AF%BC%E5%87%BAengine"> 导出engine</h2> 
<p>导出engine代码如下所示。</p> 
<pre><code class="language-python"># engine TRT 必须在GPU上
@try_export
def export_engine(model, im, file, half, dynamic, simplify, workspace=4, verbose=False, prefix=colorstr('TensorRT:')):
    # YOLOv5 TensorRT export https://developer.nvidia.com/tensorrt
    # 首先判断一下im是不是在GPU上
    assert im.device.type != 'cpu', 'export running on CPU but must be on GPU, i.e. `python export.py --device 0`'
    try:
        import tensorrt as trt
    except Exception:
        if platform.system() == 'Linux':  # 判断操作系统
            check_requirements('nvidia-tensorrt', cmds='-U --index-url https://pypi.ngc.nvidia.com')
        import tensorrt as trt

    # 判断trt版本
    if trt.__version__[0] == '7':  # TensorRT 7 handling https://github.com/ultralytics/yolov5/issues/6012
        grid = model.model[-1].anchor_grid
        model.model[-1].anchor_grid = [a[..., :1, :1, :] for a in grid]
        export_onnx(model, im, file, 12, dynamic, simplify)  # opset 12
        model.model[-1].anchor_grid = grid
    else:  # TensorRT &gt;= 8
        check_version(trt.__version__, '8.0.0', hard=True)  # require tensorrt&gt;=8.0.0
        # 先转onnx
        export_onnx(model, im, file, 12, dynamic, simplify)  # opset 12
    onnx = file.with_suffix('.onnx')  # 获取权重名

    LOGGER.info(f'\n{prefix} starting export with TensorRT {trt.__version__}...')
    assert onnx.exists(), f'failed to export ONNX file: {onnx}'
    f = file.with_suffix('.engine')  # TensorRT engine file
    # 记录trt转engine日志
    logger = trt.Logger(trt.Logger.INFO)
    if verbose:
        logger.min_severity = trt.Logger.Severity.VERBOSE
    # 1.builder构造，记录日志
    builder = trt.Builder(logger)
    # 2.builder.config建立
    config = builder.create_builder_config()
    # 3.workspace  workspace * 1 &lt;&lt; 30 表示将workspace * 1 二进制左移30位后的10进制
    config.max_workspace_size = workspace * 1 &lt;&lt; 30
    # config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, workspace &lt;&lt; 30)  # fix TRT 8.4 deprecation notice

    # 4.定义Network并加载onnx解析器
    flag = (1 &lt;&lt; int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
    network = builder.create_network(flag)
    parser = trt.OnnxParser(network, logger)
    
    if not parser.parse_from_file(str(onnx)):
        raise RuntimeError(f'failed to load ONNX file: {onnx}')
    # 5.获得网络输入输出
    inputs = [network.get_input(i) for i in range(network.num_inputs)]
    outputs = [network.get_output(i) for i in range(network.num_outputs)]
    # 下面的只是在log中打印input和output 的name和shape以及数据类型
    for inp in inputs:
        LOGGER.info(f'{prefix} input "{inp.name}" with shape{inp.shape} {inp.dtype}')
    for out in outputs:
        LOGGER.info(f'{prefix} output "{out.name}" with shape{out.shape} {out.dtype}')
    # 判断动态输入
    if dynamic:
        if im.shape[0] &lt;= 1:
            LOGGER.warning(f"{prefix} WARNING ⚠️ --dynamic model requires maximum --batch-size argument")
        profile = builder.create_optimization_profile()
        for inp in inputs:
            profile.set_shape(inp.name, (1, *im.shape[1:]), (max(1, im.shape[0] // 2), *im.shape[1:]), im.shape)
        config.add_optimization_profile(profile)

    LOGGER.info(f'{prefix} building FP{16 if builder.platform_has_fast_fp16 and half else 32} engine as {f}')
    # 判断是否支持FP16推理
    if builder.platform_has_fast_fp16 and half:
        config.set_flag(trt.BuilderFlag.FP16)
    # build engine 文件的写入  这里的f是前面定义的engine文件
    with builder.build_engine(network, config) as engine, open(f, 'wb') as t:
        # 序列化model
        t.write(engine.serialize())
    return f, None</code></pre> 
<p> </p> 
<p> 构建engine关键步骤如下：</p> 
<p><strong>1.builder构造。</strong></p> 
<p>其中的logger是记录trt转engine时的log信息。</p> 
<p>这个步骤是构建引擎的核心部分。</p> 
<pre><code class="language-python"># 记录trt转engine日志
logger = trt.Logger(trt.Logger.INFO)
builder = trt.Builder(logger)</code></pre> 
<p>builder中的属性内容下 并输出下面的log内容，主要是一些内存上面的使用初始等。</p> 
<p><img alt="" height="186" src="https://images2.imgbox.com/e3/14/x9jxd9oa_o.png" width="451"> </p> 
<blockquote> 
 <p>[10/13/2022-17:40:17] [TRT] [I] [MemUsageChange] Init CUDA: CPU +285, GPU +0, now: CPU 7095, GPU 1776 (MiB)<br> [10/13/2022-17:40:17] [TRT] [I] [MemUsageSnapshot] Begin constructing builder kernel library: CPU 7129 MiB, GPU 1776 MiB<br> [10/13/2022-17:40:18] [TRT] [I] [MemUsageSnapshot] End constructing builder kernel library: CPU 7227 MiB, GPU 1810 MiB </p> 
</blockquote> 
<p><strong>2.builder.config构造</strong></p> 
<pre><code class="language-python">config = builder.create_builder_config()</code></pre> 
<p><strong> 3.workspace分配</strong></p> 
<pre><code class="language-python">    # 3.workspace  workspace * 1 &lt;&lt; 30 表示将workspace * 1 二进制左移30位后的10进制
    config.max_workspace_size = workspace * 1 &lt;&lt; 30</code></pre> 
<p><strong> 4.网络定义并加载onnx解析器(写入网络)</strong></p> 
<p>网络的创建主要时调用builder中的create_network函数。</p> 
<p>这一部分就是创建Network</p> 
<pre><code class="language-python">flag = (1 &lt;&lt; int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
network = builder.create_network(flag)
parser = trt.OnnxParser(network, logger)</code></pre> 
<p>此时的network还是一个创建的空网络，通过下面的各属性也能看出来，后续我们会把v5网络写入。 </p> 
<p><img alt="" height="211" src="https://images2.imgbox.com/4e/4f/Epa4X4ME_o.png" width="833"> </p> 
<p> parser = trt.OnnxParser(network,logger)是加载onnx解析器。</p> 
<p>加载一行此时的network变为下面这样，可以看到num_inputs和num_layers以及num_outputs均有改变：</p> 
<p><img alt="" height="175" src="https://images2.imgbox.com/48/79/hwyUcfe3_o.png" width="839"></p> 
<p> </p> 
<p><strong> 5.获得网络的输入输出</strong></p> 
<pre><code class="language-python">inputs = [network.get_input(i) for i in range(network.num_inputs)]
outputs = [network.get_output(i) for i in range(network.num_outputs)]</code></pre> 
<p><strong> 6.判断是否支持FP16推理</strong></p> 
<pre><code class="language-python">    if builder.platform_has_fast_fp16 and half:
        config.set_flag(trt.BuilderFlag.FP16)</code></pre> 
<p><strong> 7.engine写入</strong></p> 
<p>写入engine文件需要调用前面定义的builder.build_engine函数，这里会传入两个参数，第一个就是我们定义好的网络，第二个就是针对网络的相关配置config【比如是否开发FP16等】。写入的网络也是序列化的。</p> 
<p>实际就是生成网络TRT的内部表示。</p> 
<pre><code class="language-python">    # build engine 文件的写入  这里的f是前面定义的engine文件
    with builder.build_engine(network, config) as engine, open(f, 'wb') as t:
        # 序列化model
        t.write(engine.serialize())</code></pre> 
<hr> 
<p> </p> 
<p><strong> 注：本文会在后续学习中不定时更新。</strong></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/bc42db15cd306c8b6e6b8499c39abf78/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">科目需要分配到成本对象</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/760d5be593b8cd2999f0456456d75d39/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">iOS WebView白屏问题&amp;&amp;解决方案</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>