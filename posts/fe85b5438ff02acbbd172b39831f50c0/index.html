<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>enable anomaly detection to find the operation that failed to compute its gradient, with torch.autog - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="enable anomaly detection to find the operation that failed to compute its gradient, with torch.autog" />
<meta property="og:description" content="关于pytorch中多个backward出现的问题：enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly True. 在执行代码中包含两个方向传播(backward)时，可能会出现这种问题：
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [10, 1]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True). 什么情况下会出现这种问题，我们先构建一个场景：
import torch from torch import nn as nn from torch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/fe85b5438ff02acbbd172b39831f50c0/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-26T12:49:36+08:00" />
<meta property="article:modified_time" content="2022-07-26T12:49:36+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">enable anomaly detection to find the operation that failed to compute its gradient, with torch.autog</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>关于pytorch中多个backward出现的问题：enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly True.</h4> 
</div> 
<p></p> 
<p>在执行代码中包含两个<strong>方向传播</strong>(backward)时，可能会出现这种问题：</p> 
<pre><code>RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [10, 1]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
</code></pre> 
<p>什么情况下会出现这种问题，我们先构建一个<strong>场景</strong>：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn <span class="token keyword">as</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> functional <span class="token keyword">as</span> F
<span class="token keyword">from</span> torch <span class="token keyword">import</span> optim
</code></pre> 
<p>为了<strong>简化问题</strong>，构建两个<strong>相同的</strong>神经网络：</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Net_1</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net_1<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>linear_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>linear_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>linear_2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>x<span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

<span class="token keyword">class</span> <span class="token class-name">Net_2</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net_2<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>linear_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>linear_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>linear_2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>x<span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x
</code></pre> 
<p>算法<strong>执行流程</strong>：<br> 定义模型<code>Net_1,Net_2</code>、两个模型对应的优化器(Optimizer)<code>optimizer_n1,optimizer_n2</code>，以及损失函数<code>criterion</code>：</p> 
<pre><code class="prism language-python">n_1 <span class="token operator">=</span> Net_1<span class="token punctuation">(</span><span class="token punctuation">)</span>
n_2 <span class="token operator">=</span> Net_2<span class="token punctuation">(</span><span class="token punctuation">)</span>

optimizer_n1 <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>n_1<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span>
optimizer_n2 <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>n_2<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>执行过程如下：</p> 
<pre><code class="prism language-python"><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    y <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> x

    pred_n1 <span class="token operator">=</span> n_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    optimizer_n1<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss_n1 <span class="token operator">=</span> criterion<span class="token punctuation">(</span>y<span class="token punctuation">,</span>pred_n1<span class="token punctuation">)</span>
    loss_n1<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer_n1<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

    pred_n2 <span class="token operator">=</span> n_2<span class="token punctuation">(</span>pred_n1<span class="token punctuation">)</span>
    optimizer_n2<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss_n2 <span class="token operator">=</span> criterion<span class="token punctuation">(</span>y<span class="token punctuation">,</span>pred_n2<span class="token punctuation">)</span>
    loss_n2<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer_n2<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>注意的点：该执行过程的特点是，第一个神经网络的<code>pred_n1</code>，它也参与了第二个神经网络的反向传播的过程。</p> 
<p>我们知道的是，<code>loss_n1.backward()</code>操作在执行后，计算节点被保存了，但是计算图结构被<strong>释放掉了</strong>，导致<strong>第二个损失函数</strong>进行<strong>反向传播过程中</strong>需要使用<strong>第一次反向传播的<mark>计算图结构</mark>失败</strong>。</p> 
<p>至此，我们在backward中使用<code>retain_graph=True</code>来保存它的计算图结构：<br> 这里有一<strong>小细节</strong>：<br> 实际上<code>loss_n1,loss_n2</code>的<code>backward</code>都可以添加参数<code>retain_graph=True</code>，我们之所以只在第一个里面添加，是因为<strong>如果第二个也加了，本次for循环中的计算图结构就堆积在了内存中，释放不掉，对内存是有负担的。所以一般情况下<mark>本次for循环的计算图使用完后，我们给它释放掉</mark></strong>。</p> 
<pre><code class="prism language-python"><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    y <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> x

    pred_n1 <span class="token operator">=</span> n_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    optimizer_n1<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss_n1 <span class="token operator">=</span> criterion<span class="token punctuation">(</span>y<span class="token punctuation">,</span>pred_n1<span class="token punctuation">)</span>
    loss_n1<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>retain_graph<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    optimizer_n1<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

    pred_n2 <span class="token operator">=</span> n_2<span class="token punctuation">(</span>pred_n1<span class="token punctuation">)</span>
    optimizer_n2<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss_n2 <span class="token operator">=</span> criterion<span class="token punctuation">(</span>y<span class="token punctuation">,</span>pred_n2<span class="token punctuation">)</span>
    loss_n2<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer_n2<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>修改了这个细节之后，重新运行代码：<br> <strong><mark>仍然在出错</mark></strong>。<br> 这次出错的问题，就是标题的问题：<br> 首先，按照它的要求，执行一次<code>torch.autograd.set_detect_anomaly(True)</code>：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn <span class="token keyword">as</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> functional <span class="token keyword">as</span> F
<span class="token keyword">from</span> torch <span class="token keyword">import</span> optim

torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>set_detect_anomaly<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre> 
<p>返回的报错结果如下：</p> 
<pre><code class="prism language-python">D<span class="token punctuation">:</span>\software\anaconda3\envs\pytorch\lib\site<span class="token operator">-</span>packages\torch\autograd\__init__<span class="token punctuation">.</span>py<span class="token punctuation">:</span><span class="token number">154</span><span class="token punctuation">:</span> UserWarning<span class="token punctuation">:</span> Error detected <span class="token keyword">in</span> AddmmBackward0<span class="token punctuation">.</span> Traceback of forward call that caused the error<span class="token punctuation">:</span>
  File <span class="token string">"D:\code_work\reinforcement_learning\.pytest_cache\bark_test.py"</span><span class="token punctuation">,</span> line <span class="token number">49</span><span class="token punctuation">,</span> <span class="token keyword">in</span> <span class="token operator">&lt;</span>module<span class="token operator">&gt;</span>
    pred_n1 <span class="token operator">=</span> n_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
  File <span class="token string">"D:\software\anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\module.py"</span><span class="token punctuation">,</span> line <span class="token number">1102</span><span class="token punctuation">,</span> <span class="token keyword">in</span> _call_impl
    <span class="token keyword">return</span> forward_call<span class="token punctuation">(</span><span class="token operator">*</span><span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
  File <span class="token string">"D:\code_work\reinforcement_learning\.pytest_cache\bark_test.py"</span><span class="token punctuation">,</span> line <span class="token number">18</span><span class="token punctuation">,</span> <span class="token keyword">in</span> forward
    x <span class="token operator">=</span> self<span class="token punctuation">.</span>linear_2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
  File <span class="token string">"D:\software\anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\module.py"</span><span class="token punctuation">,</span> line <span class="token number">1102</span><span class="token punctuation">,</span> <span class="token keyword">in</span> _call_impl
    <span class="token keyword">return</span> forward_call<span class="token punctuation">(</span><span class="token operator">*</span><span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
  File <span class="token string">"D:\software\anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\linear.py"</span><span class="token punctuation">,</span> line <span class="token number">103</span><span class="token punctuation">,</span> <span class="token keyword">in</span> forward
    <span class="token keyword">return</span> F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> self<span class="token punctuation">.</span>bias<span class="token punctuation">)</span>
  File <span class="token string">"D:\software\anaconda3\envs\pytorch\lib\site-packages\torch\nn\functional.py"</span><span class="token punctuation">,</span> line <span class="token number">1848</span><span class="token punctuation">,</span> <span class="token keyword">in</span> linear
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>_C<span class="token punctuation">.</span>_nn<span class="token punctuation">.</span>linear<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> weight<span class="token punctuation">,</span> bias<span class="token punctuation">)</span>
 <span class="token punctuation">(</span>Triggered internally at  <span class="token punctuation">.</span><span class="token punctuation">.</span>\torch\csrc\autograd\python_anomaly_mode<span class="token punctuation">.</span>cpp<span class="token punctuation">:</span><span class="token number">104.</span><span class="token punctuation">)</span>
  Variable<span class="token punctuation">.</span>_execution_engine<span class="token punctuation">.</span>run_backward<span class="token punctuation">(</span>
Traceback <span class="token punctuation">(</span>most recent call last<span class="token punctuation">)</span><span class="token punctuation">:</span>
  File <span class="token string">"D:\code_work\reinforcement_learning\.pytest_cache\bark_test.py"</span><span class="token punctuation">,</span> line <span class="token number">58</span><span class="token punctuation">,</span> <span class="token keyword">in</span> <span class="token operator">&lt;</span>module<span class="token operator">&gt;</span>
    loss_n2<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
  File <span class="token string">"D:\software\anaconda3\envs\pytorch\lib\site-packages\torch\_tensor.py"</span><span class="token punctuation">,</span> line <span class="token number">307</span><span class="token punctuation">,</span> <span class="token keyword">in</span> backward
    torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>self<span class="token punctuation">,</span> gradient<span class="token punctuation">,</span> retain_graph<span class="token punctuation">,</span> create_graph<span class="token punctuation">,</span> inputs<span class="token operator">=</span>inputs<span class="token punctuation">)</span>
  File <span class="token string">"D:\software\anaconda3\envs\pytorch\lib\site-packages\torch\autograd\__init__.py"</span><span class="token punctuation">,</span> line <span class="token number">154</span><span class="token punctuation">,</span> <span class="token keyword">in</span> backward
    Variable<span class="token punctuation">.</span>_execution_engine<span class="token punctuation">.</span>run_backward<span class="token punctuation">(</span>
RuntimeError<span class="token punctuation">:</span> one of the variables needed <span class="token keyword">for</span> gradient computation has been modified by an inplace operation<span class="token punctuation">:</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor <span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> which <span class="token keyword">is</span> output <span class="token number">0</span> of AsStridedBackward0<span class="token punctuation">,</span> <span class="token keyword">is</span> at version <span class="token number">2</span><span class="token punctuation">;</span> expected version <span class="token number">1</span> instead<span class="token punctuation">.</span> Hint<span class="token punctuation">:</span> the backtrace further above shows the operation that failed to compute its gradient<span class="token punctuation">.</span> The variable <span class="token keyword">in</span> question was changed <span class="token keyword">in</span> there <span class="token keyword">or</span> anywhere later<span class="token punctuation">.</span> Good luck!
</code></pre> 
<p>无论是前面<code>retain_graph=True</code>报的错还是这个错误，都是指向<code>loss_n2.backward()</code>,上面的报错信息就详细地<strong>梳理了</strong><code>loss_n2</code><strong>反向传播的计算流程</strong>，和我们代码关联的只有一项：</p> 
<pre><code class="prism language-python">x <span class="token operator">=</span> self<span class="token punctuation">.</span>linear_2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre> 
<p>在这个计算过程中，由于存在<code>pred_n1</code>的参与，我们要保证<code>loss_n2.backward()</code>执行过程中<code>requires_grad=False</code>，否则会发生冲突。<br> 因此，我们在该神经网络中的<strong>初始部分</strong>使用<code>detach</code>操作，将它的<code>requires_grad</code>停止。即：</p> 
<pre><code class="prism language-python">x <span class="token operator">=</span> self<span class="token punctuation">.</span>linear_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p><code>为什么只添加在初始位置，其余位置不加？</code><br> 很简单，因为只有神经网络的初始节点(<mark><strong>叶节点</strong></mark>)才能接收梯度(<code>requires_grad=True</code>)，<mark><strong>非叶节点</strong></mark>(隐藏层中的节点都是<code>requires_grad=False</code>)<br> 损失函数结果本身是<strong>标量</strong>，自然也不会有梯度的。<br> 至此，上述错误解决。修改后完整代码如下，大家可以对比一下：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn <span class="token keyword">as</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> functional <span class="token keyword">as</span> F
<span class="token keyword">from</span> torch <span class="token keyword">import</span> optim


<span class="token keyword">class</span> <span class="token class-name">Net_1</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net_1<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>linear_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>linear_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>linear_2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>x<span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x


<span class="token keyword">class</span> <span class="token class-name">Net_2</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net_2<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>linear_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>linear_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>linear_2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>x<span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x


n_1 <span class="token operator">=</span> Net_1<span class="token punctuation">(</span><span class="token punctuation">)</span>
n_2 <span class="token operator">=</span> Net_2<span class="token punctuation">(</span><span class="token punctuation">)</span>

optimizer_n1 <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>n_1<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span>
optimizer_n2 <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>n_2<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    y <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> x

    pred_n1 <span class="token operator">=</span> n_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    optimizer_n1<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss_n1 <span class="token operator">=</span> criterion<span class="token punctuation">(</span>y<span class="token punctuation">,</span>pred_n1<span class="token punctuation">)</span>
    loss_n1<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>retain_graph<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    optimizer_n1<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

    pred_n2 <span class="token operator">=</span> n_2<span class="token punctuation">(</span>pred_n1<span class="token punctuation">)</span>
    optimizer_n2<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss_n2 <span class="token operator">=</span> criterion<span class="token punctuation">(</span>y<span class="token punctuation">,</span>pred_n2<span class="token punctuation">)</span>
    loss_n2<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer_n2<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/1182ef1826d2a1851406aff417aa09f3/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">webapi接口文件下载时跨域问题</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/550d922519b1e83a92dd8a47c9f9056e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">毕业进入HW，从测试工程师到项目经理，现如今在鹅厂年收入百万，我的给大家的一些建议...</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>