<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>逻辑回归（LR）----机器学习 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="逻辑回归（LR）----机器学习" />
<meta property="og:description" content="基本原理
逻辑回归（Logistic Regression，LR）也称为&#34;对数几率回归&#34;，又称为&#34;逻辑斯谛&#34;回归。
logistic回归又称logistic 回归分析 ，是一种广义的线性回归分析模型，常用于数据挖掘，疾病自动诊断，经济预测等领域。 逻辑回归根据给定的自变量数据集来估计事件的发生概率，由于结果是一个概率，因此因变量的范围在 0 和 1 之间。
知识点提炼
分类，经典的二分类算法！
逻辑回归就是这样的一个过程：面对一个回归或者分类问题，建立代价函数，然后通过优化方法迭代求解出最优的模型参数，然后测试验证我们这个求解的模型的好坏。
Logistic 回归虽然名字里带“回归”，但是它实际上是一种分类方法，主要用于两分类问题（即输出只有两种，分别代表两个类别）
回归模型中，y 是一个定性变量，比如 y = 0 或 1，logistic 方法主要应用于研究某些事件发生的概率。
逻辑回归的本质：极大似然估计
逻辑回归的激活函数：Sigmoid
逻辑回归的代价函数：交叉熵
逻辑回归的优缺点
优点：
1）速度快，适合二分类问题
2）简单易于理解，直接看到各个特征的权重
3）能容易地更新模型吸收新的数据
缺点：
对数据和场景的适应能力有局限性，不如决策树算法适应性那么强
逻辑回归中最核心的概念是 Sigmoid 函数，Sigmoid函数可以看成逻辑回归的激活函数。
下图是逻辑回归网络：
对数几率函数（Sigmoid）：
y = σ ( z ) = 1 1 &#43; e − z y = \sigma (z) = \frac{1}{1&#43;e^{-z}} y=σ(z)=1&#43;e−z1​
通过对数几率函数的作用，我们可以将输出的值限制在区间[0，1]上，p(x) 则可以用来表示概率 p(y=1|x)，即当一个x发生时，y被分到1那一组的概率。可是，等等，我们上面说 y 只有两种取值，但是这里却出现了一个区间[0, 1]，这是什么鬼？？其实在真实情况下，我们最终得到的y的值是在 [0, 1] 这个区间上的一个数，然后我们可以选择一个阈值，通常是 0.5，当 y &gt; 0." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/3880c55ad94335540748c886d875cc3e/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-03T17:14:01+08:00" />
<meta property="article:modified_time" content="2024-01-03T17:14:01+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">逻辑回归（LR）----机器学习</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><strong>基本原理</strong><br> 逻辑回归（Logistic Regression，LR）也称为"对数几率回归"，又称为"逻辑斯谛"回归。</p> 
<p><mark>logistic回归又称logistic 回归分析 ，是一种广义的线性回归分析模型，常用于数据挖掘，疾病自动诊断，经济预测等领域。 逻辑回归根据给定的自变量数据集来估计事件的发生概率，由于结果是一个概率，因此因变量的范围在 0 和 1 之间。</mark><br> <img src="https://images2.imgbox.com/eb/0e/QhvaCAhE_o.png" alt="在这里插入图片描述"></p> 
<p><strong>知识点提炼</strong><br> <mark>分类，经典的二分类算法！</mark><br> 逻辑回归就是这样的一个过程：面对一个回归或者分类问题，建立代价函数，然后通过优化方法迭代求解出最优的模型参数，然后测试验证我们这个求解的模型的好坏。<br> Logistic 回归虽然名字里带“回归”，但是它实际上是一种分类方法，主要用于两分类问题（即输出只有两种，分别代表两个类别）<br> 回归模型中，y 是一个定性变量，比如 y = 0 或 1，logistic 方法主要应用于研究某些事件发生的概率。<br> 逻辑回归的<mark>本质</mark>：极大似然估计<br> 逻辑回归的<mark>激活函数</mark>：Sigmoid<br> 逻辑回归的<mark>代价函数</mark>：交叉熵<br> 逻辑回归的优缺点<br> <strong>优点：</strong><br> 1）<mark>速度快，适合二分类问题</mark><br> 2）简单易于理解，直接看到各个特征的权重<br> 3）能容易地更新模型吸收新的数据<br> <strong>缺点：</strong><br> 对数据和场景的适应能力有局限性，<mark>不如决策树算法适应性那么强</mark></p> 
<p><strong>逻辑回归中最核心的概念是 Sigmoid 函数，Sigmoid函数可以看成逻辑回归的激活函数。</strong></p> 
<p>下图是逻辑回归网络：</p> 
<p><img src="https://images2.imgbox.com/cd/9a/uCBb0Zhc_o.png" alt="在这里插入图片描述"></p> 
<p>对数几率函数（Sigmoid）：<br> <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         y 
        
       
         = 
        
       
         σ 
        
       
         ( 
        
       
         z 
        
       
         ) 
        
       
         = 
        
        
        
          1 
         
         
         
           1 
          
         
           + 
          
          
          
            e 
           
           
           
             − 
            
           
             z 
            
           
          
         
        
       
      
        y = \sigma (z) = \frac{1}{1+e^{-z}} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">σ</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.044em;">z</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.2484em; vertical-align: -0.4033em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8451em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7027em;"><span class="" style="top: -2.786em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight" style="margin-right: 0.044em;">z</span></span></span></span></span></span></span></span></span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.4033em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p> 
<p>通过对数几率函数的作用，我们可以将输出的值限制在区间[0，1]上，p(x) 则可以用来表示概率 p(y=1|x)，即当一个x发生时，y被分到1那一组的概率。可是，等等，我们上面说 y 只有两种取值，但是这里却出现了一个区间[0, 1]，这是什么鬼？？其实在真实情况下，我们最终得到的y的值是在 [0, 1] 这个区间上的一个数，然后我们可以选择一个阈值，通常是 0.5，当 y &gt; 0.5 时，就将这个 x 归到 1 这一类，如果 y&lt; 0.5 就将 x 归到 0 这一类。但是阈值是可以调整的，比如说一个比较保守的人，可能将阈值设为 0.9，也就是说有超过90%的把握，才相信这个x属于 1这一类。了解一个算法，最好的办法就是自己从头实现一次。下面是逻辑回归的具体实现。</p> 
<p><strong>Regression 常规步骤</strong></p> 
<p>1、<mark>寻找h函数（即预测函数）</mark><br> 2、<mark>构造J函数（损失函数）</mark><br> 3、<mark>想办法（迭代）使得J函数最小并求得回归参数（θ）</mark><br> 函数h(x)的值有特殊的含义，它表示结果取1的概率，于是可以看成类1的后验估计。因此对于输入x分类结果为类别1和类别0的概率分别为：<br> P(y=1│x;θ)=hθ (x)<br> P(y=0│x;θ)=1-hθ (x)</p> 
<p><strong>代价函数</strong><br> 逻辑回归一般使用交叉熵作为代价函数。关于代价函数的具体细节，请参考代价函数。</p> 
<p>神经元的目标是去计算函数 y, 且 y = y(x)。但是我们让它取而代之计算函数 a, 且 a = a(x) 。假设我们把 a 当作 y 等于 1 的概率，1−a 是 y 等于 0 的概率。那么，交叉熵衡量的是我们在知道 y 的真实值时的平均「出乎意料」程度。当输出是我们期望的值，我们的「出乎意料」程度比较低；当输出不是我们期望的，我们的「出乎意料」程度就比较高。</p> 
<p>交叉熵代价函数如下所示：<br> <img src="https://images2.imgbox.com/7f/cb/uGcFW1Ce_o.png" alt="在这里插入图片描述"></p> 
<p>注：为什么要使用交叉熵函数作为代价函数，而不是平方误差函数？请参考：逻辑回归算法之交叉熵函数理解</p> 
<p><strong>逻辑回归伪代码</strong></p> 
<pre><code class="prism language-bash">初始化线性函数参数为1
构造sigmoid函数
重复循环I次
	计算数据集梯度
	更新线性函数参数
确定最终的sigmoid函数
输入训练（测试）数据集
运用最终sigmoid函数求解分类
</code></pre> 
<p><strong>极大似然估计（Maximum Likelihood Estimation，MLE）</strong></p> 
<p>极大似然估计法（the Principle of Maximum Likelihood ）由高斯和费希尔（R.A.Figher）先后提出，是被使用最广泛的一种参数估计方法，该方法建立的依据是直观的最大似然原理。<br> <img src="https://images2.imgbox.com/cc/77/EBDZk0Zq_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="_74"></a>简介：</h4> 
<p>极大似然估计是一种用于估计概率分布参数的统计方法。其核心思想是通过最大化似然函数，选择使得观测数据出现的概率最大的参数值。在统计学中，似然函数度量了在给定参数下观察到某一组数据的概率。</p> 
<p>总结起来，<mark>最大似然估计的目的就是：利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。</mark></p> 
<hr> 
<p>原理：极大似然估计是建立在极大似然原理的基础上的一个统计方法，是概率论在统计学中的应用。极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。</p> 
<hr> 
<p>极大似然估计可以拆成三个词，分别是“极大”、“似然”、“估计”，分别的意思如下：<br> <strong>极大：最大的概率<br> 似然：看起来是这个样子的<br> 估计：就是这个样子的</strong><br> 连起来就是，最大的概率看起来是这个样子的那就是这个样子的。</p> 
<hr> 
<p>举个例子：<br> 有两个妈妈带着一个小孩到了你的面前，妈妈A和小孩长得很像，妈妈B和小孩一点都不像，问你谁是孩子的妈妈，你说是妈妈A。好的，那这种时候你所采取的方式就是极大似然估计：妈妈A和小孩长得像，所以妈妈A是小孩的妈妈的概率大，这样妈妈A看来就是小孩的妈妈，妈妈A就是小孩的妈妈。<br> 总结：极大似然估计就是在只有概率的情况下，忽略低概率事件直接将高概率事件认为是真实事件的思想。</p> 
<hr> 
<h4><a id="_98"></a>基本概念：</h4> 
<ol><li><strong>似然函数：</strong> 对于参数 ( <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          θ 
         
        
       
         \theta 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span></span></span></span></span> ) 和观测数据集 ( <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          X 
         
        
       
         X 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.0785em;">X</span></span></span></span></span> )，似然函数 ( L(<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          θ 
         
        
       
         \theta 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span></span></span></span></span> | <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          X 
         
        
          ) 
         
        
       
         X) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0785em;">X</span><span class="mclose">)</span></span></span></span></span> ) 表示在给定参数 ( <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          θ 
         
        
       
         \theta 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span></span></span></span></span> ) 下观察到数据 ( <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          X 
         
        
       
         X 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.0785em;">X</span></span></span></span></span>) 的概率。</li></ol> 
<p><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
           
        
       
         L 
        
       
         ( 
        
       
         θ 
        
       
         ∣ 
        
       
         X 
        
       
         ) 
        
       
         = 
        
       
         P 
        
       
         ( 
        
       
         X 
        
       
         ∣ 
        
       
         θ 
        
       
         ) 
        
       
      
        \ L(\theta | X) = P(X | \theta) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mspace"> </span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right: 0.0785em;">X</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.1389em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0785em;">X</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mclose">)</span></span></span></span></span></p> 
<ol start="2"><li> <p><strong>极大似然估计：</strong> 极大似然估计的目标是找到能最大化似然函数的参数值。通常采用对数似然函数（对数似然估计）进行求解，因为对数函数的增减性与原函数一致，方便求导。</p> <p><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           Log-Likelihood 
          
         
           ( 
          
         
           θ 
          
         
           ∣ 
          
         
           X 
          
         
           ) 
          
         
           = 
          
         
           log 
          
         
           ⁡ 
          
         
           L 
          
         
           ( 
          
         
           θ 
          
         
           ∣ 
          
         
           X 
          
         
           ) 
          
         
        
          \text{Log-Likelihood}(\theta | X) = \log L(\theta | X) 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord text"><span class="mord">Log-Likelihood</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right: 0.0785em;">X</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mop">lo<span style="margin-right: 0.0139em;">g</span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right: 0.0785em;">X</span><span class="mclose">)</span></span></span></span></span></p> <p>极大似然估计问题可以形式化为：</p> <p><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
           
           
             θ 
            
           
             ^ 
            
           
          
            MLE 
           
          
         
           = 
          
         
           arg 
          
         
           ⁡ 
          
          
           
           
             max 
            
           
             ⁡ 
            
           
          
            θ 
           
          
         
           log 
          
         
           ⁡ 
          
         
           L 
          
         
           ( 
          
         
           θ 
          
         
           ∣ 
          
         
           X 
          
         
           ) 
          
         
        
          \hat{\theta}_{\text{MLE}} = \arg\max_\theta \log L(\theta | X) 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.1079em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9579em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span></span><span class="" style="top: -3.2634em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.1667em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em;"><span class="" style="top: -2.55em; margin-left: -0.0278em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">MLE</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mop">ar<span style="margin-right: 0.0139em;">g</span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mop"><span class="mop">max</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0278em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mop">lo<span style="margin-right: 0.0139em;">g</span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right: 0.0785em;">X</span><span class="mclose">)</span></span></span></span></span></p> </li></ol> 
<h4><a id="_112"></a>举例：</h4> 
<p>考虑一个简单的二项分布（二分类问题）：假设观测到了 ( <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         n 
        
       
      
        n 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span></span>) 次独立的二元实验，其中有 ( <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         k 
        
       
      
        k 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal" style="margin-right: 0.0315em;">k</span></span></span></span></span>) 次成功。成功的概率为 ( <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         p 
        
       
      
        p 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord mathnormal">p</span></span></span></span></span>)，失败的概率为 ( <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         1 
        
       
         − 
        
       
         p 
        
       
      
        1-p 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;"></span><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord mathnormal">p</span></span></span></span></span>)。则似然函数为：</p> 
<p><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         L 
        
       
         ( 
        
       
         p 
        
       
         ∣ 
        
       
         k 
        
       
         , 
        
       
         n 
        
       
         ) 
        
       
         = 
        
        
        
          ( 
         
         
         
           n 
          
         
           k 
          
         
        
          ) 
         
        
        
        
          p 
         
        
          k 
         
        
       
         ( 
        
       
         1 
        
       
         − 
        
       
         p 
        
        
        
          ) 
         
         
         
           n 
          
         
           − 
          
         
           k 
          
         
        
       
      
        L(p | k, n) = \binom{n}{k} p^k (1-p)^{n-k} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right: 0.0315em;">k</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">n</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.2em; vertical-align: -0.35em;"></span><span class="mord"><span class="mopen delimcenter" style="top: 0em;"><span class="delimsizing size1">(</span></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.7454em;"><span class="" style="top: -2.355em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0315em;">k</span></span></span></span><span class="" style="top: -3.144em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em;"><span class=""></span></span></span></span></span><span class="mclose delimcenter" style="top: 0em;"><span class="delimsizing size1">)</span></span></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8491em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0315em;">k</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1.0991em; vertical-align: -0.25em;"></span><span class="mord mathnormal">p</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8491em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right: 0.0315em;">k</span></span></span></span></span></span></span></span></span></span></span></span></span></p> 
<p>对数似然函数为：</p> 
<p><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         Log-Likelihood 
        
       
         ( 
        
       
         p 
        
       
         ∣ 
        
       
         k 
        
       
         , 
        
       
         n 
        
       
         ) 
        
       
         = 
        
       
         k 
        
       
         log 
        
       
         ⁡ 
        
       
         ( 
        
       
         p 
        
       
         ) 
        
       
         + 
        
       
         ( 
        
       
         n 
        
       
         − 
        
       
         k 
        
       
         ) 
        
       
         log 
        
       
         ⁡ 
        
       
         ( 
        
       
         1 
        
       
         − 
        
       
         p 
        
       
         ) 
        
       
      
        \text{Log-Likelihood}(p | k, n) = k \log(p) + (n-k) \log(1-p) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord text"><span class="mord">Log-Likelihood</span></span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right: 0.0315em;">k</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">n</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0315em;">k</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mop">lo<span style="margin-right: 0.0139em;">g</span></span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0315em;">k</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mop">lo<span style="margin-right: 0.0139em;">g</span></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">p</span><span class="mclose">)</span></span></span></span></span></p> 
<p>最大化对数似然函数，可以得到 ( <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         p 
        
       
      
        p 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord mathnormal">p</span></span></span></span></span>) 的极大似然估计。</p> 
<h4><a id="_124"></a>面试考点：</h4> 
<ol><li> <p><strong>理解似然函数：</strong> 能够解释似然函数的含义，即在给定参数下观测到当前数据的可能性。</p> </li><li> <p><strong>极大似然估计的求解：</strong> 理解如何通过最大化似然函数或对数似然函数来估计参数，以及这一过程的数学推导。</p> </li><li> <p><strong>应用场景：</strong> 理解极大似然估计在不同概率分布、机器学习模型参数估计等方面的应用。</p> </li><li> <p><strong>性质与假设：</strong> 了解极大似然估计的一些性质，以及估计中的一些假设条件。</p> </li><li> <p><strong>比较：</strong> 能够与贝叶斯估计等其他参数估计方法进行比较，理解它们之间的异同。</p> </li><li> <p><strong>实际问题：</strong> 在实际问题中能够应用极大似然估计，例如在统计学、机器学习中的具体场景。</p> </li></ol>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/09cbcaa871770dc2993713d39e80cfc8/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">MATLAB矩阵操作</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ba0d18c4b34cad059fb678dbc8861557/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">MATLAB逻辑与流程控制</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>