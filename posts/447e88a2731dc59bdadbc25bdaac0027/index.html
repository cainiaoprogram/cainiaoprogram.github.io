<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>CVPR2023 SuperYOLO全文翻译（人工校正） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="CVPR2023 SuperYOLO全文翻译（人工校正）" />
<meta property="og:description" content="多模态遥感图像中的超分辨率辅助目标检测（SuperYOLO） 单位：西电, 西蒙菲莎大学, 密西西比州立大学
代码：https://github.com/icey-zhang/SuperYOLO
论文：https://arxiv.org/abs/2209.1335
摘要-准确和及时地从遥感图像（RSI）中检测到包含数十个像素的多尺度小物体仍然具有挑战性。现有的解决方案主要是设计复杂的深度神经网络，学习从背景分离的对象的强特征表示，这往往会造成沉重的计算负担。在本文中，我们提出了一种精确而快速的RSI目标检测方法SuperYOLO，该方法融合多模态数据（RGB和IR），利用辅助超分辨率模块（SR）学习对多尺度目标进行高分辨率（HR）目标检测。首先，我们利用对称紧凑的多模态融合（MF）从各种数据中提取互补信息，以改进RSI中小目标检测性能。此外，我们设计了一个简单而灵活的SR分支来学习HR特征表示，可以通过低分辨率（LR）输入区分小物体和大背景，从而进一步提高检测精度。此外，为了避免引入额外的计算，在推理阶段丢弃了SR分支，并且由于LR的输入而减少了网络模型的计算。实验结果表明，在广泛使用的VEDAI RS数据集上，SuperYOLO的准确率为75.09 (对于mAP50)，这比SOTA大型机型如YOLOv5l、YOLOv5x和RS设计的YOLOrs高出10%以上。同时，SuperYOLO的参数大小和GFOLPs比YOLOv的5x小约18倍和3.8倍。与现有的模型相比，我们提出的模型显示出良好的精度和速度权衡。代码将在https://github.com/icey-zhang/SuperYOLO
索引术语——目标检测、多模态遥感图像、超分辨率、特征融合
I. 介绍 目标检测在计算机辅助诊断或自主驾驶等各个领域发挥着重要作用。在过去的几十年里，许多优秀的基于深度神经网络（DNN）的目标检测框架[1]、[2]、[3]、[4] 、[5]在计算机视觉中被提出、更新和优化。由于应用大规模自然数据集的准确注释[6]，[7]，[8]，基于DNN的目标检测框架的精度有了显著的提高。
与自然场景相比，遥感图像（RSIs）中的精确目标检测存在几个重要的挑战。首先，标记样本的数量相对较少，这限制了训练DNN以实现较高的检测精度。其次，RSI中物体的尺寸要小得多，相对于复杂而广阔的背景[9]，[10]，只占几十个像素。此外，这些对象的规模是多样的，有多个类别[11]。如图1 (a)所示，目标车在一个大范围内相当小。如图1 (b)所示，这些物体有大规模的变化，汽车的规模比露营车的规模要小。
目前，大多数目标检测技术都是单独设计和应用于单一的模态信息，如RGB和红外（IR）[12]，[13]。因此，在物体检测方面，由于不同模式[14]之间缺乏互补信息，它识别地球表面物体的能力仍然不足。随着成像技术的蓬勃发展，从多模态中收集的RSIs成为可能，并为提高检测精度提供了机会。例如，如图1所示，两种不同的多模式（RGB和IR）的融合可以有效地提高RSI的检测精度。有时一种模态的分辨率很低，这需要通过技术来提高分辨率来增强信息。近年来，超分辨率技术在[15]、[16]、[17]、[18]等遥感领域显示出了巨大的潜力。由于卷积神经元网络（CNN）的蓬勃发展，遥感图像的分辨率实现了高纹理信息的解释。然而，由于CNN网络的高计算成本，SR网络在实时实际任务中的应用已成为当前研究的热点。
在这项研究中，我们的动机是提出一个机载（on-board）多模态的实时目标检测框架，在不引入额外计算开销的情况下实现高检测精度和高推理速度。受实时紧凑神经网络模型最近进展的启发，我们选择了小尺寸的YOLOv5s [19]结构作为我们的检测基线。它可以降低部署成本，并促进模型的快速部署。考虑到小物体的高分辨率（HR）保留要求，我们在基线YOLOv5s模型中删除了Focus模块，这不仅有利于定义小密集物体的位置，而且提高了检测性能。考虑到不同模态信息下的互补特性，我们提出了一种多模态融合（MF）方案来提高RSI的检测性能。我们评估不同的融合方案（像素级或特征级），并选择像素级融合以实现低计算成本。
最后，我们开发了一个超分辨率（SR）保证（assurance？？）模块，以指导网络生成能够在大背景下识别小物体的HR特征，从而减少RSI中背景污染物体引起的误报。然而，一个简单的SR解可能会显著增加计算成本。因此，我们设置了在训练过程中使用的辅助SR分支，并在推理阶段将其删除，在不增加计算成本的情况下，促进了HR中的空间信息提取。
综上所述，本文有以下贡献。
我们提出了一种计算友好的像素级融合方法，以对称和紧凑的方式双向结合内部信息的方法。与特征级融合相比，它在不牺牲精度的情况下有效地降低了计算成本。我们首次在多模态目标检测中引入了一个辅助的SR分支。我们的方法不仅在有限的检测性能方面取得了突破，而且为研究优秀的HR特征表示提供了一种更灵活的方法，这些特征表示能够通过LR输入从大的背景中区分小目标。考虑到对高质量结果和低质量结果的需求计算成本，SR模块作为一个辅助任务在推理阶段被删除，而不引入额外的计算。SR分支是通用的和可扩展的，可以插入到现有的全卷积网络（FCN）架构中。提出的SuperYOLO显著提高了目标检测性能，在实时多模态目标检测中优于SOTA检测器。与现有的模型相比，我们提出的模型显示出良好的精度-速度平衡。 II. 相关工作
A.使用多模态数据的对象检测
近年来，多模态数据在许多实际应用场景中得到了广泛应用，包括视觉问题回答[20]、自动驾驶车辆[21]、显著性检测[22]和遥感分类[23]。结果发现，结合多模态数据的内部信息可以有效地传输互补特征，避免单一模态的某些信息被遗漏。
在RSI处理领域，存在各种模式（如红绿蓝（RGB）、合成孔径雷达（SAR）、光检测和测距（激光雷达）、红外（IR）、全色（PAN）和多光谱（MS）图像），可以与互补的特性融合，以提高[24]、[25]、[26]等各种任务的性能。例如，额外的红外模态[27]捕获更长的热波长，以提高在困难的天气条件下的检测。曼尼什等人。[27]提出了一种在多模态遥感成像中进行目标检测的实时框架，其中扩展版本进行了中层融合和合并来自多种模式的数据。尽管多传感器融合可以提高检测性能，但其低精度的检测性能和有待提高的计算速度几乎难以满足实时检测任务的要求。
融合方法主要分为三种策略。e., 像素级融合、特征级融合和决策级融合方法[28]。决策级融合方法将最后阶段的检测结果进行融合，由于对不同的多模态分支进行重复计算，可能会消耗大量的计算资源。在遥感领域，主要采用多分支的特征级融合方法。将多模态图像输入到并行分支中，提取不同模态的独立特征，然后将这些特征通过一些操作进行组合，如注意模块或简单连接。随着模式的增加，并行分支带来了重复的计算，这在遥感的实时任务中并不友好。
相比之下，采用像素级融合方法可以减少不必要的计算。在本文中，我们提出的SuperYOLO在像素级进行多模态特征融合，显著降低了空间和信道域的计算成本和设计操作，以提取不同模态的内部信息，从而提高检测精度。
B.目标检测中的超分辨率
在最近的文献中，通过多尺度特征学习[29]、[30]、基于上下文的检测[31]，可以提高小目标检测的性能。这些方法总是提高了网络在不同尺度上的信息表示能力，但忽略了高分辨率的上下文信息保留。按照预处理步骤进行，SR在[32]，[33]中被证明在各种目标检测任务中是有效的。谢梅耶等人。[34]通过RSI的多重分辨率，量化了其对卫星成像检测性能的影响。基于生成式对抗网络（GANs），Courtrai等人 [35]利用SR生成HR图像，并将其输入检测器，以提高其检测性能。拉比等 [36]利用拉普拉斯算子从输入图像中提取边缘，从而增强了对HR图像的重构能力，从而提高了其在目标定位和分类方面的性能。Hong等。[37]引入了循环一致的GAN结构作为一个采用SR网络和改进的更快的R-CNN架构进行检测由SR网络产生的增强图像所产生的车辆。在这些工作中，SR结构的采用已经有效地解决了有关小对象的挑战。然而，与单一检测模型相比，还引入了额外的计算量，这是由于HR设计放大了输入图像的尺度。
最近，王等人 [38]提出了一个SR模块，它可以用LR输入来维护HR表示，同时减少分割任务中的模型计算。受[38]的启发，我们设计了一个SR辅助的分支。与之形成对比，在上述的工作中，SR被实现在开始阶段，辅助SR模块指导检测器高质量HR表示的学习，不仅增强了密集小目标的响应，而且提高了空间中目标检测的性能。此外，在推理阶段删除了SR模块，以避免额外的计算。
III. 基线模型架构 如图2所示，基线YOLOv5网络由两个主要组件组成：主干和头部（包括颈部）。该主干旨在提取低级纹理和高级语义特征。接下来，这些提示（hint??）特征被输入到Head来构建增强的特征金字塔网络，从上到下传递鲁棒的语义特征，并从下到上传播对局部纹理和模式特征的强烈响应。这就解决了物体的不同尺度问题，增强了对不同尺度物体的检测能力。
在图3中利用CSPNet [39]作为骨干来提取特征信息，由大量的采样卷积C-批归一化B-SiLu（CBS）组件和跨阶段部分（CSP）模块组成。CBS由卷积、批处理归一化和激活函数SiLu [40]等操作组成。CSP将前一层的特征映射复制为两个分支，然后通过1x1卷积将信道数减半，从而减少了计算量。对于特征图的两个副本，一个被连接到阶段的末端，另一个被发送到ResNet块或CBS块作为输入。最后，将特征映射的两个副本连接起来，以组合这些特征，然后是一个CBS块。SPP（空间金字塔池）模块[41]由不同内核大小的并行Maxpool层组成，用于提取多尺度深度特征。通过堆叠的CSP、CBS和SPP结构提取底层纹理和高级语义特征。
缺陷1：值得一提的是，引入焦点模块是为了减少计算的数量。如图2（左下角）所示，输入被划分为单独的像素，每隔一段时间进行重建，最后连接到通道维度中。将输入的大小调整到更小的规模，以降低计算成本，加速网络训练和推理速度。然而，这可能会在一定程度上牺牲目标检测精度，特别是对于容易被分辨率的小物体。
缺陷2：已知YOLO的主干采用深度卷积神经网络提取层次特征，步长为2，通过步长提取的特征的大小减半。因此，用于多尺度检测所保留的特征尺寸远小于原始输入图像。例如，当输入图像大小为608时，针对最后一个检测层的输出特征的大小分别为76、38和19。LR特性可能会导致丢失一些小目标。
IV. SuperYOLO结构 如图2所示，我们为我们的SuperYOLO网络架构介绍了三个新的贡献。首先，我们删除了主干网中的焦点模块，并将其替换为一个MF模块，以避免分辨率下降，从而导致精度下降。其次，我们探索了不同的融合方法，并选择了计算效率高的像素级融合来融合RGB和IR模式，以细化不同和互补的信息。最后，我们在训练阶段添加了一个辅助SR模块，它重构HR图像，指导空间维度上的相关主干学习，从而维护HR信息。在推理阶段，丢弃SR分支，以避免引入额外的计算开销。
A.焦点移除
如第三节和图中所示。2（左下），YOLOv5主干网中的Focus模块在空间域上以间隔分割图像，然后重新组织新图像以调整输入图像的大小。具体来说，该操作是为图像中每一组像素的值，然后进行重构，得到更小的互补图像。重建的图像的大小随着通道数量的增加而减小。因此，它会导致小目标的分辨率下降和空间信息损失。考虑到对小目标的检测更依赖于更高的分辨率，因此放弃了Focus模块，用一个MF模块取代（如图4所示）来防止分辨率降低。
B.多模态融合
区分对象的信息越多，目标检测的性能就越好。多模态融合是合并来自不同传感器的不同信息的有效路径。决策级、特征级和像素级的融合是三种主流的融合方法，可以部署在
网络的不同深度。由于决策级融合需要大量的计算，因此在SuperYOLO中不考虑它。
我们提出了一种像素级的多模态融合（MF）来从不同的模态中提取共享的和特殊的信息。MF可以以对称和紧凑的方式双向结合多模态内部信息。如图4所示，对于像素级融合，我们首先将输入的RGB图像和输入的IR图像归一化为两个[0,1]区间。输入模态 XRGB、XIR ∈ R C×H×W 被子采样为 IRGB、IIR ∈ R C× H n × W n，再送入 SE 块提取信道域的内部信息[42]，生成 FRGB、FIR：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/447e88a2731dc59bdadbc25bdaac0027/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-09-14T15:42:17+08:00" />
<meta property="article:modified_time" content="2023-09-14T15:42:17+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">CVPR2023 SuperYOLO全文翻译（人工校正）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2 style="margin-left:0;text-align:center;">多模态遥感图像中的超分辨率辅助目标检测（SuperYOLO）</h2> 
<p>单位：西电, 西蒙菲莎大学, 密西西比州立大学</p> 
<p>代码：<a href="https://link.zhihu.com/?target=https%3A//github.com/icey-zhang/SuperYOLO" title="https://github.com/icey-zhang/SuperYOLO">https://github.com/icey-zhang/SuperYOLO</a></p> 
<p>论文：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2209.13351" rel="nofollow" title="https://arxiv.org/abs/2209.1335">https://arxiv.org/abs/2209.1335</a></p> 
<p style="margin-left:0;text-align:justify;"><strong>摘要</strong>-准确和及时地从遥感图像（RSI）中检测到包含数十个像素的多尺度小物体仍然具有挑战性。现有的解决方案主要是设计复杂的深度神经网络，学习从背景分离的对象的强特征表示，这往往会造成沉重的计算负担。在本文中，我们提出了一种精确而快速的RSI目标检测方法SuperYOLO，该方法融合多模态数据（RGB和IR），利用辅助超分辨率模块（SR）学习对多尺度目标进行高分辨率（HR）目标检测。首先，我们利用对称紧凑的多模态融合（MF）从各种数据中提取互补信息，以改进RSI中小目标检测性能。此外，我们设计了一个简单而灵活的SR分支来学习HR特征表示，可以通过低分辨率（LR）输入区分小物体和大背景，从而进一步提高检测精度。此外，为了避免引入额外的计算，在推理阶段丢弃了SR分支，并且由于LR的输入而减少了网络模型的计算。实验结果表明，在广泛使用的VEDAI RS数据集上，SuperYOLO的准确率为75.09 (对于mAP50)，这比SOTA大型机型如YOLOv5l、YOLOv5x和RS设计的YOLOrs高出10%以上。同时，SuperYOLO的参数大小和GFOLPs比YOLOv的5x小约18倍和3.8倍。与现有的模型相比，我们提出的模型显示出良好的精度和速度权衡。代码将在https://github.com/icey-zhang/SuperYOLO</p> 
<p style="margin-left:0;text-align:justify;"><strong>索引术语</strong>——目标检测、多模态遥感图像、超分辨率、特征融合</p> 
<p style="margin-left:0;text-align:justify;"></p> 
<h3 style="margin-left:0;text-align:justify;">I. 介绍</h3> 
<p style="margin-left:0;text-align:justify;">       目标检测在计算机辅助诊断或自主驾驶等各个领域发挥着重要作用。在过去的几十年里，许多优秀的基于深度神经网络（DNN）的目标检测框架[1]、[2]、[3]、[4] 、[5]在计算机视觉中被提出、更新和优化。由于应用大规模自然数据集的准确注释[6]，[7]，[8]，基于DNN的目标检测框架的精度有了显著的提高。</p> 
<p style="margin-left:0;text-align:justify;">与自然场景相比，遥感图像（RSIs）中的精确目标检测存在几个重要的挑战。首先，标记样本的数量相对较少，这限制了训练DNN以实现较高的检测精度。其次，RSI中物体的尺寸要小得多，相对于复杂而广阔的背景[9]，[10]，只占几十个像素。此外，这些对象的规模是多样的，有多个类别[11]。如图1 (a)所示，目标车在一个大范围内相当小。如图1 (b)所示，这些物体有大规模的变化，汽车的规模比露营车的规模要小。</p> 
<p style="margin-left:0;text-align:justify;">目前，大多数目标检测技术都是单独设计和应用于单一的模态信息，如RGB和红外（IR）[12]，[13]。因此，在物体检测方面，由于不同模式[14]之间缺乏互补信息，它识别地球表面物体的能力仍然不足。随着成像技术的蓬勃发展，从多模态中收集的RSIs成为可能，并为提高检测精度提供了机会。例如，如图1所示，两种不同的多模式（RGB和IR）的融合可以有效地提高RSI的检测精度。有时一种模态的分辨率很低，这需要通过技术来提高分辨率来增强信息。近年来，超分辨率技术在[15]、[16]、[17]、[18]等遥感领域显示出了巨大的潜力。由于卷积神经元网络（CNN）的蓬勃发展，遥感图像的分辨率实现了高纹理信息的解释。然而，由于CNN网络的高计算成本，SR网络在实时实际任务中的应用已成为当前研究的热点。</p> 
<p style="margin-left:0;text-align:justify;">在这项研究中，我们的动机是提出一个机载（on-board）多模态的实时目标检测框架，在不引入额外计算开销的情况下实现高检测精度和高推理速度。受实时紧凑神经网络模型最近进展的启发，我们选择了小尺寸的YOLOv5s [19]结构作为我们的检测基线。它可以降低部署成本，并促进模型的快速部署。考虑到小物体的高分辨率（HR）保留要求，我们在基线YOLOv5s模型中删除了Focus模块，这不仅有利于定义小密集物体的位置，而且提高了检测性能。考虑到不同模态信息下的互补特性，我们提出了一种多模态融合（MF）方案来提高RSI的检测性能。我们评估不同的融合方案（像素级或特征级），并选择像素级融合以实现低计算成本。</p> 
<p style="margin-left:0;text-align:justify;">最后，我们开发了一个超分辨率（SR）保证（assurance？？）模块，以指导网络生成能够在大背景下识别小物体的HR特征，从而减少RSI中背景污染物体引起的误报。然而，一个简单的SR解可能会显著增加计算成本。因此，我们设置了在训练过程中使用的辅助SR分支，并在推理阶段将其删除，在不增加计算成本的情况下，促进了HR中的空间信息提取。</p> 
<p style="margin-left:0;text-align:justify;">综上所述，本文有以下贡献。</p> 
<ol><li style="text-align:justify;">我们提出了一种计算友好的像素级融合方法，以对称和紧凑的方式双向结合内部信息的方法。与特征级融合相比，它在不牺牲精度的情况下有效地降低了计算成本。</li><li style="text-align:justify;">我们首次在多模态目标检测中引入了一个辅助的SR分支。我们的方法不仅在有限的检测性能方面取得了突破，而且为研究优秀的HR特征表示提供了一种更灵活的方法，这些特征表示能够通过LR输入从大的背景中区分小目标。考虑到对高质量结果和低质量结果的需求计算成本，SR模块作为一个辅助任务在推理阶段被删除，而不引入额外的计算。SR分支是通用的和可扩展的，可以插入到现有的全卷积网络（FCN）架构中。</li><li style="text-align:justify;">提出的SuperYOLO显著提高了目标检测性能，在实时多模态目标检测中优于SOTA检测器。与现有的模型相比，我们提出的模型显示出良好的精度-速度平衡。</li></ol> 
<p style="text-align:justify;">II. 相关工作</p> 
<p style="margin-left:0;text-align:justify;"><strong>A.使用多模态数据的对象检测</strong></p> 
<p style="margin-left:0;text-align:justify;">近年来，多模态数据在许多实际应用场景中得到了广泛应用，包括视觉问题回答[20]、自动驾驶车辆[21]、显著性检测[22]和遥感分类[23]。结果发现，结合多模态数据的内部信息可以有效地传输互补特征，避免单一模态的某些信息被遗漏。</p> 
<p style="margin-left:0;text-align:justify;">在RSI处理领域，存在各种模式（如红绿蓝（RGB）、合成孔径雷达（SAR）、光检测和测距（激光雷达）、红外（IR）、全色（PAN）和多光谱（MS）图像），可以与互补的特性融合，以提高[24]、[25]、[26]等各种任务的性能。例如，额外的红外模态[27]捕获更长的热波长，以提高在困难的天气条件下的检测。曼尼什等人。[27]提出了一种在多模态遥感成像中进行目标检测的实时框架，其中扩展版本进行了中层融合和合并来自多种模式的数据。尽管多传感器融合可以提高检测性能，但其低精度的检测性能和有待提高的计算速度几乎难以满足实时检测任务的要求。</p> 
<p class="img-center"><img alt="" height="299" src="https://images2.imgbox.com/74/aa/zPe5DO5R_o.png" width="534"></p> 
<p style="margin-left:0;text-align:justify;">       融合方法主要分为三种策略。e., 像素级融合、特征级融合和决策级融合方法[28]。决策级融合方法将最后阶段的检测结果进行融合，由于对不同的多模态分支进行重复计算，可能会消耗大量的计算资源。在遥感领域，主要采用多分支的特征级融合方法。将多模态图像输入到并行分支中，提取不同模态的独立特征，然后将这些特征通过一些操作进行组合，如注意模块或简单连接。随着模式的增加，并行分支带来了重复的计算，这在遥感的实时任务中并不友好。</p> 
<p style="margin-left:0;text-align:justify;">相比之下，采用像素级融合方法可以减少不必要的计算。在本文中，我们提出的SuperYOLO在像素级进行多模态特征融合，显著降低了空间和信道域的计算成本和设计操作，以提取不同模态的内部信息，从而提高检测精度。</p> 
<p style="margin-left:0;text-align:justify;"><strong>B.目标检测中的超分辨率</strong></p> 
<p style="margin-left:0;text-align:justify;">在最近的文献中，通过多尺度特征学习[29]、[30]、基于上下文的检测[31]，可以提高小目标检测的性能。这些方法总是提高了网络在不同尺度上的信息表示能力，但忽略了高分辨率的上下文信息保留。按照预处理步骤进行，SR在[32]，[33]中被证明在各种目标检测任务中是有效的。谢梅耶等人。[34]通过RSI的多重分辨率，量化了其对卫星成像检测性能的影响。基于生成式对抗网络（GANs），Courtrai等人 [35]利用SR生成HR图像，并将其输入检测器，以提高其检测性能。拉比等 [36]利用拉普拉斯算子从输入图像中提取边缘，从而增强了对HR图像的重构能力，从而提高了其在目标定位和分类方面的性能。Hong等。[37]引入了循环一致的GAN结构作为一个采用SR网络和改进的更快的R-CNN架构进行检测由SR网络产生的增强图像所产生的车辆。在这些工作中，SR结构的采用已经有效地解决了有关小对象的挑战。然而，与单一检测模型相比，还引入了额外的计算量，这是由于HR设计放大了输入图像的尺度。</p> 
<p style="margin-left:0;text-align:justify;">最近，王等人 [38]提出了一个SR模块，它可以用LR输入来维护HR表示，同时减少分割任务中的模型计算。受[38]的启发，我们设计了一个SR辅助的分支。与之形成对比，在上述的工作中，SR被实现在开始阶段，辅助SR模块指导检测器高质量HR表示的学习，不仅增强了密集小目标的响应，而且提高了空间中目标检测的性能。此外，在推理阶段删除了SR模块，以避免额外的计算。</p> 
<p class="img-center"><img alt="" height="299" src="https://images2.imgbox.com/bf/7a/FRJNWQZI_o.png" width="534"></p> 
<h3 style="margin-left:0;text-align:justify;">    III. 基线模型架构</h3> 
<p style="margin-left:0;text-align:justify;">如图2所示，基线YOLOv5网络由两个主要组件组成：主干和头部（包括颈部）。该主干旨在提取低级纹理和高级语义特征。接下来，这些提示（hint??）特征被输入到Head来构建增强的特征金字塔网络，从上到下传递鲁棒的语义特征，并从下到上传播对局部纹理和模式特征的强烈响应。这就解决了物体的不同尺度问题，增强了对不同尺度物体的检测能力。</p> 
<p style="margin-left:0;text-align:justify;">在图3中利用CSPNet [39]作为骨干来提取特征信息，由大量的采样卷积C-批归一化B-SiLu（CBS）组件和跨阶段部分（CSP）模块组成。CBS由卷积、批处理归一化和激活函数SiLu [40]等操作组成。CSP将前一层的特征映射复制为两个分支，然后通过1x1卷积将信道数减半，从而减少了计算量。对于特征图的两个副本，一个被连接到阶段的末端，另一个被发送到ResNet块或CBS块作为输入。最后，将特征映射的两个副本连接起来，以组合这些特征，然后是一个CBS块。SPP（空间金字塔池）模块[41]由不同内核大小的并行Maxpool层组成，用于提取多尺度深度特征。通过堆叠的CSP、CBS和SPP结构提取底层纹理和高级语义特征。</p> 
<p style="margin-left:0;text-align:justify;"><strong>缺陷1</strong>：值得一提的是，引入焦点模块是为了减少计算的数量。如图2（左下角）所示，输入被划分为单独的像素，每隔一段时间进行重建，最后连接到通道维度中。将输入的大小调整到更小的规模，以降低计算成本，加速网络训练和推理速度。然而，这可能会在一定程度上牺牲目标检测精度，特别是对于容易被分辨率的小物体。</p> 
<p style="margin-left:0;text-align:justify;"><strong>缺陷2</strong>：已知YOLO的主干采用深度卷积神经网络提取层次特征，步长为2，通过步长提取的特征的大小减半。因此，用于多尺度检测所保留的特征尺寸远小于原始输入图像。例如，当输入图像大小为608时，针对最后一个检测层的输出特征的大小分别为76、38和19。LR特性可能会导致丢失一些小目标。</p> 
<h3 style="margin-left:0;text-align:justify;">IV. SuperYOLO结构</h3> 
<p style="margin-left:0;text-align:justify;">       如图2所示，我们为我们的SuperYOLO网络架构介绍了三个新的贡献。首先，我们删除了主干网中的焦点模块，并将其替换为一个MF模块，以避免分辨率下降，从而导致精度下降。其次，我们探索了不同的融合方法，并选择了计算效率高的像素级融合来融合RGB和IR模式，以细化不同和互补的信息。最后，我们在训练阶段添加了一个辅助SR模块，它重构HR图像，指导空间维度上的相关主干学习，从而维护HR信息。在推理阶段，丢弃SR分支，以避免引入额外的计算开销。</p> 
<p style="margin-left:0;text-align:justify;">A.焦点移除</p> 
<p style="margin-left:0;text-align:justify;">如第三节和图中所示。2（左下），YOLOv5主干网中的Focus模块在空间域上以间隔分割图像，然后重新组织新图像以调整输入图像的大小。具体来说，该操作是为图像中每一组像素的值，然后进行重构，得到更小的互补图像。重建的图像的大小随着通道数量的增加而减小。因此，它会导致小目标的分辨率下降和空间信息损失。考虑到对小目标的检测更依赖于更高的分辨率，因此放弃了Focus模块，用一个MF模块取代（如图4所示）来防止分辨率降低。</p> 
<p style="margin-left:0;text-align:justify;">B.多模态融合</p> 
<p style="margin-left:0;text-align:justify;">区分对象的信息越多，目标检测的性能就越好。多模态融合是合并来自不同传感器的不同信息的有效路径。决策级、特征级和像素级的融合是三种主流的融合方法，可以部署在</p> 
<p style="margin-left:0;text-align:justify;">网络的不同深度。由于决策级融合需要大量的计算，因此在SuperYOLO中不考虑它。</p> 
<p style="margin-left:0;text-align:justify;">我们提出了一种像素级的多模态融合（MF）来从不同的模态中提取共享的和特殊的信息。MF可以以对称和紧凑的方式双向结合多模态内部信息。如图4所示，对于像素级融合，我们首先将输入的RGB图像和输入的IR图像归一化为两个[0,1]区间。输入模态 XRGB、XIR ∈ R C×H×W 被子采样为 IRGB、IIR ∈ R C× H n × W n，再送入 SE 块提取信道域的内部信息[42]，生成 FRGB、FIR：</p> 
<p style="margin-left:0;text-align:center;">FRGB = SE(IRGB), FIR = SE(IIR), (1)</p> 
<p style="margin-left:0;text-align:justify;">然后，揭示了空间域的几个不同模态之间的内在关系的注意力机制定义如下：</p> 
<p style="margin-left:0;text-align:center;">mIR = f1(FIR), mRGB = f2(FRGB), (2)</p> 
<p style="margin-left:0;text-align:justify;">其中f1和f2分别代表RGB模态和IR模态下的1x1卷积。此处的⊗记为像素级的矩阵乘法。不同模态之间的内部空间信息产生于：</p> 
<p style="margin-left:0;text-align:center;">Fin1 = mRGB ⊗ FRGB, Fin2 = mIR ⊗ FIR. (3)</p> 
<p style="margin-left:0;text-align:justify;">为了整合内部内视图信息和空间纹理信息，这些特征由原始输入模式添加，然后输入到1x1个卷积中。完整特征包括：</p> 
<p style="margin-left:0;text-align:center;">Fful1 = f3(Fin1 + IRGB), Fful2 = f4(Fin2 + IIR). (4)</p> 
<p style="margin-left:0;text-align:justify;">此时f3和f4表示1x1卷积。最后特征融合为：</p> 
<p style="margin-left:0;text-align:center;">Fo = SE(Concat(Fful1, Fful2)). (5)</p> 
<p style="margin-left:0;text-align:justify;">       其中，Concat（·）表示沿通道轴的连接操作。然后将结果输入到主干中，以产生多级特性。请注意，X被下采样到原始图像的1/n大小，以完成第IV-C节中讨论的SR模块，并加速训练过程。X表示RGB或IR模态，采样图像记为I∈R CxH/nxW/n并且生成：</p> 
<p style="margin-left:0;text-align:center;">I=D（X）</p> 
<p style="margin-left:0;text-align:justify;">       其中，D（·）表示使用双线性插值法进行的n次降采样操作。</p> 
<p style="margin-left:0;text-align:justify;">C.超分辨率</p> 
<p style="margin-left:0;text-align:justify;">如第三节所述，在主干网中保留的用于多尺度检测的特征大小远小于原始输入图像的特征大小。现有的方法大多是进行上采样操作来恢复特征大小。不幸的是，由于纹理和模式上的信息丢失，这种方法取得的成功有限，这解释了使用此操作来检测RSI中需要保存HR的小目标是不合适的。</p> 
<p style="margin-left:0;text-align:justify;">为了解决这个问题，如图2所示，我们引入了一个辅助的SR分支。首先，引入的分支应便于提取骨干和高分辨率信息，达到令人满意的性能。其次，该分支不应该增加更多的计算量来降低推理速度。在推理阶段，应实现精度和计算时间之间的权衡。受Wang等人的研究启发。在[38]中，所提出的超分辨率成功地促进了分割任务，而没有额外的需求，我们在框架中引入了一个简单而有效的分支SR。我们提出的模块可以提高检测精度，在不需要计算和内存的情况下过载，特别是在LR输入的情况下。</p> 
<p style="margin-left:0;text-align:justify;">具体来说，SR结构可以看作是一个简单的编码-解码器模型。我们选择主干的低级和高级特征，分别融合局部纹理、模式和语义信息。如图4所示，我们选择第四个和第九个模块的结果分别作为低级和高级特征。该编码器集成了在主干网中生成的低级特性和高级特性。如图5所示，在编码器中，第一个CR模块是在低级特性上进行的。对于高级特征，我们使用一个上采样操作来匹配低层次特征的空间大小，然后我们使用一个连接操作和两个CR模块来合并低级特征和高级特征。CR模块包括一个卷积和ReLU。对于解码器，将LR特征放大到HR空间，其中SR模块的输出大小是输入图像的两倍。如图5所示，解码器采用三个反卷积层来实现。SR指导了空间维度的相关学习，并将其转移到主分支，从而提高了目标检测的性能。此外，我们引入了EDSR [43]作为我们的编码器结构，以探索SR性能及其对检测性能的影响。为了提供更直观的解释描述，我们在图6中可视化了YOLOv5s、YOLOv5x和SuperYOLO的骨干特征。这些特征被上采样到与输入图像相同的尺度，以进行比较。通过比较图中(c)、(f)和(i)、(d)、(g)和(j)、(e) (h)和(k)的两对图像。可以观察到，在SR的帮助下，SuperYOLO包含更清晰、分辨率更高的物体结构。最终，我们利用 SR 分支获得了高质量 HR 表示，并利用 YOLOv5 的头部探测到了小物体。</p> 
<p style="margin-left:0;text-align:justify;">D.损失函数</p> 
<p style="margin-left:0;text-align:justify;">       我们的网络的整体损失由两个组成部分组成：检测损失Lo 和SR建筑损失Ls，它可以表示为</p> 
<p style="margin-left:0;text-align:center;">Ltotal = c1Lo + c2Ls, (7)</p> 
<p style="margin-left:0;text-align:justify;">其中c1和c2是平衡两个训练任务的系数。L1损失（而不是L2损失）[44]用于在输入图像X和SR结果S之间计算SR结构损失Ls，其中的表达式被写为</p> 
<p style="margin-left:0;text-align:center;">Ls = ||S – X|| 1 (8)</p> 
<p style="margin-left:0;text-align:justify;">检测损失包括三个组成部分[19]：损失判断是否存在对象Lobj，失去目标位置Lloc，以及丢失的对象分类Lcls，它被用于评估预测的损失为</p> 
<p class="img-center"><img alt="" height="299" src="https://images2.imgbox.com/4c/10/Ox27yws6_o.png" width="534"></p> 
<p style="margin-left:0;text-align:justify;">式 9 中，l 代表头部输出层，al、bl 和 cl 是三个损失函数的不同层的权重，权重 λloc、λobj 和 λcls 调节箱体坐标、箱体尺寸、对象性、无对象性和分类之间的误差重点。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/169824b6a50ef01eab35820f46ffe728/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">匿名函数、迭代器对象、内置函数、异常捕获</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a811e7b1031de011f7882dc91edf0f46/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">学深度学习可以做什么？可以从事什么工作？</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>