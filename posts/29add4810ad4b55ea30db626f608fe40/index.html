<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ZERO-SHOT：多聚焦融合 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="ZERO-SHOT：多聚焦融合" />
<meta property="og:description" content="ZERO-SHOT MULTI-FOCUS IMAGE FUSION （零镜头多焦点图像融合）
多聚焦图像融合 (Multi-focus image fusion (MFIF)) 是消除成像过程中产生的离焦模糊的有效方法。The difﬁculties in focus level estimation and the lack of real training set for supervised learning make MFIF remai a challenging task after decades of research.
我们提出了一种名为IM-Net的新颖体系结构，该体系结构由I-Net组成，用于对融合图像的深度先验进行建模，而M-Net则用于对焦点图的深度先验进行建模。在没有任何大规模训练集的情况下，我们的方法通过提取的先验信息实现了零射击学习。
介绍 在成像系统中，由于深度场 (depth-of-ﬁeld (DOF)) 的限制，相机焦平面外的物体会变得模糊，从而难以获得全聚焦图像，并导致图像质量大大降低。近年来，已经提出了各种多焦点图像融合 (MFIF) 算法来解决此问题。它们可以将源图像对的聚焦区域 (具有不同焦距) 在同一场景中进行组合，从而获得全聚焦高质量和信息性的聚焦图像，具有广泛的应用。
通常，MFIF方法可以分为三类: 基于变换域的方法，基于空间域的方法和基于深度学习的方法。
基于变换域的方法使用手工制作的图像分解算法将原始图像转换为变换域，以便能够更好地编码以区分清晰的几何特征，然后融合变换后的图像，最后进行逆变换以获得融合的图像。1985年，Burt等人 提出了第一个基于拉普拉斯金字塔的多尺度分解MFIF方法。之后出现了一系列基于多尺度分解的融合方法，包括基于小波变换、基于DCT等。
这些基于变换域的方法已被广泛使用，因为它们可以避免直接操纵像素引起的伪像（artifacts），但是由于对高频分量的敏感性，它们容易导致失真。通过估计二进制焦点图，然后基于获得的焦点图执行源图像对的加权和，基于空间域的方法开始引起注意。
但是这些传统的基于先验的方法以活动度量和融合规则的设计为主要任务，并提出了许多手工制作的活动度量来估计基于低级特征的清晰度，例如边缘信息或梯度信息的减少，以及像素强度或对比度的降低。
同时缺点也是：这些手工制作的功能无法准确表征图像是否聚焦。为了减轻手工先验 (手工图像分解方法或手工特征) 的依赖性，已经提出了许多基于深度学习的方法，通过这些方法可以共同优化活动度量和融合规则，以获得更好的效果。
基于深度学习的MFIF方法可以进一步分为基于监督学习的方法和基于无监督学习的方法。Liu等人提出了第一个有监督的暹罗结构（Siamese structured ）CNN网络，用于MFIF在补丁级别执行分类。Guo等人提出使用全卷积网络来获得从源图像对到焦点图的端到端映射。Xiao等人设计了一个精致的监督模型，以充分利用低级和高级信息。
这些使用具有手工参数的合成数据集的基于监督学习的模型可能与真正的成像过程不一致，后者需要考虑点扩散函数 (PSF) 以及物体和镜头之间的距离。--------&gt;所以，无监督学习成为一种直接解决方案。具体来说，Prabhakar等人采用了预先训练的自动编码器来提取特征，并将原始图像映射到高维特征空间中。由于发现提取的深层特征具有良好的泛化，鲁棒性和开发潜力，因此基于这项工作进行了一些改进 。最近，一种称为MFF-GAN的无监督融合方法在联合梯度约束下进行了对抗博弈。此外Ma等人训练了一个strongly self-supervised mask generator 以直接生成binary mask，而无需任何后处理 。尽管这些基于深度学习的方法已经达到了最先进的 (SOTA) 性能，但它们中的大多数都以有监督的 (具有地面真相) 或经过训练的 (具有用于训练的大图像集) 方式工作。为了解决这种具有挑战性且较少触及的问题，人们高度期望开发一种新颖的深度神经网络，该网络可以同时以无监督和未经训练的方式工作，同时实现有希望的性能。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/29add4810ad4b55ea30db626f608fe40/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-12-18T01:00:00+08:00" />
<meta property="article:modified_time" content="2022-12-18T01:00:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ZERO-SHOT：多聚焦融合</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="ZEROSHOT_MULTIFOCUS_IMAGE_FUSION_0"></a>ZERO-SHOT MULTI-FOCUS IMAGE FUSION</h2> 
<p>（零镜头多焦点图像融合）</p> 
<p>多聚焦图像融合 (Multi-focus image fusion (MFIF)) 是消除成像过程中产生的离焦模糊的有效方法。The difﬁculties in <strong>focus level estimation</strong> and <strong>the lack of real training set</strong> for supervised learning make MFIF remai a challenging task after decades of research.<br> 我们提出了一种名为IM-Net的新颖体系结构，该体系结构由<em><strong>I-Net组成，用于对融合图像的深度先验进行建模</strong></em>，而<em><strong>M-Net则用于对焦点图的深度先验进行建模</strong></em>。在没有任何大规模训练集的情况下，我们的方法<em><strong>通过提取的先验信息实现了零射击学习</strong></em>。</p> 
<h3><a id="_8"></a>介绍</h3> 
<p>在成像系统中，由于深度场 (depth-of-ﬁeld (DOF)) 的限制，相机焦平面外的物体会变得模糊，从而难以获得全聚焦图像，并导致图像质量大大降低。近年来，已经提出了各种多焦点图像融合 (MFIF) 算法来解决此问题。它们可以将源图像对的聚焦区域 (具有不同焦距) 在同一场景中进行组合，从而获得全聚焦高质量和信息性的聚焦图像，具有广泛的应用。</p> 
<p>通常，<em><strong>MFIF方法可以分为三类:</strong></em> 基于变换域的方法，基于空间域的方法和基于深度学习的方法。<br> <em><strong>基于变换域的方法</strong></em>使用手工制作的图像分解算法将原始图像转换为变换域，以便能够更好地编码以区分清晰的几何特征，然后融合变换后的图像，最后进行逆变换以获得融合的图像。1985年，Burt等人 提出了第一个基于拉普拉斯金字塔的多尺度分解MFIF方法。之后出现了一系列基于多尺度分解的融合方法，包括基于小波变换、基于DCT等。<br> 这些基于变换域的方法已被广泛使用，因为它们可以避免直接操纵像素引起的伪像（artifacts），但是由于对高频分量的敏感性，它们容易导致失真。通过估计二进制焦点图，然后基于获得的焦点图执行源图像对的加权和，基于空间域的方法开始引起注意。</p> 
<p>但是这些<em><strong>传统的基于先验的方法以活动度量和融合规则的设计为主要任务</strong></em>，并提出了许多手工制作的活动度量来估计基于低级特征的清晰度，例如边缘信息或梯度信息的减少，以及像素强度或对比度的降低。<br> <em><strong>同时缺点也是</strong></em>：这些手工制作的功能无法准确表征图像是否聚焦。为了减轻手工先验 (手工图像分解方法或手工特征) 的依赖性，已经提出了许多基于深度学习的方法，通过这些方法可以共同优化活动度量和融合规则，以获得更好的效果。</p> 
<p>基于深度学习的MFIF方法可以进一步分为基<em><strong>于监督学习的方法和基于无监督学习的方法</strong></em>。Liu等人提出了第一个有监督的暹罗结构（Siamese structured ）CNN网络，用于MFIF在补丁级别执行分类。Guo等人提出使用全卷积网络来获得从源图像对到焦点图的端到端映射。Xiao等人设计了一个精致的监督模型，以充分利用低级和高级信息。</p> 
<p>这些使用<em>具有手工参数的合成数据集</em>的<strong>基于监督学习的模型</strong><em>可能与真正的成像过程不一致</em>，后者需要考虑点扩散函数 (PSF) 以及物体和镜头之间的距离。--------&gt;所以，无监督学习成为一种直接解决方案。具体来说，Prabhakar等人采用了预先训练的自动编码器来提取特征，并将原始图像映射到高维特征空间中。由于发现提取的深层特征具有良好的泛化，鲁棒性和开发潜力，因此基于这项工作进行了一些改进 。最近，一种称为<em>MFF-GAN</em>的无监督融合方法在联合梯度约束下进行了对抗博弈。此外Ma等人训练了一个strongly self-supervised mask generator 以直接生成binary mask，而无需任何后处理 。尽管这些基于深度学习的方法已经达到了最先进的 (SOTA) 性能，但它们中的大多数都以有监督的 (具有地面真相) 或经过训练的 (具有用于训练的大图像集) 方式工作。为了解决这种具有挑战性且较少触及的问题，人们高度期望开发一种新颖的深度神经网络，该网络可以同时以无监督和未经训练的方式工作，同时实现有希望的性能。</p> 
<p>最近，由Ulyanov等人提出的<em><strong>DIP（数字图像处理）</strong></em>。[1] 使用精心设计的 “沙漏” 发生器来捕获低级图像统计信息，向我们<strong>展示了未经训练的网络提取的深层特征可以用作许多低级任务的图像，而无需任何训练数据</strong>。基于这项工作，Gandelsman等人 提出了基于耦合DIP的图像分解任务的通用框架，Ren等人将DIP应用于图像去模糊并取得了视觉上有利的效果。</p> 
<p>受这些工作的启发，我们提出了一种称为IM-Net的新型MFIF网络，该网络采用两个联合子网络 (即I-Net，M-Net) 来生成融合图像和焦点图**。生成的融合图像和焦点图需要满足能够重建观察到的输入图像的约束**。IM-Net 仅使用观察到的输入多焦点图像中包含的信息来执行MFIF，这不遵循在具有一些地面真相的图像集上训练神经网络的常规范例。我们的IM-Net的主要优势在于，它可以避免劳动密集型数据收集和域转移问题，如现有的基于深度学习的方法，同时取得有希望的结果。</p> 
<h5><a id="_27"></a>贡献</h5> 
<p>（i）这项工作可能是MFIF的首次zero-shot methods 之一，可以预测清晰的融合图像，而无需地面真相或图像收集。<br> (ii) 受DIP的启发，应用两个生成网络同时对clean fused image和焦点图的深度先验进行建模，<em>结合了基于焦点图估计的方法可以很好地保留源图像信息的优点和基于融合图像生成的方法可以提供良好的视觉效果的优点</em>。<br> (iii) 将几个SOTAs与我们的方法进行了比较，以证明我们的IM-Net的有效性。</p> 
<h4><a id="_33"></a>方法</h4> 
<p>MFIF可以看作是根据聚焦图I<sub>m</sub> ∈<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          R 
         
         
         
           m 
          
         
           × 
          
         
           n 
          
         
        
       
      
        R^{m × n} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.771331em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.771331em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight">n</span></span></span></span></span></span></span></span></span></span></span></span></span>的源图像I<sub>A</sub> ∈<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          R 
         
         
         
           m 
          
         
           × 
          
         
           n 
          
         
           × 
          
         
           c 
          
         
        
       
      
        R^{m × n × c} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.771331em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.771331em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight">n</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight">c</span></span></span></span></span></span></span></span></span></span></span></span></span>和I<sub>B</sub> ∈<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          R 
         
         
         
           m 
          
         
           × 
          
         
           n 
          
         
           × 
          
         
           c 
          
         
        
       
      
        R^{m × n× c} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.771331em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.771331em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight">n</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight">c</span></span></span></span></span></span></span></span></span></span></span></span></span>加权和，M和N分别表示源图像的高度和宽度，C表示源图像的通道数。为简单起见，我们只考虑两个源图像的融合，因为它可以很容易地扩展到其他情况。融合图像I<sub>fused</sub> ∈<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          R 
         
         
         
           m 
          
         
           × 
          
         
           n 
          
         
           × 
          
         
           c 
          
         
        
       
      
        R^{m × n × c} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.771331em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.771331em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight">n</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight">c</span></span></span></span></span></span></span></span></span></span></span></span></span>可以计算为<br> <img src="https://images2.imgbox.com/95/b3/OROPOwa4_o.png" alt="请添加图片描述"><br> 如上所述，许多MFIF方法都深深依赖于手工制作的先验。在本文中，我们探索了深度神经网络作为图像先验的能力，并将深度先验应用于融合多焦点图像对。受DIP 和double-DIP的启发，我们将MFIF转换为由两个生成网络组成的 “zero-shot”自我监督学习形式，可以对干clean fused image I<sub>fused</sub> 的深度先验进行建模。和聚焦图I<sub>m</sub>满足上述等式。图1显示了我们方法的主要结构。分别从均匀分布中随机采样两个输入噪声Z<sub>i</sub>和Z<sub>m</sub>，然后通过两个基于U-Net的沙漏结构网络I-Net和M-Net，以获得融合图像和估计的焦点图I<sub>m</sub>。<br> <img src="https://images2.imgbox.com/26/da/WTCkL8YS_o.png" alt="请添加图片描述"></p> 
<h5><a id="Network_Architecture_42"></a>Network Architecture</h5> 
<p>网络架构DIP使用随机初始化的深度网络来拟合单个图像，并将提取特征作为图像的深度先验，但是图像样本的缺乏使得深度模型容易过度拟合，精心设计的沙漏网络可以大大缓解这一问题。<em><strong>U-Net擅长提取低级和高级信息</strong></em>，因此DIP采用U-Net作为其骨干。下采样模块和上采样模块之间的不对称设计可以有效地避免琐碎的解决方案，即，如果出现与IA或IB相同，而Im则出现全白或全黑，这会使算法难以优化。大量的BatchNorm层也可以使网络更好地适合高频组件。我们的im-net的网络体系结构如上图所示。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f84155c41d0043875d33a2941fa4c7ea/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">国产实时操作系统&#43;intel x86/龙芯平台超边缘计算机方案</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/39293401082507e755fd42dd6f481863/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">ArcGIS And ENVI：如何进行植被指数的提取并制作成专题地图？</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>