<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>语音合成综述Speech Synthesis - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="语音合成综述Speech Synthesis" />
<meta property="og:description" content="一、语音合成概述 语音信号的产生分为两个阶段，信息编码和生理控制。首先在大脑中出现某种想要表达的想法，然后由大脑将其编码为具体的语言文字序列，及语音中可能存在的强调、重读等韵律信息。经过语言的组织，大脑通过控制发音器官肌肉的运动，产生出相应的语音信号。其中第一阶段主要涉及人脑语言处理方面，第二阶段涉及语音信号产生的生理机制。
从滤波的角度，人体涉及发音的器官可以分为两部分：激励系统和声道系统。激励系统中，储存于肺部的空气源，经过胸腔的压缩排出，经过气管进入声带，根据发音单元决定是否产生振动，形成准周期的脉冲空气激励流或噪声空气激励流。这些空气流作为激励，进入声道系统，被频率整形，形成不同的声音。声道系统包括咽喉、口腔（舌、唇、颌和口）组成，可能还包括鼻道。不同周期的脉冲空气流或者噪声空气流，以及不同声道器官的位置决定了产生的声音。因此，语音合成中通常将语音的建模分解为激励建模和声道建模。
1. 语音合成的历史和研究方法 语音合成系统分为两部分，分别称为文本前端和后端。文本前端主要负责在语言层、语法层、语义层对输入文本进行文本分析；后端主要是从信号处理、模式识别、机器学习等角度，在语音层面上进行韵律特征建模，声学特征建模，然后进行声学预测或者在音库中进行单元挑选，最终经过合成器或者波形拼接等方法合成语音。
根据语音合成研究的历史，语音合成研究方法可以分为：机械式语音合成器、电子式语音合成器、共振峰参数合成器、基于波形拼接的语音合成（Concatenative Speech Synthesis）、统计参数语音合成（Statistical Parametric Speech Synthesis，SPSS）、以及神经网络语音合成。
早期的语音合成方法由于模型简单，系统复杂等原因，难以在实际场景应用。随着计算机技术的发展，基于波形拼接的语音合成被提出。基于波形拼接的语音合成 Concatenative Speech Synthesis的基本原理是首先构建一个音库，在合成阶段，通过对合成文本的分析，按照一定的准则，从音库中挑选出与待合成语音相似的声学单元，对这些声学单元进行少量调整，拼接得到合成的语音。早期的波形拼接系统受限于音库大小、挑选算法、拼接调整的限制，合成语音质量较低。1990年，基于同步叠加的时域波形修改算法被提出，解决了声学单元拼接处的局部不连续问题。更进一步，基于大语料库的波形拼接语音合成方法被提出，采用更精细的挑选策略，将语音音库极大地拓展，大幅提升了合成语音的自然度。由于直接使用发音人的原始语音，基于波形拼接的语音合成方法合成语音的音质接近自然语音，被广泛应用。但其缺点也较为明显，包括音库制作时间长、需要保存整个音库、拓展性差、合成语音自然度受音库和挑选算法影响，鲁棒性不高等。
随着统计建模理论的完善，以及对语音信号理解的深入，基于统计参数的语音合成方法（Statistical Parametric Speech Synthesis，SPSS）被提出。其基本原理是使用统计模型，对语音的参数化表征进行建模。在合成阶段，给定待合成文本，使用统计模型预测出对应的声学参数，经过声码器vocoder合成语音波形。统计参数语音合成方法是目前的主流语音合成方法之一。统计参数音合成方法的优点很多，包括只需要较少的人工干预，能够快速地自动构建系统，同时具有较强的灵活性，能够适应不同发音人，不同发音风格，多语种的语音合成，具有较强的鲁棒性等。由于语音参数化表示以及统计建模的平均效应，统计参数语音合成方法生成的语音自然度相比自然语音通常会有一定的差距。基于隐马尔科夫HMM的统计参数语音合成方法是发展最为完善的一种。基于HMM的统计参数语音合成系统能够同时对语音的基频、频谱和时长进行建模，生成出连续流畅且可懂度高的语音，被广泛应用，但其合成音质较差。
和统计参数语音合成系统类似，深度学习语音合成系统也可大致分为两个部分：文本前端和声学后端。文本前端的主要作用是文本预处理，如：为文本添加韵律信息，并将文本词面转化为语言学特征序列（Linguistic Feature Sequence）；声学后端又可以分为声学特征生成网络和声码器，其中声学特征生成网络根据文本前端输出的信息产生声学特征，如：将语言学特征序列映射到梅尔频谱Mel 或线性谱；声码器利用频谱等声学特征，生成语音样本点并重建时域波形，如：将梅尔频谱恢复为对应的语音。近年来，也出现了完全端到端的语音合成系统，将声学特征生成网络和声码器和合并起来，声学后端成为一个整体，直接将语言学特征序列，甚至文本词面端到端转换为语音波形。
2. 语音合成各部分 2.1. 文本前端 文本前端的作用是从文本中提取发音和语言学信息，其任务至少包括以下四点。
(a). 文本正则化
在语音合成中，用于合成的文本存在特殊符号、阿拉伯数字等，需要把符号转换为文本。如“1.5 元” 需要转换成“一点五元”，方便后续的语言学分析。
(b). 韵律预测
该模块的主要作用是添加句子中韵律停顿或起伏。如“在抗击新型冠状病毒的战役中，党和人民群众经受了一次次的考验”，如果停顿信息不准确就会出现：“在/抗击/新型冠状病毒/的/战役中，党/和/人民群众/经受了/一次/次/的/考验”。“一次次”的地方存在一个错误停顿，这将会导致合成语音不自然，如果严重些甚至会影响语义信息的传达。
(c). 字形转音素
文字转化为发音信息。比如“中国”是汉字表示，需要先将其转化为拼音“zhong1 guo2”，以帮助后续的声学模型更加准确地获知每个汉字的发音情况。
(d). 多音字和变调
许多语言中都有多音字的现象，比如“模型”和“模样”，这里“模”字的发音就存在差异。另外，汉字中又存在变调现象，如“一个”和“看一看”中的“一”发音音调不同。所以在输入一个句子的时候，文本前端就需要准确判断出文字中的特殊发音情况，否则可能会导致后续的声学模型合成错误的声学特征，进而生成不正确的语音。
2.2. 声学特征生成网络 Acoustic model 声学特征生成网络根据文本前端的发音信息，产生声学特征，如梅尔频谱或线性谱。近年来，基于深度学习的生成网络甚至可以去除文本前端，直接由英文等文本生成对应的频谱。但是一般来说，因为中文字形和读音关联寥寥，因此中文语音合成系统大多无法抛弃文本前端，换言之，直接将中文文本输入到声学特征生成网络中是不可行的。基于深度学习的声学特征生成网络发展迅速，比较有代表性的模型有Tacotron系列，FastSpeech系列等。近年来，也涌现出类似于VITS的语音合成模型，将声学特征生成网络和声码器融合在一起，直接将文本映射为语音波形。
2.3. 声码器 Vocoder 通过声学特征产生语音波形的系统被称作声码器，声码器是决定语音质量的一个重要因素。一般而言，声码器可以分为以下4类：纯信号处理，如Griffin-Lim、STRAIGHT和WORLD；自回归深度网络模型，如WaveNet和WaveRNN；非自回归模型，如Parallel WaveNet、ClariNet和WaveGlow；基于生成对抗网络（Generative Adversarial Network，GAN）的模型，如MelGAN、Parallel WaveGAN和HiFiGAN。
3. 语音合成评价指标 对合成语音的质量评价，主要可以分为主观和客观评价。主观评价是通过人类对语音进行打分，比如平均意见得分（Mean Opinion Score，MOS）、众包平均意见得分（CrowdMOS，CMOS）和ABX测试。客观评价是通过计算机自动给出语音音质的评估，在语音合成领域研究的比较少，论文中常常通过展示频谱细节，计算梅尔倒谱失真（Mel Cepstral Distortion，MCD）等方法作为客观评价。客观评价还可以分为有参考和无参考质量评估，这两者的主要判别依据在于该方法是否需要标准信号。有参考评估方法除了待评测信号，还需要一个音质优异的，可以认为没有损伤的参考信号。常见的有参考质量评估主要有ITU-T P.861 (MNB)、ITU-T P.862 (PESQ)、ITU-T P.863 (POLQA)、STOI和BSSEval。无参考评估方法则不需要参考信号，直接根据待评估信号，给出质量评分，无参考评估方法还可以分为基于信号、基于参数以及基于深度学习的质量评估方法。常见的基于信号的无参考质量评估包括ITU-T P." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/e39bf5ff2b9db35033c1a550874f3476/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-11-24T20:05:46+08:00" />
<meta property="article:modified_time" content="2023-11-24T20:05:46+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">语音合成综述Speech Synthesis</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2><span style="color:#be191c;"><strong>一、语音合成概述</strong></span></h2> 
<p>语音信号的产生分为两个阶段，<strong>信息编码和生理控制</strong>。首先在大脑中出现某种想要表达的想法，然后由大脑将其编码为具体的语言文字序列，及语音中可能存在的强调、重读等韵律信息。经过语言的组织，大脑通过控制发音器官肌肉的运动，产生出相应的语音信号。其中第一阶段主要涉及人脑语言处理方面，第二阶段涉及语音信号产生的生理机制。</p> 
<p>  从滤波的角度，<strong>人体涉及发音的器官</strong>可以分为两部分：<strong>激励系统</strong>和<strong>声道系统</strong>。激励系统中，储存于肺部的空气源，经过胸腔的压缩排出，经过气管进入声带，根据发音单元决定是否产生振动，形成准周期的脉冲空气激励流或噪声空气激励流。这些空气流作为激励，进入声道系统，被频率整形，形成不同的声音。声道系统包括咽喉、口腔（舌、唇、颌和口）组成，可能还包括鼻道。不同周期的脉冲空气流或者噪声空气流，以及不同声道器官的位置决定了产生的声音。因此，语音合成中通常将语音的建模分解为激励建模和声道建模。</p> 
<h3>1. 语音合成的历史和研究方法</h3> 
<p>语音合成系统分为两部分，分别称为<strong>文本前端</strong>和<strong>后端</strong>。<strong>文本前端</strong>主要负责在语言层、语法层、语义层对输入文本进行<strong>文本分析</strong>；<strong>后端</strong>主要是从信号处理、模式识别、机器学习等角度，在<strong>语音层面</strong>上进行韵律特征建模，声学特征建模，然后进行声学预测或者在音库中进行单元挑选，最终经过合成器或者波形拼接等方法合成语音。</p> 
<p><img alt="" height="217" src="https://images2.imgbox.com/3d/02/Mj4BVXRp_o.png" width="1040"></p> 
<p><img alt="" height="612" src="https://images2.imgbox.com/97/8a/ouYH386J_o.png" width="1200"></p> 
<p>根据语音合成研究的历史，语音合成研究方法可以分为：机械式语音合成器、电子式语音合成器、共振峰参数合成器、基于波形拼接的语音合成（Concatenative Speech Synthesis）、<strong>统计参数语音合成（Statistical Parametric Speech Synthesis，SPSS</strong><strong>）</strong>、以及神经网络语音合成。</p> 
<p>早期的语音合成方法由于模型简单，系统复杂等原因，难以在实际场景应用。随着计算机技术的发展，基于波形拼接的语音合成被提出。<strong>基于波形拼接的语音合成 Concatenative Speech Synthesis</strong>的基本原理是首先构建一个<strong>音库</strong>，在合成阶段，通过对合成文本的分析，按照一定的准则，从音库中挑选出与待合成语音相似的<strong>声学单元</strong>，对这些声学单元进行少量调整，拼接得到合成的语音。早期的波形拼接系统受限于音库大小、挑选算法、拼接调整的限制，合成语音质量较低。1990年，<strong>基于同步叠加的时域波形修改算法</strong>被提出，解决了声学单元拼接处的局部不连续问题。更进一步，基于大语料库的波形拼接语音合成方法被提出，采用更精细的挑选策略，将语音音库极大地拓展，大幅提升了合成语音的自然度。由于直接使用发音人的原始语音，基于波形拼接的语音合成方法合成语音的音质接近自然语音，被广泛应用。但其缺点也较为明显，包括音库制作时间长、需要保存整个音库、拓展性差、合成语音自然度受音库和挑选算法影响，鲁棒性不高等。</p> 
<p> 随着统计建模理论的完善，以及对语音信号理解的深入，<strong>基于统计参数的语音合成方法（Statistical Parametric Speech Synthesis，SPSS</strong><strong>）</strong>被提出。其基本原理是使用统计模型，对<strong>语音的参数化表征</strong>进行建模。在合成阶段，给定待合成文本，使用统计模型预测出对应的声学参数，经过声码器vocoder合成语音波形。统计参数语音合成方法是目前的主流语音合成方法之一。统计参数音合成方法的优点很多，包括只需要较少的人工干预，能够快速地自动构建系统，同时具有较强的灵活性，能够适应不同发音人，不同发音风格，多语种的语音合成，具有较强的鲁棒性等。由于语音参数化表示以及统计建模的平均效应，统计参数语音合成方法生成的语音自然度相比自然语音通常会有一定的差距。<strong>基于隐马尔科夫HMM的统计参数语音合成方法</strong>是发展最为完善的一种。基于HMM的统计参数语音合成系统能够同时对语音的<strong>基频、频谱和时长</strong>进行建模，生成出连续流畅且可懂度高的语音，被广泛应用，但其合成音质较差。</p> 
<p>和统计参数语音合成系统类似，<strong>深度学习语音合成系统</strong>也可大致分为两个部分：<strong>文本前端和声学后端</strong>。<strong>文本前端</strong>的主要作用是文本预处理，如：为文本添加韵律信息，并将文本词面转化为<strong>语言学特征序列（Linguistic Feature Sequence）</strong>；<strong>声学后端</strong>又可以分为声学特征生成网络和声码器，其中<strong>声学特征生成网络</strong>根据文本前端输出的信息产生声学特征，如：将语言学特征序列映射到梅尔频谱Mel 或线性谱；<strong>声码器利用频谱等声学特征，生成语音样本点并重建时域波形</strong>，如：将梅尔频谱恢复为对应的语音。近年来，也出现了完全端到端的语音合成系统，将声学特征生成网络和声码器和合并起来，声学后端成为一个整体，直接将语言学特征序列，甚至文本词面端到端转换为语音波形。</p> 
<h3>2. 语音合成各部分</h3> 
<h4><span style="color:#511b78;"><strong>2.1. 文本前端</strong></span></h4> 
<p>文本前端的作用是从文本中提取<strong>发音和语言学信息</strong>，其任务至少包括以下四点。<br> (a). <strong>文本正则化</strong><br> 在语音合成中，用于合成的文本存在特殊符号、阿拉伯数字等，需要把符号转换为文本。如“1.5 元” 需要转换成“一点五元”，方便后续的语言学分析。<br> (b). <strong>韵律预测</strong><br> 该模块的主要作用是添加句子中韵律<strong>停顿或起伏</strong>。如“在抗击新型冠状病毒的战役中，党和人民群众经受了一次次的考验”，如果停顿信息不准确就会出现：“在/抗击/新型冠状病毒/的/战役中，党/和/人民群众/经受了/一次/次/的/考验”。“一次次”的地方存在一个错误停顿，这将会导致合成语音不自然，如果严重些甚至会影响语义信息的传达。<br> (c). <strong>字形转音素</strong><br> 文字转化为发音信息。比如“中国”是汉字表示，需要先将其转化为拼音“zhong1 guo2”，以帮助后续的声学模型更加准确地获知每个汉字的发音情况。<br> (d). <strong>多音字和变调</strong><br> 许多语言中都有多音字的现象，比如“模型”和“模样”，这里“模”字的发音就存在差异。另外，汉字中又存在变调现象，如“一个”和“看一看”中的“一”发音音调不同。所以在输入一个句子的时候，文本前端就需要准确判断出文字中的特殊发音情况，否则可能会导致后续的声学模型合成错误的声学特征，进而生成不正确的语音。</p> 
<h4 style="background-color:transparent;"><span style="color:#511b78;"><strong>2.2. 声学特征生成网络 </strong></span>Acoustic model</h4> 
<p>声学特征生成网络根据文本前端的发音信息，产生声学特征，如<strong>梅尔频谱或线性谱</strong>。近年来，基于深度学习的生成网络甚至可以去除文本前端，直接由英文等文本生成对应的频谱。但是一般来说，因为中文字形和读音关联寥寥，因此中文语音合成系统大多无法抛弃文本前端，换言之，直接将中文文本输入到声学特征生成网络中是不可行的。基于深度学习的声学特征生成网络发展迅速，比较有代表性的模型有<strong>Tacotron系列</strong>，<strong>FastSpeech系列</strong>等。近年来，也涌现出类似于<strong>VITS的语音合成模型</strong>，将声学特征生成网络和声码器融合在一起，直接将文本映射为语音波形。</p> 
<h4><span style="color:#511b78;"><strong>2.3. 声码器 Vocoder</strong></span></h4> 
<p><strong>通过声学特征产生语音波形</strong>的系统被称作声码器，声码器是决定语音质量的一个重要因素。一般而言，声码器可以分为以下4类：<strong>纯信号处理</strong>，如Griffin-Lim、STRAIGHT和WORLD；<strong>自回归深度网络模型</strong>，如WaveNet和WaveRNN；<strong>非自回归模型</strong>，如Parallel WaveNet、ClariNet和WaveGlow；<strong>基于生成对抗网络</strong>（Generative Adversarial Network，GAN）的模型，如MelGAN、Parallel WaveGAN和HiFiGAN。</p> 
<h3><span style="color:#1a439c;"><strong>3. 语音合成评价指标</strong></span></h3> 
<p>对合成语音的质量评价，主要可以分为<strong>主观和客观评价</strong>。主观评价是通过人类对语音进行打分，比如平均意见得分（Mean Opinion Score，MOS）、众包平均意见得分（CrowdMOS，CMOS）和ABX测试。客观评价是通过计算机自动给出语音音质的评估，在语音合成领域研究的比较少，论文中常常通过展示频谱细节，计算<strong>梅尔倒谱失真（Mel Cepstral Distortion，MCD）</strong>等方法作为客观评价。<strong>客观评价</strong>还可以分为<strong>有参考和无参考质量评估</strong>，这两者的主要判别依据在于该方法是否需要标准信号。有参考评估方法除了待评测信号，还需要一个音质优异的，可以认为没有损伤的参考信号。常见的有参考质量评估主要有ITU-T P.861 (MNB)、ITU-T P.862 (PESQ)、ITU-T P.863 (POLQA)、STOI和BSSEval。无参考评估方法则不需要参考信号，直接根据待评估信号，给出质量评分，无参考评估方法还可以分为基于信号、基于参数以及基于深度学习的质量评估方法。常见的基于信号的无参考质量评估包括ITU-T P.563和ANIQUE+，基于参数的方法有ITU-T G.107(E-Model)。近年来，深度学习也逐步应用到无参考质量评估中，如：AutoMOS、QualityNet、NISQA和MOSNet。</p> 
<p>主观评价中的<strong>MOS评测</strong>是一种较为宽泛的说法，由于给出评测分数的主体是人类，因此可以灵活测试语音的不同方面。比如在语音合成领域，主要有<strong>自然度MOS（MOS of Naturalness）和相似度MOS（MOS of Similarity）</strong>。但是人类给出的评分结果受到的干扰因素较多，谷歌对合成语音的主观评估方法进行了比较，在评估较长语音中的单个句子时，音频样本的呈现形式会显著影响参与人员给出的结果。比如仅提供单个句子而不提供上下文，与相同句子给出语境相比，被测人员给出的评分差异显著。国际电信联盟（International Telecommunication Union，ITU）将MOS评测规范化为ITU-T P.800，其中绝对等级评分（Absolute Category Rating，ACR）应用最为广泛，ACR的详细评估标准有5.0-1.0从优到劣。</p> 
<p>在使用ACR方法对语音质量进行评价时，参与评测的人员（简称被试）对语音整体质量进行打分，分值范围为1~5分，分数越大表示语音质量越好。MOS大于4时，可以认为该音质受到大部分被试的认可，音质较好；若MOS低于3，则该语音有比较大的缺陷，大部分被试并不满意该音质。</p> 
<h3></h3> 
<h2 style="background-color:transparent;"><span style="color:#be191c;"><strong>二、语音信号基础</strong></span></h2> 
<h3>1. 语音基本概念</h3> 
<p>声波通过空气传播，被麦克风接收，通过 <strong>采样、量化、编码</strong>转换为离散的数字信号，即波形文件。音量、音高和音色是声音的基本属性。</p> 
<h4><span style="color:#511b78;"><strong>1.1 能量</strong></span></h4> 
<p>音频的能量通常指的是时域上每帧的能量，幅度的平方。在简单的语音活动检测（Voice Activity Detection，VAD）中，直接利用能量特征：能量大的音频片段是语音，能量小的音频片段是非语音（包括噪音、静音段等）。这种VAD的局限性比较大，正确率也不高，对噪音非常敏感。</p> 
<h4><span style="color:#511b78;"><strong>1.2 短时能量</strong></span></h4> 
<p>短时能量体现的是信号在不同时刻的强弱程度。设第 n 帧语音信号的短时能量用 <img alt="$E_n$" class="mathcode" src="https://images2.imgbox.com/1e/31/0hLhFtGf_o.png"> 表示，则其计算公式为：</p> 
<p><img alt="E_n=\sum_{m=0}^{M-1}x_n^2(m)" class="mathcode" src="https://images2.imgbox.com/ce/8a/bmAYMJ5w_o.png"></p> 
<p>上式中， <img alt="$M$" class="mathcode" src="https://images2.imgbox.com/47/8c/jiWOXncV_o.png"> 为帧长，<img alt="x_n(m)" class="mathcode" src="https://images2.imgbox.com/7e/17/3vl0tRjx_o.png"> 为该帧中的样本点。</p> 
<h4><span style="color:#511b78;"><strong>1.3 声强和声强级 </strong></span>sound intensity或acoustic intensity</h4> 
<p>单位时间内通过垂直于声波传播方向的单位面积的平均声能，称作声强，声强用 I 表示，单位为“瓦/平米”。实验研究表明，<strong>人对声音的强弱感觉并不是与声强成正比，而是与其对数成正比</strong>，所以一般声强用声强级来表示：</p> 
<p><img alt="L=10{\rm log}(\frac{I}{I'})" class="mathcode" src="https://images2.imgbox.com/d8/e3/7oVbp8ma_o.png"></p> 
<p>其中，I为声强，<img alt="$I'=10e^{-12}w/m^2$" class="mathcode" src="https://images2.imgbox.com/97/7c/bMcOodAX_o.png"> 称为基本声强，声强级的常用单位是分贝(dB)。</p> 
<h4><span style="color:#511b78;"><strong>1.4 响度 </strong></span>loudness</h4> 
<p>响度是一种主观心理量，是人类主观感觉到的声音强弱程度，又称音量。<strong>响度与声强和频率有关</strong>。一般来说，声音频率一定时，声强越强，响度也越大。相同的声强，频率不同时，响度也可能不同。响度若用对数值表示，即为<strong>响度级</strong>，响度级的单位定义为方，符号为phon。根据国际协议规定，0dB声强级的1000Hz纯音的响度级定义为0 phon，n dB声强级的1000Hz纯音的响度级就是n phon。其它频率的声强级与响度级的对应关系要从如图等响度曲线查出。</p> 
<p><img alt="" height="246" src="https://images2.imgbox.com/2e/89/enWDFxPY_o.png" width="274"></p> 
<h4><span style="color:#511b78;"><strong>1.5 过零率</strong></span></h4> 
<p>过零率体现的是信号过零点的次数，体现的是频率特性。</p> 
<p><img alt="Z_n=\sum_{n=0}^{N-1}\sum_{m=0}^{M-1}|{\rm sgn}(x_n(m))-{\rm sgn}(x_n(m-1))|" class="mathcode" src="https://images2.imgbox.com/e8/c1/v1RPHgWR_o.png"></p> 
<p>其中，<img alt="N" class="mathcode" src="https://images2.imgbox.com/ad/38/OltTK4F3_o.png"> 表示帧数，<img alt="$M$" class="mathcode" src="https://images2.imgbox.com/df/9d/qtDA7Zjd_o.png"> 表示每一帧中的样本点个数， <img alt="${\rm sgn}$" class="mathcode" src="https://images2.imgbox.com/3c/58/CncCQ6AU_o.png"> 为符号函数，即</p> 
<p><img alt="{\rm sgn}=\left\{\begin{matrix} &amp; 1,x \geq 0 \\ &amp; -1,x&lt;0 \end{matrix}\right." class="mathcode" src="https://images2.imgbox.com/98/1a/KV8VsNQs_o.png"></p> 
<h4><span style="color:#511b78;"><strong>1.6 共振峰</strong></span></h4> 
<p>声门处的准周期激励进入声道时会引起共振特性，产生一组共振频率，这一组共振频率称为共振峰频率或简称共振峰。共振峰包含在语音的频谱包络中，频谱包络的局部极大值就是共振峰。频率最低的共振峰称为第一共振峰，记作$f_1$，频率更高的共振峰称为第二共振峰$f_2$、第三共振峰$f_3$……以此类推。实践中一个元音用三个共振峰表示，复杂的辅音或鼻音，要用五个共振峰。</p> 
<h3><span style="color:#1a439c;"><strong>2. 语言学</strong></span></h3> 
<p>语言学研究人类的语言，计算语言学则是一门跨学科的研究领域，试图找出自然语言的规律，建立运算模型，语音合成其实就是计算语言学的子领域之一。在语音合成中，一般需要将文本转换为对应的音素，然后再将音素输入到后端模型中，因此需要为每个语种甚至方言构建恰当合理的音素体系。相关概念如下。</p> 
<ul><li><strong>音素（phoneme）</strong>：也称音位，是能够区别意义的最小语音单位，同一音素由不同人/环境阅读，可以形成不同的发音。</li><li><strong>字素（grapheme）</strong>：音素对应的文本。</li><li><strong>发音（phone）</strong>： 某个音素的具体发音。实际上，phoneme和phone都是指的是音素，音素可具化为实际的音，该过程称为音素的语音体现。一个音素可能包含着几个不同音值的音，因而可以体现为一个音、两个音或更多的同位音。但是在一些论述中，phoneme偏向于表示发音的符号，phone更偏向于符号对应的实际发音，因此phoneme可对应无数个phone。</li><li> <strong>音节（syllable）</strong>：音节由音素组成。在汉语中，除儿化音外，一个汉字就是一个音节。如wo3（我）是一个音节，zhong1（中）也是一个音节。</li></ul> 
<h3><span style="color:#1a439c;"><strong>3. 音频格式</strong></span></h3> 
<ul><li>*.wav: 波形无损压缩格式，是语音合成中音频语料的常用格式，主要的三个参数：采样率，量化位数和通道数。一般来说，合成语音的采样率采用16kHz、22050Hz、24kHz，对于歌唱合成等高质量合成场景采样率可达到48kHz；量化位数采用16bit；通道数采用1.</li><li>*.flac: Free Lossless Audio Codec，无损音频压缩编码。</li><li>*.mp3: Moving Picture Experts Group Audio Player III，有损压缩。</li><li>*.wma: Window Media Audio，有损压缩。</li><li>*.avi: Audio Video Interleaved，avi文件将音频和视频包含在一个文件容器中，允许音视频同步播放。</li></ul> 
<h3><span style="color:#1a439c;"><strong>4. 数字信号处理</strong></span></h3> 
<h4><span style="color:#511b78;"><strong>4.1. 模数转换 Analog to Digital Converter，ADC</strong></span></h4> 
<p>模拟信号到数字信号的转换（Analog to Digital Converter，ADC）称为模数转换。</p> 
<p><strong>奈奎斯特（Nyquist）采样定理</strong>：要从抽样信号中无失真地恢复原信号，<strong>抽样频率应大于2倍信号最高频率</strong>。抽样频率小于2倍频谱最高频率时，信号的频谱有混叠。抽样频率大于2倍频谱最高频率时，信号的频谱无混叠。如果对语音模拟信号进行采样率为16000Hz的采样，得到的离散信号中包含的最大频率为8000Hz。</p> 
<h4><span style="color:#511b78;"><strong>4.2. 频谱泄露 </strong></span>spectral leakage</h4> 
<p>音频处理中，经常需要利用傅里叶变换将时域信号转换到频域，而一次快速傅里叶变换（FFT）只能处理有限长的时域信号，但语音信号通常是长的，所以需要将原始语音截断成一帧一帧长度的数据块。这个过程叫 <strong>信号截断</strong>，也叫\lstinline{<!-- --><strong>分帧</strong>}。分完帧后再对每帧做FFT，得到对应的频域信号。FFT是离散傅里叶变换（DFT）的快速计算方式，而做DFT有一个先验条件：分帧得到的数据块必须是整数周期的信号，也即是每次截断得到的信号要求是周期主值序列。</p> 
<p>但做分帧时，很难满足 <strong>周期截断</strong>，因此就会导致 {<!-- --><strong>频谱泄露</strong>}。要解决非周期截断导致的频谱泄露是比较困难的，可以通过 {<!-- --><strong>加窗</strong>}尽可能减少频谱泄露带来的影响。窗类型可以分为汉宁窗、汉明窗、平顶窗等。虽然加窗能够减少频谱泄露，但加窗衰减了每帧信号的能量，特别是边界处的能量，这时加一个合成窗，且overlap-add，便可以补回能量。</p> 
<h4>4.3. 频率分辨率</h4> 
<p>频率分辨率是指将两个相邻谱峰分开的能力，在实际应用中是指分辨两个不同频率信号的最小间隔。</p> 
<p></p> 
<h2><span style="color:#be191c;"><strong>三、语音特征提取</strong></span></h2> 
<p>原始信号是不定长的时序信号，不适合作为机器学习的输入。因此一般需要将<strong>原始波形转换为特定的特征向量</strong>表示，该过程称为语音特征提取。</p> 
<h3><span style="color:#1a439c;"><strong>1. 预处理</strong></span></h3> 
<p>包括预加重、分帧和加窗。</p> 
<h4><span style="color:#511b78;"><strong>1.1 预加重 </strong></span><strong>pre-emphasis</strong></h4> 
<p>语音经过说话人的口唇辐射发出，受到唇端辐射抑制，高频能量明显降低。一般来说，当语音信号的频率提高两倍时，其功率谱的幅度下降约6dB，即语音信号的高频部分受到的抑制影响较大。在进行语音信号的分析和处理时，可采用<strong>预加重（pre-emphasis）</strong>的方法补偿语音信号<strong>高频</strong>部分的振幅，在傅里叶变换操作中避免数值问题，本质是<strong>施加高通滤波器</strong>。假设输入信号第 $n$ 个采样点为 $x[n]$ ，则预加重公式如下：</p> 
<p></p> 
<h2 style="background-color:transparent;"><span style="color:#be191c;"><strong>五、声学模型 Acoustic model</strong></span></h2> 
<p>现代工业级神经网络语音合成系统主要包括三个部分：文本前端、声学模型和声码器，文本输入到文本前端中，将文本转换为音素、韵律边界等文本特征。文本特征输入到声学模型，转换为对应的声学特征。声学特征输入到声码器，重建为原始波形。</p> 
<h3><span style="color:#1a439c;"><strong>1. Tacotron1</strong></span></h3> 
<p><img alt="" height="225" src="https://images2.imgbox.com/39/8d/DesuI1uD_o.png" width="309"><img alt="" height="235" src="https://images2.imgbox.com/a9/1a/vVXPVJ6v_o.png" width="373"></p> 
<h4>1.1 声学特征建模网络</h4> 
<p>Tacotron-2的声学模型部分采用典型的序列到序列seq2seq结构。编码器是3个卷积层和一个双向LSTM层组成的模块，卷积层给予了模型类似于N-gram感知上下文的能力，并且对不发音字符更加鲁棒。经词嵌入的注音序列首先进入卷积层提取上下文信息，然后送入双向LSTM生成编码器隐状态。编码器隐状态生成后，就会被送入注意力机制，以生成编码向量。我们利用了一种被称为<strong>位置敏感注意力（Location Sensitive Attention，LSA）</strong>，该注意力机制的对齐函数为：</p> 
<p><img alt="score(s_{i-1},h_j)=v_a^T{\rm tanh}(Ws_{i-1}+Vh_j+Uf_{i,j}+b)" class="mathcode" src="https://images2.imgbox.com/e6/73/WbcniaC1_o.png"></p> 
<p>其中，<img alt="$v_a,W,V,U$" class="mathcode" src="https://images2.imgbox.com/9e/4b/I2KuloDK_o.png"> 为待训练参数， <img alt="$b$" class="mathcode" src="https://images2.imgbox.com/ab/5a/V6tKkFrP_o.png"> 是偏置值， <img alt="$s_{i-1}$" class="mathcode" src="https://images2.imgbox.com/33/a9/S0yrGmPA_o.png"> 为上一时间步 <img alt="$i-1$" class="mathcode" src="https://images2.imgbox.com/5a/22/c8JU3t8M_o.png"> 的解码器隐状态， <img alt="$h_j$" class="mathcode" src="https://images2.imgbox.com/40/89/5UrvGZpV_o.png"> 为当前时间步 <img alt="$j$" class="mathcode" src="https://images2.imgbox.com/59/18/BIUY7egX_o.png"> 的编码器隐状态， <img alt="$f_{i,j}$" class="mathcode" src="https://images2.imgbox.com/35/af/1xDuog2h_o.png"> 为上一个解码步的注意力权重 <img alt="$\alpha_{i-1}$" class="mathcode" src="https://images2.imgbox.com/25/d8/Ggf4kkz7_o.png"> 经卷积获得的位置特征，如下式：</p> 
<p><img alt="f_{i,j}=F*\alpha_{i-1}" class="mathcode" src="https://images2.imgbox.com/15/fd/iICD4AsD_o.png"></p> 
<p>其中， <img alt="$\alpha_{i-1}$" class="mathcode" src="https://images2.imgbox.com/89/69/qYGksxs7_o.png"> 是经过softmax的<strong>注意力权重的累加和</strong>。位置敏感注意力机制不但综合了内容方面的信息，而且关注了位置特征。解码过程从输入上一解码步或者真实音频的频谱进入解码器预处理网络开始，到线性映射输出该时间步上的频谱帧结束，模型的解码过程如下图所示。</p> 
<p><img alt="" height="319" src="https://images2.imgbox.com/6e/ff/vVWBi64i_o.png" width="202"></p> 
<p>频谱生成网络的解码器将<strong>预处理网络的输出</strong>和<strong>注意力机制的编码向量</strong>做拼接，然后整体送入LSTM中，LSTM的输出用来计算新的编码向量，最后新计算出来的编码向量与LSTM输出做拼接，送入映射层以计算输出。输出有两种形式，一种是<strong>频谱帧</strong>，另一种是<strong>停止符的概率</strong>，后者是一个简单二分类问题，决定解码过程是否结束。为了能够有效加速计算，减小内存占用，引入<strong>缩减因子r（Reduction Factor）</strong>，即每一个时间步允许解码器<strong>预测r个频谱帧</strong>进行输出。解码完成后，送入后处理网络处理以生成最终的梅尔频谱，如下式所示。</p> 
<p><img alt="s_{final}=s_i+s_i'" class="mathcode" src="https://images2.imgbox.com/b1/97/TXkPMBZh_o.png"></p> 
<p>其中，<img alt="$s_i$" class="mathcode" src="https://images2.imgbox.com/03/0f/UxjureiQ_o.png"> 是解码器输出， <img alt="$s_{final}$" class="mathcode" src="https://images2.imgbox.com/4d/c5/YVUNNvwq_o.png"> 表示最终输出的梅尔频谱， <img alt="$s_i'$" class="mathcode" src="https://images2.imgbox.com/a8/6f/YD6T2RYq_o.png"> 是后处理网络的输出，解码器的输出经过后处理网络之后获得 <img alt="$s_i'$" class="mathcode" src="https://images2.imgbox.com/0d/f6/ZUIGvEyK_o.png"> 。</p> 
<p>在Tacotron-2原始论文中，直接将<strong>梅尔频谱送入声码器WaveNet</strong>生成最终的时域波形。但是WaveNet计算复杂度过高，几乎无法实际使用，因此可以使用其它声码器，比如Griffin-Lim、HiFiGAN等。</p> 
<h4>1.2 CBHG 模块</h4> 
<p><img alt="" height="274" src="https://images2.imgbox.com/ac/ec/gT7QPwNr_o.png" width="430"></p> 
<p></p> 
<h4>1.3 损失函数</h4> 
<p>Tacotron2的损失函数主要包括以下4个方面：</p> 
<ul><li>1. 进入后处理网络前后的平方损失。</li></ul> 
<p><img alt="{\rm MelLoss}=\frac{1}{n}\sum_{i=1}^n(y_{real,i}^{mel}-y_{before,i}^{mel})^2+\frac{1}{n}\sum_{i=1}^n(y_{real,i}^{mel}-y_{after,i}^{mel})^2" class="mathcode" src="https://images2.imgbox.com/7a/c0/Tk6qcYFo_o.png"></p> 
<p>  其中， <img alt="$y_{real,i}^{mel}$" class="mathcode" src="https://images2.imgbox.com/86/36/blwBtI2x_o.png"> 表示从音频中提取的真实频谱， <img alt="$y_{before,i}^{mel},y_{after,i}^{mel}$" class="mathcode" src="https://images2.imgbox.com/70/e7/prOT6zJS_o.png"> 分别为进入后处理网络前、后的解码器输出， <img alt="$n$" class="mathcode" src="https://images2.imgbox.com/ae/6a/nuKWMb7p_o.png"> 为每批的样本数。</p> 
<ul><li>2. 从<strong>CBHG模块</strong>中输出线性谱的平方损失。</li></ul> 
<p><img alt="{\rm LinearLoss}=\frac{1}{n}\sum_{i=1}^{n}(y_{real,i}^{linear}-y_{i}^{linear})^2" class="mathcode" src="https://images2.imgbox.com/26/2c/DnCUAuWL_o.png"></p> 
<p> 其中， <img alt="$y_{real,i}^{linear}$" class="mathcode" src="https://images2.imgbox.com/13/b6/whf5F25c_o.png"> 是从真实语音中计算获得的线性谱， <img alt="$y_{i}^{linear}$" class="mathcode" src="https://images2.imgbox.com/1f/a4/vIzNrlKy_o.png"> 是从CBHG模块输出的线性谱。</p> 
<ul><li>3. 停止符交叉熵</li></ul> 
<p><img alt="{\rm StopTokenLoss}=-[y\cdot {\rm log}(p)+(1-y)\cdot {\rm log}(1-p)]" class="mathcode" src="https://images2.imgbox.com/26/97/M9v2RfTd_o.png"></p> 
<p>其中，<img alt="$y$" class="mathcode" src="https://images2.imgbox.com/20/64/HfvVvHAn_o.png"> 为停止符真实概率分布， <img alt="$p$" class="mathcode" src="https://images2.imgbox.com/4c/2c/sEFESkNY_o.png"> 是解码器线性映射输出的<strong>预测分布</strong>。</p> 
<ul><li>4. L2正则化</li></ul> 
<p><img alt="{\rm RegulationLoss}=\frac{1}{K}\sum_{k=1}^K w_k^2" class="mathcode" src="https://images2.imgbox.com/ac/11/pr5RWIK9_o.png"></p> 
<p>其中， <img alt="$K$" class="mathcode" src="https://images2.imgbox.com/05/e7/IgnFXHyk_o.png"> 为参数总数， <img alt="$w_k$" class="mathcode" src="https://images2.imgbox.com/da/1c/VgEXlUZA_o.png"> 为模型中的参数，这里排除偏置值、RNN以及线性映射中的参数。最终的损失函数为上述4个部分的损失之和，如下式：</p> 
<p><img alt="{\rm Loss}={\rm MelLoss}+{\rm LinearLoss}+{\rm StopTokenLoss}+{\rm RegulationLoss}" class="mathcode" src="https://images2.imgbox.com/0e/f0/JYXvBgYW_o.png"></p> 
<h3></h3> 
<h3><span style="color:#1a439c;"><strong>2. FastSpeech</strong></span></h3> 
<p>FastSpeech是基于Transformer显式时长建模的声学模型，由微软和浙大提出。</p> 
<h4>1. 模型结构</h4> 
<p>FastSpeech 2和上代FastSpeech的编解码器均是采用<strong>FFT（feed-forward Transformer，前馈Transformer）块</strong>。编解码器的输入首先进行位置编码，之后进入FFT块。FFT块主要包括多头注意力模块和位置前馈网络，位置前馈网络可以由若干层Conv1d、LayerNorm和Dropout组成。</p> 
<p>论文中提到语音合成是典型的<strong>一对多问题</strong>，同样的文本可以合成无数种语音。上一代FastSpeech主要通过目标侧使用教师模型的合成频谱而非真实频谱，以简化数据偏差，减少语音中的多样性，从而降低训练难度；向模型提供额外的时长信息两个途径解决一对多的问题。在语音中，<strong>音素时长</strong>自不必说，直接影响发音长度和整体韵律；<strong>音调</strong>则是影响情感和韵律的另一个特征；<strong>能量</strong>则影响频谱的幅度，直接影响音频的音量。在FastSpeech 2中对这三个最重要的语音属性单独建模，从而缓解一对多带来的模型学习目标不确定的问题。</p> 
<p><img alt="" height="316" src="https://images2.imgbox.com/33/d3/4RQgtPrC_o.png" width="580"></p> 
<p>在对<strong>时长、基频和能量</strong>单独建模时，所使用的网络结构实际是相似的，在论文中称这种语音属性建模网络为<strong>变量适配器（Variance Adaptor）</strong>。时长预测的输出也作为基频和能量预测的输入。最后，基频预测和能量预测的输出，以及依靠时长信息展开的编码器输入元素加起来，作为下游网络的输入。变量适配器主要是由2层卷积和1层线性映射层组成，每层卷积后加ReLU激活、LayerNorm和Dropout。</p> 
<p>同样是通过<strong>长度调节器（Length Regulator）</strong>，利用时长信息将编码器输出长度扩展到频谱长度。具体实现就是根据duration的具体值，直接上采样。一个音素时长为2，就将编码器输出复制2份，给3就直接复制3份，拼接之后作为最终的输出。</p> 
<p>对于音高和能量的预测，模块的主干网络相似，但使用方法有所不同。以音高为例，能量的使用方式相似。首先对预测出的实数域音高值进行分桶，映射为一定范围内的自然数集，然后做嵌入。</p> 
<p></p> 
<h3><span style="color:#1a439c;"><strong>3. VITS</strong></span></h3> 
<p><strong>VITS（Variational Inference with adversarial learning for end-to-end Text-to-Speech）</strong>是一种结合<strong>变分推理（variational inference）</strong>、<strong>标准化流（normalizing flows）</strong>和<strong>对抗训练</strong>的高表现力语音合成模型。和Tacotron和FastSpeech不同，Tacotron / FastSpeech实际是将字符或音素映射为中间声学表征，比如梅尔频谱，然后通过声码器将梅尔频谱还原为波形，而VITS则直接将字符或音素映射为波形，不需要额外的声码器重建波形，真正的<strong>端到端</strong>语音合成模型。VITS通过隐变量而非之前的频谱串联语音合成中的声学模型和声码器，在<strong>隐变量上进行建模</strong>并利用随机时长预测器，提高了合成语音的多样性，输入同样的文本，能够合成不同声调和韵律的语音。VITS合成音质较高，并且可以借鉴之前的FastSpeech，单独对音高等特征进行建模，以进一步提升合成语音的质量，是一种非常有潜力的语音合成模型。</p> 
<h4>3.1 模型整体结构</h4> 
<p><img alt="" height="384" src="https://images2.imgbox.com/fa/22/qp8OLwet_o.png" width="666"></p> 
<p>VITS包括三个部分：</p> 
<ol><li><strong>后验编码器（Posterior Encoder）</strong>。如上图（a）的左下部分所示，在训练时输入线性谱，输出隐变量 <img alt="$z$" class="mathcode" src="https://images2.imgbox.com/d3/01/YvCq3tzR_o.png"> ，推断时隐变量 <img alt="$z$" class="mathcode" src="https://images2.imgbox.com/df/e8/Si26bSns_o.png"> 则由 <img alt="$f_\theta$" class="mathcode" src="https://images2.imgbox.com/28/d3/SYMpwsIQ_o.png"> 产生。VITS的后验编码器采用WaveGlow和Glow-TTS中的非因果WaveNet残差模块。应用于多人模型时，将说话人嵌入向量添加进残差模块，{<!-- --><strong>仅用于训练</strong>}。这里的<strong>隐变量 <img alt="$z$" class="mathcode" src="https://images2.imgbox.com/7a/3f/D48RFx2C_o.png"> 可以理解为Tacotron / FastSpeech中的梅尔频谱</strong>。</li><li><strong>解码器Decoder</strong>。如上图（a）左上部分所示，解码器从提取的隐变量 <img alt="$z$" class="mathcode" src="https://images2.imgbox.com/e5/8a/nkFTAGD8_o.png"> 中生成语音波形，这个解码器实际就是声码器HiFi-GAN V1的生成器。应用于多人模型时，在说话人嵌入向量之后添加一个线性层，拼接到 <img alt="$f_\theta$" class="mathcode" src="https://images2.imgbox.com/70/b0/Mi9PEbDf_o.png"> 的输出隐变量 <img alt="$z$" class="mathcode" src="https://images2.imgbox.com/a0/3b/tSdnMN6l_o.png"> 。</li><li>先验编码器。如上图（a）右侧部分所示，先验编码器结构比较复杂，作用类似于Tacotron / FastSpeech的声学模型，只不过VITS是将音素映射为中间表示 <img alt="$z$" class="mathcode" src="https://images2.imgbox.com/f2/a4/mIjZfvdu_o.png"> ，而不是将音素映射为频谱。包括文本编码器和提升先验分布复杂度的标准化流 <img alt="$f_\theta$" class="mathcode" src="https://images2.imgbox.com/cb/8b/4z5Fd2xl_o.png"> 。应用于多人模型时，向标准化流的残差模块中添加说话人嵌入向量。</li><li><strong>随机时长预测器Stochastic Duration Predictor</strong>。如上图（a）右侧中间橙色部分。从条件输入 <img alt="$h_{text}$" class="mathcode" src="https://images2.imgbox.com/e8/99/Gpp1YRJz_o.png"> 估算音素时长的分布。应用于多人模型时，在说话人嵌入向量之后添加一个线性层，并将其拼接到文本编码器的输出 <img alt="$h_{text}$" class="mathcode" src="https://images2.imgbox.com/05/35/BGJpOKGh_o.png">。</li><li><strong>判别器</strong>。实际就是HiFi-GAN的多周期判别器，在上图中未画出，{<!-- --><strong>仅用于训练</strong>}。目前看来，对于任意语音合成模型，加入判别器辅助都可以显著提升表现。</li></ol> 
<h4><strong>3.2 变分推断</strong></h4> 
<p>VITS可以看作是一个最大化变分下界，也即<strong>ELBO（Evidence Lower Bound）的条件VAE</strong>。<br>  </p> 
<p></p> 
<p></p> 
<h2><span style="color:#be191c;"><strong>六、声码器（Vocoder）</strong></span></h2> 
<p>声码器（Vocoder），又称<strong>语音信号分析合成系统</strong>，负责对声音进行分析和合成，主要用于合成人类的语音。声码器主要由以下功能：分析Analysis，操纵Manipulation，合成Synthesis</p> 
<p><strong>分析过程</strong>主要是从一段原始声音波形中提取声学特征，比如线性谱、MFCC；<strong>操纵过程</strong>是指对提取的原始声学特征进行压缩等降维处理，使其表征能力进一步提升；<strong>合成过程</strong>是指将此声学特征恢复至原始波形。人类发声机理可以用经典的源-滤波器模型建模，也就是输入的激励部分通过线性时不变进行操作，输出的声道谐振部分作为合成语音。输入部分被称为激励部分（Source Excitation Part），激励部分对应肺部气流与声带共同作用形成的激励，输出结果被称为声道谐振部分（Vocal Tract Resonance Part），对应人类发音结构，而声道谐振部分对应于声道的调音部分，对声音进行调制。</p> 
<p>声码器的发展可以分为两个阶段，包括用于统计参数语音合成（Statistical Parameteric Speech Synthesis，SPSS）基于信号处理的声码器，和基于神经网络的声码器。常用<strong>基于信号处理</strong>的声码器<strong>包括Griffin-Lim，STRAIGHT 和 WORLD</strong>。早期神经声码器包括WaveNet、WaveRNN等，近年来神经声码器发展迅速，涌现出包括MelGAN、HiFiGAN、LPCNet、NHV等优秀的工作。</p> 
<p></p> 
<h3>1. Griffin-Lim声码器</h3> 
<h3></h3> 
<h3></h3> 
<h3> </h3> 
<p></p> 
<p>Probabilistic formulation</p> 
<p></p> 
<p></p> 
<p></p> 
<p>重要的TTS范式。<strong>WaveNet </strong>最早是作为<strong>文本到波形模型（text-to-waveform）</strong>推出的（因此结合了<strong>声学模型（acoustic model）</strong>和<strong>声码器（vocoding）</strong>），可根据附加信息进行局部和全局调节；后来它被扩展为从输入<strong>频谱图（spectrograms）</strong>合成波形，从而沦为传统声码器的角色。GAN 通常用于将频谱图映射为波形（有效地充当<strong>声码器（vocoders）</strong>），或从随机输入中 "想象 "波形，因此包含了 TTS 管道的所有中间步骤以及决定输出何种文本的机制。<strong>Tacotron </strong>利用 <strong>seq2seq 模型</strong>来学习<strong>音素/字符（phonemes/characters）</strong>到音频特征的映射，从而隐含地将文本分析与<strong>声学模型（acoustic model）</strong>结合起来；FastSpeech 在此基础上进行了迭代，用 Transformers 代替了 RNN。</p> 
<p><img alt="" height="414" src="https://images2.imgbox.com/e6/9e/6HeX8DeW_o.png" width="725"></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<h2><span style="color:#be191c;"><strong>References</strong></span></h2> 
<p>中文：<a class="link-info" href="https://yqli.tech/pdf/other/Speech%20Synthesis%20Past%20Present%20and%20Future.pdf" rel="nofollow" title="Speech Synthesis: Past, Present and Future (2019),ppt">Speech Synthesis: Past, Present and Future (2019),ppt</a></p> 
<p>英文：<a class="link-info" href="http://www.sp.nitech.ac.jp/~tokuda/INTERSPEECH2019.pdf" rel="nofollow" title="Statistical approach to speech synthesis---past, present, and future（2019）">Statistical approach to speech synthesis---past, present, and future（2019）</a></p> 
<p><a class="link-info" href="https://era.ed.ac.uk/bitstream/handle/1842/35987/Espic%20Calder%C3%B3n2019.pdf?sequence=1&amp;isAllowed=y" rel="nofollow" title="In Search of the Optimal Acoustic Features for Statistical Parametric Speech Synthesis ">In Search of the Optimal Acoustic Features for Statistical Parametric Speech Synthesis </a></p> 
<p><a href="https://developer.aliyun.com/article/932636" rel="nofollow" title="深度学习于语音合成研究综述-阿里云开发者社区">深度学习于语音合成研究综述-阿里云开发者社区</a></p> 
<p><a href="https://cloud.tencent.com/developer/article/1116923" rel="nofollow" title="语音合成到了跳变点？深度神经网络变革TTS最新研究汇总-腾讯云开发者社区-腾讯云">语音合成到了跳变点？深度神经网络变革TTS最新研究汇总-腾讯云开发者社区-腾讯云</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/364075962" rel="nofollow" title="基于深度学习语音合成技术研究 - 知乎">基于深度学习语音合成技术研究 - 知乎</a></p> 
<p>整合向：</p> 
<p><a href="https://www.zhangzhenhu.com/audio/feature.html" rel="nofollow" title="1. 音频特征 — 张振虎的博客 张振虎 文档">1. 音频特征 — 张振虎的博客 张振虎 文档</a></p> 
<p><a href="https://github.com/cnlinxi/book-text-to-speech" title="GitHub - cnlinxi/book-text-to-speech: A book about Text-to-Speech (TTS) in Chinese.">GitHub - cnlinxi/book-text-to-speech: A book about Text-to-Speech (TTS) in Chinese.</a></p> 
<p></p> 
<p></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/7de4b32b09690627ea0f8be053d07598/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Bond 7种模式解释、举例如何配置</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/245f61c13a6c5a63414a3a5421646f1e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">在线音频视频剪辑网站推荐</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>