<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>pointNet训练预测自己的数据集Charles版本(二） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="pointNet训练预测自己的数据集Charles版本(二）" />
<meta property="og:description" content="之前博客介绍了如何跑通charles版本的pointNet，这篇介绍下如何来训练和预测自己的数据集，介绍如何在自己的数据集上做点云语义分割，此篇的环境配置和博客中保持一致。点云分类较简单，方法差不多，这边就不特地说明了。
一.在自己的点云数据集上做语义分割 1. RGB-D Scenes Dataset v.2数据集介绍 博主拿数据集RGB-D Scenes Dataset v.2来做实验，数据集下载链接如下：
RGB-D Scenes Dataset v.2
所下载的数据集的目录结构如下：
​
01.label是标注数据，01.ply是点云数据，其它类似，可看到点云和标注数据是分离开来的，这边博主手写了如下脚本来合并01.label和01.ply文件，以将label中数据作为Scalar field。脚本如下：
import numpy as np import glob import os import sys from plyfile import PlyData, PlyElement import pandas as pd BASE_DIR = os.path.dirname(os.path.abspath(__file__)) ROOT_DIR = os.path.dirname(BASE_DIR) sys.path.append(BASE_DIR) if __name__ == &#34;__main__&#34;: with open(BASE_DIR &#43; &#39;/rgbd-scenes-v2/pc/01.ply&#39;, &#39;rb&#39;) as f: plydata = PlyData.read(f) print(len(plydata.elements[0].data)) label = np.loadtxt(BASE_DIR &#43; &#39;/rgbd-scenes-v2/pc/01.label&#39;) print(label.shape) vtx = plydata[&#39;vertex&#39;] points = np." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/31083d1ad262d277549ce6f4daef71d5/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-26T23:44:15+08:00" />
<meta property="article:modified_time" content="2023-06-26T23:44:15+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">pointNet训练预测自己的数据集Charles版本(二）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>之前<a class="link-info" href="https://www.bilibili.com/video/BV1WP41137eu/?buvid=Z84EEA8BBF7BC5CF455A9DD423E843DC9C05&amp;is_story_h5=false&amp;mid=320fL4pcTKS1IlVte0ix7Q%3D%3D&amp;p=1&amp;plat_id=114&amp;share_from=ugc&amp;share_medium=iphone&amp;share_plat=ios&amp;share_session_id=6DC150D4-F308-4394-96BE-C9889646C49F&amp;share_source=WEIXIN&amp;share_tag=s_i&amp;timestamp=1687057288&amp;unique_k=gDnKPfB&amp;up_id=477163918" rel="nofollow" title="博客">博客</a>介绍了如何跑通charles版本的pointNet，这篇介绍下如何来训练和预测自己的数据集，介绍如何在自己的数据集上做<strong>点云语义分割</strong>，此篇的环境配置和<a class="link-info" href="https://blog.csdn.net/jiugeshao/article/details/130909618?spm=1001.2014.3001.5501" title="博客">博客</a>中保持一致。点云分类较简单，方法差不多，这边就不特地说明了。</p> 
<p></p> 
<h3><strong><span style="color:#1a439c;">一.在自己的点云数据集上做语义分割</span></strong></h3> 
<h4><span style="color:#0d0016;"><strong>1. RGB-D Scenes Dataset v.2数据集介绍</strong></span></h4> 
<p>博主拿数据集RGB-D Scenes Dataset v.2来做实验，数据集下载链接如下：</p> 
<p><a href="http://rgbd-dataset.cs.washington.edu/dataset/rgbd-scenes-v2/" rel="nofollow" title="RGB-D Scenes Dataset v.2">RGB-D Scenes Dataset v.2</a></p> 
<p> 所下载的数据集的目录结构如下：</p> 
<div> 
 <p class="img-center"><img alt="" height="364" src="https://images2.imgbox.com/56/91/dK1wrlCB_o.png" width="1135"><span title="点击并拖拽以改变尺寸">​</span></p> 
</div> 
<p> 01.label是标注数据，01.ply是点云数据，其它类似，可看到点云和标注数据是分离开来的，这边博主手写了如下脚本来合并01.label和01.ply文件，以将label中数据作为Scalar field。脚本如下：</p> 
<div> 
 <pre><code class="language-python hljs"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> glob
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> sys
<span class="hljs-keyword">from</span> plyfile <span class="hljs-keyword">import</span> PlyData, PlyElement
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.dirname(BASE_DIR)
sys.path.append(BASE_DIR)

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(BASE_DIR + <span class="hljs-string">'/rgbd-scenes-v2/pc/01.ply'</span>, <span class="hljs-string">'rb'</span>) <span class="hljs-keyword">as</span> f:
        plydata = PlyData.read(f)

<span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(plydata.elements[<span class="hljs-number">0</span>].data))

label = np.loadtxt(BASE_DIR + <span class="hljs-string">'/rgbd-scenes-v2/pc/01.label'</span>)
<span class="hljs-built_in">print</span>(label.shape)

vtx = plydata[<span class="hljs-string">'vertex'</span>]
points = np.stack([vtx[<span class="hljs-string">'x'</span>], vtx[<span class="hljs-string">'y'</span>], vtx[<span class="hljs-string">'z'</span>],vtx[<span class="hljs-string">'diffuse_red'</span>],vtx[<span class="hljs-string">'diffuse_green'</span>],vtx[<span class="hljs-string">'diffuse_blue'</span>]], axis=-<span class="hljs-number">1</span>)

label = label[<span class="hljs-number">1</span>:<span class="hljs-built_in">len</span>(label)]
label = label[:,np.newaxis]
<span class="hljs-built_in">print</span>(label.shape)
<span class="hljs-built_in">print</span>(points.shape)


combined = np.concatenate([points, label], <span class="hljs-number">1</span>)
<span class="hljs-comment"># current_label = np.squeeze(current_label)</span>

<span class="hljs-built_in">print</span>(combined.shape)

<span class="hljs-comment">#write the points into txt</span>

out_data_label_filename = BASE_DIR + <span class="hljs-string">'/01_data_label.txt'</span>
fout_data_label = <span class="hljs-built_in">open</span>(out_data_label_filename, <span class="hljs-string">'w'</span>)

<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(combined.shape[<span class="hljs-number">0</span>]):
    fout_data_label.write(<span class="hljs-string">'%f %f %f %d %d %d %d\n'</span> % (combined[i,<span class="hljs-number">0</span>], combined[i,<span class="hljs-number">1</span>], combined[i,<span class="hljs-number">2</span>], combined[i,<span class="hljs-number">3</span>], combined[i,<span class="hljs-number">4</span>], combined[i,<span class="hljs-number">5</span>], combined[i,<span class="hljs-number">6</span>],))

fout_data_label.close()</code></pre> 
</div> 
<p>这里用cloudcompare软件打开合并和的01_data_label.txt文件，显示效果如下：</p> 
<div> 
 <p class="img-center"><img alt="" height="358" src="https://images2.imgbox.com/1d/78/aLPKR9g0_o.png" width="600"><span title="点击并拖拽以改变尺寸">​</span></p> 
</div> 
<p></p> 
<h4><span style="color:#0d0016;"><strong>2. 新数据集生成</strong></span></h4> 
<p>这边博主会对原数据集RGB-D Scenes Dataset v.2做预处理(各ply文件)，生成新的数据集。</p> 
<p>用cloudcompare打开01.ply文件，通过segment工具（小剪刀）来切割点云，获取桌子及桌子上物品点云。</p> 
<div> 
 <p class="img-center"><img alt="" height="298" src="https://images2.imgbox.com/17/28/Xn1nlOa4_o.png" width="605"><span title="点击并拖拽以改变尺寸">​</span></p> 
</div> 
<p> 裁切出的点云数据集如下</p> 
<div> 
 <p class="img-center"><img alt="" height="338" src="https://images2.imgbox.com/b4/03/3LZI8d7f_o.png" width="500"><span title="点击并拖拽以改变尺寸">​</span></p> 
</div> 
<p>然后保存桌子点云到本地磁盘上， 其它点云文件(ply）文件类似，从中只获得桌子的点云。博主这边上传下所裁切得到的12份点云文件。</p> 
<p>链接: https://pan.baidu.com/s/1rCDhruH_C_hpoZb5BreujA 提取码: ec0h<br><img alt="" height="218" src="https://images2.imgbox.com/ae/f2/rn7xeemv_o.png" width="216"><img alt="" height="216" src="https://images2.imgbox.com/39/6e/ReRQMGzz_o.png" width="227"><img alt="" height="215" src="https://images2.imgbox.com/06/72/LlwtFv5P_o.png" width="223"></p> 
<p> <img alt="" height="212" src="https://images2.imgbox.com/9e/a6/wX5OmvLa_o.png" width="211"><img alt="" height="213" src="https://images2.imgbox.com/5a/2a/38Zjh0fQ_o.png" width="228"><img alt="" height="211" src="https://images2.imgbox.com/ab/fe/w0gGBnvO_o.png" width="231"></p> 
<p><img alt="" height="205" src="https://images2.imgbox.com/16/82/hFhImtJQ_o.png" width="217"><img alt="" height="204" src="https://images2.imgbox.com/04/c6/WUNVPkBX_o.png" width="213"><img alt="" height="201" src="https://images2.imgbox.com/57/c5/YyJE7YAK_o.png" width="199"></p> 
<p><img alt="" height="229" src="https://images2.imgbox.com/47/47/89d6lNkR_o.png" width="218"><img alt="" height="226" src="https://images2.imgbox.com/18/f7/xAVGfRCX_o.png" width="224"><img alt="" height="225" src="https://images2.imgbox.com/cb/6f/6JSPIrJZ_o.png" width="224"></p> 
<p> 博主对这12份点云又做了一些裁切操作, <strong>生成了35份点云出来</strong>，如下链接</p> 
<p>链接: https://pan.baidu.com/s/1w0hhOhgEonxniiMOvcFsIA 提取码: 4eo6<br>  </p> 
<h4><span style="color:#0d0016;"><strong>3. 新数据集标注(35份点云）</strong></span></h4> 
<p>这边博主只做3种标签，桌面像素打标签为0，书本打标签1，帽子打标签为2，杯子碗一类的打标签为3。如下是用cloudcompare给tabel01 - Cloud_1（可从如上百度网盘链接中获取文件）打标签过程。先对点云使用如上的剪刀工具，把点云分割成桌面点云和碗两部分点云，然后点击工具栏的“+”符号。</p> 
<div> 
 <p class="img-center"><img alt="" height="3" src="https://images2.imgbox.com/b0/b5/4GFAJc0G_o.png" width="500"><span title="点击并拖拽以改变尺寸">​</span></p> 
</div> 
<div> 
 <p class="img-center"><img alt="" height="278" src="https://images2.imgbox.com/82/3a/SO52RAIG_o.png" width="500"><span title="点击并拖拽以改变尺寸">​</span></p> 
</div> 
<div> 
 <p class="img-center"><img alt="" height="265" src="https://images2.imgbox.com/2e/00/XmOQVatJ_o.png" width="500"><span title="点击并拖拽以改变尺寸">​</span></p> 
</div> 
<div> 
 <p class="img-center"><img alt="" height="310" src="https://images2.imgbox.com/7f/92/6XMLi77V_o.png" width="500"><span title="点击并拖拽以改变尺寸">​</span></p> 
</div> 
<p> 然后选中两个点云和合并 </p> 
<div> 
 <p class="img-center"><img alt="" height="218" src="https://images2.imgbox.com/52/2e/NUlsSjeB_o.png" width="425"><span title="点击并拖拽以改变尺寸">​</span></p> 
</div> 
<div> 
 <p class="img-center"><img alt="" height="302" src="https://images2.imgbox.com/eb/42/1WIdpXV8_o.png" width="500"><span title="点击并拖拽以改变尺寸">​</span></p> 
</div> 
<p> 生成后点云效果如下：</p> 
<div> 
 <p class="img-center"><img alt="" height="222" src="https://images2.imgbox.com/41/42/1OfMXjwn_o.png" width="500"><span title="点击并拖拽以改变尺寸">​</span></p> 
</div> 
<p> 如上如果点击的是Yes按钮，则合并后的点云保存的点云文件如下格式</p> 
<div> 
 <p class="img-center"><img alt="" height="215" src="https://images2.imgbox.com/85/08/lVWAXt5U_o.png" width="500"><span title="点击并拖拽以改变尺寸">​</span></p> 
</div> 
<p>如上如果点击No按钮，则合并后的点云保存的点云文件如下格式(<span style="color:#fe2c24;"><strong>这里采取这种方式保存</strong></span>）</p> 
<div> 
 <p class="img-center"><img alt="" height="358" src="https://images2.imgbox.com/41/ae/dfuLnwR6_o.png" width="500"><span title="点击并拖拽以改变尺寸">​</span></p> 
</div> 
<p> 但合并后的带标注数据的点云和原点云的，点的排列顺序不一致</p> 
<div> 
 <p class="img-center"><img alt="" height="277" src="https://images2.imgbox.com/c7/17/ab8pP2j2_o.png" width="700"><span title="点击并拖拽以改变尺寸">​</span></p> 
</div> 
<p> 剩余34份文件都按照此方法进行标注下。博主上传下这35份带标注信息的点云文件，链接如下：</p> 
<p>链接: https://pan.baidu.com/s/1jbVWHlVKUWcq4t9K-Yk6bQ 提取码: 4t92<br>  </p> 
<p><span style="color:#0d0016;"><strong>4. 生成训练用的h5文件</strong></span></p> 
<p>博主这边修改了gen_indoor3d_h5.py文件，代码如下：</p> 
<div> 
 <pre><code class="language-python hljs"><span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> sys
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.dirname(BASE_DIR)
sys.path.append(BASE_DIR)
sys.path.append(os.path.join(ROOT_DIR, <span class="hljs-string">'utils'</span>))
<span class="hljs-comment"># import data_prep_util</span>
<span class="hljs-keyword">import</span> indoor3d_util
<span class="hljs-keyword">import</span> glob
<span class="hljs-keyword">import</span> h5py

<span class="hljs-comment"># Constants</span>
<span class="hljs-comment"># indoor3d_data_dir = os.path.join(data_dir, 'mydata_h5')</span>
NUM_POINT = <span class="hljs-number">4096</span>
H5_BATCH_SIZE = <span class="hljs-number">1000</span>
data_dim = [NUM_POINT, <span class="hljs-number">6</span>]
label_dim = [NUM_POINT]
data_dtype = <span class="hljs-string">'float32'</span>
label_dtype = <span class="hljs-string">'uint8'</span>

<span class="hljs-comment"># Set paths</span>
<span class="hljs-comment"># filelist = os.path.join(BASE_DIR, 'meta/all_data_label.txt')</span>
<span class="hljs-comment"># data_label_files = [os.path.join(indoor3d_data_dir, line.rstrip()) for line in open(filelist)]</span>

output_dir = os.path.join(ROOT_DIR, <span class="hljs-string">'data/mydata_h5'</span>)
<span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(output_dir):
    os.mkdir(output_dir)
output_filename_prefix = os.path.join(output_dir, <span class="hljs-string">'ply_data_all'</span>)
output_room_filelist = os.path.join(output_dir, <span class="hljs-string">'all_files.txt'</span>)
fout_room = <span class="hljs-built_in">open</span>(output_room_filelist, <span class="hljs-string">'w'</span>)

<span class="hljs-comment"># --------------------------------------</span>
<span class="hljs-comment"># ----- BATCH WRITE TO HDF5 -----</span>
<span class="hljs-comment"># --------------------------------------</span>
batch_data_dim = [H5_BATCH_SIZE] + data_dim
batch_label_dim = [H5_BATCH_SIZE] + label_dim
h5_batch_data = np.zeros(batch_data_dim, dtype = np.float32)
h5_batch_label = np.zeros(batch_label_dim, dtype = np.uint8)
buffer_size = <span class="hljs-number">0</span>  <span class="hljs-comment"># state: record how many samples are currently in buffer</span>
h5_index = <span class="hljs-number">0</span> <span class="hljs-comment"># state: the next h5 file to save</span>

<span class="hljs-keyword">def</span> <span class="function_ hljs-title">save_h5</span>(<span class="hljs-params">h5_filename, data, label, data_dtype=<span class="hljs-string">'uint8'</span>, label_dtype=<span class="hljs-string">'uint8'</span></span>):
    h5_fout = h5py.File(h5_filename)
    h5_fout.create_dataset(
            <span class="hljs-string">'data'</span>, data=data,
            compression=<span class="hljs-string">'gzip'</span>, compression_opts=<span class="hljs-number">4</span>,
            dtype=data_dtype)
    h5_fout.create_dataset(
            <span class="hljs-string">'label'</span>, data=label,
            compression=<span class="hljs-string">'gzip'</span>, compression_opts=<span class="hljs-number">1</span>,
            dtype=label_dtype)
    h5_fout.close()

<span class="hljs-keyword">def</span> <span class="function_ hljs-title">insert_batch</span>(<span class="hljs-params">data, label, last_batch=<span class="hljs-literal">False</span></span>):
    <span class="hljs-keyword">global</span> h5_batch_data, h5_batch_label
    <span class="hljs-keyword">global</span> buffer_size, h5_index
    data_size = data.shape[<span class="hljs-number">0</span>]
    <span class="hljs-comment"># If there is enough space, just insert</span>
    <span class="hljs-keyword">if</span> buffer_size + data_size &lt;= h5_batch_data.shape[<span class="hljs-number">0</span>]:
        h5_batch_data[buffer_size:buffer_size+data_size, ...] = data
        h5_batch_label[buffer_size:buffer_size+data_size] = label
        buffer_size += data_size
    <span class="hljs-keyword">else</span>: <span class="hljs-comment"># not enough space</span>
        capacity = h5_batch_data.shape[<span class="hljs-number">0</span>] - buffer_size
        <span class="hljs-keyword">assert</span>(capacity&gt;=<span class="hljs-number">0</span>)
        <span class="hljs-keyword">if</span> capacity &gt; <span class="hljs-number">0</span>:
           h5_batch_data[buffer_size:buffer_size+capacity, ...] = data[<span class="hljs-number">0</span>:capacity, ...] 
           h5_batch_label[buffer_size:buffer_size+capacity, ...] = label[<span class="hljs-number">0</span>:capacity, ...] 
        <span class="hljs-comment"># Save batch data and label to h5 file, reset buffer_size</span>
        h5_filename =  output_filename_prefix + <span class="hljs-string">'_'</span> + <span class="hljs-built_in">str</span>(h5_index) + <span class="hljs-string">'.h5'</span>
        save_h5(h5_filename, h5_batch_data, h5_batch_label, data_dtype, label_dtype)
        fout_room.write(<span class="hljs-string">'mydata_h5'</span> + <span class="hljs-string">'\''</span> + h5_filename)
        <span class="hljs-built_in">print</span>(<span class="hljs-string">'Stored {0} with size {1}'</span>.<span class="hljs-built_in">format</span>(h5_filename, h5_batch_data.shape[<span class="hljs-number">0</span>]))
        h5_index += <span class="hljs-number">1</span>
        buffer_size = <span class="hljs-number">0</span>
        <span class="hljs-comment"># recursive call</span>
        insert_batch(data[capacity:, ...], label[capacity:, ...], last_batch)
    <span class="hljs-keyword">if</span> last_batch <span class="hljs-keyword">and</span> buffer_size &gt; <span class="hljs-number">0</span>:
        h5_filename =  output_filename_prefix + <span class="hljs-string">'_'</span> + <span class="hljs-built_in">str</span>(h5_index) + <span class="hljs-string">'.h5'</span>
        save_h5(h5_filename, h5_batch_data[<span class="hljs-number">0</span>:buffer_size, ...], h5_batch_label[<span class="hljs-number">0</span>:buffer_size, ...], data_dtype, label_dtype)
        fout_room.write(<span class="hljs-string">'mydata_h5/ply_data_all'</span> + <span class="hljs-string">'_'</span> + <span class="hljs-built_in">str</span>(h5_index) + <span class="hljs-string">'.h5'</span>)
        <span class="hljs-built_in">print</span>(<span class="hljs-string">'Stored {0} with size {1}'</span>.<span class="hljs-built_in">format</span>(h5_filename, buffer_size))
        h5_index += <span class="hljs-number">1</span>
        buffer_size = <span class="hljs-number">0</span>
    <span class="hljs-keyword">return</span>

path = os.path.join(BASE_DIR + <span class="hljs-string">'/mydata_withlabel'</span>, <span class="hljs-string">'*.asc'</span>)
files = glob.glob(path)
points_list = []

<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> files:
    <span class="hljs-built_in">print</span>(f)
    points = np.loadtxt(f)
    <span class="hljs-built_in">print</span>(points.shape)
    sample = np.random.choice(points.shape[<span class="hljs-number">0</span>], NUM_POINT)
    sample_data = points[sample,...]
    <span class="hljs-built_in">print</span>(sample_data.shape)
    points_list.append(sample_data)

data_label = np.stack(points_list, axis=<span class="hljs-number">0</span>)
<span class="hljs-built_in">print</span>(data_label.shape)

data = data_label[:,:,<span class="hljs-number">0</span>:<span class="hljs-number">6</span>]
label = data_label[:,:,<span class="hljs-number">6</span>]

<span class="hljs-built_in">print</span>(data.shape)
<span class="hljs-built_in">print</span>(label.shape)
sample_cnt = <span class="hljs-number">0</span>

insert_batch(data, label, <span class="hljs-literal">True</span>)


<span class="hljs-comment"># for i, data_label_filename in enumerate(data_label_files):</span>
<span class="hljs-comment">#     print(data_label_filename)</span>
<span class="hljs-comment">#     data, label = indoor3d_util.room2blocks_wrapper_normalized(data_label_filename, NUM_POINT, block_size=1.0, stride=0.5,</span>
<span class="hljs-comment">#                                                  random_sample=False, sample_num=None)</span>
<span class="hljs-comment">#     print('{0}, {1}'.format(data.shape, label.shape))</span>
<span class="hljs-comment">#     for _ in range(data.shape[0]):</span>
<span class="hljs-comment">#         fout_room.write(os.path.basename(data_label_filename)[0:-4]+'\n')</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#     sample_cnt += data.shape[0]</span>
<span class="hljs-comment">#     insert_batch(data, label, i == len(data_label_files)-1)</span>
<span class="hljs-comment">#</span>
fout_room.close()
<span class="hljs-comment"># print("Total samples: {0}".format(sample_cnt))</span>
</code></pre> 
</div> 
<p>运行结果如下：</p> 
<div> 
 <p class="img-center"><img alt="" height="390" src="https://images2.imgbox.com/d3/e4/Ct64VEGe_o.png" width="571"><span title="点击并拖拽以改变尺寸">​</span></p> 
</div> 
<p></p> 
<div> 
 <p class="img-center"><img alt="" height="111" src="https://images2.imgbox.com/86/6e/CTARZF9L_o.png" width="550"><span title="点击并拖拽以改变尺寸">​</span></p> 
</div> 
<p></p> 
<p> <span style="color:#0d0016;"><strong>5. 分割网络训练</strong></span></p> 
<p>修改sem_seg/train.py文件，代码如下：</p> 
<div> 
 <pre><code class="language-python hljs"><span class="hljs-keyword">import</span> tensorflow.compat.v1 <span class="hljs-keyword">as</span> tf
tf.compat.v1.disable_eager_execution()
<span class="hljs-keyword">import</span> argparse
<span class="hljs-keyword">import</span> math
<span class="hljs-keyword">import</span> h5py
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> socket

<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> sys
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.dirname(BASE_DIR)
sys.path.append(BASE_DIR)
sys.path.append(ROOT_DIR)
sys.path.append(os.path.join(ROOT_DIR, <span class="hljs-string">'utils'</span>))
<span class="hljs-keyword">import</span> provider
<span class="hljs-keyword">import</span> tf_util
<span class="hljs-keyword">from</span> model <span class="hljs-keyword">import</span> *

parser = argparse.ArgumentParser()
parser.add_argument(<span class="hljs-string">'--gpu'</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">0</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">'GPU to use [default: GPU 0]'</span>)
parser.add_argument(<span class="hljs-string">'--log_dir'</span>, default=<span class="hljs-string">'log'</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">'Log dir [default: log]'</span>)
parser.add_argument(<span class="hljs-string">'--num_point'</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">4096</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">'Point number [default: 4096]'</span>)
parser.add_argument(<span class="hljs-string">'--max_epoch'</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">500</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">'Epoch to run [default: 50]'</span>)
parser.add_argument(<span class="hljs-string">'--batch_size'</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">2</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">'Batch Size during training [default: 24]'</span>)
parser.add_argument(<span class="hljs-string">'--learning_rate'</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">float</span>, default=<span class="hljs-number">0.001</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">'Initial learning rate [default: 0.001]'</span>)
parser.add_argument(<span class="hljs-string">'--momentum'</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">float</span>, default=<span class="hljs-number">0.9</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">'Initial learning rate [default: 0.9]'</span>)
parser.add_argument(<span class="hljs-string">'--optimizer'</span>, default=<span class="hljs-string">'adam'</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">'adam or momentum [default: adam]'</span>)
parser.add_argument(<span class="hljs-string">'--decay_step'</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">300000</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">'Decay step for lr decay [default: 300000]'</span>)
parser.add_argument(<span class="hljs-string">'--decay_rate'</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">float</span>, default=<span class="hljs-number">0.5</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">'Decay rate for lr decay [default: 0.5]'</span>)
parser.add_argument(<span class="hljs-string">'--test_area'</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">6</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">'Which area to use for test, option: 1-6 [default: 6]'</span>)
FLAGS = parser.parse_args()


BATCH_SIZE = FLAGS.batch_size
NUM_POINT = FLAGS.num_point
MAX_EPOCH = FLAGS.max_epoch
NUM_POINT = FLAGS.num_point
BASE_LEARNING_RATE = FLAGS.learning_rate
GPU_INDEX = FLAGS.gpu
MOMENTUM = FLAGS.momentum
OPTIMIZER = FLAGS.optimizer
DECAY_STEP = FLAGS.decay_step
DECAY_RATE = FLAGS.decay_rate

LOG_DIR = FLAGS.log_dir
<span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(LOG_DIR): os.mkdir(LOG_DIR)
os.system(<span class="hljs-string">'cp model.py %s'</span> % (LOG_DIR)) <span class="hljs-comment"># bkp of model def</span>
os.system(<span class="hljs-string">'cp train.py %s'</span> % (LOG_DIR)) <span class="hljs-comment"># bkp of train procedure</span>
LOG_FOUT = <span class="hljs-built_in">open</span>(os.path.join(LOG_DIR, <span class="hljs-string">'log_train.txt'</span>), <span class="hljs-string">'w'</span>)
LOG_FOUT.write(<span class="hljs-built_in">str</span>(FLAGS)+<span class="hljs-string">'\n'</span>)

MAX_NUM_POINT = <span class="hljs-number">4096</span>
NUM_CLASSES = <span class="hljs-number">4</span>

BN_INIT_DECAY = <span class="hljs-number">0.5</span>
BN_DECAY_DECAY_RATE = <span class="hljs-number">0.5</span>
<span class="hljs-comment">#BN_DECAY_DECAY_STEP = float(DECAY_STEP * 2)</span>
BN_DECAY_DECAY_STEP = <span class="hljs-built_in">float</span>(DECAY_STEP)
BN_DECAY_CLIP = <span class="hljs-number">0.99</span>

HOSTNAME = socket.gethostname()

ALL_FILES = provider.getDataFiles(ROOT_DIR + <span class="hljs-string">'/data/mydata_h5/all_files.txt'</span>)
<span class="hljs-comment"># room_filelist = [line.rstrip() for line in open('indoor3d_sem_seg_hdf5_data/room_filelist.txt')]</span>

<span class="hljs-comment"># Load ALL data</span>
data_batch_list = []
label_batch_list = []
<span class="hljs-keyword">for</span> h5_filename <span class="hljs-keyword">in</span> ALL_FILES:
    data_batch, label_batch = provider.loadDataFile(ROOT_DIR + <span class="hljs-string">'/data/'</span> + h5_filename)
    data_batch_list.append(data_batch)
    label_batch_list.append(label_batch)
data_batches = np.concatenate(data_batch_list, <span class="hljs-number">0</span>)
label_batches = np.concatenate(label_batch_list, <span class="hljs-number">0</span>)
<span class="hljs-built_in">print</span>(data_batches.shape)
<span class="hljs-built_in">print</span>(label_batches.shape)

train_data = data_batches
train_label = label_batches

test_data = data_batches
test_label = label_batches

<span class="hljs-built_in">print</span>(train_data.shape, train_label.shape)
<span class="hljs-built_in">print</span>(test_data.shape, test_label.shape)

<span class="hljs-keyword">def</span> <span class="function_ hljs-title">log_string</span>(<span class="hljs-params">out_str</span>):
    LOG_FOUT.write(out_str+<span class="hljs-string">'\n'</span>)
    LOG_FOUT.flush()
    <span class="hljs-built_in">print</span>(out_str)


<span class="hljs-keyword">def</span> <span class="function_ hljs-title">get_learning_rate</span>(<span class="hljs-params">batch</span>):
    learning_rate = tf.train.exponential_decay(
                        BASE_LEARNING_RATE,  <span class="hljs-comment"># Base learning rate.</span>
                        batch * BATCH_SIZE,  <span class="hljs-comment"># Current index into the dataset.</span>
                        DECAY_STEP,          <span class="hljs-comment"># Decay step.</span>
                        DECAY_RATE,          <span class="hljs-comment"># Decay rate.</span>
                        staircase=<span class="hljs-literal">True</span>)
    learning_rate = tf.maximum(learning_rate, <span class="hljs-number">0.00001</span>) <span class="hljs-comment"># CLIP THE LEARNING RATE!!</span>
    <span class="hljs-keyword">return</span> learning_rate        

<span class="hljs-keyword">def</span> <span class="function_ hljs-title">get_bn_decay</span>(<span class="hljs-params">batch</span>):
    bn_momentum = tf.train.exponential_decay(
                      BN_INIT_DECAY,
                      batch*BATCH_SIZE,
                      BN_DECAY_DECAY_STEP,
                      BN_DECAY_DECAY_RATE,
                      staircase=<span class="hljs-literal">True</span>)
    bn_decay = tf.minimum(BN_DECAY_CLIP, <span class="hljs-number">1</span> - bn_momentum)
    <span class="hljs-keyword">return</span> bn_decay

<span class="hljs-keyword">def</span> <span class="function_ hljs-title">train</span>():
    <span class="hljs-keyword">with</span> tf.Graph().as_default():
        <span class="hljs-keyword">with</span> tf.device(<span class="hljs-string">'/gpu:'</span>+<span class="hljs-built_in">str</span>(GPU_INDEX)):
            pointclouds_pl, labels_pl = placeholder_inputs(BATCH_SIZE, NUM_POINT)
            is_training_pl = tf.placeholder(tf.<span class="hljs-built_in">bool</span>, shape=())
            
            <span class="hljs-comment"># Note the global_step=batch parameter to minimize. </span>
            <span class="hljs-comment"># That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.</span>
            batch = tf.Variable(<span class="hljs-number">0</span>)
            bn_decay = get_bn_decay(batch)
            tf.summary.scalar(<span class="hljs-string">'bn_decay'</span>, bn_decay)

            <span class="hljs-comment"># Get model and loss </span>
            pred = get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)
            loss = get_loss(pred, labels_pl)
            tf.summary.scalar(<span class="hljs-string">'loss'</span>, loss)

            correct = tf.equal(tf.argmax(pred, <span class="hljs-number">2</span>), tf.to_int64(labels_pl))
            accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / <span class="hljs-built_in">float</span>(BATCH_SIZE*NUM_POINT)
            tf.summary.scalar(<span class="hljs-string">'accuracy'</span>, accuracy)

            <span class="hljs-comment"># Get training operator</span>
            learning_rate = get_learning_rate(batch)
            tf.summary.scalar(<span class="hljs-string">'learning_rate'</span>, learning_rate)
            <span class="hljs-keyword">if</span> OPTIMIZER == <span class="hljs-string">'momentum'</span>:
                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)
            <span class="hljs-keyword">elif</span> OPTIMIZER == <span class="hljs-string">'adam'</span>:
                optimizer = tf.train.AdamOptimizer(learning_rate)
            train_op = optimizer.minimize(loss, global_step=batch)
            
            <span class="hljs-comment"># Add ops to save and restore all the variables.</span>
            saver = tf.train.Saver()
            
        <span class="hljs-comment"># Create a session</span>
        config = tf.ConfigProto()
        config.gpu_options.allow_growth = <span class="hljs-literal">True</span>
        config.allow_soft_placement = <span class="hljs-literal">True</span>
        config.log_device_placement = <span class="hljs-literal">True</span>
        sess = tf.Session(config=config)

        <span class="hljs-comment"># Add summary writers</span>
        merged = tf.summary.merge_all()
        train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, <span class="hljs-string">'train'</span>),
                                  sess.graph)
        test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, <span class="hljs-string">'test'</span>))

        <span class="hljs-comment"># Init variables</span>
        init = tf.global_variables_initializer()
        sess.run(init, {is_training_pl:<span class="hljs-literal">True</span>})

        ops = {<!-- --><span class="hljs-string">'pointclouds_pl'</span>: pointclouds_pl,
               <span class="hljs-string">'labels_pl'</span>: labels_pl,
               <span class="hljs-string">'is_training_pl'</span>: is_training_pl,
               <span class="hljs-string">'pred'</span>: pred,
               <span class="hljs-string">'loss'</span>: loss,
               <span class="hljs-string">'train_op'</span>: train_op,
               <span class="hljs-string">'merged'</span>: merged,
               <span class="hljs-string">'step'</span>: batch}

        <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(MAX_EPOCH):
            log_string(<span class="hljs-string">'**** EPOCH %03d ****'</span> % (epoch))
            sys.stdout.flush()
             
            train_one_epoch(sess, ops, train_writer)
            eval_one_epoch(sess, ops, test_writer)
            
            <span class="hljs-comment"># Save the variables to disk.</span>
            <span class="hljs-keyword">if</span> epoch % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:
                save_path = saver.save(sess, os.path.join(LOG_DIR, <span class="hljs-string">"model.ckpt"</span>))
                log_string(<span class="hljs-string">"Model saved in file: %s"</span> % save_path)



<span class="hljs-keyword">def</span> <span class="function_ hljs-title">train_one_epoch</span>(<span class="hljs-params">sess, ops, train_writer</span>):
    <span class="hljs-string">""" ops: dict mapping from string to tf ops """</span>
    is_training = <span class="hljs-literal">True</span>
    
    log_string(<span class="hljs-string">'----'</span>)
    current_data, current_label, _ = provider.shuffle_data(train_data, train_label)
    current_data = current_data[:,<span class="hljs-number">0</span>:NUM_POINT,:]
    current_label = current_label[:,<span class="hljs-number">0</span>:NUM_POINT]

    file_size = current_data.shape[<span class="hljs-number">0</span>]
    num_batches = file_size // BATCH_SIZE
    
    total_correct = <span class="hljs-number">0</span>
    total_seen = <span class="hljs-number">0</span>
    loss_sum = <span class="hljs-number">0</span>
    
    <span class="hljs-keyword">for</span> batch_idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_batches):
        <span class="hljs-keyword">if</span> batch_idx % <span class="hljs-number">1</span> == <span class="hljs-number">0</span>:
            <span class="hljs-built_in">print</span>(<span class="hljs-string">'Current batch/total batch num: %d/%d'</span>%(batch_idx,num_batches))
        start_idx = batch_idx * BATCH_SIZE
        end_idx = (batch_idx+<span class="hljs-number">1</span>) * BATCH_SIZE
        
        feed_dict = {ops[<span class="hljs-string">'pointclouds_pl'</span>]: current_data[start_idx:end_idx, :, :],
                     ops[<span class="hljs-string">'labels_pl'</span>]: current_label[start_idx:end_idx],
                     ops[<span class="hljs-string">'is_training_pl'</span>]: is_training,}
        summary, step, _, loss_val, pred_val = sess.run([ops[<span class="hljs-string">'merged'</span>], ops[<span class="hljs-string">'step'</span>], ops[<span class="hljs-string">'train_op'</span>], ops[<span class="hljs-string">'loss'</span>], ops[<span class="hljs-string">'pred'</span>]],
                                         feed_dict=feed_dict)
        train_writer.add_summary(summary, step)
        pred_val = np.argmax(pred_val, <span class="hljs-number">2</span>)
        correct = np.<span class="hljs-built_in">sum</span>(pred_val == current_label[start_idx:end_idx])
        total_correct += correct
        total_seen += (BATCH_SIZE*NUM_POINT)
        loss_sum += loss_val
    
    log_string(<span class="hljs-string">'mean loss: %f'</span> % (loss_sum / <span class="hljs-built_in">float</span>(num_batches)))
    log_string(<span class="hljs-string">'accuracy: %f'</span> % (total_correct / <span class="hljs-built_in">float</span>(total_seen)))

        
<span class="hljs-keyword">def</span> <span class="function_ hljs-title">eval_one_epoch</span>(<span class="hljs-params">sess, ops, test_writer</span>):
    <span class="hljs-string">""" ops: dict mapping from string to tf ops """</span>
    is_training = <span class="hljs-literal">False</span>
    total_correct = <span class="hljs-number">0</span>
    total_seen = <span class="hljs-number">0</span>
    loss_sum = <span class="hljs-number">0</span>
    total_seen_class = [<span class="hljs-number">0</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(NUM_CLASSES)]
    total_correct_class = [<span class="hljs-number">0</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(NUM_CLASSES)]
    
    log_string(<span class="hljs-string">'----'</span>)
    current_data, current_label, _ = provider.shuffle_data(test_data, test_label)

    current_data = current_data[:, <span class="hljs-number">0</span>:NUM_POINT, :]
    current_label = current_label[:, <span class="hljs-number">0</span>:NUM_POINT]

    current_label = np.squeeze(current_label)
    file_size = current_data.shape[<span class="hljs-number">0</span>]
    num_batches = file_size // BATCH_SIZE
    
    <span class="hljs-keyword">for</span> batch_idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_batches):
        start_idx = batch_idx * BATCH_SIZE
        end_idx = (batch_idx+<span class="hljs-number">1</span>) * BATCH_SIZE

        feed_dict = {ops[<span class="hljs-string">'pointclouds_pl'</span>]: current_data[start_idx:end_idx, :, :],
                     ops[<span class="hljs-string">'labels_pl'</span>]: current_label[start_idx:end_idx],
                     ops[<span class="hljs-string">'is_training_pl'</span>]: is_training}
        summary, step, loss_val, pred_val = sess.run([ops[<span class="hljs-string">'merged'</span>], ops[<span class="hljs-string">'step'</span>], ops[<span class="hljs-string">'loss'</span>], ops[<span class="hljs-string">'pred'</span>]],
                                      feed_dict=feed_dict)
        test_writer.add_summary(summary, step)
        pred_val = np.argmax(pred_val, <span class="hljs-number">2</span>)
        correct = np.<span class="hljs-built_in">sum</span>(pred_val == current_label[start_idx:end_idx])
        total_correct += correct
        total_seen += (BATCH_SIZE*NUM_POINT)
        loss_sum += (loss_val*BATCH_SIZE)
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(start_idx, end_idx):
            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(NUM_POINT):
                l = current_label[i, j]
                total_seen_class[l] += <span class="hljs-number">1</span>
                total_correct_class[l] += (pred_val[i-start_idx, j] == l)
            
    log_string(<span class="hljs-string">'eval mean loss: %f'</span> % (loss_sum / <span class="hljs-built_in">float</span>(total_seen/NUM_POINT)))
    log_string(<span class="hljs-string">'eval accuracy: %f'</span>% (total_correct / <span class="hljs-built_in">float</span>(total_seen)))
    log_string(<span class="hljs-string">'eval avg class acc: %f'</span> % (np.mean(np.array(total_correct_class)/np.array(total_seen_class,dtype=np.<span class="hljs-built_in">float</span>))))
         


<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:
    train()
    LOG_FOUT.close()
</code></pre> 
</div> 
<p>运行文件，开始训练</p> 
<div> 
 <p class="img-center"><img alt="" height="512" src="https://images2.imgbox.com/96/53/clysgnD5_o.png" width="600"><span title="点击并拖拽以改变尺寸">​</span></p> 
</div> 
<p>  <span style="color:#0d0016;"><strong>6. 分割网络预测</strong></span></p> 
<p>博主修改了下sem_seg/batch_inference.py中的代码，如下：</p> 
<div> 
 <pre><code class="language-python hljs"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> tensorflow.compat.v1 <span class="hljs-keyword">as</span> tf
tf.compat.v1.disable_eager_execution()
<span class="hljs-keyword">import</span> argparse
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> sys
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.dirname(BASE_DIR)
sys.path.append(BASE_DIR)
<span class="hljs-keyword">from</span> model <span class="hljs-keyword">import</span> *
<span class="hljs-keyword">import</span> indoor3d_util

parser = argparse.ArgumentParser()
parser.add_argument(<span class="hljs-string">'--gpu'</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">0</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">'GPU to use [default: GPU 0]'</span>)
parser.add_argument(<span class="hljs-string">'--batch_size'</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">1</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">'Batch Size during training [default: 1]'</span>)
parser.add_argument(<span class="hljs-string">'--num_point'</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">4096</span>*<span class="hljs-number">20</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">'Point number [default: 4096]'</span>)
parser.add_argument(<span class="hljs-string">'--model_path'</span>, default=<span class="hljs-string">'log/model.ckpt'</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">'model checkpoint file path'</span>)
parser.add_argument(<span class="hljs-string">'--dump_dir'</span>, default=<span class="hljs-string">'dump'</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">'dump folder path'</span>)
parser.add_argument(<span class="hljs-string">'--output_filelist'</span>, default=<span class="hljs-string">'output.txt'</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">'TXT filename, filelist, each line is an output for a room'</span>)
parser.add_argument(<span class="hljs-string">'--room_data_filelist'</span>, default=<span class="hljs-string">'meta/area6_data_label.txt'</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">'TXT filename, filelist, each line is a test room data label file.'</span>)
parser.add_argument(<span class="hljs-string">'--no_clutter'</span>, action=<span class="hljs-string">'store_true'</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">'If true, donot count the clutter class'</span>)
parser.add_argument(<span class="hljs-string">'--visu'</span>, default=<span class="hljs-string">'true'</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">'Whether to output OBJ file for prediction visualization.'</span>)
FLAGS = parser.parse_args()

BATCH_SIZE = FLAGS.batch_size
NUM_POINT = FLAGS.num_point
MODEL_PATH = FLAGS.model_path
GPU_INDEX = FLAGS.gpu
DUMP_DIR = FLAGS.dump_dir
<span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(DUMP_DIR): os.mkdir(DUMP_DIR)
LOG_FOUT = <span class="hljs-built_in">open</span>(os.path.join(DUMP_DIR, <span class="hljs-string">'log_evaluate.txt'</span>), <span class="hljs-string">'w'</span>)
LOG_FOUT.write(<span class="hljs-built_in">str</span>(FLAGS)+<span class="hljs-string">'\n'</span>)

ROOM_PATH_LIST = [BASE_DIR + <span class="hljs-string">"/mydata_withlabel/tabel01 - Cloud_1_withlabel.asc"</span>,
                  BASE_DIR + <span class="hljs-string">"/mydata_withlabel/tabel01 - Cloud_2_withlabel.asc"</span>,
                  BASE_DIR + <span class="hljs-string">"/mydata_withlabel/tabel01 - Cloud_3_withlabel.asc"</span>,
                  BASE_DIR + <span class="hljs-string">"/mydata_withlabel/tabel05 - Cloud_1_withlabel.asc"</span>,
                  BASE_DIR + <span class="hljs-string">"/mydata_withlabel/tabel10 - Cloud_2_withlabel.asc"</span>]

NUM_CLASSES = <span class="hljs-number">4</span>

<span class="hljs-keyword">def</span> <span class="function_ hljs-title">log_string</span>(<span class="hljs-params">out_str</span>):
    LOG_FOUT.write(out_str+<span class="hljs-string">'\n'</span>)
    LOG_FOUT.flush()
    <span class="hljs-built_in">print</span>(out_str)

<span class="hljs-keyword">def</span> <span class="function_ hljs-title">evaluate</span>():
    is_training = <span class="hljs-literal">False</span>
     
    <span class="hljs-keyword">with</span> tf.device(<span class="hljs-string">'/gpu:'</span>+<span class="hljs-built_in">str</span>(GPU_INDEX)):
        pointclouds_pl, labels_pl = placeholder_inputs(BATCH_SIZE, NUM_POINT)
        is_training_pl = tf.placeholder(tf.<span class="hljs-built_in">bool</span>, shape=())

        <span class="hljs-comment"># simple model</span>
        pred = get_model(pointclouds_pl, is_training_pl)
        loss = get_loss(pred, labels_pl)
        pred_softmax = tf.nn.softmax(pred)
 
        <span class="hljs-comment"># Add ops to save and restore all the variables.</span>
        saver = tf.train.Saver()
        
    <span class="hljs-comment"># Create a session</span>
    config = tf.ConfigProto()
    config.gpu_options.allow_growth = <span class="hljs-literal">True</span>
    config.allow_soft_placement = <span class="hljs-literal">True</span>
    config.log_device_placement = <span class="hljs-literal">True</span>
    sess = tf.Session(config=config)

    <span class="hljs-comment"># Restore variables from disk.</span>
    saver.restore(sess, MODEL_PATH)
    log_string(<span class="hljs-string">"Model restored."</span>)

    ops = {<!-- --><span class="hljs-string">'pointclouds_pl'</span>: pointclouds_pl,
           <span class="hljs-string">'labels_pl'</span>: labels_pl,
           <span class="hljs-string">'is_training_pl'</span>: is_training_pl,
           <span class="hljs-string">'pred'</span>: pred,
           <span class="hljs-string">'pred_softmax'</span>: pred_softmax,
           <span class="hljs-string">'loss'</span>: loss}

    <span class="hljs-keyword">for</span> room_path <span class="hljs-keyword">in</span> ROOM_PATH_LIST:
        out_data_label_filename = os.path.basename(room_path)[:-<span class="hljs-number">4</span>] + <span class="hljs-string">'_pred.txt'</span>
        out_data_label_filename = os.path.join(DUMP_DIR, out_data_label_filename)
        out_gt_label_filename = os.path.basename(room_path)[:-<span class="hljs-number">4</span>] + <span class="hljs-string">'_gt.txt'</span>
        out_gt_label_filename = os.path.join(DUMP_DIR, out_gt_label_filename)
        <span class="hljs-built_in">print</span>(room_path, out_data_label_filename)
        eval_one_epoch(sess, ops, room_path, out_data_label_filename, out_gt_label_filename)

<span class="hljs-keyword">def</span> <span class="function_ hljs-title">eval_one_epoch</span>(<span class="hljs-params">sess, ops, room_path, out_data_label_filename, out_gt_label_filename</span>):
    error_cnt = <span class="hljs-number">0</span>
    is_training = <span class="hljs-literal">False</span>
    total_correct = <span class="hljs-number">0</span>
    total_seen = <span class="hljs-number">0</span>
    loss_sum = <span class="hljs-number">0</span>
    total_seen_class = [<span class="hljs-number">0</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(NUM_CLASSES)]
    total_correct_class = [<span class="hljs-number">0</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(NUM_CLASSES)]

    points = np.loadtxt(room_path)
    <span class="hljs-built_in">print</span>(points.shape)

    sample = np.random.choice(points.shape[<span class="hljs-number">0</span>], NUM_POINT)
    sample_data = points[sample,...]

    points_list = []
    points_list.append(sample_data)
    data_label = np.stack(points_list, axis=<span class="hljs-number">0</span>)
    <span class="hljs-built_in">print</span>(data_label.shape)

    current_data = data_label[:, :, <span class="hljs-number">0</span>:<span class="hljs-number">6</span>]
    current_label = data_label[:, :, <span class="hljs-number">6</span>]

    <span class="hljs-built_in">print</span>(current_data .shape)
    <span class="hljs-built_in">print</span>(current_label.shape)

    file_size = current_data.shape[<span class="hljs-number">0</span>]
    num_batches = file_size // BATCH_SIZE
    <span class="hljs-built_in">print</span>(file_size)

    <span class="hljs-keyword">for</span> batch_idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_batches):
        start_idx = batch_idx * BATCH_SIZE
        end_idx = (batch_idx+<span class="hljs-number">1</span>) * BATCH_SIZE
        cur_batch_size = end_idx - start_idx
        
        feed_dict = {ops[<span class="hljs-string">'pointclouds_pl'</span>]: current_data[start_idx:end_idx, :, :],
                     ops[<span class="hljs-string">'labels_pl'</span>]: current_label[start_idx:end_idx],
                     ops[<span class="hljs-string">'is_training_pl'</span>]: is_training}
        loss_val, pred_val = sess.run([ops[<span class="hljs-string">'loss'</span>], ops[<span class="hljs-string">'pred_softmax'</span>]],
                                      feed_dict=feed_dict)

        <span class="hljs-keyword">if</span> FLAGS.no_clutter:
            pred_label = np.argmax(pred_val[:,:,<span class="hljs-number">0</span>:<span class="hljs-number">12</span>], <span class="hljs-number">2</span>) <span class="hljs-comment"># BxN</span>
        <span class="hljs-keyword">else</span>:
            pred_label = np.argmax(pred_val, <span class="hljs-number">2</span>) <span class="hljs-comment"># BxN</span>
        correct = np.<span class="hljs-built_in">sum</span>(pred_label == current_label[start_idx:end_idx,:])
        total_correct += correct
        total_seen += (cur_batch_size*NUM_POINT)
        loss_sum += (loss_val*BATCH_SIZE)

        pred_label = pred_label[:, :, np.newaxis]
        pred_data_label = np.concatenate([current_data, pred_label], <span class="hljs-number">2</span>)
        np.savetxt(out_data_label_filename, pred_data_label[<span class="hljs-number">0</span>, :, :], fmt=<span class="hljs-string">"%.8f %.8f %.8f %.8f %.8f %.8f %d"</span>,
                   delimiter=<span class="hljs-string">" "</span>)

    log_string(<span class="hljs-string">'eval mean loss: %f'</span> % (loss_sum / <span class="hljs-built_in">float</span>(total_seen/NUM_POINT)))
    log_string(<span class="hljs-string">'eval accuracy: %f'</span>% (total_correct / <span class="hljs-built_in">float</span>(total_seen)))

    <span class="hljs-keyword">return</span>

<span class="hljs-keyword">if</span> __name__==<span class="hljs-string">'__main__'</span>:
    <span class="hljs-keyword">with</span> tf.Graph().as_default():
        evaluate()
    LOG_FOUT.close()
</code></pre> 
</div> 
<p>注意model.py文件中13需要改为4(自己的数据集上只区分了四类）</p> 
<div> 
 <p class="img-center"><img alt="" height="232" src="https://images2.imgbox.com/8a/d4/j04l6Kix_o.png" width="600"><span title="点击并拖拽以改变尺寸">​</span></p> 
</div> 
<p>运行文件，结果保存在 dump中，可用cloudcompare打开以预测标签作为scalar field的文件</p> 
<div> 
 <p class="img-center"><img alt="" height="186" src="https://images2.imgbox.com/87/d5/uyYREBN0_o.png" width="343"><span title="点击并拖拽以改变尺寸">​</span></p> 
</div> 
<p></p> 
<div> 
 <p class="img-center"><img alt="" height="250" src="https://images2.imgbox.com/cc/55/KmSDizij_o.png" width="600"><span title="点击并拖拽以改变尺寸">​</span></p> 
</div> 
<p> 可看到，大体分割出来了。</p> 
<div> 
 <p class="img-center"><img alt="" height="407" src="https://images2.imgbox.com/09/7f/k70792Nf_o.png" width="600"><span title="点击并拖拽以改变尺寸">​</span></p> 
</div> 
<p>这边由于在前面训练时候，只随机从各点云文件中提取了4096个点，还是很稀疏的。一些类别的点参与训练不充分。后续可以就这些点再去优化，博主这边暂时不继续做了，感兴趣的童鞋可以继续优化下去，这边只说明如何在自己的训练集上做训练和预测。</p> 
<p>上传下博主的工程，链接如下：</p> 
<p>链接: https://pan.baidu.com/s/1HWRCwtorUC6fVWeaKjh5Qg 提取码: 318v<br>  </p> 
<p><strong>参考博客</strong></p> 
<p><a href="https://blog.csdn.net/CC047964/article/details/124345423" title="制作PointNet以及PointNet++点云训练样本_点云数据集制作_CC047964的博客-CSDN博客">制作PointNet以及PointNet++点云训练样本_点云数据集制作_CC047964的博客-CSDN博客</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/344245796?utm_id=0" rel="nofollow" title="点云标注 - 知乎">点云标注 - 知乎</a></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/3a4cc3f4635bc7540f2cdfac18b87bc1/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【HTML】通过meta代码强制浏览器使用WebKit内核极速模式（解决 meta name=“renderer“ content=“webkit“ 不起作用）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c23dd31b9030444beac9a3a1b6c30314/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【Flink】Flink 中的时间和窗口之水位线(Watermark)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>