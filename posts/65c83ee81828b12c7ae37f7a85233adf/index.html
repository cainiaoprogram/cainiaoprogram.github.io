<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【标准化方法】(3) Group Normalization 原理解析、代码复现，附Pytorch代码 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【标准化方法】(3) Group Normalization 原理解析、代码复现，附Pytorch代码" />
<meta property="og:description" content="今天和各位分享一下深度学习中常用的标准化方法，Group Normalization 数据分组归一化，向大家介绍一下数学原理，并用 Pytorch 复现。
Group Normalization 论文地址：https://arxiv.org/pdf/1803.08494.pdf
1. 原理介绍 在目标检测，视频分类等大型计算机视觉应用中，受到计算机内存的限制，必须设置较小的样本数量，但是样本量小势必会导致批归一化的性能有所影响。
分组归一化（Group Normalization，GN）是针对批归一化算法对批次大小依赖性强这一弱点而提出的改进算法。因为 BN 层统计信息的计算与批次的大小有关，因此当批次变小时，很明显统计均值和方差的计算会越不准确和稳定，最终会有小批次高错误率的这一现象发生。
分组归一化 GN 介于层归一化 LN 和实例归一化 IN 之间，对于输入大小为 [N,C,H,W] 的图像，N 代表批次的大小，C 表示输入通道数，H,W 表示输入图片高度和宽度。
分组归一化首先将输入通道 C 分为 G 个小组，然后分别对每一小组做归一化操作，也就是先把输入的特征维度由 变成 ，归一化的维度为 。
事实上，当 G 等于 1 时，即所有的输入通道为 1 组时 GN 与 LN 的计算方式相同，而当 G 等于 C 时，1 个输入通道为 1 组时 GN 与 IN 的计算方式相同。
上图是批归一化算法 BN、层归一化算法 LN、实例归一化 IN 和分组归一化 GN 的简单图示。图中的立方体是三维，蓝色的方块是各个算法计算均值和方差的区域。
其中 C 代表通道数，N 是批量大小，H,W 是高度和宽度，第三个维度的大小是 H*W，这样输入就可以用三维图形来表示。从上图中可以看出只有 BN 的计算与批次大小 N 有关，LN、IN 和 GN 的计算都在单个样本上进行， LN、IN 和 GN 三者可相互转换。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/65c83ee81828b12c7ae37f7a85233adf/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-09T15:04:22+08:00" />
<meta property="article:modified_time" content="2023-05-09T15:04:22+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【标准化方法】(3) Group Normalization 原理解析、代码复现，附Pytorch代码</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><span style="color:#0d0016;">今天和各位分享一下深度学习中常用的标准化方法，Group Normalization 数据分组归一化，向大家介绍一下数学原理，并用 Pytorch 复现。</span></p> 
<p><span style="color:#0d0016;">Group Normalization 论文地址：<a href="https://arxiv.org/pdf/1803.08494.pdf" rel="nofollow" title="https://arxiv.org/pdf/1803.08494.pdf">https://arxiv.org/pdf/1803.08494.pdf</a></span></p> 
<hr> 
<h2><strong><span style="color:#ff9900;">1. 原理介绍</span></strong></h2> 
<p><span style="color:#0d0016;">在目标检测，视频分类等大型计算机视觉应用中，受到计算机内存的限制，必须设置较小的样本数量，但是样本量小势必会导致批归一化的性能有所影响。</span></p> 
<p><span style="color:#be191c;"><strong>分组归一化</strong></span><span style="color:#0d0016;">（Group Normalization，</span><span style="color:#be191c;"><strong>GN</strong></span><span style="color:#0d0016;">）是</span><span style="color:#956fe7;"><strong>针对批归一化算法对批次大小依赖性强这一弱点而提出的改进算法</strong></span><span style="color:#0d0016;">。因为 <strong>BN 层统计信息的计算与批次的大小有关</strong>，</span><span style="color:#1a439c;"><strong>因此当批次变小时，很明显统计均值和方差的计算会越不准确和稳定，最终会有小批次高错误率的这一现象发生</strong></span><span style="color:#0d0016;">。</span></p> 
<p><span style="color:#0d0016;">分组归一化 GN 介于层归一化 LN 和实例归一化 IN 之间，对于输入大小为 [N,C,H,W] 的图像，N 代表批次的大小，C 表示输入通道数，H,W 表示输入图片高度和宽度。</span></p> 
<p><span style="color:#0d0016;"><strong>分组归一化首先</strong></span><span style="color:#1c7331;"><strong>将输入通道 C 分为 G 个小组</strong></span><span style="color:#0d0016;"><strong>，然后分别对每一小组做归一化操作，也就是先把输入的特征维度由 <img alt="[N,C,H,W]" class="mathcode" src="https://images2.imgbox.com/33/fa/LLO5Rgfo_o.png"> 变成 <img alt="[N,G,\frac{C}{G},H,W]" class="mathcode" src="https://images2.imgbox.com/36/04/3dld2zCS_o.png">，</strong></span><span style="color:#b95514;"><strong>归一化的维度</strong></span><span style="color:#0d0016;"><strong>为 </strong><img alt="[\frac{C}{G},H,W]" class="mathcode" src="https://images2.imgbox.com/7c/96/p0Wh0HgX_o.png">。</span></p> 
<p><span style="color:#0d0016;">事实上，</span><span style="color:#be191c;"><strong>当 G 等于 1 时</strong></span><span style="color:#0d0016;">，即</span><span style="color:#1c7331;"><strong>所有的输入通道为 1 组</strong></span><span style="color:#0d0016;">时 <strong>GN 与 LN 的计算方式相同</strong>，而</span><span style="color:#be191c;"><strong>当 G 等于 C 时</strong></span><span style="color:#0d0016;">，</span><span style="color:#1c7331;"><strong>1 个输入通道为 1 组</strong></span><span style="color:#0d0016;">时 <strong>GN 与 IN 的计算方式相同</strong>。</span></p> 
<p class="img-center"><img alt="" height="203" src="https://images2.imgbox.com/8c/7a/gBELavMD_o.png" width="676"></p> 
<p><span style="color:#0d0016;">上图是批归一化算法 BN、层归一化算法 LN、实例归一化 IN 和分组归一化 GN 的简单图示。图中的立方体是三维，</span><span style="color:#1a439c;"><strong>蓝色的方块是各个算法计算均值和方差的区域</strong></span><span style="color:#0d0016;">。</span></p> 
<p><span style="color:#0d0016;">其中 C 代表通道数，N 是批量大小，H,W 是高度和宽度，第三个维度的大小是 H*W，这样输入就可以用三维图形来表示。从上图中可以看出</span><span style="color:#b95514;"><strong>只有 BN  的计算与批次大小 N 有关</strong></span><span style="color:#0d0016;">，</span><span style="color:#1c7331;"><strong>LN、IN 和 GN 的计算都在单个样本上进行</strong></span><span style="color:#0d0016;">，  LN、IN 和 GN 三者可相互转换。</span></p> 
<p><strong><span style="color:#0d0016;">通常来说，归一化的方式如下所示：</span></strong></p> 
<p style="text-align:center;"><img alt="\mu_i=\frac{1}{m}\sum_{k\in S_i}x_k" class="mathcode" src="https://images2.imgbox.com/58/fb/MRUOF4iP_o.png"></p> 
<p style="text-align:center;"><img alt="\sigma_i=\sqrt{\frac{1}{m}\sum_{k\in S_i}\left(x_k-\mu_i\right)^2+\epsilon}" class="mathcode" src="https://images2.imgbox.com/dd/9f/45qRWbG5_o.png"></p> 
<p><span style="color:#0d0016;"><img alt="S_i" class="mathcode" src="https://images2.imgbox.com/a2/8a/a45dSYkv_o.png"> 是均值和方差的计算区域，在 BN 中有：</span></p> 
<p style="text-align:center;"><span style="color:#0d0016;"><img alt="S_i=\left\{k|k_C=i_C\right\}" class="mathcode" src="https://images2.imgbox.com/5c/30/o3MYMRzQ_o.png"></span></p> 
<p><span style="color:#0d0016;">在 LN 中：</span></p> 
<p style="text-align:center;"><img alt="S_i=\left\{k|k_N=i_N\right\}" class="mathcode" src="https://images2.imgbox.com/ee/10/B2P4o73W_o.png"></p> 
<p><span style="color:#0d0016;">在 GN 中：</span></p> 
<p style="text-align:center;"><img alt="S_i=\{k\mid k_N=i_N,floor(\frac{k_C}{C/G})=floor(\frac{i_C}{C/G})\}" class="mathcode" src="https://images2.imgbox.com/66/98/AdBadbzK_o.png"></p> 
<p id="72"><span style="color:#0d0016;"><strong>优点：</strong>不依赖批量大小。</span></p> 
<p id="73"><span style="color:#0d0016;"><strong>缺点：</strong>当批量大小较大时，性能不如BN。</span></p> 
<hr> 
<h2><strong><span style="color:#ff9900;">2. 代码展示</span></strong></h2> 
<pre><code class="language-python">import torch 
from torch import nn

class GN(nn.Module):
    # 初始化
    def __init__(self, groups:int, channels:int, 
                 eps:float=1e-5, affine:bool=True):
        super(GN, self).__init__()
        # 通道数要整除组数
        assert channels % groups == 0, 'channels should be evenly divisible by groups'
        self.groups = groups  # 把通道分成多少组
        self.channels = channels  # 通道数
        self.eps = eps  # 防止分母为0
        self.affine = affine  # 是否使用可学习的线性变化参数
        if self.affine:
            self.scale = nn.Parameter(torch.ones(channels))  # 缩放因子
            self.shift = nn.Parameter(torch.zeros(channels))  # 偏置
    # 前向传播
    def forward(self, x: torch.Tensor):
        x_shape = x.shape  # 输入特征的维度 [b,c,w,h]
        batch_size = x_shape[0]  # 样本量
        assert self.channels == x.shape[1]  # 预设通道数和输入特征的通道数要保持一致
        # [b,c,w,h]--&gt;[b,g,w*h*c/g]
        x = x.view(batch_size, self.groups, -1)
        # 在最后一个维度上做标准化
        mean = x.mean(dim=[-1], keepdim=True)  # [b,g,1]
        mean_x2 = (x**2).mean(dim=[-1], keepdim=True)  # [b,g,1]
        var = mean_x2 - mean**2
        x_norm = (x-mean) / torch.sqrt(var+self.eps)  # [b,g,w*h*c/g]
        # 线性变化
        if self.affine:
            x_norm = x_norm.view(batch_size, self.channels, -1)  # [b,c,w*h]
            x_norm = self.scale.view(1,-1,1)* x_norm + self.shift.view(1,-1,1)  # [1,c,1]*[b,c,w*h]+[1,c,1]
        # [b,c,w*h]--&gt;[b,c,w,h]
        return x_norm.view(x_shape)

# ---------------------------------- #
# 验证
# ---------------------------------- #

if __name__ == '__main__':
    # 构造输入层
    x = torch.linspace(0, 47, 48, dtype=torch.float32)  # 构造输入层
    x = x.reshape([2,6,2,2])  # [b,c,w,h]
    # 实例化
    gn = GN(groups=3, channels=6)
    # 前向传播
    x = gn(x)
    print(x.shape)</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/39ab4aaf8096953337cbb31ae042baa6/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">备战数学建模2——MATLAB导入数据，处理缺失值</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0a401cc32a8290d6fef3e877be89e216/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Linux 虚拟化技术 KVM</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>