<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>SimCSE初步使用且和Bert的简单对比 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="SimCSE初步使用且和Bert的简单对比" />
<meta property="og:description" content="SimCSE初步使用且和Bert的简单对比 在很多 NLP 任务中都会用到句子向量，例如文本检索、文本粗排、语义匹配等任务。现在有不少基于 Bert 的方式获取句子向量，例如 Bert-flow 和 Bert-whitening 等，这些方法会对预训练 Bert 的输出进行变换从而得到更好的句子向量。本文介绍 SimCSE，SimCSE 通过对比学习的方法训练模型，取得 SOTA 的效果。
模型下载 huggingface这个网站真的是太棒了。提供了封装好后的SimCSE。其实SimCSE也是在Bert基础上进行了修改。所以使用方式和Bert没有什么区别。
可以去这个网址下载https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased模型文件。我是预先下载到本地了。
简单使用 import torch from scipy.spatial.distance import cosine from transformers import AutoModel, AutoTokenizer model_path_simcse = &#34;../pretrained_models/sup-simcse-bert-base-uncased&#34; model_path_bert = &#34;../pretrained_models/bert-base-uncased&#34; # 从本地加载simcse模型 tokenizer_simcse = AutoTokenizer.from_pretrained(model_path_simcse) model_simcse = AutoModel.from_pretrained(model_path_simcse) # 从本地加载bert模型 tokenizer_bert = AutoTokenizer.from_pretrained(model_path_bert) model_bert = AutoModel.from_pretrained(model_path_bert) # Tokenize input texts texts = [ &#34;Deep Learning&#34;, &#34;Hello&#34;, &#34;World&#34; ] inputs_simcse = tokenizer_simcse(texts, padding=True, truncation=True, return_tensors=&#34;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/4a2abcc874bc9e5251d6b7b33041c815/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-03-30T17:07:35+08:00" />
<meta property="article:modified_time" content="2022-03-30T17:07:35+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">SimCSE初步使用且和Bert的简单对比</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-dracula">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="SimCSEBert_0"></a>SimCSE初步使用且和Bert的简单对比</h2> 
<blockquote> 
 <p>在很多 NLP 任务中都会用到句子向量，例如文本检索、文本粗排、语义匹配等任务。现在有不少基于 Bert 的方式获取句子向量，例如 Bert-flow 和 Bert-whitening 等，这些方法会对预训练 Bert 的输出进行变换从而得到更好的句子向量。本文介绍 SimCSE，SimCSE 通过对比学习的方法训练模型，取得 SOTA 的效果。</p> 
</blockquote> 
<h3><a id="_4"></a>模型下载</h3> 
<p><code>huggingface</code>这个网站真的是太棒了。提供了封装好后的SimCSE。其实SimCSE也是在Bert基础上进行了修改。所以使用方式和Bert没有什么区别。</p> 
<p>可以去这个网址下载<code>https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased</code>模型文件。我是预先下载到本地了。</p> 
<h3><a id="_10"></a>简单使用</h3> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> scipy<span class="token punctuation">.</span>spatial<span class="token punctuation">.</span>distance <span class="token keyword">import</span> cosine
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModel<span class="token punctuation">,</span> AutoTokenizer

model_path_simcse <span class="token operator">=</span> <span class="token string">"../pretrained_models/sup-simcse-bert-base-uncased"</span>
model_path_bert <span class="token operator">=</span> <span class="token string">"../pretrained_models/bert-base-uncased"</span>
<span class="token comment"># 从本地加载simcse模型</span>
tokenizer_simcse <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_path_simcse<span class="token punctuation">)</span>
model_simcse <span class="token operator">=</span> AutoModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_path_simcse<span class="token punctuation">)</span>
<span class="token comment"># 从本地加载bert模型</span>
tokenizer_bert <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_path_bert<span class="token punctuation">)</span>
model_bert <span class="token operator">=</span> AutoModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_path_bert<span class="token punctuation">)</span>

<span class="token comment"># Tokenize input texts</span>
texts <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token string">"Deep Learning"</span><span class="token punctuation">,</span>
    <span class="token string">"Hello"</span><span class="token punctuation">,</span>
    <span class="token string">"World"</span>
<span class="token punctuation">]</span>

inputs_simcse <span class="token operator">=</span> tokenizer_simcse<span class="token punctuation">(</span>texts<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span>
inputs_bert <span class="token operator">=</span> tokenizer_bert<span class="token punctuation">(</span>texts<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span>

<span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    embeddings_simcse <span class="token operator">=</span> model_simcse<span class="token punctuation">(</span><span class="token operator">**</span>inputs_simcse<span class="token punctuation">,</span> output_hidden_states<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> return_dict<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span>pooler_output
    embeddings_bert <span class="token operator">=</span> model_bert<span class="token punctuation">(</span><span class="token operator">**</span>inputs_bert<span class="token punctuation">,</span> output_hidden_states<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> return_dict<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span>pooler_output

<span class="token comment"># 计算余弦相似度</span>
cosine_sim_0_1_simcse <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">-</span> cosine<span class="token punctuation">(</span>embeddings_simcse<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> embeddings_simcse<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
cosine_sim_0_2_simcse <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">-</span> cosine<span class="token punctuation">(</span>embeddings_simcse<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> embeddings_simcse<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"使用SimCSE"</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Cosine similarity between \"%s\" and \"%s\" is: %.3f"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>texts<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> texts<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> cosine_sim_0_1_simcse<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Cosine similarity between \"%s\" and \"%s\" is: %.3f"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>texts<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> texts<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> cosine_sim_0_2_simcse<span class="token punctuation">)</span><span class="token punctuation">)</span>

cosine_sim_0_1_bert <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">-</span> cosine<span class="token punctuation">(</span>embeddings_bert<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> embeddings_bert<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
cosine_sim_0_2_bert <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">-</span> cosine<span class="token punctuation">(</span>embeddings_bert<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> embeddings_bert<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"使用原生Bert"</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Cosine similarity between \"%s\" and \"%s\" is: %.3f"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>texts<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> texts<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> cosine_sim_0_1_bert<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Cosine similarity between \"%s\" and \"%s\" is: %.3f"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>texts<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> texts<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> cosine_sim_0_2_bert<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="_54"></a>结果展示</h3> 
<pre><code class="prism language-python">使用SimCSE
Cosine similarity between <span class="token string">"Deep Learning"</span> <span class="token keyword">and</span> <span class="token string">"Hello"</span> <span class="token keyword">is</span><span class="token punctuation">:</span> <span class="token number">0.209</span>
Cosine similarity between <span class="token string">"Deep Learning"</span> <span class="token keyword">and</span> <span class="token string">"World"</span> <span class="token keyword">is</span><span class="token punctuation">:</span> <span class="token number">0.432</span>
使用原生Bert
Cosine similarity between <span class="token string">"Deep Learning"</span> <span class="token keyword">and</span> <span class="token string">"Hello"</span> <span class="token keyword">is</span><span class="token punctuation">:</span> <span class="token number">0.919</span>
Cosine similarity between <span class="token string">"Deep Learning"</span> <span class="token keyword">and</span> <span class="token string">"World"</span> <span class="token keyword">is</span><span class="token punctuation">:</span> <span class="token number">0.818</span>
</code></pre> 
<h3><a id="_65"></a>小结</h3> 
<p>引用知乎文章<code>https://zhuanlan.zhihu.com/p/430580960</code>的解释.Bert存在两个问题</p> 
<ol><li>Bert encode出来的向量表达具有各向异性 
  <ul><li>什么叫各向异性？举个例子，一些电阻原件，正接是良导体，反接是绝缘体或者电阻很大，沿不同方向差异很大。在bert出来的向量中表现为，用不同的方式去衡量它，他表现出不同的语义，差别很大，也就是不能完整的衡量出bert向量中全部语义信息</li></ul> </li><li>分布不均匀，低频词分布稀疏，高频词分布紧密。 
  <ul><li>也就是高频词会集中在头部，离原点近，低频词会集中在尾部，离远点远高频词与低频词分布在不同的区域，那高频词与低频词之间的相识度也就没法计算了。这也反映出来的就是明显的低频词没有得到一个很好的训练。同时，高频词频次高，也会主宰句子表达。</li></ul> </li></ol> 
<p>从实验中我们也可以看出来Bert做文本表达任务很明显效果不好。<code>Deep Learning</code> 和 <code>Hello</code> 的余弦相似度竟然高达<code>0.919</code>。而<code>SimCSE</code>很明显比<code>Bert</code>的表达更好一点。</p> 
<h3><a id="_76"></a>补充</h3> 
<p>值得注意的是，<code>单词级</code>相似度比较不适用于BERT embeddings，因为这些嵌入是上下文相关的，这意味着单词vector会根据它出现在的句子而变化。这就允许了像一词多义这样的奇妙的东西，例如，你的表示编码了river “bank”，而不是金融机构“bank”，但却使得直接的词与词之间的相似性比较变得不那么有价值。但是，对于句子嵌入相似性比较仍然是有效的，这样就可以对一个句子查询其他句子的数据集，从而找到最相似的句子。根据使用的相似度度量，得到的相似度值将比相似度输出的相对排序提供的信息更少，因为许多相似度度量对向量空间(例如，等权重维度)做了假设，而这些假设不适用于768维向量空间。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/9307b5e92d5c1bcc26efd9364a326661/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Python之Socket自动重连</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/523a3427b678e6f2d7ef95f14dd96354/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">第1章 准备工作</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>