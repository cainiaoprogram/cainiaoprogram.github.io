<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>机器学习各类优化算法总结 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="机器学习各类优化算法总结" />
<meta property="og:description" content="目录
1 Intro
2 一阶优化算法
2.1 Gradient descent
Batch Gradient Descent
Stochastic Gradient Descent
Mini-batch Gradient Descent
2.2 Momentum
2.3 Nesterov accelerated gradient （NAG）
2.4 AdaGrad
2.5 Adadelta与Rmsprop
2.6 Adam
2.7 AdaMax
2.8 Nadam
2.9 AMSgrad
2.10 Adafactor
2.11 Adabound
3 二阶优化算法
3.1 牛顿法
3.2 拟牛顿法
Conclusion
Reference
1 Intro 深度学习模型的优化是一个非凸优化问题，这是与凸优化问题对应的。
对于凸优化来说，任何局部最优解即为全局最优解。用贪婪算法或梯度下降法都能收敛到全局最优解。而非凸优化问题则可能存在无数个局部最优点，损失曲面如下，可以看出有非常多的极值点，有极大值也有极小值。
除了极大极小值，还有一类值为“鞍点”，简单来说，它就是在某一些方向梯度下降，另一些方向梯度上升，形状似马鞍，如下图红点就是鞍点。
对于深度学习模型的优化来说，鞍点比局部极大值点或者极小值点带来的问题更加严重。
目前常用的优化方法分为一阶和二阶，这里的阶对应导数，一阶方法只需要一阶导数，二阶方法需要二阶导数。
常用的一阶算法就是：随机梯度下降SGD及其各类变种了。
常用的二阶算法就是：牛顿法等。
2 一阶优化算法 2.1 Gradient descent Batch Gradient Descent Stochastic Gradient Descent Mini-batch Gradient Descent 2." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/ab493ad1b441f0e3fa3b2f2b75afaf56/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-02-05T15:50:22+08:00" />
<meta property="article:modified_time" content="2020-02-05T15:50:22+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">机器学习各类优化算法总结</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="1%C2%A0Intro-toc" style="margin-left:0px;"><a href="#1%C2%A0Intro" rel="nofollow">1 Intro</a></p> 
<p id="2%C2%A0%E4%B8%80%E9%98%B6%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95-toc" style="margin-left:0px;"><a href="#2%C2%A0%E4%B8%80%E9%98%B6%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95" rel="nofollow">2 一阶优化算法</a></p> 
<p id="2.1%C2%A0Gradient%C2%A0descent-toc" style="margin-left:40px;"><a href="#2.1%C2%A0Gradient%C2%A0descent" rel="nofollow">2.1 Gradient descent</a></p> 
<p id="Batch%20Gradient%20Descent-toc" style="margin-left:80px;"><a href="#Batch%20Gradient%20Descent" rel="nofollow">Batch Gradient Descent</a></p> 
<p id="Stochastic%20Gradient%20Descent-toc" style="margin-left:80px;"><a href="#Stochastic%20Gradient%20Descent" rel="nofollow">Stochastic Gradient Descent</a></p> 
<p id="Mini-batch%20Gradient%20Descent-toc" style="margin-left:80px;"><a href="#Mini-batch%20Gradient%20Descent" rel="nofollow">Mini-batch Gradient Descent</a></p> 
<p id="2.2%C2%A0Momentum-toc" style="margin-left:40px;"><a href="#2.2%C2%A0Momentum" rel="nofollow">2.2 Momentum</a></p> 
<p id="2.3%C2%A0Nesterov%20accelerated%20gradient%C2%A0%EF%BC%88NAG%EF%BC%89-toc" style="margin-left:40px;"><a href="#2.3%C2%A0Nesterov%20accelerated%20gradient%C2%A0%EF%BC%88NAG%EF%BC%89" rel="nofollow">2.3 Nesterov accelerated gradient （NAG）</a></p> 
<p id="2.4%C2%A0AdaGrad-toc" style="margin-left:40px;"><a href="#2.4%C2%A0AdaGrad" rel="nofollow">2.4 AdaGrad</a></p> 
<p id="2.5%C2%A0Adadelta%E4%B8%8ERmsprop-toc" style="margin-left:40px;"><a href="#2.5%C2%A0Adadelta%E4%B8%8ERmsprop" rel="nofollow">2.5 Adadelta与Rmsprop</a></p> 
<p id="2.6%C2%A0Adam-toc" style="margin-left:40px;"><a href="#2.6%C2%A0Adam" rel="nofollow">2.6 Adam</a></p> 
<p id="2.7%C2%A0AdaMax-toc" style="margin-left:40px;"><a href="#2.7%C2%A0AdaMax" rel="nofollow">2.7 AdaMax</a></p> 
<p id="2.8%C2%A0Nadam-toc" style="margin-left:40px;"><a href="#2.8%C2%A0Nadam" rel="nofollow">2.8 Nadam</a></p> 
<p id="2.9%C2%A0AMSgrad-toc" style="margin-left:40px;"><a href="#2.9%C2%A0AMSgrad" rel="nofollow">2.9 AMSgrad</a></p> 
<p id="2.10%20Adafactor-toc" style="margin-left:40px;"><a href="#2.10%20Adafactor" rel="nofollow">2.10 Adafactor</a></p> 
<p id="2.11%C2%A0Adabound-toc" style="margin-left:40px;"><a href="#2.11%C2%A0Adabound" rel="nofollow">2.11 Adabound</a></p> 
<p id="3%C2%A0%E4%BA%8C%E9%98%B6%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95-toc" style="margin-left:0px;"><a href="#3%C2%A0%E4%BA%8C%E9%98%B6%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95" rel="nofollow">3 二阶优化算法</a></p> 
<p id="3.1%C2%A0%E7%89%9B%E9%A1%BF%E6%B3%95-toc" style="margin-left:40px;"><a href="#3.1%C2%A0%E7%89%9B%E9%A1%BF%E6%B3%95" rel="nofollow">3.1 牛顿法</a></p> 
<p id="3.2%C2%A0%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95-toc" style="margin-left:40px;"><a href="#3.2%C2%A0%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95" rel="nofollow">3.2 拟牛顿法</a></p> 
<p id="Conclusion-toc" style="margin-left:0px;"><a href="#Conclusion" rel="nofollow">Conclusion</a></p> 
<p id="Reference-toc" style="margin-left:0px;"><a href="#Reference" rel="nofollow">Reference</a></p> 
<hr id="hr-toc"> 
<h2 id="1%C2%A0Intro">1 Intro</h2> 
<p>深度学习模型的优化是一个非凸优化问题，这是与凸优化问题对应的。</p> 
<p>对于凸优化来说，任何局部最优解即为全局最优解。用贪婪算法或梯度下降法都能收敛到全局最优解。而非凸优化问题则可能存在无数个局部最优点，损失曲面如下，可以看出有非常多的极值点，有极大值也有极小值。</p> 
<p>除了极大极小值，还有一类值为“鞍点”，简单来说，它就是在某一些方向梯度下降，另一些方向梯度上升，形状似马鞍，如下图<strong>红点就是鞍点</strong>。</p> 
<p><img alt="" class="has" height="238" src="https://images2.imgbox.com/8b/4a/UOXmukfD_o.png" width="606"></p> 
<p>对于深度学习模型的优化来说，鞍点比局部极大值点或者极小值点带来的问题更加严重。</p> 
<p>目前常用的优化方法分为一阶和二阶，这里的阶对应导数，一阶方法只需要一阶导数，二阶方法需要二阶导数。</p> 
<p>常用的一阶算法就是：随机梯度下降SGD及其各类变种了。</p> 
<p>常用的二阶算法就是：牛顿法等。</p> 
<h2 id="2%C2%A0%E4%B8%80%E9%98%B6%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95">2 一阶优化算法</h2> 
<h3 id="2.1%C2%A0Gradient%C2%A0descent">2.1 Gradient <strong>descent</strong></h3> 
<h4 id="Batch%20Gradient%20Descent">Batch Gradient Descent</h4> 
<p><img alt="" class="has" src="https://images2.imgbox.com/b3/21/oKhxSeoM_o.png"></p> 
<h4 id="Stochastic%20Gradient%20Descent">Stochastic Gradient Descent</h4> 
<p><img alt="" class="has" src="https://images2.imgbox.com/83/c7/NJQIGu6Y_o.png"></p> 
<h4 id="Mini-batch%20Gradient%20Descent">Mini-batch Gradient Descent</h4> 
<p><img alt="" class="has" src="https://images2.imgbox.com/9e/4a/qhBJzepi_o.png"></p> 
<h3 id="2.2%C2%A0Momentum">2.2 Momentum</h3> 
<p>前面说了梯度下降算法是按照梯度的反方向进行参数更新，但是<strong>刚开始的时候梯度不稳定呀，方向改变是很正常的</strong>，梯度就是抽疯了似的一下正一下反，导致做了很多无用的迭代。</p> 
<p>而动量法做的很简单，<strong>相信之前的梯度</strong>。如果梯度方向不变，就越发更新的快，反之减弱当前梯度。</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/9f/31/HeyIe5Hp_o.png"></p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/34/10/4GIKRlWA_o.png"></p> 
<ul><li><strong>优点：</strong>可以使得梯度方向不变的维度上速度变快，梯度方向有所改变的维度上的更新速度变慢，这样就可以加快收敛并减小震荡。</li><li><strong>缺点：</strong>这种情况相当于小球从山上滚下来时是在盲目地沿着坡滚，如果它能具备一些先知，例如快要上坡时，就知道需要减速了的话，适应性会更好。</li></ul> 
<h3 id="2.3%C2%A0Nesterov%20accelerated%20gradient%C2%A0%EF%BC%88NAG%EF%BC%89">2.3 Nesterov accelerated gradient （NAG）</h3> 
<p>仍然是动量法，只是它要求这个下降更加智能。</p> 
<p>既然动量法已经把前一次的梯度和当前梯度融合，那何不更进一步，直接先按照前一次梯度方向更新一步将它作为当前的梯度，看下面的式子就明白了。</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/4e/49/sIaPCZvj_o.png"></p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/c2/df/f2UOJYwg_o.png"></p> 
<h3 id="2.4%C2%A0AdaGrad">2.4 AdaGrad</h3> 
<p>前面的一系列优化算法有一个共同的特点，就是对于每一个参数都用相同的学习率进行更新。但是在实际应用中各个参数的重要性肯定是不一样的，所以我们对于不同的参数要动态的采取不同的学习率，让目标函数更快的收敛。<br> adagrad方法是将每一个参数的每一次迭代的梯度取平方累加再开方，用基础学习率除以这个数，来做学习率的动态更新。【这样每一个参数的学习率就与他们的梯度有关系了，那么每一个参数的学习率就不一样了！也就是所谓的<strong>自适应学习率</strong>】</p> 
<p><img alt="" class="has" height="326" src="https://images2.imgbox.com/e1/26/PAergjce_o.png" width="813"></p> 
<ul><li><strong>优点：</strong>减少了学习率的手动调节</li><li><strong>缺点：</strong>分母会不断积累，这样学习率就会收缩并最终会变得非常小。</li></ul> 
<h3 id="2.5%C2%A0Adadelta%E4%B8%8ERmsprop">2.5 Adadelta与Rmsprop</h3> 
<p><img alt="这里写图片描述" class="has" src="https://images2.imgbox.com/5f/b1/G5I2VlDP_o.png"></p> 
<p>RMSprop方法的不同就在于分子上还是使用学习率η而不是Adadelta中的RMS</p> 
<p><img alt="这里写图片描述" class="has" src="https://images2.imgbox.com/47/93/MhvT7ShG_o.png"></p> 
<h3 id="2.6%C2%A0Adam">2.6 Adam</h3> 
<p>Adam对梯度的一阶和二阶都进行了估计与偏差修正，使用梯度的一阶矩估计和二阶矩估计来动态调整每个参数的学习率。</p> 
<p><img alt="" class="has" height="658" src="https://images2.imgbox.com/45/12/JfGuH6bG_o.png" width="1077"></p> 
<h3 id="2.7%C2%A0AdaMax">2.7 AdaMax</h3> 
<p>将Adam使用的二阶矩变成更高阶，就成了Adamax算法。</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/fd/29/ajDTksgQ_o.png"></p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/9c/39/A1drcKqP_o.png"></p> 
<h3 id="2.8%C2%A0Nadam">2.8 Nadam</h3> 
<p>Nag加上Adam，就成了Nadam方法，即带有动量项的Adam。</p> 
<p><img alt="这里写图片描述" class="has" src="https://images2.imgbox.com/76/00/wnvmw1o9_o.png"></p> 
<h3 id="2.9%C2%A0AMSgrad">2.9 AMSgrad</h3> 
<p>ICLR 2018最佳论文提出了AMSgrad方法，研究人员观察到Adam类的方法之所以会不能收敛到好的结果，是因为在优化算法中广泛使用的指数衰减方法会使得梯度的记忆时间太短。</p> 
<p>在深度学习中，每一个mini-batch对结果的优化贡献是不一样的，有的产生的梯度特别有效，但是也一视同仁地被时间所遗忘。</p> 
<p>具体的做法是使用过去平方梯度的最大值来更新参数，而不是指数平均。</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/3e/e5/5ziXFPhw_o.png"></p> 
<h3 id="2.10%20Adafactor">2.10 Adafactor</h3> 
<p>Adam算法有两个参数，beta1和beta2，相关研究表明beta2的值对收敛结果有影响，如果较低，衰减太大容易不收敛，反之就容易收敛不稳定。Adafactor是通过给beta1和beta2本身也增加了一个衰减。</p> 
<p>beta2的值刚开始是0，之后随着时间的增加而逼近预设值。</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/d0/55/GyouHMqH_o.png"></p> 
<h3 id="2.11%C2%A0Adabound">2.11 Adabound</h3> 
<p>上面说了，beta2的值造成Adam算法有可能不收敛或者不稳定而找不到全局最优解，落实到最后的优化参数那就是不稳定和异常(过大或者过小)的学习率。Adabound采用的解决问题的方式就非常的简单了，那就是限制最大和最小值范围，约束住学习率的大小。</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/d0/60/6YiegUs6_o.png"></p> 
<p>ηl(t)和ηu(t)分别是一个随着时间单调递增和递减的函数，最后两者收敛到同一个值。</p> 
<h2 id="3%C2%A0%E4%BA%8C%E9%98%B6%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95">3 二阶优化算法</h2> 
<p>二阶的方法因为使用了导数的二阶信息，因此其优化方向更加准确，速度也更快，这是它的优势。</p> 
<p>但是它的劣势也极其明显，使用二阶方法通常需要直接计算或者近似估计Hessian 矩阵，一阶方法一次迭代更新复杂度为O(N)，二阶方法就是O(N*N)，深层神经网络中变量实在是太多了，搞不动的。</p> 
<h3 id="3.1%C2%A0%E7%89%9B%E9%A1%BF%E6%B3%95">3.1 牛顿法</h3> 
<p>待完善</p> 
<h3 id="3.2%C2%A0%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95">3.2 拟牛顿法</h3> 
<p>待完善</p> 
<h2 id="Conclusion">Conclusion</h2> 
<p><img alt="这里写图片描述" class="has" src="https://images2.imgbox.com/ba/d8/9Lq9Tbqh_o.gif"></p> 
<p><img alt="这里写图片描述" class="has" src="https://images2.imgbox.com/3a/0a/Q5aEULue_o.gif"></p> 
<p><img alt="这里写图片描述" class="has" src="https://images2.imgbox.com/28/b7/5yXFl5Wv_o.png"></p> 
<h2 id="Reference">Reference</h2> 
<p><a href="https://zhuanlan.zhihu.com/p/57860231?edition=yidianzixun&amp;utm_source=yidianzixun&amp;yidian_docid=0LO5CYFK" rel="nofollow">【AI初识境】为了围剿SGD大家这些年想过的那十几招(从momentum到Adabound)</a></p> 
<p><a href="https://blog.csdn.net/qq_23269761/article/details/80901411">深度学习总结（一）各种优化算法</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/9c256ab3dbaad26e58d72e060ae9aeb6/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Python3中利用map将列表当中的string类型转换为int类型或其它类型</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/272e12d27d9b524e725852edb7d15096/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">第三章第十五题（游戏：彩票）(Game: lottery)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>