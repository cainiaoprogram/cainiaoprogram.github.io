<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>tensorrt推理 onxx转engine代码（python），cyclegan网络推理（python、C&#43;&#43;） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="tensorrt推理 onxx转engine代码（python），cyclegan网络推理（python、C&#43;&#43;）" />
<meta property="og:description" content="将onnx文件导出为engine，FP16格式
​ ​ import tensorrt as trt import pycuda.driver as cuda import pycuda.autoinit # 加载ONNX文件 onnx_file_path = &#39;model.onnx&#39; engine_file_path = &#39;model_tesfp16.trt&#39; TRT_LOGGER = trt.Logger(trt.Logger.WARNING) builder = trt.Builder(TRT_LOGGER) network = builder.create_network(1) parser = trt.OnnxParser(network, TRT_LOGGER) # 解析ONNX文件 with open(onnx_file_path, &#39;rb&#39;) as f: data = f.read() parser.parse(data) # 构建TensorRT引擎 builder_config = builder.create_builder_config() builder_config.max_workspace_size = 4*(1 &lt;&lt; 30) builder_config.set_flag(trt.BuilderFlag.FP16) engine = builder.build_engine(network, builder_config) # 构建TensorRT引擎 # builder_config = builder.create_builder_config() # builder_config.max_workspace_size = 1 &lt;&lt; 30 # # builder_config." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/6f622e69f73eb504d9fc472b8a7c0052/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-11-08T09:41:49+08:00" />
<meta property="article:modified_time" content="2023-11-08T09:41:49+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">tensorrt推理 onxx转engine代码（python），cyclegan网络推理（python、C&#43;&#43;）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>将onnx文件导出为engine，FP16格式</p> 
<pre><code class="language-python">​
​
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit

# 加载ONNX文件
onnx_file_path = 'model.onnx'
engine_file_path = 'model_tesfp16.trt'

TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
builder = trt.Builder(TRT_LOGGER)
network = builder.create_network(1)
parser = trt.OnnxParser(network, TRT_LOGGER)

# 解析ONNX文件
with open(onnx_file_path, 'rb') as f:
    data = f.read()
    parser.parse(data)

# 构建TensorRT引擎
builder_config = builder.create_builder_config()
builder_config.max_workspace_size = 4*(1 &lt;&lt; 30)
builder_config.set_flag(trt.BuilderFlag.FP16) 
engine = builder.build_engine(network, builder_config)

# 构建TensorRT引擎
# builder_config = builder.create_builder_config()
# builder_config.max_workspace_size = 1 &lt;&lt; 30
# # builder_config.max_batch_size = 1  # 设置最大批量大小
# builder_config.set_flag(trt.BuilderFlag.FP16) 
# # builder_config.set_flag(trt.BuilderFlag.INT8) 
# engine = builder.build_engine(network, builder_config)




# 保存TensorRT引擎到文件
with open(engine_file_path, 'wb') as f:
    f.write(engine.serialize())

​

​</code></pre> 
<p>以cyclegan网络，分别用python 、C++对网络进行推理</p> 
<pre><code class="language-python">import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit
import numpy as np
import time
import cv2
# 加载TRT引擎
# engine_file_path = 'model_fp16.trt'
engine_file_path = 'model_tesfp16.trt'
with open(engine_file_path, 'rb') as f:
    engine_data = f.read()
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
trt.init_libnvinfer_plugins(TRT_LOGGER, '')
runtime = trt.Runtime(TRT_LOGGER)

engine = runtime.deserialize_cuda_engine(engine_data)

# 创建执行上下文
context = engine.create_execution_context()

# 分配内存
# 创建输入和输出缓冲区
# 分配输入和输出内存
input_shape = (1, 1, 512, 512)  # 输入数据的形状 如果是三通道（1，3，512，512）
output_shape = (1, 1,512,512)  # 输出数据的形状 如果是三通道（1，3，512，512）
# input_data = np.random.randn(*input_shape).astype(np.float32)
input_data = cv2.imread("image1644.png",0)
input_data = input_data.reshape((1,1,512, 512,)).astype(np.float32) 如果是三通道（1，3，512，512）

output_data = np.empty(output_shape, dtype=np.float32)
# 在GPU上分配内存
d_input = cuda.mem_alloc(input_data.nbytes)
d_output = cuda.mem_alloc(output_data.nbytes)
# 创建CUDA流
stream = cuda.Stream()

# 将输入数据从主机内存复制到GPU内存
cuda.memcpy_htod_async(d_input, input_data, stream)

# 执行TensorRT推理
T1 = time.time()
bindings = [int(d_input), int(d_output)]
stream_handle = stream.handle
context.execute_async_v2(bindings=bindings, stream_handle=stream_handle)

# 将输出数据从GPU内存复制到主机内存
cuda.memcpy_dtoh_async(output_data, d_output, stream)

# 等待推理完成
stream.synchronize()
T2 = time.time()
print('程序运行时间:%s毫秒' % ((T2 - T1)*1000))
# 打印输出结果
print(type(output_data))

a_sque = np.squeeze(output_data)
a_sque =-a_sque*255
a_sque = 255 -a_sque
# print(a_sque)
# img = cv2.cvtColor(a_sque, cv2.COLOR_GRAY2BGR)
cv2.imwrite("tensorrt_ilubuntu.jpg",a_sque)
print("output_data = ",output_data)</code></pre> 
<p>C++版本首先配置cmakelists</p> 
<pre><code class="language-python">cmake_minimum_required(VERSION 2.6)

project(cycle_gan)

add_definitions(-std=c++11)
add_definitions(-DAPI_EXPORTS)
option(CUDA_USE_STATIC_CUDA_RUNTIME OFF)
set(CMAKE_CXX_STANDARD 11)
# set(CMAKE_BUILD_TYPE Release)
set(CMAKE_BUILD_TYPE Release)
find_package(CUDA REQUIRED)


enable_language(CUDA)


include_directories(${PROJECT_SOURCE_DIR}/include)
# include and link dirs of cuda and tensorrt, you need adapt them if yours are different
# cuda
include_directories(/usr/local/cuda-11.6/include)
link_directories(/usr/local/cuda-11.6/lib64)
# tensorrt
include_directories(/home/mao/bag/TensorRT-8.2.5.1/include/)
link_directories(/home/mao/bag/TensorRT-8.2.5.1/lib/)


# include_directories(/usr/include/x86_64-linux-gnu/)
# link_directories(/usr/lib/x86_64-linux-gnu/)

set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -std=c++11 -Wall -Ofast -g -Wfatal-errors -D_MWAITXINTRIN_H_INCLUDED")



find_package(OpenCV)
include_directories(${OpenCV_INCLUDE_DIRS})

cuda_add_executable(cycle_gan main.cpp )
#cuda_add_library(yolov5 SHARED ${PROJECT_SOURCE_DIR}/yolov5.cpp ${PROJECT_SOURCE_DIR}/yololayer.cu ${PROJECT_SOURCE_DIR}/yololayer.h ${PROJECT_SOURCE_DIR}/preprocess.cu)

target_link_libraries(cycle_gan nvonnxparser nvinfer nvinfer_plugin)
target_link_libraries(cycle_gan cudart)

target_link_libraries(cycle_gan ${OpenCV_LIBS})

if(UNIX)
add_definitions(-O2 -pthread)
endif(UNIX)
</code></pre> 
<pre><code class="language-cpp">#include "NvInfer.h"
#include "cuda_runtime_api.h"
#include &lt;fstream&gt;
#include &lt;iostream&gt;
#include &lt;map&gt;
#include &lt;sstream&gt;
#include &lt;vector&gt;
#include &lt;chrono&gt;
#include "NvInferPlugin.h"
#include  "opencv2/opencv.hpp"


#include &lt;time.h&gt; 
// 定义Engine文件路径
const std::string enginePath = "/home/mao/code/style/GAN/model_tesfp16.trt";

class Logger : public nvinfer1::ILogger
{
public:
    void log(Severity severity, const char* msg) noexcept override
    {
        // 根据需要自定义日志输出逻辑
        switch (severity)
        {
            case Severity::kINTERNAL_ERROR:
                std::cerr &lt;&lt; "INTERNAL_ERROR: " &lt;&lt; msg &lt;&lt; std::endl;
                break;
            case Severity::kERROR:
                std::cerr &lt;&lt; "ERROR: " &lt;&lt; msg &lt;&lt; std::endl;
                break;
            case Severity::kWARNING:
                std::cerr &lt;&lt; "WARNING: " &lt;&lt; msg &lt;&lt; std::endl;
                break;
            case Severity::kINFO:
                std::cout &lt;&lt; "INFO: " &lt;&lt; msg &lt;&lt; std::endl;
                break;
            default:
                break;
        }
    }
};

using namespace nvinfer1;

static Logger gLogger;


// void doInference(IExecutionContext&amp; context, float* input, float* output, int batchSize)
// {
//     const ICudaEngine&amp; engine = context.getEngine();

//     // Pointers to input and output device buffers to pass to engine.
//     // Engine requires exactly IEngine::getNbBindings() number of buffers.
//     assert(engine.getNbBindings() == 2);
//     void* buffers[2];

//     // In order to bind the buffers, we need to know the names of the input and output tensors.
//     // Note that indices are guaranteed to be less than IEngine::getNbBindings()
//     const int inputIndex = engine.getBindingIndex(INPUT_BLOB_NAME);
//     const int outputIndex = engine.getBindingIndex(OUTPUT_BLOB_NAME);

//     // Create GPU buffers on device
//     CHECK(cudaMalloc(&amp;buffers[inputIndex], batchSize * 3 * INPUT_H * INPUT_W * sizeof(float)));
//     CHECK(cudaMalloc(&amp;buffers[outputIndex], batchSize * OUTPUT_SIZE * sizeof(float)));

//     // Create stream
//     cudaStream_t stream;
//     CHECK(cudaStreamCreate(&amp;stream));

//     // DMA input batch data to device, infer on the batch asynchronously, and DMA output back to host
//     CHECK(cudaMemcpyAsync(buffers[inputIndex], input, batchSize * 3 * INPUT_H * INPUT_W * sizeof(float), cudaMemcpyHostToDevice, stream));
//     context.enqueue(batchSize, buffers, stream, nullptr);
//     CHECK(cudaMemcpyAsync(output, buffers[outputIndex], batchSize * OUTPUT_SIZE * sizeof(float), cudaMemcpyDeviceToHost, stream));
//     cudaStreamSynchronize(stream);

//     // Release stream and buffers
//     cudaStreamDestroy(stream);
//     CHECK(cudaFree(buffers[inputIndex]));
//     CHECK(cudaFree(buffers[outputIndex]));
// }



int main(int argc, char** argv)
{

    // nvinfer1::ILogger* gLogger;
     initLibNvInferPlugins(&amp;gLogger, ""); 
    // 创建TensorRT的运行时对象
    IRuntime* runtime = createInferRuntime(gLogger);

    // 从文件中反序列化Engine对象
    std::ifstream engineFile(enginePath, std::ios::binary);
    if (!engineFile)
    {
        std::cerr &lt;&lt; "无法打开Engine文件进行读取。" &lt;&lt; std::endl;
        return 1;
    }
    engineFile.seekg(0, std::ios::end);
    const size_t fileSize = engineFile.tellg();
    engineFile.seekg(0, std::ios::beg);
    std::vector&lt;char&gt; engineData(fileSize);
    engineFile.read(engineData.data(), fileSize);
    engineFile.close();

    // 反序列化Engine对象
    IPluginFactory* pluginFactory = nullptr;  // 如果有自定义插件，可以传递一个插件工厂对象
    ICudaEngine* engine = runtime-&gt;deserializeCudaEngine(engineData.data(), fileSize, pluginFactory);
    if (!engine)
    {
        std::cerr &lt;&lt; "无法反序列化Engine对象。" &lt;&lt; std::endl;
        return 1;
    }

    // 创建TensorRT的执行上下文对象
    IExecutionContext* context = engine-&gt;createExecutionContext();

       // 分配输入和输出内存
   const int batchSize = 1;
   const int inputSize = 512; // 输入张量的大小
   const int outputSize = 512; // 输出张量的大小

   void* deviceInput;
   void* deviceOutput;

   cudaMalloc(&amp;deviceInput, batchSize * inputSize * sizeof(float));
   cudaMalloc(&amp;deviceOutput, batchSize * outputSize * sizeof(float));
   std::vector&lt;void*&gt; bindings = {deviceInput,deviceOutput};

   // 创建CUDA流
   cudaStream_t stream;
   cudaStreamCreate(&amp;stream);
   // 将输入数据复制到GPU内存中
   std::vector&lt;float&gt; input(batchSize * inputSize);

       // 读取图片作为输入
    cv::Mat image = cv::imread("/home/mao/code/style/GAN/0.png",0);
    
    // 将图片数据复制到输入张量
    cv::Mat resizedImage;
    cv::resize(image, resizedImage, cv::Size(512, 512));
    cv::Mat floatImage;
    resizedImage.convertTo(floatImage, CV_32F, 1.0 / 255.0);
    std::vector&lt;cv::Mat&gt; inputChannels(1);
    cv::split(floatImage, inputChannels);
    std::memcpy(input.data(), inputChannels[0].data, inputSize * sizeof(float));
   cudaMemcpyAsync(deviceInput, input.data(), batchSize * inputSize * sizeof(float), cudaMemcpyHostToDevice, stream);
    clock_t t;
    t = clock(); 
   // 执行推理
   context-&gt;enqueue(batchSize, bindings.data(), stream, nullptr);
    t = clock() - t;
    printf("             %d\n",t);
   // 将输出数据从GPU内存复制回主机内存
   std::vector&lt;float&gt; output(batchSize * outputSize);
   cudaMemcpyAsync(output.data(), deviceOutput, batchSize * outputSize * sizeof(float), cudaMemcpyDeviceToHost, stream);

   // 等待推理完成
   cudaStreamSynchronize(stream);

   // 处理输出结果输出是一维需要自己转换为二维
   std::cout &lt;&lt; output.data() &lt;&lt; std::endl;

    // 释放资源
    context-&gt;destroy();
    engine-&gt;destroy();
    runtime-&gt;destroy();

    return 0;
}

</code></pre> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a3efac4246fb2c600357c90fe6046642/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Ansible 安装与使用/ssh免密互信-以及解决免密不生效</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/59d5b5f299121ee58f6ff1365b1e6a54/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Docker指定容器使用内存</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>