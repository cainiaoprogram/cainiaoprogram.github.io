<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Robert&#43;Prompt&#43;对比学习&#43;对抗训练文本分类 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Robert&#43;Prompt&#43;对比学习&#43;对抗训练文本分类" />
<meta property="og:description" content="基于Robert的文本分类任务，在此基础上考虑融合对比学习、Prompt和对抗训练来提升模型的文本分类能力，我本地有SST-2数据集的train.txt、dev.txt两个文件，每个文件包含文本内容和标签两列，是个二分类任务，本项目基于pytorch实现。
先介绍一下要融合的三个技术。
1. 对比学习旨在通过对比相似和不相似的样本来提高分类模型的性能。对于每个样本，我们可以在训练时随机选取一个与其相似的样本，并加入到训练中，以鼓励模型更好地学习相似样本的特征，同时在训练时也要随机选取一个不相似的样本，并将其加入到训练中。这可以帮助模型更好地区分不同类别之间的特征。
2. Prompt是一种基于预设文本片段的模型输入方式。通过给定关键词和语法结构，Prompt可以引导模型学习某些具体任务。在文本分类任务中，我们可以给模型预设一些文本提示，以帮助模型更好地学习关键特征。
3. 对抗训练是一种在训练模型时加入干扰数据（扰动）的技术，以增强模型的鲁棒性。在文本分类任务中，我们可以通过向文本中添加词语或修改词语顺序，来生成干扰数据，从而帮助模型更好地区分和理解输入文本。
目录
一、安装依赖库
二、载数据集并进行数据预处理
三、定义模型并训练模型
四、对比学习实现
五、Prompt实现
六、对抗训练实现
七、整个过程封装成一个函数
一、安装依赖库 下面是具体实现的代码，我们将使用PyTorch框架：
首先安装必要的库：
!pip install transformers !pip install torch !pip install scikit-learn 然后我们导入需要的库以及设置随机种子以保证实验可重复性等必要组件： import random import numpy as np import torch from sklearn.metrics import accuracy_score, f1_score from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler from transformers import get_linear_schedule_with_warmup device = torch.device(&#39;cuda&#39;) if torch.cuda.is_available() else torch.device(&#39;cpu&#39;) random.seed(42) np.random.seed(42) torch.manual_seed(42) torch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/bd5a32d6757d20485f7cab60d88ea3b2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-12T20:39:59+08:00" />
<meta property="article:modified_time" content="2023-06-12T20:39:59+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Robert&#43;Prompt&#43;对比学习&#43;对抗训练文本分类</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>基于Robert的文本分类任务，在此基础上考虑融合对比学习、Prompt和对抗训练来提升模型的文本分类能力，我本地有SST-2数据集的train.txt、dev.txt两个文件，每个文件包含文本内容和标签两列，是个二分类任务，本项目基于pytorch实现。</p> 
<p>先介绍一下要融合的三个技术。</p> 
<p>1. 对比学习旨在通过对比相似和不相似的样本来提高分类模型的性能。对于每个样本，我们可以在训练时随机选取一个与其相似的样本，并加入到训练中，以鼓励模型更好地学习相似样本的特征，同时在训练时也要随机选取一个不相似的样本，并将其加入到训练中。这可以帮助模型更好地区分不同类别之间的特征。</p> 
<p>2. Prompt是一种基于预设文本片段的模型输入方式。通过给定关键词和语法结构，Prompt可以引导模型学习某些具体任务。在文本分类任务中，我们可以给模型预设一些文本提示，以帮助模型更好地学习关键特征。</p> 
<p>3. 对抗训练是一种在训练模型时加入干扰数据（扰动）的技术，以增强模型的鲁棒性。在文本分类任务中，我们可以通过向文本中添加词语或修改词语顺序，来生成干扰数据，从而帮助模型更好地区分和理解输入文本。</p> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="%E4%B8%80%E3%80%81%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96%E5%BA%93-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96%E5%BA%93" rel="nofollow">一、安装依赖库</a></p> 
<p id="%E4%BA%8C%E3%80%81%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86%E5%B9%B6%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86%E5%B9%B6%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86" rel="nofollow">二、载数据集并进行数据预处理</a></p> 
<p id="%E4%B8%89%E3%80%81%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-toc" style="margin-left:0px;"><a href="#%E4%B8%89%E3%80%81%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B" rel="nofollow">三、定义模型并训练模型</a></p> 
<p id="%E5%9B%9B%E3%80%81%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%8E%B0-toc" style="margin-left:0px;"><a href="#%E5%9B%9B%E3%80%81%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%8E%B0" rel="nofollow">四、对比学习实现</a></p> 
<p id="%E4%BA%94%E3%80%81Prompt%E5%AE%9E%E7%8E%B0-toc" style="margin-left:0px;"><a href="#%E4%BA%94%E3%80%81Prompt%E5%AE%9E%E7%8E%B0" rel="nofollow">五、Prompt实现</a></p> 
<p id="%E5%85%AD%E3%80%81%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83%E5%AE%9E%E7%8E%B0-toc" style="margin-left:0px;"><a href="#%E5%85%AD%E3%80%81%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83%E5%AE%9E%E7%8E%B0" rel="nofollow">六、对抗训练实现</a></p> 
<p id="%E4%B8%83%E3%80%81%E6%95%B4%E4%B8%AA%E8%BF%87%E7%A8%8B%E5%B0%81%E8%A3%85%E6%88%90%E4%B8%80%E4%B8%AA%E5%87%BD%E6%95%B0-toc" style="margin-left:0px;"><a href="#%E4%B8%83%E3%80%81%E6%95%B4%E4%B8%AA%E8%BF%87%E7%A8%8B%E5%B0%81%E8%A3%85%E6%88%90%E4%B8%80%E4%B8%AA%E5%87%BD%E6%95%B0" rel="nofollow">七、整个过程封装成一个函数</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="%E4%B8%80%E3%80%81%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96%E5%BA%93">一、安装依赖库</h2> 
<p>下面是具体实现的代码，我们将使用PyTorch框架：</p> 
<p>首先安装必要的库：</p> 
<pre><code class="hljs">!pip install transformers
!pip install torch
!pip install scikit-learn
</code></pre> 
<p>然后我们导入需要的库以及设置随机种子以保证实验可重复性等必要组件： </p> 
<pre><code class="language-python">import random
import numpy as np
import torch
from sklearn.metrics import accuracy_score, f1_score
from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW
from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler
from transformers import get_linear_schedule_with_warmup

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
torch.cuda.manual_seed_all(42)
torch.backends.cudnn.deterministic = True
</code></pre> 
<h2 id="%E4%BA%8C%E3%80%81%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86%E5%B9%B6%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86">二、载数据集并进行数据预处理</h2> 
<pre><code class="language-python">class TextDataset(Dataset):
    def __init__(self, tokenizer, path, max_length):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.labels = []
        self.texts = []
        with open(path) as f:
            for line in f:
                line = line.strip().split('\t')
                text, label = line[0], int(line[1])
                self.labels.append(label)
                self.texts.append(text)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        text, label = self.texts[idx], self.labels[idx]
        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length,
                                  return_tensors='pt')
        return dict(
            text=text,
            input_ids=encoding['input_ids'].squeeze(),
            attention_mask=encoding['attention_mask'].squeeze(),
            labels=torch.tensor(label)
        )

tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
train_dataset = TextDataset(tokenizer, 'train.txt', 256)
dev_dataset = TextDataset(tokenizer, 'dev.txt', 256)

train_sampler = RandomSampler(train_dataset)
dev_sampler = SequentialSampler(dev_dataset)

train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=16)
dev_loader = DataLoader(dev_dataset, sampler=dev_sampler, batch_size=16)
</code></pre> 
<h2 id="%E4%B8%89%E3%80%81%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B">三、定义模型并训练模型</h2> 
<pre><code class="language-python">model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2).to(device)
optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)

# We will use a linear decay scheduler
total_steps = len(train_loader) * 5
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)

for epoch in range(5):
    model.train()
    for batch in train_loader:
        batch = {k: v.to(device) for k, v in batch.items()}
        optimizer.zero_grad()

        outputs = model(**batch)
        loss = outputs[0]
        loss.backward()
        optimizer.step()
        scheduler.step()

    model.eval()
    with torch.no_grad():
        targets, preds = [], []
        for batch in dev_loader:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            targets.extend(batch['labels'].tolist())
            preds.extend(torch.argmax(outputs.logits, axis=-1).tolist())

    acc = accuracy_score(targets, preds)
    f1 = f1_score(targets, preds)
    print(f'\nEpoch {epoch + 1}:')
    print(f'Dev Accuracy: {acc:.4f}')
    print(f'Dev F1 Score: {f1:.4f}')
</code></pre> 
<p>至此，我们已经成功地训练了一款基于RoBERTa模型的文本分类器。下面是加入融合技术的实现。</p> 
<h2 id="%E5%9B%9B%E3%80%81%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%8E%B0">四、对比学习实现</h2> 
<pre><code class="language-python">def random_similar_text(texts, labels):
    res_texts, res_labels = [], []
    for idx, text in enumerate(texts):
        res_texts.append(text)
        res_labels.append(labels[idx])
        # 随机选择一个与当前样本相似的样本，将它加入到数据集中
        rand_idx = np.random.choice(len(texts), 1)[0]
        res_texts.append(texts[rand_idx])
        res_labels.append(labels[rand_idx])
        # 随机选择一个不相似的样本，将它加入到数据集中
        rand_idx = np.random.choice(len(texts), 1)[0]
        while rand_idx == idx:
            rand_idx = np.random.choice(len(texts), 1)[0]
        res_texts.append(texts[rand_idx])
        res_labels.append(labels[rand_idx])
    return res_texts, res_labels

train_texts, train_labels = random_similar_text(train_dataset.texts, train_dataset.labels)
train_dataset = TextDataset(tokenizer, 'train.txt', 256)
</code></pre> 
<h2 id="%E4%BA%94%E3%80%81Prompt%E5%AE%9E%E7%8E%B0">五、Prompt实现</h2> 
<pre><code class="language-python">def add_prompt(prompt, texts):
    return [f'{prompt}{text}' for text in texts]

train_dataset.texts = add_prompt('This text is', train_dataset.texts)
dev_dataset.texts = add_prompt('This text is', dev_dataset.texts)
</code></pre> 
<h2 id="%E5%85%AD%E3%80%81%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83%E5%AE%9E%E7%8E%B0">六、对抗训练实现</h2> 
<pre><code class="language-python">def add_perturbations(text, n):
    # 随机选择n个词，并在其周围添加一些噪声生成n个干扰文本
    words = text.split()
    idx_list = np.random.choice(len(words), n, replace=False)
    for idx in idx_list:
        words[idx] = f'[{words[idx]}]'
    return ' '.join(words)

def generate_perturbations(texts):
    return [add_perturbations(text, 3) for text in texts]

train_dataset.texts += generate_perturbations(train_dataset.texts)
dev_dataset.texts += generate_perturbations(dev_dataset.texts)
</code></pre> 
<h2 id="%E4%B8%83%E3%80%81%E6%95%B4%E4%B8%AA%E8%BF%87%E7%A8%8B%E5%B0%81%E8%A3%85%E6%88%90%E4%B8%80%E4%B8%AA%E5%87%BD%E6%95%B0">七、整个过程封装成一个函数</h2> 
<pre><code class="language-python">def train_roberta_with_fusion(train_path, dev_path, num_classes, fusion_type):
    def random_similar_text(texts, labels):
        res_texts, res_labels = [], []
        for idx, text in enumerate(texts):
            res_texts.append(text)
            res_labels.append(labels[idx])
            rand_idx = np.random.choice(len(texts), 1)[0]
            res_texts.append(texts[rand_idx])
            res_labels.append(labels[rand_idx])
            rand_idx = np.random.choice(len(texts), 1)[0]
            while rand_idx == idx:
                rand_idx = np.random.choice(len(texts), 1)[0]
            res_texts.append(texts[rand_idx])
            res_labels.append(labels[rand_idx])
        return res_texts, res_labels

    def add_perturbations(text, n):
        words = text.split()
        idx_list = np.random.choice(len(words), n, replace=False)
        for idx in idx_list:
            words[idx] = f'[{words[idx]}]'
        return ' '.join(words)

    def add_prompt(prompt, texts):
        return [f'{prompt}{text}' for text in texts]

    def generate_perturbations(texts):
        return [add_perturbations(text, 3) for text in texts]

    class TextDataset(Dataset):
        def __init__(self, tokenizer, path, max_length):
            self.tokenizer = tokenizer
            self.max_length = max_length
            self.labels = []
            self.texts = []
            with open(path) as f:
                for line in f:
                    line = line.strip().split('\t')
                    text, label = line[0], int(line[1])
                    self.labels.append(label)
                    self.texts.append(text)

            if fusion_type == 'contrastive':
                self.texts, self.labels = random_similar_text(self.texts, self.labels)

            if fusion_type == 'adversarial':
                self.texts += generate_perturbations(self.texts)

            if fusion_type == 'prompt':
                self.texts = add_prompt('This text is', self.texts)

        def __len__(self):
            return len(self.labels)

        def __getitem__(self, idx):
            text, label = self.texts[idx], self.labels[idx]
            encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length,
                                      return_tensors='pt')
            return dict(
                text=text,
                input_ids=encoding['input_ids'].squeeze(),
                attention_mask=encoding['attention_mask'].squeeze(),
                labels=torch.tensor(label)
            )

    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
    train_dataset = TextDataset(tokenizer, train_path, 256)
    dev_dataset = TextDataset(tokenizer, dev_path, 256)

    train_sampler = RandomSampler(train_dataset)
    dev_sampler = SequentialSampler(dev_dataset)

    train_loader = DataLoader(train_dataset, sampler=train
</code></pre> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/369bb7effc04ccbc3d0054be26090b46/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">torch.cat ( )和 np.concatenate() 的用法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/97465dfd89d8dadb79cd22fa305a30bd/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Java异常处理的十个建议，希望对大家有帮助~</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>