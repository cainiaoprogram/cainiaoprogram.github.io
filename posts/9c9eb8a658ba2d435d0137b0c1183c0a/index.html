<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>roberta与albert - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="roberta与albert" />
<meta property="og:description" content="roberta 简介
RoBERTA，是BERT模型的改进版，并获得了更好的自然语言处理效果，且其在GLUE、SQuAD、RACE等三个榜单上取得了SOTA效果细节 训练数据集上，RoBERTa采用了160G的训练文本，而BERT仅使用16G的训练文本
模型评估上，模型主要基于三个基准来评估：1、GLUE通用语言理解模型；2、SQuAD斯坦福问题答疑数据集；3、RACE考试的重新理解
预训练任务上，RoBERTa使用的是动态mask，BERT采用静态mask,其含义如下：
原来Bert对每一个序列随机选择15%的Tokens替换成[MASK]，为了消除与下游任务的不匹配，还对这15%的Tokens进行（1）80%的时间替换成[MASK]；（2）10%的时间不变；（3）10%的时间替换成其他词。但整个训练过程，这15%的Tokens一旦被选择就不再改变，也就是说从一开始随机选择了这15%的Tokens，之后的N个epoch里都不再改变了。这就叫做静态Masking。
在此预训练任务中，取消了NSP任务，增加了FULL_SENTENCES机制：在该机制下，输入的不再是两个句子，而是用大段话填满指定的字节长度，如果句子跨越了文章就增加一个分割的token。
训练参数：RoBERTa模型增加了训练的batch_size，并将adam的0.999改成了0.98，增加了训练的step,最后使用的batch_size为8k，训练步数为500步。输入的token编码为BPE编码。
albert albert主要解决Bert参数过大、训练国漫的问题，其主要通过两个参数削减技术克服预训练模型扩展的障碍：
embedding参数因式分解：将两个大的词嵌入矩阵分解为两个小的矩阵，从而将隐藏层与词典的大小关系分割开来，两者不再直接关系，使得隐藏层的节点数扩展不再受到限制跨层参数共享：避免参数随着网络的深度增加而增加句间连贯性损失：正例与bert一样，两个连贯的句子；负例也是原文中两个连贯的语句，但是顺序交换一下 模型调用 bert模型测试
roberta模型测试
albert模型测试" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/9c9eb8a658ba2d435d0137b0c1183c0a/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-09-01T08:53:11+08:00" />
<meta property="article:modified_time" content="2020-09-01T08:53:11+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">roberta与albert</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h5><a id="roberta_0"></a>roberta</h5> 
<hr> 
<ol><li>简介<br> RoBERTA，是BERT模型的改进版，并获得了更好的自然语言处理效果，且其在GLUE、SQuAD、RACE等三个榜单上取得了SOTA效果</li><li>细节</li></ol> 
<ul><li> <p>训练数据集上，RoBERTa采用了160G的训练文本，而BERT仅使用16G的训练文本</p> </li><li> <p>模型评估上，模型主要基于三个基准来评估：1、GLUE通用语言理解模型；2、SQuAD斯坦福问题答疑数据集；3、RACE考试的重新理解</p> </li><li> <p>预训练任务上，RoBERTa使用的是动态mask，BERT采用静态mask,其含义如下：</p> <p>原来Bert对每一个序列随机选择15%的Tokens替换成[MASK]，为了消除与下游任务的不匹配，还对这15%的Tokens进行（1）80%的时间替换成[MASK]；（2）10%的时间不变；（3）10%的时间替换成其他词。但整个训练过程，这15%的Tokens一旦被选择就不再改变，也就是说从一开始随机选择了这15%的Tokens，之后的N个epoch里都不再改变了。这就叫做静态Masking。</p> <p>在此预训练任务中，取消了NSP任务，增加了FULL_SENTENCES机制：在该机制下，输入的不再是两个句子，而是用大段话填满指定的字节长度，如果句子跨越了文章就增加一个分割的token。</p> </li><li> <p>训练参数：RoBERTa模型增加了训练的batch_size，并将adam的0.999改成了0.98，增加了训练的step,最后使用的batch_size为8k，训练步数为500步。输入的token编码为BPE编码。</p> </li></ul> 
<h5><a id="albert_15"></a>albert</h5> 
<hr> 
<p>albert主要解决Bert参数过大、训练国漫的问题，其主要通过两个参数削减技术克服预训练模型扩展的障碍：</p> 
<ul><li>embedding参数因式分解：将两个大的词嵌入矩阵分解为两个小的矩阵，从而将隐藏层与词典的大小关系分割开来，两者不再直接关系，使得隐藏层的节点数扩展不再受到限制</li><li>跨层参数共享：避免参数随着网络的深度增加而增加</li><li>句间连贯性损失：正例与bert一样，两个连贯的句子；负例也是原文中两个连贯的语句，但是顺序交换一下</li></ul> 
<h5><a id="_22"></a>模型调用</h5> 
<hr> 
<ol><li> <p>bert模型测试</p> </li><li> <p>roberta模型测试</p> </li><li> <p>albert模型测试</p> </li></ol>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4dab2bb223a272a39c513464019896af/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">new一个对象</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/15bc3de02451dba588b894df808cd302/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Linux GUI自动化测试工具 -- LDTP</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>