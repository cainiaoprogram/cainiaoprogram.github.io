<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>人工智能入门 | K-means聚类算法的应用案例实战（含代码&#43;图示） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="人工智能入门 | K-means聚类算法的应用案例实战（含代码&#43;图示）" />
<meta property="og:description" content="前言： Hello大家好，我是小哥谈。 K-means算法是很典型的基于距离的聚类算法，采用距离作为相似性的评价指标，即认为两个对象的距离越近，其相似度就越大。该算法认为簇是由距离靠近的对象组成的，因此把得到紧凑且独立的簇作为最终目标。🌈 目录
🚀1.k-means算法优缺点
🚀2.算法思想
🚀3.解决的问题
🚀4.k-means原理介绍
🚀5.k-means算法实战
🚀1.k-means算法优缺点 优点：容易实现 缺点：可能收敛到局部最小值，在大规模数据集上收敛较慢.
使用数据类型： 数值型数据 🚀2.算法思想 k-means算法 实际上就是通过计算不同样本间的距离来判断他们的相 近关系的，相近的就会放到同一个类别中去。📚 🚀3.解决的问题 k-means算法属于无监督学习的一种聚类算法，其目的为：在不知数据所属类别及类别数量的前提下，依据数据自身所暗含的特点对数据 进行聚类。对于聚类过程中类别数量 k 的选取，需要一定的先验知识， 也可根据“类内间距小，类间间距大“（一种聚类算法的理想情况） 为目标进行实现。🍃
🚀4.k-means原理介绍 k-means算法以数据间的距离作为数据对象相似性度量的标准，因此选择计算数据间距离的计算方式对最后的聚类效果有显著的影响，常用计算距离的方式有：余弦距离、欧式距离、曼哈顿距离等。🌻
🚀5.k-means算法实战 实现效果如下： 代码（每一步都已经清楚的进行了说明）： from numpy import * import pandas as pd import matplotlib.pyplot as plt #距离度量函数 def calc_distance(vec1,vec2): return sqrt(sum(power(vec1-vec2,2))) #创建初始聚类中心 def creat_centroid(data,k): centroids = zeros((k,n)) centroids[0,0] = 2 centroids[0,1] = 10 centroids[1, 0] = 5 centroids[1, 1] = 8 centroids[2, 0] = 1 centroids[2, 1] = 2 return centroids # k-means聚类 def kMeans(data,k,dist=calc_distance,creat_center = creat_centroid): # 初始化cluster_assment，存储中间结果 #第一列存储索引，第二列存储距离 # 样本的个数 m = shape(data)[0] init = zeros((m,2)) cluster_assment = mat(init) # 初始化聚类中心矩阵 centroids = creat_centroid(data,k) for epoch in range(1): # 对数据集合中每个样本点进行计算 for i in range(m): min_dist = inf min_index = -1 # 对每个样本点到每个中心的距离进行计算 for j in range(k): dist_ij = calc_distance(centroids[j, :],data[i, :]) # 找到距离最近的中心的距离和索引 if dist_ij &lt; min_dist: min_dist = dist_ij min_index = j cluster_assment[i,:] = min_index,min_dist # 对所有节点聚类之后，重新更新中心 for i in range(k): # ." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/68d8075ddd2f1a12c57ed989a92996d6/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-09-08T21:34:14+08:00" />
<meta property="article:modified_time" content="2023-09-08T21:34:14+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">人工智能入门 | K-means聚类算法的应用案例实战（含代码&#43;图示）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><img alt="" height="435" src="https://images2.imgbox.com/6e/c6/9msV3Kj7_o.png" width="1200"></p> 
<div></div> 
<blockquote> 
 <span style="color:#be191c;"><strong>前言：</strong></span> 
 <span style="color:#4da8ee;"><strong>Hello大家好，我是小哥谈。</strong></span> 
 <span style="color:#0d0016;">K-means算法是很典型的基于距离的聚类算法，采用距离作为相似性的评价指标，即认为两个对象的距离越近，其相似度就越大。该算法认为簇是由距离靠近的对象组成的，因此把得到紧凑且独立的簇作为最终目标。</span>🌈 
</blockquote> 
<p id="main-toc"><strong>  <span style="color:#0d0016;">   目录</span></strong></p> 
<p id="%F0%9F%9A%801.k-means%E7%AE%97%E6%B3%95%E4%BC%98%E7%BC%BA%E7%82%B9-toc" style="margin-left:80px;"><a href="#%F0%9F%9A%801.k-means%E7%AE%97%E6%B3%95%E4%BC%98%E7%BC%BA%E7%82%B9" rel="nofollow">🚀1.k-means算法优缺点</a></p> 
<p id="%F0%9F%9A%802.%E7%AE%97%E6%B3%95%E6%80%9D%E6%83%B3-toc" style="margin-left:80px;"><a href="#%F0%9F%9A%802.%E7%AE%97%E6%B3%95%E6%80%9D%E6%83%B3" rel="nofollow">🚀2.算法思想</a></p> 
<p id="%F0%9F%9A%803.%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98-toc" style="margin-left:80px;"><a href="#%F0%9F%9A%803.%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98" rel="nofollow">🚀3.解决的问题</a></p> 
<p id="%F0%9F%9A%804.k-means%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D-toc" style="margin-left:80px;"><a href="#%F0%9F%9A%804.k-means%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D" rel="nofollow">🚀4.k-means原理介绍</a></p> 
<p id="%F0%9F%9A%805.k-means%E7%AE%97%E6%B3%95%E5%AE%9E%E6%88%98-toc" style="margin-left:80px;"><a href="#%F0%9F%9A%805.k-means%E7%AE%97%E6%B3%95%E5%AE%9E%E6%88%98" rel="nofollow">🚀5.k-means算法实战</a></p> 
<p class="img-center"><img alt="" height="130" src="https://images2.imgbox.com/6b/39/yv280oZI_o.gif" width="769"></p> 
<h4 id="%F0%9F%9A%801.k-means%E7%AE%97%E6%B3%95%E4%BC%98%E7%BC%BA%E7%82%B9"><span style="color:#0d0016;"><strong>🚀1.k-means算法优缺点 </strong></span></h4> 
<p><span style="color:#fe2c24;"><strong>优点：</strong></span><span style="color:#0d0016;">容易实现 </span></p> 
<p><span style="color:#fe2c24;"><strong>缺点：</strong></span><span style="color:#0d0016;">可能收敛到局部最小值，在大规模数据集上收敛较慢.</span></p> 
<div> 
 <span style="color:#fe2c24;"><strong>使用数据类型：</strong></span> 
 <span style="color:#0d0016;">数值型数据</span> 
 <hr> 
 <h4 id="%F0%9F%9A%802.%E7%AE%97%E6%B3%95%E6%80%9D%E6%83%B3"><span style="color:#0d0016;"><strong>🚀2.算法思想 </strong></span></h4> 
</div> 
<div> 
 <span style="color:#fe2c24;"><strong>k-means算法</strong></span> 
 <span style="color:#0d0016;">实际上就是通过计算不同样本间的距离来判断他们的相 近关系的，相近的就会放到同一个类别中去。</span>📚 
 <hr> 
 <h4 id="%F0%9F%9A%803.%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98"><span style="color:#0d0016;"><strong>🚀3.解决的问题 </strong></span></h4> 
 <p><span style="color:#fe2c24;"><strong>k-means算法</strong></span><span style="color:#0d0016;">属于</span><strong><span style="color:#1c7331;">无监督学习</span></strong><span style="color:#0d0016;">的一种聚类算法，其</span><span style="color:#fe2c24;"><strong>目的</strong></span><span style="color:#0d0016;">为：在不知数据所属类别及类别数量的前提下，依据数据自身所暗含的特点对数据 进行聚类。对于聚类过程中类别数量 k 的选取，需要一定的先验知识， 也可根据“</span><span style="color:#1c7331;"><strong>类内间距小，类间间距大</strong></span><span style="color:#0d0016;">“（一种聚类算法的理想情况） 为目标进行实现。</span>🍃</p> 
 <hr> 
 <h4 id="%F0%9F%9A%804.k-means%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D"><span style="color:#0d0016;"><strong>🚀4.k-means原理介绍 </strong></span></h4> 
 <p><span style="color:#fe2c24;"><strong>k-means算法</strong></span><span style="color:#0d0016;">以数据间的距离作为数据对象相似性度量的标准，因此选择计算数据间距离的计算方式对最后的聚类效果有显著的影响，常用计算距离的方式有：</span><span style="color:#511b78;"><strong>余弦距离</strong></span><span style="color:#0d0016;">、</span><span style="color:#511b78;"><strong>欧式距离</strong></span><span style="color:#0d0016;">、</span><span style="color:#511b78;"><strong>曼哈顿距离</strong></span><span style="color:#0d0016;">等。</span>🌻</p> 
 <hr> 
 <h4 id="%F0%9F%9A%805.k-means%E7%AE%97%E6%B3%95%E5%AE%9E%E6%88%98"><span style="color:#0d0016;"><strong>🚀5.k-means算法实战</strong></span></h4> 
</div> 
<div> 
 <span style="color:#0d0016;"><span style="background-color:#ff9900;">实现效果如下：</span></span> 
</div> 
<div></div> 
<div> 
 <p class="img-center"><img alt="" height="422" src="https://images2.imgbox.com/84/09/2gI68lkS_o.png" width="655"></p> 
</div> 
<div> 
 <span style="color:#0d0016;"><span style="background-color:#ff9900;">代码（每一步都已经清楚的进行了说明）：</span></span> 
</div> 
<div> 
 <pre><code class="language-python">from numpy import *
import pandas as pd
import matplotlib.pyplot as plt
#距离度量函数
def calc_distance(vec1,vec2):
    return sqrt(sum(power(vec1-vec2,2)))

#创建初始聚类中心
def creat_centroid(data,k):
    centroids = zeros((k,n))
    centroids[0,0] = 2
    centroids[0,1] = 10
    centroids[1, 0] = 5
    centroids[1, 1] = 8
    centroids[2, 0] = 1
    centroids[2, 1] = 2
    return  centroids

# k-means聚类
def kMeans(data,k,dist=calc_distance,creat_center =  creat_centroid):
    # 初始化cluster_assment，存储中间结果
    #第一列存储索引，第二列存储距离
    # 样本的个数
    m = shape(data)[0]
    init = zeros((m,2))
    cluster_assment = mat(init)

    # 初始化聚类中心矩阵
    centroids = creat_centroid(data,k)

    for epoch in range(1):
        # 对数据集合中每个样本点进行计算
        for i in range(m):
            min_dist = inf
            min_index = -1
            # 对每个样本点到每个中心的距离进行计算
            for j in range(k):
                dist_ij = calc_distance(centroids[j, :],data[i, :])
                # 找到距离最近的中心的距离和索引
                if dist_ij &lt; min_dist:
                    min_dist =  dist_ij
                    min_index = j
                    cluster_assment[i,:] = min_index,min_dist
        # 对所有节点聚类之后，重新更新中心
        for i in range(k):
            # .A把矩阵转成数组
            pts_in_cluster = data[nonzero(cluster_assment[:,0].A == i)[0]]
            centroids[i,:] = mean(pts_in_cluster,axis=0)
    return centroids,cluster_assment


if __name__ == '__main__':
    # 创建数据集
    data = array([[2,10],[2,5],[8,4],[5,8],
                  [7,5] ,[6,4],[1,2],[4,9]])
    k = 3 # k为聚类个数
    n = 2 # n为特征个数
    centroids,cluster_assment = kMeans(data,k,dist = calc_distance,creat_center=creat_centroid)
    predict_label = cluster_assment[:,0]
    data_and_pred = column_stack((data,predict_label))

    # df是原样数据样本和预测出来的类别
    df = pd.DataFrame(data_and_pred,columns=['data1','data2','pred'])
    df0 = df[df.pred == 0].values
    df1 = df[df.pred == 1].values
    df2 = df[df.pred == 2].values

    #画图
    plt.scatter(df0[:,0],df0[:,1],c ='turquoise',marker = 'o',label = 'label0')
    plt.scatter(df1[:, 0], df1[:, 1], c='green', marker='*', label='label1')
    plt.scatter(df2[:, 0], df2[:, 1], c='blue', marker='+', label='label2')
    plt.scatter(centroids[:,0].tolist(),centroids[:,1].tolist(),c='red')
    plt.legend(loc = 2)
    plt.show()</code></pre> 
 <p><span style="color:#0d0016;"><span style="background-color:#ff9900;">具体实现效果：</span></span></p> 
 <p class="img-center"><img alt="" height="436" src="https://images2.imgbox.com/16/09/ohxMOIk9_o.png" width="697"></p> 
 <p><span style="color:#0d0016;">小伙伴们，可以根据需求添加数据集，本文只是简单添加了几组数据集做一下演示。</span>🌟🌟🌟</p> 
 <hr> 
 <p class="img-center"><img alt="" height="158" src="https://images2.imgbox.com/3e/25/q6h3vYhq_o.gif" width="433"></p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4a9decabb9dc2b25b27b05741c2d7540/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">人工智能入门 | 分类算法-KNN(原理&#43;代码&#43;结果)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e9debc994be0ae76d9e6ff3bedd63f27/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Flask路由的参数传递</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>