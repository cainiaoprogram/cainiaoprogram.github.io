<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>人工智能入门 | 分类算法-KNN(原理&#43;代码&#43;结果) - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="人工智能入门 | 分类算法-KNN(原理&#43;代码&#43;结果)" />
<meta property="og:description" content="前言：Hello大家好，我是小哥谈。KNN，即K最邻近算法，是数据挖掘分类技术中比较简单的方法之一，简单来说，就是根据“最邻近”这一特征对样本进行分类。🌈
目录
🚀1.K-means和KNN区别
🚀2.KNN的算法思想
🚀3.算法步骤
🚀4.KNN算法的优缺点
🚀5.数据集
🚀6.代码实现
🚀7.结果
🚀1.K-means和KNN区别 K-means是一种比较经典的聚类算法，本质上是无监督学习，而KNN是分类（或回归）算法，是监督学习。✅
K-means算法的训练过程需要反复迭代的操作（寻找新的质心），但是KNN不需要。K-means中的 K 代表的是簇中心，KNN的 K 代表的是选择与新测试样本距离最近的前K个训练样本数。🌳
🚀2.KNN的算法思想 KNN（k-NearestNeighbor）又被称为近邻算法，它的核心思想是：物以类聚，人以群分。✅
假设一个未知样本数据x需要归类，总共有ABC三个类别，那么离x距离最近的有k个邻居，这k个邻居里有k1个邻居属于A类，k2个邻居属于B类，k3个邻居属于C类，如果k1&gt;k2&gt;k3，那么x就属于A类，也就是说x的类别完全由邻居来推断出来。🌳
🚀3.算法步骤 🍀（1）计算测试对象到训练集中每个对象的距离
🍀（2）按照距离的远近排序
🍀（3）选取与当前测试对象最近的K的训练对象，作为该测试对象的邻居。
🍀（4）统计这K个邻居的类别概率
🍀（5）K个邻居里频率最高的类别，即为测试对象的类别。
🚀4.KNN算法的优缺点 优点：非常简单的分类算法没有之一，人性化，易于理解，易于实现，适合处理多分类问题，比如推荐用户。
缺点：属于懒惰算法，时间复杂度较高，因为需要计算未知样本到所有已知样本的距离，样本平衡度依赖高，当出现极端情况样本不平衡时，分类绝对会出现偏差。可解释性差，无法给出类似决策树那样的规则。向量的维度越高，欧式距离的区分能力就越弱。
🚀5.数据集 名称：Prostate_Cancer
提取链接：https://pan.baidu.com/s/1xv8r54qaLpH8RiMRkcrzyA 提取码：66hg 数据集展示：
🚀6.代码实现 from sklearn.neighbors import KNeighborsClassifier import numpy as np import random import pandas as pd def knn(): K = 8 data=pd.read_csv(r&#34;C:\Users\Lenovo\Desktop\course\Prostate_Cancer.csv&#34;) n = len(data) // 3 test_set = data[0:n] train_set = data[n:] train_set = np.array(train_set) test_set = np." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/4a9decabb9dc2b25b27b05741c2d7540/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-09-08T21:33:09+08:00" />
<meta property="article:modified_time" content="2023-09-08T21:33:09+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">人工智能入门 | 分类算法-KNN(原理&#43;代码&#43;结果)</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p class="img-center"><img alt="" height="435" src="https://images2.imgbox.com/a9/a3/qib2I26x_o.png" width="1200"></p> 
<blockquote> 
 <p style="margin-left:.0001pt;text-align:justify;"><strong><span style="color:#be191c;">前言：</span><span style="color:#4da8ee;">Hello大家好，我是小哥谈。</span></strong><span style="color:#0d0016;">KNN，即K最邻近算法，是数据挖掘分类技术中比较简单的方法之一，简单来说，就是根据“最邻近”这一特征对样本进行分类。</span>🌈</p> 
</blockquote> 
<p id="main-toc"><strong>  <span style="color:#0d0016;">   目录</span></strong></p> 
<p id="%F0%9F%9A%801.K-means%E5%92%8CKNN%E5%8C%BA%E5%88%AB-toc" style="margin-left:80px;"><a href="#%F0%9F%9A%801.K-means%E5%92%8CKNN%E5%8C%BA%E5%88%AB" rel="nofollow">🚀1.K-means和KNN区别</a></p> 
<p id="%F0%9F%9A%802.KNN%E7%9A%84%E7%AE%97%E6%B3%95%E6%80%9D%E6%83%B3-toc" style="margin-left:80px;"><a href="#%F0%9F%9A%802.KNN%E7%9A%84%E7%AE%97%E6%B3%95%E6%80%9D%E6%83%B3" rel="nofollow">🚀2.KNN的算法思想</a></p> 
<p id="%F0%9F%9A%803.%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4-toc" style="margin-left:80px;"><a href="#%F0%9F%9A%803.%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4" rel="nofollow">🚀3.算法步骤</a></p> 
<p id="%F0%9F%9A%804.KNN%E7%AE%97%E6%B3%95%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9-toc" style="margin-left:80px;"><a href="#%F0%9F%9A%804.KNN%E7%AE%97%E6%B3%95%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9" rel="nofollow">🚀4.KNN算法的优缺点</a></p> 
<p id="%F0%9F%9A%805.%E6%95%B0%E6%8D%AE%E9%9B%86-toc" style="margin-left:80px;"><a href="#%F0%9F%9A%805.%E6%95%B0%E6%8D%AE%E9%9B%86" rel="nofollow">🚀5.数据集</a></p> 
<p id="%F0%9F%9A%806.%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-toc" style="margin-left:80px;"><a href="#%F0%9F%9A%806.%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0" rel="nofollow">🚀6.代码实现</a></p> 
<p id="%F0%9F%9A%807.%E7%BB%93%E6%9E%9C-toc" style="margin-left:80px;"><a href="#%F0%9F%9A%807.%E7%BB%93%E6%9E%9C" rel="nofollow">🚀7.结果</a></p> 
<p class="img-center"><img alt="" height="119" src="https://images2.imgbox.com/00/c6/y7Ii3dz8_o.gif" width="704"></p> 
<h4 id="%F0%9F%9A%801.K-means%E5%92%8CKNN%E5%8C%BA%E5%88%AB" style="text-align:justify;"><span style="color:#0d0016;"><strong>🚀1.K-means和KNN区别</strong></span></h4> 
<p style="text-align:justify;"><span style="color:#fe2c24;"><strong>K-means</strong></span><span style="color:#0d0016;">是一种比较经典的聚类算法，本质上是</span><span style="color:#fe2c24;"><strong>无监督学习</strong></span><span style="color:#0d0016;">，而</span><span style="color:#fe2c24;"><strong>KNN</strong></span><span style="color:#0d0016;">是分类（或回归）算法，是</span><span style="color:#fe2c24;"><strong>监督学习</strong></span><span style="color:#0d0016;">。</span>✅</p> 
<p style="text-align:justify;"><span style="color:#0d0016;">K-means<span style="background-color:#ffffff;">算法的训练过程需要反复迭代的操作（寻找新的质心），但是KNN不需要。</span>K-means<span style="background-color:#ffffff;">中的 K 代表的是簇中心，KNN的 K 代表的是选择与新测试样本距离最近的前K个训练样本数。</span></span>🌳</p> 
<hr> 
<h4 id="%F0%9F%9A%802.KNN%E7%9A%84%E7%AE%97%E6%B3%95%E6%80%9D%E6%83%B3"><span style="color:#0d0016;"><strong>🚀<span style="background-color:#ffffff;">2.KNN的算法思想</span></strong></span></h4> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="color:#fe2c24;"><strong>KNN（k-NearestNeighbor）</strong></span><span style="color:#0d0016;">又被称为近邻算法，它的</span><span style="color:#fe2c24;"><strong>核心思想</strong></span><span style="color:#0d0016;">是：</span><strong><span style="color:#956fe7;">物以类聚，人以群分</span></strong><span style="color:#0d0016;">。</span>✅</p> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="color:#0d0016;">假设一个未知样本数据x需要归类，总共有ABC三个类别，那么离x距离最近的有k个邻居，这k个邻居里有k1个邻居属于A类，k2个邻居属于B类，k3个邻居属于C类，如果k1&gt;k2&gt;k3，那么x就属于A类，也就是说x的类别完全由邻居来推断出来。</span>🌳</p> 
<hr> 
<h4 id="%F0%9F%9A%803.%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4" style="margin-left:.0001pt;text-align:justify;"><span style="color:#0d0016;"><strong>🚀3.算法步骤</strong></span></h4> 
<p style="margin-left:.0001pt;text-align:justify;">🍀<span style="color:#ff9900;"><strong>（1）计算测试对象到训练集中每个对象的距离</strong></span></p> 
<p style="margin-left:.0001pt;text-align:justify;">🍀<span style="color:#956fe7;"><strong>（2）按照距离的远近排序</strong></span></p> 
<p style="margin-left:.0001pt;text-align:justify;">🍀<span style="color:#4da8ee;"><strong>（3）选取与当前测试对象最近的K的训练对象，作为该测试对象的邻居。</strong></span></p> 
<p style="margin-left:.0001pt;text-align:justify;">🍀<span style="color:#b95514;"><strong>（4）统计这K个邻居的类别概率</strong></span></p> 
<p style="margin-left:.0001pt;text-align:justify;">🍀<span style="color:#1c7331;"><strong>（5）K个邻居里频率最高的类别，即为测试对象的类别。</strong></span></p> 
<hr> 
<h4 id="%F0%9F%9A%804.KNN%E7%AE%97%E6%B3%95%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9" style="margin-left:.0001pt;text-align:justify;"><span style="color:#0d0016;"><strong>🚀4.KNN算法的优缺点</strong></span></h4> 
<p style="margin-left:0;text-align:left;"><strong><span style="color:#fe2c24;"><span style="background-color:#ffffff;">优点：</span></span></strong><span style="color:#0d0016;"><span style="background-color:#ffffff;"><span style="background-color:#ffffff;">非常简单的分类算法没有之一，人性化，易于理解，易于实现</span><span style="background-color:#ffffff;">，</span><span style="background-color:#ffffff;">适合处理多分类问题，比如推荐用户</span><span style="background-color:#ffffff;">。</span></span></span></p> 
<p style="margin-left:0;text-align:left;"><span style="color:#fe2c24;"><strong><span style="background-color:#ffffff;">缺点：</span></strong></span><span style="color:#0d0016;">属于懒惰算法，时间复杂度较高，因为需要计算未知样本到所有已知样本的距离，样本平衡度依赖高，当出现极端情况样本不平衡时，分类绝对会出现偏差。可解释性差，无法给出类似决策树那样的规则。向量的维度越高，欧式距离的区分能力就越弱。</span></p> 
<hr> 
<h4 id="%F0%9F%9A%805.%E6%95%B0%E6%8D%AE%E9%9B%86" style="margin-left:0px;text-align:left;"><span style="color:#0d0016;"><strong>🚀5.数据集</strong></span></h4> 
<blockquote> 
 <p style="margin-left:0;text-align:left;"><span style="color:#be191c;"><strong>名称：</strong></span><span style="color:#0d0016;">Prostate_Cancer</span></p> 
 <p style="margin-left:0;text-align:left;"><span style="color:#be191c;"><strong>提取链接：</strong></span><span style="color:#0d0016;">https://pan.baidu.com/s/1xv8r54qaLpH8RiMRkcrzyA </span><br><span style="color:#be191c;"><strong>提取码：</strong></span><span style="color:#0d0016;">66hg </span></p> 
</blockquote> 
<p style="margin-left:0;text-align:left;"><span style="color:#0d0016;"><span style="background-color:#ff9900;">数据集展示：</span></span></p> 
<p style="margin-left:0px;text-align:center;"><img alt="" height="470" src="https://images2.imgbox.com/62/57/AkixSRxp_o.png" width="752"></p> 
<hr> 
<h4 id="%F0%9F%9A%806.%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span style="color:#0d0016;"><strong>🚀6.代码实现</strong></span></h4> 
<pre><code class="language-python">from sklearn.neighbors import KNeighborsClassifier
import numpy as np
import random
import pandas as pd

def knn():
    K = 8
    data=pd.read_csv(r"C:\Users\Lenovo\Desktop\course\Prostate_Cancer.csv")
    n = len(data) // 3
    test_set = data[0:n]
    train_set = data[n:]
    train_set = np.array(train_set)
    test_set = np.array(test_set)
    A = [i for i in range(0, len(train_set))]
    B = [i for i in range(2, 10)]
    C = [i for i in range(n)]
    D = [1]
    x_train = train_set[A]
    x_train = x_train[:, B]
    y_train = train_set[A]
    y_train = y_train[:, D]
    x_test = test_set[C]
    x_test = x_test[:, B]
    y_test = test_set[C]
    y_test = y_test[:, D]
    # 训练模型
    model = KNeighborsClassifier(n_neighbors=K)
    model.fit(x_train, y_train)
    score = model.score(x_test, y_test)
    print("准确率为:", score)

if __name__=='__main__':
    knn()
</code></pre> 
<p style="margin-left:0px;text-align:center;"><img alt="" src="https://images2.imgbox.com/f4/c1/EIq9zJVe_o.png"></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/49/63/zSNOHIW1_o.png"></p> 
<hr> 
<h4 id="%F0%9F%9A%807.%E7%BB%93%E6%9E%9C"><span style="color:#0d0016;"><strong>🚀7.结果</strong></span></h4> 
<p><span style="color:#0d0016;">当</span><span style="color:#fe2c24;"><strong>k=8</strong></span><span style="color:#0d0016;">时候，结果为<strong>0.7575757575757576</strong></span></p> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="color:#0d0016;">当</span><span style="color:#fe2c24;"><strong>k=5</strong></span><span style="color:#0d0016;">时候，结果为<strong>0.8181818181818182</strong></span></p> 
<blockquote> 
 <p style="margin-left:.0001pt;text-align:justify;"><span style="color:#be191c;"><strong>总结：</strong></span>♨️♨️♨️</p> 
 <p style="margin-left:.0001pt;text-align:justify;"><span style="color:#0d0016;">KNN是一种快速高效的分类算法，其逻辑简单，易于理解。但当训练数据集很大时，需要大量的存储空间，而且需要计算训练样本和测试样本的距离，计算量较大。个人认为，这是由于KNN是一种懒惰的学习法，简单的认为“</span><span style="color:#1c7331;"><strong>物以类聚，人以群分</strong></span><span style="color:#0d0016;">”，但实际上，无论是人际交往还是数据事实，可能并不完全符合这个逻辑</span></p> 
</blockquote> 
<hr> 
<p class="img-center"><img alt="" height="130" src="https://images2.imgbox.com/79/89/Fhf3Tb6c_o.gif" width="357"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a1063e8328f58141be3e2f1f6daa6bda/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【Opencv入门到项目实战】（十一）：harris角点检测|SIFT|特征匹配</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/68d8075ddd2f1a12c57ed989a92996d6/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">人工智能入门 | K-means聚类算法的应用案例实战（含代码&#43;图示）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>