<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>从BERT到ROBERTA：预训练语言模型的优化之路 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="从BERT到ROBERTA：预训练语言模型的优化之路" />
<meta property="og:description" content="❤️觉得内容不错的话，欢迎点赞收藏加关注😊😊😊，后续会继续输入更多优质内容❤️ 👉有问题欢迎大家加关注私戳或者评论（包括但不限于NLP算法相关，linux学习相关，读研读博相关......）👈 （封面图由ERNIE-ViLG AI 作画大模型生成） 从BERT到ROBERTA：预训练语言模型的优化之路 自从深度学习在自然语言处理领域得到广泛应用以来，Transformer模型一直是自然语言处理的重要研究方向。2017年，谷歌推出了Transformer模型的创新之作——BERT（Bidirectional Encoder Representations from Transformers），在自然语言处理领域引起了极大的关注。2020年，Facebook AI Research推出了ROBERTA模型（Robustly Optimized BERT Pretraining Approach），它在BERT的基础上做了一些改进，取得了更好的效果。本文将详细介绍ROBERTA模型的原理、优势和劣势，并通过案例和代码的方式帮助读者深入理解。
1. ROBERTA模型的原理 1.1 BERT模型简介 BERT是一种预训练的语言模型，它利用了Transformer编码器的双向特性，能够将文本转换成高维向量表示，进而实现文本分类、命名实体识别、问答等自然语言处理任务。BERT的核心思想是使用Transformer编码器对文本进行预训练，然后在具体的任务上进行微调。
BERT的编码器由多个Transformer块组成，每个块包含多头自注意力机制（Multi-Head Self-Attention）和全连接层（Feed-Forward）。其中，自注意力机制可以实现上下文相关的编码，全连接层则将编码后的向量映射到新的向量空间中。
BERT的预训练任务有两个：掩码语言模型（Masked Language Model，MLM）和下一句预测（Next Sentence Prediction，NSP）。其中，掩码语言模型是指在输入的文本中随机选择一些词，然后用“[MASK]”代替这些词，并让模型预测它们的正确性；下一句预测是指判断两个句子是否是连续的。
1.2 ROBERTA模型的改进 ROBERTA模型是在BERT的基础上做了一些改进，取得了更好的效果。具体来说，ROBERTA模型主要有以下三个改进：
训练数据的改进：ROBERTA模型使用了更多、更大的文本数据进行训练，包括了互联网上的网页、论坛、书籍、新闻等。此外，它还使用了更长的文本片段进行训练，从而更好地捕捉上下文相关性。
训练方法的改进：ROBERTA模型采用了更长的训练时间、更小的批次和更高的学习率，从而提高了模型的鲁棒性和性能。
掩码语言模型的改进：ROBERTA模型不再使用原始BERT中的掩码语言模型，而是采用了更严格的掩码策略。具体来说，ROBERTA模型将输入的文本中所有的词都替换成“[MASK]”，然后让模型预测这些词的正确性，从而更好地利用了训练数据中的信息。
与BERT相比，ROBERTA模型的另一个重要改进是在预训练过程中采用了更多的参数和更深的网络结构，进一步提高了模型的性能。
1.3 ROBERTA模型的结构 ROBERTA模型的结构与BERT基本一致，由多个Transformer块组成。每个块包含多头自注意力机制、全连接层和残差连接，其中多头自注意力机制可以实现上下文相关的编码，全连接层则将编码后的向量映射到新的向量空间中。具体来说，ROBERTA模型的结构可以表示为：
h l = T r a n s f o r m e r B l o c k ( h l − 1 ) h_l = TransformerBlock(h_{l-1}) hl​=TransformerBlock(hl−1​)
其中， h l h_l hl​表示第 l l l层的输出向量， h l − 1 h_{l-1} hl−1​表示第 l − 1 l-1 l−1层的输出向量， T r a n s f o r m e r B l o c k TransformerBlock TransformerBlock表示Transformer块。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/e74e23e89931e5c713226b9cf74e2a1d/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-03-14T13:57:54+08:00" />
<meta property="article:modified_time" content="2023-03-14T13:57:54+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">从BERT到ROBERTA：预训练语言模型的优化之路</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <hr> 
<center> 
 <strong>❤️觉得内容不错的话，欢迎点赞收藏加关注😊😊😊，后续会继续输入更多优质内容❤️</strong> 
</center> 
<br> 
<center> 
 <strong>👉有问题欢迎大家加关注私戳或者评论（包括但不限于NLP算法相关，linux学习相关，读研读博相关......）👈</strong> 
</center> 
<hr> 
<p><img src="https://images2.imgbox.com/55/01/ojugZytY_o.jpg" alt="ROBERTA"></p> 
<center> 
 <font size="1">（封面图由ERNIE-ViLG AI 作画大模型生成）</font> 
</center> 
<h3><a id="BERTROBERTA_11"></a>从BERT到ROBERTA：预训练语言模型的优化之路</h3> 
<p>自从深度学习在自然语言处理领域得到广泛应用以来，Transformer模型一直是自然语言处理的重要研究方向。2017年，谷歌推出了Transformer模型的创新之作——BERT（Bidirectional Encoder Representations from Transformers），在自然语言处理领域引起了极大的关注。2020年，Facebook AI Research推出了ROBERTA模型（Robustly Optimized BERT Pretraining Approach），它在BERT的基础上做了一些改进，取得了更好的效果。本文将详细介绍ROBERTA模型的原理、优势和劣势，并通过案例和代码的方式帮助读者深入理解。</p> 
<h3><a id="1_ROBERTA_15"></a>1. ROBERTA模型的原理</h3> 
<h4><a id="11_BERT_16"></a>1.1 BERT模型简介</h4> 
<p>BERT是一种预训练的语言模型，它利用了Transformer编码器的双向特性，能够将文本转换成高维向量表示，进而实现文本分类、命名实体识别、问答等自然语言处理任务。BERT的核心思想是使用Transformer编码器对文本进行预训练，然后在具体的任务上进行微调。</p> 
<p>BERT的编码器由多个Transformer块组成，每个块包含多头自注意力机制（Multi-Head Self-Attention）和全连接层（Feed-Forward）。其中，自注意力机制可以实现上下文相关的编码，全连接层则将编码后的向量映射到新的向量空间中。</p> 
<p>BERT的预训练任务有两个：掩码语言模型（Masked Language Model，MLM）和下一句预测（Next Sentence Prediction，NSP）。其中，掩码语言模型是指在输入的文本中随机选择一些词，然后用“[MASK]”代替这些词，并让模型预测它们的正确性；下一句预测是指判断两个句子是否是连续的。</p> 
<h4><a id="12_ROBERTA_23"></a>1.2 ROBERTA模型的改进</h4> 
<p>ROBERTA模型是在BERT的基础上做了一些改进，取得了更好的效果。具体来说，ROBERTA模型主要有以下三个改进：</p> 
<ul><li> <p>训练数据的改进：ROBERTA模型使用了更多、更大的文本数据进行训练，包括了互联网上的网页、论坛、书籍、新闻等。此外，它还使用了更长的文本片段进行训练，从而更好地捕捉上下文相关性。</p> </li><li> <p>训练方法的改进：ROBERTA模型采用了更长的训练时间、更小的批次和更高的学习率，从而提高了模型的鲁棒性和性能。</p> </li><li> <p>掩码语言模型的改进：ROBERTA模型不再使用原始BERT中的掩码语言模型，而是采用了更严格的掩码策略。具体来说，ROBERTA模型将输入的文本中所有的词都替换成“[MASK]”，然后让模型预测这些词的正确性，从而更好地利用了训练数据中的信息。</p> </li><li> <p>与BERT相比，ROBERTA模型的另一个重要改进是在预训练过程中采用了更多的参数和更深的网络结构，进一步提高了模型的性能。</p> </li></ul> 
<h4><a id="13_ROBERTA_34"></a>1.3 ROBERTA模型的结构</h4> 
<p>ROBERTA模型的结构与BERT基本一致，由多个Transformer块组成。每个块包含多头自注意力机制、全连接层和残差连接，其中多头自注意力机制可以实现上下文相关的编码，全连接层则将编码后的向量映射到新的向量空间中。具体来说，ROBERTA模型的结构可以表示为：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           h 
          
         
           l 
          
         
        
          = 
         
        
          T 
         
        
          r 
         
        
          a 
         
        
          n 
         
        
          s 
         
        
          f 
         
        
          o 
         
        
          r 
         
        
          m 
         
        
          e 
         
        
          r 
         
        
          B 
         
        
          l 
         
        
          o 
         
        
          c 
         
        
          k 
         
        
          ( 
         
         
         
           h 
          
          
          
            l 
           
          
            − 
           
          
            1 
           
          
         
        
          ) 
         
        
       
         h_l = TransformerBlock(h_{l-1}) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0197em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.1389em;">T</span><span class="mord mathnormal" style="margin-right: 0.0278em;">r</span><span class="mord mathnormal">an</span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="mord mathnormal" style="margin-right: 0.0278em;">or</span><span class="mord mathnormal">m</span><span class="mord mathnormal" style="margin-right: 0.0278em;">er</span><span class="mord mathnormal" style="margin-right: 0.0197em;">Bl</span><span class="mord mathnormal">oc</span><span class="mord mathnormal" style="margin-right: 0.0315em;">k</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0197em;">l</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></span></p> 
<p>其中，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          h 
         
        
          l 
         
        
       
      
        h_l 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0197em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>表示第<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         l 
        
       
      
        l 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal" style="margin-right: 0.0197em;">l</span></span></span></span></span>层的输出向量，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          h 
         
         
         
           l 
          
         
           − 
          
         
           1 
          
         
        
       
      
        h_{l-1} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.9028em; vertical-align: -0.2083em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0197em;">l</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em;"><span class=""></span></span></span></span></span></span></span></span></span></span>表示第<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         l 
        
       
         − 
        
       
         1 
        
       
      
        l-1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7778em; vertical-align: -0.0833em;"></span><span class="mord mathnormal" style="margin-right: 0.0197em;">l</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">1</span></span></span></span></span>层的输出向量，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         T 
        
       
         r 
        
       
         a 
        
       
         n 
        
       
         s 
        
       
         f 
        
       
         o 
        
       
         r 
        
       
         m 
        
       
         e 
        
       
         r 
        
       
         B 
        
       
         l 
        
       
         o 
        
       
         c 
        
       
         k 
        
       
      
        TransformerBlock 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.1389em;">T</span><span class="mord mathnormal" style="margin-right: 0.0278em;">r</span><span class="mord mathnormal">an</span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="mord mathnormal" style="margin-right: 0.0278em;">or</span><span class="mord mathnormal">m</span><span class="mord mathnormal" style="margin-right: 0.0278em;">er</span><span class="mord mathnormal" style="margin-right: 0.0197em;">Bl</span><span class="mord mathnormal">oc</span><span class="mord mathnormal" style="margin-right: 0.0315em;">k</span></span></span></span></span>表示Transformer块。</p> 
<p>ROBERTA模型的输入向量是由标记嵌入（Token Embedding）、位置嵌入（Position Embedding）和段嵌入（Segment Embedding）三部分组成的。其中，标记嵌入将输入的文本转换成对应的向量表示，位置嵌入表示输入文本中每个词的位置信息，段嵌入表示输入文本中每个句子的边界信息。</p> 
<h3><a id="2_ROBERTA_43"></a>2. ROBERTA模型的优势和劣势</h3> 
<h4><a id="21__44"></a>2.1 优势</h4> 
<p>ROBERTA模型相比于BERT有以下几个优势：</p> 
<ul><li> <p>更好的性能：ROBERTA模型在多个自然语言处理任务中都取得了更好的性能，例如GLUE（General Language Understanding Evaluation）任务集、SuperGLUE任务集等。</p> </li><li> <p>更大的训练数据：ROBERTA模型使用了更多、更大的文本数据进行训练，包括了互联网上的网页、论坛、书籍、新闻等。这使得模型更好地捕捉了自然语言中的上下文相关性，从而提高了模型的性能。</p> </li><li> <p>更严格的掩码策略：ROBERTA模型采用了更严格的掩码策略，将输入文本中所有的词都替换成“[MASK]”，从而更好地利用了训练数据中的信息。</p> </li><li> <p>更深的网络结构：ROBERTA模型采用了更深的网络结构，进一步提高了模型的性能。</p> </li></ul> 
<h4><a id="22__55"></a>2.2 劣势</h4> 
<p>虽然ROBERTA模型在自然语言处理任务中取得了很好的性能，但它仍然存在一些劣势：</p> 
<ul><li> <p>训练时间更长：由于ROBERTA模型使用了更多、更大的文本数据进行训练，并且采用了更深的网络结构，因此相比于BERT模型，ROBERTA模型的训练时间更长。</p> </li><li> <p>模型大小更大：由于ROBERTA模型使用了更多的训练数据和更深的网络结构，因此相比于BERT模型，ROBERTA模型的模型大小更大。</p> </li><li> <p>训练数据的选择和预处理对模型性能的影响更大：由于ROBERTA模型使用了更多、更大的文本数据进行训练，因此训练数据的选择和预处理对模型性能的影响更大。</p> </li></ul> 
<h3><a id="3_ROBERTA_64"></a>3. ROBERTA模型的应用案例</h3> 
<p>在这个案例中，我们将使用ROBERTA模型来对IMDB电影评论进行情感分析，即判断该评论是积极的还是消极的。</p> 
<p>首先，我们需要准备数据。我们将使用Python中的torchtext库来加载IMDB数据集，并将其分为训练集和测试集。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torchtext
<span class="token keyword">from</span> torchtext<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> IMDB
<span class="token keyword">from</span> torchtext<span class="token punctuation">.</span>data <span class="token keyword">import</span> Field<span class="token punctuation">,</span> LabelField<span class="token punctuation">,</span> TabularDataset<span class="token punctuation">,</span> BucketIterator

<span class="token comment"># 定义Field</span>
TEXT <span class="token operator">=</span> Field<span class="token punctuation">(</span>tokenize<span class="token operator">=</span><span class="token string">'spacy'</span><span class="token punctuation">,</span> tokenizer_language<span class="token operator">=</span><span class="token string">'en_core_web_sm'</span><span class="token punctuation">,</span> include_lengths<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
LABEL <span class="token operator">=</span> LabelField<span class="token punctuation">(</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>

<span class="token comment"># 加载IMDB数据集</span>
train_data<span class="token punctuation">,</span> test_data <span class="token operator">=</span> IMDB<span class="token punctuation">.</span>splits<span class="token punctuation">(</span>TEXT<span class="token punctuation">,</span> LABEL<span class="token punctuation">)</span>

<span class="token comment"># 构建词汇表</span>
TEXT<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> max_size<span class="token operator">=</span><span class="token number">25000</span><span class="token punctuation">,</span> vectors<span class="token operator">=</span><span class="token string">"glove.6B.100d"</span><span class="token punctuation">,</span> unk_init<span class="token operator">=</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">.</span>normal_<span class="token punctuation">)</span>
LABEL<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">)</span>

<span class="token comment"># 定义迭代器</span>
BATCH_SIZE <span class="token operator">=</span> <span class="token number">64</span>
device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">'cpu'</span><span class="token punctuation">)</span>
train_iterator<span class="token punctuation">,</span> test_iterator <span class="token operator">=</span> BucketIterator<span class="token punctuation">.</span>splits<span class="token punctuation">(</span><span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> test_data<span class="token punctuation">)</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span>BATCH_SIZE<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>
</code></pre> 
<p>接下来，我们可以使用Hugging Face的transformers库来加载ROBERTA模型，并使用PyTorch构建分类器。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> RobertaTokenizer<span class="token punctuation">,</span> RobertaForSequenceClassification

<span class="token comment"># 加载预训练模型和tokenizer</span>
tokenizer <span class="token operator">=</span> RobertaTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'roberta-base'</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> RobertaForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'roberta-base'</span><span class="token punctuation">,</span> num_labels<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token comment"># 将模型移动到GPU上（如果可用）</span>
model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
</code></pre> 
<p>在训练模型之前，我们需要定义训练过程中所使用的一些超参数，并编写训练函数和测试函数。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim

<span class="token comment"># 定义损失函数和优化器</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCEWithLogitsLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">2e-5</span><span class="token punctuation">)</span>

<span class="token comment"># 训练函数</span>
<span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> iterator<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span><span class="token punctuation">:</span>
    epoch_loss <span class="token operator">=</span> <span class="token number">0</span>
    epoch_acc <span class="token operator">=</span> <span class="token number">0</span>
    
    model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token keyword">for</span> batch <span class="token keyword">in</span> iterator<span class="token punctuation">:</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        text<span class="token punctuation">,</span> text_lengths <span class="token operator">=</span> batch<span class="token punctuation">.</span>text
        text <span class="token operator">=</span> text<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        text_lengths <span class="token operator">=</span> text_lengths<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        labels <span class="token operator">=</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        
        <span class="token comment"># 将文本输入模型中</span>
        outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>text<span class="token punctuation">,</span> attention_mask<span class="token operator">=</span><span class="token punctuation">(</span>text <span class="token operator">!=</span> tokenizer<span class="token punctuation">.</span>pad_token_id<span class="token punctuation">)</span><span class="token punctuation">,</span> labels<span class="token operator">=</span>labels<span class="token punctuation">)</span>
        
        <span class="token comment"># 计算损失和准确率</span>
        loss <span class="token operator">=</span> outputs<span class="token punctuation">.</span>loss
        acc <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>outputs<span class="token punctuation">.</span>logits <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> labels<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        <span class="token comment"># 反向传播和优化</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        epoch_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
        epoch_acc <span class="token operator">+=</span> acc<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
    <span class="token keyword">return</span> epoch_loss <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span><span class="token punctuation">,</span> epoch_acc <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span>

<span class="token comment"># 测试函数</span>
<span class="token keyword">def</span> <span class="token function">evaluate</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> iterator<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span><span class="token punctuation">:</span>
    epoch_loss <span class="token operator">=</span> <span class="token number">0</span>
    epoch_acc <span class="token operator">=</span> <span class="token number">0</span>
    
    model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> batch <span class="token keyword">in</span> iterator<span class="token punctuation">:</span>
            text<span class="token punctuation">,</span> text_lengths <span class="token operator">=</span> batch<span class="token punctuation">.</span>text
            text <span class="token operator">=</span> text<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
            text_lengths <span class="token operator">=</span> text_lengths<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
            labels <span class="token operator">=</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>

            <span class="token comment"># 将文本输入模型中</span>
            outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>text<span class="token punctuation">,</span> attention_mask<span class="token operator">=</span><span class="token punctuation">(</span>text <span class="token operator">!=</span> tokenizer<span class="token punctuation">.</span>pad_token_id<span class="token punctuation">)</span><span class="token punctuation">,</span> labels<span class="token operator">=</span>labels<span class="token punctuation">)</span>

            <span class="token comment"># 计算损失和准确率</span>
            loss <span class="token operator">=</span> outputs<span class="token punctuation">.</span>loss
            acc <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>outputs<span class="token punctuation">.</span>logits <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> labels<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>

            epoch_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
            epoch_acc <span class="token operator">+=</span> acc<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> epoch_loss <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span><span class="token punctuation">,</span> epoch_acc <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span>
</code></pre> 
<p>接下来，我们可以开始训练模型。</p> 
<pre><code class="prism language-python"><span class="token comment"># 定义训练循环</span>
N_EPOCHS <span class="token operator">=</span> <span class="token number">2</span>

<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>N_EPOCHS<span class="token punctuation">)</span><span class="token punctuation">:</span>
    train_loss<span class="token punctuation">,</span> train_acc <span class="token operator">=</span> train<span class="token punctuation">(</span>model<span class="token punctuation">,</span> train_iterator<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span>
    valid_loss<span class="token punctuation">,</span> valid_acc <span class="token operator">=</span> evaluate<span class="token punctuation">(</span>model<span class="token punctuation">,</span> test_iterator<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span>
    
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Epoch: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token format-spec">02</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'\tTrain Loss: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>train_loss<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string"> | Train Acc: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>train_acc<span class="token operator">*</span><span class="token number">100</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string">%'</span></span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'\t Val. Loss: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>valid_loss<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string"> |  Val. Acc: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>valid_acc<span class="token operator">*</span><span class="token number">100</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string">%'</span></span><span class="token punctuation">)</span>
</code></pre> 
<p>训练完成后，我们可以使用训练好的模型对新的评论进行情感分析。我们可以将评论输入到模型中，然后将输出结果进行分类（大于0为积极，小于0为消极）。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">predict_sentiment</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> tokenizer<span class="token punctuation">,</span> sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>
    model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 将文本输入模型中</span>
    tokens <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>sentence<span class="token punctuation">,</span> add_special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    input_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>tokens<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>

    <span class="token comment"># 进行分类</span>
    logits <span class="token operator">=</span> outputs<span class="token punctuation">.</span>logits<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    sentiment <span class="token operator">=</span> <span class="token string">'positive'</span> <span class="token keyword">if</span> logits <span class="token operator">&gt;</span> <span class="token number">0</span> <span class="token keyword">else</span> <span class="token string">'negative'</span>

    <span class="token keyword">return</span> sentiment
</code></pre> 
<p>现在，我们可以使用上述函数对新的评论进行情感分析。</p> 
<pre><code class="prism language-python">sentiment <span class="token operator">=</span> predict_sentiment<span class="token punctuation">(</span>model<span class="token punctuation">,</span> tokenizer<span class="token punctuation">,</span> <span class="token string">"This movie was really good, I enjoyed it a lot!"</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>sentiment<span class="token punctuation">)</span>
<span class="token comment"># 输出：positive</span>

sentiment <span class="token operator">=</span> predict_sentiment<span class="token punctuation">(</span>model<span class="token punctuation">,</span> tokenizer<span class="token punctuation">,</span> <span class="token string">"This movie was terrible, I hated it."</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>sentiment<span class="token punctuation">)</span>
<span class="token comment"># 输出：negative</span>
</code></pre> 
<hr> 
<center> 
 <strong>❤️觉得内容不错的话，欢迎点赞收藏加关注😊😊😊，后续会继续输入更多优质内容❤️</strong> 
</center> 
<br> 
<center> 
 <strong>👉有问题欢迎大家加关注私戳或者评论（包括但不限于NLP算法相关，linux学习相关，读研读博相关......）👈</strong> 
</center> 
<hr>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f6c0e30092806903a30c3db299af8c3c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">STM32-微项目08-ADC单通道/多通道模式采集</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/5dfec85021633c3d381349414d430e40/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">ImportError: libffi.so.7: cannot open shared object file: No such file or directory解决方法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>