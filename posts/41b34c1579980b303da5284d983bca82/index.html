<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>tensorRt部署原理 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="tensorRt部署原理" />
<meta property="og:description" content="详解tensorRT的高性能部署方案，并附有强大的yolov5、yolox、retinaface、arcface、scrfd、deepsort、alphapose的高性能实现，低耦合，哪来即可用，集成到项目中 repo地址：https://github.com/shouxieai/tensorRT_cpp YoloX-m@640x640 fp16@2080Ti性能：2.54ms / image, 每秒393.54帧
第1章 tensorRT介绍 1.1 tensorRt是什么 答：是:nvdia发布的dnn推理加速引擎。
tensorRT:nvdia发布的dnn推理加速引擎，是针对nvdia系列硬件进行优化加速、实现最大程度的利用GPU资源，提升推理性能。
1.2 tensorrt 方案介绍 Tensorrt构建模型的形式，即需要告知它你模型的架构和权重。那tensorRT提供基于C&#43;&#43;接口和Python 接口的构建方案。那tensorrt各个版本有接口说明。通过api 你告诉它你模型需要的参数，结构，权重。
Tensorrt 提供的基础api 接口让你实现加速推理。为了方便起见，他们也提供更高级的使用方式，就是说你只需要把你的模型转换成onnx，它就可以直接转换成tensorrt的API.就是直接调用api,实现整个引擎的编译。
下面是3种常见路径。
Pytorch 到onnx 是pytorch 维护，也就是有新算子出来由官方维护。 Onnx到engine是 nvdia 来维护，即libnvonnxparser.so不断更新。
那编译也是很简单，就一句话。
第2章：使用TensorRT 的深度学习开发流程： 首先需要先训练深度学习网络框架，训练后会生成一个神经网络模型，然后可以指定 Batch-Size 和Precision 使用 TensorRT 进行优化，就是会得到一个优化后的推理引擎，将其序列化到磁盘，在使用时先进行反序列化，之后进行推理验证。
第3章 tensorrt加速优化原理 TensorRT能够加速的原因主要有两点，一方面是支持INT8和FP16的计算；另一方面是对网络结构进行了重构和优化
3.1 TensorRT支持INT8和FP16的计算 深度学习网络在训练时，通常使用 32 位或 16 位数据。TensorRT支持kFLOAT（float32）、kHALF（float16）、kINT8（int8）三种精度的计算，在使用时通过低精度进行网络推理，达到加速的目的。
3.2 TensorRT对网络结构进行了重构和优化 TensorRT对网络结构进行重构，把一些能合并的运算合并在一起，根据GPU的特性做了优化。具体表现在下面4个方面。
1 tensorRT通过解析网络模型将网络中无用的输出层消除以减小计算。
2对于网络结构的垂直整合，即将目前主流神经网络的conv、BN、Relu三个层融合为了一个层，例如将下图1所示的常见的Inception结构重构为图2所示的网络结构。
对网络的水平组合，水平组合是指将输入为相同张量和执行相同操作的层融合一起，如图2向图3的转化。 4去掉 concat 层，将原需输入 contact 层的直接送入 concat 下一级的操作中，不再单独进行 concat 步骤，相当于减少了一次传输吞吐量去掉concat层（
TensorRT中使用了一个新的操作——Concatenation操作来代替原有的concat层。Concatenation操作实现了多个tensor的拼接，但是采用了更加高效的实现方式，可以有效地减少计算和内存的占用，提高推理的速度和效率。
在TensorRT中，Concatenation操作可以对多个输入tensor进行拼接，并且可以指定拼接的维度，可以实现灵活的拼接操作。同时，TensorRT还支持对Concatenation操作进行多种优化，例如采用分块处理的方式，减少内存占用；采用流水线处理的方式，提高计算效率等等。这些优化可以进一步提高Concatenation操作的效率和性能。
。）
第4章Yolov5 使用tensorrt 部署步骤 4." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/41b34c1579980b303da5284d983bca82/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-28T21:13:14+08:00" />
<meta property="article:modified_time" content="2023-06-28T21:13:14+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">tensorRt部署原理</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2 style="text-align:center;"></h2> 
<p style="margin-left:.0001pt;text-align:left;"><span style="background-color:#ffffff;"><span style="color:#18191c;">详解tensorRT的高性能部署方案，并附有强大的yolov5、yolox、retinaface、arcface、scrfd、deepsort、alphapose的高性能实现，低耦合，哪来即可用，集成到项目中 repo地址：https://github.com/shouxieai/tensorRT_cpp YoloX-m@640x640 fp16@2080Ti性能：2.54ms / image, 每秒393.54帧</span></span></p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<h3 style="text-align:justify;"><strong><strong><strong>第1章 tensorRT介绍</strong></strong></strong></h3> 
<h4 style="text-align:justify;"><strong><strong><strong>1.1 </strong></strong><strong><strong>ten</strong></strong><strong><strong>sor</strong></strong><strong><strong>Rt是什么</strong></strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">答：是:nvdia发布的dnn推理加速引擎。</p> 
<p style="margin-left:.0001pt;text-align:justify;">tensorRT:nvdia发布的dnn推理加速引擎，是针对nvdia系列硬件进行优化加速、实现最大程度的利用GPU资源，提升推理性能。</p> 
<h4 style="text-align:justify;"><strong><strong><strong>1.2 tensorrt</strong></strong><strong> </strong><strong><strong>方案介绍</strong></strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">  Tensorrt构建模型的形式，即需要告知它你模型的架构和权重。那tensorRT提供基于C++接口和Python 接口的构建方案。那tensorrt各个版本有接口说明。通过api 你告诉它你模型需要的参数，结构，权重。</p> 
<p style="margin-left:.0001pt;text-align:justify;">  Tensorrt 提供的基础api 接口让你实现加速推理。为了方便起见，他们也提供更高级的使用方式，就是说你只需要把你的模型转换成onnx，它就可以直接转换成tensorrt的API.就是直接调用api,实现整个引擎的编译。</p> 
<p style="margin-left:.0001pt;text-align:justify;">下面是3种常见路径。</p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="254" src="https://images2.imgbox.com/92/ac/kRMu3ya7_o.png" width="639"></p> 
<p> </p> 
<p style="margin-left:.0001pt;text-align:justify;">Pytorch 到onnx 是pytorch 维护，也就是有新算子出来由官方维护。 Onnx到engine是 nvdia 来维护，即libnvonnxparser.so不断更新。</p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p style="margin-left:.0001pt;text-align:justify;">那编译也是很简单，就一句话。</p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="167" src="https://images2.imgbox.com/24/8f/Kx8qiJIJ_o.png" width="424"></p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="508" src="https://images2.imgbox.com/cf/96/a5O51TYL_o.png" width="578"></p> 
<p> </p> 
<p> </p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<h3 style="text-align:justify;"><strong><strong> </strong><strong><span style="background-color:#ffffff;"><strong>第2章：使用</strong></span></strong><strong><span style="background-color:#ffffff;"><strong>TensorRT 的深度学习开发流程</strong></span></strong><strong><span style="background-color:#ffffff;"><strong>：</strong></span></strong></strong></h3> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="194" src="https://images2.imgbox.com/80/b8/CdRpdtb9_o.png" width="565"></p> 
<p> </p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p style="margin-left:.0001pt;text-align:left;"><span style="background-color:#ffffff;">首先需要先训练深度学习网络框架，训练后会生成一个神经网络模型，然后可以指定 Batch-Size 和Precision 使用 TensorRT 进行优化，</span><span style="background-color:#ffffff;">就</span><span style="background-color:#ffffff;">是会得到一个优化后的推理引擎，将其序列化到磁盘，在使用时先进行反序列化，之后进行推理验证</span><span style="background-color:#ffffff;">。</span></p> 
<h3 style="text-align:justify;"><strong><strong><span style="background-color:#ffffff;"><strong>第3章 tensor</strong></span></strong><strong><span style="background-color:#ffffff;"><strong>rt</strong></span></strong><strong><span style="background-color:#ffffff;"><strong>加速优化原理</strong></span></strong></strong></h3> 
<p style="margin-left:.0001pt;text-align:left;"><span style="background-color:#ffffff;"><span style="color:#4d4d4d;">TensorRT能够加速的原因主要有两点，一方面是支持INT8和FP16的计算；另一方面是对网络结构进行了重构和优化</span></span></p> 
<h4 style="text-align:justify;"><strong><strong><strong>3.1 TensorRT支持INT8和FP16的计算</strong></strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:left;"><span style="color:#4d4d4d;">深度学习网络在训练时，通常使用 32 位或 16 位数据。TensorRT支持kFLOAT（float32）、kHALF（float16）、kINT8（int8）三种精度的计算，在使用时通过低精度进行网络推理，达到加速的目的。</span></p> 
<h4 style="text-align:justify;"><strong><strong><strong>3.2 TensorRT对网络结构进行了重构和优化</strong></strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:left;"><span style="color:#4d4d4d;">TensorRT对网络结构进行重构，把一些能合并的运算合并在一起，根据GPU的特性做了优化。</span><span style="color:#4d4d4d;">具体表现在下面4个方面。</span></p> 
<p style="margin-left:.0001pt;text-align:left;"><span style="color:#4d4d4d;">1 tensorRT通过解析网络模型将网络中无用的输出层消除以减小计算。</span></p> 
<p style="margin-left:.0001pt;text-align:left;"><span style="color:#4d4d4d;">2</span><span style="color:#4d4d4d;">对于网络结构的垂直整合，即将目前主流神经网络的conv、BN、Relu三个层融合为了一个层，例如将下图1所示的常见的Inception结构重构为图2所示的网络结构。</span></p> 
<p style="margin-left:.0001pt;text-align:left;"><img alt="" height="340" src="https://images2.imgbox.com/4a/58/ZJhwITba_o.png" width="679"></p> 
<p> </p> 
<p style="margin-left:.0001pt;text-align:left;"></p> 
<ol><li style="text-align:left;">对网络的水平组合，水平组合是指将输入为相同张量和执行相同操作的层融合一起，如图2向图3的转化。</li></ol> 
<p style="margin-left:.0001pt;text-align:left;"><img alt="" height="360" src="https://images2.imgbox.com/ee/c5/ApJWiMXy_o.png" width="612"></p> 
<p> </p> 
<p style="margin-left:27pt;text-align:left;"> 4去掉 concat 层，将原需输入 contact 层的直接送入 concat 下一级的操作中，不再单独进行 concat 步骤，相当于减少了一次传输吞吐量去掉concat层（</p> 
<p style="margin-left:27pt;text-align:left;">TensorRT中使用了一个新的操作——Concatenation操作来代替原有的concat层。Concatenation操作实现了多个tensor的拼接，但是采用了更加高效的实现方式，可以有效地减少计算和内存的占用，提高推理的速度和效率。</p> 
<p style="margin-left:27pt;text-align:left;"></p> 
<p style="margin-left:27pt;text-align:left;">在TensorRT中，Concatenation操作可以对多个输入tensor进行拼接，并且可以指定拼接的维度，可以实现灵活的拼接操作。同时，TensorRT还支持对Concatenation操作进行多种优化，例如采用分块处理的方式，减少内存占用；采用流水线处理的方式，提高计算效率等等。这些优化可以进一步提高Concatenation操作的效率和性能。</p> 
<p style="margin-left:27pt;text-align:left;">。）</p> 
<p style="margin-left:.0001pt;text-align:left;"></p> 
<p style="margin-left:.0001pt;text-align:left;"></p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<h3 style="text-align:justify;"><strong><strong><strong>第</strong></strong><strong><strong>4</strong></strong><strong><strong>章</strong></strong><strong><strong>Y</strong></strong><strong><strong>o</strong></strong><strong><strong>lov5</strong></strong><strong><strong> 使用tensor</strong></strong><strong><strong>rt</strong></strong><strong><strong> 部署步骤</strong></strong></strong></h3> 
<h4 style="text-align:justify;"><strong><strong><strong>4.1 定义</strong></strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="265" src="https://images2.imgbox.com/52/04/GDnSOiLu_o.png" width="570"></p> 
<p> </p> 
<p style="margin-left:.0001pt;text-align:left;">1<span style="background-color:#ffffff;">网络定义是指 TensorRT 中模型的表示。网络定义是张量和运算符的图；</span></p> 
<p style="margin-left:.0001pt;text-align:left;"><span style="background-color:#ffffff;">2编译</span><span style="background-color:#ffffff;">：</span><span style="background-color:#ffffff;">是指 TensorRT 的模型优化器</span><span style="background-color:#ffffff;">。</span><span style="background-color:#ffffff;">构建器将网络定义作为输入</span>，<span style="background-color:#ffffff;">执行与设备无关和针对特定设备的优化</span><span style="background-color:#ffffff;">，</span><span style="background-color:#ffffff;">并创建引擎</span><span style="background-color:#ffffff;">en</span><span style="background-color:#ffffff;">gine.</span></p> 
<p style="margin-left:.0001pt;text-align:left;"><span style="background-color:#ffffff;">3 引擎是指由 TensorRT 构建器优化的模型的表示；</span></p> 
<p style="margin-left:.0001pt;text-align:left;"><span style="background-color:#ffffff;">4 </span><span style="background-color:#ffffff;">运行是指 TensorRT 的组件，可在 TensorRT 引擎上执行推理。</span></p> 
<p style="margin-left:.0001pt;text-align:left;"></p> 
<h4 style="text-align:justify;"><strong><strong><span style="background-color:#ffffff;"><span style="color:#ff0000;"><strong>4.2模型转换代码解析</strong></span></span></strong></strong></h4> 
<h4 style="text-align:justify;"><strong><strong><span style="color:#ff0000;"><strong>4.3 你部署过程中遇到的问题</strong></span></strong><strong>   </strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<h3 style="text-align:justify;"><strong><strong><span style="background-color:#ffffff;"><strong>第</strong></span></strong><strong><span style="background-color:#ffffff;"><strong>5</strong></span></strong><strong><span style="background-color:#ffffff;"><strong>章 部署方案</strong></span></strong></strong></h3> 
<p style="margin-left:.0001pt;text-align:justify;">硬件：基于 jeston tx2 开发板下，进行基于tensorrt 加速部署。</p> 
<p style="margin-left:.0001pt;text-align:left;">软件：<span style="background-color:#ffffff;">TensorRT + C++的部署方式不仅能过在网络模型方面获得推理加速，而且由于 C++是更偏向底层的语言，因此还能获得语言层面的加速</span></p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<ol><li style="text-align:justify;"><strong><strong><span style="color:#ff0000;"><strong>加速实验对比</strong></span></strong></strong></li></ol> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="color:#000000;">使用pt 模型速度：</span></p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="111" src="https://images2.imgbox.com/e6/69/5uMNViP4_o.png" width="692"></p> 
<p> </p> 
<p style="margin-left:.0001pt;text-align:justify;">使用engine模型加速后：</p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p> <img alt="" height="83" src="https://images2.imgbox.com/0a/18/3RHxHf3o_o.png" width="820"></p> 
<hr> 
<p> </p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a13f28bad2856ac30c0ccefa7ed233b4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Mybatis plus中遇到的分页查询报错问题解决</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ca4171e4af9cbadcaaa2a2dd40806296/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">目标检测中NMS和mAP指标中的的IoU阈值和置信度阈值</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>