<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>目标检测：Object Detection with Deep Learning: A Review - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="目标检测：Object Detection with Deep Learning: A Review" />
<meta property="og:description" content="文章目录 1. INTRODUCTION2. A BRIEFOVERVIEW OFDEEPLEARNING3. 通用目标检测A. Region Proposal Based Framework:基于区域提案的框架1） R-CNN2） SPP网络3） Fast R-CNN4) Faster R-CNN5) R-FCN6) FPN7） Mask R-CNN8） 多任务学习、多尺度表示和上下文建模9） 基于深度学习的目标检测思考 B. Regression/Classification Based Framework：基于回归/分类的框架1） Pioneer Works2） YOLO3） SSD C. Experimental Evaluation1） PASCAL VOC 2007/20122） MicrosoftCoCo3) Timing Analysis 4. 显著目标检测5. 人脸检测6. 行人检测7. 展望未来的方向和任务8. CONCLUSION 摘要
由于目标检测与视频分析和图像理解有着密切的关系，近年来引起了人们的广泛关注。传统的目标检测方法建立在手工制作的特征和浅层可训练的体系结构之上。通过构建复杂的集合，将多个低层图像特征与来自对象检测器和场景分类器的高层上下文相结合，它们的性能很容易停滞。随着深度学习的快速发展，越来越多的功能强大的工具被引入到解决传统体系结构中存在的问题中，这些工具能够学习语义、高层次、更深层次的特性。这些模型在网络结构、训练策略和优化功能等方面表现不同。在本文中，我们回顾了基于深度学习的目标检测框架。我们的回顾首先简要介绍了深度学习的历史及其代表性工具，即卷积神经网络（CNN）。然后，我们重点介绍了典型的通用对象检测体系结构以及一些修改和有用的技巧，以进一步提高检测性能。由于不同的特定检测任务表现出不同的特点，我们还简要介绍了几个特定任务，包括显著目标检测、人脸检测和行人检测。通过实验分析，对各种方法进行了比较，得出了一些有意义的结论。最后，本文提出了几个有希望的方向和任务，为目标检测和相关的基于神经网络的学习系统的未来工作提供了指导。
1. INTRODUCTION 为了获得一个完整的图像理解，我们不仅应该集中精力对不同的图像进行分类，还应该尝试精确地估计每个图像中包含的对象的概念和位置。此任务称为目标检测[1][S1]，通常由不同的子任务组成，如人脸检测[2][S2]、行人检测[3][S2]和骨架检测[4][S3]。作为基本的计算机视觉问题之一，目标检测能够为图像和视频的语义理解提供有价值的信息，并且涉及到许多应用，包括图像分类[5]、[6]、人类行为分析[7][S4]、人脸识别[8][S5]和自动驾驶[9]、[10]。同时，这些领域的进展继承了神经网络和相关的学习系统，将发展神经网络算法，也将对可视为学习系统的目标检测技术产生重大影响。[11]-[14][S6]。然而，由于视点、姿态、遮挡和光照条件的巨大变化，使用额外的目标定位任务很难完美地完成目标检测。 近年来，这一领域引起了广泛的关注。
目标检测的问题定义是确定目标在给定图像中的位置（目标定位）以及每个目标所属的类别（目标分类）。因此，传统的目标检测模型主要分为三个阶段：信息区域选择、特征提取和分类。
信息区域选择：由于不同的对象可能出现在图像的任何位置，并且具有不同的纵横比或大小，因此使用多尺度滑动窗口扫描整个图像是一种自然选择。虽然这种穷举策略可以找出对象的所有可能位置，但其缺点也是显而易见的。由于候选窗口的数量很大，因此计算成本很高，并且会产生太多冗余窗口。但是，如果仅应用固定数量的滑动窗口模板，则可能会产生不符合要求的区域。
特征提取：为了识别不同的物体，我们需要提取能够提供语义和鲁棒性表示的视觉特征。SIFT[19]、HOG[20]和Haar-like[21]特征是典型特征。这是因为这些特征可以产生与人脑复杂细胞相关的表征[19]。然而，由于外观、光照条件和背景的多样性，很难手动设计一个健壮的特征描述符来完美地描述所有类型的对象。
分类：此外，还需要一个分类器来区分目标对象与所有其他类别，并使表示更具有层次性、语义性和信息性，便于视觉识别。通常，支持向量机（SVM）[22]、AdaBoost[23]和基于变形零件的模型（DPM）[24]是不错的选择。在这些分类器中，DPM是一种灵活的模型，通过将对象部分与变形代价相结合来处理严重变形。在DPM中，借助图形模型，精心设计的底层特征和受运动学启发的零件分解被结合起来。图形模型的区别性学习允许为各种对象类构建基于零件的高精度模型。
基于这些判别式局部特征描述符和浅层可学习体系结构，在PASCAL VOC目标检测竞赛[25]上取得了最新成果，并以较低的硬件负担获得了实时嵌入式系统。然而，在2010-2012年期间，仅通过构建集成系统和采用成功方法的微小变化，就获得了较小的收益[15]。这一事实是由于以下原因：1）使用滑动窗口策略生成候选边界框是冗余、低效和不准确的。2)人工设计的低级描述符和经过区别训练的浅层模型不能弥补语义鸿沟。
由于深度神经网络（DNN）[6][S7]的出现，通过引入Regions with CNN features (R-CNN)[15]，获得了更显著的增益。DNN或最具代表性的CNN的行为方式与传统方法截然不同。他们有更深层次的体系结构，能够比浅层次的体系结构学习更复杂的功能。此外，表达能力和稳健的训练算法允许学习信息对象表示，而无需手动设计特征[26]。
自R-CNN提出以来，已经提出了许多改进模型，包括联合优化分类和包围盒回归任务的Fast R-CNN[16]，需要额外子网络生成区域建议的 Faster R-CNN[18]，以及通过固定网格回归实现目标检测的YOLO[17]。所有这些都在不同程度上提高了R-CNN的检测性能，使目标检测的实时性和准确性变得更容易实现。
本文对几个应用领域的代表性模型及其不同特征进行了系统综述，包括通用目标检测[15]、[16]、[18]、显著目标检测[27]、[28]、人脸检测[29]–[31]和行人检测[32]、[33]。它们之间的关系如图1所示。基于CNN的基本架构，通过边界盒回归实现通用目标检测，通过局部对比度增强和像素级分割实现显著目标检测。人脸检测和行人检测与一般目标检测密切相关，主要分别通过多尺度自适应和多特征融合/增强森林来实现。虚线表示在某些条件下，相应的域相互关联。应该注意的是，覆盖的领域是多样化的。行人和人脸图像具有规则的结构，而一般对象和场景图像在几何结构和布局上具有更复杂的变化。因此，不同的图像需要不同的深度模型。
有一个相关的工作[34]，主要集中于相关软件工具来实现图像分类和目标检测的深度学习技术，但很少关注具体算法的细节。 与之不同的是，本文对基于深度学习的目标检测模型和算法进行了详细的综述，并给出了相应的实验比较和有意义的分析。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/6ae32ef565f0a6b743b4dca280a4cf1f/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-10-06T22:23:23+08:00" />
<meta property="article:modified_time" content="2021-10-06T22:23:23+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">目标检测：Object Detection with Deep Learning: A Review</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-dracula">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><ul><li><a href="#1_INTRODUCTION_5" rel="nofollow">1. INTRODUCTION</a></li><li><a href="#2_A_BRIEFOVERVIEW_OFDEEPLEARNING_42" rel="nofollow">2. A BRIEFOVERVIEW OFDEEPLEARNING</a></li><li><a href="#3__75" rel="nofollow">3. 通用目标检测</a></li><li><ul><li><a href="#A_Region_Proposal_Based_Framework_79" rel="nofollow">A. Region Proposal Based Framework:基于区域提案的框架</a></li><li><ul><li><a href="#1_RCNN_82" rel="nofollow">1） R-CNN</a></li><li><a href="#2_SPP_102" rel="nofollow">2） SPP网络</a></li><li><a href="#3_Fast_RCNN_113" rel="nofollow">3） Fast R-CNN</a></li><li><a href="#4_Faster_RCNN_133" rel="nofollow">4) Faster R-CNN</a></li><li><a href="#5_RFCN_144" rel="nofollow">5) R-FCN</a></li><li><a href="#6_FPN_152" rel="nofollow">6) FPN</a></li><li><a href="#7_Mask_RCNN_159" rel="nofollow">7） Mask R-CNN</a></li><li><a href="#8__166" rel="nofollow">8） 多任务学习、多尺度表示和上下文建模</a></li><li><a href="#9__179" rel="nofollow">9） 基于深度学习的目标检测思考</a></li></ul> 
    </li><li><a href="#B_RegressionClassification_Based_Framework_184" rel="nofollow">B. Regression/Classification Based Framework：基于回归/分类的框架</a></li><li><ul><li><a href="#1_Pioneer_Works_189" rel="nofollow">1） Pioneer Works</a></li><li><a href="#2_YOLO_195" rel="nofollow">2） YOLO</a></li><li><a href="#3_SSD_205" rel="nofollow">3） SSD</a></li></ul> 
    </li><li><a href="#C_Experimental_Evaluation_214" rel="nofollow">C. Experimental Evaluation</a></li><li><ul><li><a href="#1_PASCAL_VOC_20072012_219" rel="nofollow">1） PASCAL VOC 2007/2012</a></li><li><a href="#2_MicrosoftCoCo_229" rel="nofollow">2） MicrosoftCoCo</a></li><li><a href="#3_Timing_Analysis_239" rel="nofollow">3) Timing Analysis</a></li></ul> 
   </li></ul> 
   </li><li><a href="#4__251" rel="nofollow">4. 显著目标检测</a></li><li><a href="#5__252" rel="nofollow">5. 人脸检测</a></li><li><a href="#6__253" rel="nofollow">6. 行人检测</a></li><li><a href="#7__263" rel="nofollow">7. 展望未来的方向和任务</a></li><li><a href="#8_CONCLUSION_285" rel="nofollow">8. CONCLUSION</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<blockquote> 
 <p>摘要</p> 
 <hr> 
 <p>由于目标检测与视频分析和图像理解有着密切的关系，近年来引起了人们的广泛关注。传统的目标检测方法建立在手工制作的特征和浅层可训练的体系结构之上。通过构建复杂的集合，将多个低层图像特征与来自对象检测器和场景分类器的高层上下文相结合，它们的性能很容易停滞。随着深度学习的快速发展，越来越多的功能强大的工具被引入到解决传统体系结构中存在的问题中，这些工具能够学习语义、高层次、更深层次的特性。这些模型在网络结构、训练策略和优化功能等方面表现不同。在本文中，我们回顾了基于深度学习的目标检测框架。我们的回顾首先简要介绍了深度学习的历史及其代表性工具，即卷积神经网络（CNN）。然后，我们重点介绍了典型的通用对象检测体系结构以及一些修改和有用的技巧，以进一步提高检测性能。由于不同的特定检测任务表现出不同的特点，我们还简要介绍了几个特定任务，包括显著目标检测、人脸检测和行人检测。通过实验分析，对各种方法进行了比较，得出了一些有意义的结论。最后，本文提出了几个有希望的方向和任务，为目标检测和相关的基于神经网络的学习系统的未来工作提供了指导。</p> 
</blockquote> 
<h3><a id="1_INTRODUCTION_5"></a>1. INTRODUCTION</h3> 
<p>为了获得一个完整的图像理解，我们不仅应该集中精力对不同的图像进行分类，还应该尝试精确地估计每个图像中包含的对象的概念和位置。此任务称为目标检测[1][S1]，通常由不同的子任务组成，如人脸检测[2][S2]、行人检测[3][S2]和骨架检测[4][S3]。作为基本的计算机视觉问题之一，目标检测能够为图像和视频的语义理解提供有价值的信息，并且涉及到许多应用，包括图像分类[5]、[6]、人类行为分析[7][S4]、人脸识别[8][S5]和自动驾驶[9]、[10]。同时，这些领域的进展继承了神经网络和相关的学习系统，将发展神经网络算法，也将对可视为学习系统的目标检测技术产生重大影响。[11]-[14][S6]。然而，由于视点、姿态、遮挡和光照条件的巨大变化，使用额外的目标定位任务很难完美地完成目标检测。 近年来，这一领域引起了广泛的关注。</p> 
<p>目标检测的问题定义是确定目标在给定图像中的位置（目标定位）以及每个目标所属的类别（目标分类）。因此，传统的目标检测模型主要分为三个阶段：<strong>信息区域选择、特征提取和分类</strong>。</p> 
<p><strong>信息区域选择</strong>：由于不同的对象可能出现在图像的任何位置，并且具有不同的纵横比或大小，因此使用多尺度滑动窗口扫描整个图像是一种自然选择。虽然这种穷举策略可以找出对象的所有可能位置，但其缺点也是显而易见的。由于候选窗口的数量很大，因此计算成本很高，并且会产生太多冗余窗口。但是，如果仅应用固定数量的滑动窗口模板，则可能会产生不符合要求的区域。</p> 
<p><strong>特征提取</strong>：为了识别不同的物体，我们需要提取能够提供语义和鲁棒性表示的视觉特征。SIFT[19]、HOG[20]和Haar-like[21]特征是典型特征。这是因为这些特征可以产生与人脑复杂细胞相关的表征[19]。然而，由于外观、光照条件和背景的多样性，很难手动设计一个健壮的特征描述符来完美地描述所有类型的对象。</p> 
<p><strong>分类</strong>：此外，还需要一个分类器来区分目标对象与所有其他类别，并使表示更具有层次性、语义性和信息性，便于视觉识别。通常，支持向量机（SVM）[22]、AdaBoost[23]和基于变形零件的模型（DPM）[24]是不错的选择。在这些分类器中，DPM是一种灵活的模型，通过将对象部分与变形代价相结合来处理严重变形。在DPM中，借助图形模型，精心设计的底层特征和受运动学启发的零件分解被结合起来。图形模型的区别性学习允许为各种对象类构建基于零件的高精度模型。</p> 
<p>基于这些判别式局部特征描述符和浅层可学习体系结构，在PASCAL VOC目标检测竞赛[25]上取得了最新成果，并以较低的硬件负担获得了实时嵌入式系统。然而，在2010-2012年期间，仅通过构建集成系统和采用成功方法的微小变化，就获得了较小的收益[15]。这一事实是由于以下原因：1）使用滑动窗口策略生成候选边界框是冗余、低效和不准确的。2)人工设计的低级描述符和经过区别训练的浅层模型不能弥补语义鸿沟。</p> 
<p>由于深度神经网络（DNN）[6][S7]的出现，通过引入Regions with CNN features (R-CNN)[15]，获得了更显著的增益。DNN或最具代表性的CNN的行为方式与传统方法截然不同。他们有更深层次的体系结构，能够比浅层次的体系结构学习更复杂的功能。此外，表达能力和稳健的训练算法允许学习信息对象表示，而无需手动设计特征[26]。</p> 
<p>自R-CNN提出以来，已经提出了许多改进模型，包括联合优化分类和包围盒回归任务的Fast R-CNN[16]，需要额外子网络生成区域建议的 Faster R-CNN[18]，以及通过固定网格回归实现目标检测的YOLO[17]。所有这些都在不同程度上提高了R-CNN的检测性能，使目标检测的实时性和准确性变得更容易实现。</p> 
<p>本文对几个应用领域的代表性模型及其不同特征进行了系统综述，包括通用目标检测[15]、[16]、[18]、显著目标检测[27]、[28]、人脸检测[29]–[31]和行人检测[32]、[33]。它们之间的关系如图1所示。基于CNN的基本架构，通过边界盒回归实现通用目标检测，通过局部对比度增强和像素级分割实现显著目标检测。人脸检测和行人检测与一般目标检测密切相关，主要分别通过多尺度自适应和多特征融合/增强森林来实现。虚线表示在某些条件下，相应的域相互关联。应该注意的是，覆盖的领域是多样化的。行人和人脸图像具有规则的结构，而一般对象和场景图像在几何结构和布局上具有更复杂的变化。因此，不同的图像需要不同的深度模型。<br> <img src="https://images2.imgbox.com/3e/1c/NmkvqYDK_o.png" alt=""><br> 有一个相关的工作[34]，主要集中于相关软件工具来实现图像分类和目标检测的深度学习技术，但很少关注具体算法的细节。 与之不同的是，本文对基于深度学习的目标检测模型和算法进行了详细的综述，并给出了相应的实验比较和有意义的分析。</p> 
<p>本文的其余部分组织如下。<br> 第2节简要介绍了深度学习的历史和CNN的基本架构。<br> 第3节介绍了通用的目标检测体系结构。<br> 第4-6节分别介绍了CNN在几个具体任务中的应用，包括显著目标检测、人脸检测和行人检测。<br> 第7节提出了几个有希望的未来方向。<br> 最后，第8节给出了一些结论。</p> 
<h3><a id="2_A_BRIEFOVERVIEW_OFDEEPLEARNING_42"></a>2. A BRIEFOVERVIEW OFDEEPLEARNING</h3> 
<p>在概述基于深度学习的目标检测方法之前，我们回顾了深度学习的历史，并介绍了CNN的基本架构和优势。</p> 
<p><strong>A. 历史：诞生、衰落和繁荣</strong></p> 
<p>Deep models 可以称为具有深层结构的神经网络。神经网络的历史可以追溯到20世纪40年代[35]，其初衷是模拟人脑系统，以有原则的方式解决一般的学习问题。Hinton等人[36]提出的反向传播算法在20世纪80年代和90年代非常流行。然而，由于训练的过度拟合、缺乏大规模训练数据、计算能力有限以及与其他机器学习工具相比性能不显著，神经网络(neural networks)在21世纪初已经过时。</p> 
<p>自2006年以来，随着语音识别领域的突破，深度学习(Deep learning)已经变得非常流行[37][S7]。深度学习的恢复可归因于以下因素。</p> 
<ul><li>出现了大规模带注释的训练数据，如ImageNet[39]，以充分展示其巨大的学习能力；</li><li>高性能并行计算系统的快速发展，如GPU集群；</li><li>在网络结构设计和训练战略方面取得重大进展。通过自动编码器（AE）[40]或Restricted Boltzmann Machine (RBM)[41]引导的无监督分层预训练，可以提供良好的初始化。随着dropout和 data augmentation，训练中的过度拟合问题得到了缓解[6]，[42]。通过batch normalization（BN），非常深入的神经网络的训练变得非常有效[43]。同时，各种网络结构，如AlexNet[6]、Overfeat[44]、GoogLeNet[45]、VGG[46]和ResNet[47]已被广泛研究以提高性能。</li></ul> 
<p>是什么促使深度学习对整个学术界产生巨大影响？这可能要归功于Hinton小组的贡献，他们的持续努力表明，深度学习将带来重大挑战方面的革命性突破，而不仅仅是小型数据集的明显改进。他们的成功来自于对120万张有标记的图像进行训练，并使用一些技术[6]（例如，ReLU操作[48]和‘dropout’ 正则化）。</p> 
<p><strong>B. CNN的架构和优势</strong><br> CNN是最具代表性的深度学习模式[26]。一种典型的CNN架构，称为VGG16，可以在图S1中找到。CNN的每一层称为特征图。输入层的特征图是不同颜色通道（如RGB）像素强度的3D矩阵。任何内部层的特征图都是一个诱导的多通道图像，其“像素”可以被视为一个特定的特征。每个神经元都与前一层（感受野）的一小部分相邻神经元相连。可以在特征图上执行不同类型的转换[6]、[49]、[50]，如过滤和池化。 滤波(卷积)运算是将一个滤波矩阵(学习权值)与神经元接受域的值进行卷积，并取一个非线性函数(如sigmoid [51]， ReLU)来得到最终的响应。 池化操作，如最大池化、平均池化、L2-池化和局部对比度归一化[52]，将接受域的响应总结为一个值，以产生更稳健的特征描述。</p> 
<p>通过卷积和池之间的交织，构造初始特征层次结构，通过添加几个完全连接（FC）层以适应不同的视觉任务，可以以有监督的方式对初始特征层次结构进行微调。根据所涉及的任务，添加具有不同激活函数的最后一层[6]，以获得每个输出神经元的特定条件概率。通过随机梯度下降（SGD）方法，可以在目标函数（如均方误差或交叉熵损失）上对整个网络进行优化。典型的VGG16共有13个卷积（conv）层、3个完全连接层、3个最大池层和一个softmax分类层。conv要素图是通过卷积3*3个过滤窗口生成的，要素图分辨率通过2个最大跨步池层降低。训练后的网络可以处理与训练样本大小相同的任意测试图像。如果提供不同的尺寸，则可能需要重新缩放或裁剪操作[6]。</p> 
<p>CNN相对于传统方法的优势可以总结如下：</p> 
<ul><li>分层特征表示，即通过分层多级结构学习的从像素到高级语义特征的多级表示[15]，[53]，可以从数据中自动学习，并且可以通过多级非线性映射分离输入数据的隐藏因素。</li><li>与传统的浅层模型相比，更深层次的体系结构提供了成倍增长的表达能力</li><li>CNN的体系结构提供了一个共同优化几个相关任务的机会（例如，Fast RCNN将分类和包围盒回归结合到多任务学习方式中</li><li>得益于深层CNN的强大学习能力，一些经典的计算机视觉挑战可以被重新描述为高维数据转换问题，并从不同的角度解决。</li></ul> 
<p>基于这些优势，CNN已被广泛应用于许多研究领域，如图像超分辨率重建[54]、[55]、图像分类[5]、[56]、图像检索[57]、[58]、人脸识别[8][S5]、行人检测[59]-[61]和视频分析[62]、[63]。</p> 
<h3><a id="3__75"></a>3. 通用目标检测</h3> 
<p>通用目标检测的目的是定位和分类任何一幅图像中的现有目标，并用矩形边界框标记它们，以显示存在的可信度。通用对象检测方法的框架主要可以分为两种类型（见图2）。一种方法遵循传统的目标检测流程，首先生成区域建议，然后将每个建议分类为不同的目标类别。另一种是将目标检测视为一个回归或分类问题，采用统一的框架直接获得最终结果（类别和位置）。基于区域建议的方法主要包括R-CNN[15]、SPP-net[64]、Fast R-CNN[16]、Faster R-CNN[18]、R-FCN[65]、FPN[66]和Mask R-CNN[67]，其中一些方法相互关联（例如，SPP-net使用SPP层修改RCNN）。基于回归/分类的方法主要包括MultiBox[68]、AttentionNet[69]、G-CNN[70]、YOLO[17]、SSD[71]、YOLOv2[72]、DSSD[73]和DSOD[74]。这两条管道之间的相关性通过快速R-CNN中引入的锚anchors来桥接。这些方法的详情如下。</p> 
<p><img src="https://images2.imgbox.com/b4/db/oaUtBsSy_o.png" alt=""></p> 
<h4><a id="A_Region_Proposal_Based_Framework_79"></a>A. Region Proposal Based Framework:基于区域提案的框架</h4> 
<p>基于区域提议的框架是一个两步过程，在一定程度上符合人脑的注意机制，它首先对整个场景进行粗略扫描，然后关注感兴趣的区域。在之前的相关作品[44]、[75]、[76]中，最具代表性的是Overfeat[44]。该模型将CNN插入滑动窗口方法中，该方法在获得底层对象类别的可信度后，直接从最顶部特征地图的位置预测边界框。</p> 
<h5><a id="1_RCNN_82"></a>1） R-CNN</h5> 
<p>这对于提高候选边界框的质量和采用深层架构来提取高级特征具有重要意义。为了解决这些问题，Ross Girshick于2014年提出了R-CNN[15]，并获得了53.3%的平均精度（mAP）。比PASCAL VOC 2012上一个最佳结果（DPM HSC[77]）提高了30%以上。图3显示了R-CNN的流程图，可分为以下三个阶段。<br> <strong>区域建议生成</strong>。R-CNN采用选择性搜索[78]为每幅图像生成约2k个区域建议。选择性搜索方法依赖于简单的自底向上分组和显著性线索，以快速提供任意大小的更准确候选框，并减少目标检测中的搜索空间[24]，[39]。<br> <strong>基于CNN的深度特征提取</strong>。在此阶段，将每个区域建议扭曲或裁剪为固定分辨率，并利用[6]中的CNN模块提取4096维特征作为最终表示。由于CNN具有强大的学习能力、强大的表达能力和层次结构，因此可以为每个区域建议获得高层次、语义和鲁棒的特征表示。<br> **分类和本地化。**使用预先训练的多类特定类别线性支持向量机，在一组正区域和背景（负）区域上对不同的区域建议进行评分。得分区域然后使用边界框回归并使用贪婪的非最大值抑制（NMS）进行过滤，以生成保留对象位置的最终边界框。</p> 
<p><img src="https://images2.imgbox.com/11/b6/IXgOsiPm_o.png" alt=""><br> 当标记数据稀少或不足时，通常进行预训练。R-CNN不是无监督的预训练[79]，而是首先对ILSVRC（一个非常大的辅助数据集）进行有监督的预训练，然后进行特定领域的微调。该方案已被大多数后续方法采用[16]，[18]。</p> 
<p>尽管它比传统方法有了改进，并且在将CNN引入实际目标检测方面具有重要意义，但仍然存在一些缺点。</p> 
<ul><li>由于FC层的存在，CNN需要固定大小（例如227×227）的输入图像，这直接导致对每个评估区域重新计算整个CNN，在测试期间花费大量时间。</li><li>R-CNN的训练是一个多阶段的过程。首先，对对象建议的卷积网络（ConvNet）进行微调。然后用支持向量机代替通过微调学习的softmax分类器，以适应ConvNet特征。最后，训练边界盒回归器。</li><li>在空间和时间上训练都很耗力。从不同的区域方案中提取特征并存储在磁盘上。处理一个相对较小、网络非常深入的训练集（如VGG16）需要很长时间。同时，这些功能所需的存储内存也应引起关注</li><li>虽然选择性搜索可以生成召回率相对较高的区域建议，但获得的区域建议仍然是多余的，并且此过程非常耗时（提取2k个区域建议大约需要2秒钟）</li></ul> 
<p>为了解决这些问题，人们提出了许多方法。GOP[80]采用了更快的基于测地线的分割来取代传统的图形切割。MCG[81]搜索图像的不同比例，以进行多层次分割，并对不同区域进行组合分组，以生成建议。边缘框方法[82]没有提取视觉上不同的线段，而是采用了这样的思想，即对象更有可能存在于边界框中，边界上的轮廓较少。此外，一些研究试图重新排序或细化预提取的区域建议，以删除不必要的建议，并获得有限数量的有价值的建议，如DeepBox[83]和SharpMask[84]。</p> 
<p>此外，还有一些改进来解决定位不准确的问题。Zhang等人[85]利用基于贝叶斯优化的搜索算法，顺序引导不同边界框的回归，并使用结构化损失训练特定类别的CNN分类器，以明确惩罚定位不准确。Saurabh Gupta等人改进了具有语义丰富的图像和深度特征的RGB-D图像的目标检测[86]，并学习了一种新的深度图像地心嵌入方法，对每个像素进行编码。将目标检测器与超像素分类框架相结合，在语义场景分割任务中取得了良好的效果。Ouyang等人提出了一种可变形的深CNN（DeepID Net）[87]，它引入了一种新的变形约束池（def池）层，对不同对象部分的变形施加几何惩罚，并使用不同的设置对模型进行集成。Lenc等人[88]分析了提议生成在基于CNN的检测器中的作用，并试图用一个恒定且平凡的区域生成方案来取代这一阶段。这一目标是通过偏置采样来实现的，通过K-均值聚类来匹配地面真值边界框的统计信息。然而，需要更多的候选框才能获得与R-CNN类似的结果。</p> 
<h5><a id="2_SPP_102"></a>2） SPP网络</h5> 
<p>FC层必须采用固定大小的输入。这就是为什么R-CNN选择将每个地区的提案扭曲或裁剪成相同的大小。然而，对象可能部分存在于裁剪区域中，并且由于翘曲操作可能产生不需要的几何失真。这些内容丢失或失真将降低识别精度，尤其是当对象的比例变化时。</p> 
<p>为了解决这个问题，He等人考虑了空间金字塔匹配（SPM）[89]，[90]的理论，提出了一种新的CNN体系结构，名为SPP-net[64]。<strong>SPM采用多个从细到粗的尺度将图像划分为多个分区，并将量化的局部特征聚合为中级表示</strong>。</p> 
<p>用于目标检测的SPP网络的体系结构如图4所示。与R-CNN不同，SPP-net重用第5转换层(conv5)的特征映射，<strong>将任意大小的区域建议投影到固定长度的特征向量上</strong>。这些特征图的可重用性是由于特征图不仅涉及局部响应的强度，还与它们的空间位置有关[64]。<strong>在最后的转换层之后的层称为空间金字塔池化层</strong>(SPP层)。如果conv5中feature maps的个数为256。采用三层金字塔，SPP层后得到的每个区域提案的最终特征向量为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         256 
        
       
         ∗ 
        
       
         ( 
        
        
        
          1 
         
        
          2 
         
        
       
         + 
        
        
        
          2 
         
        
          2 
         
        
       
         + 
        
        
        
          4 
         
        
          2 
         
        
       
         ) 
        
       
         = 
        
       
         5376 
        
       
      
        256 * (1^{2} +2^{2}+4^2)=5376 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">2</span><span class="mord">5</span><span class="mord">6</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1.06411em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord">1</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.897438em; vertical-align: -0.08333em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1.06411em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord">4</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">5</span><span class="mord">3</span><span class="mord">7</span><span class="mord">6</span></span></span></span></span>。</p> 
<p>SPP-net不仅在不同区域提案的相应尺度下正确估计得到了更好的结果，同时，通过分担SPP层之前的计算代价，提高了测试周期内的检测效率。<br> <img src="https://images2.imgbox.com/08/85/F4XYtASK_o.png" alt=""></p> 
<h5><a id="3_Fast_RCNN_113"></a>3） Fast R-CNN</h5> 
<p>尽管SPP网络在准确性和效率方面都比R-CNN有了显著的提高，但它仍然存在一些明显的缺点。SPP网络采用与R-CNN几乎相同的多阶段管道，包括特征提取、网络微调、SVM训练和边界盒回归拟合。因此，仍然需要额外的存储空间费用。此外，SPP层之前的conv层无法使用[64]中介绍的微调算法进行更新。因此，深度网络的准确度下降并不令人惊讶。为此，Girshick[16]引入了一种<strong>基于分类和包围盒回归的多任务损失模型，并提出了一种新的CNN体系结构Fast R-CNN</strong>。</p> 
<p>Fast R-CNN的体系结构如图5所示。与SPP网络类似，整个图像使用conv层进行处理以生成特征地图。然后，使用感兴趣区域（RoI）池层从每个区域建议中提取固定长度的特征向量。RoI池层是SPP层的一个特例，SPP层只有一个金字塔级别。然后将每个特征向量送入一系列FC层，最后分支为两个同级输出层。一个输出层负责为所有C+1类（C object类加上一个“背景”类）生成softmax概率，另一个输出层用四个实数编码细化的边界框位置。这些过程中的所有参数（区域建议书生成除外）均通过端到端的多任务损失优化。</p> 
<p><img src="https://images2.imgbox.com/2d/20/afc5ZWX4_o.png" alt=""></p> 
<p>多任务LOSSL定义如下，用于联合训练分类和包围盒回归，</p> 
<p><img src="https://images2.imgbox.com/28/1d/CSQ7XyRG_o.png" alt=""><br> <img src="https://images2.imgbox.com/97/0c/YSQpZoet_o.png" alt=""></p> 
<p>为了加速快速R-CNN的发展，还需要另外两个技巧。一方面，如果训练样本（即ROI）来自不同的图像，则通过SPP层的反向传播将变得非常低效。Fast R-CNN对小批量进行分层采样，即首先随机采样图像，然后在每个图像中采样R/NRoIs，其中R表示ROI的数量。关键的是，在向前和向后过程中，来自同一图像的ROI共享计算和内存。另一方面，在向前传递期间，计算FC层花费了大量时间[16]。截断奇异值分解（SVD）[91]可用于压缩大型FC层并加速测试过程。</p> 
<p>在Fast R-CNN中，无论区域建议生成如何，所有网络层的训练都可以在一个阶段中处理，并且会丢失多个任务。它节省了额外的存储空间费用，并通过更合理的培训方案提高了准确性和效率。</p> 
<p>接下来分别讲了：</p> 
<h5><a id="4_Faster_RCNN_133"></a>4) Faster R-CNN</h5> 
<p>尽管尝试使用有偏采样生成候选框[88]，但最先进的目标检测网络主要依靠其他方法（如选择性搜索和Edgebox）来生成孤立区域建议的候选池。区域建议计算也是提高效率的瓶颈。为了解决这个问题，Ren等人引入了一个额外的区域建议网络（RPN）[18]，[92]，该网络通过与检测网络共享完整的图像conv特征以几乎无成本的方式工作。</p> 
<p>**RPN是通过完全卷积网络实现的，该网络能够同时预测对象边界和每个位置的分数。**与[78]类似，RPN采用任意大小的图像生成一组矩形对象。RPN在特定的conv层上运行，前面的层与目标检测网络共享。</p> 
<p><img src="https://images2.imgbox.com/01/4e/ERqgcLzv_o.png" alt=""></p> 
<p>随着更快的R-CNN的提出，基于区域建议的CNN目标检测体系结构可以真正<strong>以端到端的方式进行训练</strong>。此外，在PASCAL VOC 2007和2012上，GPU上的帧速率为5 FPS（每秒帧数），具有最先进的目标检测精度。然而，alternate training algorithm非常耗时，RPN生成类似对象的区域（包括背景）而不是对象实例，并且不擅长处理具有极端比例或形状的对象。</p> 
<h5><a id="5_RFCN_144"></a>5) R-FCN</h5> 
<p>通过RoI池层划分，一个流行的目标检测深度网络家族[16]、[18]由两个子网络组成：一个共享的完全卷积子网络（独立于RoI）和一个非共享的RoI子网络。这种分解起源于开创性的分类体系结构（如AlexNet[6]和VGG16[46]），它由一个卷积子网和几个由特定空间池层分隔的FC层组成。</p> 
<p>最近最先进的图像分类网络，如残差网（Resnet）[47]和GoogLeNets[45]，[93]，是完全卷积的。为了适应这些体系结构，需要构建一个完全卷积的目标检测网络，而不需要RoI子网络。然而，事实证明，使用这种解决方案，它的性能较差[47]。<strong>这种不一致性是由于在目标检测中尊重平移方差与在图像分类中增加平移不变性之间的矛盾造成的</strong>。换句话说，在图像分类中，移动图像内的对象应该是无差别的，而在边界框中对象的任何平移在对象检测中可能是有意义的。手动将RoI池层插入到卷积中<strong>可以破坏平移不变性</strong>，代价是额外的非共享区域层。因此，Li等人[65]提出了一种基于区域的完全卷积网络</p> 
<p>使用R-FCN，可以采用更强大的分类网络，通过共享几乎所有的层，在完全卷积的体系结构中完成目标检测，并在PASCAL VOC和Microsoft COCO[94]数据集上以每幅图像170ms的测试速度获得最新结果。</p> 
<h5><a id="6_FPN_152"></a>6) FPN</h5> 
<p>基于图像金字塔（特征化图像金字塔）的特征金字塔已广泛应用于许多目标检测系统中，<strong>以提高尺度不变性</strong>[24]，[64]（图7（a））。然而，训练时间和内存消耗迅速增加。为此，一些技术仅采用单个输入标度来表示高级语义，并提高标度变化的鲁棒性（图7（b）），并且在测试时构建图像金字塔，这导致训练/测试时间推断之间的不一致[16]，[18]。deep ConvNet中的网络内特征层次产生了不同空间分辨率的特征图，同时引入了由不同深度引起的较大语义差距（图7（c））。为了避免使用低级特征，pioneer works[71]、[95]通常从中间层开始构建金字塔，或者只对转换后的特征响应求和，而忽略了较高的层次特性层次结构的分辨率映射。</p> 
<p>由于特征金字塔可以从各个层次提取丰富的语义，并且可以在所有尺度下进行端到端的训练，所以可以在不牺牲速度和内存的情况下获得艺术状态表示。同时，FPN独立于主干CNN架构，可应用于目标检测的不同阶段（如区域建议生成）和许多其他计算机视觉任务（如实例分割）。</p> 
<h5><a id="7_Mask_RCNN_159"></a>7） Mask R-CNN</h5> 
<p>实例分割[96]是一项具有挑战性的任务，需要检测图像中的所有对象并分割每个实例（语义分割[97]）。这两项任务通常被视为两个独立的过程。多任务方案将产生虚假边缘，并在重叠实例上显示系统误差[98]。为了解决这个问题，与用于分类和边界盒回归的Faster R-CNN中的现有分支平行，mask R-CNN[67]添加了一个分支，以像素到像素的方式预测分割掩码（图8）。</p> 
<p>由于快速R-CNN中的核心操作RoI池对特征提取执行粗略的空间量化，因此在RoI和特征之间引入了不对齐。它对分类影响很小，因为它对小的翻译具有鲁棒性。然而，它对像素到像素掩模预测有很大的负面影响。为了解决这个问题，Mask R-CNN采用了一个简单且无量化的层，即ROIALLIGN，以忠实地保持显式的每像素空间对应。RoIAlign是通过用双线性插值代替RoI池的严格量化[99]来实现的，计算每个RoI箱中四个定期采样位置的输入特征的精确值。尽管简单，但这一看似微小的变化极大地提高了掩模精度，特别是在严格的本地化度量下。</p> 
<p>考虑到更快的R-CNN框架，掩模分支只增加了较小的计算负担，并且它与其他任务的合作为目标检测提供了补充信息。因此，<strong>Mask R-CNN易于实现，具有良好的实例分割和目标检测结果。总之，Mask R-CNN是一个灵活有效的实例级识别框架，只需稍加修改即可轻松推广到其他任务</strong>（如人体姿势估计[7][S4]）。</p> 
<h5><a id="8__166"></a>8） 多任务学习、多尺度表示和上下文建模</h5> 
<p>尽管Faster R-CNN通过数百个方案获得了有希望的结果，**但它在小尺寸目标检测和定位方面仍存在困难，主要原因是其特征图粗糙，并且在特定候选框中提供的信息有限。**这一现象在Microsoft COCO数据集上更为明显，该数据集由范围广泛的对象组成，原型图像较少，需要更精确的定位。为了解决这些问题，有必要使用多任务学习[100]、多尺度表示[95]和上下文建模[101]来完成目标检测，以结合来自多个来源的补充信息。</p> 
<p><strong>多任务学习</strong>: 从同一输入中学习多个相关任务的有用表示[102]，[103]。Brahmbhatt等人介绍了为对象分割和“填充”（如地面和水等无定形类别）而训练的conv特征，以指导小对象的精确对象检测（StuffNet）[100]。Dai等人[97]提出了三个网络的多任务网络级联，即类无关区域建议生成、像素级实例分割和区域实例分类。Li等人将弱监督对象分割线索和基于区域的对象检测合并到多阶段架构中，以充分利用学习到的分割特征[104]。</p> 
<p><strong>多尺度表示</strong>：将来自多个层的激活与跳过层连接相结合，以提供不同空间分辨率的语义信息[66]。Cai等人提出了MS-CNN[105]，以<strong>缓解具有多个尺度独立输出层的对象大小和感受野之间的不一致性</strong>。Yang等人研究了两种策略，即规模相关池（SDP）和分层级联拒绝分类器（CRC），以利用适当的规模相关conv特征[33]。Kong等人提出了超网，通过将不同分辨率的分层特征映射聚合和压缩到一个超网中，计算RPN和目标检测网络之间的共享特征。该论文已被IEEE神经网络和学习系统事务公开8统一空间[101]所接受。</p> 
<p><strong>上下文建模</strong>：<strong>通过利用不同支持区域和分辨率的ROI或其周围的特征来处理遮挡和局部相似性，从而提高检测性能</strong>[95]。Zhu等人提出了SEGDepm来利用对象分割，从而减少了对带有马尔可夫随机场的初始候选框的依赖[106]。Moysset等人利用4个方向2D LSTM[107]在不同的局部区域之间传递全局上下文，并通过局部参数共享减少可训练参数[108]。Zeng等人通过引入选通函数来控制不同支持区域之间的消息传输，提出了一种新的GBD网络[109]。</p> 
<p><strong>组合</strong>：<strong>将上述不同组件合并到同一模型中，以进一步提高检测性能</strong>。Gidaris等人提出了<strong>多区域CNN（MR-CNN）模型[110]，以捕获对象的不同方面、不同对象部分的不同外观和语义分割感知特征</strong>。为了获得上下文和多尺度表示，Bell等人<strong>利用空间递归神经网络[111]和跳跃池[101]利用RoI内外的信息[95]，提出了内外网（ION）</strong>。Zagoruyko等人通过对Fast R-CNN[112]进行三次修改，提出了多径结构，包括<strong>多尺度跳跃连接[95]、修改的中心凹结构[110]和对不同IoU损耗求和的新损耗函数。</strong></p> 
<h5><a id="9__179"></a>9） 基于深度学习的目标检测思考</h5> 
<p>除了上述方法之外，还有许多重要的因素需要持续改进。</p> 
<p>标志对象和背景示例的数量之间存在很大的不平衡。为了解决这个问题，Shrivastava等人<strong>提出了一种有效的在线挖掘算法（OHEM）[113]，用于自动选择硬示例，从而实现更有效的训练</strong>。Ren等人没有专注于特征提取，而是对目标分类器进行了详细分析[114]，发现仔细构建深度卷积的每区域分类器对于目标检测特别重要，尤其是对于Resnet[47]和GoogLeNets[45]。用于对象检测的传统CNN框架不擅长处理显著的<strong>尺度变化、遮挡或截断</strong>，尤其是当仅涉及2D对象检测时。为了解决这个问题，Xiang等人提出了一种新的子类别感知区域建议网络[60]，该网络利用与对象姿势相关的子类别信息指导区域建议的生成，并联合优化对象检测和子类别分类。Ouyang等人发现，<strong>来自不同类别的样本遵循长尾分布</strong>[115]，这表明<strong>样本数量不同的不同类别对特征学习有不同程度的影响</strong>。为此，首先将对象聚类成视觉上相似的类别组，然后采用<strong>分层特征学习方案</strong>分别学习每个类别的深度表示。为了最大限度地降低计算成本并实现最先进的性能，Hong等人根据“深而薄”的设计原则，遵循Fast R-CNN的管道，提出了PVANET的体系结构[116]，该体系结构采用了一些构建块，包括串联ReLU[117]、Inception[45]和HyperNet[101]为了减少多尺度特征提取的费用，并通过批量标准化[43]、剩余连接[47]和基于平台检测的学习率调度来训练网络[47]。PVANET达到了最先进的性能，可以在Titan X GPU（21 FPS）上实时处理。</p> 
<h4><a id="B_RegressionClassification_Based_Framework_184"></a>B. Regression/Classification Based Framework：基于回归/分类的框架</h4> 
<p>基于区域建议的框架由几个相关的阶段组成，包括<strong>区域建议生成、CNN特征提取、分类和包围盒回归</strong>，这些阶段通常是单独训练的。即使在最近的端到端模块R-CNN中，仍然需要进行替代训练，以获得RPN和检测网络之间的共享卷积参数。因此，处理不同组件所花费的时间成为实时应用程序的瓶颈。</p> 
<p>基于全局回归/分类的一阶段框架，直接从图像像素映射到边界框坐标和类概率，可以减少时间开销。我们首先回顾了一些先驱CNN模型，然后重点介绍了两个重要的框架，即You only look once（YOLO）[17]和Single Shot MultiBox Detector（SSD）[71]。</p> 
<h5><a id="1_Pioneer_Works_189"></a>1） Pioneer Works</h5> 
<p>在YOLO和SSD之前，许多研究人员已经尝试将目标检测建模为回归或分类任务。</p> 
<p>Szegedy等人将目标检测任务表述为基于DNN的回归[118]，为测试图像生成二值掩码，并通过简单的边界框推断提取检测。然而，该模型在处理<strong>重叠对象方面存在困难</strong>，并且直接上采样生成的边界框还远远不够完美。Pinheiro等人提出了一个CNN模型，该模型有两个分支：一个生成类无关的分割掩码，另一个预测以对象为中心的给定面片的可能性[119]。推理是有效的，因为类分数和分割可以在一个单一的模型中获得，并且大多数CNN操作是共享的。Erhan et al.提出了基于回归的多框方法，以产生评分类不可知区域建议[68]，[120]。引入统一的损失，使多个分量的局部化和可信度都有偏差，从而预测类不可知边界框的坐标。但是，在最后一层中会引入大量附加参数。Yoo等人采用了一种迭代分类方法来处理目标检测，并提出了一种令人印象深刻的端到端CNN体系结构，名为<strong>AttentionNet</strong>[69]。从图像的左上角（TL）和右下角（BR）开始，AttentionNet通过生成量化弱方向指向目标对象，并通过迭代预测集合收敛到精确的对象边界框。然而，当使用渐进式两步程序处理多个类别时，该模型变得非常低效。</p> 
<h5><a id="2_YOLO_195"></a>2） YOLO</h5> 
<p>Redmon等人[17]提出了一种称为YOLO的新框架，该框架利用<strong>整个最顶端的特征映射来预测多个类别和边界框的可信度</strong>。图9展示了YOLO的基本思想。YOLO将输入图像划分为一个S×S网格，每个网格单元负责预测以该网格单元为中心的对象。<strong>每个网格单元预测边界框及其相应的置信度分数</strong>。同时，无论框的数量如何，也应在每个网格单元中预测条件类概率（pr（Classi | Object））。应注意，仅计算包含对象的栅格单元的贡献。<br> <img src="https://images2.imgbox.com/c1/0e/MpBDZLUv_o.png" alt=""></p> 
<p>在测试时，通过将单个盒子置信度预测与条件类别概率相乘，获得每个盒子的特定类别置信度分数。</p> 
<p>请注意，只有当网格单元中存在对象时，损失函数才会惩罚分类错误。类似地，当预测器“负责”地面真值框（即达到该网格单元中任何预测器的最高IoU）时，边界框坐标错误将受到惩罚。</p> 
<p>YOLO由24个conv层和2个FC层组成，其中一些conv层构建了初始模块的集合，其中包含1×1还原层和3×3 conv层。该网络可以以45 FPS的速度实时处理图像，简化版Fast YLO可以达到155 FPS，效果优于其他实时检测器。此外，YOLO在背景上产生的误报更少，这使得与Fast R-CNN的合作成为可能。后来在[72]中提出了一个改进版本<strong>YOLOv2，它采用了几种令人印象深刻的策略，如BN, anchor boxes、维度集群和多尺度训练</strong>。</p> 
<h5><a id="3_SSD_205"></a>3） SSD</h5> 
<p><strong>YOLO难以处理成组的小对象，这是由于对边界框预测施加了强大的空间约束</strong>[17]。同时，YOLO努力推广到具有新的/不寻常的纵横比/配置的对象，并由于多次下采样操作而产生相对粗糙的特征。</p> 
<p>针对这些问题，Liu等人提出了单激发多盒检测器（SSD）[71]，其灵感来源于多盒[68]、RPN[18]和多尺度表示[95]中采用的锚。给定一个特定的特征映射，而不是YOLO中采用的固定网格，<strong>SSD利用一组具有不同纵横比和比例的默认锚定框来离散边界框的输出空间</strong>。<strong>为了处理不同大小的对象，该网络融合了来自多个具有不同分辨率的特征地图的预测</strong>。</p> 
<p>SSD的体系结构如图10所示。考虑到VGG16主干架构，SSD向网络末端添加了几个功能层，这些功能层负责预测到具有不同比例和纵横比的默认框的偏移量及其相关置信度。该网络使用定位损失（例如平滑L1）和置信度损失（例如Softmax）的加权和进行训练，这类似于（1）。通过在多尺度细化边界盒上进行NMS得到最终检测结果。</p> 
<p>结合硬负挖掘、数据扩充和大量精心选择的默认锚定，SSD在PASCAL VOC和COCO的准确性方面明显优于速度更快的R-CNN，同时速度快了三倍。SSD300（输入图像大小为300×300）以59 FPS的速度运行，比YOLO更准确、更高效。然而，<strong>SSD不擅长处理小对象，可以通过采用更好的特征提取主干（例如ResNet101）、添加带跳过连接的反褶积层以引入额外的大规模上下文[73]和设计更好的网络结构（例如干块和密集块）[74]来缓解这种情况。</strong></p> 
<h4><a id="C_Experimental_Evaluation_214"></a>C. Experimental Evaluation</h4> 
<p>我们在三个基准数据集上比较了各种目标检测方法，包括PASCAL VOC 2007[25]、PASCAL VOC 2012[121]和Microsoft COCO[94]。评估的方法包括R-CNN[15]、SPP-net[64]、Fast R-CNN[16]、NOC[114]、Bayes[85]、MR-CNN&amp;S-CNN[105]、Faster R-CNN[18]、HyperNet[101]、ION[95]、MSGR[104]、StuffNet[100]、SSD300[71]、SSD512[71]、OHEM[113]、SDP CRC[33]、GCNN[70]、SubCNN[60]、GBD-net[109]、PV-ANET[116]、YOLO[17]、YOLOv2[72]、R-FCN[65]、FPN[66]、FPN[66]，掩码R-CNN[67]，DSSD[73]和DSOD[74]。如果没有具体说明所采用的框架，则使用的模型是在1000-way ImageNet分类任务上预训练的VGG16 [46]。由于论文篇幅的限制，我们仅提供表1中突出体系结构的概述，包括建议、学习方法、损失函数、编程语言和平台。原始论文中缺少详细的实验设置。除了检测精度的比较外，还提供了另一个比较，以评估它们在PASCAL VOC 2007上的测试消耗。<br> <img src="https://images2.imgbox.com/f5/ed/IKR3J49S_o.png" alt=""></p> 
<h5><a id="1_PASCAL_VOC_20072012_219"></a>1） PASCAL VOC 2007/2012</h5> 
<p>PASCAL VOC 2007和2012数据集包括20个类别。评估术语为每个类别的平均精度（AP）和所有20个类别的平均平均精度（mAP）。表二和表三列出了比较结果，从中可以得出以下:<br> <img src="https://images2.imgbox.com/e2/b9/nCoGgqKx_o.png" alt=""></p> 
<ul><li>如果采用适当的方法，更强大的主干CNN模型肯定可以提高目标检测性能（R-CNN与AlexNet、R-CNN与VGG16、SPP-net与ZF-net[122]之间的比较）。(<strong>强调主干网的重要</strong>)</li><li>随着SPP层（SPP-net）、端到端多任务体系结构（FRCN）和RPN（fasterrcnn）的引入，目标检测性能逐渐得到明显提高。</li><li>由于大量的可训练参数，为了获得多层次的鲁棒性特征，<strong>数据扩充</strong>对于基于深度学习的模型非常重要（Faster R-CNN with ‘07’ ,‘07+12’ and ‘07+12+coco’）。</li><li>除了基本模型外，还有许多其他因素影响目标检测性能，如多尺度和多区域特征提取（如MR-CNN）、改进的分类网络（如NOC）、其他相关任务的附加信息（如StuffNet、HyperNet）、多尺度表示（如离子）和hard negative samples（如OHEM）的开采。</li><li>由于YOLO不擅长生成高IoU的对象定位，因此它在VOC 2012上的结果非常差。然而，利用来自Fast R-CNN（<strong>YOLO+ FRCN</strong>）的补充信息和其他策略（如<strong>锚框、BN和细粒度特征</strong>）的帮助，定位错误得到纠正（YLOV2）。</li><li><strong>通过结合许多最新的技巧并将整个网络建模为完全卷积网络</strong>，R-FCN实现了比其他方法更明显的检测性能改进。</li></ul> 
<h5><a id="2_MicrosoftCoCo_229"></a>2） MicrosoftCoCo</h5> 
<p>MicrosoftCoCo由300000个完全分割的图像组成，其中每个图像平均有来自80个类别的7个对象实例。由于<strong>有许多不太具有标志性的对象</strong>，<strong>具有广泛的比例范围，并且对对象定位的要求更严格</strong>，<strong>因此该数据集比PASCAL 2012更具挑战性</strong>。<strong>在不同的IOU程度和不同的目标大小下，通过计算AP来评估目标检测性能</strong>。结果见表四。<br> <img src="https://images2.imgbox.com/8c/0d/LQMzWapJ_o.png" alt=""></p> 
<ul><li><strong>多尺度训练和测试有助于提高目标检测性能，提供不同分辨率下的附加信息</strong>。<strong>FPN和DSSD</strong>为构建特征金字塔以实现多尺度表示提供了更好的方法。来自其他相关任务的补充信息也有助于准确的目标定位(Mask R-CNN与实例分割任务)。</li><li>总的来说，<strong>基于区域建议的方法，如Faster R-CNN和R-FCN，比基于回归/分类的方法（即YOLO和SSD）表现得更好</strong>，因为<strong>基于回归/分类的方法会产生很多定位错误</strong>。</li><li><strong>上下文建模有助于定位小对象</strong>，它通过查询附近的对象和环境（GBD网络和多路径）提供附加信息。</li><li>由于存在大量的非标准小对象，该数据集的结果比VOC 2007/2012差很多。随着其他强大框架(如<strong>ResNeXt</strong>[123])和有用策略(如<strong>多任务学习</strong>[67]，[124])的引入，性能可以得到提高。</li><li><strong>DSOD从头开始训练的成功强调了网络设计的重要性</strong>，以释放对相关任务和大量注释样本的完美预训练分类器的需求。</li></ul> 
<h5><a id="3_Timing_Analysis_239"></a>3) Timing Analysis</h5> 
<p>除了用CPU处理的“SS”外，其他与CNN相关的程序都在GPU上进行评估。从表五中，我们可以得出如下结论。</p> 
<ul><li>通过在共享特征映射（SPP-net）上计算CNN特征，测试消耗大大减少。通过统一多任务学习（FRCN）和删除额外的区域建议生成阶段（Faster R-CNN），测试时间进一步缩短。使用SVD[91]（PAVNET和FRCN）压缩FC层的参数也很有帮助。</li><li><strong>提取多尺度特征和上下文信息（ION和MR-RCNN&amp;SRCNN）需要额外的测试时间</strong></li><li><strong>训练更复杂更深入的网络（针对VGG16的ResNet101）需要更多的时间，通过在共享的完全卷积层（FRCN）中添加尽可能多的层，可以减少时间消耗</strong>。</li><li><strong>基于回归的模型通常可以实时处理，但与基于区域建议的模型相比，其精度有所下降</strong>。此外，<font color="red">通过引入其他技巧(PVANET)，如BN[43]、residual connections[123]，可以将基于区域提议的模型修改为实时系统</font>。</li></ul> 
<hr> 
<h3><a id="4__251"></a>4. 显著目标检测</h3> 
<h3><a id="5__252"></a>5. 人脸检测</h3> 
<h3><a id="6__253"></a>6. 行人检测</h3> 
<p>分析其数据集存在的问题，结合一些方法，并对其进行了验证。<br> <strong>此部分略。。。。。。</strong></p> 
<hr> 
<h3><a id="7__263"></a>7. 展望未来的方向和任务</h3> 
<p>尽管目标检测技术发展迅速，取得了可喜的进展，但仍有许多有待进一步研究的问题。</p> 
<p><strong>第一种是小目标检测</strong>，如COCO数据集和人脸检测任务中的小目标检测。<strong>为了提高局部遮挡下小目标的定位精度，需要从以下几个方面修改网络结构</strong>：</p> 
<ul><li> <p><strong>多任务联合优化和多模态信息融合</strong>：由于目标检测内外不同任务之间的相关性，许多研究人员已经对多任务联合优化进行了研究[16][18]。但是，除了SUB中提到的任务之外。在III-A8中，需要考虑目标检测的不同子任务的特征（例如突出目标检测中的超像素语义分割），并将多任务优化扩展到其他应用，如实例分割[66]、多目标跟踪[202]和多人姿势估计[S4]。此外，给定特定应用，来自不同模式的信息，例如文本[212]、热数据[205]和图像[65]可以融合在一起，以实现更具鉴别性的网络。</p> </li><li> <p><strong>尺度自适应</strong>：目标通常以不同的尺度存在，这在人脸检测和行人检测中更为明显。<strong>为了提高对尺度变化的鲁棒性，需要训练尺度不变、多尺度或尺度自适应检测器</strong>。</p> 
  <ul><li>对于<strong>尺度不变检测器</strong>，<strong>更强大的主干架构（如ResNext[123]）、负样本挖掘[113]、反向连接[213]和子类别建模[60]都是有益的</strong>。</li><li>对于<strong>多尺度探测器</strong>，<strong>产生多尺度特征图的FPN[66]和缩小小对象和具有低成本架构的大对象之间表示差异的生成性对抗网络[214]都提供了生成有意义特征金字塔的见解</strong>。</li><li><strong>对于尺度自适应检测器，结合知识图[215]、注意机制[216]、级联网络[180]和尺度分布估计[171]来自适应检测对象是有用的。</strong></li></ul> </li><li> <p><strong>空间相关性和上下文建模</strong>：空间分布在目标检测中起着重要的作用。因此，区域建议生成和网格回归被用来获得可能的目标位置。但是，多个提案和对象类别之间的相关性被忽略。此外，R-FCN中的位置敏感得分图放弃了全局结构信息。为了解决这些问题，我们可以参考不同子集选择[217]和顺序推理任务[218]以获得可能的解决方案。掩盖突出部分并以联合学习的方式将其与全球结构结合起来也是有意义的[219]。</p> </li></ul> 
<p>第二个是随着大规模图像和视频数据的出现，减轻人工负担，<strong>实现实时目标检测</strong>。可以考虑以下三个方面：</p> 
<ul><li><strong>级联网络</strong>。在级联网络中，探测器级联构建在不同的阶段或层[180]、[220]。容易区分的示例在浅层被拒绝，以便后期的特征和分类器能够在前一阶段决策的帮助下处理更困难的样本。然而，当前的级联是以贪婪的方式构建的，在训练新的级联时，级联中的前几级是固定的。因此，不同CNN的优化是孤立的，这强调了CNN级联端到端优化的必要性。同时，与现有层构建上下文相关联的级联网络也是一个值得关注的问题。</li><li><strong>无监督和弱监督学习</strong>。手动绘制大量边界框非常耗时。为了减轻这一负担，可以将<strong>语义优先[55]、无监督对象发现[221]、多实例学习[222]和深度神经网络预测[47]结合起来</strong>，以充分利用图像级监督，将对象类别标记分配给相应的对象区域，并细化对象边界。此外，<strong>弱注释</strong>（例如，点击中心注释[223]）也有助于通过适度的注释工作实现高质量的检测器，特别是在移动平台的帮助下。</li><li>网络优化。在特定的应用和平台下，选择最优的检测体系结构[116]、[224]，在速度、内存和准确性之间取得平衡是非常重要的。然而，尽管检测精度降低，但学习参数数量较少的紧凑模型更有意义[209]。<strong>通过引入更好的预训练方案[225]、knowledge distillation [226] and hint learning[227]，可以缓解这种情况</strong>。DSOD还为从无到有的训练提供了一个有希望的指导方针，<strong>以弥合不同图像源和任务之间的差距</strong>[74]。</li></ul> 
<p>第三个是<strong>扩展2D目标检测的典型方法，以适应3D目标检测和视频目标检测，满足自主驾驶、智能交通和智能监控的要求</strong>。</p> 
<ul><li><strong>3D目标检测</strong>。随着3D传感器（如激光雷达和照相机）的应用，可以利用额外的深度信息更好地理解2D图像，并将图像级知识扩展到现实世界。然而，这些3D感知技术很少以在检测到的对象周围放置正确的3D边界框为目标。为了获得更好的边界结果，多视图表示[181]和3D提案网络[228]可以提供一些指南，以借助惯性传感器（加速计和陀螺仪）对深度信息进行编码[229]。</li><li>视频目标检测。跨不同框架的时间信息在理解不同对象的行为方面起着重要作用。然而，视频中的对象外观退化（如运动模糊和视频散焦）会影响精度，并且网络通常不进行端到端的训练。为此，应考虑时空管[230]、光流[199]和LSTM[107]从根本上模拟连续帧之间的对象关联。</li></ul> 
<h3><a id="8_CONCLUSION_285"></a>8. CONCLUSION</h3> 
<p>由于其强大的学习能力以及在<strong>处理遮挡、尺度变换和背景切换</strong>等方面的优势，基于深度学习的目标检测成为近年来的研究热点。本文详细回顾了基于深度学习的目标检测框架，这些框架处理不同的子问题，如occlusion, clutter和低分辨率，并对R-CNN进行了不同程度的修改。审查从通用对象检测管道开始，该管道为其他相关任务提供基础架构。然后，简要回顾了另外三个常见的任务，即显著目标检测、人脸检测和行人检测。最后，我们提出了几个有希望的未来方向，以获得对目标检测前景的透彻理解。这篇综述对神经网络和相关学习系统的发展也很有意义，它为未来的发展提供了宝贵的见解和指导。<br> <img src="https://images2.imgbox.com/85/73/xAHHGrDu_o.png" alt=""></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b195da3eef2fcddadb308ad9fbbf26b5/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">c语言 求24小时制时间间隔</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a6503edb2b2f2ff133a780dcf94c79c4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">sv-semaphore</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>