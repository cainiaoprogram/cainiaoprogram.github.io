<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>pytorch实现DQN - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="pytorch实现DQN" />
<meta property="og:description" content="DQN（Deep Q Network）
DQN可以使计算机玩游戏比人类厉害，Q-learning和Deep-learning的融合
当游戏的状态和动作多种多样，多到天文数字的时候，如果用表格来存储的话，显然不切实际，如果用到神经网络就是
给出一个状态和动作，通过神经网络，得到这个状态和动作下的value；也可以经过一个状态，通过神经网络，得到不止一个对应的value
s &#43; op —&gt; value1
s —&gt; value1,value2,value3…
DQN在原来的Q网络的基础上又引入了一个target Q网络，即用来计算target的网络。它和Q网络结构一样，初始的权重也一样，只是Q网络每次迭代都会更新，而target Q网络是每隔一段时间才会更新。
实验之前首先import gym是一个实验模拟模块，可以可视化一些深度学习过结果，直接在prompt里面pip install gym
import torch import torch.nn as nn import torch.nn.functional as F import numpy as np import gym #超参数 BATCH_SIZE = 32 LR = 0.01 EPSILON = 0.9 #随机选取的概率，如果概率小于这个随机数，就采取greedy的行为 GAMMA = 0.9 TARGET_REPLACE_ITER = 100 MEMORY_CAPACITY = 2000 #导入openAI gym实验的模拟场所，&#39;CartPole-v0&#39;表示倒立摆的实验 env = gym.make(&#39;CartPole-v0&#39;) env = env.unwrapped N_ACTIONS = env.action_space.n #小车的动作 N_STATES = env." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/9793b7886b06d25ee07bbe48e678deed/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-07-15T22:42:48+08:00" />
<meta property="article:modified_time" content="2020-07-15T22:42:48+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">pytorch实现DQN</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atelier-sulphurpool-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>DQN（Deep Q Network）</p> 
<p>DQN可以使计算机玩游戏比人类厉害，Q-learning和Deep-learning的融合</p> 
<p>当游戏的状态和动作多种多样，多到天文数字的时候，如果用表格来存储的话，显然不切实际，如果用到神经网络就是</p> 
<p>给出一个状态和动作，通过神经网络，得到这个状态和动作下的value；也可以经过一个状态，通过神经网络，得到不止一个对应的value</p> 
<p>s + op —&gt; value1<br> s —&gt; value1,value2,value3…</p> 
<p>DQN在原来的Q网络的基础上又引入了一个target Q网络，即用来计算target的网络。它和Q网络结构一样，初始的权重也一样，只是Q网络每次迭代都会更新，而target Q网络是每隔一段时间才会更新。</p> 
<p>实验之前首先import gym是一个实验模拟模块，可以可视化一些深度学习过结果，直接在prompt里面<code>pip install gym</code></p> 
<p><img src="https://images2.imgbox.com/d4/4a/RM49hGWe_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> gym


<span class="token comment">#超参数</span>
BATCH_SIZE <span class="token operator">=</span> <span class="token number">32</span>
LR <span class="token operator">=</span> <span class="token number">0.01</span>
EPSILON <span class="token operator">=</span> <span class="token number">0.9</span> <span class="token comment">#随机选取的概率，如果概率小于这个随机数，就采取greedy的行为</span>
GAMMA <span class="token operator">=</span> <span class="token number">0.9</span>
TARGET_REPLACE_ITER <span class="token operator">=</span> <span class="token number">100</span>
MEMORY_CAPACITY <span class="token operator">=</span> <span class="token number">2000</span>
<span class="token comment">#导入openAI gym实验的模拟场所，'CartPole-v0'表示倒立摆的实验</span>
env <span class="token operator">=</span> gym<span class="token punctuation">.</span>make<span class="token punctuation">(</span><span class="token string">'CartPole-v0'</span><span class="token punctuation">)</span>
env <span class="token operator">=</span> env<span class="token punctuation">.</span>unwrapped
N_ACTIONS <span class="token operator">=</span> env<span class="token punctuation">.</span>action_space<span class="token punctuation">.</span>n <span class="token comment">#小车的动作</span>
N_STATES <span class="token operator">=</span> env<span class="token punctuation">.</span>observation_space<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token comment">#实验环境的状态</span>
ENV_A_SHAPE <span class="token operator">=</span> <span class="token number">0</span> <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>env<span class="token punctuation">.</span>action_space<span class="token punctuation">.</span>sample<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">)</span> <span class="token keyword">else</span> env<span class="token punctuation">.</span>action_space<span class="token punctuation">.</span>sample<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape



<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>N_STATES<span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>   <span class="token comment"># initialization</span>
        self<span class="token punctuation">.</span>out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">50</span><span class="token punctuation">,</span> N_ACTIONS<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>out<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>   <span class="token comment"># initialization</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        actions_value <span class="token operator">=</span> self<span class="token punctuation">.</span>out<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> actions_value


<span class="token keyword">class</span> <span class="token class-name">DQN</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">#DQN是Q-Leaarning的一种方法，但是有两个神经网络，一个是eval_net一个是target_net</span>
        <span class="token comment">#两个神经网络相同，参数不同，是不是把eval_net的参数转化成target_net的参数，产生延迟的效果</span>
        self<span class="token punctuation">.</span>eval_net<span class="token punctuation">,</span>self<span class="token punctuation">.</span>target_net <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>Net<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>learn_step_counter <span class="token operator">=</span> <span class="token number">0</span> <span class="token comment">#学习步数计数器</span>
        self<span class="token punctuation">.</span>memory_counter <span class="token operator">=</span> <span class="token number">0</span> <span class="token comment">#记忆库中位值的计数器</span>
        self<span class="token punctuation">.</span>memory <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>MEMORY_CAPACITY<span class="token punctuation">,</span>N_STATES <span class="token operator">*</span> <span class="token number">2</span> <span class="token operator">+</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">#初始化记忆库</span>
        <span class="token comment">#记忆库初始化为全0，存储两个state的数值加上一个a(action)和一个r(reward)的数值</span>
        self<span class="token punctuation">.</span>optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>eval_net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr <span class="token operator">=</span> LR<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>loss_func <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#优化器和损失函数</span>
        
    
    <span class="token comment">#接收环境中的观测值，并采取动作</span>
    <span class="token keyword">def</span> <span class="token function">choose_action</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">#x为观测值</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> EPSILON<span class="token punctuation">:</span>
            <span class="token comment">#随机值得到的数有百分之九十的可能性&lt;0.9,所以该if成立的几率是90%</span>
            <span class="token comment">#90%的情况下采取actions_value高的作为最终动作</span>
            actions_value <span class="token operator">=</span> self<span class="token punctuation">.</span>eval_net<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            action <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>actions_value<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
            action <span class="token operator">=</span> action<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">if</span> ENV_A_SHAPE <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">else</span> action<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>ENV_A_SHAPE<span class="token punctuation">)</span> <span class="token comment"># return the argmax index</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment">#其他10%采取随机选取动作</span>
            action <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span>N_ACTIONS<span class="token punctuation">)</span> <span class="token comment">#从动作中选一个动作</span>
            action <span class="token operator">=</span> action <span class="token keyword">if</span> ENV_A_SHAPE <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">else</span> action<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>ENV_A_SHAPE<span class="token punctuation">)</span>
        <span class="token keyword">return</span> action    

    
    <span class="token comment">#记忆库，存储之前的记忆，学习之前的记忆库里的东西</span>
    <span class="token keyword">def</span> <span class="token function">store_transition</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>s<span class="token punctuation">,</span>a<span class="token punctuation">,</span>r<span class="token punctuation">,</span>s_<span class="token punctuation">)</span><span class="token punctuation">:</span>
        transition <span class="token operator">=</span> np<span class="token punctuation">.</span>hstack<span class="token punctuation">(</span><span class="token punctuation">(</span>s<span class="token punctuation">,</span> <span class="token punctuation">[</span>a<span class="token punctuation">,</span> r<span class="token punctuation">]</span><span class="token punctuation">,</span> s_<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 如果记忆库满了, 就覆盖老数据</span>
        index <span class="token operator">=</span> self<span class="token punctuation">.</span>memory_counter <span class="token operator">%</span> MEMORY_CAPACITY
        self<span class="token punctuation">.</span>memory<span class="token punctuation">[</span>index<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> transition
        self<span class="token punctuation">.</span>memory_counter <span class="token operator">+=</span> <span class="token number">1</span>
    
    <span class="token keyword">def</span> <span class="token function">Learn</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
         <span class="token comment"># target net 参数更新,每隔TARGET_REPLACE_ITE更新一下</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>learn_step_counter <span class="token operator">%</span> TARGET_REPLACE_ITER <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>target_net<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>self<span class="token punctuation">.</span>eval_net<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>learn_step_counter <span class="token operator">+=</span> <span class="token number">1</span>
        <span class="token comment">#targetnet是时不时更新一下，evalnet是每一步都更新</span>

        <span class="token comment"># 抽取记忆库中的批数据</span>
        sample_index <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>MEMORY_CAPACITY<span class="token punctuation">,</span> BATCH_SIZE<span class="token punctuation">)</span>
        b_memory <span class="token operator">=</span> self<span class="token punctuation">.</span>memory<span class="token punctuation">[</span>sample_index<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> 
        <span class="token comment">#打包记忆，分开保存进b_s，b_a，b_r，b_s</span>
        b_s <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span>b_memory<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>N_STATES<span class="token punctuation">]</span><span class="token punctuation">)</span>
        b_a <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>b_memory<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> N_STATES<span class="token punctuation">:</span>N_STATES<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        b_r <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span>b_memory<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> N_STATES<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">:</span>N_STATES<span class="token operator">+</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        b_s_ <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span>b_memory<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span>N_STATES<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        <span class="token comment"># 针对做过的动作b_a, 来选 q_eval 的值, (q_eval 原本有所有动作的值)</span>
        q_eval <span class="token operator">=</span> self<span class="token punctuation">.</span>eval_net<span class="token punctuation">(</span>b_s<span class="token punctuation">)</span><span class="token punctuation">.</span>gather<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> b_a<span class="token punctuation">)</span>  <span class="token comment"># shape (batch, 1)</span>
        q_next <span class="token operator">=</span> self<span class="token punctuation">.</span>target_net<span class="token punctuation">(</span>b_s_<span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>     <span class="token comment"># q_next 不进行反向传递误差, 所以 detach</span>
        q_target <span class="token operator">=</span> b_r <span class="token operator">+</span> GAMMA <span class="token operator">*</span> q_next<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>   <span class="token comment"># shape (batch, 1)</span>
        loss <span class="token operator">=</span> self<span class="token punctuation">.</span>loss_func<span class="token punctuation">(</span>q_eval<span class="token punctuation">,</span> q_target<span class="token punctuation">)</span>

        <span class="token comment"># 计算, 更新 eval net</span>
        self<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#误差反向传播</span>
        self<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
dqn <span class="token operator">=</span> DQN<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'\nCollection experience...'</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> i_episode <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">400</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    s <span class="token operator">=</span> env<span class="token punctuation">.</span>reset<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#得到环境的反馈，现在的状态</span>
    ep_r <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
        env<span class="token punctuation">.</span>render<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#环境渲染，可以看到屏幕上的环境</span>
        a <span class="token operator">=</span> dqn<span class="token punctuation">.</span>choose_action<span class="token punctuation">(</span>s<span class="token punctuation">)</span> <span class="token comment">#根据dqn来接受现在的状态，得到一个行为</span>
        s_<span class="token punctuation">,</span>r<span class="token punctuation">,</span>done<span class="token punctuation">,</span>info <span class="token operator">=</span> env<span class="token punctuation">.</span>step<span class="token punctuation">(</span>a<span class="token punctuation">)</span> <span class="token comment">#根据环境的行为，给出一个反馈</span>

        
        <span class="token comment"># 修改 reward, 使 DQN 快速学习</span>
        x<span class="token punctuation">,</span> x_dot<span class="token punctuation">,</span> theta<span class="token punctuation">,</span> theta_dot <span class="token operator">=</span> s_
        r1 <span class="token operator">=</span> <span class="token punctuation">(</span>env<span class="token punctuation">.</span>x_threshold <span class="token operator">-</span> <span class="token builtin">abs</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> env<span class="token punctuation">.</span>x_threshold <span class="token operator">-</span> <span class="token number">0.8</span>
        r2 <span class="token operator">=</span> <span class="token punctuation">(</span>env<span class="token punctuation">.</span>theta_threshold_radians <span class="token operator">-</span> <span class="token builtin">abs</span><span class="token punctuation">(</span>theta<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> env<span class="token punctuation">.</span>theta_threshold_radians <span class="token operator">-</span> <span class="token number">0.5</span>
        r <span class="token operator">=</span> r1 <span class="token operator">+</span> r2
        
        dqn<span class="token punctuation">.</span>store_transition<span class="token punctuation">(</span>s<span class="token punctuation">,</span>a<span class="token punctuation">,</span>r<span class="token punctuation">,</span>s_<span class="token punctuation">)</span> <span class="token comment">#dqn存储现在的状态，行为，反馈，和环境导引的下一个状态</span>
        
        ep_r <span class="token operator">+=</span> r
        
        <span class="token keyword">if</span> dqn<span class="token punctuation">.</span>memory_counter <span class="token operator">&gt;</span> MEMORY_CAPACITY<span class="token punctuation">:</span>
            dqn<span class="token punctuation">.</span>learn<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> done<span class="token punctuation">:</span>
                <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Ep: '</span><span class="token punctuation">,</span> i_episode<span class="token punctuation">,</span>
                      <span class="token string">'| Ep_r: '</span><span class="token punctuation">,</span> <span class="token builtin">round</span><span class="token punctuation">(</span>ep_r<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        
        <span class="token keyword">if</span> done<span class="token punctuation">:</span>
            <span class="token keyword">break</span>
        
        s <span class="token operator">=</span> s_ <span class="token comment"># 现在的状态赋值到下一个状态上去</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/c4/b7/RWlbGSmD_o.png" alt="在这里插入图片描述"><br> 刚开始每次都震动很大，回合结束，到后来慢慢抖动变缓和，说明已经可以立起来了，但是还不能保持在中间，会移动走</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/fb159f034940a5dbe3e85ffcb073bf16/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【面试】---【MySQL】</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/638eefccd4bddda6167b19ef94f85b47/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">事务特点以及分布式事务</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>