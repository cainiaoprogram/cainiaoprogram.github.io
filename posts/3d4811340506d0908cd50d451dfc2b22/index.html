<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>机器学习样本不平衡处理方法 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="机器学习样本不平衡处理方法" />
<meta property="og:description" content="数据--样本不平衡处理
不同类别的样本量差异大，或少量样本代表了业务的关键数据，属于样本不平衡的情况，要求对少量样本的模式有很好的学习。
大数据情况下：整体数据规模大，小样本类别占比少，但是小样本也覆盖大部分或全部特征；
小数据情况下，整体数据规模小，小样本类别也少，导致样本特征分布不均匀。
一般比例差异超过10倍就要注意，超过20倍就得处理
工程方法中，通常从三个方面处理样本不均衡：
扩大数据集，但是在扩大小样本比例的时候，大样本也在增加，通过获取更多少量样本数据，尽可能扩大这些少量样本的数据集。
1、数据相关处理：欠采样和过采样，欠采样又称为下采样，过采样又称为上采样。实现简单，速度快
对大样本（超过1w或10w）进行欠采样，放弃一部分大样本数据；
对小样本（小于1w）进行过采样，相当于添加部分小样本的副本。
上采样 通过增加分类中少数样本的数量实现样本均衡，通常做法是在少数样本中加入随机噪声、干扰数据或通过一定规则产生新的合成样本。
ADASYN采样
样本较少的类生成合成数据，其生成的数据与更容易学习的样本相比，更难学习，.基本思想是根据学习难度不同，对不同少数类样本使用加权分布，其中，更难学习的少数类样本比那些更容易学习的少数类样本要产生更多的合成数据。ADASYN通过两种方式：减少由于类别不平衡带来的偏差。自适应地将分类决策边界转移到困难的例子。
SMOTE方法
基本思想是对于少数类别样本进行分析模拟，将人工模拟的新样本添加到数据集中。具体实现先找出一个正样本（少数），采用 KNN 算法找到该正样本的 K 个邻近，并随机从 K 个邻近中选出一个样本，最后再正样本和选中的随机样本之间连线上，随机选取一个点，作为人工合成的新正样本。合成后所有类别样本数量相当。
缺点：无法克服非平衡数据集的数据分布问题，容易产生分布边缘化问题。
SMOTE代码
from imblearn.over_sampling import SMOTE
print(&#34;Start over-sampling&#34;)
X_train, y_train = SMOTE().fit_resample(X_train, y_train)
下采样 减少分类中多数样本的数量实现样本均衡，通常做法是对样本进行聚类。
本项目采用 imblearn 的 ClusterCentroids 算法，本质上是一种原型生成 (Prototype generation) 的方法，对多数类样本采用 K-means 算法生成 N 个簇样本中心，并且通过样本中心合成新的样本，替代原始样本。新的多数类样本数将与少数类样本数达到平衡。
from imblearn.under_sampling import ClusterCentroids
print(&#34;Start under-sampling&#34;)
cc = ClusterCentroids()
X_train, y_train = cc.fit_resample(X_train, y_train)
过采样和欠采样结合 由于过采样容易导致过拟合，欠采样容易丢失有效信息，因此结合两种方法进一步削弱二者的缺点。这里采样 SMOTEENN 算法，先采用 SMOTE 算法进行过采样，再采用欠采样保留能够体现各类别特征的样本。其中 ENN (Edited Nearest Neighbours) 算法是基于 K 最近邻算法编辑数据集，找出与邻居不友好的样本然后移除，从而实现数据平衡。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/3d4811340506d0908cd50d451dfc2b22/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-11-09T08:12:35+08:00" />
<meta property="article:modified_time" content="2022-11-09T08:12:35+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">机器学习样本不平衡处理方法</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p style="margin-left:0;text-align:justify;"><span style="color:#24292e;"><strong>数据</strong><strong>--</strong><strong>样本不平衡处理</strong></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#24292e;">不同类别的样本量差异大，或<strong>少量样本代表了业务的关键数据</strong>，属于样本不平衡的情况，<strong>要求对少量样本的模式有很好的学习</strong>。</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#24292e;">大数据情况下：整体数据规模大，小样本类别占比少，但是小样本也覆盖大部分或全部特征；</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#24292e;">小数据情况下，整体数据规模小，小样本类别也少，导致样本特征分布不均匀。</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#24292e;">一般比例差异超过10倍就要注意，超过20倍就得处理</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#24292e;"><strong>工程方法中，通常从三个方面处理样本不均衡：</strong></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#24292e;">扩大数据集，但是在扩大小样本比例的时候，大样本也在增加，通过获取更多少量样本数据，尽可能扩大这些少量样本的数据集。</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#24292e;">1、数据相关处理：欠采样和过采样，欠采样又称为下采样，过采样又称为上采样。实现简单，速度快</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#24292e;">对大样本（超过1w或10w）进行欠采样，放弃一部分大样本数据；</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#24292e;">对小样本（小于1w）进行过采样，相当于添加部分小样本的副本。</span></p> 
<p style="margin-left:0;text-align:justify;"></p> 
<h2 style="margin-left:0;text-align:justify;"><span style="color:#24292e;">上采样</span></h2> 
<p style="margin-left:0;text-align:justify;"><span style="color:#24292e;">通过增加分类中少数样本的数量实现样本均衡，通常做法是在少数样本中加入随机噪声、干扰数据或通过一定规则产生新的合成样本。</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#24292e;">ADASYN采样</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#24292e;">样本较少的类生成合成数据，其生成的数据与更容易学习的样本相比，更难学习，.基本思想是根据学习难度不同，对不同少数类样本使用加权分布，其中，更难学习的少数类样本比那些更容易学习的少数类样本要产生更多的合成数据。ADASYN通过两种方式：减少由于类别不平衡带来的偏差。自适应地将分类决策边界转移到困难的例子。</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#24292e;">SMOTE方法</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#24292e;">基本思想是对于少数类别样本进行分析模拟，将人工模拟的新样本添加到数据集中。具体实现先找出一个正样本（少数），采用 KNN 算法找到该正样本的 K 个邻近，并随机从 K 个邻近中选出一个样本，最后再正样本和选中的随机样本之间连线上，随机选取一个点，作为人工合成的新正样本。合成后所有类别样本数量相当。</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#24292e;">缺点：无法克服非平衡数据集的数据分布问题，容易产生分布边缘化问题。</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#24292e;">SMOTE代码</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#24292e;">from imblearn.over_sampling import SMOTE</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#24292e;">print("Start over-sampling")</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#24292e;">X_train, y_train = SMOTE().fit_resample(X_train, y_train)</span></p> 
<h2 style="margin-left:0;text-align:justify;"><span style="color:#24292e;">下采样</span></h2> 
<p style="margin-left:0;text-align:justify;"><span style="color:#24292e;">减少分类中多数样本的数量实现样本均衡，通常做法是对样本进行聚类。</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#24292e;">本项目采用 imblearn 的 ClusterCentroids 算法，本质上是一种原型生成 (Prototype generation) 的方法，对多数类样本采用 K-means 算法生成 N 个簇样本中心，并且通过样本中心合成新的样本，替代原始样本。新的多数类样本数将与少数类样本数达到平衡。</span></p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#f6f6f6;"><span style="color:#24292e;"><span style="color:#121212;">from imblearn.under_sampling <strong>import</strong> ClusterCentroids</span></span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#f6f6f6;"><span style="color:#24292e;"><span style="color:#121212;">print(</span><span style="color:#f1403c;">"Start under-sampling"</span><span style="color:#121212;">)</span></span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#f6f6f6;"><span style="color:#24292e;"><span style="color:#121212;">cc <strong>=</strong> ClusterCentroids()</span></span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#f6f6f6;"><span style="color:#24292e;"><span style="color:#121212;">X_train, y_train <strong>=</strong> cc.fit_resample(X_train, y_train)</span></span></span></p> 
<p style="margin-left:0;text-align:justify;"></p> 
<h2 style="margin-left:0;text-align:justify;"><span style="color:#24292e;">过采样和欠采样结合</span></h2> 
<p style="margin-left:0;text-align:justify;"><span style="color:#24292e;">由于过采样容易导致过拟合，欠采样容易丢失有效信息，因此结合两种方法进一步削弱二者的缺点。这里采样 SMOTEENN 算法，先采用 SMOTE 算法进行过采样，再采用欠采样保留能够体现各类别特征的样本。其中 ENN (Edited Nearest Neighbours) 算法是基于 K 最近邻算法编辑数据集，找出与邻居不友好的样本然后移除，从而实现数据平衡。</span></p> 
<pre style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#f6f6f6;"><span style="color:#121212;">from </span></span><span style="background-color:#f6f6f6;"><span style="color:#121212;">imblearn</span><span style="color:#121212;">.</span><span style="color:#121212;">combine  </span></span><span style="background-color:#f6f6f6;"><strong><span style="color:#121212;">import </span></strong></span><span style="background-color:#f6f6f6;"><span style="color:#121212;">SMOTEENN</span></span></pre> 
<pre style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#f6f6f6;"><span style="color:#121212;">print</span><span style="color:#121212;">(</span><span style="color:#f1403c;">"Start over-sampling and under-sampling"</span><span style="color:#121212;">)</span></span></pre> 
<pre style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#f6f6f6;"><span style="color:#121212;">sme </span></span><span style="background-color:#f6f6f6;"><strong><span style="color:#121212;">= </span></strong></span><span style="background-color:#f6f6f6;"><span style="color:#121212;">SMOTEENN</span><span style="color:#121212;">()</span></span></pre> 
<pre style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#f6f6f6;"><span style="color:#121212;">X_train</span><span style="color:#121212;">, </span></span><span style="background-color:#f6f6f6;"><span style="color:#121212;">y_train </span></span><span style="background-color:#f6f6f6;"><strong><span style="color:#121212;">= </span></strong></span><span style="background-color:#f6f6f6;"><span style="color:#121212;">sme</span><span style="color:#121212;">.</span><span style="color:#121212;">fit_resample</span><span style="color:#121212;">(</span><span style="color:#121212;">X_train</span><span style="color:#121212;">, </span></span><span style="background-color:#f6f6f6;"><span style="color:#121212;">y_train</span><span style="color:#121212;">)</span></span></pre> 
<p style="margin-left:0;text-align:justify;"></p> 
<h2 style="margin-left:0;text-align:justify;"><span style="color:#24292e;">采用深度学习模型</span></h2> 
<p style="margin-left:0;text-align:justify;"><span style="color:#24292e;">前面的方法采用的都是机器学习模型，机器学习方法需要对输入内容做具体特征工程。而深度学习方法将输入文本映射成高维的词向量，并通过深度学习模型从词向量中自动提取特征。</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#24292e;">2、模型相关处理：引入有倚重的模型算法，针对少量样本着重拟合，提升对少量样本特征的学习；或者选择对数据分布不敏感的算法，如“树模型”</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#24292e;">3、评价指标相关处理：如 G-mean/MacroP/MicroP等。对于数据极端不平衡时，可以观察不同算法在同一份数据下的训练结果的precision和recall，建议更多的使用PR曲线。避免只使用AUC，很容易因为AUC值过高忽略，如果对少两个样本的效果并不理想的情况。</span></p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#24292e;">注意：在进行相对应的采样方法后，要尽可能保持训练集和测试集分布一致。</span></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b0ba8f7dd22bd96cdacf2f4fea22e71e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Gromacs伞形采样</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/5ba695ed6f5854f382d208b0da28a7ed/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">我在 vue3 开发中踩的坑</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>