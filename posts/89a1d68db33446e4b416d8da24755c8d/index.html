<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>清华ChatGLM-6B本地GPU推理部署 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="清华ChatGLM-6B本地GPU推理部署" />
<meta property="og:description" content="目录
1 简介
2 硬件需求
3 Demo和模型下载
3.1 安装Git LFS
3.2 Demo下载
3.3 模型下载
3.4 文件目录
4 环境安装
5 运行
5.1 FP16
5.2 量化
6 演示
1 简介 ChatGLM-6B是一个开源的、支持中英双语的对话语言模型，基于General Language Model（GLM）架构，具有62亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4量化级别下最低只需6GB显存）。ChatGLM-6B使用了和ChatGPT相似的技术，针对中文问答和对话进行了优化。经过约1T标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62亿参数的ChatGLM-6B已经能生成相当符合人类偏好的回答。
本文主要参考官方流程，在Ubuntu22.04上将ChatGLM-6B部署在本地Nvidia RTX 3080Ti Laptop GPU（16GB显存）。
2 硬件需求 默认情况下，模型以FP16精度加载，运行上述代码需要大概13GB显存。8-bit量化下GPU显存占用约为8GB，4-bit量化下仅需6GB占用。所以理论上，只要GPU的显存在6GB以上，就可以尝试在本地部署ChatGLM-6B。
随着对话轮数的增多，对应消耗显存也随之增长，由于采用了相对位置编码，理论上ChatGLM-6B支持无限长的context-length，但总长度超过2048（训练长度）后性能会逐渐下降。
模型量化会带来一定的性能损失，经过测试，ChatGLM-6B在4-bit量化下仍然能够进行自然流畅的生成。使用GPT-Q等量化方案可以进一步压缩量化精度/提升相同量化精度下的模型性能。
3 Demo和模型下载 3.1 安装Git LFS sudo apt install git-lfs 3.2 Demo下载 mkdir THUDM cd THUDM git clone https://github.com/THUDM/ChatGLM-6B.git 3.3 模型下载 先下载模型实现。
GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/THUDM/chatglm-6b 再下载模型参数文件，并将下载的文件替换到本地的chatglm-6b目录下。
3.4 文件目录 Demo和模型下载完成之后的文件目录如下图所示。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/89a1d68db33446e4b416d8da24755c8d/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-14T17:53:34+08:00" />
<meta property="article:modified_time" content="2023-07-14T17:53:34+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">清华ChatGLM-6B本地GPU推理部署</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="1%20%E7%AE%80%E4%BB%8B-toc" style="margin-left:0px;"><a href="#1%20%E7%AE%80%E4%BB%8B" rel="nofollow">1 简介</a></p> 
<p id="2%20%E7%A1%AC%E4%BB%B6%E9%9C%80%E6%B1%82-toc" style="margin-left:0px;"><a href="#2%20%E7%A1%AC%E4%BB%B6%E9%9C%80%E6%B1%82" rel="nofollow">2 硬件需求</a></p> 
<p id="3%20Demo%E5%92%8C%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD-toc" style="margin-left:0px;"><a href="#3%20Demo%E5%92%8C%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD" rel="nofollow">3 Demo和模型下载</a></p> 
<p id="3.1%20%E5%AE%89%E8%A3%85Git%20LFS-toc" style="margin-left:40px;"><a href="#3.1%20%E5%AE%89%E8%A3%85Git%20LFS" rel="nofollow">3.1 安装Git LFS</a></p> 
<p id="3.2%20Demo%E4%B8%8B%E8%BD%BD-toc" style="margin-left:40px;"><a href="#3.2%20Demo%E4%B8%8B%E8%BD%BD" rel="nofollow">3.2 Demo下载</a></p> 
<p id="3.3%20%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD-toc" style="margin-left:40px;"><a href="#3.3%20%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD" rel="nofollow">3.3 模型下载</a></p> 
<p id="3.4%20%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95-toc" style="margin-left:40px;"><a href="#3.4%20%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95" rel="nofollow">3.4 文件目录</a></p> 
<p id="4%20%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85-toc" style="margin-left:0px;"><a href="#4%20%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85" rel="nofollow">4 环境安装</a></p> 
<p id="5%20%E8%BF%90%E8%A1%8C-toc" style="margin-left:0px;"><a href="#5%20%E8%BF%90%E8%A1%8C" rel="nofollow">5 运行</a></p> 
<p id="5.1%20FP16-toc" style="margin-left:40px;"><a href="#5.1%20FP16" rel="nofollow">5.1 FP16</a></p> 
<p id="5.2%20%E9%87%8F%E5%8C%96-toc" style="margin-left:40px;"><a href="#5.2%20%E9%87%8F%E5%8C%96" rel="nofollow">5.2 量化</a></p> 
<p id="6%20%E6%BC%94%E7%A4%BA-toc" style="margin-left:0px;"><a href="#6%20%E6%BC%94%E7%A4%BA" rel="nofollow">6 演示</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="1%20%E7%AE%80%E4%BB%8B">1 简介</h2> 
<p><a href="https://github.com/THUDM/ChatGLM-6B" title="ChatGLM-6B">ChatGLM-6B</a>是一个开源的、支持中英双语的对话语言模型，基于General Language Model（GLM）架构，具有62亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4量化级别下最低只需6GB显存）。ChatGLM-6B使用了和ChatGPT相似的技术，针对中文问答和对话进行了优化。经过约1T标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62亿参数的ChatGLM-6B已经能生成相当符合人类偏好的回答。</p> 
<p>本文主要参考官方流程，在Ubuntu22.04上将ChatGLM-6B部署在本地Nvidia RTX 3080Ti Laptop GPU（16GB显存）。</p> 
<h2 id="2%20%E7%A1%AC%E4%BB%B6%E9%9C%80%E6%B1%82">2 硬件需求</h2> 
<p>默认情况下，模型以FP16精度加载，运行上述代码需要大概13GB显存。8-bit量化下GPU显存占用约为8GB，4-bit量化下仅需6GB占用。所以理论上，只要GPU的显存在6GB以上，就可以尝试在本地部署ChatGLM-6B。</p> 
<p>随着对话轮数的增多，对应消耗显存也随之增长，由于采用了相对位置编码，理论上ChatGLM-6B支持无限长的context-length，但总长度超过2048（训练长度）后性能会逐渐下降。</p> 
<p>模型量化会带来一定的性能损失，经过测试，ChatGLM-6B在4-bit量化下仍然能够进行自然流畅的生成。使用GPT-Q等量化方案可以进一步压缩量化精度/提升相同量化精度下的模型性能。</p> 
<p><img alt="" height="191" src="https://images2.imgbox.com/b6/fa/0qGrxSfs_o.png" width="720"></p> 
<h2 id="3%20Demo%E5%92%8C%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD">3 Demo和模型下载</h2> 
<h3 id="3.1%20%E5%AE%89%E8%A3%85Git%20LFS">3.1 安装Git LFS</h3> 
<pre><code class="language-bash">sudo apt install git-lfs</code></pre> 
<h3 id="3.2%20Demo%E4%B8%8B%E8%BD%BD">3.2 Demo下载</h3> 
<pre><code class="language-bash">mkdir THUDM
cd THUDM
git clone https://github.com/THUDM/ChatGLM-6B.git</code></pre> 
<h3 id="3.3%20%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD">3.3 模型下载</h3> 
<p>先下载模型实现。</p> 
<pre><code class="language-bash">GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/THUDM/chatglm-6b</code></pre> 
<p>再下载<a href="https://cloud.tsinghua.edu.cn/d/fb9f16d6dc8f482596c2/" rel="nofollow" title="模型参数文件">模型参数文件</a>，并将下载的文件替换到本地的chatglm-6b目录下。</p> 
<h3 id="3.4%20%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95">3.4 文件目录</h3> 
<p>Demo和模型下载完成之后的文件目录如下图所示。</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/bf/82/AL9cvlep_o.png" width="720"></p> 
<h2 id="4%20%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85">4 环境安装</h2> 
<p>在Nvidia GPU上运行，安装所需的依赖包，如transformers、gradio等。</p> 
<pre><code class="language-bash">cd ChatGLM-6B
pip install -r requirements.txt</code></pre> 
<h2 id="5%20%E8%BF%90%E8%A1%8C">5 运行</h2> 
<h3 id="5.1%20FP16">5.1 FP16</h3> 
<p>可以通过如下代码调用ChatGLM-6B模型来生成对话：</p> 
<pre><code class="language-python">&gt;&gt;&gt; from transformers import AutoTokenizer, AutoModel
&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
&gt;&gt;&gt; model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().cuda()
&gt;&gt;&gt; model = model.eval()
&gt;&gt;&gt; response, history = model.chat(tokenizer, "你好", history=[])
&gt;&gt;&gt; print(response)
你好👋!我是人工智能助手 ChatGLM-6B,很高兴见到你,欢迎问我任何问题。
&gt;&gt;&gt; response, history = model.chat(tokenizer, "晚上睡不着应该怎么办", history=history)
&gt;&gt;&gt; print(response)
晚上睡不着可能会让你感到焦虑或不舒服,但以下是一些可以帮助你入睡的方法:

1. 制定规律的睡眠时间表:保持规律的睡眠时间表可以帮助你建立健康的睡眠习惯,使你更容易入睡。尽量在每天的相同时间上床,并在同一时间起床。
2. 创造一个舒适的睡眠环境:确保睡眠环境舒适,安静,黑暗且温度适宜。可以使用舒适的床上用品,并保持房间通风。
3. 放松身心:在睡前做些放松的活动,例如泡个热水澡,听些轻柔的音乐,阅读一些有趣的书籍等,有助于缓解紧张和焦虑,使你更容易入睡。
4. 避免饮用含有咖啡因的饮料:咖啡因是一种刺激性物质,会影响你的睡眠质量。尽量避免在睡前饮用含有咖啡因的饮料,例如咖啡,茶和可乐。
5. 避免在床上做与睡眠无关的事情:在床上做些与睡眠无关的事情,例如看电影,玩游戏或工作等,可能会干扰你的睡眠。
6. 尝试呼吸技巧:深呼吸是一种放松技巧,可以帮助你缓解紧张和焦虑,使你更容易入睡。试着慢慢吸气,保持几秒钟,然后缓慢呼气。

如果这些方法无法帮助你入睡,你可以考虑咨询医生或睡眠专家,寻求进一步的建议。</code></pre> 
<p>代码实现在ChatGLM-6B/web_demo.py，需要将下面两行代码里的模型文件路径"THUDM/chatglm-6b"修改为3.3中模型文件在本地下载后的绝对路径"/absolute-path/THUDM/chatglm-6b"，否则运行时会重新从远端下载。</p> 
<pre><code class="language-python">tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().cuda()</code></pre> 
<p>修改完成之后，直接运行以下代码即可。程序会运行一个Web Server，并输出地址。在浏览器中打开输出的地址（比如http://127.0.0.1:7860）即可使用。</p> 
<pre><code class="language-bash">python web_demo.py</code></pre> 
<p>也可以运行以下代码在命令行中进行交互式的对话，在命令行中输入指示并回车即可生成回复，输入clear可以清空对话历史，输入stop终止程序。</p> 
<pre><code class="language-bash">python cli_demo.py</code></pre> 
<h3 id="5.2%20%E9%87%8F%E5%8C%96">5.2 量化</h3> 
<p>8-bit量化使用方法如下，4-bit量化使用方法类似。</p> 
<pre><code class="language-python">model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).quantize(8).half().cuda()</code></pre> 
<p>量化过程需要在内存中首先加载FP16格式的模型，消耗大概13GB的内存。如果机器上内存不足的话，可以直接加载量化后的模型，INT4量化后的模型仅需大概5.2GB的内存：</p> 
<pre><code class="language-python">model = AutoModel.from_pretrained("THUDM/chatglm-6b-int4", trust_remote_code=True).half().cuda()</code></pre> 
<h2 id="6%20%E6%BC%94%E7%A4%BA">6 演示</h2> 
<p>以FP16 Web Server为例演示，对话如下，有兴趣的可以自己部署体验一下。</p> 
<p><img alt="" height="416" src="https://images2.imgbox.com/3a/3d/4xiUb6xT_o.png" width="720"></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ec22bb7f2921f05cb2900fc27793f34c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">住宅小区的拓扑规划与网络设计（完整文档&#43;ensp拓扑图）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/bc2b62c6fd8f429ea6b9afc4d9f5f6af/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【原创】linux中[root@localhost ~]#代表什么?</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>