<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>姿态估计之2D人体姿态估计 - Simple Baseline(SBL) - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="姿态估计之2D人体姿态估计 - Simple Baseline(SBL)" />
<meta property="og:description" content="论文地址：Simple Baselines for Human Pose Estimation and Tracking
代码地址：GitHub - leoxiaobin/pose.pytorch: Simple Baselines for Human Pose Estimation and Tracking
Simple Baselines，是2018年MSRA的工作，网络结构如下图所示。之所以叫这个名字，是因为这个网络真的很简单。该网络就是在ResNet的基础上接了一个head，这个head仅仅包含几个deconvolutional layer，用于提升ResNet输出的feature map的分辨率，我们提到过多次高分辨率是姿态估计任务的需要。这里的deconvolutional layer是一种不太严谨的说法，阅读源代码可知，deconvolutional layer实际上是将transpose convolution、BatchNorm、ReLU封装成了一个结构。所以关键之处在于transpose convolution，可以认为是convolution的逆过程。
从图中看可以发现Simple Baselines的网络结构有点类似Hourglass中的一个module，但可以发现：①该网络没有使用类似Hourglass中的skip connection；②该网络是single-stage，Hourglass是multi-stage的。但令人惊讶的是，该网络的效果却超过了Hourglass。我个人认为有两点原因，一是Simple Baselines以ResNet作为backbone，特征提取能力相比Hourglass更强。二是Hourglass中上采样使用的是简单的nearest neighbor upsampling，而这里使用的是deconvolutional layer，后者的效果更好（后面可以看到在MSRA的Higher-HRNet中依旧使用了这种结构）。
SBL网络结构 SBL（Simple Baseline） [7] 为人体姿态估计提供了一套基准框架。SBL 在主干网络后接逆卷积模块来预测热图，就是在ResNet后加上几层Deconvolution直接生成热力图。相比于其他模型，就是使用Deconvolution替换了上采样结构。将上采样和卷积参数以一种更简单的方式组合到反卷积层中，而不使用跳跃层连接。
Hourglass、CPN、SBL共同点是，采用三个上采样步骤和三个水平的非线性(来自最深处的特征)来获得高分辨率的特征图和heatmap
上图中a是Hourglass网络，b是CPN，c是本文的SimplePose，可以直观看出结构的复杂度对比前两种结构需要构造金字塔特征结构，如FPN或从Resnet构建SimplePose则不需要构建金字塔特征结构，它是直接在Resnet后面设计反卷积模块并输出结果，是从deep和low分辨率特征生成热图的最简单方法具体结构：首先：在Resnet的基础上，取最后残差模块输出特征层（命名C5）然后：后面接上三个反卷积模块（每个模块为：Deconv &#43; batchnorm &#43; relu，反卷积参数，256通道，4X4卷积核，stride为2，pad为1），最后：用1X1卷积层生成 k个关键点输出热力图。均方误差（MSE）被用作预测热图和目标热图之间的损失通过应用以第 k 个关节的GT位置为中心的2D高斯函数，生成 k 关节的目标热图 。 在这些模型中，可以看出如何生成高分辨率特征图是姿态估计的一个关键，SimplePose采用Deconv扩大特征图的分辨率，Hourglass，CPN中采用的是upsampling&#43;skip方式；当然我们很难就这一个实例就判定那种方式好
姿态追踪问题描述
ICCV’17 PoseTrack Challenge[2]的获胜者[11]解决了这个多人位姿跟踪问题，首先使用Mask RCNN[12]在帧中估计人体位姿，然后使用贪婪二部图匹配算法逐帧进行在线跟踪。
这个贪婪匹配算法，简单来说就是，在视频第一帧中每个检测到的人给一个id，然后之后的每一帧检测到的人都和上一帧检测到的人通过某种度量方式（文中提到的是计算检测框的IOU）算一个相似度，将相似度大的（大于阈值）作为同一个id，并删去。重复以上步骤，直到没有与当前帧相似的实例，此时给剩下的实例分配一个新的id。
本文提出的方法保留了这一方法的主要流程，并且在此之上提出了两点改进：
一是除了检测网络之外，还使用光流法补充一些检测框，用以解决检测网络的漏检问题（比如图2©中最左边的人就没有被检测网络检测到）。
二是使用 Object Keypoint Similarity (OKS)代替检测框的IOU来计算相似度。这是因为当人的动作比较快时，用IOU可能并不合理。
OKS是关键点距离的一种度量方式，计算方式如下：
文中提出的新的相似度计算方式具体是使用光流法计算某一帧的关键点会出现在的另外一帧的位置，然后用这个计算出来的位置和这一帧检测出来的关键点之间计算OKS,以此作为两帧之间的不同人的相似度值。
Joint Propagation using Optical Flow" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/3d9f64c1ceea785dbdf74b1fb7ad05aa/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-06-29T15:43:21+08:00" />
<meta property="article:modified_time" content="2022-06-29T15:43:21+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">姿态估计之2D人体姿态估计 - Simple Baseline(SBL)</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p> 论文地址：<a href="https://arxiv.org/abs/1804.06208" rel="nofollow" title="Simple Baselines for Human Pose Estimation and Tracking">Simple Baselines for Human Pose Estimation and Tracking</a><br> 代码地址：<a href="https://github.com/leoxiaobin/pose.pytorch" title="GitHub - leoxiaobin/pose.pytorch: Simple Baselines for Human Pose Estimation and Tracking">GitHub - leoxiaobin/pose.pytorch: Simple Baselines for Human Pose Estimation and Tracking</a></p> 
<blockquote> 
 <p>Simple Baselines，是2018年MSRA的工作，网络结构如下图所示。之所以叫这个名字，是因为这个网络真的很简单。该网络就是在ResNet的基础上接了一个head，这个head仅仅包含几个<strong>deconvolutional layer</strong>，用于提升ResNet输出的feature map的分辨率，我们提到过多次高分辨率是姿态估计任务的需要。这里的deconvolutional layer是一种不太严谨的说法，阅读源代码可知，deconvolutional layer实际上是将<strong>transpose convolution、BatchNorm、ReLU</strong>封装成了一个结构。所以关键之处在于transpose convolution，可以认为是convolution的逆过程。</p> 
 <p>从图中看可以发现Simple Baselines的网络结构有点类似Hourglass中的一个module，但可以发现：①该网络没有使用类似Hourglass中的skip connection；②该网络是single-stage，Hourglass是multi-stage的。但令人惊讶的是，该网络的效果却超过了Hourglass。我个人认为有两点原因，一是Simple Baselines以ResNet作为backbone，特征提取能力相比Hourglass更强。二是Hourglass中上采样使用的是简单的nearest neighbor upsampling，而这里使用的是deconvolutional layer，后者的效果更好（后面可以看到在MSRA的Higher-HRNet中依旧使用了这种结构）。</p> 
</blockquote> 
<p><strong>SBL网络结构 </strong></p> 
<p><a href="https://link.zhihu.com/?target=https%3A//mmpose.readthedocs.io/en/latest/papers/algorithms.html%23simplebaseline2d-eccv-2018" rel="nofollow" title="SBL">SBL</a>（Simple Baseline） [7] 为人体姿态估计提供了一套<strong>基准框架</strong>。SBL 在<strong>主干网络</strong>后接<strong>逆卷积模块来预测热图</strong>，就是在ResNet后加上几层Deconvolution直接生成热力图。相比于其他模型，就是使用Deconvolution替换了上采样结构。<strong>将上采样和卷积参数以一种更简单的方式组合到反卷积层中，而不使用跳跃层连接。</strong></p> 
<p>Hourglass、CPN、SBL共同点是，采用三个上采样步骤和三个水平的非线性(来自最深处的特征)来获得高分辨率的特征图和heatmap</p> 
<p></p> 
<p><img alt="" height="827" src="https://images2.imgbox.com/d1/9c/TAW1Vqi3_o.png" width="1093"></p> 
<ol><li>上图中a是Hourglass网络，b是CPN，c是本文的SimplePose，可以直观看出结构的复杂度对比</li><li>前两种结构需要构造金字塔特征结构，如FPN或从Resnet构建</li><li>SimplePose则不需要构建金字塔特征结构，它是直接在Resnet后面设计反卷积模块并输出结果，是从deep和low分辨率特征生成热图的最简单方法</li><li>具体结构：首先：在Resnet的基础上，取最后残差模块输出特征层（命名C5）然后：后面接上三个反卷积模块（每个模块为：Deconv + batchnorm + relu，反卷积参数，256通道，4X4卷积核，stride为2，pad为1），最后：用1X1卷积层生成 k个关键点输出热力图<img alt="" height="23" src="https://images2.imgbox.com/8a/25/zjQkPWBZ_o.png" width="95">。</li><li>均方误差（MSE）被用作预测热图和目标热图之间的损失</li><li>通过应用以第 k 个关节的GT位置为中心的2D高斯函数，生成 k 关节的目标热图 <img alt="" height="25" src="https://images2.imgbox.com/08/e8/zPKe53ip_o.png" width="25">。</li></ol> 
<p></p> 
<p>在这些模型中，可以看出<strong>如何生成高分辨率特征图</strong>是姿态估计的一个关键，<strong>SimplePose采用Deconv扩大特征图的分辨率</strong>，<strong>Hourglass，CPN中采用的是upsampling+skip方式</strong>；当然我们很难就这一个实例就判定那种方式好<br>  </p> 
<p><strong>姿态追踪问题描述</strong><br><img alt="" height="161" src="https://images2.imgbox.com/f5/3b/xSQlNZ56_o.png" width="977"></p> 
<p>ICCV’17 PoseTrack Challenge[2]的获胜者[11]解决了这个多人位姿跟踪问题，首先使用Mask RCNN[12]在帧中估计人体位姿，然后使用贪婪二部图匹配算法逐帧进行在线跟踪。<br> 这个贪婪匹配算法，简单来说就是，在视频第一帧中每个检测到的人给一个id，然后之后的每一帧检测到的人都和上一帧检测到的人通过某种度量方式（文中提到的是计算检测框的IOU）算一个相似度，将相似度大的（大于阈值）作为同一个id，并删去。重复以上步骤，直到没有与当前帧相似的实例，此时给剩下的实例分配一个新的id。</p> 
<p><img alt="" height="517" src="https://images2.imgbox.com/68/8d/2aln1d0g_o.png" width="852"><br> 本文提出的方法保留了这一方法的主要流程，并且在此之上提出了两点改进：<br> 一是除了检测网络之外，还使用光流法补充一些检测框，用以解决检测网络的漏检问题（比如图2©中最左边的人就没有被检测网络检测到）。<br> 二是使用 Object Keypoint Similarity (OKS)代替检测框的IOU来计算相似度。这是因为当人的动作比较快时，用IOU可能并不合理。<br> OKS是关键点距离的一种度量方式，计算方式如下：<br><img alt="" height="271" src="https://images2.imgbox.com/b1/12/W1kpPNAk_o.png" width="658"><br> 文中提出的新的相似度计算方式具体是使用光流法计算某一帧的关键点会出现在的另外一帧的位置，然后用这个计算出来的位置和这一帧检测出来的关键点之间计算<strong>OKS</strong>,以此作为两帧之间的不同人的相似度值。</p> 
<p> <strong>Joint Propagation using Optical Flow</strong></p> 
<p>如果在视频中简单地使用单一图像级的检测器(如fast - rcnn [27]， R-FCN[16])，由于视频帧引入了运动模糊和遮挡，可能会导致漏检和误检。如图2©所示，由于快速运动，探测器错过了左边的黑人。时间信息经常被用来产生更可靠的检测[36,35]。</p> 
<p>我们建议使用以光流表示的时间信息，从附近的帧为处理帧生成行人框。</p> 
<p><strong>具体的方法是</strong>： 给定<img alt="I^{k-1}" src="https://images2.imgbox.com/2f/d1/kXRBjZpj_o.png"> 帧处的一个实例 i ，有关键点集合 <img alt="J^{k-1}_i" src="https://images2.imgbox.com/a6/0c/AIRaBGKz_o.png">​ 以及 <img alt="I^{k-1}" src="https://images2.imgbox.com/9b/be/qoEa8eCY_o.png"> 和<img alt="I^{k}" src="https://images2.imgbox.com/8e/6c/XE7bd9vt_o.png">之间的光流场 <img alt="F_{k-1 \rightarrow k}" src="https://images2.imgbox.com/a0/c0/Vl7C9xIU_o.png">，我们可以估计出相应的关键点坐标集合<img alt="\hat{J}_{i}^{k}" src="https://images2.imgbox.com/c2/87/a7dqVbwQ_o.png"> 。具体的说，就是对于<img alt="J^{k-1}_i" class="mathcode" src="https://images2.imgbox.com/82/17/s8joUJfV_o.png">中的 joints 位置  (x,y)，下一帧可能在 ( x + δ x , y + δ y )，其中 δ x 和 δ y 是在(x,y) 处的流场值（ ﬂow ﬁeld values ）。当我们计算出<img alt="\hat{J}_{i}^{k}" class="mathcode" src="https://images2.imgbox.com/9a/69/mGdbFnh6_o.png">的边界，并对其进行扩展后得到的Box 作为 candidated box 。实验使用的扩充值是 15 % 。<br> 当由于运动模糊或遮挡，导致行人检测器对当前帧产生了漏检后，我们可以使用从以前帧传播过来的 Boxes ，漏掉的人会被这些框检测到。如图2 (c ) 所示，对于图中左边的黑人，由于我们在图2(a)中有前一帧的跟踪结果，所以传播的box成功地包含了这个人。 </p> 
<p> <strong> Flow-based Pose Similarity</strong></p> 
<p>使用边界框IoU (Intersection-over-Union) 作为相似度度量 (<img alt="S_{Bbox}" src="https://images2.imgbox.com/58/3f/orfJHH3D_o.png">​) 来连接实例可能出现问题，一是当实例移动得很快时，这些框不会重叠；二是在拥挤的场景中，离得近的框中实例不一定有联系。更细粒度的度量可以是姿态相似性 ( <img alt="S_{Pose}" src="https://images2.imgbox.com/9b/6e/8cEgRMET_o.png">​) ，它使用对象关键点相似性(OKS)计算两个实例之间的身体关节距离。当不同帧时，同一个人的姿势可能会改变，此时姿势相似性也会产生问题。所以，我们建议使用基于流的姿态相似性度量。</p> 
<p>给定 <img alt="I^{k}" src="https://images2.imgbox.com/8d/67/YtNBxwj0_o.png"> 帧处的一个实例关键点 <img alt="J^{k}_i" src="https://images2.imgbox.com/96/0d/g64wv72y_o.png"> 和 <img alt="I^{l}" src="https://images2.imgbox.com/eb/6a/iSgWzpyZ_o.png">l 帧处的实例 <img alt="J^{l}_j" src="https://images2.imgbox.com/8e/af/wragiYKo_o.png">​ ，基于流的姿态相似度度量表示为：<br><img alt="S_{Flow}\left(J_{i}^{k}, J_{j}^{l}\right)=OKS\left(\hat{J}_{i}^{l}, J_{j}^{l}\right)" src="https://images2.imgbox.com/24/2e/ZsFvzulZ_o.png"></p> 
<p>其中OKS表示两个人体姿态之间的 Object Keypoint Similarity (OKS) 计算。对于<img alt="J^{k}_i" src="https://images2.imgbox.com/75/78/bao5nPUl_o.png"> 实例，我们根据光流场<img alt="F_{k \rightarrow l}" src="https://images2.imgbox.com/95/4e/qVNTJvCg_o.png">计算<img alt="I^{l}" src="https://images2.imgbox.com/95/d1/BNxAhYgA_o.png">帧时的对应，记为<img alt="\hat{J}_{i}^{l}" src="https://images2.imgbox.com/ba/85/sbuAzM3U_o.png"></p> 
<p>由于与他人或物体的遮挡，行人经常会消失，然后再次出现。考虑连续两帧是不够的。因此，我们有考虑多帧的基于流的位姿相似性，记为 <img alt="S_{Multi - flow}" src="https://images2.imgbox.com/94/1b/XEUyhpvW_o.png">，这意味着传播 <img alt="\hat{J}^k" src="https://images2.imgbox.com/ab/c8/7HSN2DQL_o.png"> 来自多个之前帧。通过这种方式，我们甚至可以重新链接在中间帧中消失的实例。</p> 
<p><strong>Flow-based Pose Tracking Algorithm</strong></p> 
<p><strong><strong><img alt="" src="https://images2.imgbox.com/c2/01/qRUV8IDR_o.png"></strong></strong></p> 
<p><strong><strong><img alt="" height="297" src="https://images2.imgbox.com/10/d9/E6dR3FMi_o.png" width="674"></strong></strong></p> 
<p></p> 
<p>    首先<strong>解决姿态估计问题</strong>。对于当前处理帧，检测框由行人检测器和之前帧利用光流得到的框组成，并进行非极大抑制（NMS）操作。然后将剪裁和缩放的图片送入姿态估计网络进行姿态估计。<br>     <strong>解决跟踪问题</strong>。我们将已经跟踪到的实例存储在一个双端队列(Deque) Q 中：<img alt="Q=\left[\mathcal{P}_{k-1}, \mathcal{P}_{k-2}, \ldots, \mathcal{P}_{k-L_{Q}}\right]" src="https://images2.imgbox.com/87/71/PGvGstWd_o.png"><br><img alt="" height="74" src="https://images2.imgbox.com/a0/3c/0tvRgpUn_o.png" width="916"></p> 
<p></p> 
<ul><li>首先，我们解决了姿态估计问题。</li><li>对于视频中的处理帧，使用bbox非最大抑制（NMS）操作来统一来自人类探测器的box和使用optical flow从先前帧传播关节生成的box。由progagating joints产生的boxes作为检测器缺失检测的补充</li><li>然后通过我们提出的位姿估计网络，利用这些boxes对经过裁剪和调整大小的图像进行人体姿态估计</li><li>其次，解决了跟踪问题。我们将被跟踪的实例存储在具有固定长度LQ的双端队列（Deque）中，表示为</li></ul> 
<p> <img alt="" height="40" src="https://images2.imgbox.com/bb/bf/6WPX5Dtx_o.png" width="600"> </p> 
<ul><li>其中 <img alt="P_{k-i}" src="https://images2.imgbox.com/e8/51/1eR6VIvo_o.png"> 表示在前一帧<img alt="I^{k-i}" src="https://images2.imgbox.com/d4/21/kXwDW1OG_o.png"> 中设置的被跟踪实例， Q的长度 <img alt="L_Q" src="https://images2.imgbox.com/60/bd/lryOBUVo_o.png">表示执行匹配时考虑的前帧数量。</li><li>Q可以用来捕捉先前的多帧链接关系，在视频的第一帧初始化。对于第 k 帧 <img alt="I^k" src="https://images2.imgbox.com/70/71/rq7pXm0c_o.png">，我们计算未跟踪的身体关节 <img alt="J^k" src="https://images2.imgbox.com/bb/a7/lnLibpmO_o.png">（id为none）与Q中先前实例集之间flow<strong>基于流的姿势相似性矩阵<strong><img alt="M_{sim}" src="https://images2.imgbox.com/1d/68/ZHir5iD8_o.png"></strong></strong> 。然后通过贪婪匹配和<strong><strong><img alt="M_{sim}" class="mathcode" src="https://images2.imgbox.com/e4/91/sPOVIUCY_o.png"></strong></strong> 为 <img alt="J^k" class="mathcode" src="https://images2.imgbox.com/a0/80/FvoOWMED_o.png">中的每个bodyjoints实例 J分配id ，得到指定的实例集<img alt="P^k" src="https://images2.imgbox.com/5f/0d/yAuQtQv2_o.png"> 。最后，我们通过添加第 k帧实例集合 <img alt="P^k" class="mathcode" src="https://images2.imgbox.com/c7/2d/LimHMa9P_o.png"> 来更新跟踪的实例Q。</li></ul> 
<p></p> 
<p></p> 
<p>参见</p> 
<p><a href="https://zhuanlan.zhihu.com/p/195209573" rel="nofollow" title="论文笔记_人体姿态估计：Simple Baselines - 知乎">论文笔记_人体姿态估计：Simple Baselines - 知乎</a> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/24cf31946c339395062caaa3918c982d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">linux系统 常用命令(全面总结)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/42bcad7f757229a2a7243c5ec0648c7c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Jmeter常用时间技巧</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>