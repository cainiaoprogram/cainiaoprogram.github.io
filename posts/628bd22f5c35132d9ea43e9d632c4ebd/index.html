<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>大数据项目分享 - 深度学习 大数据 股票预测系统 - python lstm - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="大数据项目分享 - 深度学习 大数据 股票预测系统 - python lstm" />
<meta property="og:description" content="文章目录 0 前言1 课题意义1.1 股票预测主流方法 2 什么是LSTM2.1 循环神经网络2.1 LSTM诞生 2 如何用LSTM做股票预测2.1 算法构建流程2.2 部分代码 3 实现效果3.1 数据3.2 预测结果项目运行展示开发环境数据获取 0 前言 🔥 这两年开始毕业设计和毕业答辩的要求和难度不断提升，传统的毕设题目缺少创新和亮点，往往达不到毕业答辩的要求，这两年不断有学弟学妹告诉学长自己做的项目系统达不到老师的要求。
为了大家能够顺利以及最少的精力通过毕设，学长分享优质毕业设计项目，今天要分享的是
🚩 深度学习 大数据 股票预测系统
🥇学长这里给一个题目综合评分(每项满分5分)
难度系数：3分工作量：3分创新点：4分 🧿 选题指导, 项目分享：
https://gitee.com/yaa-dc/warehouse-1/blob/master/python/README.md
1 课题意义 利用神经网络模型如果能够提高对股票价格的预测精度，更好地掌握股票价格发展趋势，这对于投资者来说可以及时制定相应的发展策略，更好地应对未来发生的不确定性事件，对于个人来说可以降低投资风险，减少财产损失，实现高效投资，具有一定的实践价值。
1.1 股票预测主流方法 股票市场复杂、非线性的特点使我们难以捉摸其变化规律，目前有很多预测股票走势的论文和算法。
定量分析从精确的数据资料中获得股票发展的价值规律，通过建立模型利用数学语言对股市的发展情况做出解释与预测。
目前常用的定量分析方法有：
传统时间序列预测模型马尔可夫链预测灰色系统理论预测遗传算法机器学习预测等方法 2 什么是LSTM LSTM是长短期记忆网络（LSTM，Long Short-Term Memory），想要理解什么是LSTM，首先要了解什么是循环神经网络。
2.1 循环神经网络 对于传统的BP神经网络如深度前馈网络、卷积神经网络来说，同层及跨层之间的神经元是独立的，但实际应用中对于一些有上下联系的序列来说，如果能够学习到它们之间的相互关系，使网络能够对不同时刻的输入序列产生一定的联系，像生物的大脑一样有“记忆功能”，这样的话我们的模型也就会有更低的训练出错频率及更好的泛化能力。
JordanMI提出序列理论，描述了一种体现“并行分布式处理”的网络动态系统，适用于语音生成中的协同发音问题，并进行了相关仿真实验，ElmanJL认为连接主义模型中对时间如何表示是至关重要的，1990年他提出使用循环连接为网络提供动态内存，从相对简单的异或问题到探寻单词的语义特征，网络均学习到了有趣的内部表示，网络还将任务需求和内存需求结合在一起，由此形成了简单循环网络的基础框架。
循环神经网络（RNN）之间的神经元是相互连接的，不仅在层与层之间的神经元建立连接，而且每一层之间的神经元也建立了连接，隐藏层神经元的输入由当前输入和上一时刻隐藏层神经元的输出共同决定，每一时刻的隐藏层神经元记住了上一时刻隐藏层神经元的输出，相当于对网络增添了“记忆”功能。我们都知道在输入序列中不可避免会出现重复或相似的某些序列信息，我们希望RNN能够保留这些记忆信息便于再次调用，且RNN结构中不同时刻参数是共享的，这一优点便于网络在不同位置依旧能将该重复信息识别出来，这样一来模型的泛化能力自然有所上升。
RNN结构如下：
2.1 LSTM诞生 RNN在解决长序列问题时未能有良好的建模效果，存在长期依赖的弊端，对此HochreiterS等人对神经单元做出了改进，引入自循环使梯度信息得以长时间持续流动，即模型可以拥有长期记忆信息，且自循环权重可以根据前后信息进行调整并不是固定的。作为RNN的一种特殊结构，它可以根据前后输入情况决定历史信息的去留，增进的门控机制可以动态改变累积的时间尺度进而控制神经单元的信息流，这样神经网络便能够自己根据情况决定清除或保留旧的信息，不至于状态信息过长造成网络崩溃，这便是长短期记忆（LSTM）网络。随着信息不断流入，该模型每个神经元内部的遗忘门、输入门、输出门三个门控机制会对每一时刻的信息做出判断并及时进行调整更新，LSTM模型现已广泛应用于无约束手写识别、语音识别、机器翻译等领域。
2 如何用LSTM做股票预测 2.1 算法构建流程 2.2 部分代码 import numpy as np import matplotlib.pyplot as plt import tensorflow as tf import pandas as pd import math def LSTMtest(data): n1 = len(data[0]) - 1 #因为最后一位为label n2 = len(data) print(n1, n2) # 设置常量 input_size = n1 # 输入神经元个数 rnn_unit = 10 # LSTM单元(一层神经网络)中的中神经元的个数 lstm_layers = 7 # LSTM单元个数 output_size = 1 # 输出神经元个数（预测值） lr = 0." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/628bd22f5c35132d9ea43e9d632c4ebd/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-08T10:10:36+08:00" />
<meta property="article:modified_time" content="2023-10-08T10:10:36+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">大数据项目分享 - 深度学习 大数据 股票预测系统 - python lstm</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#0__4" rel="nofollow">0 前言</a></li><li><a href="#1__22" rel="nofollow">1 课题意义</a></li><li><ul><li><a href="#11__26" rel="nofollow">1.1 股票预测主流方法</a></li></ul> 
  </li><li><a href="#2_LSTM_41" rel="nofollow">2 什么是LSTM</a></li><li><ul><li><a href="#21__45" rel="nofollow">2.1 循环神经网络</a></li><li><a href="#21_LSTM_57" rel="nofollow">2.1 LSTM诞生</a></li></ul> 
  </li><li><a href="#2_LSTM_66" rel="nofollow">2 如何用LSTM做股票预测</a></li><li><ul><li><a href="#21__68" rel="nofollow">2.1 算法构建流程</a></li><li><a href="#22__72" rel="nofollow">2.2 部分代码</a></li></ul> 
  </li><li><a href="#3__293" rel="nofollow">3 实现效果</a></li><li><ul><li><a href="#31__295" rel="nofollow">3.1 数据</a></li><li><a href="#32__302" rel="nofollow">3.2 预测结果</a></li><li><ul><li><ul><li><a href="#_312" rel="nofollow">项目运行展示</a></li><li><a href="#_321" rel="nofollow">开发环境</a></li><li><a href="#_338" rel="nofollow">数据获取</a></li></ul> 
   </li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<hr color="#000000" size='1"'> 
<h2><a id="0__4"></a>0 前言</h2> 
<p>🔥 这两年开始毕业设计和毕业答辩的要求和难度不断提升，传统的毕设题目缺少创新和亮点，往往达不到毕业答辩的要求，这两年不断有学弟学妹告诉学长自己做的项目系统达不到老师的要求。</p> 
<p>为了大家能够顺利以及最少的精力通过毕设，学长分享优质毕业设计项目，今天要分享的是</p> 
<p>🚩 <strong>深度学习 大数据 股票预测系统</strong></p> 
<p>🥇学长这里给一个题目综合评分(每项满分5分)</p> 
<ul><li>难度系数：3分</li><li>工作量：3分</li><li>创新点：4分</li></ul> 
<p>🧿 <strong>选题指导, 项目分享：</strong></p> 
<p><a href="https://gitee.com/yaa-dc/warehouse-1/blob/master/python/README.md" rel="nofollow">https://gitee.com/yaa-dc/warehouse-1/blob/master/python/README.md</a></p> 
<h2><a id="1__22"></a>1 课题意义</h2> 
<p>利用神经网络模型如果能够提高对股票价格的预测精度，更好地掌握股票价格发展趋势，这对于投资者来说可以及时制定相应的发展策略，更好地应对未来发生的不确定性事件，对于个人来说可以降低投资风险，减少财产损失，实现高效投资，具有一定的实践价值。</p> 
<h3><a id="11__26"></a>1.1 股票预测主流方法</h3> 
<p>股票市场复杂、非线性的特点使我们难以捉摸其变化规律，目前有很多预测股票走势的论文和算法。</p> 
<p>定量分析从精确的数据资料中获得股票发展的价值规律，通过建立模型利用数学语言对股市的发展情况做出解释与预测。</p> 
<p>目前常用的定量分析方法有：</p> 
<ul><li>传统时间序列预测模型</li><li>马尔可夫链预测</li><li>灰色系统理论预测</li><li>遗传算法</li><li>机器学习预测等方法</li></ul> 
<h2><a id="2_LSTM_41"></a>2 什么是LSTM</h2> 
<p>LSTM是长短期记忆网络（LSTM，Long Short-Term Memory），想要理解什么是LSTM，首先要了解什么是循环神经网络。</p> 
<h3><a id="21__45"></a>2.1 循环神经网络</h3> 
<p>对于传统的BP神经网络如深度前馈网络、卷积神经网络来说，同层及跨层之间的神经元是独立的，但实际应用中对于一些有上下联系的序列来说，如果能够学习到它们之间的相互关系，使网络能够对不同时刻的输入序列产生一定的联系，像生物的大脑一样有“记忆功能”，这样的话我们的模型也就会有更低的训练出错频率及更好的泛化能力。</p> 
<p>JordanMI提出序列理论，描述了一种体现“并行分布式处理”的网络动态系统，适用于语音生成中的协同发音问题，并进行了相关仿真实验，ElmanJL认为连接主义模型中对时间如何表示是至关重要的，1990年他提出使用循环连接为网络提供动态内存，从相对简单的异或问题到探寻单词的语义特征，网络均学习到了有趣的内部表示，网络还将任务需求和内存需求结合在一起，由此形成了简单循环网络的基础框架。</p> 
<p>循环神经网络（RNN）之间的神经元是相互连接的，不仅在层与层之间的神经元建立连接，而且每一层之间的神经元也建立了连接，隐藏层神经元的输入由当前输入和上一时刻隐藏层神经元的输出共同决定，每一时刻的隐藏层神经元记住了上一时刻隐藏层神经元的输出，相当于对网络增添了“记忆”功能。我们都知道在输入序列中不可避免会出现重复或相似的某些序列信息，我们希望RNN能够保留这些记忆信息便于再次调用，且RNN结构中不同时刻参数是共享的，这一优点便于网络在不同位置依旧能将该重复信息识别出来，这样一来模型的泛化能力自然有所上升。</p> 
<p>RNN结构如下：</p> 
<p><img src="https://images2.imgbox.com/ee/ae/SzNuqpjk_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="21_LSTM_57"></a>2.1 LSTM诞生</h3> 
<p>RNN在解决长序列问题时未能有良好的建模效果，存在长期依赖的弊端，对此HochreiterS等人对神经单元做出了改进，引入自循环使梯度信息得以长时间持续流动，即模型可以拥有长期记忆信息，且自循环权重可以根据前后信息进行调整并不是固定的。作为RNN的一种特殊结构，它可以根据前后输入情况决定历史信息的去留，增进的门控机制可以动态改变累积的时间尺度进而控制神经单元的信息流，这样神经网络便能够自己根据情况决定清除或保留旧的信息，不至于状态信息过长造成网络崩溃，这便是长短期记忆（LSTM）网络。随着信息不断流入，该模型每个神经元内部的遗忘门、输入门、输出门三个门控机制会对每一时刻的信息做出判断并及时进行调整更新，LSTM模型现已广泛应用于无约束手写识别、语音识别、机器翻译等领域。</p> 
<p><img src="https://images2.imgbox.com/f3/a4/plnuELzq_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="2_LSTM_66"></a>2 如何用LSTM做股票预测</h2> 
<h3><a id="21__68"></a>2.1 算法构建流程</h3> 
<p><img src="https://images2.imgbox.com/1a/4a/RPkqTXB7_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="22__72"></a>2.2 部分代码</h3> 
<pre><code class="prism language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">import</span> math

<span class="token keyword">def</span> <span class="token function">LSTMtest</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">:</span>

    n1 <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span> <span class="token comment">#因为最后一位为label</span>
    n2 <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>n1<span class="token punctuation">,</span> n2<span class="token punctuation">)</span>

    <span class="token comment"># 设置常量</span>
    input_size <span class="token operator">=</span> n1  <span class="token comment"># 输入神经元个数</span>
    rnn_unit <span class="token operator">=</span> <span class="token number">10</span>    <span class="token comment"># LSTM单元(一层神经网络)中的中神经元的个数</span>
    lstm_layers <span class="token operator">=</span> <span class="token number">7</span>  <span class="token comment"># LSTM单元个数</span>
    output_size <span class="token operator">=</span> <span class="token number">1</span>  <span class="token comment"># 输出神经元个数（预测值）</span>
    lr <span class="token operator">=</span> <span class="token number">0.0006</span>      <span class="token comment"># 学习率</span>

    train_end_index <span class="token operator">=</span> math<span class="token punctuation">.</span>floor<span class="token punctuation">(</span>n2<span class="token operator">*</span><span class="token number">0.9</span><span class="token punctuation">)</span>  <span class="token comment"># 向下取整</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'train_end_index'</span><span class="token punctuation">,</span> train_end_index<span class="token punctuation">)</span>
    <span class="token comment"># 前90%数据作为训练集，后10%作为测试集</span>
    <span class="token comment"># 获取训练集</span>
    <span class="token comment"># time_step 时间步，batch_size 每一批次训练多少个样例</span>
    <span class="token keyword">def</span> <span class="token function">get_train_data</span><span class="token punctuation">(</span>batch_size<span class="token operator">=</span><span class="token number">60</span><span class="token punctuation">,</span> time_step<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> train_begin<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> train_end<span class="token operator">=</span>train_end_index<span class="token punctuation">)</span><span class="token punctuation">:</span>
        batch_index <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        data_train <span class="token operator">=</span> data<span class="token punctuation">[</span>train_begin<span class="token punctuation">:</span>train_end<span class="token punctuation">]</span>
        normalized_train_data <span class="token operator">=</span> <span class="token punctuation">(</span>data_train <span class="token operator">-</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>data_train<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>std<span class="token punctuation">(</span>data_train<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># 标准化</span>
        train_x<span class="token punctuation">,</span> train_y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment"># 训练集</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>normalized_train_data<span class="token punctuation">)</span> <span class="token operator">-</span> time_step<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> i <span class="token operator">%</span> batch_size <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
                <span class="token comment"># 开始位置</span>
                batch_index<span class="token punctuation">.</span>append<span class="token punctuation">(</span>i<span class="token punctuation">)</span>
                <span class="token comment"># 一次取time_step行数据</span>
            <span class="token comment"># x存储输入维度（不包括label） :X(最后一个不取）</span>
            <span class="token comment"># 标准化(归一化）</span>
            x <span class="token operator">=</span> normalized_train_data<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i <span class="token operator">+</span> time_step<span class="token punctuation">,</span> <span class="token punctuation">:</span>n1<span class="token punctuation">]</span>
            <span class="token comment"># y存储label</span>
            y <span class="token operator">=</span> normalized_train_data<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i <span class="token operator">+</span> time_step<span class="token punctuation">,</span> n1<span class="token punctuation">,</span> np<span class="token punctuation">.</span>newaxis<span class="token punctuation">]</span>
            <span class="token comment"># np.newaxis分别是在行或列上增加维度</span>
            train_x<span class="token punctuation">.</span>append<span class="token punctuation">(</span>x<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            train_y<span class="token punctuation">.</span>append<span class="token punctuation">(</span>y<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 结束位置</span>
        batch_index<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>normalized_train_data<span class="token punctuation">)</span> <span class="token operator">-</span> time_step<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'batch_index'</span><span class="token punctuation">,</span> batch_index<span class="token punctuation">)</span>
        <span class="token comment"># print('train_x', train_x)</span>
        <span class="token comment"># print('train_y', train_y)</span>
        <span class="token keyword">return</span> batch_index<span class="token punctuation">,</span> train_x<span class="token punctuation">,</span> train_y

    <span class="token comment"># 获取测试集</span>
    <span class="token keyword">def</span> <span class="token function">get_test_data</span><span class="token punctuation">(</span>time_step<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> test_begin<span class="token operator">=</span>train_end_index<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        data_test <span class="token operator">=</span> data<span class="token punctuation">[</span>test_begin<span class="token punctuation">:</span><span class="token punctuation">]</span>
        mean <span class="token operator">=</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>data_test<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        std <span class="token operator">=</span> np<span class="token punctuation">.</span>std<span class="token punctuation">(</span>data_test<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># 矩阵标准差</span>
        <span class="token comment"># 标准化(归一化）</span>
        normalized_test_data <span class="token operator">=</span> <span class="token punctuation">(</span>data_test <span class="token operator">-</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>data_test<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>std<span class="token punctuation">(</span>data_test<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token comment"># " // "表示整数除法。有size个sample</span>
        test_size <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>normalized_test_data<span class="token punctuation">)</span> <span class="token operator">+</span> time_step <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> time_step
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'test_size$$$$$$$$$$$$$$'</span><span class="token punctuation">,</span> test_size<span class="token punctuation">)</span>
        test_x<span class="token punctuation">,</span> test_y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>test_size <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> normalized_test_data<span class="token punctuation">[</span>i <span class="token operator">*</span> time_step<span class="token punctuation">:</span><span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> time_step<span class="token punctuation">,</span> <span class="token punctuation">:</span>n1<span class="token punctuation">]</span>
            y <span class="token operator">=</span> normalized_test_data<span class="token punctuation">[</span>i <span class="token operator">*</span> time_step<span class="token punctuation">:</span><span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> time_step<span class="token punctuation">,</span> n1<span class="token punctuation">]</span>
            test_x<span class="token punctuation">.</span>append<span class="token punctuation">(</span>x<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            test_y<span class="token punctuation">.</span>extend<span class="token punctuation">(</span>y<span class="token punctuation">)</span>
        test_x<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span>normalized_test_data<span class="token punctuation">[</span><span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> time_step<span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>n1<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        test_y<span class="token punctuation">.</span>extend<span class="token punctuation">(</span><span class="token punctuation">(</span>normalized_test_data<span class="token punctuation">[</span><span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> time_step<span class="token punctuation">:</span><span class="token punctuation">,</span> n1<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> mean<span class="token punctuation">,</span> std<span class="token punctuation">,</span> test_x<span class="token punctuation">,</span> test_y

    <span class="token comment"># ——————————————————定义神经网络变量——————————————————</span>
    <span class="token comment"># 输入层、输出层权重、偏置、dropout参数</span>
    <span class="token comment"># 随机产生 w,b</span>
    weights <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string">'in'</span><span class="token punctuation">:</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>random_normal<span class="token punctuation">(</span><span class="token punctuation">[</span>input_size<span class="token punctuation">,</span> rnn_unit<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token string">'out'</span><span class="token punctuation">:</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>random_normal<span class="token punctuation">(</span><span class="token punctuation">[</span>rnn_unit<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">}</span>
    biases <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string">'in'</span><span class="token punctuation">:</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span>rnn_unit<span class="token punctuation">,</span> <span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token string">'out'</span><span class="token punctuation">:</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">}</span>
    keep_prob <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'keep_prob'</span><span class="token punctuation">)</span>  <span class="token comment"># dropout 防止过拟合</span>

    <span class="token comment"># ——————————————————定义神经网络——————————————————</span>
    <span class="token keyword">def</span> <span class="token function">lstmCell</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># basicLstm单元</span>
        <span class="token comment"># tf.nn.rnn_cell.BasicLSTMCell(self, num_units, forget_bias=1.0,</span>
        <span class="token comment"># tate_is_tuple=True, activation=None, reuse=None, name=None) </span>
        <span class="token comment"># num_units:int类型，LSTM单元(一层神经网络)中的中神经元的个数，和前馈神经网络中隐含层神经元个数意思相同</span>
        <span class="token comment"># forget_bias:float类型，偏置增加了忘记门。从CudnnLSTM训练的检查点(checkpoin)恢复时，必须手动设置为0.0。</span>
        <span class="token comment"># state_is_tuple:如果为True，则接受和返回的状态是c_state和m_state的2-tuple；如果为False，则他们沿着列轴连接。后一种即将被弃用。</span>
        <span class="token comment"># （LSTM会保留两个state，也就是主线的state(c_state),和分线的state(m_state)，会包含在元组（tuple）里边</span>
        <span class="token comment"># state_is_tuple=True就是判定生成的是否为一个元组）</span>
        <span class="token comment">#   初始化的 c 和 a 都是zero_state 也就是都为list[]的zero，这是参数state_is_tuple的情况下</span>
        <span class="token comment">#   初始state,全部为0，慢慢的累加记忆</span>
        <span class="token comment"># activation:内部状态的激活函数。默认为tanh</span>
        <span class="token comment"># reuse:布尔类型，描述是否在现有范围中重用变量。如果不为True，并且现有范围已经具有给定变量，则会引发错误。</span>
        <span class="token comment"># name:String类型，层的名称。具有相同名称的层将共享权重，但为了避免错误，在这种情况下需要reuse=True.</span>
        <span class="token comment">#</span>

        basicLstm <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>rnn_cell<span class="token punctuation">.</span>BasicLSTMCell<span class="token punctuation">(</span>rnn_unit<span class="token punctuation">,</span> forget_bias<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">,</span> state_is_tuple<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token comment"># dropout 未使用</span>
        drop <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>rnn_cell<span class="token punctuation">.</span>DropoutWrapper<span class="token punctuation">(</span>basicLstm<span class="token punctuation">,</span> output_keep_prob<span class="token operator">=</span>keep_prob<span class="token punctuation">)</span>
        <span class="token keyword">return</span> basicLstm

   

    <span class="token keyword">def</span> <span class="token function">lstm</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 参数：输入网络批次数目</span>
        batch_size <span class="token operator">=</span> tf<span class="token punctuation">.</span>shape<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        time_step <span class="token operator">=</span> tf<span class="token punctuation">.</span>shape<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
        w_in <span class="token operator">=</span> weights<span class="token punctuation">[</span><span class="token string">'in'</span><span class="token punctuation">]</span>
        b_in <span class="token operator">=</span> biases<span class="token punctuation">[</span><span class="token string">'in'</span><span class="token punctuation">]</span>

        <span class="token comment"># 忘记门（输入门）</span>
        <span class="token comment"># 因为要进行矩阵乘法,所以reshape</span>
        <span class="token comment"># 需要将tensor转成2维进行计算</span>
        <span class="token builtin">input</span> <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>X<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> input_size<span class="token punctuation">]</span><span class="token punctuation">)</span>
        input_rnn <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> w_in<span class="token punctuation">)</span> <span class="token operator">+</span> b_in
        <span class="token comment"># 将tensor转成3维，计算后的结果作为忘记门的输入</span>
        input_rnn <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>input_rnn<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> time_step<span class="token punctuation">,</span> rnn_unit<span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'input_rnn'</span><span class="token punctuation">,</span> input_rnn<span class="token punctuation">)</span>
        <span class="token comment"># 更新门</span>
        <span class="token comment"># 构建多层的lstm</span>
        cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>rnn_cell<span class="token punctuation">.</span>MultiRNNCell<span class="token punctuation">(</span><span class="token punctuation">[</span>lstmCell<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>lstm_layers<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        init_state <span class="token operator">=</span> cell<span class="token punctuation">.</span>zero_state<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>

        <span class="token comment"># 输出门</span>
        w_out <span class="token operator">=</span> weights<span class="token punctuation">[</span><span class="token string">'out'</span><span class="token punctuation">]</span>
        b_out <span class="token operator">=</span> biases<span class="token punctuation">[</span><span class="token string">'out'</span><span class="token punctuation">]</span>
        <span class="token comment"># output_rnn是最后一层每个step的输出,final_states是每一层的最后那个step的输出</span>
        output_rnn<span class="token punctuation">,</span> final_states <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>dynamic_rnn<span class="token punctuation">(</span>cell<span class="token punctuation">,</span> input_rnn<span class="token punctuation">,</span> initial_state<span class="token operator">=</span>init_state<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
        output <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>output_rnn<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> rnn_unit<span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token comment"># 输出值，同时作为下一层输入门的输入</span>
        pred <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>output<span class="token punctuation">,</span> w_out<span class="token punctuation">)</span> <span class="token operator">+</span> b_out
        <span class="token keyword">return</span> pred<span class="token punctuation">,</span> final_states

    <span class="token comment"># ————————————————训练模型————————————————————</span>

    <span class="token keyword">def</span> <span class="token function">train_lstm</span><span class="token punctuation">(</span>batch_size<span class="token operator">=</span><span class="token number">60</span><span class="token punctuation">,</span> time_step<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> train_begin<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> train_end<span class="token operator">=</span>train_end_index<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 于是就有了tf.placeholder，</span>
        <span class="token comment"># 我们每次可以将 一个minibatch传入到x = tf.placeholder(tf.float32,[None,32])上，</span>
        <span class="token comment"># 下一次传入的x都替换掉上一次传入的x，</span>
        <span class="token comment"># 这样就对于所有传入的minibatch x就只会产生一个op，</span>
        <span class="token comment"># 不会产生其他多余的op，进而减少了graph的开销。</span>

        X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> time_step<span class="token punctuation">,</span> input_size<span class="token punctuation">]</span><span class="token punctuation">)</span>
        Y <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> time_step<span class="token punctuation">,</span> output_size<span class="token punctuation">]</span><span class="token punctuation">)</span>
        batch_index<span class="token punctuation">,</span> train_x<span class="token punctuation">,</span> train_y <span class="token operator">=</span> get_train_data<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> time_step<span class="token punctuation">,</span> train_begin<span class="token punctuation">,</span> train_end<span class="token punctuation">)</span>
        <span class="token comment"># 用tf.variable_scope来定义重复利用,LSTM会经常用到</span>
        <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"sec_lstm"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            pred<span class="token punctuation">,</span> state_ <span class="token operator">=</span> lstm<span class="token punctuation">(</span>X<span class="token punctuation">)</span> <span class="token comment"># pred输出值，state_是每一层的最后那个step的输出</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'pred,state_'</span><span class="token punctuation">,</span> pred<span class="token punctuation">,</span> state_<span class="token punctuation">)</span>

        <span class="token comment"># 损失函数</span>
        <span class="token comment"># [-1]——列表从后往前数第一列，即pred为预测值，Y为真实值(Label)</span>
        <span class="token comment">#tf.reduce_mean 函数用于计算张量tensor沿着指定的数轴（tensor的某一维度）上的的平均值</span>
        loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">-</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>Y<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 误差loss反向传播——均方误差损失</span>
        <span class="token comment"># 本质上是带有动量项的RMSprop，它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。</span>
        <span class="token comment"># Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳.</span>
        train_op <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>AdamOptimizer<span class="token punctuation">(</span>lr<span class="token punctuation">)</span><span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
        saver <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>Saver<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>global_variables<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> max_to_keep<span class="token operator">=</span><span class="token number">15</span><span class="token punctuation">)</span>

        <span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>
            <span class="token comment"># 初始化</span>
            sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>global_variables_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            theloss <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
            <span class="token comment"># 迭代次数</span>
            <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token keyword">for</span> step <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>batch_index<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                    <span class="token comment"># sess.run(b, feed_dict = replace_dict)</span>
                    state_<span class="token punctuation">,</span> loss_ <span class="token operator">=</span> sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">[</span>train_op<span class="token punctuation">,</span> loss<span class="token punctuation">]</span><span class="token punctuation">,</span>
                                        feed_dict<span class="token operator">=</span><span class="token punctuation">{<!-- --></span>X<span class="token punctuation">:</span> train_x<span class="token punctuation">[</span>batch_index<span class="token punctuation">[</span>step<span class="token punctuation">]</span><span class="token punctuation">:</span>batch_index<span class="token punctuation">[</span>step <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                                                   Y<span class="token punctuation">:</span> train_y<span class="token punctuation">[</span>batch_index<span class="token punctuation">[</span>step<span class="token punctuation">]</span><span class="token punctuation">:</span>batch_index<span class="token punctuation">[</span>step <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                                                   keep_prob<span class="token punctuation">:</span> <span class="token number">0.5</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
                                        <span class="token comment">#  使用feed_dict完成矩阵乘法 处理多输入</span>
                                        <span class="token comment">#  feed_dict的作用是给使用placeholder创建出来的tensor赋值</span>


                                        <span class="token comment">#  [batch_index[step]: batch_index[step + 1]]这个区间的X与Y</span>
                                        <span class="token comment">#  keep_prob的意思是：留下的神经元的概率，如果keep_prob为0的话， 就是让所有的神经元都失活。</span>
                <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Number of iterations:"</span><span class="token punctuation">,</span> i<span class="token punctuation">,</span> <span class="token string">" loss:"</span><span class="token punctuation">,</span> loss_<span class="token punctuation">)</span>
                theloss<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss_<span class="token punctuation">)</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"model_save: "</span><span class="token punctuation">,</span> saver<span class="token punctuation">.</span>save<span class="token punctuation">(</span>sess<span class="token punctuation">,</span> <span class="token string">'model_save2\\modle.ckpt'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"The train has finished"</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> theloss

    theloss <span class="token operator">=</span> train_lstm<span class="token punctuation">(</span><span class="token punctuation">)</span>

    
            <span class="token comment"># 相对误差=（测量值-计算值）/计算值×100%</span>
            test_y <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>test_y<span class="token punctuation">)</span> <span class="token operator">*</span> std<span class="token punctuation">[</span>n1<span class="token punctuation">]</span> <span class="token operator">+</span> mean<span class="token punctuation">[</span>n1<span class="token punctuation">]</span>
            test_predict <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>test_predict<span class="token punctuation">)</span> <span class="token operator">*</span> std<span class="token punctuation">[</span>n1<span class="token punctuation">]</span> <span class="token operator">+</span> mean<span class="token punctuation">[</span>n1<span class="token punctuation">]</span>
            acc <span class="token operator">=</span> np<span class="token punctuation">.</span>average<span class="token punctuation">(</span>np<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span>test_predict <span class="token operator">-</span> test_y<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token builtin">len</span><span class="token punctuation">(</span>test_predict<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">/</span> test_y<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token builtin">len</span><span class="token punctuation">(</span>test_predict<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"预测的相对误差:"</span><span class="token punctuation">,</span> acc<span class="token punctuation">)</span>

            <span class="token keyword">print</span><span class="token punctuation">(</span>theloss<span class="token punctuation">)</span>
            plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span><span class="token punctuation">)</span>
            plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>theloss<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> theloss<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token punctuation">)</span>
            plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'times'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>
            plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'loss valuet'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>
            plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'loss-----blue'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>
            plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token comment"># 以折线图表示预测结果</span>
            plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span><span class="token punctuation">)</span>
            plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>test_predict<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> test_predict<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token punctuation">)</span>
            plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>test_y<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> test_y<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'r'</span><span class="token punctuation">)</span>
            plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'time value/day'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>
            plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'close value/point'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>
            plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'predict-----blue,real-----red'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>
            plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>



    prediction<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>需要完整代码工程的同学，请联系学长获取</strong></p> 
<h2><a id="3__293"></a>3 实现效果</h2> 
<h3><a id="31__295"></a>3.1 数据</h3> 
<p>采集股票数据<br> <img src="https://images2.imgbox.com/f0/c1/dZOsHKje_o.png" alt="在这里插入图片描述"><br> 任选几支股票作为研究对象。</p> 
<h3><a id="32__302"></a>3.2 预测结果</h3> 
<p><img src="https://images2.imgbox.com/3d/cf/wu2GazpI_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/dc/2a/DrYC5v5p_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/2b/ff/sUuHzkxN_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/c9/e5/FwKyrlQF_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="_312"></a>项目运行展示</h5> 
<p>废话不多说, 先展示项目运行结果, 后面才进行技术讲解</p> 
<p>对某公司的股票进行分析和预测 :<br> <img src="https://images2.imgbox.com/9b/ad/QtPVQm7w_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="_321"></a>开发环境</h5> 
<p>如果只运行web项目，则只需安装如下包：</p> 
<ul><li> <p>python 3.6.x</p> </li><li> <p>django &gt;= 2.1.4 （或者使用conda安装最新版）</p> </li><li> <p>pandas &gt;= 0.23.4 （或者使用conda安装最新版）</p> </li><li> <p>numpy &gt;= 1.15.2 （或者使用conda安装最新版）<br> *apscheduler = 2.1.2 （请用pip install apscheduler==2.1.2 安装，conda装的版本不兼容）<br> 如果需要训练模型或者使用模型来预测(注：需要保证本机拥有 NVIDIA GPU以及显卡驱动)，则还需要安装：</p> </li><li> <p>tensorflow-gpu &gt;= 1.10.0 （可以使用conda安装最新版。如用conda安装，cudatoolkit和cudnn会被自动安装）</p> </li><li> <p>cudatoolkit &gt;= 9.0 （根据自己本机的显卡型号决定，请去NVIDIA官网查看）</p> </li><li> <p>cudnn &gt;= 7.1.4 （版本与cudatoolkit9.0对应的，其他版本请去NVIDIA官网查看对应的cudatoolkit版本）</p> </li><li> <p>keras &gt;= 2.2.2 （可以使用conda安装最新版）</p> </li><li> <p>matplotlib &gt;= 2.2.2 （可以使用conda安装最新版）</p> </li></ul> 
<h5><a id="_338"></a>数据获取</h5> 
<p>训练模型的数据，即10个公司的历史股票数据。获取国内上市公司历史股票数据, 并以csv格式保存下来。csv格式方便用pandas读取，输入到LSTM神经网络模型， 用于训练模型以及预测股票数据。</p> 
<p>🧿 <strong>选题指导, 项目分享：</strong></p> 
<p><a href="https://gitee.com/yaa-dc/warehouse-1/blob/master/python/README.md" rel="nofollow">https://gitee.com/yaa-dc/warehouse-1/blob/master/python/README.md</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/27bcdfe6f6f8c90d85eb64d0dd9237e2/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">SpringBoot 如何配置 SSL</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/aa621dfc539067295fdb585610fe7c4d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Camunda适配达梦数据库</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>