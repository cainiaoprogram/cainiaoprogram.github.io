<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>看源码逐行学习ChatGLM2-6B大模型，项目中的modeling_chatglm.py文件 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="看源码逐行学习ChatGLM2-6B大模型，项目中的modeling_chatglm.py文件" />
<meta property="og:description" content="模型代码地址
&#34;&#34;&#34; PyTorch ChatGLM model. ChatGLMModel模型结构 (假设输入X大小为 3x5) 转载自：https://blog.csdn.net/hjyai94/article/details/132504200 (embedding) Embedding (转置后 5x3x4096) word_embeddings: Embedding(65024, 4096) (rotary_pos_emb) RotaryEmbedding() (encoder) GLMTransformer (layers) ModuleList 0-27: 28 x GLMBlock (input_layernorm) RMSNorm() (输入输出大小: 5x3x4096) (self_attention) SelfAttention (query_key_value) Linear(in_features=4096, out_features=4608, bias=True) (core_attention) CoreAttention(attention_dropout) Dropout(p=0.0, inplace=False)) (dense) Linear(in_features=4096, out_features=4096, bias=False) (post_attention_layernorm) RMSNorm() (mlp) MLP (dense_h_to_4h) Linear(in_features=4096, out_features=27392, bias=False) (dense_4h_to_h) Linear(in_features=13696, out_features=4096, bias=False) (final_layernorm) RMSNorm() (output_layer) Linear(in_features=4096, out_features=65024, bias=False) (输出大小: 3x5x65024) &#34;&#34;&#34; #导入基础库 import math import copy import warnings import re import sys #导入pytorch相关库 import torch import torch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/87137d0e0210f6b07c65740db7d71fab/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-06T11:23:25+08:00" />
<meta property="article:modified_time" content="2023-10-06T11:23:25+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">看源码逐行学习ChatGLM2-6B大模型，项目中的modeling_chatglm.py文件</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atelier-sulphurpool-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><strong><a href="https://huggingface.co/THUDM/chatglm2-6b" rel="nofollow">模型代码地址</a></strong></p> 
<pre><code class="prism language-python"><span class="token triple-quoted-string string">""" PyTorch ChatGLM model. 
ChatGLMModel模型结构 (假设输入X大小为 3x5)  转载自：https://blog.csdn.net/hjyai94/article/details/132504200
(embedding) Embedding (转置后 5x3x4096)
    word_embeddings: Embedding(65024, 4096)
(rotary_pos_emb) RotaryEmbedding()
(encoder) GLMTransformer
    (layers) ModuleList
    0-27: 28 x GLMBlock
        (input_layernorm) RMSNorm() (输入输出大小: 5x3x4096)
        (self_attention) SelfAttention
            (query_key_value) Linear(in_features=4096, out_features=4608, bias=True)
            (core_attention) CoreAttention(attention_dropout) Dropout(p=0.0, inplace=False))
            (dense) Linear(in_features=4096, out_features=4096, bias=False)
        (post_attention_layernorm) RMSNorm()
        (mlp) MLP
            (dense_h_to_4h) Linear(in_features=4096, out_features=27392, bias=False)
            (dense_4h_to_h) Linear(in_features=13696, out_features=4096, bias=False)
    (final_layernorm) RMSNorm()
(output_layer) Linear(in_features=4096, out_features=65024, bias=False) (输出大小: 3x5x65024)

"""</span>
<span class="token comment">#导入基础库</span>
<span class="token keyword">import</span> math
<span class="token keyword">import</span> copy
<span class="token keyword">import</span> warnings
<span class="token keyword">import</span> re
<span class="token keyword">import</span> sys
<span class="token comment">#导入pytorch相关库</span>
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>checkpoint
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> CrossEntropyLoss<span class="token punctuation">,</span> LayerNorm
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>utils <span class="token keyword">import</span> skip_init
<span class="token keyword">from</span> typing <span class="token keyword">import</span> Optional<span class="token punctuation">,</span> Tuple<span class="token punctuation">,</span> Union<span class="token punctuation">,</span> List<span class="token punctuation">,</span> Callable<span class="token punctuation">,</span> Dict<span class="token punctuation">,</span> Any
<span class="token comment">#导入transformer相关库</span>
<span class="token keyword">from</span> transformers<span class="token punctuation">.</span>modeling_outputs <span class="token keyword">import</span> <span class="token punctuation">(</span>
    BaseModelOutputWithPast<span class="token punctuation">,</span>
    CausalLMOutputWithPast<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
<span class="token keyword">from</span> transformers<span class="token punctuation">.</span>modeling_utils <span class="token keyword">import</span> PreTrainedModel
<span class="token keyword">from</span> transformers<span class="token punctuation">.</span>utils <span class="token keyword">import</span> logging
<span class="token keyword">from</span> transformers<span class="token punctuation">.</span>generation<span class="token punctuation">.</span>logits_process <span class="token keyword">import</span> LogitsProcessor
<span class="token keyword">from</span> transformers<span class="token punctuation">.</span>generation<span class="token punctuation">.</span>utils <span class="token keyword">import</span> LogitsProcessorList<span class="token punctuation">,</span> StoppingCriteriaList<span class="token punctuation">,</span> GenerationConfig<span class="token punctuation">,</span> ModelOutput
<span class="token comment">#导入同一目录下的configuration_chatglm.py的ChatGLMConfig类，这个类里面就是定义了模型结构参数，例如网络层数num_layers，词表大小vocab_size等参数</span>
<span class="token keyword">from</span> <span class="token punctuation">.</span>configuration_chatglm <span class="token keyword">import</span> ChatGLMConfig

<span class="token comment"># flags required to enable jit fusion kernels</span>
<span class="token comment">#在非 macOS 系统上禁用性能分析模式和执行器，并允许在 CPU 和 GPU 上执行运算图融合。</span>
<span class="token keyword">if</span> sys<span class="token punctuation">.</span>platform <span class="token operator">!=</span> <span class="token string">'darwin'</span><span class="token punctuation">:</span>
    torch<span class="token punctuation">.</span>_C<span class="token punctuation">.</span>_jit_set_profiling_mode<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">)</span>
    torch<span class="token punctuation">.</span>_C<span class="token punctuation">.</span>_jit_set_profiling_executor<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">)</span>
    torch<span class="token punctuation">.</span>_C<span class="token punctuation">.</span>_jit_override_can_fuse_on_cpu<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    torch<span class="token punctuation">.</span>_C<span class="token punctuation">.</span>_jit_override_can_fuse_on_gpu<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment">#logging来自transformers.utils 模块</span>
<span class="token comment">#__name__: 是一个内置的 Python 变量，表示当前模块的名称。如果当前模块是主程序，则 __name__ 的值为 '__main__'。如果在其它地方被导入，则为该模块的名称。</span>
<span class="token comment">#logger 是一个用来记录（log）信息的对象。在配置了 logger 后，你可以通过它在代码的各个部分记录不同级别的消息（例如：debug, info, warning, error, critical）。</span>
<span class="token comment">#例如可以logger.info("This is an info message")   logger.warning("This is a warning message")</span>
logger <span class="token operator">=</span> logging<span class="token punctuation">.</span>get_logger<span class="token punctuation">(</span>__name__<span class="token punctuation">)</span>

<span class="token comment">#模型的地址</span>
_CHECKPOINT_FOR_DOC <span class="token operator">=</span> <span class="token string">"THUDM/ChatGLM2-6B"</span>

<span class="token comment">#模型配置参数文件的地址</span>
_CONFIG_FOR_DOC <span class="token operator">=</span> <span class="token string">"ChatGLM6BConfig"</span>
<span class="token comment">#预训练模型文件的地址</span>
CHATGLM_6B_PRETRAINED_MODEL_ARCHIVE_LIST <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token string">"THUDM/chatglm2-6b"</span><span class="token punctuation">,</span>
    <span class="token comment"># See all ChatGLM models at https://huggingface.co/models?filter=chatglm</span>
<span class="token punctuation">]</span>

<span class="token comment">#类的初始化方法</span>
<span class="token keyword">def</span> <span class="token function">default_init</span><span class="token punctuation">(</span>cls<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> cls<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">InvalidScoreLogitsProcessor</span><span class="token punctuation">(</span>LogitsProcessor<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__call__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_ids<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">,</span> scores<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">:</span>
        <span class="token keyword">if</span> torch<span class="token punctuation">.</span>isnan<span class="token punctuation">(</span>scores<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">any</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">or</span> torch<span class="token punctuation">.</span>isinf<span class="token punctuation">(</span>scores<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">any</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            scores<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>
            scores<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">5e4</span>
        <span class="token keyword">return</span> scores

<span class="token comment">#该前缀编码层类用于微调，会在原ChatGLM2的模型上，在模型输入时加入一个前缀编码层，微调时只会更新这部分参数的梯度</span>
<span class="token keyword">class</span> <span class="token class-name">PrefixEncoder</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    The torch.nn model to encode the prefix
    Input shape: (batch-size, prefix-length)
    Output shape: (batch-size, prefix-length, 2*layers*hidden)
    """</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">:</span> ChatGLMConfig<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment">#默认为false,是否自定义前缀编码器层，如果self.prefix_projection是True进入if</span>
        self<span class="token punctuation">.</span>prefix_projection <span class="token operator">=</span> config<span class="token punctuation">.</span>prefix_projection
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>prefix_projection<span class="token punctuation">:</span>
            <span class="token comment"># Use a two-layer MLP to encode the prefix</span>
            <span class="token comment">#默认num_layers=28，kv_channels=128，multi_query_group_num=1，这些可以从同目录下的configuration_chatglm.py的ChatGLMConfig类看到</span>
            kv_size <span class="token operator">=</span> config<span class="token punctuation">.</span>num_layers <span class="token operator">*</span> config<span class="token punctuation">.</span>kv_channels <span class="token operator">*</span> config<span class="token punctuation">.</span>multi_query_group_num <span class="token operator">*</span> <span class="token number">2</span>
            <span class="token comment">#默认pre_seq_len=None，pre_seq_len表示每次前缀序列的预定义长度，作为Embedding的输入节点数，kv_size表示Embedding的输出节点数</span>
            self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>config<span class="token punctuation">.</span>pre_seq_len<span class="token punctuation">,</span> kv_size<span class="token punctuation">)</span>
            <span class="token comment">#定义一个trans层，数据流为embedding层--&gt;trans层--&gt;decoding层，用于帮助特征转化一下再进入decoding层</span>
            self<span class="token punctuation">.</span>trans <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
                torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>kv_size<span class="token punctuation">,</span> config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span><span class="token punctuation">,</span>
                torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Tanh<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> kv_size<span class="token punctuation">)</span>
            <span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment">#否则直接定义embedding</span>
            self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>config<span class="token punctuation">.</span>pre_seq_len<span class="token punctuation">,</span>
                                                config<span class="token punctuation">.</span>num_layers <span class="token operator">*</span> config<span class="token punctuation">.</span>kv_channels <span class="token operator">*</span> config<span class="token punctuation">.</span>multi_query_group_num <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> prefix<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">#如果使用自定义层则数据流为prompt--&gt;Embedding层(后面定义的)--&gt;embedding层(self.embedding)--&gt;trans层--&gt;decoding层</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>prefix_projection<span class="token punctuation">:</span>
            prefix_tokens <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>prefix<span class="token punctuation">)</span>
            past_key_values <span class="token operator">=</span> self<span class="token punctuation">.</span>trans<span class="token punctuation">(</span>prefix_tokens<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment">#如果不使用自定义层则数据流为prompt--&gt;Embedding层(后面定义的)--&gt;embedding层(self.embedding)--&gt;decoding层</span>
            past_key_values <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>prefix<span class="token punctuation">)</span>
        <span class="token keyword">return</span> past_key_values

<span class="token comment">#定义了一个方法来分tensor变量，方法为根据最好一层分，例如输入为[2,512,8]分解块数为4个则会生成4个[2,512,2]</span>
<span class="token keyword">def</span> <span class="token function">split_tensor_along_last_dim</span><span class="token punctuation">(</span>
        tensor<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>
        num_partitions<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>
        contiguous_split_chunks<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> List<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Split a tensor along its last dimension.

    Arguments:
        tensor: input tensor.
        num_partitions: number of partitions to split the tensor
        contiguous_split_chunks: If True, make each chunk contiguous
                                 in memory.

    Returns:
        A list of Tensors
    """</span>
    <span class="token comment"># Get the size and dimension.</span>
    <span class="token comment">#假设tensor为[2,512,8]，tensor.dim()会返回3，因此last_dim=2</span>
    last_dim <span class="token operator">=</span> tensor<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span>
    <span class="token comment">#tensor.size()会返回一个元组（2,512,8），因此tensor.size()[last_dim]=8</span>
    <span class="token comment">#因此num_partitions为4的话，last_dim_size为2，注意//为向下取整</span>
    last_dim_size <span class="token operator">=</span> tensor<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span>last_dim<span class="token punctuation">]</span> <span class="token operator">//</span> num_partitions
    <span class="token comment"># Split.</span>
    <span class="token comment">#根据最后一维度划分，得到4个[2,512,2]，如果多的话最后一个可能为[2,512,1]</span>
    tensor_list <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> last_dim_size<span class="token punctuation">,</span> dim<span class="token operator">=</span>last_dim<span class="token punctuation">)</span>
    <span class="token comment"># Note: torch.split does not create contiguous tensors by default.</span>
    <span class="token comment">#如果需要得到内存连续的张量</span>
    <span class="token keyword">if</span> contiguous_split_chunks<span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token builtin">tuple</span><span class="token punctuation">(</span>chunk<span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> chunk <span class="token keyword">in</span> tensor_list<span class="token punctuation">)</span>

    <span class="token keyword">return</span> tensor_list

<span class="token comment">#位置编码层，采用了RoPE位置编码方式，采用了PaLM的实现方式</span>
<span class="token keyword">class</span> <span class="token class-name">RotaryEmbedding</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> original_impl<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment">#先计算好θ</span>
        inv_freq <span class="token operator">=</span> <span class="token number">1.0</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">10000</span> <span class="token operator">**</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> dim<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span> <span class="token operator">/</span> dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"inv_freq"</span><span class="token punctuation">,</span> inv_freq<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dim <span class="token operator">=</span> dim
        self<span class="token punctuation">.</span>original_impl <span class="token operator">=</span> original_impl

    <span class="token keyword">def</span> <span class="token function">forward_impl</span><span class="token punctuation">(</span>
            self<span class="token punctuation">,</span> seq_len<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> n_elem<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> dtype<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span> device<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">,</span> base<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">10000</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Enhanced Transformer with Rotary Position Embedding.

        Derived from: https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/
        transformers/rope/__init__.py. MIT License:
        https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/license.
        """</span>
        <span class="token comment"># $\Theta = {\theta_i = 10000^{\frac{2(i-1)}{d}}, i \in [1, 2, ..., \frac{d}{2}]}$</span>
        theta <span class="token operator">=</span> <span class="token number">1.0</span> <span class="token operator">/</span> <span class="token punctuation">(</span>base <span class="token operator">**</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> n_elem<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span> <span class="token operator">/</span> n_elem<span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token comment"># Create position indexes `[0, 1, ..., seq_len - 1]`</span>
        seq_idx <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>seq_len<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>

        <span class="token comment"># Calculate the product of position index and $\theta_i$</span>
        idx_theta <span class="token operator">=</span> torch<span class="token punctuation">.</span>outer<span class="token punctuation">(</span>seq_idx<span class="token punctuation">,</span> theta<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

        cache <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>torch<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>idx_theta<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>idx_theta<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token comment"># this is to mimic the behaviour of complex32, else we will get different results</span>
        <span class="token keyword">if</span> dtype <span class="token keyword">in</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float16<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>bfloat16<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>int8<span class="token punctuation">)</span><span class="token punctuation">:</span>
            cache <span class="token operator">=</span> cache<span class="token punctuation">.</span>bfloat16<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> dtype <span class="token operator">==</span> torch<span class="token punctuation">.</span>bfloat16 <span class="token keyword">else</span> cache<span class="token punctuation">.</span>half<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> cache

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> max_seq_len<span class="token punctuation">,</span> offset<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>forward_impl<span class="token punctuation">(</span>
            max_seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> dtype<span class="token operator">=</span>self<span class="token punctuation">.</span>inv_freq<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>self<span class="token punctuation">.</span>inv_freq<span class="token punctuation">.</span>device
        <span class="token punctuation">)</span>

<span class="token comment">#把下面的函数注释成了pytorch库函数，其中jit是代表"just-in-time"（即时）编译。jit模块是PyTorch的一个子模块，提供了用于将Python代码转换为高效、优化的机器码的工具。</span>
<span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>script</span>
<span class="token keyword">def</span> <span class="token function">apply_rotary_pos_emb</span><span class="token punctuation">(</span>x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> rope_cache<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
    <span class="token comment"># x: [sq, b, np, hn]</span>
    sq<span class="token punctuation">,</span> b<span class="token punctuation">,</span> np<span class="token punctuation">,</span> hn <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>
    rot_dim <span class="token operator">=</span> rope_cache<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">2</span>
    x<span class="token punctuation">,</span> x_pass <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>rot_dim<span class="token punctuation">]</span><span class="token punctuation">,</span> x<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> rot_dim<span class="token punctuation">:</span><span class="token punctuation">]</span>
    <span class="token comment"># truncate to support variable sizes</span>
    rope_cache <span class="token operator">=</span> rope_cache<span class="token punctuation">[</span><span class="token punctuation">:</span>sq<span class="token punctuation">]</span>
    xshaped <span class="token operator">=</span> x<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>sq<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> np<span class="token punctuation">,</span> rot_dim <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
    rope_cache <span class="token operator">=</span> rope_cache<span class="token punctuation">.</span>view<span class="token punctuation">(</span>sq<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> xshaped<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
    x_out2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>
        <span class="token punctuation">[</span>
            xshaped<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> rope_cache<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">-</span> xshaped<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> rope_cache<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
            xshaped<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> rope_cache<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> xshaped<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> rope_cache<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
    x_out2 <span class="token operator">=</span> x_out2<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>x_out2<span class="token punctuation">,</span> x_pass<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token comment">#RMSNorm 类继承自 torch.nn.Module，它是创建 PyTorch 模块的基类。</span>
<span class="token comment">#定义了一个自定义的 PyTorch 模块 RMSNorm，它通过计算均方根归一化输入张量并应用可训练的权重，来实现一种特定的归一化操作。</span>
<span class="token keyword">class</span> <span class="token class-name">RMSNorm</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> normalized_shape<span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-5</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment">#通过 torch.nn.Parameter 创建了一个可训练的权重张量 self.weight，其形状由 normalized_shape 指定，并将其存储为模块的属性。</span>
        self<span class="token punctuation">.</span>weight <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>normalized_shape<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>eps <span class="token operator">=</span> eps<span class="token comment">#用于设置一个小的常数以防止除以零错误</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
        input_dtype <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>dtype<span class="token comment">#用于保存输入张量的数据类型，以便最后返回时将输出转换回相同的数据类型。</span>
        variance <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment">#首先，通过 hidden_states.to(torch.float32) 将输入张量转换为 torch.float32 数据类型，然后计算其平方（.pow(2))，再在最后一个维度上求平均值（.mean(-1, keepdim=True)）。这样得到的 variance 张量表示输入张量在最后一个维度上的方差。</span>
        hidden_states <span class="token operator">=</span> hidden_states <span class="token operator">*</span> torch<span class="token punctuation">.</span>rsqrt<span class="token punctuation">(</span>variance <span class="token operator">+</span> self<span class="token punctuation">.</span>eps<span class="token punctuation">)</span><span class="token comment">#接下来，将输入张量 hidden_states 乘以 torch.rsqrt(variance + self.eps)，其中 torch.rsqrt 是计算倒数的平方根的函数。这样做是为了对输入张量进行归一化，使其具有单位标准差。</span>
        <span class="token comment">#hidden_states 乘以权重张量 self.weight，并将结果转换回输入张量的数据类型 input_dtype。</span>
        <span class="token keyword">return</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>weight <span class="token operator">*</span> hidden_states<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>input_dtype<span class="token punctuation">)</span>

<span class="token comment">#自注意力层核心</span>
<span class="token keyword">class</span> <span class="token class-name">CoreAttention</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">:</span> ChatGLMConfig<span class="token punctuation">,</span> layer_number<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>CoreAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment">#默认apply_query_key_layer_scaling为True，使用Q，K层的维度</span>
        self<span class="token punctuation">.</span>apply_query_key_layer_scaling <span class="token operator">=</span> config<span class="token punctuation">.</span>apply_query_key_layer_scaling
        <span class="token comment">#默认attention_softmax_in_fp32为True，softmax保留32位</span>
        self<span class="token punctuation">.</span>attention_softmax_in_fp32 <span class="token operator">=</span> config<span class="token punctuation">.</span>attention_softmax_in_fp32
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>apply_query_key_layer_scaling<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>attention_softmax_in_fp32 <span class="token operator">=</span> <span class="token boolean">True</span>
        self<span class="token punctuation">.</span>layer_number <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> layer_number<span class="token punctuation">)</span>
        <span class="token comment">#默认kv的维度kv_channels=128，多头数量num_attention_heads=32</span>
        projection_size <span class="token operator">=</span> config<span class="token punctuation">.</span>kv_channels <span class="token operator">*</span> config<span class="token punctuation">.</span>num_attention_heads

        <span class="token comment"># Per attention head and per partition values.</span>
        <span class="token comment">#进入前维度</span>
        self<span class="token punctuation">.</span>hidden_size_per_partition <span class="token operator">=</span> projection_size
        每个attention head的维度
        self<span class="token punctuation">.</span>hidden_size_per_attention_head <span class="token operator">=</span> projection_size <span class="token operator">//</span> config<span class="token punctuation">.</span>num_attention_heads
        <span class="token comment">#多头数量num_attention_heads=32</span>
        self<span class="token punctuation">.</span>num_attention_heads_per_partition <span class="token operator">=</span> config<span class="token punctuation">.</span>num_attention_heads
        <span class="token comment">#下面计算了注意力机制公式里K*V除以的根号dk</span>
        coeff <span class="token operator">=</span> <span class="token boolean">None</span>
        self<span class="token punctuation">.</span>norm_factor <span class="token operator">=</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size_per_attention_head<span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>apply_query_key_layer_scaling<span class="token punctuation">:</span>
            coeff <span class="token operator">=</span> self<span class="token punctuation">.</span>layer_number
            self<span class="token punctuation">.</span>norm_factor <span class="token operator">*=</span> coeff
        self<span class="token punctuation">.</span>coeff <span class="token operator">=</span> coeff
        <span class="token comment">#默认attention_dropout=0</span>
        self<span class="token punctuation">.</span>attention_dropout <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>config<span class="token punctuation">.</span>attention_dropout<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> query_layer<span class="token punctuation">,</span> key_layer<span class="token punctuation">,</span> value_layer<span class="token punctuation">,</span> attention_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
        pytorch_major_version <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>__version__<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'.'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> pytorch_major_version <span class="token operator">&gt;=</span> <span class="token number">2</span><span class="token punctuation">:</span>
            <span class="token comment">#for k in [query_layer, key_layer, value_layer]: 这个循环会遍历query_layer, key_layer, 和value_layer这三个张量。</span>
            <span class="token comment">#k.permute(1, 2, 0, 3): permute是一个PyTorch的方法，用于改变张量的轴的顺序。假设原张量的维度顺序是(0, 1, 2, 3)（假设张量有四个维度），permute(1, 2, 0, 3)将会把这个顺序改变为(1, 2, 0, 3)。</span>
            <span class="token comment">#具体来说，原来在位置0的维度（通常是batch_size）现在移动到了位置2，位置1和2的维度向前移动了一个位置，而位置3的维度保持不变。</span>
            query_layer<span class="token punctuation">,</span> key_layer<span class="token punctuation">,</span> value_layer <span class="token operator">=</span> <span class="token punctuation">[</span>k<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span> <span class="token keyword">for</span> k <span class="token keyword">in</span> <span class="token punctuation">[</span>query_layer<span class="token punctuation">,</span> key_layer<span class="token punctuation">,</span> value_layer<span class="token punctuation">]</span><span class="token punctuation">]</span>
            <span class="token keyword">if</span> attention_mask <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">and</span> query_layer<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">==</span> key_layer<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
                context_layer <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>scaled_dot_product_attention<span class="token punctuation">(</span>query_layer<span class="token punctuation">,</span> key_layer<span class="token punctuation">,</span> value_layer<span class="token punctuation">,</span>
                                                                                 is_causal<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                <span class="token keyword">if</span> attention_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                    <span class="token comment">#将attention_mask中0，1呼唤</span>
                    attention_mask <span class="token operator">=</span> <span class="token operator">~</span>attention_mask
                <span class="token comment">#实现softmax（QK^T/sqrt(dk))*V</span>
                context_layer <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>scaled_dot_product_attention<span class="token punctuation">(</span>query_layer<span class="token punctuation">,</span> key_layer<span class="token punctuation">,</span> value_layer<span class="token punctuation">,</span>
                                                                                 attention_mask<span class="token punctuation">)</span>
            <span class="token comment">#换维度</span>
            context_layer <span class="token operator">=</span> context_layer<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>

            new_context_layer_shape <span class="token operator">=</span> context_layer<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size_per_partition<span class="token punctuation">,</span><span class="token punctuation">)</span>
            <span class="token comment">#重新转化维度</span>
            context_layer <span class="token operator">=</span> context_layer<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">*</span>new_context_layer_shape<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment"># Raw attention scores</span>

            <span class="token comment"># [b, np, sq, sk]</span>
            output_size <span class="token operator">=</span> <span class="token punctuation">(</span>query_layer<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> query_layer<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> query_layer<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> key_layer<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

            <span class="token comment"># [sq, b, np, hn] -&gt; [sq, b * np, hn]</span>
            query_layer <span class="token operator">=</span> query_layer<span class="token punctuation">.</span>view<span class="token punctuation">(</span>output_size<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> output_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> output_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token comment"># [sk, b, np, hn] -&gt; [sk, b * np, hn]</span>
            key_layer <span class="token operator">=</span> key_layer<span class="token punctuation">.</span>view<span class="token punctuation">(</span>output_size<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> output_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> output_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

            <span class="token comment"># preallocting input tensor: [b * np, sq, sk]</span>
            matmul_input_buffer <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>
                output_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> output_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> output_size<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> output_size<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>query_layer<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span>
                device<span class="token operator">=</span>query_layer<span class="token punctuation">.</span>device
            <span class="token punctuation">)</span>

            <span class="token comment"># Raw attention scores. [b * np, sq, sk]</span>
            matmul_result <span class="token operator">=</span> torch<span class="token punctuation">.</span>baddbmm<span class="token punctuation">(</span>
                matmul_input_buffer<span class="token punctuation">,</span>
                query_layer<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># [b * np, sq, hn]</span>
                key_layer<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># [b * np, hn, sk]</span>
                beta<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span>
                alpha<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">/</span> self<span class="token punctuation">.</span>norm_factor<span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token punctuation">)</span>

            <span class="token comment"># change view to [b, np, sq, sk]</span>
            attention_scores <span class="token operator">=</span> matmul_result<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">*</span>output_size<span class="token punctuation">)</span>

            <span class="token comment"># ===========================</span>
            <span class="token comment"># Attention probs and dropout</span>
            <span class="token comment"># ===========================</span>

            <span class="token comment"># attention scores and attention mask [b, np, sq, sk]</span>
            <span class="token keyword">if</span> self<span class="token punctuation">.</span>attention_softmax_in_fp32<span class="token punctuation">:</span>
                attention_scores <span class="token operator">=</span> attention_scores<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> self<span class="token punctuation">.</span>coeff <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                attention_scores <span class="token operator">=</span> attention_scores <span class="token operator">*</span> self<span class="token punctuation">.</span>coeff
            <span class="token keyword">if</span> attention_mask <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">and</span> attention_scores<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">==</span> attention_scores<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
                attention_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>output_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> output_size<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> output_size<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                                            device<span class="token operator">=</span>attention_scores<span class="token punctuation">.</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">bool</span><span class="token punctuation">)</span>
                attention_mask<span class="token punctuation">.</span>tril_<span class="token punctuation">(</span><span class="token punctuation">)</span>
                attention_mask <span class="token operator">=</span> <span class="token operator">~</span>attention_mask
            <span class="token keyword">if</span> attention_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                attention_scores <span class="token operator">=</span> attention_scores<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>attention_mask<span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token string">"-inf"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            attention_probs <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attention_scores<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            attention_probs <span class="token operator">=</span> attention_probs<span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>value_layer<span class="token punctuation">)</span>

            <span class="token comment"># This is actually dropping out entire tokens to attend to, which might</span>
            <span class="token comment"># seem a bit unusual, but is taken from the original Transformer paper.</span>
            attention_probs <span class="token operator">=</span> self<span class="token punctuation">.</span>attention_dropout<span class="token punctuation">(</span>attention_probs<span class="token punctuation">)</span>
            <span class="token comment"># =========================</span>
            <span class="token comment"># Context layer. [sq, b, hp]</span>
            <span class="token comment"># =========================</span>

            <span class="token comment"># value_layer -&gt; context layer.</span>
            <span class="token comment"># [sk, b, np, hn] --&gt; [b, np, sq, hn]</span>

            <span class="token comment"># context layer shape: [b, np, sq, hn]</span>
            output_size <span class="token operator">=</span> <span class="token punctuation">(</span>value_layer<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> value_layer<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> query_layer<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> value_layer<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token comment"># change view [sk, b * np, hn]</span>
            value_layer <span class="token operator">=</span> value_layer<span class="token punctuation">.</span>view<span class="token punctuation">(</span>value_layer<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> output_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> output_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token comment"># change view [b * np, sq, sk]</span>
            attention_probs <span class="token operator">=</span> attention_probs<span class="token punctuation">.</span>view<span class="token punctuation">(</span>output_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> output_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> output_size<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token comment"># matmul: [b * np, sq, hn]</span>
            context_layer <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>attention_probs<span class="token punctuation">,</span> value_layer<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token comment"># change view [b, np, sq, hn]</span>
            context_layer <span class="token operator">=</span> context_layer<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">*</span>output_size<span class="token punctuation">)</span>
            <span class="token comment"># [b, np, sq, hn] --&gt; [sq, b, np, hn]</span>
            context_layer <span class="token operator">=</span> context_layer<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token comment"># [sq, b, np, hn] --&gt; [sq, b, hp]</span>
            new_context_layer_shape <span class="token operator">=</span> context_layer<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size_per_partition<span class="token punctuation">,</span><span class="token punctuation">)</span>
            context_layer <span class="token operator">=</span> context_layer<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">*</span>new_context_layer_shape<span class="token punctuation">)</span>

        <span class="token keyword">return</span> context_layer


<span class="token keyword">class</span> <span class="token class-name">SelfAttention</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Parallel self-attention layer abstract class.

    Self-attention layer takes input with size [s, b, h]
    and returns output of the same size.
    """</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">:</span> ChatGLMConfig<span class="token punctuation">,</span> layer_number<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>SelfAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layer_number <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> layer_number<span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>projection_size <span class="token operator">=</span> config<span class="token punctuation">.</span>kv_channels <span class="token operator">*</span> config<span class="token punctuation">.</span>num_attention_heads

        <span class="token comment"># Per attention head and per partition values.</span>
        self<span class="token punctuation">.</span>hidden_size_per_attention_head <span class="token operator">=</span> self<span class="token punctuation">.</span>projection_size <span class="token operator">//</span> config<span class="token punctuation">.</span>num_attention_heads
        self<span class="token punctuation">.</span>num_attention_heads_per_partition <span class="token operator">=</span> config<span class="token punctuation">.</span>num_attention_heads

        self<span class="token punctuation">.</span>multi_query_attention <span class="token operator">=</span> config<span class="token punctuation">.</span>multi_query_attention
        self<span class="token punctuation">.</span>qkv_hidden_size <span class="token operator">=</span> <span class="token number">3</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>projection_size
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>multi_query_attention<span class="token punctuation">:</span>
            <span class="token comment">#默认multi_query_group_num=1</span>
            self<span class="token punctuation">.</span>num_multi_query_groups_per_partition <span class="token operator">=</span> config<span class="token punctuation">.</span>multi_query_group_num
            self<span class="token punctuation">.</span>qkv_hidden_size <span class="token operator">=</span> <span class="token punctuation">(</span>
                    self<span class="token punctuation">.</span>projection_size <span class="token operator">+</span> <span class="token number">2</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>hidden_size_per_attention_head <span class="token operator">*</span> config<span class="token punctuation">.</span>multi_query_group_num
            <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>query_key_value <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>qkv_hidden_size<span class="token punctuation">,</span>
                                         bias<span class="token operator">=</span>config<span class="token punctuation">.</span>add_bias_linear <span class="token keyword">or</span> config<span class="token punctuation">.</span>add_qkv_bias<span class="token punctuation">,</span>
                                         device<span class="token operator">=</span>device<span class="token punctuation">,</span> <span class="token operator">**</span>_config_to_kwargs<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
                                         <span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>core_attention <span class="token operator">=</span> CoreAttention<span class="token punctuation">(</span>config<span class="token punctuation">,</span> self<span class="token punctuation">.</span>layer_number<span class="token punctuation">)</span>

        <span class="token comment"># Output.</span>
        self<span class="token punctuation">.</span>dense <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>projection_size<span class="token punctuation">,</span> config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> bias<span class="token operator">=</span>config<span class="token punctuation">.</span>add_bias_linear<span class="token punctuation">,</span>
                               device<span class="token operator">=</span>device<span class="token punctuation">,</span> <span class="token operator">**</span>_config_to_kwargs<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
                               <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">_allocate_memory</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inference_max_sequence_len<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>multi_query_attention<span class="token punctuation">:</span>
            num_attention_heads <span class="token operator">=</span> self<span class="token punctuation">.</span>num_multi_query_groups_per_partition
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            num_attention_heads <span class="token operator">=</span> self<span class="token punctuation">.</span>num_attention_heads_per_partition
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>
            inference_max_sequence_len<span class="token punctuation">,</span>
            batch_size<span class="token punctuation">,</span>
            num_attention_heads<span class="token punctuation">,</span>
            self<span class="token punctuation">.</span>hidden_size_per_attention_head<span class="token punctuation">,</span>
            dtype<span class="token operator">=</span>dtype<span class="token punctuation">,</span>
            device<span class="token operator">=</span>device<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
            self<span class="token punctuation">,</span> hidden_states<span class="token punctuation">,</span> attention_mask<span class="token punctuation">,</span> rotary_pos_emb<span class="token punctuation">,</span> kv_cache<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> use_cache<span class="token operator">=</span><span class="token boolean">True</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># hidden_states: [sq, b, h]</span>

        <span class="token comment"># =================================================</span>
        <span class="token comment"># Pre-allocate memory for key-values for inference.</span>
        <span class="token comment"># =================================================</span>
        <span class="token comment"># =====================</span>
        <span class="token comment"># Query, Key, and Value</span>
        <span class="token comment"># =====================</span>

        <span class="token comment"># Attention heads [sq, b, h] --&gt; [sq, b, (np * 3 * hn)]</span>
        mixed_x_layer <span class="token operator">=</span> self<span class="token punctuation">.</span>query_key_value<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>multi_query_attention<span class="token punctuation">:</span>
            <span class="token punctuation">(</span>query_layer<span class="token punctuation">,</span> key_layer<span class="token punctuation">,</span> value_layer<span class="token punctuation">)</span> <span class="token operator">=</span> mixed_x_layer<span class="token punctuation">.</span>split<span class="token punctuation">(</span>
                <span class="token punctuation">[</span>
                    self<span class="token punctuation">.</span>num_attention_heads_per_partition <span class="token operator">*</span> self<span class="token punctuation">.</span>hidden_size_per_attention_head<span class="token punctuation">,</span>
                    self<span class="token punctuation">.</span>num_multi_query_groups_per_partition <span class="token operator">*</span> self<span class="token punctuation">.</span>hidden_size_per_attention_head<span class="token punctuation">,</span>
                    self<span class="token punctuation">.</span>num_multi_query_groups_per_partition <span class="token operator">*</span> self<span class="token punctuation">.</span>hidden_size_per_attention_head<span class="token punctuation">,</span>
                <span class="token punctuation">]</span><span class="token punctuation">,</span>
                dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>
            <span class="token punctuation">)</span>
            query_layer <span class="token operator">=</span> query_layer<span class="token punctuation">.</span>view<span class="token punctuation">(</span>
                query_layer<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_attention_heads_per_partition<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size_per_attention_head<span class="token punctuation">)</span>
            <span class="token punctuation">)</span>
            key_layer <span class="token operator">=</span> key_layer<span class="token punctuation">.</span>view<span class="token punctuation">(</span>
                key_layer<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_multi_query_groups_per_partition<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size_per_attention_head<span class="token punctuation">)</span>
            <span class="token punctuation">)</span>
            value_layer <span class="token operator">=</span> value_layer<span class="token punctuation">.</span>view<span class="token punctuation">(</span>
                value_layer<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
                <span class="token operator">+</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_multi_query_groups_per_partition<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size_per_attention_head<span class="token punctuation">)</span>
            <span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            new_tensor_shape <span class="token operator">=</span> mixed_x_layer<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> \
                               <span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_attention_heads_per_partition<span class="token punctuation">,</span>
                                <span class="token number">3</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>hidden_size_per_attention_head<span class="token punctuation">)</span>
            mixed_x_layer <span class="token operator">=</span> mixed_x_layer<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">*</span>new_tensor_shape<span class="token punctuation">)</span>

            <span class="token comment"># [sq, b, np, 3 * hn] --&gt; 3 [sq, b, np, hn]</span>
            <span class="token punctuation">(</span>query_layer<span class="token punctuation">,</span> key_layer<span class="token punctuation">,</span> value_layer<span class="token punctuation">)</span> <span class="token operator">=</span> split_tensor_along_last_dim<span class="token punctuation">(</span>mixed_x_layer<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>

        <span class="token comment"># apply relative positional encoding (rotary embedding)</span>
        <span class="token keyword">if</span> rotary_pos_emb <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            query_layer <span class="token operator">=</span> apply_rotary_pos_emb<span class="token punctuation">(</span>query_layer<span class="token punctuation">,</span> rotary_pos_emb<span class="token punctuation">)</span>
            key_layer <span class="token operator">=</span> apply_rotary_pos_emb<span class="token punctuation">(</span>key_layer<span class="token punctuation">,</span> rotary_pos_emb<span class="token punctuation">)</span>

        <span class="token comment"># adjust key and value for inference</span>
        <span class="token keyword">if</span> kv_cache <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            cache_k<span class="token punctuation">,</span> cache_v <span class="token operator">=</span> kv_cache
            key_layer <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>cache_k<span class="token punctuation">,</span> key_layer<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
            value_layer <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>cache_v<span class="token punctuation">,</span> value_layer<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> use_cache<span class="token punctuation">:</span>
            kv_cache <span class="token operator">=</span> <span class="token punctuation">(</span>key_layer<span class="token punctuation">,</span> value_layer<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            kv_cache <span class="token operator">=</span> <span class="token boolean">None</span>

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>multi_query_attention<span class="token punctuation">:</span>
            key_layer <span class="token operator">=</span> key_layer<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span>
            key_layer <span class="token operator">=</span> key_layer<span class="token punctuation">.</span>expand<span class="token punctuation">(</span>
                <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_attention_heads_per_partition <span class="token operator">//</span> self<span class="token punctuation">.</span>num_multi_query_groups_per_partition<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span>
            <span class="token punctuation">)</span>
            key_layer <span class="token operator">=</span> key_layer<span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>
                key_layer<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_attention_heads_per_partition<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size_per_attention_head<span class="token punctuation">)</span>
            <span class="token punctuation">)</span>
            value_layer <span class="token operator">=</span> value_layer<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span>
            value_layer <span class="token operator">=</span> value_layer<span class="token punctuation">.</span>expand<span class="token punctuation">(</span>
                <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_attention_heads_per_partition <span class="token operator">//</span> self<span class="token punctuation">.</span>num_multi_query_groups_per_partition<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span>
            <span class="token punctuation">)</span>
            value_layer <span class="token operator">=</span> value_layer<span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>
                value_layer<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_attention_heads_per_partition<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size_per_attention_head<span class="token punctuation">)</span>
            <span class="token punctuation">)</span>

        <span class="token comment"># ==================================</span>
        <span class="token comment"># core attention computation</span>
        <span class="token comment"># ==================================</span>

        context_layer <span class="token operator">=</span> self<span class="token punctuation">.</span>core_attention<span class="token punctuation">(</span>query_layer<span class="token punctuation">,</span> key_layer<span class="token punctuation">,</span> value_layer<span class="token punctuation">,</span> attention_mask<span class="token punctuation">)</span>

        <span class="token comment"># =================</span>
        <span class="token comment"># Output. [sq, b, h]</span>
        <span class="token comment"># =================</span>

        output <span class="token operator">=</span> self<span class="token punctuation">.</span>dense<span class="token punctuation">(</span>context_layer<span class="token punctuation">)</span>

        <span class="token keyword">return</span> output<span class="token punctuation">,</span> kv_cache


<span class="token keyword">def</span> <span class="token function">_config_to_kwargs</span><span class="token punctuation">(</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
    common_kwargs <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string">"dtype"</span><span class="token punctuation">:</span> args<span class="token punctuation">.</span>torch_dtype<span class="token punctuation">,</span>
    <span class="token punctuation">}</span>
    <span class="token keyword">return</span> common_kwargs

<span class="token comment">#atttention后的MLP层</span>
<span class="token keyword">class</span> <span class="token class-name">MLP</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""MLP.

    MLP will take the input with h hidden state, project it to 4*h
    hidden dimension, perform nonlinear transformation, and project the
    state back into h hidden dimension.
    """</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">:</span> ChatGLMConfig<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>MLP<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>add_bias <span class="token operator">=</span> config<span class="token punctuation">.</span>add_bias_linear

        <span class="token comment"># Project to 4h. If using swiglu double the output width, see https://arxiv.org/pdf/2002.05202.pdf</span>
        self<span class="token punctuation">.</span>dense_h_to_4h <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>
            config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span>
            config<span class="token punctuation">.</span>ffn_hidden_size <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span>
            bias<span class="token operator">=</span>self<span class="token punctuation">.</span>add_bias<span class="token punctuation">,</span>
            device<span class="token operator">=</span>device<span class="token punctuation">,</span>
            <span class="token operator">**</span>_config_to_kwargs<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
        <span class="token punctuation">)</span>

        <span class="token keyword">def</span> <span class="token function">swiglu</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> torch<span class="token punctuation">.</span>chunk<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token keyword">return</span> F<span class="token punctuation">.</span>silu<span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>

        self<span class="token punctuation">.</span>activation_func <span class="token operator">=</span> swiglu

        <span class="token comment"># Project back to h.</span>
        self<span class="token punctuation">.</span>dense_4h_to_h <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>
            config<span class="token punctuation">.</span>ffn_hidden_size<span class="token punctuation">,</span>
            config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span>
            bias<span class="token operator">=</span>self<span class="token punctuation">.</span>add_bias<span class="token punctuation">,</span>
            device<span class="token operator">=</span>device<span class="token punctuation">,</span>
            <span class="token operator">**</span>_config_to_kwargs<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
        <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_states<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># [s, b, 4hp]</span>
        intermediate_parallel <span class="token operator">=</span> self<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>
        intermediate_parallel <span class="token operator">=</span> self<span class="token punctuation">.</span>activation_func<span class="token punctuation">(</span>intermediate_parallel<span class="token punctuation">)</span>
        <span class="token comment"># [s, b, h]</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">(</span>intermediate_parallel<span class="token punctuation">)</span>
        <span class="token keyword">return</span> output

<span class="token comment">#28层指的就是28个GLMBlock，每个里面包含(RMSNorm,SelfAttention(Linear,CoreAttention,Linear),RMSNorm,MLP),最后一层的最后再加个RMSNorm</span>
<span class="token keyword">class</span> <span class="token class-name">GLMBlock</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""A single transformer layer.

    Transformer layer takes input with size [s, b, h] and returns an
    output of the same size.
    """</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">:</span> ChatGLMConfig<span class="token punctuation">,</span> layer_number<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>GLMBlock<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layer_number <span class="token operator">=</span> layer_number

        self<span class="token punctuation">.</span>apply_residual_connection_post_layernorm <span class="token operator">=</span> config<span class="token punctuation">.</span>apply_residual_connection_post_layernorm

        self<span class="token punctuation">.</span>fp32_residual_connection <span class="token operator">=</span> config<span class="token punctuation">.</span>fp32_residual_connection

        LayerNormFunc <span class="token operator">=</span> RMSNorm <span class="token keyword">if</span> config<span class="token punctuation">.</span>rmsnorm <span class="token keyword">else</span> LayerNorm
        <span class="token comment"># Layernorm on the input data.</span>
        self<span class="token punctuation">.</span>input_layernorm <span class="token operator">=</span> LayerNormFunc<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> eps<span class="token operator">=</span>config<span class="token punctuation">.</span>layernorm_epsilon<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span>
                                             dtype<span class="token operator">=</span>config<span class="token punctuation">.</span>torch_dtype<span class="token punctuation">)</span>

        <span class="token comment"># Self attention.</span>
        self<span class="token punctuation">.</span>self_attention <span class="token operator">=</span> SelfAttention<span class="token punctuation">(</span>config<span class="token punctuation">,</span> layer_number<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>hidden_dropout <span class="token operator">=</span> config<span class="token punctuation">.</span>hidden_dropout

        <span class="token comment"># Layernorm on the attention output</span>
        self<span class="token punctuation">.</span>post_attention_layernorm <span class="token operator">=</span> LayerNormFunc<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> eps<span class="token operator">=</span>config<span class="token punctuation">.</span>layernorm_epsilon<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span>
                                                      dtype<span class="token operator">=</span>config<span class="token punctuation">.</span>torch_dtype<span class="token punctuation">)</span>

        <span class="token comment"># MLP</span>
        self<span class="token punctuation">.</span>mlp <span class="token operator">=</span> MLP<span class="token punctuation">(</span>config<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
            self<span class="token punctuation">,</span> hidden_states<span class="token punctuation">,</span> attention_mask<span class="token punctuation">,</span> rotary_pos_emb<span class="token punctuation">,</span> kv_cache<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> use_cache<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># hidden_states: [s, b, h]</span>

        <span class="token comment"># Layer norm at the beginning of the transformer layer.</span>
        layernorm_output <span class="token operator">=</span> self<span class="token punctuation">.</span>input_layernorm<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>
        <span class="token comment"># Self attention.</span>
        attention_output<span class="token punctuation">,</span> kv_cache <span class="token operator">=</span> self<span class="token punctuation">.</span>self_attention<span class="token punctuation">(</span>
            layernorm_output<span class="token punctuation">,</span>
            attention_mask<span class="token punctuation">,</span>
            rotary_pos_emb<span class="token punctuation">,</span>
            kv_cache<span class="token operator">=</span>kv_cache<span class="token punctuation">,</span>
            use_cache<span class="token operator">=</span>use_cache
        <span class="token punctuation">)</span>

        <span class="token comment"># Residual connection.</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>apply_residual_connection_post_layernorm<span class="token punctuation">:</span>
            residual <span class="token operator">=</span> layernorm_output
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            residual <span class="token operator">=</span> hidden_states

        layernorm_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>attention_output<span class="token punctuation">,</span> p<span class="token operator">=</span>self<span class="token punctuation">.</span>hidden_dropout<span class="token punctuation">,</span> training<span class="token operator">=</span>self<span class="token punctuation">.</span>training<span class="token punctuation">)</span>
        layernorm_input <span class="token operator">=</span> residual <span class="token operator">+</span> layernorm_input

        <span class="token comment"># Layer norm post the self attention.</span>
        layernorm_output <span class="token operator">=</span> self<span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">(</span>layernorm_input<span class="token punctuation">)</span>

        <span class="token comment"># MLP.</span>
        mlp_output <span class="token operator">=</span> self<span class="token punctuation">.</span>mlp<span class="token punctuation">(</span>layernorm_output<span class="token punctuation">)</span>

        <span class="token comment"># Second residual connection.</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>apply_residual_connection_post_layernorm<span class="token punctuation">:</span>
            residual <span class="token operator">=</span> layernorm_output
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            residual <span class="token operator">=</span> layernorm_input

        output <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>mlp_output<span class="token punctuation">,</span> p<span class="token operator">=</span>self<span class="token punctuation">.</span>hidden_dropout<span class="token punctuation">,</span> training<span class="token operator">=</span>self<span class="token punctuation">.</span>training<span class="token punctuation">)</span>
        output <span class="token operator">=</span> residual <span class="token operator">+</span> output

        <span class="token keyword">return</span> output<span class="token punctuation">,</span> kv_cache

<span class="token comment">#28个GLMBlock块组成的Transformer网络</span>
<span class="token keyword">class</span> <span class="token class-name">GLMTransformer</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Transformer class."""</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">:</span> ChatGLMConfig<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>GLMTransformer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment">#默认false</span>
        self<span class="token punctuation">.</span>fp32_residual_connection <span class="token operator">=</span> config<span class="token punctuation">.</span>fp32_residual_connection
        <span class="token comment">#默认True</span>
        self<span class="token punctuation">.</span>post_layer_norm <span class="token operator">=</span> config<span class="token punctuation">.</span>post_layer_norm

        <span class="token comment"># Number of layers.默认28</span>
        self<span class="token punctuation">.</span>num_layers <span class="token operator">=</span> config<span class="token punctuation">.</span>num_layers

        <span class="token comment"># Transformer layers.</span>
        <span class="token keyword">def</span> <span class="token function">build_layer</span><span class="token punctuation">(</span>layer_number<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> GLMBlock<span class="token punctuation">(</span>config<span class="token punctuation">,</span> layer_number<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>build_layer<span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>post_layer_norm<span class="token punctuation">:</span>
            <span class="token comment">#默认rmsnorm为True</span>
            LayerNormFunc <span class="token operator">=</span> RMSNorm <span class="token keyword">if</span> config<span class="token punctuation">.</span>rmsnorm <span class="token keyword">else</span> LayerNorm
            <span class="token comment"># Final layer norm before output.</span>
            self<span class="token punctuation">.</span>final_layernorm <span class="token operator">=</span> LayerNormFunc<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> eps<span class="token operator">=</span>config<span class="token punctuation">.</span>layernorm_epsilon<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span>
                                                 dtype<span class="token operator">=</span>config<span class="token punctuation">.</span>torch_dtype<span class="token punctuation">)</span>
        <span class="token comment">#减少GPU内存消耗</span>
        self<span class="token punctuation">.</span>gradient_checkpointing <span class="token operator">=</span> <span class="token boolean">False</span>

    <span class="token keyword">def</span> <span class="token function">_get_layer</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> layer_number<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">[</span>layer_number<span class="token punctuation">]</span>

    <span class="token comment">#Optional 是Python的typing模块提供的一个类型提示，用于表示某个参数可以是特定的类型或None。</span>
    <span class="token comment">#例如，Optional[int] 表示该值可以是int类型或None</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
            self<span class="token punctuation">,</span> hidden_states<span class="token punctuation">,</span> attention_mask<span class="token punctuation">,</span> rotary_pos_emb<span class="token punctuation">,</span> kv_caches<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
            use_cache<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>
            output_hidden_states<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> kv_caches<span class="token punctuation">:</span>
            kv_caches <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token boolean">None</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">]</span>
        presents <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> use_cache <span class="token keyword">else</span> <span class="token boolean">None</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>gradient_checkpointing <span class="token keyword">and</span> self<span class="token punctuation">.</span>training<span class="token punctuation">:</span>
            <span class="token keyword">if</span> use_cache<span class="token punctuation">:</span>
                logger<span class="token punctuation">.</span>warning_once<span class="token punctuation">(</span>
                    <span class="token string">"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."</span>
                <span class="token punctuation">)</span>
                use_cache <span class="token operator">=</span> <span class="token boolean">False</span>

        all_self_attentions <span class="token operator">=</span> <span class="token boolean">None</span>
        all_hidden_states <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> output_hidden_states <span class="token keyword">else</span> <span class="token boolean">None</span>
        <span class="token keyword">for</span> index <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment">#默认false</span>
            <span class="token keyword">if</span> output_hidden_states<span class="token punctuation">:</span>
                all_hidden_states <span class="token operator">=</span> all_hidden_states <span class="token operator">+</span> <span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span><span class="token punctuation">)</span>

            layer <span class="token operator">=</span> self<span class="token punctuation">.</span>_get_layer<span class="token punctuation">(</span>index<span class="token punctuation">)</span>
            <span class="token keyword">if</span> self<span class="token punctuation">.</span>gradient_checkpointing <span class="token keyword">and</span> self<span class="token punctuation">.</span>training<span class="token punctuation">:</span>
                layer_ret <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>checkpoint<span class="token punctuation">.</span>checkpoint<span class="token punctuation">(</span>
                    layer<span class="token punctuation">,</span>
                    hidden_states<span class="token punctuation">,</span>
                    attention_mask<span class="token punctuation">,</span>
                    rotary_pos_emb<span class="token punctuation">,</span>
                    kv_caches<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span>
                    use_cache
                <span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                layer_ret <span class="token operator">=</span> layer<span class="token punctuation">(</span>
                    hidden_states<span class="token punctuation">,</span>
                    attention_mask<span class="token punctuation">,</span>
                    rotary_pos_emb<span class="token punctuation">,</span>
                    kv_cache<span class="token operator">=</span>kv_caches<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span>
                    use_cache<span class="token operator">=</span>use_cache
                <span class="token punctuation">)</span>
            hidden_states<span class="token punctuation">,</span> kv_cache <span class="token operator">=</span> layer_ret
            <span class="token keyword">if</span> use_cache<span class="token punctuation">:</span>
                presents <span class="token operator">=</span> presents <span class="token operator">+</span> <span class="token punctuation">(</span>kv_cache<span class="token punctuation">,</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> output_hidden_states<span class="token punctuation">:</span>
            all_hidden_states <span class="token operator">=</span> all_hidden_states <span class="token operator">+</span> <span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span><span class="token punctuation">)</span>

        <span class="token comment"># Final layer norm. 默认True</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>post_layer_norm<span class="token punctuation">:</span>
            hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>final_layernorm<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>

        <span class="token keyword">return</span> hidden_states<span class="token punctuation">,</span> presents<span class="token punctuation">,</span> all_hidden_states<span class="token punctuation">,</span> all_self_attentions

<span class="token comment">#用于预训练的模型类</span>
<span class="token keyword">class</span> <span class="token class-name">ChatGLMPreTrainedModel</span><span class="token punctuation">(</span>PreTrainedModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    An abstract class to handle weights initialization and
    a simple interface for downloading and loading pretrained models.
    """</span>

    is_parallelizable <span class="token operator">=</span> <span class="token boolean">False</span>
    supports_gradient_checkpointing <span class="token operator">=</span> <span class="token boolean">True</span>
    config_class <span class="token operator">=</span> ChatGLMConfig
    base_model_prefix <span class="token operator">=</span> <span class="token string">"transformer"</span>
    _no_split_modules <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"GLMBlock"</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">_init_weights</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> module<span class="token punctuation">:</span> nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Initialize the weights."""</span>
        <span class="token keyword">return</span>

    <span class="token keyword">def</span> <span class="token function">get_masks</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_ids<span class="token punctuation">,</span> past_key_values<span class="token punctuation">,</span> padding_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">#默认seq_length=2048</span>
        batch_size<span class="token punctuation">,</span> seq_length <span class="token operator">=</span> input_ids<span class="token punctuation">.</span>shape
        full_attention_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_length<span class="token punctuation">,</span> seq_length<span class="token punctuation">,</span> device<span class="token operator">=</span>input_ids<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        full_attention_mask<span class="token punctuation">.</span>tril_<span class="token punctuation">(</span><span class="token punctuation">)</span>
        past_length <span class="token operator">=</span> <span class="token number">0</span>
        <span class="token keyword">if</span> past_key_values<span class="token punctuation">:</span>
            past_length <span class="token operator">=</span> past_key_values<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        <span class="token keyword">if</span> past_length<span class="token punctuation">:</span>
            full_attention_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_length<span class="token punctuation">,</span> past_length<span class="token punctuation">,</span>
                                                        device<span class="token operator">=</span>input_ids<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> full_attention_mask<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> padding_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            full_attention_mask <span class="token operator">=</span> full_attention_mask <span class="token operator">*</span> padding_mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> past_length <span class="token keyword">and</span> padding_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            full_attention_mask <span class="token operator">-=</span> padding_mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span>
        full_attention_mask <span class="token operator">=</span> <span class="token punctuation">(</span>full_attention_mask <span class="token operator">&lt;</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">bool</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        full_attention_mask<span class="token punctuation">.</span>unsqueeze_<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> full_attention_mask

    <span class="token keyword">def</span> <span class="token function">get_position_ids</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_ids<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>
        batch_size<span class="token punctuation">,</span> seq_length <span class="token operator">=</span> input_ids<span class="token punctuation">.</span>shape
        position_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>seq_length<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> position_ids

    <span class="token keyword">def</span> <span class="token function">_set_gradient_checkpointing</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> module<span class="token punctuation">,</span> value<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>module<span class="token punctuation">,</span> GLMTransformer<span class="token punctuation">)</span><span class="token punctuation">:</span>
            module<span class="token punctuation">.</span>gradient_checkpointing <span class="token operator">=</span> value

<span class="token comment">#Embedding层</span>
<span class="token keyword">class</span> <span class="token class-name">Embedding</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Language model embeddings."""</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">:</span> ChatGLMConfig<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Embedding<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> config<span class="token punctuation">.</span>hidden_size
        <span class="token comment"># Word embeddings (parallel).</span>
        self<span class="token punctuation">.</span>word_embeddings <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>
            <span class="token comment">#padded_vocab_size默认65024</span>
            config<span class="token punctuation">.</span>padded_vocab_size<span class="token punctuation">,</span>
            self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span>
            dtype<span class="token operator">=</span>config<span class="token punctuation">.</span>torch_dtype<span class="token punctuation">,</span>
            device<span class="token operator">=</span>device
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fp32_residual_connection <span class="token operator">=</span> config<span class="token punctuation">.</span>fp32_residual_connection

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_ids<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Embeddings.</span>
        words_embeddings <span class="token operator">=</span> self<span class="token punctuation">.</span>word_embeddings<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>
        embeddings <span class="token operator">=</span> words_embeddings
        <span class="token comment"># Data format change to avoid explicit tranposes : [b s h] --&gt; [s b h].</span>
        embeddings <span class="token operator">=</span> embeddings<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># If the input flag for fp32 residual connection is set, convert for float.</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>fp32_residual_connection<span class="token punctuation">:</span>
            embeddings <span class="token operator">=</span> embeddings<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> embeddings

<span class="token comment">#继承预训练模型类</span>
<span class="token keyword">class</span> <span class="token class-name">ChatGLMModel</span><span class="token punctuation">(</span>ChatGLMPreTrainedModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">:</span> ChatGLMConfig<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> empty_init<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
        <span class="token comment">#是否先不初始化参数empty_init</span>
        <span class="token keyword">if</span> empty_init<span class="token punctuation">:</span>
            <span class="token comment">#skip_init是2.0版本以上的torch.nn.utils下一个库，在不初始化参数 / 缓冲区的情况下实例化模块</span>
            init_method <span class="token operator">=</span> skip_init
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            init_method <span class="token operator">=</span> default_init
        init_kwargs <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>
        <span class="token keyword">if</span> device <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            init_kwargs<span class="token punctuation">[</span><span class="token string">"device"</span><span class="token punctuation">]</span> <span class="token operator">=</span> device
        <span class="token comment">#定义输入的embedding层</span>
        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> init_method<span class="token punctuation">(</span>Embedding<span class="token punctuation">,</span> config<span class="token punctuation">,</span> <span class="token operator">**</span>init_kwargs<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>num_layers <span class="token operator">=</span> config<span class="token punctuation">.</span>num_layers
        self<span class="token punctuation">.</span>multi_query_group_num <span class="token operator">=</span> config<span class="token punctuation">.</span>multi_query_group_num
        self<span class="token punctuation">.</span>kv_channels <span class="token operator">=</span> config<span class="token punctuation">.</span>kv_channels

        <span class="token comment">#定义输入的Rotary位置编码embedding层</span>
        <span class="token comment"># Rotary positional embeddings</span>
        self<span class="token punctuation">.</span>seq_length <span class="token operator">=</span> config<span class="token punctuation">.</span>seq_length
        rotary_dim <span class="token operator">=</span> <span class="token punctuation">(</span>
            config<span class="token punctuation">.</span>hidden_size <span class="token operator">//</span> config<span class="token punctuation">.</span>num_attention_heads <span class="token keyword">if</span> config<span class="token punctuation">.</span>kv_channels <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">else</span> config<span class="token punctuation">.</span>kv_channels
        <span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>rotary_pos_emb <span class="token operator">=</span> RotaryEmbedding<span class="token punctuation">(</span>rotary_dim <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">,</span> original_impl<span class="token operator">=</span>config<span class="token punctuation">.</span>original_rope<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span>
                                              dtype<span class="token operator">=</span>config<span class="token punctuation">.</span>torch_dtype<span class="token punctuation">)</span>
        <span class="token comment">#定义主体模块GLMTransformer层</span>
        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> init_method<span class="token punctuation">(</span>GLMTransformer<span class="token punctuation">,</span> config<span class="token punctuation">,</span> <span class="token operator">**</span>init_kwargs<span class="token punctuation">)</span>
        <span class="token comment">#定义输出层output_layer</span>
        self<span class="token punctuation">.</span>output_layer <span class="token operator">=</span> init_method<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">,</span> config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> config<span class="token punctuation">.</span>padded_vocab_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
                                        dtype<span class="token operator">=</span>config<span class="token punctuation">.</span>torch_dtype<span class="token punctuation">,</span> <span class="token operator">**</span>init_kwargs<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>pre_seq_len <span class="token operator">=</span> config<span class="token punctuation">.</span>pre_seq_len
        <span class="token comment">#默认false</span>
        self<span class="token punctuation">.</span>prefix_projection <span class="token operator">=</span> config<span class="token punctuation">.</span>prefix_projection
        <span class="token comment">#如果前缀长度不为空，说明需要经过prefix_encoder层，则定义prefix_encoder</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>pre_seq_len <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token comment">#微调prefix_encoder不更新主体网络参数</span>
            <span class="token keyword">for</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                param<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">False</span>
            <span class="token comment">#使用torch.arange函数生成一个从0开始，到self.pre_seq_len - 1结束的整数序列的tensor</span>
            <span class="token comment">#.long(): 这个方法用于将上述生成的tensor转换为长整型（int64）</span>
            <span class="token comment">#即生成一个从0、1、2到self.pre_seq_len - 1的序列的long类型tensor</span>
            self<span class="token punctuation">.</span>prefix_tokens <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>self<span class="token punctuation">.</span>pre_seq_len<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>prefix_encoder <span class="token operator">=</span> PrefixEncoder<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">get_input_embeddings</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">#self.embedding.word_embeddings是一个定义好的nn.Embedding()层</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>word_embeddings

    <span class="token keyword">def</span> <span class="token function">get_prompt</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>half<span class="token punctuation">)</span><span class="token punctuation">:</span>

        prefix_tokens <span class="token operator">=</span> self<span class="token punctuation">.</span>prefix_tokens<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>

        past_key_values <span class="token operator">=</span> self<span class="token punctuation">.</span>prefix_encoder<span class="token punctuation">(</span>prefix_tokens<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>dtype<span class="token punctuation">)</span>

        past_key_values <span class="token operator">=</span> past_key_values<span class="token punctuation">.</span>view<span class="token punctuation">(</span>
            batch_size<span class="token punctuation">,</span>
            self<span class="token punctuation">.</span>pre_seq_len<span class="token punctuation">,</span>
            self<span class="token punctuation">.</span>num_layers <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span>
            self<span class="token punctuation">.</span>multi_query_group_num<span class="token punctuation">,</span>
            self<span class="token punctuation">.</span>kv_channels
        <span class="token punctuation">)</span>
        <span class="token comment"># seq_len, b, nh, hidden_size</span>
        past_key_values <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>past_key_values<span class="token punctuation">)</span>
        past_key_values <span class="token operator">=</span> past_key_values<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> past_key_values

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
            self<span class="token punctuation">,</span>
            input_ids<span class="token punctuation">,</span>
            position_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
            attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>BoolTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
            full_attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>BoolTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
            past_key_values<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
            inputs_embeds<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
            use_cache<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
            output_hidden_states<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
            return_dict<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        output_hidden_states <span class="token operator">=</span> <span class="token punctuation">(</span>
            output_hidden_states <span class="token keyword">if</span> output_hidden_states <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>output_hidden_states
        <span class="token punctuation">)</span>
        use_cache <span class="token operator">=</span> use_cache <span class="token keyword">if</span> use_cache <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>use_cache
        return_dict <span class="token operator">=</span> return_dict <span class="token keyword">if</span> return_dict <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>use_return_dict
        <span class="token comment">#获取批次大小batch_size和输入长度seq_length</span>
        batch_size<span class="token punctuation">,</span> seq_length <span class="token operator">=</span> input_ids<span class="token punctuation">.</span>shape

        <span class="token comment">#inputs_embeds为self.embedding层</span>
        <span class="token keyword">if</span> inputs_embeds <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            inputs_embeds <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>
        <span class="token comment">#有前缀长度，则使用经过处理后的past_key_values</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>pre_seq_len <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> past_key_values <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                <span class="token comment">#获得past_key_values</span>
                past_key_values <span class="token operator">=</span> self<span class="token punctuation">.</span>get_prompt<span class="token punctuation">(</span>batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span> device<span class="token operator">=</span>input_ids<span class="token punctuation">.</span>device<span class="token punctuation">,</span>
                                                  dtype<span class="token operator">=</span>inputs_embeds<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
            <span class="token keyword">if</span> attention_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                attention_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>attention_mask<span class="token punctuation">.</span>new_ones<span class="token punctuation">(</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>pre_seq_len<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                            attention_mask<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> full_attention_mask <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> <span class="token punctuation">(</span>attention_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">and</span> <span class="token keyword">not</span> attention_mask<span class="token punctuation">.</span><span class="token builtin">all</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">or</span> <span class="token punctuation">(</span>past_key_values <span class="token keyword">and</span> seq_length <span class="token operator">!=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                full_attention_mask <span class="token operator">=</span> self<span class="token punctuation">.</span>get_masks<span class="token punctuation">(</span>input_ids<span class="token punctuation">,</span> past_key_values<span class="token punctuation">,</span> padding_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">)</span>

        <span class="token comment"># Rotary positional embeddings</span>
        rotary_pos_emb <span class="token operator">=</span> self<span class="token punctuation">.</span>rotary_pos_emb<span class="token punctuation">(</span>self<span class="token punctuation">.</span>seq_length<span class="token punctuation">)</span>
        <span class="token keyword">if</span> position_ids <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            rotary_pos_emb <span class="token operator">=</span> rotary_pos_emb<span class="token punctuation">[</span>position_ids<span class="token punctuation">]</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            rotary_pos_emb <span class="token operator">=</span> rotary_pos_emb<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>seq_length<span class="token punctuation">]</span>
        rotary_pos_emb <span class="token operator">=</span> rotary_pos_emb<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment"># Run encoder.</span>
        <span class="token comment">#主体模块GLMTransformer层</span>
        hidden_states<span class="token punctuation">,</span> presents<span class="token punctuation">,</span> all_hidden_states<span class="token punctuation">,</span> all_self_attentions <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>
            inputs_embeds<span class="token punctuation">,</span> full_attention_mask<span class="token punctuation">,</span> rotary_pos_emb<span class="token operator">=</span>rotary_pos_emb<span class="token punctuation">,</span>
            kv_caches<span class="token operator">=</span>past_key_values<span class="token punctuation">,</span> use_cache<span class="token operator">=</span>use_cache<span class="token punctuation">,</span> output_hidden_states<span class="token operator">=</span>output_hidden_states
        <span class="token punctuation">)</span>
        
        <span class="token comment">#</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> return_dict<span class="token punctuation">:</span>
            <span class="token keyword">return</span> <span class="token builtin">tuple</span><span class="token punctuation">(</span>v <span class="token keyword">for</span> v <span class="token keyword">in</span> <span class="token punctuation">[</span>hidden_states<span class="token punctuation">,</span> presents<span class="token punctuation">,</span> all_hidden_states<span class="token punctuation">,</span> all_self_attentions<span class="token punctuation">]</span> <span class="token keyword">if</span> v <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> BaseModelOutputWithPast<span class="token punctuation">(</span>
            last_hidden_state<span class="token operator">=</span>hidden_states<span class="token punctuation">,</span>
            past_key_values<span class="token operator">=</span>presents<span class="token punctuation">,</span>
            hidden_states<span class="token operator">=</span>all_hidden_states<span class="token punctuation">,</span>
            attentions<span class="token operator">=</span>all_self_attentions<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
    <span class="token comment">#用于量化方法</span>
    <span class="token keyword">def</span> <span class="token function">quantize</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> weight_bit_width<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">from</span> <span class="token punctuation">.</span>quantization <span class="token keyword">import</span> quantize
        quantize<span class="token punctuation">(</span>self<span class="token punctuation">.</span>encoder<span class="token punctuation">,</span> weight_bit_width<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self

<span class="token comment">#用于条件生成chatglm2的类</span>
<span class="token keyword">class</span> <span class="token class-name">ChatGLMForConditionalGeneration</span><span class="token punctuation">(</span>ChatGLMPreTrainedModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">:</span> ChatGLMConfig<span class="token punctuation">,</span> empty_init<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>config<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>max_sequence_length <span class="token operator">=</span> config<span class="token punctuation">.</span>max_length
        self<span class="token punctuation">.</span>transformer <span class="token operator">=</span> ChatGLMModel<span class="token punctuation">(</span>config<span class="token punctuation">,</span> empty_init<span class="token operator">=</span>empty_init<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>config <span class="token operator">=</span> config
        self<span class="token punctuation">.</span>quantized <span class="token operator">=</span> <span class="token boolean">False</span>

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>quantization_bit<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>quantize<span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>quantization_bit<span class="token punctuation">,</span> empty_init<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">_update_model_kwargs_for_generation</span><span class="token punctuation">(</span>
            self<span class="token punctuation">,</span>
            outputs<span class="token punctuation">:</span> ModelOutput<span class="token punctuation">,</span>
            model_kwargs<span class="token punctuation">:</span> Dict<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> Any<span class="token punctuation">]</span><span class="token punctuation">,</span>
            is_encoder_decoder<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
            standardize_cache_format<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Dict<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> Any<span class="token punctuation">]</span><span class="token punctuation">:</span>
        <span class="token comment"># update past_key_values</span>
        model_kwargs<span class="token punctuation">[</span><span class="token string">"past_key_values"</span><span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>_extract_past_from_model_output<span class="token punctuation">(</span>
            outputs<span class="token punctuation">,</span> standardize_cache_format<span class="token operator">=</span>standardize_cache_format
        <span class="token punctuation">)</span>

        <span class="token comment"># update attention mask</span>
        <span class="token keyword">if</span> <span class="token string">"attention_mask"</span> <span class="token keyword">in</span> model_kwargs<span class="token punctuation">:</span>
            attention_mask <span class="token operator">=</span> model_kwargs<span class="token punctuation">[</span><span class="token string">"attention_mask"</span><span class="token punctuation">]</span>
            model_kwargs<span class="token punctuation">[</span><span class="token string">"attention_mask"</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>
                <span class="token punctuation">[</span>attention_mask<span class="token punctuation">,</span> attention_mask<span class="token punctuation">.</span>new_ones<span class="token punctuation">(</span><span class="token punctuation">(</span>attention_mask<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>
            <span class="token punctuation">)</span>

        <span class="token comment"># update position ids</span>
        <span class="token keyword">if</span> <span class="token string">"position_ids"</span> <span class="token keyword">in</span> model_kwargs<span class="token punctuation">:</span>
            position_ids <span class="token operator">=</span> model_kwargs<span class="token punctuation">[</span><span class="token string">"position_ids"</span><span class="token punctuation">]</span>
            new_position_id <span class="token operator">=</span> position_ids<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>
            new_position_id <span class="token operator">+=</span> <span class="token number">1</span>
            model_kwargs<span class="token punctuation">[</span><span class="token string">"position_ids"</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>
                <span class="token punctuation">[</span>position_ids<span class="token punctuation">,</span> new_position_id<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>
            <span class="token punctuation">)</span>

        model_kwargs<span class="token punctuation">[</span><span class="token string">"is_first_forward"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">False</span>
        <span class="token keyword">return</span> model_kwargs

    <span class="token keyword">def</span> <span class="token function">prepare_inputs_for_generation</span><span class="token punctuation">(</span>
            self<span class="token punctuation">,</span>
            input_ids<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">,</span>
            past_key_values<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
            attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
            position_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
            is_first_forward<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>
            <span class="token operator">**</span>kwargs
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">dict</span><span class="token punctuation">:</span>
        <span class="token comment"># only last token for input_ids if past is not None</span>
        <span class="token keyword">if</span> position_ids <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            position_ids <span class="token operator">=</span> self<span class="token punctuation">.</span>get_position_ids<span class="token punctuation">(</span>input_ids<span class="token punctuation">,</span> device<span class="token operator">=</span>input_ids<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> is_first_forward<span class="token punctuation">:</span>
            position_ids <span class="token operator">=</span> position_ids<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
            input_ids <span class="token operator">=</span> input_ids<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
        <span class="token keyword">return</span> <span class="token punctuation">{<!-- --></span>
            <span class="token string">"input_ids"</span><span class="token punctuation">:</span> input_ids<span class="token punctuation">,</span>
            <span class="token string">"past_key_values"</span><span class="token punctuation">:</span> past_key_values<span class="token punctuation">,</span>
            <span class="token string">"position_ids"</span><span class="token punctuation">:</span> position_ids<span class="token punctuation">,</span>
            <span class="token string">"attention_mask"</span><span class="token punctuation">:</span> attention_mask<span class="token punctuation">,</span>
            <span class="token string">"return_last_logit"</span><span class="token punctuation">:</span> <span class="token boolean">True</span>
        <span class="token punctuation">}</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
            self<span class="token punctuation">,</span>
            input_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
            position_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
            attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
            past_key_values<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
            inputs_embeds<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
            labels<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
            use_cache<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
            output_attentions<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
            output_hidden_states<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
            return_dict<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
            return_last_logit<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        use_cache <span class="token operator">=</span> use_cache <span class="token keyword">if</span> use_cache <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>use_cache
        return_dict <span class="token operator">=</span> return_dict <span class="token keyword">if</span> return_dict <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>use_return_dict
        
        <span class="token triple-quoted-string string">'''
            self.transformer(...)获得chatGLMModel模型forward输出BaseModelOutputWithPast,通常包含：
            last_hidden_state: 最后一层的隐藏状态。
            past_key_values (或者叫 past): 用于注意力机制的key和value对。
            "Past"在这里指的是在Transformer模型中用于注意力机制的key和value对。
            hidden_states 是模型所有层的隐藏状态输出的列表
            attentions 是模型所有层的注意力权重的列表。这些权重显示了每个输入token对其他tokens的注意力分布
            在一些应用中，例如文本生成，保存这些“过去”的值是很有用的，因为这样可以避免重新计算整个输入序列，从而实现效率更高的逐个词的解码。
            BaseModelOutputWithPast(
                last_hidden_state=hidden_states,
                past_key_values=presents,
                hidden_states=all_hidden_states,
                attentions=all_self_attentions,
            )
        '''</span>
        transformer_outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>transformer<span class="token punctuation">(</span>
            input_ids<span class="token operator">=</span>input_ids<span class="token punctuation">,</span>
            position_ids<span class="token operator">=</span>position_ids<span class="token punctuation">,</span>
            attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">,</span>
            past_key_values<span class="token operator">=</span>past_key_values<span class="token punctuation">,</span>
            inputs_embeds<span class="token operator">=</span>inputs_embeds<span class="token punctuation">,</span>
            use_cache<span class="token operator">=</span>use_cache<span class="token punctuation">,</span>
            output_hidden_states<span class="token operator">=</span>output_hidden_states<span class="token punctuation">,</span>
            return_dict<span class="token operator">=</span>return_dict<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        <span class="token comment">#提出输出的隐藏层状态</span>
        hidden_states <span class="token operator">=</span> transformer_outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        <span class="token keyword">if</span> return_last_logit<span class="token punctuation">:</span>
            hidden_states <span class="token operator">=</span> hidden_states<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
        <span class="token comment">#获得每个词的概率</span>
        lm_logits <span class="token operator">=</span> self<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>output_layer<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>
        <span class="token comment">#transpose(0, 1)交换维度0和维度1</span>
        lm_logits <span class="token operator">=</span> lm_logits<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>


        loss <span class="token operator">=</span> <span class="token boolean">None</span>
        <span class="token comment">#训练时有label,下面会计算loss值</span>
        <span class="token keyword">if</span> labels <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            lm_logits <span class="token operator">=</span> lm_logits<span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>

            <span class="token comment"># Shift so that tokens &lt; n predict n</span>
            shift_logits <span class="token operator">=</span> lm_logits<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>
            shift_labels <span class="token operator">=</span> labels<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token comment"># Flatten the tokens</span>
            loss_fct <span class="token operator">=</span> CrossEntropyLoss<span class="token punctuation">(</span>ignore_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">100</span><span class="token punctuation">)</span>
            <span class="token comment">#view(-1, shift_logits.size(-1))会自动计算维度，假设shift_logits为(10, 20, 50)，那么shift_logits的总元素数量是 10 * 20 * 50 = 10000</span>
            <span class="token comment">#则shift_logits.view(-1, shift_logits.size(-1))会得到(10,1000)的一个tensor</span>
            loss <span class="token operator">=</span> loss_fct<span class="token punctuation">(</span>shift_logits<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> shift_logits<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> shift_labels<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

            lm_logits <span class="token operator">=</span> lm_logits<span class="token punctuation">.</span>to<span class="token punctuation">(</span>hidden_states<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
            loss <span class="token operator">=</span> loss<span class="token punctuation">.</span>to<span class="token punctuation">(</span>hidden_states<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>

        <span class="token keyword">if</span> <span class="token keyword">not</span> return_dict<span class="token punctuation">:</span>
            output <span class="token operator">=</span> <span class="token punctuation">(</span>lm_logits<span class="token punctuation">,</span><span class="token punctuation">)</span> <span class="token operator">+</span> transformer_outputs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
            <span class="token keyword">return</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>loss<span class="token punctuation">,</span><span class="token punctuation">)</span> <span class="token operator">+</span> output<span class="token punctuation">)</span> <span class="token keyword">if</span> loss <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> output
        <span class="token triple-quoted-string string">'''
            CausalLMOutputWithPast输出包含的内容通常是：
            loss: 如果提供了标签，则计算并返回损失值。
            logits: 对应每个token的预测分数。
            past_key_values: 与上面描述的类似，这是用于注意力机制的key和value对。
            hidden_states: 可选的，模型的所有隐藏层的输出。
            attentions: 可选的，注意力权重。
        '''</span>
        <span class="token keyword">return</span> CausalLMOutputWithPast<span class="token punctuation">(</span>
            loss<span class="token operator">=</span>loss<span class="token punctuation">,</span>
            logits<span class="token operator">=</span>lm_logits<span class="token punctuation">,</span>
            past_key_values<span class="token operator">=</span>transformer_outputs<span class="token punctuation">.</span>past_key_values<span class="token punctuation">,</span>
            hidden_states<span class="token operator">=</span>transformer_outputs<span class="token punctuation">.</span>hidden_states<span class="token punctuation">,</span>
            attentions<span class="token operator">=</span>transformer_outputs<span class="token punctuation">.</span>attentions<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
    <span class="token comment">#staticmethod用于修饰类中的方法,使其可以在不创建类实例的情况下调用方法</span>
    <span class="token decorator annotation punctuation">@staticmethod</span>
    <span class="token keyword">def</span> <span class="token function">_reorder_cache</span><span class="token punctuation">(</span>
            past<span class="token punctuation">:</span> Tuple<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span> beam_idx<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>LongTensor
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Tuple<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        This function is used to re-order the `past_key_values` cache if [`~PreTrainedModel.beam_search`] or
        [`~PreTrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct
        beam_idx at every generation step.

        Output shares the same memory storage as `past`.
        """</span>
        <span class="token keyword">return</span> <span class="token builtin">tuple</span><span class="token punctuation">(</span>
            <span class="token punctuation">(</span>
                layer_past<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>index_select<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> beam_idx<span class="token punctuation">.</span>to<span class="token punctuation">(</span>layer_past<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                layer_past<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>index_select<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> beam_idx<span class="token punctuation">.</span>to<span class="token punctuation">(</span>layer_past<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token punctuation">)</span>
            <span class="token keyword">for</span> layer_past <span class="token keyword">in</span> past
        <span class="token punctuation">)</span>

    <span class="token comment">#处理response字符串</span>
    <span class="token keyword">def</span> <span class="token function">process_response</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        response <span class="token operator">=</span> response<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>
        response <span class="token operator">=</span> response<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">"[[训练时间]]"</span><span class="token punctuation">,</span> <span class="token string">"2023年"</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> response


    <span class="token keyword">def</span> <span class="token function">build_inputs</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tokenizer<span class="token punctuation">,</span> query<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span> history<span class="token punctuation">:</span> List<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> <span class="token builtin">str</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">#将历史和当前的对话内容转化成prompt</span>
        prompt <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>build_prompt<span class="token punctuation">(</span>query<span class="token punctuation">,</span> history<span class="token operator">=</span>history<span class="token punctuation">)</span>
        <span class="token comment">#返回PyTorch tensor</span>
        inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span><span class="token punctuation">[</span>prompt<span class="token punctuation">]</span><span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span>
        inputs <span class="token operator">=</span> inputs<span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        <span class="token keyword">return</span> inputs

    <span class="token keyword">def</span> <span class="token function">build_stream_inputs</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tokenizer<span class="token punctuation">,</span> query<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span> history<span class="token punctuation">:</span> List<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> <span class="token builtin">str</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> history<span class="token punctuation">:</span>
            prompt <span class="token operator">=</span> <span class="token string">"\n\n[Round {}]\n\n问：{}\n\n答："</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>history<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> query<span class="token punctuation">)</span>
            input_ids <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>prompt<span class="token punctuation">,</span> add_special_tokens<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
            input_ids <span class="token operator">=</span> input_ids<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
            inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>batch_encode_plus<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span>input_ids<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">,</span> add_special_tokens<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            prompt <span class="token operator">=</span> <span class="token string">"[Round {}]\n\n问：{}\n\n答："</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>history<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> query<span class="token punctuation">)</span>
            inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span><span class="token punctuation">[</span>prompt<span class="token punctuation">]</span><span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span>
        inputs <span class="token operator">=</span> inputs<span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        <span class="token keyword">return</span> inputs

    <span class="token comment">#@torch.inference_mode() 是PyTorch的一个上下文管理器，当使用它作为装饰器时，它确保包装的函数内的所有代码都在推理模式下运行</span>
    <span class="token triple-quoted-string string">'''
        使用模型时的示例代码，适合api
        response, history = model.chat(tokenizer,
                                prompt,
                                history=history,
                                max_length=max_length if max_length else 2048,
                                top_p=top_p if top_p else 0.7,
                                temperature=temperature if temperature else 0.95)
    '''</span>
    <span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>inference_mode</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">chat</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tokenizer<span class="token punctuation">,</span> query<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span> history<span class="token punctuation">:</span> List<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> <span class="token builtin">str</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span> max_length<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">8192</span><span class="token punctuation">,</span> num_beams<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
             do_sample<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> top_p<span class="token operator">=</span><span class="token number">0.8</span><span class="token punctuation">,</span> temperature<span class="token operator">=</span><span class="token number">0.8</span><span class="token punctuation">,</span> logits_processor<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> history <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            history <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">if</span> logits_processor <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token triple-quoted-string string">'''
            当生成文本时，模型为每个可能的token输出一个logit（即原始未归一化的预测值）。
            LogitsProcessorList是一种工具list，它包含了一系列的处理器，这些处理器可以修改这些logits。
            通过修改logits，可以影响模型的输出。
            例如，可以使用一个LogitsProcessor来实现温度调整、最小/最大长度限制、特定token的惩罚/奖励等。
            例如，可以对模型计算出的logits进行进一步处理，例如对“复读机现象”相应的概率进行惩罚，以避免模型生成结果不断重复。
            '''</span>
            logits_processor <span class="token operator">=</span> LogitsProcessorList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment">#添加一个处理，用于处理无效的概率输出，即输出预测字符的概率可能会很低，需要处理一下</span>
        logits_processor<span class="token punctuation">.</span>append<span class="token punctuation">(</span>InvalidScoreLogitsProcessor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment">#num_beams是beam search的参数，这里默认为1，top_p是预设概率阈值，概率小于topp的得分置为0</span>
        gen_kwargs <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token string">"max_length"</span><span class="token punctuation">:</span> max_length<span class="token punctuation">,</span> <span class="token string">"num_beams"</span><span class="token punctuation">:</span> num_beams<span class="token punctuation">,</span> <span class="token string">"do_sample"</span><span class="token punctuation">:</span> do_sample<span class="token punctuation">,</span> <span class="token string">"top_p"</span><span class="token punctuation">:</span> top_p<span class="token punctuation">,</span>
                      <span class="token string">"temperature"</span><span class="token punctuation">:</span> temperature<span class="token punctuation">,</span> <span class="token string">"logits_processor"</span><span class="token punctuation">:</span> logits_processor<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">}</span>
        inputs <span class="token operator">=</span> self<span class="token punctuation">.</span>build_inputs<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> query<span class="token punctuation">,</span> history<span class="token operator">=</span>history<span class="token punctuation">)</span>

        <span class="token comment">#生成id输出,self.generate是PreTrainedModel的方法，可以控制max_length，temperature，top_p等</span>
        <span class="token comment">#控制temperature、top_k、top_p等参数的原理在beam search中，可以参考博客https://blog.csdn.net/weixin_44826203/article/details/130708623</span>
        <span class="token comment">#beam search会对score整体除以temperature做缩放控制模型的状态，只取top_k的概率对应的词汇，其余的概率置为-inf</span>
        outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>generate<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">,</span> <span class="token operator">**</span>gen_kwargs<span class="token punctuation">)</span>
        outputs <span class="token operator">=</span> outputs<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>inputs<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
        <span class="token comment">#解码成字符</span>
        response <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>outputs<span class="token punctuation">)</span>
        response <span class="token operator">=</span> self<span class="token punctuation">.</span>process_response<span class="token punctuation">(</span>response<span class="token punctuation">)</span>
        history <span class="token operator">=</span> history <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token punctuation">(</span>query<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">]</span>
        <span class="token comment">#返回当前轮回答和历史记录</span>
        <span class="token keyword">return</span> response<span class="token punctuation">,</span> history

    <span class="token comment">#实现流式的一次一次对话，通过记录past_key_values，实现高效问答</span>
    <span class="token triple-quoted-string string">'''
        使用模型时的示例代码，适合web
        for response, history, past_key_values in model.stream_chat(tokenizer, input, history, past_key_values=past_key_values,
                                                        return_past_key_values=True,
                                                        max_length=max_length, top_p=top_p,
                                                        temperature=temperature):
        chatbot[-1] = (parse_text(input), parse_text(response))

        yield chatbot, history, past_key_values
    '''</span>
    <span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>inference_mode</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">stream_chat</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tokenizer<span class="token punctuation">,</span> query<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span> history<span class="token punctuation">:</span> List<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> <span class="token builtin">str</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span> past_key_values<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                    max_length<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">8192</span><span class="token punctuation">,</span> do_sample<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> top_p<span class="token operator">=</span><span class="token number">0.8</span><span class="token punctuation">,</span> temperature<span class="token operator">=</span><span class="token number">0.8</span><span class="token punctuation">,</span> logits_processor<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                    return_past_key_values<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> history <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            history <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">if</span> logits_processor <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            logits_processor <span class="token operator">=</span> LogitsProcessorList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        logits_processor<span class="token punctuation">.</span>append<span class="token punctuation">(</span>InvalidScoreLogitsProcessor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        gen_kwargs <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token string">"max_length"</span><span class="token punctuation">:</span> max_length<span class="token punctuation">,</span> <span class="token string">"do_sample"</span><span class="token punctuation">:</span> do_sample<span class="token punctuation">,</span> <span class="token string">"top_p"</span><span class="token punctuation">:</span> top_p<span class="token punctuation">,</span>
                      <span class="token string">"temperature"</span><span class="token punctuation">:</span> temperature<span class="token punctuation">,</span> <span class="token string">"logits_processor"</span><span class="token punctuation">:</span> logits_processor<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">}</span>
        <span class="token keyword">if</span> past_key_values <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">and</span> <span class="token keyword">not</span> return_past_key_values<span class="token punctuation">:</span>
            inputs <span class="token operator">=</span> self<span class="token punctuation">.</span>build_inputs<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> query<span class="token punctuation">,</span> history<span class="token operator">=</span>history<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment">#上面定义了build_stream_inputs，是一个问答的形式构建的inputs</span>
            inputs <span class="token operator">=</span> self<span class="token punctuation">.</span>build_stream_inputs<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> query<span class="token punctuation">,</span> history<span class="token operator">=</span>history<span class="token punctuation">)</span>
        
        <span class="token keyword">if</span> past_key_values <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            past_length <span class="token operator">=</span> past_key_values<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
            <span class="token keyword">if</span> self<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>pre_seq_len <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                past_length <span class="token operator">-=</span> self<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>pre_seq_len
            <span class="token comment">#加上之前的长度</span>
            inputs<span class="token punctuation">.</span>position_ids <span class="token operator">+=</span> past_length
            <span class="token comment">#获得新attention_mask</span>
            attention_mask <span class="token operator">=</span> inputs<span class="token punctuation">.</span>attention_mask
            attention_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>attention_mask<span class="token punctuation">.</span>new_ones<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> past_length<span class="token punctuation">)</span><span class="token punctuation">,</span> attention_mask<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
            inputs<span class="token punctuation">[</span><span class="token string">'attention_mask'</span><span class="token punctuation">]</span> <span class="token operator">=</span> attention_mask

        <span class="token keyword">for</span> outputs <span class="token keyword">in</span> self<span class="token punctuation">.</span>stream_generate<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">,</span> past_key_values<span class="token operator">=</span>past_key_values<span class="token punctuation">,</span>
                                            return_past_key_values<span class="token operator">=</span>return_past_key_values<span class="token punctuation">,</span> <span class="token operator">**</span>gen_kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> return_past_key_values<span class="token punctuation">:</span>
                outputs<span class="token punctuation">,</span> past_key_values <span class="token operator">=</span> outputs
            outputs <span class="token operator">=</span> outputs<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>inputs<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
            response <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>outputs<span class="token punctuation">)</span>
            <span class="token keyword">if</span> response <span class="token keyword">and</span> response<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">!=</span> <span class="token string">"�"</span><span class="token punctuation">:</span>
                response <span class="token operator">=</span> self<span class="token punctuation">.</span>process_response<span class="token punctuation">(</span>response<span class="token punctuation">)</span>
                new_history <span class="token operator">=</span> history <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token punctuation">(</span>query<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">]</span>
                <span class="token keyword">if</span> return_past_key_values<span class="token punctuation">:</span>
                    <span class="token comment">#yield: 当一个函数包含yield关键字，它将不再是一个常规函数，而是一个生成器函数。这种函数在调用时不会执行，而是返回一个生成器对象。</span>
                    <span class="token keyword">yield</span> response<span class="token punctuation">,</span> new_history<span class="token punctuation">,</span> past_key_values
                <span class="token keyword">else</span><span class="token punctuation">:</span>
                    <span class="token keyword">yield</span> response<span class="token punctuation">,</span> new_history

    <span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>inference_mode</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">stream_generate</span><span class="token punctuation">(</span>
            self<span class="token punctuation">,</span>
            input_ids<span class="token punctuation">,</span>
            generation_config<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>GenerationConfig<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
            logits_processor<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>LogitsProcessorList<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
            stopping_criteria<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>StoppingCriteriaList<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
            prefix_allowed_tokens_fn<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Callable<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span> List<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
            return_past_key_values<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
            <span class="token operator">**</span>kwargs<span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        batch_size<span class="token punctuation">,</span> input_ids_seq_length <span class="token operator">=</span> input_ids<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> input_ids<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>

        <span class="token keyword">if</span> generation_config <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            generation_config <span class="token operator">=</span> self<span class="token punctuation">.</span>generation_config
        generation_config <span class="token operator">=</span> copy<span class="token punctuation">.</span>deepcopy<span class="token punctuation">(</span>generation_config<span class="token punctuation">)</span>
        model_kwargs <span class="token operator">=</span> generation_config<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        bos_token_id<span class="token punctuation">,</span> eos_token_id <span class="token operator">=</span> generation_config<span class="token punctuation">.</span>bos_token_id<span class="token punctuation">,</span> generation_config<span class="token punctuation">.</span>eos_token_id

        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>eos_token_id<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            eos_token_id <span class="token operator">=</span> <span class="token punctuation">[</span>eos_token_id<span class="token punctuation">]</span>

        has_default_max_length <span class="token operator">=</span> kwargs<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"max_length"</span><span class="token punctuation">)</span> <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">and</span> generation_config<span class="token punctuation">.</span>max_length <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span>
        <span class="token keyword">if</span> has_default_max_length <span class="token keyword">and</span> generation_config<span class="token punctuation">.</span>max_new_tokens <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            warnings<span class="token punctuation">.</span>warn<span class="token punctuation">(</span>
                <span class="token string-interpolation"><span class="token string">f"Using `max_length`'s default (</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>generation_config<span class="token punctuation">.</span>max_length<span class="token punctuation">}</span></span><span class="token string">) to control the generation length. "</span></span>
                <span class="token string">"This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we"</span>
                <span class="token string">" recommend using `max_new_tokens` to control the maximum length of the generation."</span><span class="token punctuation">,</span>
                UserWarning<span class="token punctuation">,</span>
            <span class="token punctuation">)</span>
        <span class="token keyword">elif</span> generation_config<span class="token punctuation">.</span>max_new_tokens <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            generation_config<span class="token punctuation">.</span>max_length <span class="token operator">=</span> generation_config<span class="token punctuation">.</span>max_new_tokens <span class="token operator">+</span> input_ids_seq_length
            <span class="token keyword">if</span> <span class="token keyword">not</span> has_default_max_length<span class="token punctuation">:</span>
                logger<span class="token punctuation">.</span>warn<span class="token punctuation">(</span>
                    <span class="token string-interpolation"><span class="token string">f"Both `max_new_tokens` (=</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>generation_config<span class="token punctuation">.</span>max_new_tokens<span class="token punctuation">}</span></span><span class="token string">) and `max_length`(="</span></span>
                    <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>generation_config<span class="token punctuation">.</span>max_length<span class="token punctuation">}</span></span><span class="token string">) seem to have been set. `max_new_tokens` will take precedence. "</span></span>
                    <span class="token string">"Please refer to the documentation for more information. "</span>
                    <span class="token string">"(https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)"</span><span class="token punctuation">,</span>
                    UserWarning<span class="token punctuation">,</span>
                <span class="token punctuation">)</span>

        <span class="token keyword">if</span> input_ids_seq_length <span class="token operator">&gt;=</span> generation_config<span class="token punctuation">.</span>max_length<span class="token punctuation">:</span>
            input_ids_string <span class="token operator">=</span> <span class="token string">"decoder_input_ids"</span> <span class="token keyword">if</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>is_encoder_decoder <span class="token keyword">else</span> <span class="token string">"input_ids"</span>
            logger<span class="token punctuation">.</span>warning<span class="token punctuation">(</span>
                <span class="token string-interpolation"><span class="token string">f"Input length of </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>input_ids_string<span class="token punctuation">}</span></span><span class="token string"> is </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>input_ids_seq_length<span class="token punctuation">}</span></span><span class="token string">, but `max_length` is set to"</span></span>
                <span class="token string-interpolation"><span class="token string">f" </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>generation_config<span class="token punctuation">.</span>max_length<span class="token punctuation">}</span></span><span class="token string">. This can lead to unexpected behavior. You should consider"</span></span>
                <span class="token string">" increasing `max_new_tokens`."</span>
            <span class="token punctuation">)</span>

        <span class="token comment"># 2. Set generation parameters if not already defined</span>
        logits_processor <span class="token operator">=</span> logits_processor <span class="token keyword">if</span> logits_processor <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> LogitsProcessorList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment">#对生成过程做停止控制的工具，例如达到一定长度时强行停止，达到一定生成时间时停止等</span>
        stopping_criteria <span class="token operator">=</span> stopping_criteria <span class="token keyword">if</span> stopping_criteria <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> StoppingCriteriaList<span class="token punctuation">(</span><span class="token punctuation">)</span>

        logits_processor <span class="token operator">=</span> self<span class="token punctuation">.</span>_get_logits_processor<span class="token punctuation">(</span>
            generation_config<span class="token operator">=</span>generation_config<span class="token punctuation">,</span>
            input_ids_seq_length<span class="token operator">=</span>input_ids_seq_length<span class="token punctuation">,</span>
            encoder_input_ids<span class="token operator">=</span>input_ids<span class="token punctuation">,</span>
            prefix_allowed_tokens_fn<span class="token operator">=</span>prefix_allowed_tokens_fn<span class="token punctuation">,</span>
            logits_processor<span class="token operator">=</span>logits_processor<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>

        stopping_criteria <span class="token operator">=</span> self<span class="token punctuation">.</span>_get_stopping_criteria<span class="token punctuation">(</span>
            generation_config<span class="token operator">=</span>generation_config<span class="token punctuation">,</span> stopping_criteria<span class="token operator">=</span>stopping_criteria
        <span class="token punctuation">)</span>
        logits_warper <span class="token operator">=</span> self<span class="token punctuation">.</span>_get_logits_warper<span class="token punctuation">(</span>generation_config<span class="token punctuation">)</span>

        unfinished_sequences <span class="token operator">=</span> input_ids<span class="token punctuation">.</span>new<span class="token punctuation">(</span>input_ids<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>fill_<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        scores <span class="token operator">=</span> <span class="token boolean">None</span>
        <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
            model_inputs <span class="token operator">=</span> self<span class="token punctuation">.</span>prepare_inputs_for_generation<span class="token punctuation">(</span>input_ids<span class="token punctuation">,</span> <span class="token operator">**</span>model_kwargs<span class="token punctuation">)</span>
            <span class="token comment"># forward pass to get next token</span>
            outputs <span class="token operator">=</span> self<span class="token punctuation">(</span>
                <span class="token operator">**</span>model_inputs<span class="token punctuation">,</span>
                return_dict<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                output_attentions<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
                output_hidden_states<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
            <span class="token punctuation">)</span>

            next_token_logits <span class="token operator">=</span> outputs<span class="token punctuation">.</span>logits<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>

            <span class="token comment"># pre-process distribution</span>
            next_token_scores <span class="token operator">=</span> logits_processor<span class="token punctuation">(</span>input_ids<span class="token punctuation">,</span> next_token_logits<span class="token punctuation">)</span>
            next_token_scores <span class="token operator">=</span> logits_warper<span class="token punctuation">(</span>input_ids<span class="token punctuation">,</span> next_token_scores<span class="token punctuation">)</span>

            <span class="token comment"># sample</span>
            probs <span class="token operator">=</span> nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>next_token_scores<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> generation_config<span class="token punctuation">.</span>do_sample<span class="token punctuation">:</span>
                next_tokens <span class="token operator">=</span> torch<span class="token punctuation">.</span>multinomial<span class="token punctuation">(</span>probs<span class="token punctuation">,</span> num_samples<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                next_tokens <span class="token operator">=</span> torch<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>probs<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

            <span class="token comment"># update generated ids, model inputs, and length for next step</span>
            input_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>input_ids<span class="token punctuation">,</span> next_tokens<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            model_kwargs <span class="token operator">=</span> self<span class="token punctuation">.</span>_update_model_kwargs_for_generation<span class="token punctuation">(</span>
                outputs<span class="token punctuation">,</span> model_kwargs<span class="token punctuation">,</span> is_encoder_decoder<span class="token operator">=</span>self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>is_encoder_decoder
            <span class="token punctuation">)</span>
            unfinished_sequences <span class="token operator">=</span> unfinished_sequences<span class="token punctuation">.</span>mul<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token builtin">sum</span><span class="token punctuation">(</span>next_tokens <span class="token operator">!=</span> i <span class="token keyword">for</span> i <span class="token keyword">in</span> eos_token_id<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> return_past_key_values<span class="token punctuation">:</span>
                <span class="token keyword">yield</span> input_ids<span class="token punctuation">,</span> outputs<span class="token punctuation">.</span>past_key_values
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                <span class="token keyword">yield</span> input_ids
            <span class="token comment"># stop when each sentence is finished, or if we exceed the maximum length</span>
            <span class="token keyword">if</span> unfinished_sequences<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">or</span> stopping_criteria<span class="token punctuation">(</span>input_ids<span class="token punctuation">,</span> scores<span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token keyword">break</span>
    <span class="token comment">#用于量化方法</span>
    <span class="token keyword">def</span> <span class="token function">quantize</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> bits<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> empty_init<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> bits <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span>

        <span class="token keyword">from</span> <span class="token punctuation">.</span>quantization <span class="token keyword">import</span> quantize

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>quantized<span class="token punctuation">:</span>
            logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">"Already quantized."</span><span class="token punctuation">)</span>
            <span class="token keyword">return</span> self

        self<span class="token punctuation">.</span>quantized <span class="token operator">=</span> <span class="token boolean">True</span>

        self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>quantization_bit <span class="token operator">=</span> bits

        self<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>encoder <span class="token operator">=</span> quantize<span class="token punctuation">(</span>self<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">,</span> bits<span class="token punctuation">,</span> empty_init<span class="token operator">=</span>empty_init<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span>
                                            <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/886e9c3cbdc9480dc9a460a2c01ddc2b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Vue中如何进行音视频录制与视频剪辑</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/cffee28e22f4f633888788ceea8494e4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">打包spring boot项目时候出现Error starting ApplicationContext. To display the conditions report re-run your a</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>