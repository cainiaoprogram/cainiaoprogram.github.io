<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>《Towards Viewpoint Invariant 3D Human Pose Estimation》--深度图领域人体姿态估计的CNN算法 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="《Towards Viewpoint Invariant 3D Human Pose Estimation》--深度图领域人体姿态估计的CNN算法" />
<meta property="og:description" content="《Towards Viewpoint Invariant 3D Human Pose Estimation》–深度图领域人体姿态估计的CNN算法
这篇文章是ECCV 2016的一篇3D人体姿态估计的文章，一作Albert Haque是Li FeiFei的学生，出自Stanford University。 本文从视角变化作为切入点，研究了深度图领域的3D人体姿态估计任务，自从2011年Shotton, J等人在[Real-time human pose recognition in parts from single depth images. In: CVPR (2011)]中提出随机森林算法以来，该算法以及随后衍生的包括random tree walk系列算法基本上将深度图领域的人体姿态估计任务做到了非常高的水准，微软的Kinect深度相机自带SDK的骨骼提取算法用的就是随机森林算法，该算法的最大优点就是快，实时性要求完全能够得到满足，商业化的应用毫无疑问的证明了该算法的成功。然而CNN的崛起几年之内迅速攻占CV、NLP等领域各个任务，取得了巨大的成功，CNN的优势是大数据的训练支持（当然还有NVIDIA的GPU）以及其本身所具有的强大的非线性拟合能力。本文采用了一个CNN&#43;RNN的逐步refine的网络结构，通过不同视角（Side和Top）的数据进行训练，试图利用CNN的深层次特征来解决视角变化条件下的自遮挡等问题。本文中一个大的贡献是提供了一个大约100K带有标注的深度图数据库（https://www.albert.cm/projects/viewpoint_3d_pose/），这对于一些需要用到深度图做姿态估计等任务的人还是很有帮助的。
一：简介 文中先分析了目前人体姿态估计领域的研究现状：大都以正面视角或者侧面视角的图片进行分析，这虽然提高了精度，但却没能够做到抗视角变化（viewpoint variances）。事实上很多场合下，比如机场、医院、零售店，大都不能获得一个正面或者侧面的理想视角，因而如果考虑实用性的话，viewpoint variances就是一个必须要解决的问题。 解决视角变化的问题主要有两个障碍： 1、模型必须能够同时理解局部和全局信息，也就是说模型必须要具有bottom-up的判别方法所能提供的细节信息和top-down的生成方法所能提供的全局信息。 2、目前存在的depth数据库都比较小，包括数据库尺寸和数据类别。这就使得包括表征学习（representation learning）和viewpoint transfer techniques的技术的使用受到了很大限制。 本文提出了通过投射局部姿态信息到一个预训练的具有视角不变特性的特征空间的方法，并且采用了误差迭代反馈的思想，分stage逐步refine的网络结构。最后，本文提出了一个100K的depth数据库，并且在这个数据库和之前的一个公共数据库EVAL上面取得了state-of-art的效果。
二：模型 输入一张单深度图，分stage迭代refine，每一个stage包含两个输入，即原图处理生成的一个patch（称为Glimpses）和之前生成的估计姿态。模型预测输出的实际上是一个位置的偏移量和一个可见性蒙版。 模型细节： 输入表达：模型的输入图片并不直接输送到网络中去，而是通过提取图片中的不同的patches，并且将其转化成为Glimpses，Glimpse是一种对原始输入的特殊编码形式，它的中心分辨率高，周围分辨率逐渐降低，这就使得中心的特征得到重点的学习，同时又保留了部分空间信息。 Embedding：对于上面所说的通过投射局部姿态信息到一个预训练的具有视角不变特性的特征空间，本文实际上是利用了一个STN（spatial transformer networks）的结构，具体见下图：
对于一个输入的Glimpse，通过一个3D的STN网络（具体细节参见文章Spatial transformer networks. In: NIPS (2015)）得到一个3D的特征表达V并映射回到二维空间得到一个新的Embedding作为卷积网络的输入（U）。 CNN&#43;RNN网络结构：将前面得到的特征表达U组成一个H×W×J（J代表Joints，即关节点的数量）的tensor输入到卷积网络中去，由于直接从全连接层回归出关节点的实际位置非常困难（这是一个高度非线性映射的过程），本文借鉴了这篇文章的思路：Human pose estimation with iterative error feedback. In: CVPR (2016)，提出了一个分Stage逐步refine的结构，并且不同于借鉴的这篇文章，本文将两次迭代的结果用RNN的LSTM结构连接起来以增强时序信息的关联性。
三：loss和训练 由于视角变化所带来的遮挡问题对于人体姿态估计是个很大的困扰，本文将这个任务视为一个多任务学习问题，具体地，本文将整个模型优化问题分为两个步骤来做： 1、关节检测： 这一部分的任务是判断哪些关节是可见的，哪些关节受到了遮挡。这部分任务可以表示为模型中的可见性蒙版 α ，这是一个1×J的向量，如果预测得出的关节点是可见的，则 α 为1，否则为0。 α 是由LSTM生成的一个未归一化的概率经过Softmax得到的。这部分的检测损失是一个交叉熵的形式： 2、关节回归 这一部分的任务是输出关节点的偏移量信息，也就是实际的关节点位置信息。训练的时候只希望训练可见的关节点，因此，损失函数的表达形式为： 其中仅当 α =1时，才计算损失以更新权重。 全局损失可以表示为上面两项之和： 训练的时候是一个端到端的优化过程，CNN&#43;RNN的结构从头开始训练，优化器采用的是Adam optimizer，学习率设为1&#43;e-5， β(1) =0." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/956faa3c5f4492ebb71b13eac9f6ef54/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2017-04-26T21:56:54+08:00" />
<meta property="article:modified_time" content="2017-04-26T21:56:54+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">《Towards Viewpoint Invariant 3D Human Pose Estimation》--深度图领域人体姿态估计的CNN算法</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><strong>《Towards Viewpoint Invariant 3D Human Pose Estimation》–深度图领域人体姿态估计的CNN算法</strong></p> 
<p>这篇文章是ECCV 2016的一篇3D人体姿态估计的文章，一作Albert Haque是Li FeiFei的学生，出自Stanford University。 <br> 本文从视角变化作为切入点，研究了深度图领域的3D人体姿态估计任务，自从2011年Shotton, J等人在[Real-time human pose recognition in parts from single depth images. In: CVPR (2011)]中提出随机森林算法以来，该算法以及随后衍生的包括random tree walk系列算法基本上将深度图领域的人体姿态估计任务做到了非常高的水准，微软的Kinect深度相机自带SDK的骨骼提取算法用的就是随机森林算法，该算法的最大优点就是快，实时性要求完全能够得到满足，商业化的应用毫无疑问的证明了该算法的成功。然而CNN的崛起几年之内迅速攻占CV、NLP等领域各个任务，取得了巨大的成功，CNN的优势是大数据的训练支持（当然还有NVIDIA的GPU）以及其本身所具有的强大的非线性拟合能力。本文采用了一个CNN+RNN的逐步refine的网络结构，通过不同视角（Side和Top）的数据进行训练，试图利用CNN的深层次特征来解决视角变化条件下的自遮挡等问题。本文中一个大的贡献是提供了一个大约100K带有标注的深度图数据库（<a href="https://www.albert.cm/projects/viewpoint_3d_pose/" rel="nofollow">https://www.albert.cm/projects/viewpoint_3d_pose/</a>），这对于一些需要用到深度图做姿态估计等任务的人还是很有帮助的。</p> 
<p><em>一：简介</em> <br> 文中先分析了目前人体姿态估计领域的研究现状：大都以正面视角或者侧面视角的图片进行分析，这虽然提高了精度，但却没能够做到抗视角变化（viewpoint variances）。事实上很多场合下，比如机场、医院、零售店，大都不能获得一个正面或者侧面的理想视角，因而如果考虑实用性的话，viewpoint variances就是一个必须要解决的问题。 <br> 解决视角变化的问题主要有两个障碍： <br> 1、模型必须能够同时理解局部和全局信息，也就是说模型必须要具有bottom-up的判别方法所能提供的细节信息和top-down的生成方法所能提供的全局信息。 <br> 2、目前存在的depth数据库都比较小，包括数据库尺寸和数据类别。这就使得包括表征学习（representation learning）和viewpoint transfer techniques的技术的使用受到了很大限制。 <br> 本文提出了通过投射局部姿态信息到一个预训练的具有视角不变特性的特征空间的方法，并且采用了误差迭代反馈的思想，分stage逐步refine的网络结构。最后，本文提出了一个100K的depth数据库，并且在这个数据库和之前的一个公共数据库EVAL上面取得了state-of-art的效果。</p> 
<p><em>二：模型</em> <br> <img src="https://images2.imgbox.com/3f/c5/TEjx1xmP_o.png" alt="这里写图片描述" title=""></p> 
<p>输入一张单深度图，分stage迭代refine，每一个stage包含两个输入，即原图处理生成的一个patch（称为Glimpses）和之前生成的估计姿态。模型预测输出的实际上是一个位置的偏移量和一个可见性蒙版。 <br> 模型细节： <br> 输入表达：模型的输入图片并不直接输送到网络中去，而是通过提取图片中的不同的patches，并且将其转化成为Glimpses，Glimpse是一种对原始输入的特殊编码形式，它的中心分辨率高，周围分辨率逐渐降低，这就使得中心的特征得到重点的学习，同时又保留了部分空间信息。 <br> Embedding：对于上面所说的通过投射局部姿态信息到一个预训练的具有视角不变特性的特征空间，本文实际上是利用了一个STN（spatial transformer networks）的结构，具体见下图：</p> 
<p><img src="https://images2.imgbox.com/93/25/WTc9X50M_o.png" alt="这里写图片描述" title=""></p> 
<p>对于一个输入的Glimpse，通过一个3D的STN网络（具体细节参见文章Spatial transformer networks. In: NIPS (2015)）得到一个3D的特征表达V并映射回到二维空间得到一个新的Embedding作为卷积网络的输入（U）。 <br> CNN+RNN网络结构：将前面得到的特征表达U组成一个H×W×J（J代表Joints，即关节点的数量）的tensor输入到卷积网络中去，由于直接从全连接层回归出关节点的实际位置非常困难（这是一个高度非线性映射的过程），本文借鉴了这篇文章的思路：Human pose estimation with iterative error feedback. In: CVPR (2016)，提出了一个分Stage逐步refine的结构，并且不同于借鉴的这篇文章，本文将两次迭代的结果用RNN的LSTM结构连接起来以增强时序信息的关联性。</p> 
<p><em>三：loss和训练</em> <br> 由于视角变化所带来的遮挡问题对于人体姿态估计是个很大的困扰，本文将这个任务视为一个多任务学习问题，具体地，本文将整个模型优化问题分为两个步骤来做： <br> 1、关节检测： <br> 这一部分的任务是判断哪些关节是可见的，哪些关节受到了遮挡。这部分任务可以表示为模型中的可见性蒙版 <span class="MathJax_Preview"></span><span class="MathJax" id="MathJax-Element-1-Frame"> 
   
   <span class="math" id="MathJax-Span-1" style="width: 0.803em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.643em; height: 0px; font-size: 125%;"><span style="position: absolute; clip: rect(1.976em 1000em 2.723em -0.424em); top: -2.557em; left: 0.003em;"><span class="mrow" id="MathJax-Span-2"><span class="mi" id="MathJax-Span-3" style="font-family: MathJax_Math-italic;">α</span></span><span style="display: inline-block; width: 0px; height: 2.563em;"></span></span></span><span style="border-left: 0.003em solid; display: inline-block; overflow: hidden; width: 0px; height: 0.67em; vertical-align: -0.063em;"></span></span> 
  </span><script type="math/tex" id="MathJax-Element-1">\alpha</script>，这是一个1×J的向量，如果预测得出的关节点是可见的，则<span class="MathJax_Preview"></span><span class="MathJax" id="MathJax-Element-2-Frame"> 
   
   <span class="math" id="MathJax-Span-4" style="width: 0.803em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.643em; height: 0px; font-size: 125%;"><span style="position: absolute; clip: rect(1.976em 1000em 2.723em -0.424em); top: -2.557em; left: 0.003em;"><span class="mrow" id="MathJax-Span-5"><span class="mi" id="MathJax-Span-6" style="font-family: MathJax_Math-italic;">α</span></span><span style="display: inline-block; width: 0px; height: 2.563em;"></span></span></span><span style="border-left: 0.003em solid; display: inline-block; overflow: hidden; width: 0px; height: 0.67em; vertical-align: -0.063em;"></span></span> 
  </span><script type="math/tex" id="MathJax-Element-2">\alpha</script> 为1，否则为0。 <span class="MathJax_Preview"></span><span class="MathJax" id="MathJax-Element-3-Frame"> 
   
   <span class="math" id="MathJax-Span-7" style="width: 0.803em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.643em; height: 0px; font-size: 125%;"><span style="position: absolute; clip: rect(1.976em 1000em 2.723em -0.424em); top: -2.557em; left: 0.003em;"><span class="mrow" id="MathJax-Span-8"><span class="mi" id="MathJax-Span-9" style="font-family: MathJax_Math-italic;">α</span></span><span style="display: inline-block; width: 0px; height: 2.563em;"></span></span></span><span style="border-left: 0.003em solid; display: inline-block; overflow: hidden; width: 0px; height: 0.67em; vertical-align: -0.063em;"></span></span> 
  </span><script type="math/tex" id="MathJax-Element-3">\alpha</script>是由LSTM生成的一个未归一化的概率经过Softmax得到的。这部分的检测损失是一个交叉熵的形式： <br> <img src="https://images2.imgbox.com/6e/15/oNmIlCOi_o.png" alt="这里写图片描述" title=""></p> 
<p>2、关节回归 <br> 这一部分的任务是输出关节点的偏移量信息，也就是实际的关节点位置信息。训练的时候只希望训练可见的关节点，因此，损失函数的表达形式为： <br> <img src="https://images2.imgbox.com/87/96/HX2XydzW_o.png" alt="这里写图片描述" title=""></p> 
<p>其中仅当 <span class="MathJax_Preview"></span><span class="MathJax" id="MathJax-Element-4-Frame"> 
   
   <span class="math" id="MathJax-Span-10" style="width: 0.803em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.643em; height: 0px; font-size: 125%;"><span style="position: absolute; clip: rect(1.976em 1000em 2.723em -0.424em); top: -2.557em; left: 0.003em;"><span class="mrow" id="MathJax-Span-11"><span class="mi" id="MathJax-Span-12" style="font-family: MathJax_Math-italic;">α</span></span><span style="display: inline-block; width: 0px; height: 2.563em;"></span></span></span><span style="border-left: 0.003em solid; display: inline-block; overflow: hidden; width: 0px; height: 0.67em; vertical-align: -0.063em;"></span></span> 
  </span><script type="math/tex" id="MathJax-Element-4">\alpha</script>=1时，才计算损失以更新权重。 <br> 全局损失可以表示为上面两项之和： <br> <img src="https://images2.imgbox.com/ef/cb/MyXvEBIN_o.png" alt="这里写图片描述" title=""></p> 
<p>训练的时候是一个端到端的优化过程，CNN+RNN的结构从头开始训练，优化器采用的是Adam optimizer，学习率设为1+e-5，<span class="MathJax_Preview"></span><span class="MathJax" id="MathJax-Element-5-Frame"> 
   
   <span class="math" id="MathJax-Span-13" style="width: 2.349em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.869em; height: 0px; font-size: 125%;"><span style="position: absolute; clip: rect(1.816em 1000em 3.149em -0.477em); top: -2.717em; left: 0.003em;"><span class="mrow" id="MathJax-Span-14"><span class="mi" id="MathJax-Span-15" style="font-family: MathJax_Math-italic;">β<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-16" style="font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-17" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-18" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.723em;"></span></span></span><span style="border-left: 0.003em solid; display: inline-block; overflow: hidden; width: 0px; height: 1.403em; vertical-align: -0.397em;"></span></span> 
  </span><script type="math/tex" id="MathJax-Element-5">\beta(1)</script> =0.9， <span class="MathJax_Preview"></span><span class="MathJax" id="MathJax-Element-6-Frame"> 
   
   <span class="math" id="MathJax-Span-19" style="width: 2.349em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.869em; height: 0px; font-size: 125%;"><span style="position: absolute; clip: rect(1.816em 1000em 3.149em -0.477em); top: -2.717em; left: 0.003em;"><span class="mrow" id="MathJax-Span-20"><span class="mi" id="MathJax-Span-21" style="font-family: MathJax_Math-italic;">β<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-22" style="font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-23" style="font-family: MathJax_Main;">2</span><span class="mo" id="MathJax-Span-24" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.723em;"></span></span></span><span style="border-left: 0.003em solid; display: inline-block; overflow: hidden; width: 0px; height: 1.403em; vertical-align: -0.397em;"></span></span> 
  </span><script type="math/tex" id="MathJax-Element-6">\beta(2)</script> =0.999，decay rate设为0.99（每迭代1000次）。</p> 
<p><em>四：数据库</em> <br> Stanford的EVAL（Real-time human pose tracking from range data. In: ECCV. Springer (2012)）包含9K面对相机的图片，一共有3个人依次做出的8个动作序列。这个数据库的采集使用的是微软的Kinect，帧率30FPS。 <br> 本文提出的数据库包含100K的包括side和top两个视角的图片，这是由20个人各做出15个动作序列构成的，命名为ITOP数据库。每一张深度图片都标注有真实世界的3D位置信息。数据的采集使用的是两台华硕的Xtion PRO相机，分别采集side和top两个视角的图片。图片的标注采用的策略是：先用相机自带的SDK（采用的仍然是随机森林的算法）保存关节点信息，再将这些坐标投射到真实世界的坐标中去，随后采用了一个k-nearest neighbors and center of mass convergence的算法对Ground_Truth进行纠正，最后人工检查纠正。整个标注过程大约每秒一帧。</p> 
<p><em>五：实验</em> <br> TensorFlow上面进行，batch-size设为10同时采用10个stage的refine过程，CNN结构采用的VGG16，将Softmax层移除并且将Softmax前面的全连接层作为LSTM的输入。LSTM包含有2048个隐含单元。整个网络的训练从头开始。 <br> Baselines：随机森林（RF）、随机行走树（RTW）、IEF（iterative error feedback）。 <br> 测试分为三类进行，即：在front上面训练并在front上面测试；在top上面训练并在top上面测试；在front上面训练、top上面测试。 <br> 结果：Table1所示，在Front上面测试的结果和state-of-art相比差距很小，在top视角取得的结果是state-of-art。右侧EVAL数据库上面的结果没有RF和IEF，这是因为RF是一个逐像素分类算法，EVAL并没有提供每个像素的类别信息，而IEF则是因为无法在这个数据库上面取得可比的结果（可能是因为IEF设计的最初任务是针对彩色图片的）。</p> 
<p><img src="https://images2.imgbox.com/e9/fe/hwi1wA8F_o.png" alt="这里写图片描述" title=""></p> 
<p>Fig.5则给出了四个方法的最后结果PCKs比较。</p> 
<p><img src="https://images2.imgbox.com/e8/e8/2oZN8b9P_o.png" alt="这里写图片描述" title=""></p> 
<p>Fig.6给出了一些可视化的结果。</p> 
<p><img src="https://images2.imgbox.com/95/14/7eU7mSjk_o.png" alt="这里写图片描述" title=""></p> 
<p>Table 2给出了当视角变化的时候四种方法的效果比较。可以看到，两个基于CNN的方法IEF和本文中的模型都能对全局有一个认知，而RF和RTW则表现得较差。</p> 
<p><img src="https://images2.imgbox.com/9e/a7/pV8398j4_o.png" alt="这里写图片描述" title=""></p> 
<p>时间分析：本模型需要1.7s每帧，RTW需要0.1s每帧。</p> 
<p><em>六：Ablation Studies</em> <br> 为了证明模型中的每一个component是有用的，本文做了Ablation Studies。 <br> 首先研究了LSTM的作用： <br> Table 3显示了直接预测、迭代反馈预测、加上LSTM结构的三种网络结构的效果，可以看到LSTM的加入对于体很效果还是很有帮助的。 <br> Fig.7则展示了这种误差分stage迭代refine的结构对于预测最后的结果至关重要。</p> 
<p><img src="https://images2.imgbox.com/5b/5e/ocQ2wwGa_o.png" alt="这里写图片描述" title=""></p> 
<p>随后文章分析了Glimpse的作用： <br> Glimpse的设计初衷是为了在提取局部特征信息的时候同时保留一部分全局信息，这样能够帮助模型更好的理解图像。Fig.8左边（a）对比了Glimpse作为输入和Heatmap作为输入的区别。（b）则展示了Glimpse作为卷积网络的输入可以提供更多的空间纹理信息，对于提高关节点定位准确率非常有帮助。其实从（a）中也可以看出来，Heatmap作为输入的时候只能够提供非常有限的空间信息，而Glimpse则能够保留较多的周围的空间信息。</p> 
<p><img src="https://images2.imgbox.com/83/40/CioQzFdx_o.png" alt="这里写图片描述" title=""></p> 
<p><em>七：结论</em> <br> 总结本文的几个主要贡献：提出了一个具有视角不变特性的3D人体姿态估计模型，该模型通过将输入处理成为Glimpse的形式再映射到另一个预训练的特征空间中，提高了抗视角变化特性；将人体姿态估计任务理解为一个多任务学习的问题，这提高了该模型处理遮挡关节点的能力；作者公开了一个100K图片大小的深度图数据库ITOP，并且文中的模型在该数据库上的结果达到了state-of-art。</p> 
<hr>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b6c080efbebf449f02270728b39b6ce6/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">解决 MUI QQ登陆功能报错“该应用非官方正版应用，请到......100044”</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/9f6181e63ba8ba0cc61ef365623b4ee4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">vue2 前后端分离项目ajax跨域session问题解决</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>