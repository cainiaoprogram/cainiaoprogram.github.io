<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>一种解决bert长文本匹配的方法 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="一种解决bert长文本匹配的方法" />
<meta property="og:description" content="引言 bert提出来后打开了迁移学习的大门，首先通过无监督的语料训练通用的语言模型，然后基于自己的语料微调(finetune)模型来达到不同的业务需求。我们知道bert可以支持的最大token长度为512，如果最大长度超过512，该怎么样处理呢？下面这边论文提供一种简单有效的解决思路。
Simple Applications of BERT for Ad Hoc Document Retrieval 201903发表
1. 摘要 bert大招很好用，但是其最大长度为512以及其性能这两个缺点给我们的线上部署提出了挑战。我们在做document级别的召回的时候，其文本长度远超bert可以处理的长度，本文提出了一种简单并且有效的解决思路。将长的document分解成多个短的句子，每个句子在bert上独立推断，然后将这些句子的得分聚合得到document的得分。
2. 论文细节以及实验结果 2.1 长文本匹配解决思路
作者先以短文本匹配任务-社交媒体的帖子来做召回实验，通过query来召回相关的帖子，一般帖子的长度是较短的文本，在bert可以处理的范围内。实验的评价指标为两个平均召回(AP)和top30的召回率(P30)，下表是最近的深度模型在这个数据集上的结果。
我觉得上述实验数据主要说一点：
bert在短文本匹配类型的任务上效果很好，性能SOTA 长文本的docment匹配一般解决方法：
直接截断，取top长度，丢失了后面的数据；片段级递归机制，解决长文本依赖，如Transformer-XL，一定程度上可以解决长依赖问题（看递归长度），但模型稍复杂；基于抽取模型，抽取长文本docment的关键句子作为doc的摘要，然后基于此摘要进行匹配模型训练，这样只考虑了摘要，没有考虑其他句子，比较片面；将长文本划分为多个短句子，选择匹配度最高的来做匹配，同样没有考虑其他句子。 本文的方法
针对新闻语料的长文本召回问题，本文首先利用NLTK工具将长文本分为短的句子，不同于考虑最匹配的句子，本文考虑top n个句子。最终长文本docment的匹配得分计算公司如下：
其中S_doc是原始的长文本得分（文本得分），例如BM25得分，S_i表示第i个top的基于bert句子的匹配得分（语义得分），其中参数a的参数范围[0,1]，w1的值为1，wi参数范围[0,1]，基于gridsearch去调参，获得一个比较好的性能。
2.2 实验结果
finetune的数据
我们的原始的微调数据是查询query和长文本document的关系，而我们将长文本拆分为n个短句子后，不是所有的句子和当前的query是强相关的（正样本），因此我们不能简单依赖现在的长文本数据了。本论文的解决方法是基于外部语料，基于QA或者Microblog数据，首先bert基于通用的无监督语料学习到了词语以及句子的表征，所以基于少量的数据微调也可以获得较好的效果，因此本文选择外部相关的语料进行微调。具体效果如下表，我们发现长文本的匹配基于本文的方法可以取得比较好的效果。
3. 总结与问题 总结
本文提出一种加权的短句子得分方法来解决长文本匹配得分问题；该方法在该论文实验数据集上可以取得SOTA的效果，方法简单有效； 思考
论文中微调的数据使用外部数据，微调的模型没有很好拟合当前的数据，是不是可以从分割的短句子中进行正负样本的采样，这样微调的数据也是从长文本中得出；论文中如果选取的top n，如果n过大的话，调参有点复杂，n过大感觉可以取top3调参，然后后面平均。 参考文献 Simple Applications of BERT for Ad Hoc Document Retrieval" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/83890b21e01a51df9e73bfec49375918/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-02-14T14:57:00+08:00" />
<meta property="article:modified_time" content="2022-02-14T14:57:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">一种解决bert长文本匹配的方法</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3>引言</h3> 
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1810.04805" rel="nofollow" title="bert">bert</a>提出来后打开了迁移学习的大门，首先通过无监督的语料训练通用的语言模型，然后基于自己的语料微调(finetune)模型来达到不同的业务需求。我们知道bert可以支持的最大token长度为512，如果最大长度超过512，该怎么样处理呢？下面这边论文提供一种简单有效的解决思路。</p> 
<h3>Simple Applications of BERT for Ad Hoc Document Retrieval</h3> 
<p><strong>201903发表</strong></p> 
<h3>1. 摘要</h3> 
<p>bert大招很好用，但是其最大长度为512以及其性能这两个缺点给我们的线上部署提出了挑战。我们在做document级别的召回的时候，其文本长度远超bert可以处理的长度，本文提出了一种简单并且有效的解决思路。将长的document分解成多个短的句子，每个句子在bert上独立推断，然后将这些句子的得分聚合得到document的得分。</p> 
<h3>2. 论文细节以及实验结果</h3> 
<p><strong>2.1 长文本匹配解决思路</strong></p> 
<p>作者先以短文本匹配任务-社交媒体的帖子来做召回实验，通过query来召回相关的帖子，一般帖子的长度是较短的文本，在bert可以处理的范围内。实验的评价指标为两个<strong>平均召回(AP)和top30的召回率(P30)</strong>，下表是最近的深度模型在这个数据集上的结果。</p> 
<p></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/da/e0/vAEMdcNp_o.png"></p> 
<p><br> 我觉得上述实验数据主要说一点：</p> 
<blockquote>
  bert在短文本匹配类型的任务上效果很好，性能SOTA 
</blockquote> 
<p><strong>长文本的docment匹配一般解决方法：</strong></p> 
<ul><li>直接截断，取top长度，丢失了后面的数据；</li><li>片段级递归机制，解决长文本依赖，如<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1901.02860" rel="nofollow" title="Transformer-XL">Transformer-XL</a>，一定程度上可以解决长依赖问题（看递归长度），但模型稍复杂；</li><li>基于抽取模型，抽取长文本docment的关键句子作为doc的摘要，然后基于此摘要进行匹配模型训练，这样只考虑了摘要，没有考虑其他句子，比较片面；</li><li>将长文本划分为多个短句子，选择匹配度最高的来做匹配，同样没有考虑其他句子。</li></ul> 
<p><strong>本文的方法</strong></p> 
<p>针对新闻语料的长文本召回问题，本文首先利用NLTK工具将长文本分为短的句子，不同于考虑最匹配的句子，本文考虑<strong>top n</strong>个句子。最终长文本docment的匹配得分计算公司如下：</p> 
<p><img alt="" height="130" src="https://images2.imgbox.com/2e/f0/WMz51xbr_o.png" width="740"><br> 其中S_doc是原始的长文本得分（<strong>文本得分</strong>），例如BM25得分，S_i表示第i个top的基于bert句子的匹配得分（<strong>语义得分</strong>），其中参数a的参数范围[0,1]，w1的值为1，wi参数范围[0,1]，基于gridsearch去调参，获得一个比较好的性能。</p> 
<p> </p> 
<p><strong>2.2 实验结果</strong></p> 
<p><strong>finetune的数据</strong></p> 
<p>我们的原始的微调数据是查询query和长文本document的关系，而我们将长文本拆分为n个短句子后，不是所有的句子和当前的query是强相关的（正样本），因此我们不能简单依赖现在的长文本数据了。本论文的解决方法是基于外部语料，基于QA或者Microblog数据，首先bert基于通用的无监督语料学习到了词语以及句子的表征，所以基于少量的数据微调也可以获得较好的效果，因此本文选择外部相关的语料进行微调。具体效果如下表，我们发现长文本的匹配基于本文的方法可以取得比较好的效果。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/96/c7/LA9YmhTs_o.png"></p> 
<p></p> 
<h3>3. 总结与问题</h3> 
<p><strong>总结</strong></p> 
<ul><li>本文提出一种加权的短句子得分方法来解决长文本匹配得分问题；</li><li>该方法在该论文实验数据集上可以取得SOTA的效果，方法简单有效；</li></ul> 
<p><strong>思考</strong></p> 
<ul><li>论文中微调的数据使用外部数据，微调的模型没有很好拟合当前的数据，是不是可以从分割的短句子中进行正负样本的采样，这样微调的数据也是从长文本中得出；</li><li>论文中如果选取的top n，如果n过大的话，调参有点复杂，n过大感觉可以取top3调参，然后后面平均。</li></ul> 
<h3>参考文献</h3> 
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1903.10972" rel="nofollow" title="Simple Applications of BERT for Ad Hoc Document Retrieval">Simple Applications of BERT for Ad Hoc Document Retrieval</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c7aba14e61d293749bc136439d4ecc82/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">腾讯云服务器端口开启教程（安全组开放全部端口）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/5e3df0316a9e319ce12552b58688d283/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">python快速生成restful接口</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>