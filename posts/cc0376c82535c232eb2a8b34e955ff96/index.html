<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【论文阅读笔记】Cascaded Pyramid Network for Multi-Person Pose Estimation - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【论文阅读笔记】Cascaded Pyramid Network for Multi-Person Pose Estimation" />
<meta property="og:description" content="论文地址：CPN
论文总结 本文方法名为CPN，是个top-down的检测方法，即先用检测器得到人类的bounding box，再使用CPN来检测关键点。CPN是2017年COCO关键点检测的冠军算法。
如名字所言，是一个级联的金字塔网络。CPN由三个子网络组成：Backbone、GlobalNet、RefineNet。其中Backbone用于提取特征，GlobalNet用于融合不同尺度的特征（金字塔），RefineNet用于得到精细的输出。文中Backbone使用的是ResNet。
在训练时，在每个GlobalNet的输出上都添加一个预测，当做“中间监督”，使得收敛更加容易。由于有多个输出，CPN旨在GlobalNet预测出较为“simple”的关键点，但可能对遮挡、不可见的关键点无效；在RefineNet预测出较为“hard”的关键点，为达这个目的，RefineNet输出的损失函数使用online hard-keypoint mining loss，简写为ohkm方法。由于设计时是对GlobalNet和RefineNet有不同的目标期待，所以CPN又称为two-stage的姿态估计方法，RefineNet的名字由来也是对输出的keypoint 进行Refine的意思。
论文介绍 CPN是个two-stage的网络架。GlobalNet使用FPN网络学习好的特征表示，去预测那些可以通过简单特征表现可以看出来的simple joints；RefineNet使用FPN网络得到的金字塔特征，去预测那些不可见、遮挡的hard joints。
CPN所使用的检测器为FPN detector，其将RoIAlign应用在FPN上，COCO数据集的80个类别都正常检测，最后只提取出human bounding box用于pose检测。
CPN所使用的backbone为ResNet，特征提取层（为Keypoints生成heatmaps）为 C 2 ∼ C 5 C2\sim C5 C2∼C5，即每个stage的残差快的最后一个输出。
在特征金字塔与上采样的特征进行element sum之前，使用一个 1 ∗ 1 1*1 1∗1卷积对提取出的特征进行处理。
经过上采样的融合特征，可以添加pose检测器直接预测关节点，即GlobalNet的各级输出。
RefineNet将GlobalNet中不同level的信息进行整合，最后通过上采样到相同尺度后，使用Concat操作结合在一起。而信息整合的过程，如上图所示，在较低level的特征上使用较多的bottleneck blocks，然后一步直接使用大尺度的双线性上采样到指定的输出尺度。RefineNet的损失函数使用ohkm方法后，只对Loss最大的k个joints进行关键点回归。
论文实验 实验细节 Crop策略：对每个human bounding box，都扩展到某个固定的比率（比如 256 : 192 256:192 256:192），然后从没有经过变形操作（长宽比）的图像中进行Crop操作。最后将crop出来的图片Resize到 256 ∗ 192 256*192 256∗192。
数据增强策略：在Crop之后，使用随机flip，随机rotation（ − 45 ° ∼ 45 ° -45°\sim45° −45°∼45°），随机Scale（ 0.7 ∼ 1.35 0.7\sim1.35 0.7∼1.35）。
在测试时，使用高斯核与predict heamaps上，用以减少预测的方差。
消融学习 在人类检测器上，hard NMS的阈值越高，keypoint检测的效果越好；soft NMS对于detections和key points detections都有效。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/cc0376c82535c232eb2a8b34e955ff96/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-07-10T11:04:40+08:00" />
<meta property="article:modified_time" content="2020-07-10T11:04:40+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【论文阅读笔记】Cascaded Pyramid Network for Multi-Person Pose Estimation</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>论文地址：<a href="https://arxiv.org/abs/1711.07319" rel="nofollow">CPN</a></p> 
<h4><a id="_2"></a>论文总结</h4> 
<p>  本文方法名为<b>CPN</b>，是个<b>top-down</b>的检测方法，即先用检测器得到人类的bounding box，再使用CPN来检测关键点。CPN是2017年COCO关键点检测的冠军算法。<br>   如名字所言，是一个级联的金字塔网络。CPN由三个子网络组成：<b>Backbone、GlobalNet、RefineNet</b>。其中Backbone用于提取特征，GlobalNet用于融合不同尺度的特征（金字塔），RefineNet用于得到精细的输出。文中Backbone使用的是ResNet。</p> 
<p>  在训练时，在每个GlobalNet的输出上都添加一个预测，当做<b>“中间监督”</b>，使得收敛更加容易。由于有多个输出，CPN旨在GlobalNet预测出较为“simple”的关键点，但可能对遮挡、不可见的关键点无效；在RefineNet预测出较为“hard”的关键点，为达这个目的，RefineNet输出的损失函数使用<b>online hard-keypoint mining loss</b>，简写为<b>ohkm</b>方法。由于设计时是对GlobalNet和RefineNet有不同的目标期待，所以CPN又称为<b>two-stage</b>的姿态估计方法，RefineNet的名字由来也是对输出的keypoint 进行Refine的意思。</p> 
<div align="center"> 
 <img src="https://images2.imgbox.com/29/93/KvR1sfDN_o.png"> 
</div> 
<h4><a id="_12"></a>论文介绍</h4> 
<p>  CPN是个two-stage的网络架。GlobalNet使用FPN网络学习好的特征表示，去预测那些可以通过简单特征表现可以看出来的simple joints；RefineNet使用FPN网络得到的金字塔特征，去预测那些不可见、遮挡的hard joints。</p> 
<p>  CPN所使用的检测器为FPN detector，其将RoIAlign应用在FPN上，COCO数据集的80个类别都正常检测，最后只提取出human bounding box用于pose检测。</p> 
<p>  CPN所使用的backbone为ResNet，特征提取层（为Keypoints生成heatmaps）为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         C 
        
       
         2 
        
       
         ∼ 
        
       
         C 
        
       
         5 
        
       
      
        C2\sim C5 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.07153em;">C</span><span class="mord">2</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">∼</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.07153em;">C</span><span class="mord">5</span></span></span></span></span>，即每个stage的残差快的最后一个输出。<br>   在特征金字塔与上采样的特征进行element sum之前，使用一个<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         1 
        
       
         ∗ 
        
       
         1 
        
       
      
        1*1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span></span></span></span></span>卷积对提取出的特征进行处理。<br>   经过上采样的融合特征，可以添加pose检测器直接预测关节点，即GlobalNet的各级输出。<br>   RefineNet将GlobalNet中不同level的信息进行整合，最后通过上采样到相同尺度后，使用Concat操作结合在一起。而信息整合的过程，如上图所示，在<b>较低level的特征上使用较多的bottleneck blocks</b>，然后<b>一步直接使用大尺度的双线性上采样到指定的输出尺度</b>。RefineNet的损失函数使用ohkm方法后，只对Loss最大的k个joints进行关键点回归。</p> 
<h4><a id="_22"></a>论文实验</h4> 
<h6><a id="_23"></a>实验细节</h6> 
<p>  <strong>Crop策略</strong>：对每个human bounding box，都扩展到某个固定的比率（比如<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         256 
        
       
         : 
        
       
         192 
        
       
      
        256:192 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">2</span><span class="mord">5</span><span class="mord">6</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span><span class="mord">9</span><span class="mord">2</span></span></span></span></span>），然后从没有经过变形操作（长宽比）的图像中进行Crop操作。最后将crop出来的图片Resize到<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         256 
        
       
         ∗ 
        
       
         192 
        
       
      
        256*192 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">2</span><span class="mord">5</span><span class="mord">6</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span><span class="mord">9</span><span class="mord">2</span></span></span></span></span>。<br>   <strong>数据增强策略</strong>：在Crop之后，使用随机flip，随机rotation（<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         − 
        
       
         45 
        
       
         ° 
        
       
         ∼ 
        
       
         45 
        
       
         ° 
        
       
      
        -45°\sim45° 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.77777em; vertical-align: -0.08333em;"></span><span class="mord">−</span><span class="mord">4</span><span class="mord">5</span><span class="mord">°</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">∼</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord">4</span><span class="mord">5</span><span class="mord">°</span></span></span></span></span>），随机Scale（<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         0.7 
        
       
         ∼ 
        
       
         1.35 
        
       
      
        0.7\sim1.35 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">7</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">∼</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span><span class="mord">.</span><span class="mord">3</span><span class="mord">5</span></span></span></span></span>）。<br>   在<b>测试时，使用高斯核与predict heamaps上，用以减少预测的方差</b>。</p> 
<h6><a id="_28"></a>消融学习</h6> 
<p>  在<b>人类检测器上</b>，hard NMS的阈值越高，keypoint检测的效果越好；soft NMS对于detections和key points detections都有效。<br>   从下表可以看出，<b>随着检测框的AP增加，keypoint detection获得的AP增益越来越少。</b>因此，可以认为较高的检测AP已经包含了大部分的medium和large实例。</p> 
<div align="center"> 
 <img src="https://images2.imgbox.com/b0/d0/ovUIEhxY_o.png"> 
</div> 
<p>  <b>作者认为，现在姿态检测的关键在于加强姿态估计的难点keypoint的准确率。</b><br>   在CPN上，使用Hourglass 和 ResNet50作为backbone的实验如下表所示。<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         2 
        
       
      
        2 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">2</span></span></span></span></span>个stack hourglass已经达到了<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         8 
        
       
      
        8 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">8</span></span></span></span></span>个stack glass的性能。<b>dialation在浅层应用汇大大增加计算量（FLOPs）</b>。</p> 
<div align="center"> 
 <img src="https://images2.imgbox.com/8f/a1/1kHcOrsd_o.png"> 
</div> 
<p>  从上表可以看出，RefineNet的使用，可以为CPN增加2.0的AP。<br>   <b>RefineNet的消融学习</b>如下所示，即（1）直接使用Concat；（2）使用1个bottleneck后使用Concat；（3）使用多个bottleneck后直接使用Concat。在concat后接一个conv生成heatmaps。</p> 
<div align="center"> 
 <img src="https://images2.imgbox.com/62/41/sHKOl28L_o.png"> 
</div> 
<p>  <b>ohkm的top-k中的k的消融学习</b>，可以看出在COCO数据集上，K取8时，会得到最好的结果；</p> 
<div align="center"> 
 <img src="https://images2.imgbox.com/e3/e1/ZlpXXW24_o.png"> 
</div> 
<p>  <b>损失函数的消融学习</b>：没有中间监督会少0.9AP。</p> 
<div align="center"> 
 <img src="https://images2.imgbox.com/fc/73/nNO5OXHl_o.png"> 
</div> 
<p>  <b>输入尺寸的消融学习</b>：可以看出<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         256 
        
       
         ∗ 
        
       
         192 
        
       
      
        256*192 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">2</span><span class="mord">5</span><span class="mord">6</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span><span class="mord">9</span><span class="mord">2</span></span></span></span></span>的效果和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         256 
        
       
         ∗ 
        
       
         256 
        
       
      
        256*256 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">2</span><span class="mord">5</span><span class="mord">6</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">2</span><span class="mord">5</span><span class="mord">6</span></span></span></span></span>差不多，但<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         256 
        
       
         ∗ 
        
       
         256 
        
       
      
        256*256 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">2</span><span class="mord">5</span><span class="mord">6</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">2</span><span class="mord">5</span><span class="mord">6</span></span></span></span></span>比<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         256 
        
       
         ∗ 
        
       
         192 
        
       
      
        256*192 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">2</span><span class="mord">5</span><span class="mord">6</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span><span class="mord">9</span><span class="mord">2</span></span></span></span></span>多了<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         6.4 
        
       
         G 
        
       
      
        6.4G 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord">6</span><span class="mord">.</span><span class="mord">4</span><span class="mord mathdefault">G</span></span></span></span></span>FOLPs。</p> 
<div align="center"> 
 <img src="https://images2.imgbox.com/23/1e/6jnRAxMU_o.png"> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a4ac7832bdead1d306c579ea29b5965c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Python pandas库  常用方法使用</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/111038c601ceeb27c083ae0f2356d5be/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">React解决跨域</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>