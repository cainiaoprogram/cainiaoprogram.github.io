<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>onnx转换TensorRT的步骤 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="onnx转换TensorRT的步骤" />
<meta property="og:description" content="A. 解析onnx 已有的trt不适配，需要将onnx转为trt
parse onnxserialize trt保存trt文件 注意：如果不使用Int8模式，onnx的parser代码几乎通用
概览 构建阶段
建立logger（日志）建立builder（网络元数据)创建network（计算图）（API独需）生成序列化的网络（网络的trt内部表示） 运行阶段
建立engine（可执行代码）创建context（gpu进程）buffer准备（host&#43;device）拷贝host to device执行推理execute拷贝device to host善后 A.1 构建阶段 1. 创建logger 记录器
getTRTLogger();
2. 创建builder 模型搭建的入口，网络的trt内部表示和引擎都是builder的成员方法生成的
builder.create_optimization_profile()：创建用于dynamic shape输入的配置器
createInferBuilder()
builder.create_network()：创建tensorrt网络对象
createNetworkV2()
在builderconfig下面进行细节设置
另外builder需要创建optimazation profile
在给定输入张量的最小最常见最大尺寸后，将设置的profile传给config
auto profile = builder-&gt;createOptimizationProfile(); profile-&gt;setDimensions(); config-&gt;addOptimizationProfile(profile); 3. 设置builder config 进行设置网络属性
config=builder.create_builder_config()
auto config = std::unique_ptr&lt;nvinfer1::IBuilderConfig, samplesCommon::InferDeleter&gt;(builder-&gt;createBuilderConfig()); 指定构建期可用显存设置标志位开关指定校正器添加用于dynamic shape输入的配置器 config-&gt;addOptimizationProfile(profile);//添加用于dynamic shape输入的配置器 config-&gt;setFlag(); 4. 搭建network 创建network（计算图）是API独需的因为其他两种方法使用parser从onnx导入，不用一层层添加
network=builder.create_network()
在onnx-parser中一旦模型parser解析完成，network就自动填好了，成为了serialized network
onnx-parser解析
createParser(*network, sample::gLogger.getTRTLogger(); ​ parser-&gt;parseFromFile(modelFile.c_str(), static_cast&lt;int&gt;(sample::gLogger.getReportableSeverity())); A.2 运行阶段 runtime 5." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/e3924776b5ac8e0413d5c3d98decfcee/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-10T14:03:37+08:00" />
<meta property="article:modified_time" content="2023-10-10T14:03:37+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">onnx转换TensorRT的步骤</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3 id="h_652641086_8"><strong>A. 解析onnx</strong></h3> 
<p>已有的trt不适配，需要将onnx转为trt</p> 
<ol><li>parse onnx</li><li>serialize trt</li><li>保存trt文件</li></ol> 
<p>注意：如果不使用Int8模式，onnx的parser代码几乎通用</p> 
<h3 id="h_652641086_9"><strong>概览</strong></h3> 
<p></p> 
<p class="img-center"><img alt="" height="575" src="https://images2.imgbox.com/f1/eb/JW8lzX6d_o.png" width="720"></p> 
<p>构建阶段</p> 
<ol><li>建立logger（日志）</li><li>建立builder（网络元数据)</li><li>创建network（计算图）（API独需）</li><li>生成序列化的网络（网络的trt内部表示）</li></ol> 
<p>运行阶段</p> 
<ol><li>建立engine（可执行代码）</li><li>创建context（gpu进程）</li><li>buffer准备（host+device）</li><li>拷贝host to device</li><li>执行推理execute</li><li>拷贝device to host</li><li>善后</li></ol> 
<h3 id="h_652641086_10"><strong>A.1 构建阶段</strong></h3> 
<h4 id="h_652641086_11"><strong>1. 创建logger</strong></h4> 
<p>记录器</p> 
<p><code>getTRTLogger();</code></p> 
<h4 id="h_652641086_12"><strong>2. 创建builder</strong></h4> 
<p>模型搭建的入口，网络的trt内部表示和引擎都是builder的成员方法生成的</p> 
<p>builder.create_optimization_profile()：创建用于dynamic shape输入的配置器</p> 
<p><code>createInferBuilder()</code></p> 
<p>builder.create_network()：创建tensorrt网络对象</p> 
<p><code>createNetworkV2()</code></p> 
<p>在builderconfig下面进行细节设置</p> 
<hr> 
<p>另外builder需要创建optimazation profile</p> 
<p>在给定输入张量的最小最常见最大尺寸后，将设置的profile传给config</p> 
<pre><code>auto profile = builder-&gt;createOptimizationProfile();
profile-&gt;setDimensions();
config-&gt;addOptimizationProfile(profile);
</code></pre> 
<h4 id="h_652641086_13"><strong>3. 设置builder config</strong></h4> 
<p>进行设置网络属性</p> 
<p>config=builder.create_builder_config()</p> 
<pre><code>auto config = std::unique_ptr&lt;nvinfer1::IBuilderConfig, samplesCommon::InferDeleter&gt;(builder-&gt;createBuilderConfig());
</code></pre> 
<ol><li>指定构建期可用显存</li><li>设置标志位开关</li><li>指定校正器</li><li>添加用于dynamic shape输入的配置器</li></ol> 
<pre><code>config-&gt;addOptimizationProfile(profile);//添加用于dynamic shape输入的配置器
config-&gt;setFlag();
</code></pre> 
<h4 id="h_652641086_14"><strong>4. 搭建network</strong></h4> 
<p>创建network（计算图）是API独需的因为其他两种方法使用parser从onnx导入，不用一层层添加</p> 
<p>network=builder.create_network()</p> 
<p>在onnx-parser中一旦模型parser解析完成，network就自动填好了，成为了serialized network</p> 
<p><strong>onnx-parser解析</strong></p> 
<pre><code>createParser(*network, sample::gLogger.getTRTLogger();
​
parser-&gt;parseFromFile(modelFile.c_str(), static_cast&lt;int&gt;(sample::gLogger.getReportableSeverity()));
</code></pre> 
<h3 id="h_652641086_15"><strong>A.2 运行阶段 runtime</strong></h3> 
<h4 id="h_652641086_16"><strong>5. 生成TRT内部表示-serialized network</strong></h4> 
<p>build_serialized_network(network,config)</p> 
<h4 id="h_652641086_17"><strong>6. 生成engine</strong></h4> 
<p>推理引擎，可执行的代码段</p> 
<p>生成engine：</p> 
<pre><code>m_engine = std::unique_ptr&lt;nvinfer1::ICudaEngine, samplesCommon::InferDeleter&gt;(builder-&gt;buildEngineWithConfig(*network, *config), samplesCommon::InferDeleter());
</code></pre> 
<h4 id="h_652641086_18"><strong>7. 创建context</strong></h4> 
<p>context即GPU进程</p> 
<p>创建context：</p> 
<p>python:<code>engine.create_execution_context()</code></p> 
<pre><code> m_context = std::unique_ptr&lt;nvinfer1::IExecutionContext, samplesCommon::InferDeleter&gt;(m_engine-&gt;createExecutionContext(), samplesCommon::InferDeleter());
</code></pre> 
<h4 id="h_652641086_19"><strong>绑定输入输出</strong></h4> 
<p>仅dynamic shape需要</p> 
<h4 id="h_652641086_20"><strong>8. 准备buffer</strong></h4> 
<ol><li>内存和显存的分别申请</li><li>拷贝</li><li>释放</li></ol> 
<p>python:<code>cudart.cudaMalloc(inputHost.nbytes)[1]</code></p> 
<p><strong>课程第四部分会对buffer部分的优化做介绍</strong></p> 
<p></p> 
<p class="img-center"><img alt="" height="137" src="https://images2.imgbox.com/13/74/2z6UABAv_o.png" width="720"></p> 
<p></p> 
<p class="img-center"><img alt="" height="368" src="https://images2.imgbox.com/9c/c9/biBZ9QcS_o.png" width="720"></p> 
<h4 id="h_652641086_21"><strong>9. 执行计算-execute</strong></h4> 
<p>拷贝到cuda buffer上执行再拷贝回host，这一步一般是<strong>B.解析trt</strong>中做，但是读取onnx后也可以做</p> 
<p></p> 
<p class="img-center"><img alt="" height="79" src="https://images2.imgbox.com/93/b2/1XZYRUec_o.png" width="720"></p> 
<h4 id="h_652641086_22"><strong>10. 序列化引擎</strong></h4> 
<p><code>engine-&gt;serialize()</code></p> 
<h4 id="h_652641086_23"><strong>11. 导出trt</strong></h4> 
<h3 id="h_652641086_24"><strong>特殊情况</strong></h3> 
<p>遇到tensorrt不支持的onnx模型节点</p> 
<ol><li>修改源模型</li><li>修改onnx计算图，onnx-surgeon</li><li>tensorrt中实现plugin</li><li>修改parser：修改源码，重新编译trt，因为tensorrt部分开源</li></ol> 
<h3 id="h_652641086_25"><strong>B. 解析trt</strong></h3> 
<p>已有trt，直接导入然后使用</p> 
<p>parse TRT后得到engine和context</p> 
<h4 id="h_652641086_26"><strong>1. 创建logger</strong></h4> 
<p><code>getTRTLogger()</code></p> 
<h4 id="h_652641086_27"><strong>2. 创建cudaruntime</strong></h4> 
<p><code>createInferRuntime()</code></p> 
<h4 id="h_652641086_28"><strong>3. 解析/反序列化trt文件，生成引擎</strong></h4> 
<p><code>runtime-&gt;deserializeCudaEngine()</code></p> 
<h4 id="h_652641086_29"><strong>4. 创建context</strong></h4> 
<p><code>engine-&gt;createExecutionContext()</code></p> 
<h4 id="h_652641086_30"><strong>5. 使用</strong></h4>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8f583705c4b3e313b7150b13144f2b42/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">uView自定义图标和普通引入图标（iconfont-阿里巴巴图标库）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/16f701965dcf856a54388614866d2e1e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">使用burpsuite进行布尔盲注(sqli-labs第7关)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>