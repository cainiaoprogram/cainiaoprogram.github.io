<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>PyTorch学习之归一化层（BatchNorm、LayerNorm、InstanceNorm、GroupNorm） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="PyTorch学习之归一化层（BatchNorm、LayerNorm、InstanceNorm、GroupNorm）" />
<meta property="og:description" content="BN，LN，IN，GN从学术化上解释差异：
BatchNorm：batch方向做归一化，算NHW的均值，对小batchsize效果不好；BN主要缺点是对batchsize的大小比较敏感，由于每次计算均值和方差是在一个batch上，所以如果batchsize太小，则计算的均值、方差不足以代表整个数据分布
LayerNorm：channel方向做归一化，算CHW的均值，主要对RNN作用明显；
InstanceNorm：一个channel内做归一化，算H*W的均值，用在风格化迁移；因为在图像风格化中，生成结果主要依赖于某个图像实例，所以对整个batch归一化不适合图像风格化中，因而对HW做归一化。可以加速模型收敛，并且保持每个图像实例之间的独立。
GroupNorm：将channel方向分group，然后每个group内做归一化，算(C//G)HW的均值；这样与batchsize无关，不受其约束。
SwitchableNorm是将BN、LN、IN结合，赋予权重，让网络自己去学习归一化层应该使用什么方法。
1 BatchNorm torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
torch.nn.BatchNorm3d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
参数：
num_features： 来自期望输入的特征数，该期望输入的大小为’batch_size x num_features [x width]’
eps： 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。
momentum： 动态均值和动态方差所使用的动量。默认为0.1。
affine： 布尔值，当设为true，给该层添加可学习的仿射变换参数。
track_running_stats：布尔值，当设为true，记录训练过程中的均值和方差；
实现公式：
2 GroupNorm torch.nn.GroupNorm(num_groups, num_channels, eps=1e-05, affine=True)
参数：
num_groups：需要划分为的groups
num_features： 来自期望输入的特征数，该期望输入的大小为’batch_size x num_features [x width]’
eps： 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。
momentum： 动态均值和动态方差所使用的动量。默认为0.1。
affine： 布尔值，当设为true，给该层添加可学习的仿射变换参数。
实现公式：
3 InstanceNorm torch.nn.InstanceNorm1d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
torch.nn.InstanceNorm2d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/8d74e44ecb26d7b377fa55419f66d005/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-12-18T19:31:10+08:00" />
<meta property="article:modified_time" content="2018-12-18T19:31:10+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">PyTorch学习之归一化层（BatchNorm、LayerNorm、InstanceNorm、GroupNorm）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <blockquote> 
 <p><strong>BN，LN，IN，GN从学术化上解释差异：</strong><br> <strong>BatchNorm</strong>：batch方向做归一化，算N<em>H</em>W的均值，对小batchsize效果不好；BN主要缺点是对batchsize的大小比较敏感，由于每次计算均值和方差是在一个batch上，所以如果batchsize太小，则计算的均值、方差不足以代表整个数据分布<br> <strong>LayerNorm</strong>：channel方向做归一化，算C<em>H</em>W的均值，主要对RNN作用明显；<br> <strong>InstanceNorm</strong>：一个channel内做归一化，算H*W的均值，用在风格化迁移；因为在图像风格化中，生成结果主要依赖于某个图像实例，所以对整个batch归一化不适合图像风格化中，因而对HW做归一化。可以加速模型收敛，并且保持每个图像实例之间的独立。<br> <strong>GroupNorm</strong>：将channel方向分group，然后每个group内做归一化，算(C//G)<em>H</em>W的均值；这样与batchsize无关，不受其约束。<br> <strong>SwitchableNorm</strong>是将BN、LN、IN结合，赋予权重，让网络自己去学习归一化层应该使用什么方法。</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/79/04/wAh6HHbC_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="1_BatchNorm_9"></a>1 BatchNorm</h4> 
<p><code>torch.nn.BatchNorm1d</code>(<em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=True</em>, <em>track_running_stats=True</em>)<br> <code>torch.nn.BatchNorm2d</code>(<em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=True</em>, <em>track_running_stats=True</em>)<br> <code>torch.nn.BatchNorm3d</code>(<em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=True</em>, <em>track_running_stats=True</em>)</p> 
<p><strong>参数：</strong></p> 
<blockquote> 
 <p>num_features： 来自期望输入的特征数，该期望输入的大小为’batch_size x num_features [x width]’<br> eps： 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。<br> momentum： 动态均值和动态方差所使用的动量。默认为0.1。<br> affine： 布尔值，当设为true，给该层添加可学习的仿射变换参数。<br> track_running_stats：布尔值，当设为true，记录训练过程中的均值和方差；</p> 
</blockquote> 
<p><strong>实现公式：</strong><br> <img src="https://images2.imgbox.com/19/b6/YiPPJmQy_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="2_GroupNorm_24"></a>2 GroupNorm</h4> 
<p><code>torch.nn.GroupNorm</code>(<em>num_groups</em>, <em>num_channels</em>, <em>eps=1e-05</em>, <em>affine=True</em>)</p> 
<p><strong>参数：</strong></p> 
<blockquote> 
 <p>num_groups：需要划分为的groups<br> num_features： 来自期望输入的特征数，该期望输入的大小为’batch_size x num_features [x width]’<br> eps： 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。<br> momentum： 动态均值和动态方差所使用的动量。默认为0.1。<br> affine： 布尔值，当设为true，给该层添加可学习的仿射变换参数。</p> 
</blockquote> 
<p><strong>实现公式：</strong><br> <img src="https://images2.imgbox.com/7c/32/Qk3wH9c3_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="3_InstanceNorm_37"></a>3 InstanceNorm</h4> 
<p><code>torch.nn.InstanceNorm1d</code>(<em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=False</em>, <em>track_running_stats=False</em>)<br> <code>torch.nn.InstanceNorm2d</code>(<em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=False</em>, <em>track_running_stats=False</em>)<br> <code>torch.nn.InstanceNorm3d</code>(<em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=False</em>, <em>track_running_stats=False</em>)</p> 
<p><strong>参数：</strong></p> 
<blockquote> 
 <p>num_features： 来自期望输入的特征数，该期望输入的大小为’batch_size x num_features [x width]’<br> eps： 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。<br> momentum： 动态均值和动态方差所使用的动量。默认为0.1。<br> affine： 布尔值，当设为true，给该层添加可学习的仿射变换参数。<br> track_running_stats：布尔值，当设为true，记录训练过程中的均值和方差；</p> 
</blockquote> 
<p><strong>实现公式：</strong><br> <img src="https://images2.imgbox.com/04/d0/hY1WgFFY_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="4_LayerNorm_52"></a>4 LayerNorm</h4> 
<p><code>torch.nn.LayerNorm</code>(<em>normalized_shape</em>, <em>eps=1e-05</em>, <em>elementwise_affine=True</em>)</p> 
<p><strong>参数：</strong></p> 
<blockquote> 
 <p>normalized_shape： 输入尺寸<br> [∗×normalized_shape[0]×normalized_shape[1]×…×normalized_shape[−1]]<br> eps： 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。<br> elementwise_affine： 布尔值，当设为true，给该层添加可学习的仿射变换参数。</p> 
</blockquote> 
<p><strong>实现公式：</strong><br> <img src="https://images2.imgbox.com/1b/94/Pw6pJL8P_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="5_LocalResponseNorm_64"></a>5 LocalResponseNorm</h4> 
<p><code>torch.nn.LocalResponseNorm</code>(<em>size</em>, <em>alpha=0.0001</em>, <em>beta=0.75</em>, <em>k=1.0</em>)</p> 
<p><strong>参数：</strong></p> 
<blockquote> 
 <p>size：用于归一化的邻居通道数<br> alpha：乘积因子，Default: 0.0001<br> beta ：指数，Default: 0.75<br> k：附加因子，Default: 1</p> 
</blockquote> 
<p><strong>实现公式：</strong><br> <img src="https://images2.imgbox.com/2b/03/mLMWGWGv_o.png" alt="在这里插入图片描述"></p> 
<p>参考：<a href="https://blog.csdn.net/liuxiao214/article/details/81037416">BatchNormalization、LayerNormalization、InstanceNorm、GroupNorm、SwitchableNorm总结</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/53324e14cb3f1530355999b0920f7a17/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Android全局异常捕获，并将错误信息保存到SD卡中</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1ed48a97255da2b20a5a4c1c48f1df26/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">MATLAB 正则表达式</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>