<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Python爬虫的起点 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Python爬虫的起点" />
<meta property="og:description" content="作者| 猪哥
来源 | 裸睡的猪
第一章主要讲解爬虫相关的知识如：http、网页、爬虫法律等，让大家对爬虫有了一个比较完善的了解和一些题外的知识点。
今天这篇文章将是我们第二章的第一篇，我们从今天开始就正式进入实战阶段，后面将会有更多的实际案例。
爬虫系列文章的第一篇，猪哥便为大家讲解了HTTP原理，很多人好奇：好好的讲爬虫和HTTP有什么关系？其实我们常说的爬虫（也叫网络爬虫）就是使用一些网络协议发起的网络请求，而目前使用最多的网络协议便是HTTP/S网络协议簇。
一、Python有哪些网络库 在真实浏览网页我们是通过鼠标点击网页然后由浏览器帮我们发起网络请求，那在Python中我们又如何发起网络请求的呢？答案当然是库，具体哪些库？猪哥给大家列一下：
Python2: httplib、httplib2、urllib、urllib2、urllib3、requests
Python3: httplib2、urllib、urllib3、requests
Python网络请求库有点多，而且还看见网上还都有用过的，那他们之间有何关系？又该如何选择？
httplib/2：
这是一个Python内置http库，但是它是偏于底层的库，一般不直接用。
而httplib2是一个基于httplib的第三方库，比httplib实现更完整，支持缓存、压缩等功能。
一般这两个库都用不到，如果需要自己 封装网络请求可能会需要用到。
urllib/urllib2/urllib3：
urlliib是一个基于httplib的上层库，而urllib2和urllib3都是第三方库，urllib2相对于urllib增加一些高级功能，如：
HTTP身份验证或Cookie等，在Python3中将urllib2合并到了urllib中。
urllib3提供线程安全连接池和文件post等支持，与urllib及urllib2的关系不大。
requests：
requests库是一个基于urllib/3的第三方网络库，它的特点是功能强大，API优雅。
由上图我们可以看到，对于http客户端python官方文档也推荐我们使用requests库，实际工作中requests库也是使用的比较多的库。
综上所述，我们选择选择requests库作为我们爬虫入门的起点。另外以上的这些库都是同步网络库，如果需要高并发请求的话可以使用异步网络库：aiohttp，这个后面猪哥也会为大家讲解。
二、requests介绍 希望大家永远记住：学任何一门语言，都不要忘记去看看官方文档。也许官方文档不是最好的入门教程，但绝对是最新、最全的教学文档！
1.首页 requests的官方文档（目前已支持中文）链接：http://cn.python-requests.org
源代码地址：https://github.com/kennethreitz/requests
从首页中让HTTP服务人类这几个字中我们便能看出，requests核心宗旨便是让用户使用方便，间接表达了他们设计优雅的理念。
注：PEP 20便是鼎鼎大名的Python之禅。
警告：非专业使用其他 HTTP 库会导致危险的副作用，包括：安全缺陷症、冗余代码症、重新发明轮子症、啃文档症、抑郁、头疼、甚至死亡。
2.功能特性 都说requests功能强大，那我们来看看requests到底有哪些功能特性吧：
Keep-Alive &amp; 连接池
国际化域名和 URL
带持久 Cookie 的会话
浏览器式的 SSL 认证
自动内容解码
基本/摘要式的身份认证
优雅的 key/value Cookie
自动解压
Unicode 响应体
HTTP(S) 代理支持
文件分块上传
流下载
连接超时
分块请求
支持 .netrc
requests 完全满足今日 web 的需求。Requests 支持 Python 2." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/43e243eba4cd6e396e2dd94f11d3adbe/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-07-09T11:52:14+08:00" />
<meta property="article:modified_time" content="2019-07-09T11:52:14+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Python爬虫的起点</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div class="rich_media_content" id="js_content"> 
 <p><span style="font-size:14px;color:rgb(136,136,136);">作者| 猪哥</span><br></p> 
 <p><span style="font-size:14px;color:rgb(136,136,136);">来源 | 裸睡的猪</span><br></p> 
 <p>第一章主要讲解爬虫相关的知识如：http、网页、爬虫法律等，让大家对爬虫有了一个比较完善的了解和一些题外的知识点。<br><img src="https://images2.imgbox.com/29/83/1tN9bcLL_o.png" alt="640?wx_fmt=png"></p> 
 <p>今天这篇文章将是我们第二章的第一篇，我们从今天开始就正式进入实战阶段，后面将会有更多的实际案例。</p> 
 <p>爬虫系列文章的第一篇，猪哥便为大家讲解了HTTP原理，很多人好奇：好好的讲爬虫和HTTP有什么关系？其实我们常说的爬虫（也叫网络爬虫）就是使用一些网络协议发起的网络请求，而目前使用最多的网络协议便是HTTP/S网络协议簇。</p> 
 <h3 style="font-weight:bold;font-size:1.4em;border-bottom:1px solid rgb(238,238,238);">一、Python有哪些网络库</h3> 
 <p>在真实浏览网页我们是通过鼠标点击网页然后由浏览器帮我们发起网络请求，那在Python中我们又如何发起网络请求的呢？答案当然是库，具体哪些库？猪哥给大家列一下：</p> 
 <ul style="margin-left:0px;" class="list-paddingleft-2"><li><p>Python2: httplib、httplib2、urllib、urllib2、urllib3、requests</p></li><li><p>Python3: httplib2、urllib、urllib3、requests</p></li></ul> 
 <p>Python网络请求库有点多，而且还看见网上还都有用过的，那他们之间有何关系？又该如何选择？</p> 
 <ul style="margin-left:0px;" class="list-paddingleft-2"><li><p>httplib/2：</p><p>这是一个Python内置http库，但是<strong>它是偏于底层的库，一般不直接用</strong>。</p><p>而httplib2是一个基于httplib的第三方库，比httplib实现更完整，支持缓存、压缩等功能。</p><p><strong>一般这两个库都用不到</strong>，如果需要自己 封装网络请求可能会需要用到。</p><p><img src="https://images2.imgbox.com/fe/8d/okCLmqjw_o.png" alt="640?wx_fmt=png"></p></li><li><p>urllib/urllib2/urllib3：</p><p>urlliib是一个基于httplib的上层库，而urllib2和urllib3都是第三方库，urllib2相对于urllib增加一些高级功能，如：</p><p>HTTP身份验证或Cookie等，在Python3中将urllib2合并到了urllib中。</p><p>urllib3提供线程安全连接池和文件post等支持，与urllib及urllib2的关系不大。</p><p><img src="https://images2.imgbox.com/a0/30/nIrp2t2B_o.png" alt="640?wx_fmt=png"></p></li><li><p>requests：</p><p>requests库是一个基于urllib/3的第三方网络库，它的特点是功<strong>能强大，API优雅</strong>。</p><p>由上图我们可以看到，对于http客户端python官方文档也推荐我们使用requests库，实际工作中requests库也是使用的比较多的库。</p></li></ul> 
 <p>综上所述，我们选择选择<strong>requests库作为我们爬虫入门的起点</strong>。另外以上的这些库都是同步网络库，如果需要高并发请求的话可以使用异步网络库：aiohttp，这个后面猪哥也会为大家讲解。</p> 
 <h3 style="font-weight:bold;font-size:1.4em;border-bottom:1px solid rgb(238,238,238);">二、requests介绍</h3> 
 <p><strong>希望大家永远记住：学任何一门语言，都不要忘记去看看官方文档。</strong>也许官方文档不是最好的入门教程，但绝对是最新、最全的教学文档！</p> 
 <h4 style="font-weight:bold;font-size:1.3em;">1.首页</h4> 
 <p>requests的官方文档（目前已支持中文）链接：http://cn.python-requests.org<br>源代码地址：https://github.com/kennethreitz/requests<br><img src="https://images2.imgbox.com/b0/a9/Xfz7RfIZ_o.png" alt="640?wx_fmt=png"></p> 
 <p>从首页中<code style="font-size:.85em;font-family:Consolas, Inconsolata, Courier, monospace;border-width:1px;border-style:solid;border-color:rgb(234,234,234);">让HTTP服务人类</code>这几个字中我们便能看出，requests核心宗旨便是让用户使用方便，间接表达了他们设计优雅的理念。<img src="https://images2.imgbox.com/8c/ec/ipMz3MGf_o.png" alt="640?wx_fmt=png"><br>注：PEP 20便是鼎鼎大名的<code style="font-size:.85em;font-family:Consolas, Inconsolata, Courier, monospace;border-width:1px;border-style:solid;border-color:rgb(234,234,234);">Python之<span style="color:rgb(26,26,26);font-family:'-apple-system', 'system-ui', 'Helvetica Neue', 'PingFang SC', 'Microsoft YaHei', 'Source Han Sans SC', 'Noto Sans CJK SC', 'WenQuanYi Micro Hei', sans-serif;font-size:16px;text-align:left;">禅</span></code>。</p> 
 <blockquote style="border-left:4px solid rgb(221,221,221);color:rgb(119,119,119);"> 
  <p>警告：非专业<strong>使用其他 HTTP 库</strong>会导致危险的副作用，包括：安全缺陷症、冗余代码症、重新发明轮子症、啃文档症、抑郁、头疼、甚至死亡。</p> 
 </blockquote> 
 <h4 style="font-weight:bold;font-size:1.3em;">2.功能特性</h4> 
 <p>都说requests功能强大，那我们来看看requests到底有哪些功能特性吧：</p> 
 <ul style="margin-left:0px;" class="list-paddingleft-2"><li><p>Keep-Alive &amp; 连接池</p></li><li><p>国际化域名和 URL</p></li><li><p>带持久 Cookie 的会话</p></li><li><p>浏览器式的 SSL 认证</p></li><li><p>自动内容解码</p></li><li><p>基本/摘要式的身份认证</p></li><li><p>优雅的 key/value Cookie</p></li><li><p>自动解压</p></li><li><p>Unicode 响应体</p></li><li><p>HTTP(S) 代理支持</p></li><li><p>文件分块上传</p></li><li><p>流下载</p></li><li><p>连接超时</p></li><li><p>分块请求</p></li><li><p>支持 .netrc</p></li></ul> 
 <p>requests 完全满足今日 web 的需求。Requests 支持 Python 2.6—2.7以及3.3—3.7，而且能在 PyPy 下完美运行</p> 
 <h3 style="font-weight:bold;font-size:1.4em;border-bottom:1px solid rgb(238,238,238);">三、安装requests</h3> 
 <blockquote style="border-left:4px solid rgb(221,221,221);color:rgb(119,119,119);"> 
  <p>pip install requests</p> 
 </blockquote> 
 <p>如果是pip3则使用</p> 
 <blockquote style="border-left:4px solid rgb(221,221,221);color:rgb(119,119,119);"> 
  <p>pip3 install requests</p> 
 </blockquote> 
 <p>如果你使用anaconda则可以</p> 
 <blockquote style="border-left:4px solid rgb(221,221,221);color:rgb(119,119,119);"> 
  <p>conda install requests</p> 
 </blockquote> 
 <p>如果你不想用命令行，可在pycharm中这样下载库<br><img src="https://images2.imgbox.com/92/82/iZWckR59_o.png" alt="640?wx_fmt=png"></p> 
 <h3 style="font-weight:bold;font-size:1.4em;border-bottom:1px solid rgb(238,238,238);">四、爬虫流程</h3> 
 <p>下图是猪哥之前工作总结的一个<strong>项目开发流程</strong>，算是比较详细，在开发一个大型的项目真的需要这么详细，不然项目上线出故障或者修改需求都无法做<strong>项目复盘</strong>，到时候程序员就有可能背锅祭天。。。</p> 
 <p><img src="https://images2.imgbox.com/dd/d1/J2V0jBNv_o.png" alt="640?wx_fmt=png"></p> 
 <p>言归正传，给大家看项目的开发流程是想引出爬虫爬取数据的流程：</p> 
 <ol style="margin-left:0px;" class="list-paddingleft-2"><li><p>确定需要爬取的网页</p></li><li><p>浏览器检查数据来源（静态网页or动态加载）</p></li><li><p>寻找加载数据url的参数规律（如分页）</p></li><li><p>代码模拟请求爬取数据</p></li></ol> 
 <h3 style="font-weight:bold;font-size:1.4em;border-bottom:1px solid rgb(238,238,238);">五、爬取某东商品页</h3> 
 <p>猪哥就以某东商品页为例子带大家学习爬虫的简单流程，为什么以某东下手而不是某宝？因为某东浏览商品页不需要登录，简单便于大家快速入门！</p> 
 <h4 style="font-weight:bold;font-size:1.3em;">1.第一步：浏览器中找到你想爬取的商品</h4> 
 <p><img src="https://images2.imgbox.com/a9/8d/GBzw6IcW_o.png" alt="640?wx_fmt=png"><br><img src="https://images2.imgbox.com/a9/6f/JmNazhy7_o.png" alt="640?wx_fmt=png"><br><img src="https://images2.imgbox.com/b6/71/mNeBd60Q_o.png" alt="640?wx_fmt=png"><br>ps：猪哥并不是在开车哦，为什么选这款商品？因为后面会爬取这款商品的评价做数据分析，是不是很刺激！</p> 
 <h4 style="font-weight:bold;font-size:1.3em;">2.第二步：浏览器检查数据来源</h4> 
 <p>打开浏览器调试窗口是为了查看网络请求，看看数据是怎么加载的？是直接返回静态页面呢，还是js动态加载呢？<br><img src="https://images2.imgbox.com/86/a5/wCCFstDz_o.png" alt="640?wx_fmt=png"><br><strong>鼠标右键然后点检查或者直接F12</strong>即可打开调试窗口，这里猪哥推荐大家使用Chrome浏览器，为什么？因为好用，程序员都在用！具体的Chrome如何调试，大家自行网上看教程！</p> 
 <p>打开调试窗口之后，我们就可以重新请求数据，然后查看返回的数据，确定数据来源。<br><img src="https://images2.imgbox.com/09/2a/bFD624yu_o.png" alt="640?wx_fmt=png"></p> 
 <h4 style="font-weight:bold;font-size:1.3em;">3.第三步：寻找加载数据url的参数规律</h4> 
 <p>我们可以看到第一个请求链接：https://item.jd.com/1263013576.html 返回的数据便是我们要的网页数据。因为我们是爬取商品页，所以不存在分页之说。<br><img src="https://images2.imgbox.com/f8/1e/vlZE9rfI_o.png" alt="640?wx_fmt=png"></p> 
 <p>当然价格和一些优惠券等核心信息是通过另外的请求加载，这里我们暂时不讨论，先完成我们的第一个小例子！</p> 
 <h4 style="font-weight:bold;font-size:1.3em;">4.第四步：代码模拟请求爬取数据</h4> 
 <p>获取url链接之后我们来开始写代码吧</p> 
 <pre class="has"><code class="language-javascript">import requestsdef spider_jd():    """爬取京东商品页"""    url = 'https://item.jd.com/1263013576.html'    try:        r = requests.get(url)        # 有时候请求错误也会有返回数据        # raise_for_status会判断返回状态码，如果4XX或5XX则会抛出异常        r.raise_for_status()        print(r.text[:500])    except:        print('爬取失败')if __name__ == '__main__':    spider_jd()</code></pre> 
 <p>检查返回结果<br></p> 
 <p style="text-align:center;"><img class="rich_pages" src="https://images2.imgbox.com/30/cc/8elHsmij_o.png" alt="640?wx_fmt=png"></p> 
 <p>至此我们就完成了某东商品页的爬取，虽然案例简单，代码很少，但是爬虫的流程基本差不多，希望想学爬虫的同学自己动动手实践一把，选择自己喜欢的商品抓取一下，只有自己动手才能真的学到知识！</p> 
 <h3 style="font-weight:bold;font-size:1.4em;border-bottom:1px solid rgb(238,238,238);">六、requests库介绍</h3> 
 <p>上面我们使用了requests的get方法，我们可以查看源码发现还有其他几个方法：post、put、patch、delete、options、head，他们就是对应HTTP的请求方法。</p> 
 <p style="text-align:center;"><img class="rich_pages" src="https://images2.imgbox.com/0b/ee/n7UmWaCh_o.png" alt="640?wx_fmt=jpeg"></p> 
 <p>这里简单给大家列一下，后面会用大量的案例来<strong>用而后学</strong>，毕竟枯燥的讲解没人愿意看。<br></p> 
 <pre class="has"></pre> 
 <blockquote style="border-left:4px solid rgb(221,221,221);color:rgb(119,119,119);"> 
  <p>注：httpbin.org是一个测试http请求的网站，能正常回应请求</p> 
 </blockquote> 
 <p>对于HTTP的几种请求方法，没做过RestFul API的同学并不是很清楚每个请求方式表达的含义，这里给大家列一下：</p> 
 <ul style="margin-left:0px;" class="list-paddingleft-2"><li><p>GET：</p><p>获取用户列表：</p><p>http://project.company.com/api/v1/users</p> 
   <ul style="list-style-type:square;margin-left:0px;" class="list-paddingleft-2"><li><p>GET：</p><p>获取单个用户：</p><p>http://project.company.com/api/v1/users/{uid}</p></li><li><p>POST：</p><p>创建单个用户：</p><p>http://project.company.com/api/v1/users/{uid}</p></li><li><p>PUT：</p><p>完全替换用户：</p><p>http://project.company.com/api/v1/users/{uid}</p></li><li><p>PATCH：</p><p>局部更新用户：</p><p>http://project.company.com/api/v1/users/{uid}</p></li><li><p>DELETE：</p><p>删除单个用户：</p><p>http://project.company.com/api/v1/users/{uid}</p></li></ul></li></ul> 
 <p>想了解requests更多使用方法请参考：http://cn.python-requests.org</p> 
 <p>后面猪哥也会用大量案例来一点一点学习requests库的一些使用技巧。</p> 
 <h3 style="font-weight:bold;font-size:1.4em;border-bottom:1px solid rgb(238,238,238);">七、总结</h3> 
 <p>今天为大家简单介绍了一下这个非常重要的库：requests，requests可以胜任很多简单的爬虫需求，它强大的功能以及优美的api得到一致的认同。</p> 
 <p>有人多同学会问：爬虫到什么境界才算是入门？<strong>你会熟练使用requests库去实现一些简单的爬虫功能就算入门</strong>，并不是说需要会各种框架才算是入门，相反能使用低级工具实现功能的才更具潜力！</p> 
 <p>如果你有 有趣的爬虫案例或者想法，务必在下方留言，让我看看你们的骚操作～</p> 
 <p style="min-height:1em;font-family:'-apple-system-font', 'system-ui', 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;letter-spacing:.544px;color:rgb(62,62,62);font-size:15px;word-spacing:1px;text-align:center;line-height:1.75em;"><span style="letter-spacing:1px;font-size:14px;color:rgb(136,136,136);">【完】</span></p> 
 <pre class="has"><code class="language-javascript"></code></pre> 
 <ul class="list-paddingleft-2"><li><p style="min-height:auto;font-size:14px;"><a href="https://mp.weixin.qq.com/s?__biz=MzA3MTM3NTA5Ng==&amp;mid=2651061796&amp;idx=1&amp;sn=603aa157caefdd2aa63fc7864430df2f&amp;scene=21#wechat_redirect" rel="nofollow">最担心的事情终于发生了，APP已经可以一键“脱掉”你的衣服了</a></p></li><li><p style="min-height:auto;font-size:14px;"><a href="https://mp.weixin.qq.com/s?__biz=MzA3MTM3NTA5Ng==&amp;mid=2651061687&amp;idx=1&amp;sn=f9e6651bbd2eb876431e135deb862565&amp;scene=21#wechat_redirect" rel="nofollow" style="letter-spacing:.544px;">我造的假我自己打，Adobe推出“反PS”</a><br></p></li><li><p style="min-height:auto;font-size:14px;"><a href="https://mp.weixin.qq.com/s?__biz=MzA3MTM3NTA5Ng==&amp;mid=2651061810&amp;idx=1&amp;sn=3306b457630b5a76a6f7a1804d0804ca&amp;scene=21#wechat_redirect" rel="nofollow">李彦宏被泼水，是“多数人的暴力”还是“群众的宣泄”</a></p></li></ul> 
 <p style="min-height:1em;letter-spacing:.544px;font-family:'微软雅黑';font-size:16px;color:rgb(62,62,62);text-align:center;"><img class="rich_pages" width="677" src="https://images2.imgbox.com/95/cb/boqzhPUf_o.png" alt="640?wx_fmt=png"></p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/25024f1f9997aba4e5edeee8bc9eecaa/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">vue filters过滤器传2个或多个参数</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f8359981a7e0c844bc3970d3f4545119/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">odoo-ORM API</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>