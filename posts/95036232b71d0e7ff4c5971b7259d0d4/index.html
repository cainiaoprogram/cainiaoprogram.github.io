<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>PyTorch里的多分类损失函数 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="PyTorch里的多分类损失函数" />
<meta property="og:description" content="http://blog.leanote.com/post/lincent/PyTorch%E9%87%8C%E7%9A%84%E5%A4%9A%E5%88%86%E7%B1%BB%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-2
最近大热的PyTorch即将推出1.0版，做为各类深度学习任务的最优框架之一，PyTorch提供了丰富的损失函数，而多分类任务用到最多的就是nn.CrossEntropyLoss和nn.NLLLoss了，不妨讨论一下。
nn.CrossEntropyLoss CrossEntropy顾名思义就是交叉熵，概念来自香农的信息论，用于度量两个概率分布间的差异性信息，可以认为是在给定的真实分布下，使用非真实分布的策略消除系统的不确定性所需要付出的努力的大小。交叉熵越小，证明计算出的非真实分布越接近真实分布。
公式如下：
H(p,q)=−∑k=1N(pk∗logqk)H(p,q)=−∑k=1N(pk∗logqk)
在PyTroch的文档中明确指出它和nn.NLLLoss之间的关系，后面我们会进行测试。
This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class.
nn.NLLLoss 全名是负对数似然损失函数（Negative Log Likelihood），在PyTorch的文档中有如下说明：
Obtaining log-probabilities in a neural network is easily achieved by adding a LogSoftmax layer in the last layer of your network. You may use CrossEntropyLoss instead, if you prefer not to add an extra layer.
简单来说，如果最后一层做了log softmax处理，那就直接使用nn.NLLLoss，我们动手试试吧。
以下演示基于0.2.1版
首先我们构造一个网络的输出做为我们的预测值，再构造一个真实分布，一起做为损失函数的输入。
#预测值 inputs_tensor = torch.FloatTensor( [ [10, 0, 3,-1, 1], [-1,-2, 0,-3,-4], [-1, 1, 4, 8, 2] ]) #真实值 targets_tensor = torch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/95036232b71d0e7ff4c5971b7259d0d4/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-09-10T14:25:55+08:00" />
<meta property="article:modified_time" content="2018-09-10T14:25:55+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">PyTorch里的多分类损失函数</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><a href="http://blog.leanote.com/post/lincent/PyTorch%E9%87%8C%E7%9A%84%E5%A4%9A%E5%88%86%E7%B1%BB%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-2" rel="nofollow">http://blog.leanote.com/post/lincent/PyTorch%E9%87%8C%E7%9A%84%E5%A4%9A%E5%88%86%E7%B1%BB%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-2</a></p> 
<p>最近大热的PyTorch即将推出1.0版，做为各类深度学习任务的最优框架之一，PyTorch提供了丰富的损失函数，而多分类任务用到最多的就是<strong>nn.CrossEntropyLoss</strong>和<strong>nn.NLLLoss</strong>了，不妨讨论一下。</p> 
<h3 id="p"><strong>nn.CrossEntropyLoss</strong></h3> 
<p>CrossEntropy顾名思义就是交叉熵，概念来自香农的信息论，用于度量两个概率分布间的差异性信息，可以认为是在给定的真实分布下，使用非真实分布的策略消除系统的不确定性所需要付出的努力的大小。交叉熵越小，证明计算出的非真实分布越接近真实分布。</p> 
<p>公式如下：</p> 
<p> </p> 
<p>H(p,q)=−∑k=1N(pk∗logqk)H(p,q)=−∑k=1N(pk∗logqk)</p> 
<p>在PyTroch的文档中明确指出它和<strong>nn.NLLLoss</strong>之间的关系，后面我们会进行测试。</p> 
<blockquote> 
 <p>This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class.</p> 
</blockquote> 
<h3 id="title"><strong>nn.NLLLoss</strong></h3> 
<p>全名是负对数似然损失函数（Negative Log Likelihood），在PyTorch的文档中有如下说明：</p> 
<blockquote> 
 <p>Obtaining log-probabilities in a neural network is easily achieved by adding a LogSoftmax layer in the last layer of your network. You may use CrossEntropyLoss instead, if you prefer not to add an extra layer.</p> 
</blockquote> 
<p>简单来说，如果最后一层做了log softmax处理，那就直接使用<strong>nn.NLLLoss</strong>，我们动手试试吧。</p> 
<p>以下演示基于0.2.1版</p> 
<ol><li> <p>首先我们构造一个网络的输出做为我们的预测值，再构造一个真实分布，一起做为损失函数的输入。</p> <pre class="has"><code>
#预测值

inputs_tensor = torch.FloatTensor( [
[10, 0, 3,-1, 1],
[-1,-2, 0,-3,-4],
[-1, 1, 4, 8, 2]
])

#真实值

targets_tensor = torch.LongTensor([1,2,3])</code></pre> </li><li> <p>计算预测值的log softmax，这里有先计算softmax再取log和直接调用log_softmax函数两种方式，结果相同。</p> <pre class="has"><code>
#分步计算log softmax

softmax_result = F.softmax(inputs_variable)
logsoftmax_result = np.log(softmax_result.data)

#logsoftmax_result:


#-0.0011 -10.0011  -7.0011 -11.0011  -9.0011


#-1.4519  -2.4519  -0.4519  -3.4519  -4.4519


#-9.0216  -7.0216  -4.0216  -0.0216  -6.0216


#[torch.FloatTensor of size 3x5]



#直接调用log_softmax

log_softmax_result = F.log_softmax(inputs_variable)

#log_softmax_result:


#-0.0011 -10.0011  -7.0011 -11.0011  -9.0011


#-1.4519  -2.4519  -0.4519  -3.4519  -4.4519


#-9.0216  -7.0216  -4.0216  -0.0216  -6.0216


#[torch.FloatTensor of size 3x5]</code></pre> </li><li> <p>计算每个样本的交叉熵再求平均，得到的结果和直接调用nn.CrossEntropyLoss函数相同。</p> <pre class="has"><code>loss = nn.CrossEntropyLoss()
output = loss(inputs_variable, targets_variable)

# 3.4915


# [torch.FloatTensor of size 1]</code></pre> </li><li> <p>将前面求出的log softmax结果直接做为nn.NLLLoss的输入，得到的结果相同。</p> <pre class="has"><code>loss = nn.NLLLoss()
output = loss(log_softmax_result, targets_variable)

# 3.4915


# [torch.FloatTensor of size 1]</code></pre> </li></ol>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f7ecf7e84a65d540b37aa0351c063d61/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">别跟我谈EF抵抗并发，敢问你到底会不会用EntityFramework</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/705b49dc4a0e7b0b8319853fb5d8a31c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">汉字的ASCII码对照表</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>