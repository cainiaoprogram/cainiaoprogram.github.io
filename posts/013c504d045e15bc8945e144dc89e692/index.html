<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>信息、信息量、信息熵、互信息、基尼系数、信息增益、KL散度 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="信息、信息量、信息熵、互信息、基尼系数、信息增益、KL散度" />
<meta property="og:description" content="能消除不确定性的内容才能叫信息
信息量的公式：
熵是平均意义上对随机变量的编码长度，即信息量的期望：
例如抛硬币和掷骰子的熵：
在计算机中表示抛硬币结果需要1 bit，表示掷骰子结果需要log6 bit（实际向上取整3 bit）
必然事件的熵是0，必然事件没有不确定性，不含有信息量；信息越有用信息熵越大，不可为负数。同时也代表信息的压缩大小，一段话里面有重复的，把重复的去掉就等于压缩，压缩的极限就是信息熵。
信息熵是在信息的基础上，将有可能产生的信息定义为一个随机变量，
那么变量的期望就是信息熵，比如有两个取值，两个都有自己的信息，
分别乘以概率再求和，就得到了事情的信息熵：
信息增益是决策树中 ID3 算法用来特征选择的方法，
用整体的信息熵减掉按某一特征分裂后的条件熵，结果越大说明此特征越能消除不确定性，
最极端的情况，按这个特征分裂后信息增益与信息熵一模一样，则该特征就能获得唯一结果。
条件熵：
信息增益率是在信息增益的基础上，增加一个选取的特征包含的类别的惩罚项，
主要是考虑到信息增益会导致包含类别越多的特征的信息增益越大，
极端一点有多少个样本，这个特征就有多少个类别，那么就会导致决策树非常浅
基尼系数也是衡量信息不确定性的方法，与信息熵计算的结果差距很小，基本可以忽略，
但是基尼系数要计算快得多，因为没有对数；
与信息熵一样，当类别概率趋于平均时，基尼系数越大
互信息：
如抛硬币100次，90次为正，10词为负，其信息熵为
而正常抛硬币的信息熵应该是1，这里的差值为0.531，就是互信息；推导互信息的公式：
H(X)表示为原随机变量的信息量， H(X|Y)为知道事实 Y后的信息量，
互信息 I(X;Y)则表示为知道事实 Y 后，原来信息量减少了多少。 如果随机变量 X, Y独立，则互信息是0，即知道事实Y 并没有减少 X的信息量，独立即互不影响。
KL散度：
P(x)为正式样本分布，Q(x)代表预测样本分布。
KL散度越小，表示与P(x)更加接近，可以通过反复训练Q(x)来使Q(x)的分布逼近P(x)。
三分类任务，X1，X2，X3 分别代表猫，狗，马图片，一张猫的图片真实分布P(X)=[1,0,0]，预测分布Q(X)=[0.7,0.2,0.1]，计算KL散度：
参考：
决策树①——信息熵&amp;信息增益&amp;基尼系数_cindy407的博客-CSDN博客_决策树信息熵
https://zhuanlan.zhihu.com/p/36192699
【Deep Learning】Softmax和交叉熵损失函数_pangpd的博客-CSDN博客_softmax 交叉熵" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/013c504d045e15bc8945e144dc89e692/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-11-19T16:54:27+08:00" />
<meta property="article:modified_time" content="2023-11-19T16:54:27+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">信息、信息量、信息熵、互信息、基尼系数、信息增益、KL散度</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><strong>能消除不确定性的内容才能叫信息</strong></p> 
<p><strong>信息量的公式：</strong><img alt="" height="39" src="https://images2.imgbox.com/3b/1f/VhpV21jJ_o.png" width="116"></p> 
<p><strong>熵是平均意义上对随机变量的编码长度，即信息量的期望：</strong></p> 
<p><strong>        例如抛硬币和掷骰子的熵：</strong></p> 
<p><strong>        </strong><img alt="" height="30" src="https://images2.imgbox.com/1f/ec/JcQ1AJeB_o.png" width="235"><img alt="" height="30" src="https://images2.imgbox.com/7d/6f/xhL1FWha_o.png" width="215"></p> 
<p>         在计算机中表示抛硬币结果需要1 bit，表示掷骰子结果需要log6 bit（实际向上取整3 bit）</p> 
<p>        <img alt="" height="88" src="https://images2.imgbox.com/b0/b8/f1AR4aI3_o.png" width="414"></p> 
<p>必然事件的熵是0，必然事件没有不确定性，不含有信息量；信息越有用信息熵越大，不可为负数。同时也代表信息的压缩大小，一段话里面有重复的，把重复的去掉就等于压缩，压缩的极限就是信息熵。</p> 
<p>信息熵是在信息的基础上，将有可能产生的信息定义为一个随机变量，</p> 
<p>那么变量的期望就是信息熵，比如有两个取值，两个都有自己的信息，</p> 
<p>分别乘以概率再求和，就得到了事情的<strong>信息熵</strong>：</p> 
<p><img alt="" height="74" src="https://images2.imgbox.com/6f/52/Zc0Dryfg_o.png" width="218"></p> 
<p>信息增益是决策树中 ID3 算法用来特征选择的方法，</p> 
<p>用整体的信息熵减掉按某一特征分裂后的条件熵，结果越大说明此特征越能消除不确定性，</p> 
<p>最极端的情况，按这个特征分裂后信息增益与信息熵一模一样，则该特征就能获得唯一结果。</p> 
<p>条件熵：</p> 
<p><img alt="" height="108" src="https://images2.imgbox.com/bd/8f/jqeCbJpR_o.png" width="561"></p> 
<p><img alt="" height="103" src="https://images2.imgbox.com/6b/42/1MW2Dnr0_o.png" width="398"></p> 
<p>信息增益率是在信息增益的基础上，增加一个选取的特征包含的类别的惩罚项，</p> 
<p>主要是考虑到信息增益会导致包含类别越多的特征的信息增益越大，</p> 
<p>极端一点有多少个样本，这个特征就有多少个类别，那么就会导致决策树非常浅</p> 
<p><img alt="" height="98" src="https://images2.imgbox.com/de/43/nuNshj4J_o.png" width="254"><img alt="" height="95" src="https://images2.imgbox.com/79/42/F6zspbZ6_o.png" width="460"></p> 
<p>基尼系数也是衡量信息不确定性的方法，与信息熵计算的结果差距很小，基本可以忽略，</p> 
<p>但是基尼系数要计算快得多，因为没有对数；</p> 
<p>与信息熵一样，当类别概率趋于平均时，基尼系数越大</p> 
<p><img alt="" height="90" src="https://images2.imgbox.com/1a/06/TKv4GlKw_o.png" width="258"><img alt="" height="74" src="https://images2.imgbox.com/bd/32/klLc3ApZ_o.png" width="258"></p> 
<p>互信息：</p> 
<p>        <img alt="" height="80" src="https://images2.imgbox.com/05/f1/tPA55RXm_o.png" width="443"></p> 
<p>如抛硬币100次，90次为正，10词为负，其信息熵为<img alt="" height="37" src="https://images2.imgbox.com/48/bb/nooupjPA_o.png" width="394"></p> 
<p>而正常抛硬币的信息熵应该是1，这里的差值为0.531，就是互信息；推导互信息的公式：</p> 
<p><img alt="" height="146" src="https://images2.imgbox.com/27/ec/eELExmvZ_o.png" width="506"></p> 
<p>H(X)表示为原随机变量的信息量， H(X|Y)为知道事实 Y后的信息量，</p> 
<p><strong>互信息 I(X;Y)则表示为知道事实 Y 后，原来信息量减少了多少。</strong> </p> 
<p>如果随机变量 X, Y独立，则互信息是0，即知道事实Y 并没有减少 X的信息量，独立即互不影响。</p> 
<p>KL散度：</p> 
<p><img alt="" height="71" src="https://images2.imgbox.com/4f/19/hgbHw9EK_o.png" width="326">P(x)为正式样本分布，Q(x)代表预测样本分布。</p> 
<p>KL散度越小，表示与P(x)更加接近，可以通过反复训练Q(x)来使Q(x)的分布逼近P(x)。</p> 
<p>三分类任务，X1，X2，X3 分别代表猫，狗，马图片，一张猫的图片真实分布P(X)=[1,0,0]，预测分布Q(X)=[0.7,0.2,0.1]，计算KL散度：</p> 
<p style="text-align:center;"><img alt="" height="127" src="https://images2.imgbox.com/1e/e5/LXHz6yES_o.png" width="431"></p> 
<p></p> 
<p></p> 
<p>参考：</p> 
<p><a href="https://blog.csdn.net/cindy407/article/details/92850901" title="决策树①——信息熵&amp;信息增益&amp;基尼系数_cindy407的博客-CSDN博客_决策树信息熵">决策树①——信息熵&amp;信息增益&amp;基尼系数_cindy407的博客-CSDN博客_决策树信息熵</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/3619269" rel="nofollow" title="https://zhuanlan.zhihu.com/p/3619269">https://zhuanlan.zhihu.com/p/3619269</a>9</p> 
<p><a href="https://blog.csdn.net/weixin_43519707/article/details/104330081" title="【Deep Learning】Softmax和交叉熵损失函数_pangpd的博客-CSDN博客_softmax 交叉熵">【Deep Learning】Softmax和交叉熵损失函数_pangpd的博客-CSDN博客_softmax 交叉熵</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/9d54d027a530182da71209f646a5f921/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【GCN】GCN学习笔记一</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/5cd01f9c745aed688b105b74894e24e4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【c&#43;&#43;】左值和右值</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>