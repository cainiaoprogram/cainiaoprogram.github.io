<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>轻量级网络——ShuffleNetV2 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="轻量级网络——ShuffleNetV2" />
<meta property="og:description" content="文章目录 1.ShuffleNetV2的介绍2.ShuffleNetV2的四条实用指南G1) Equal channel width minimizes memory access cost (MAC)G2) Excessive group convolution increases MACG3) Network fragmentation reduces degree of parallelismG4) Element-wise operations are non-negligible 3.ShuffleNetV2的结构Channel Split and ShuffleNet V2Analysis of Network Accuracy 4.ShuffleNetV2的性能统计5.ShuffleNetV2的pytorch实现 回顾： 轻量级网络——ShuffleNetV1 paper链接：ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design
关键内容：
ShuffleNetV2中提出了一个关键点，之前的轻量级网络都是通过计算网络复杂度的一个间接度量，即FLOPs为指导。通过计算浮点运算量来描述轻量级网络的快慢。但是从来不直接考虑运行的速度。在移动设备中的运行速度不仅仅需要考虑FLOPs，还需要考虑其他的因素，比如内存访问成本(memory access cost)和平台特点(platform characterics)。
所以，ShuffleNetV2直接通过控制不同的环境来直接测试网络在设备上运行速度的快慢，而不是通过FLOPs来判断。
1.ShuffleNetV2的介绍 在之间轻量级网络的发展中，为了度量计算复杂度，一个广泛使用的度量标准是浮点运算的数量(FLOPs)。然而，FLOPs是一个间接的指标。这值是一个近似，但通常不等同于我们真正关心的直接指标，比如速度或延迟。
如上图，具有相似FLOPs的网络却具有不同的网速。因此，使用FLOPs作为计算复杂度的唯一指标是不够的，可能会导致次优化设计。
间接(FLOPs)和直接(速度)指标之间的差异可以归结为两个主要原因。首先，FLOPs没有考虑到几个对速度有相当大影响的重要因素。其中一个因素就是内存访问成本(MAC)。在某些操作(如组卷积)中，这种代价构成了运行时的很大一部分。它可能会成为具有强大计算能力的设备的瓶颈，例如gpu。在设计网络体系结构时，不应该简单地忽略这个代价。另一个是并行度。在相同的FLOPs下，具有高并行度的模型可能比另一个具有低并行度的模型快得多。
其次，由于平台的不同是，使用相同的FLOPs操作可能有不同的运行时间。我们不能肯定地认为3 × 3 conv比1 × 1 conv慢9倍。因为最新的CUDNN库专门针对3 × 3 conv进行了优化。
因为，ShuffleNetv2提出了设计应该考虑两个原则：
应该使用直接度量(如速度)而不是间接度量(如FLOPs。这些指标应该在目标平台上进行评估。（也就是实事求是） 然后，ShuffleNetv2根据这两个原则，提出了四种有效的网络设计原则：
G1: Equal channel width minimizes memory access cost (MAC)G2: Excessive group convolution increases MACG3: Network fragmentation reduces degree of parallelismG4: Element-wise operations are non-negligible 在计算复杂度为40M FLOPs的情况下，ShuffleNet v2的精度分别比ShuffleNet v1和MobileNet v2高3." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/7177567b99a188ff5aa9f2df6e3812eb/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-06-24T18:19:47+08:00" />
<meta property="article:modified_time" content="2021-06-24T18:19:47+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">轻量级网络——ShuffleNetV2</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night-eighties">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#1ShuffleNetV2_11" rel="nofollow">1.ShuffleNetV2的介绍</a></li><li><a href="#2ShuffleNetV2_32" rel="nofollow">2.ShuffleNetV2的四条实用指南</a></li><li><ul><li><a href="#G1_Equal_channel_width_minimizes_memory_access_cost_MAC_35" rel="nofollow">G1) Equal channel width minimizes memory access cost (MAC)</a></li><li><a href="#G2_Excessive_group_convolution_increases_MAC_58" rel="nofollow">G2) Excessive group convolution increases MAC</a></li><li><a href="#G3_Network_fragmentation_reduces_degree_of_parallelism_73" rel="nofollow">G3) Network fragmentation reduces degree of parallelism</a></li><li><a href="#G4_Elementwise_operations_are_nonnegligible_86" rel="nofollow">G4) Element-wise operations are non-negligible</a></li></ul> 
  </li><li><a href="#3ShuffleNetV2_106" rel="nofollow">3.ShuffleNetV2的结构</a></li><li><ul><li><a href="#Channel_Split_and_ShuffleNet_V2_107" rel="nofollow">Channel Split and ShuffleNet V2</a></li><li><a href="#Analysis_of_Network_Accuracy_120" rel="nofollow">Analysis of Network Accuracy</a></li></ul> 
  </li><li><a href="#4ShuffleNetV2_126" rel="nofollow">4.ShuffleNetV2的性能统计</a></li><li><a href="#5ShuffleNetV2pytorch_131" rel="nofollow">5.ShuffleNetV2的pytorch实现</a></li></ul> 
</div> 
<br> 
<strong>回顾：</strong> 
<a href="https://blog.csdn.net/weixin_44751294/article/details/116935677"><strong>轻量级网络——ShuffleNetV1</strong></a> 
<p></p> 
<p>paper链接：<a href="https://arxiv.org/pdf/1807.11164.pdf" rel="nofollow">ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</a></p> 
<p><strong>关键内容：</strong></p> 
<p>ShuffleNetV2中提出了一个关键点，之前的轻量级网络都是通过计算网络复杂度的一个间接度量，即FLOPs为指导。通过计算浮点运算量来描述轻量级网络的快慢。但是从来不直接考虑运行的速度。在移动设备中的运行速度不仅仅需要考虑FLOPs，还需要考虑其他的因素，比如<strong>内存访问成本</strong>(memory access cost)和<strong>平台特点</strong>(platform characterics)。</p> 
<p>所以，ShuffleNetV2直接通过控制不同的环境来直接测试网络在设备上运行速度的快慢，而不是通过FLOPs来判断。</p> 
<h2><a id="1ShuffleNetV2_11"></a>1.ShuffleNetV2的介绍</h2> 
<p>在之间轻量级网络的发展中，为了度量计算复杂度，一个广泛使用的度量标准是浮点运算的数量(FLOPs)。然而，FLOPs是一个间接的指标。这值是一个近似，但通常不等同于我们真正关心的直接指标，比如速度或延迟。<br> <img src="https://images2.imgbox.com/f2/0d/z2hB2xuK_o.png" alt="在这里插入图片描述"><br> 如上图，具有相似FLOPs的网络却具有不同的网速。因此，使用FLOPs作为计算复杂度的唯一指标是不够的，可能会导致次优化设计。</p> 
<p>间接(FLOPs)和直接(速度)指标之间的差异可以归结为两个主要原因。首先，FLOPs没有考虑到几个对速度有相当大影响的重要因素。<strong>其中一个因素就是内存访问成本</strong>(MAC)。在某些操作(如组卷积)中，这种代价构成了运行时的很大一部分。它可能会成为具有强大计算能力的设备的瓶颈，例如gpu。在设计网络体系结构时，不应该简单地忽略这个代价。<strong>另一个是并行度</strong>。在相同的FLOPs下，具有高并行度的模型可能比另一个具有低并行度的模型快得多。</p> 
<p>其次，由于平台的不同是，使用相同的FLOPs操作可能有不同的运行时间。我们不能肯定地认为3 × 3 conv比1 × 1 conv慢9倍。因为最新的CUDNN库专门针对3 × 3 conv进行了优化。</p> 
<p>因为，ShuffleNetv2提出了设计应该考虑两个原则：</p> 
<ul><li>应该使用直接度量(如速度)而不是间接度量(如FLOPs。</li><li>这些指标应该在目标平台上进行评估。（也就是实事求是）</li></ul> 
<p>然后，ShuffleNetv2根据这两个原则，提出了四种有效的网络设计原则：</p> 
<ul><li>G1: Equal channel width minimizes memory access cost (MAC)</li><li>G2: Excessive group convolution increases MAC</li><li>G3: Network fragmentation reduces degree of parallelism</li><li>G4: Element-wise operations are non-negligible</li></ul> 
<p>在计算复杂度为40M FLOPs的情况下，ShuffleNet v2的精度分别比ShuffleNet v1和MobileNet v2高3.5%和3.7%</p> 
<h2><a id="2ShuffleNetV2_32"></a>2.ShuffleNetV2的四条实用指南</h2> 
<p>整个运行时被分解为不同的操作，如图2所示。我们注意到FLOPs度量只考虑了卷积部分。虽然这一部分消耗的时间最多，但其他操作包括data I/O、data shuffle和element-wise operations(AddTensor、ReLU等)也占用了相当多的时间。因此，FLOPs并不是对实际运行时的足够准确的估计。<br> <img src="https://images2.imgbox.com/07/5c/HnmGN6Lp_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="G1_Equal_channel_width_minimizes_memory_access_cost_MAC_35"></a>G1) Equal channel width minimizes memory access cost (MAC)</h3> 
<p><strong>当卷积层的输入特征矩阵与输出特征矩阵channel相等时MAC最小(保持FLOPs不变时)</strong></p> 
<p>现代网络通常采用depthwise separable convolutions ，其中pointwise convolution(即1 × 1卷积)占了复杂性的大部分。研究了1 × 1卷积的核形。形状由两个参数指定:输入通道c1和输出通道c2的数量。设h和w为feature map的空间大小，1 × 1卷积的FLOPs为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         B 
        
       
         = 
        
       
         h 
        
       
         ∗ 
        
       
         w 
        
       
         ∗ 
        
        
        
          c 
         
        
          1 
         
        
       
         ∗ 
        
        
        
          c 
         
        
          2 
         
        
       
      
        B = h*w*c_{1}*c_{2} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.05017em;">B</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathdefault">h</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.46528em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.61528em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.58056em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>.内存访问成本(MAC)，即内存访问操作数，为<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          M 
         
        
          A 
         
        
          C 
         
        
          = 
         
        
          h 
         
        
          w 
         
        
          ( 
         
         
         
           c 
          
         
           1 
          
         
        
          + 
         
         
         
           c 
          
         
           2 
          
         
        
          ) 
         
        
          + 
         
         
         
           c 
          
         
           1 
          
         
        
          ∗ 
         
         
         
           c 
          
         
           2 
          
         
        
       
         MAC = hw(c_{1}+c_{2})+c_{1}*c_{2} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.10903em;">M</span><span class="mord mathdefault">A</span><span class="mord mathdefault" style="margin-right: 0.07153em;">C</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault">h</span><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.61528em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.58056em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span></span></p> 
<p>这两个术语分别对应于输入/输出特性映射的内存访问和内核权重。其实这条公式可以看成由三个部分组成：第一部分是<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         h 
        
       
         w 
        
        
        
          c 
         
        
          1 
         
        
       
      
        hwc_{1} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord mathdefault">h</span><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>，对应的是输入特征矩阵的内存消耗；第二部分是<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         h 
        
       
         w 
        
        
        
          c 
         
        
          2 
         
        
       
      
        hwc_{2} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord mathdefault">h</span><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>，对应的是输出特征矩阵的内存消耗。第三部分是<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          c 
         
        
          1 
         
        
       
         ∗ 
        
        
        
          c 
         
        
          2 
         
        
       
      
        c_{1}*c_{2} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.61528em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.58056em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>,对应的是卷积核的内存消耗。</p> 
<p>由均值不等式<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
         
          
          
            c 
           
          
            1 
           
          
         
           + 
          
          
          
            c 
           
          
            2 
           
          
         
        
          2 
         
        
       
         ≥ 
        
        
         
          
          
            c 
           
          
            1 
           
          
          
          
            c 
           
          
            2 
           
          
         
        
       
      
        \frac{c_{1}+c_{2}}{2} ≥ \sqrt{c_{1}c_{2}} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.16343em; vertical-align: -0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.818431em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.4101em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.317314em;"><span class="" style="top: -2.357em; margin-left: 0em; margin-right: 0.0714286em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em;"><span class=""></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathdefault mtight">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.317314em;"><span class="" style="top: -2.357em; margin-left: 0em; margin-right: 0.0714286em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em;"><span class=""></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.04em; vertical-align: -0.31472em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.72528em;"><span class="svg-align" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord" style="padding-left: 0.833em;"><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -2.68528em;"><span class="pstrut" style="height: 3em;"></span><span class="hide-tail" style="min-width: 0.853em; height: 1.08em;"> 
           <svg width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"> 
            <path d="M95,702c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,
-10,-9.5,-14c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54c44.2,-33.3,65.8,
-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10s173,378,173,378c0.7,0,
35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429c69,-144,104.5,-217.7,106.5,
-221c5.3,-9.3,12,-14,20,-14H400000v40H845.2724s-225.272,467,-225.272,467
s-235,486,-235,486c-2.7,4.7,-9,7,-19,7c-6,0,-10,-1,-12,-3s-194,-422,-194,-422
s-65,47,-65,47z M834 80H400000v40H845z"></path> 
           </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.31472em;"><span class=""></span></span></span></span></span></span></span></span></span>得出：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
          
           
            
            
              M 
             
            
              A 
             
            
              C 
             
            
           
          
          
           
            
             
            
              ≥ 
             
            
              2 
             
            
              h 
             
            
              w 
             
             
              
               
               
                 c 
                
               
                 1 
                
               
               
               
                 c 
                
               
                 2 
                
               
              
             
            
              + 
             
             
             
               c 
              
             
               1 
              
             
             
             
               c 
              
             
               2 
              
             
            
           
          
         
         
          
           
            
           
          
          
           
            
             
            
              ≥ 
             
            
              2 
             
             
              
              
                h 
               
              
                w 
               
              
                B 
               
              
             
            
              + 
             
             
             
               B 
              
              
              
                h 
               
              
                w 
               
              
             
            
           
          
         
        
       
         \begin{aligned} MAC &amp;≥ 2hw\sqrt{c_{1}c_{2}}+c_{1}c_{2} \\ &amp;≥2\sqrt{hwB} + \frac{B}{hw} \end{aligned} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 3.84633em; vertical-align: -1.67317em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.17317em;"><span class="" style="top: -4.6935em;"><span class="pstrut" style="height: 3.36033em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.10903em;">M</span><span class="mord mathdefault">A</span><span class="mord mathdefault" style="margin-right: 0.07153em;">C</span></span></span><span class="" style="top: -2.67317em;"><span class="pstrut" style="height: 3.36033em;"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.67317em;"><span class=""></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.17317em;"><span class="" style="top: -4.6935em;"><span class="pstrut" style="height: 3.36033em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mord">2</span><span class="mord mathdefault">h</span><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.774155em;"><span class="svg-align" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord" style="padding-left: 0.833em;"><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -2.73415em;"><span class="pstrut" style="height: 3em;"></span><span class="hide-tail" style="min-width: 0.853em; height: 1.08em;"> 
                    <svg width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"> 
                     <path d="M95,702c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,
-10,-9.5,-14c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54c44.2,-33.3,65.8,
-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10s173,378,173,378c0.7,0,
35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429c69,-144,104.5,-217.7,106.5,
-221c5.3,-9.3,12,-14,20,-14H400000v40H845.2724s-225.272,467,-225.272,467
s-235,486,-235,486c-2.7,4.7,-9,7,-19,7c-6,0,-10,-1,-12,-3s-194,-422,-194,-422
s-65,47,-65,47z M834 80H400000v40H845z"></path> 
                    </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.265845em;"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -2.67317em;"><span class="pstrut" style="height: 3.36033em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mord">2</span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.981095em;"><span class="svg-align" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord" style="padding-left: 0.833em;"><span class="mord mathdefault">h</span><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="mord mathdefault" style="margin-right: 0.05017em;">B</span></span></span><span class="" style="top: -2.94109em;"><span class="pstrut" style="height: 3em;"></span><span class="hide-tail" style="min-width: 0.853em; height: 1.08em;"> 
                    <svg width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"> 
                     <path d="M95,702c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,
-10,-9.5,-14c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54c44.2,-33.3,65.8,
-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10s173,378,173,378c0.7,0,
35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429c69,-144,104.5,-217.7,106.5,
-221c5.3,-9.3,12,-14,20,-14H400000v40H845.2724s-225.272,467,-225.272,467
s-235,486,-235,486c-2.7,4.7,-9,7,-19,7c-6,0,-10,-1,-12,-3s-194,-422,-194,-422
s-65,47,-65,47z M834 80H400000v40H845z"></path> 
                    </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.058905em;"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.36033em;"><span class="" style="top: -2.314em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.05017em;">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.686em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.67317em;"><span class=""></span></span></span></span></span></span></span></span></span></span></span></span></p> 
<p>其中：<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         B 
        
       
         = 
        
       
         h 
        
       
         w 
        
        
        
          c 
         
        
          1 
         
        
        
        
          c 
         
        
          2 
         
        
       
      
        B = hwc_{1}c_{2} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.05017em;">B</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord mathdefault">h</span><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>因此，MAC有一个由FLOPs给出的下限。当输入和输出通道的数量相等时，它达到下界。</p> 
<p>为了验证上述结论，进行了如下实验。一个基准网络由10个构件重复堆叠而成。每个块包含两个卷积层。第一个卷积层由c1输入通道和c2输出通道组成，第二个则相反，输入通道是c2输出通道是c1。<br> <img src="https://images2.imgbox.com/10/e1/GVVmxssz_o.png" alt="在这里插入图片描述"><br> 表1通过改变比率c1: c2报告了在固定总FLOPs时的运行速度。可见，当c1: c2接近1:1时，MAC变小，网络评估速度加快。</p> 
<h3><a id="G2_Excessive_group_convolution_increases_MAC_58"></a>G2) Excessive group convolution increases MAC</h3> 
<p><strong>当GConv的groups增大时(保持FLOPs不变时)，MAC也会增大</strong></p> 
<p>组卷积是现代网络体系结构的核心。它通过将所有通道之间的密集卷积改变为稀疏卷积(仅在通道组内)来降低计算复杂度(FLOPs)。一方面，它允许在一个固定的FLOPs下使用更多的channels，并增加网络容量(从而提高准确性)。然而，另一方面，增加的通道数量导致更多的MAC。</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
          
           
            
            
              M 
             
            
              A 
             
            
              C 
             
            
           
          
          
           
            
             
            
              = 
             
            
              h 
             
            
              w 
             
            
              ( 
             
             
             
               c 
              
             
               1 
              
             
            
              + 
             
             
             
               c 
              
             
               2 
              
             
            
              ) 
             
            
              + 
             
             
              
               
               
                 c 
                
               
                 1 
                
               
               
               
                 c 
                
               
                 2 
                
               
              
             
               g 
              
             
            
           
          
         
         
          
           
            
           
          
          
           
            
             
            
              = 
             
            
              h 
             
            
              w 
             
             
             
               c 
              
             
               1 
              
             
            
              + 
             
             
              
              
                B 
               
              
                g 
               
              
              
              
                c 
               
              
                1 
               
              
             
            
              + 
             
             
             
               B 
              
              
              
                h 
               
              
                w 
               
              
             
            
           
          
         
        
       
         \begin{aligned} MAC &amp;= hw(c_{1}+c_{2}) + \frac{c_{1}c_{2}}{g} \\ &amp;= hwc_{1} + \frac{Bg}{c_{1}} + \frac{B}{hw} \end{aligned} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 4.78433em; vertical-align: -2.14217em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.64217em;"><span class="" style="top: -4.89494em;"><span class="pstrut" style="height: 3.36033em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.10903em;">M</span><span class="mord mathdefault">A</span><span class="mord mathdefault" style="margin-right: 0.07153em;">C</span></span></span><span class="" style="top: -2.35417em;"><span class="pstrut" style="height: 3.36033em;"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 2.14217em;"><span class=""></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.64217em;"><span class="" style="top: -4.89494em;"><span class="pstrut" style="height: 3.36033em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mord mathdefault">h</span><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.10756em;"><span class="" style="top: -2.314em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">g</span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.88044em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span class="" style="top: -2.35417em;"><span class="pstrut" style="height: 3.36033em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mord mathdefault">h</span><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.36033em;"><span class="" style="top: -2.314em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.05017em;">B</span><span class="mord mathdefault" style="margin-right: 0.03588em;">g</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.836em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.36033em;"><span class="" style="top: -2.314em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.05017em;">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.686em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 2.14217em;"><span class=""></span></span></span></span></span></span></span></span></span></span></span></span><br> 其中g为分组数，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         B 
        
       
         = 
        
       
         h 
        
       
         w 
        
        
        
          c 
         
        
          1 
         
        
        
        
          c 
         
        
          2 
         
        
       
         / 
        
       
         g 
        
       
      
        B = hwc_{1}c_{2}/g 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.05017em;">B</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault">h</span><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord">/</span><span class="mord mathdefault" style="margin-right: 0.03588em;">g</span></span></span></span></span>为FLOPs。不难看出，给定固定的输入形状c1× h × w，计算代价B, MAC随着g的增长而增加<br> <img src="https://images2.imgbox.com/e6/1f/2TWIGWvq_o.png" alt="在这里插入图片描述"><br> 很明显，使用大量的组数会显著降低运行速度。例如，在GPU上使用8group比使用1group(标准密集卷积)慢两倍以上，在ARM上慢30%。这主要是由于MAC的增加。所以使用比较大组去进行组卷积是不明智的。对速度会造成比较大的影响。</p> 
<h3><a id="G3_Network_fragmentation_reduces_degree_of_parallelism_73"></a>G3) Network fragmentation reduces degree of parallelism</h3> 
<p><strong>网络设计的碎片化程度越高，速度越慢</strong></p> 
<p>虽然这种碎片化结构已经被证明有利于提高准确性，但它可能会降低效率，因为它对GPU等具有强大并行计算能力的设备不友好。它还引入了额外的开销，比如内核启动和同步。</p> 
<p><img src="https://images2.imgbox.com/b8/eb/G0gb0oYg_o.png" alt="在这里插入图片描述"><br> 为了量化网络分片如何影响效率，我们评估了一系列不同分片程度的网络块。具体来说,每个构造块由1到4个1 × 1的卷积组成，这些卷积是按顺序或平行排列的。每个块重复堆叠10次。块结构上图所示。</p> 
<p><img src="https://images2.imgbox.com/a5/b9/MBZsKR9q_o.png" alt="在这里插入图片描述"><br> 表3的结果显示，在GPU上碎片化明显降低了速度，如4-fragment结构比1-fragment慢3倍。在ARM上，速度降低相对较小。</p> 
<p>一个比较容易理解为啥4-fragment结构比较慢的说法是，4-fragment结构需要等待每个分支处理完之后再进行下一步的操作，也就是需要等待最慢的那一个。所以，效率是比较低的。</p> 
<h3><a id="G4_Elementwise_operations_are_nonnegligible_86"></a>G4) Element-wise operations are non-negligible</h3> 
<p><strong>Element-wise操作带来的影响是不可忽视的</strong></p> 
<p><img src="https://images2.imgbox.com/75/63/hjUrol5u_o.png" alt="在这里插入图片描述"><br> 轻量级模型中，元素操作占用了相当多的时间，特别是在GPU上。这里的元素操作符包括ReLU、AddTensor、AddBias等。将depthwise convolution作为一个element-wise operator，因为它的MAC/FLOPs比率也很高<br> <img src="https://images2.imgbox.com/27/9d/V5PWTdn2_o.png" alt="在这里插入图片描述"><br> 可以看见表4中报告了不同变体的运行时间。我们观察到，在移除ReLU和shortcut后，GPU和ARM都获得了大约20%的加速。</p> 
<p>这里主要突出的是，这些操作会比我们想象当中的要耗时。</p> 
<p><strong>总结：</strong></p> 
<p>基于上述准则和实证研究，我们得出结论:一个高效的网络架构应该</p> 
<ul><li>1）使用“平衡”卷积(等信道宽度);</li><li>2）了解使用群卷积的代价;</li><li>3）降低碎片化程度;</li><li>4）减少元素操作。</li></ul> 
<p>这些理想的属性依赖于平台特性(如内存操作和代码优化)，这些特性超出了理论FLOPs。在实际的网络设计中应该考虑到这些因素。而轻量级神经网络体系结构最新进展大多基于FLOPs的度量，没有考虑上述这些特性。</p> 
<h2><a id="3ShuffleNetV2_106"></a>3.ShuffleNetV2的结构</h2> 
<h3><a id="Channel_Split_and_ShuffleNet_V2_107"></a>Channel Split and ShuffleNet V2</h3> 
<p>回顾ShuffleNetV1的结构，其主要采用了两种技术：pointwise group convolutions与bottleneck-like structures。然后引入“channel shuffle”操作，以实现不同信道组之间的信息通信，提高准确性。</p> 
<p>both pointwise group convolutions与bottleneck structures均增加了MAC，与G1和G2不符合。这一成本是不可忽视的，特别是对于轻型机型。此外，使用太多group违反G3。shortcut connection中的元素element-wise add操作也是不可取的，违反了G4。因此，要实现高模型容量和高效率，关键问题是如何在不密集卷积和不过多分组的情况下，保持大量的、同样宽的信道。</p> 
<p><strong>其中图c对应stride=1的情况，图d对应stride=2的情况</strong><br> <img src="https://images2.imgbox.com/15/a6/ehPagFgX_o.png" alt="在这里插入图片描述"><br> <strong>为此，ShuffleNetV2做出了改进，如图( c )所示，在每个单元的开始，c特征通道的输入被分为两个分支（在ShuffleNetV2中这里是对channels均分成两半）。根据G3，不能使用太多的分支，所以其中一个分支不作改变，另外的一个分支由三个卷积组成，它们具有相同的输入和输出通道以满足G1。两个1 × 1卷积不再是组卷积，而改变为普通的1x1卷积操作，这是为了遵循G2（需要考虑组的代价）。卷积后，两个分支被连接起来，而不是相加(G4)。因此，通道的数量保持不变(G1)。然后使用与ShuffleNetV1中相同的“channels shuffle”操作来启用两个分支之间的信息通信。需要注意，ShuffleNet v1中的“Add”操作不再存在。像ReLU和depthwise convolutions 这样的元素操作只存在于一个分支中。</strong></p> 
<p>对于空间下采样，单元稍作修改，移除通道分离操作符。因此，输出通道的数量增加了一倍。具体结构见图（d）。所提出的构建块( c )( d )以及由此产生的网络称为ShuffleNet V2。基于上述分析，我们得出结论，该体系结构设计是高效的，因为它遵循了所有的指导原则。积木重复堆叠，构建整个网络。</p> 
<p><strong>总体网络结构类似于ShuffleNet v1，如表所示。只有一个区别:在全局平均池之前增加了一个1 × 1的卷积层来混合特性，这在ShuffleNet v1中是没有的</strong>。与下图类似，每个block中的通道数量被缩放，生成不同复杂度的网络，标记为0.5x，1x，1.5x，2x<br> <img src="https://images2.imgbox.com/56/c8/W1Y3TRLL_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="Analysis_of_Network_Accuracy_120"></a>Analysis of Network Accuracy</h3> 
<p>ShuffleNet v2不仅高效，而且准确。主要有两个原因：</p> 
<ul><li>首先，每个构建块的高效率使使用更多的特征通道和更大的网络容量成为可能</li><li>第二，<strong>在每个块中，有一半的特征通道直接穿过该块并加入下一个块。这可以看作是一种特性重用</strong>，就像DenseNet和CondenseNet的思想一样。</li></ul> 
<h2><a id="4ShuffleNetV2_126"></a>4.ShuffleNetV2的性能统计</h2> 
<p><img src="https://images2.imgbox.com/bc/81/jl9DG1oz_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/7a/a2/M5lrYma0_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/89/00/oEU9qW6h_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="5ShuffleNetV2pytorch_131"></a>5.ShuffleNetV2的pytorch实现</h2> 
<p>参考github代码：代码与ShuffleNetV1类似</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torchvision


<span class="token comment"># 3x3DW卷积(含激活函数)</span>
<span class="token keyword">def</span> <span class="token function">Conv3x3BNReLU</span><span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span>out_channels<span class="token punctuation">,</span>stride<span class="token punctuation">,</span>groups<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span>stride<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> groups<span class="token operator">=</span>groups<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU6<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>


<span class="token comment"># 3x3DW卷积(不激活函数)</span>
<span class="token keyword">def</span> <span class="token function">Conv3x3BN</span><span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span>out_channels<span class="token punctuation">,</span>stride<span class="token punctuation">,</span>groups<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span>stride<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> groups<span class="token operator">=</span>groups<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">)</span>
        <span class="token punctuation">)</span>


<span class="token comment"># 1x1PW卷积(含激活函数)</span>
<span class="token keyword">def</span> <span class="token function">Conv1x1BNReLU</span><span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span>out_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU6<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>


<span class="token comment"># 1x1PW卷积(不含激活函数)</span>
<span class="token keyword">def</span> <span class="token function">Conv1x1BN</span><span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span>out_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">)</span>
        <span class="token punctuation">)</span>


<span class="token comment"># 划分channels: dim默认为0，但是由于channnels位置在1，所以传参为1</span>
<span class="token keyword">class</span> <span class="token class-name">HalfSplit</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> first_half<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>HalfSplit<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>first_half <span class="token operator">=</span> first_half
        self<span class="token punctuation">.</span>dim <span class="token operator">=</span> dim

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 对input的channesl进行分半操作</span>
        splits <span class="token operator">=</span> torch<span class="token punctuation">.</span>chunk<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span>self<span class="token punctuation">.</span>dim<span class="token punctuation">)</span>        <span class="token comment"># 由于shape=[b, c, h, w],对于dim=1，针对channels</span>
        <span class="token keyword">return</span> splits<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">if</span> self<span class="token punctuation">.</span>first_half <span class="token keyword">else</span> splits<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>  <span class="token comment"># 返回其中的一半</span>


<span class="token comment"># channels shuffle增加组间交流</span>
<span class="token keyword">class</span> <span class="token class-name">ChannelShuffle</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> groups<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>ChannelShuffle<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>groups <span class="token operator">=</span> groups

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">'''Channel shuffle: [N,C,H,W] -&gt; [N,g,C/g,H,W] -&gt; [N,C/g,g,H,w] -&gt; [N,C,H,W]'''</span>
        N<span class="token punctuation">,</span> C<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
        g <span class="token operator">=</span> self<span class="token punctuation">.</span>groups
        <span class="token keyword">return</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>N<span class="token punctuation">,</span> g<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>C <span class="token operator">/</span> g<span class="token punctuation">)</span><span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>N<span class="token punctuation">,</span> C<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span>


<span class="token comment"># ShuffleNet的基本单元</span>
<span class="token keyword">class</span> <span class="token class-name">ShuffleNetUnits</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> stride<span class="token punctuation">,</span> groups<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>ShuffleNetUnits<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>stride <span class="token operator">=</span> stride

        <span class="token comment"># 如果stride = 2，由于主分支需要加上从分支的channels，为了两者加起来等于planes，所以需要先减一下</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>stride <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
            mid_channels <span class="token operator">=</span> out_channels <span class="token operator">-</span> in_channels
        <span class="token comment"># 如果stride = 2，mid_channels是一半，直接除以2即可</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            mid_channels <span class="token operator">=</span> out_channels <span class="token operator">//</span> <span class="token number">2</span>
            in_channels <span class="token operator">=</span> mid_channels
            <span class="token comment"># 进行两次切分，一次接受一半，一次接受另外一半</span>
            self<span class="token punctuation">.</span>first_half <span class="token operator">=</span> HalfSplit<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> first_half<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>     <span class="token comment"># 对channels进行切半操作, 第一次分: first_half=True</span>
            self<span class="token punctuation">.</span>second_split <span class="token operator">=</span> HalfSplit<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> first_half<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token comment"># 返回输入的另外一半channesl，两次合起来才是完整的一份channels</span>

        <span class="token comment"># 两个结构的主分支都是一样的，只是3x3DW卷积中的stride不一样，所以可以调用同样的self.bottleneck，stride会自动改变</span>
        self<span class="token punctuation">.</span>bottleneck <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            Conv1x1BNReLU<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> in_channels<span class="token punctuation">)</span><span class="token punctuation">,</span>                <span class="token comment"># 没有改变channels</span>
            Conv3x3BN<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> mid_channels<span class="token punctuation">,</span> stride<span class="token punctuation">,</span> groups<span class="token punctuation">)</span><span class="token punctuation">,</span>   <span class="token comment"># 升维</span>
            Conv1x1BNReLU<span class="token punctuation">(</span>mid_channels<span class="token punctuation">,</span> mid_channels<span class="token punctuation">)</span>                <span class="token comment"># 没有改变channels</span>
        <span class="token punctuation">)</span>

        <span class="token comment"># 结构(d)的从分支，3x3的DW卷积——&gt;1x1卷积</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>stride <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>shortcut <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
                Conv3x3BN<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>in_channels<span class="token punctuation">,</span> stride<span class="token operator">=</span>stride<span class="token punctuation">,</span> groups<span class="token operator">=</span>groups<span class="token punctuation">)</span><span class="token punctuation">,</span>
                Conv1x1BNReLU<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> in_channels<span class="token punctuation">)</span>
            <span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>channel_shuffle <span class="token operator">=</span> ChannelShuffle<span class="token punctuation">(</span>groups<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># stride = 2: 对于结构(d)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>stride <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
            x1 <span class="token operator">=</span> self<span class="token punctuation">.</span>bottleneck<span class="token punctuation">(</span>x<span class="token punctuation">)</span>     <span class="token comment"># torch.Size([1, 220, 28, 28])</span>
            x2 <span class="token operator">=</span> self<span class="token punctuation">.</span>shortcut<span class="token punctuation">(</span>x<span class="token punctuation">)</span>       <span class="token comment"># torch.Size([1, 24, 28, 28])</span>
        <span class="token comment"># 两个分支作concat操作之后, 输出的channels便为224，与planes[0]值相等</span>
        <span class="token comment"># out输出为: torch.Size([1, 244, 28, 28])</span>

        <span class="token comment"># stride = 1: 对于结构(c)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            x1 <span class="token operator">=</span> self<span class="token punctuation">.</span>first_half<span class="token punctuation">(</span>x<span class="token punctuation">)</span>     <span class="token comment"># 一开始直接将channels等分两半，x1称为主分支的一半，此时的x1: channels = 112</span>
            x2 <span class="token operator">=</span> self<span class="token punctuation">.</span>second_split<span class="token punctuation">(</span>x<span class="token punctuation">)</span>   <span class="token comment"># x2称为输入的另外一半channels: 此时x2:: channels = 112</span>
            x1 <span class="token operator">=</span> self<span class="token punctuation">.</span>bottleneck<span class="token punctuation">(</span>x1<span class="token punctuation">)</span>    <span class="token comment"># 结构(c)的主分支处理</span>
        <span class="token comment"># 两个分支作concat操作之后, 输出的channels便为224，与planes[0]值相等</span>
        <span class="token comment"># out输出为: torch.Size([1, 244, 28, 28])</span>

        out <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>x1<span class="token punctuation">,</span> x2<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment"># torch.Size([1, 244, 28, 28])</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>channel_shuffle<span class="token punctuation">(</span>out<span class="token punctuation">)</span>     <span class="token comment"># ShuffleNet的精髓</span>
        <span class="token keyword">return</span> out

<span class="token keyword">class</span> <span class="token class-name">ShuffleNetV2</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># shufflenet_v2_x2_0: planes = [244, 488, 976]  layers = [4, 8, 4]</span>
    <span class="token comment"># shufflenet_v2_x1_5: planes = [176, 352, 704]  layers = [4, 8, 4]</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> planes<span class="token punctuation">,</span> layers<span class="token punctuation">,</span> groups<span class="token punctuation">,</span> is_shuffle2_0<span class="token punctuation">,</span> num_classes<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>ShuffleNetV2<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># self.groups = 1</span>
        self<span class="token punctuation">.</span>groups <span class="token operator">=</span> groups

        <span class="token comment"># input: torch.Size([1, 3, 224, 224])</span>
        self<span class="token punctuation">.</span>stage1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            <span class="token comment"># 结构图中，对于conv1与MaxPool的stride均为2</span>
            Conv3x3BNReLU<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">24</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> groups<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># torch.Size([1, 24, 112, 112])</span>
            nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>                    <span class="token comment"># torch.Size([1, 24, 56, 56])</span>
        <span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>stage2 <span class="token operator">=</span> self<span class="token punctuation">.</span>_make_layer<span class="token punctuation">(</span><span class="token number">24</span><span class="token punctuation">,</span> planes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> layers<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span>          <span class="token comment"># torch.Size([1, 244, 28, 28])</span>
        self<span class="token punctuation">.</span>stage3 <span class="token operator">=</span> self<span class="token punctuation">.</span>_make_layer<span class="token punctuation">(</span>planes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> planes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> layers<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token comment"># torch.Size([1, 488, 14, 14])</span>
        self<span class="token punctuation">.</span>stage4 <span class="token operator">=</span> self<span class="token punctuation">.</span>_make_layer<span class="token punctuation">(</span>planes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> planes<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> layers<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token comment"># torch.Size([1, 976, 7, 7])</span>

        <span class="token comment"># 0.5x / 1x / 1.5x 输出为1024, 2x 输出为 2048</span>
        self<span class="token punctuation">.</span>conv5 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>planes<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">1024</span><span class="token operator">*</span>is_shuffle2_0<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>global_pool <span class="token operator">=</span> nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>      <span class="token comment"># torch.Size([1, 976, 1, 1])</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span>    <span class="token comment"># 丢失概率为0.2</span>

        <span class="token comment"># 0.5x / 1x / 1.5x 输入为1024, 2x 输入为 2048</span>
        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">1024</span><span class="token operator">*</span>is_shuffle2_0<span class="token punctuation">,</span> out_features<span class="token operator">=</span>num_classes<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>init_params<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 此处的is_stage2作用不大，以为均采用3x3的DW卷积，也就是group=1的组卷积</span>
    <span class="token keyword">def</span> <span class="token function">_make_layer</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> block_num<span class="token punctuation">,</span> is_stage2<span class="token punctuation">)</span><span class="token punctuation">:</span>
        layers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token comment"># 在ShuffleNetV2中，每个stage的第一个结构的stride均为2；此stage的其余结构的stride均为1.</span>
        <span class="token comment"># 对于stride =2 的情况，对应结构(d): 一开始无切分操作，主分支经过1x1——&gt;3x3——&gt;1x1，从分支经过3x3——&gt;1x1，两个分支作concat操作</span>
        layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>ShuffleNetUnits<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>out_channels<span class="token punctuation">,</span> stride<span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span> groups<span class="token operator">=</span><span class="token number">1</span> <span class="token keyword">if</span> is_stage2 <span class="token keyword">else</span> self<span class="token punctuation">.</span>groups<span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token comment"># 对于stride = 1的情况，对应结构(c): 一开始就切分channel，主分支经过1x1——&gt;3x3——&gt;1x1再与shortcut进行concat操作</span>
        <span class="token keyword">for</span> idx <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>ShuffleNetUnits<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>out_channels<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>out_channels<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> groups<span class="token operator">=</span>self<span class="token punctuation">.</span>groups<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>layers<span class="token punctuation">)</span>

    <span class="token comment"># 何凯明的方法初始化权重</span>
    <span class="token keyword">def</span> <span class="token function">init_params</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> m <span class="token keyword">in</span> self<span class="token punctuation">.</span>modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
                nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>kaiming_normal_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
                nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
            <span class="token keyword">elif</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">)</span> <span class="token operator">or</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">)</span><span class="token punctuation">:</span>
                nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
                nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>

    <span class="token comment"># input: torch.Size([1, 3, 224, 224])</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>stage1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>      <span class="token comment"># torch.Size([1, 24, 56, 56])</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>stage2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>      <span class="token comment"># torch.Size([1, 244, 28, 28])</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>stage3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>      <span class="token comment"># torch.Size([1, 488, 14, 14])</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>stage4<span class="token punctuation">(</span>x<span class="token punctuation">)</span>      <span class="token comment"># torch.Size([1, 976, 7, 7])</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv5<span class="token punctuation">(</span>x<span class="token punctuation">)</span>       <span class="token comment"># torch.Size([1, 2048, 7, 7])</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>global_pool<span class="token punctuation">(</span>x<span class="token punctuation">)</span>     <span class="token comment"># torch.Size([1, 2048, 1, 1])</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>   <span class="token comment"># torch.Size([1, 2048])</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    <span class="token comment"># torch.Size([1, 5])</span>
        <span class="token keyword">return</span> out

<span class="token keyword">def</span> <span class="token function">shufflenet_v2_x2_0</span><span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    planes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">244</span><span class="token punctuation">,</span> <span class="token number">488</span><span class="token punctuation">,</span> <span class="token number">976</span><span class="token punctuation">]</span>
    layers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span>
    model <span class="token operator">=</span> ShuffleNetV2<span class="token punctuation">(</span>planes<span class="token punctuation">,</span> layers<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> model

<span class="token keyword">def</span> <span class="token function">shufflenet_v2_x1_5</span><span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    planes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">176</span><span class="token punctuation">,</span> <span class="token number">352</span><span class="token punctuation">,</span> <span class="token number">704</span><span class="token punctuation">]</span>
    layers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span>
    model <span class="token operator">=</span> ShuffleNetV2<span class="token punctuation">(</span>planes<span class="token punctuation">,</span> layers<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> model

<span class="token keyword">def</span> <span class="token function">shufflenet_v2_x1_0</span><span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    planes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">116</span><span class="token punctuation">,</span> <span class="token number">232</span><span class="token punctuation">,</span> <span class="token number">464</span><span class="token punctuation">]</span>
    layers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span>
    model <span class="token operator">=</span> ShuffleNetV2<span class="token punctuation">(</span>planes<span class="token punctuation">,</span> layers<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> model

<span class="token keyword">def</span> <span class="token function">shufflenet_v2_x0_5</span><span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    planes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">48</span><span class="token punctuation">,</span> <span class="token number">96</span><span class="token punctuation">,</span> <span class="token number">192</span><span class="token punctuation">]</span>
    layers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span>
    model <span class="token operator">=</span> ShuffleNetV2<span class="token punctuation">(</span>planes<span class="token punctuation">,</span> layers<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> model

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    model <span class="token operator">=</span> shufflenet_v2_x2_0<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># model = shufflenet_v2_x1_5()</span>
    <span class="token comment"># model = shufflenet_v2_x1_0()</span>
    <span class="token comment"># model = shufflenet_v2_x0_5()</span>
    <span class="token comment"># print(model)</span>

    <span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">)</span>
    out <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>out<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>

    torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"shufflenet_v2_x2_0.mdl"</span><span class="token punctuation">)</span>
</code></pre> 
<p>可以查看文件大小：<br> <img src="https://images2.imgbox.com/0c/6b/SZEqlABp_o.png" alt="在这里插入图片描述"></p> 
<p><strong>参考资料：</strong><br> https://www.bilibili.com/video/BV15y4y1Y7SY<br> https://arxiv.org/pdf/1807.11164.pdf</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/7223cff8732646f7d7f30d92e090d54b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Meterpreter session 2 closed. Reason: Died 问题解决方法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/11a7eed0f7dbc2a592a8ae76930368a1/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">win7计算机未连接网络连接,win7连接可用，未连接怎么办？</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>