<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>大数据技术之Kafka - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="大数据技术之Kafka" />
<meta property="og:description" content="第1章 Kafka概述 1.1 定义 Kafka传统定义: Kafka是一个分布式的基于发布/订阅模式的消息队列 (MessageQueue)，主要应用于大数据实时处理领域。
发布/订阅 :消息的发布者不会将消息直接发送给特定的订阅者，而是将发布的消息分为不同的类别，订阅者只接收感兴趣的消息。
Kafka最新定义 : Kaka是一个开源的分布式事件流平台 (Event StreamingPlatformm)，被数千家公司用于高性能数据管道、流分析、数据集成和关键任务应用。
1.2 消息队列 目前企业中比较常见的消息队列产品主要有Kafka、ActiveMQ、RabbitMQ、RocketMQ等。
在大数据场景主要采用Kafka作为消息队列在JavaEE开发中主要采用ActiveMQ、RabbitMQ、RocketMQ 1.2.1 传统消息队列的应用场景 传统的消费队列的主要应用场景有：缓存/削峰（缓冲）、解耦（少依赖）、异步通信（不必要及时处理）。
缓存/削峰（缓冲）：有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。解耦:允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。异步通信：允许用户把一个消息放入队列，但并不立即处理它，然后在需要的时候再去处理它们。 1.2.2 消息队列的两种模式 消息队列主要分为两种模式：点对点模式（一个生产者对口一个消费者）和发布/订阅模式（一对多）。
1.3 Kafka基础架构 1）Producer ：消息生产者，就是向kafka broker发消息的客户端；
2）Consumer ：消息消费者，向kafka broker获取消息的客户端；
3）Consumer Group （CG）：消费者组，由多个consumer组成。消费者组内每个消费者负责消费不同分区的数据，一个broker可以有多个不同的topic，一个topic下的一个分区只能被一个消费者组内的一个消费者所消费；消费者组之间互不影响。消费者组是逻辑上的一个订阅者。
4）Broker ：一台kafka服务器就是一个broker。一个broker可以容纳多个不同topic。
5）Topic ：可以理解为一个队列，生产者和消费者面向的都是一个topic；
6）Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列；
7）Replica：副本，为保证集群中的某个节点发生故障时，该节点上的partition数据不丢失，且kafka仍然能够继续工作，kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个leader和若干个follower。
8）leader：每个分区副本中的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是leader。
9）follower：每个分区副本中的“从”，实时与leader副本保持同步，在leader发生故障时，成为新的leader。
第2章 Kafka快速入门 2.1 安装部署 后续更新
2.2 Kafka命令行操作 2.2.1 主题命令行操作 查看操作主题命令需要的参数
[atguigu@hadoop102 kafka]$ bin/kafka-topics.sh
重要的参数如下
| 参数 | 描述|
|–|–|
| --bootstrap-server | 连接kafka Broker主机名称和端口号 |" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/11879a37b2a58215b9a83a7f04b87c62/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-01T19:45:02+08:00" />
<meta property="article:modified_time" content="2023-04-01T19:45:02+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">大数据技术之Kafka</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="1_Kafka_1"></a>第1章 Kafka概述</h2> 
<h3><a id="11__2"></a>1.1 定义</h3> 
<ul><li> <p><strong>Kafka传统定义:</strong> Kafka是一个<strong>分布式</strong>的基于<strong>发布/订阅模式</strong>的<strong>消息队列</strong> (MessageQueue)，主要应用于大数据实时处理领域。</p> </li><li> <p><strong>发布/订阅 :<strong>消息的发布者不会将消息直接发送给特定的订阅者，而是</strong>将发布的消息分为不同的类别</strong>，订阅者<strong>只接收感兴趣的消息</strong>。</p> </li><li> <p><strong>Kafka最新定义 :</strong> Kaka是一个开源的<strong>分布式事件流平台</strong> (Event StreamingPlatformm)，被数千家公司用于高性能<strong>数据管道、流分析、数据集成和关键任务应用</strong>。</p> </li></ul> 
<h3><a id="12__9"></a>1.2 消息队列</h3> 
<p>目前企业中比较常见的消息队列产品主要有Kafka、ActiveMQ、RabbitMQ、RocketMQ等。</p> 
<ul><li>在大数据场景主要采用Kafka作为消息队列</li><li>在JavaEE开发中主要采用ActiveMQ、RabbitMQ、RocketMQ</li></ul> 
<h4><a id="121__13"></a>1.2.1 传统消息队列的应用场景</h4> 
<p>传统的消费队列的主要应用场景有：<strong>缓存/削峰（缓冲）、解耦（少依赖）、异步通信（不必要及时处理）</strong>。</p> 
<ul><li><strong>缓存/削峰（缓冲）</strong>：有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。</li><li><strong>解耦</strong>:允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。</li><li><strong>异步通信</strong>：允许用户把一个消息放入队列，但并不立即处理它，然后在需要的时候再去处理它们。</li></ul> 
<h4><a id="122__18"></a>1.2.2 消息队列的两种模式</h4> 
<p>消息队列主要分为两种模式：<strong>点对点模式</strong>（一个生产者对口一个消费者）和<strong>发布/订阅模式</strong>（一对多）。<br> <img src="https://images2.imgbox.com/f9/4a/5AjPII76_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="13_Kafka_21"></a>1.3 Kafka基础架构</h3> 
<p><img src="https://images2.imgbox.com/37/41/Xn01kIle_o.png" alt="在这里插入图片描述"><br> 1）<strong>Producer</strong> ：消息生产者，就是向kafka broker发消息的客户端；<br> 2）<strong>Consumer</strong> ：消息消费者，向kafka broker获取消息的客户端；<br> 3）<strong>Consumer Group （CG）</strong>：消费者组，由多个consumer组成。消费者<strong>组内每个消费者负责消费不同分区的数据</strong>，一个broker可以有多个不同的topic，<strong>一个topic下的一个分区只能被一个消费者组内的一个消费者所消费</strong>；消费者组之间互不影响。消费者组是逻辑上的一个订阅者。<br> 4）<strong>Broker</strong> ：一台kafka服务器就是一个broker。一个broker可以容纳多个不同topic。<br> 5）<strong>Topic</strong> ：可以理解为一个<strong>队列</strong>，生产者和消费者面向的都是一个topic；<br> 6）<strong>Partition</strong>：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，<strong>一个topic可以分为多个partition</strong>，每个partition是一个有序的队列；<br> 7）<strong>Replica</strong>：副本，为保证集群中的<strong>某个节点发生故障时，该节点上的partition数据不丢失，且kafka仍然能够继续工作</strong>，kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个<strong>leader</strong>和若干个<strong>follower</strong>。<br> 8）<strong>leader</strong>：每个分区副本中的“<strong>主</strong>”，生产者发送数据的对象，以及消费者消费数据的对象都是leader。<br> 9）<strong>follower</strong>：每个分区副本中的“<strong>从</strong>”，实时与leader副本保持同步，在leader发生故障时，成为新的leader。</p> 
<hr> 
<h2><a id="2_Kafka_34"></a>第2章 Kafka快速入门</h2> 
<h3><a id="21__35"></a>2.1 安装部署</h3> 
<p>后续更新</p> 
<h3><a id="22_Kafka_37"></a>2.2 Kafka命令行操作</h3> 
<p><img src="https://images2.imgbox.com/a2/ee/z7mugML1_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="221__39"></a>2.2.1 主题命令行操作</h4> 
<ol><li> <p>查看操作主题命令需要的参数<br> <code>[atguigu@hadoop102 kafka]$ bin/kafka-topics.sh</code></p> </li><li> <p>重要的参数如下<br> | 参数 | 描述|<br> |–|–|<br> | --bootstrap-server | 连接kafka Broker主机名称和端口号 |<br> |–topic | 操作的topic名称|<br> |–create |创建主题|<br> |–delete| 删除主题|<br> |–alter| 修改主题|<br> |–list |查看所有主题|<br> |–describe |查看主题详细描述|<br> |–partitions |设置主题分区数|<br> |–replication-factor |设置主题分区副本|<br> |–config |更新系统默认的配置|</p> </li><li> <p>查看当前服务器中的所有topic<br> <code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --list</code></p> </li><li> <p>创建一个主题名为first的topic<br> <code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --create --replication-factor 3 --partitions 3 --topic first</code></p> </li><li> <p>查看Topic的详情<br> <code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic first</code><br> Topic: first TopicId: EVV4qHcSR_q0O8YyD32gFg PartitionCount: 1 ReplicationFactor: 3 Configs: segment.bytes=1073741824<br> Topic: first Partition: 0 Leader: 102 Replicas: 102,103,104 Isr: 102,103,104</p> </li><li> <p>修改分区数（注意：分区数只能增加，不能减少）<br> <code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --alter --topic first --partitions 3</code></p> </li><li> <p>再次查看Topic的详情<br> <code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic first</code><br> Topic: first TopicId: EVV4qHcSR_q0O8YyD32gFg PartitionCount: 3 ReplicationFactor: 3 Configs: segment.bytes=1073741824<br> Topic: first Partition: 0 Leader: 102 Replicas: 102,103,104 Isr: 102,103,104<br> Topic: first Partition: 1 Leader: 103 Replicas: 103,104,102 Isr: 103,104,102<br> Topic: first Partition: 2 Leader: 104 Replicas: 104,102,103 Isr: 104,102,103</p> </li><li> <p>删除topic<br> <code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --delete --topic first</code></p> </li></ol> 
<h4><a id="222__76"></a>2.2.2 生产者命令行操作</h4> 
<ol><li>查看命令行生产者的参数<br> <code>[atguigu@hadoop102 kafka]$ bin/kafka-console-producer.sh </code></li><li>重要的参数如下：</li></ol> 
<table><thead><tr><th>参数</th><th>描述</th></tr></thead><tbody><tr><td>–bootstrap-server</td><td>连接kafka Broker主机名称和端口号</td></tr><tr><td>–topic</td><td>操作的topic名称</td></tr></tbody></table> 
<ol start="4"><li>生产消息<br> <code>bin/kafka-console-producer.sh --broker-list hadoop102:9092 --topic first</code></li></ol> 
<p>2.2.3 消费者命令行操作</p> 
<ol><li>查看命令行消费者的参数<br> <code>bin/kafka-console-consumer.sh </code></li><li>重要的参数如下：</li></ol> 
<table><thead><tr><th>参数</th><th>描述</th></tr></thead><tbody><tr><td>–bootstrap-server</td><td>连接kafka Broker主机名称和端口号</td></tr><tr><td>–topic</td><td>操作的topic名称</td></tr><tr><td>–from-beginning</td><td>从头开始消费</td></tr><tr><td>–group</td><td>指定消费者组名称</td></tr></tbody></table> 
<ol start="3"><li>消费消息<br> <code>bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first</code></li><li>从头开始消费<br> <code>bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first</code></li></ol> 
<hr> 
<h2><a id="3_Kafka_104"></a>第3章 Kafka生产者</h2> 
<h3><a id="31__105"></a>3.1 生产者消息发送流程</h3> 
<h4><a id="311__106"></a>3.1.1 发送原理</h4> 
<p>Kafka的Producer发送消息采用的是<strong>异步发送</strong>的方式。<br> 在消息发送的过程中，涉及到了两个线程:<strong>main线程</strong>和<strong>Sender线程</strong>，以及一个<strong>线程共享变量: RecordAccumulator</strong>。<br> ① main线程中创建了一个双端队列RecordAccumulator，将消息发送给RecordAccumulator。<br> ② Sender线程不断从RecordAccumulator中拉取消息发送到Kafka broker。<br> <img src="https://images2.imgbox.com/86/82/Y8gKCrHi_o.png" alt="在这里插入图片描述"></p> 
<ol><li>Kafka Producer生产者调用main()线程，将外部数据打包成ProducerRecord对象，并通过sand()方法发送给拦截器、序列化器和分区器后，将消息放到线程共享队列中。</li><li>只有数据累积到batch.size(默认16K)或者到达linger.ms(默认0ms)设置的的时间后，sender()线程读取数据到NetworkClient。</li><li>sender()线程通过网络将数据发送给相应的Kafka集群的分区后，根据设置的应答acks等级，回复sender线程数据是否发送成功。</li><li>如果发送成功，则将清理NetworkClient和线程共享变量队列的数据。</li></ol> 
<ul><li>main线程：在客户端将数据放入双端队列里</li><li>Sender线程：从队列里读取数据发送到kafka集群</li><li>DQueue：双端队列，每个分区对应一个双端队列。队列中的内容就是ProducerBatch，即DQueue，写入缓存时放入尾部，Sender读取消息时从头部读取。</li><li>batch.size：只有数据积累到batch.size之后，sender才会取数据发送，默认大小为16k。</li><li>linger.ms：如果数据迟迟没有到达batch.size，那么sender线程在等待linger.ms设置的实践到达后就会取数据发送。（默认是0，表示不等待）</li><li>ProducerBatch：就是一个消息批次，包含很多ProducerRecord</li><li>RecordAccumulator主要用来缓存消息以便 Sender 线程可以批量发送，减少网络传输的资源消耗以提升性能。可通过生产者客户端参数buffer.memory配置，默认大小为32M。当生产者的缓冲区已满，这些方法就会阻塞。在阻塞时间达到max.block.ms时，生产者抛出超时异常。</li><li>InFlightRequests：缓存已经发出去但还没有收到响应的请求，配置参数为max.in.flight.requests.per.connection，默认值为5，即最多只能缓存5个未收到响应的请求。</li></ul> 
<p><strong>拦截器</strong><br> 拦截器可以在消息发送前做一些准备工作，如过滤某些不要的信息、修改消息的内容等等。</p> 
<p><strong>序列化器</strong><br> 序列化器(Serializer)用于把对象转换成字节数组能通过网络发送给broker。消费者需要用反序列化器(Deserializer)把从Kafka中收到的字节数组转换成相应的对象。</p> 
<p><strong>分区器</strong><br> 主要有以下策略:</p> 
<ol><li>指定partition 如果在创建ProducerRecord的时候，指定了分区号，那么就会存储到这个分区中去。</li><li>指定Key 将key的hash值与topic的分区数取余得到最终的分区号。</li><li>若没有key，也没有分区号，采用默认的RoundRobin轮询分配。(kafka2.4后使用StickyPartitioning Strategy分区)。</li></ol> 
<p><strong>ack应答级别</strong><br> producer发送给broker有以下三种应答级别:</p> 
<ul><li>0：生产者发送到服务端即可，不需要等待应答如果服务端接收到消息后突然宕机都就会导致数据丢失 ，此种级别可靠性很差但效率很高。</li><li>1：leader收到即可，如果leader还未同步到follower就挂了，那么也会导致丢数问题 ，此种级别可靠性中等效率中等。</li><li>-1：ISR队列里的所有节点都收到消息后返回 ，此种级别可靠性很高但效率很低。</li></ul> 
<p>注意： 数据完全可靠条件 = ACK级别设置为-1 + 分区副本大于等于2 + ISR里应答的最小副本数量大于等于2</p> 
<h4><a id="312__147"></a>3.1.2 生产者重要参数列表</h4> 
<table><thead><tr><th>参数名称</th><th>描述</th></tr></thead><tbody><tr><td>bootstrap.servers</td><td>生产者连接集群所需的broker地址清单。可以设置1个或者多个，中间用逗号隔开。生产者从给定的broker里查找到其他broker信息。</td></tr><tr><td>key.serializer、 value.serializer</td><td>指定发送消息的key和value的序列化类型。要写全类名。（反射获取）</td></tr><tr><td>buffer.memory</td><td>RecordAccumulator缓冲区总大小，默认32m。</td></tr><tr><td>batch.size</td><td>缓冲区一批数据最大值，默认16k。适当增加该值，可以提高吞吐量，但是如果该值设置太大，会导致数据传输延迟增加。</td></tr><tr><td>linger.ms</td><td>如果数据迟迟未达到batch.size，sender等待linger.time之后就会发送数据。单位ms，默认值是0ms，表示没有延迟。生产环境建议该值大小为5-100ms之间。</td></tr><tr><td>acks</td><td>0：生产者发送过来的数据，不需要等数据落盘应答。1：生产者发送过来的数据，Leader数据落盘后应答。-1（all）：生产者发送过来的数据，Leader和isr队列里面的所有节点数据都落盘后应答。默认值是-1</td></tr><tr><td>max.in.flight.requests.per.connection</td><td>允许最多没有返回ack的次数，默认为5，开启幂等性要保证该值是 1-5的数字。</td></tr><tr><td>Retries</td><td>（重试） 当消息发送出现错误的时候，系统会重发消息。retries表示重试次数。默认是int最大值，2147483647。如果设置了重试，还想保证消息的有序性，需要设置MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION=1否则在重试此失败消息的时候，其他的消息可能发送成功了。</td></tr><tr><td>retry.backoff.ms</td><td>两次重试之间的时间间隔，默认是100ms。</td></tr><tr><td>enable.idempotence</td><td>是否开启幂等性，默认true，开启幂等性。</td></tr><tr><td>compression.type</td><td>生产者发送的所有数据的压缩方式。默认是none，不压缩。 支持压缩类型：none、gzip、snappy、lz4和zstd。</td></tr></tbody></table> 
<h3><a id="32__161"></a>3.2 生产者分区</h3> 
<h4><a id="321_162"></a>3.2.1分区的原因</h4> 
<p><strong>分区的好处</strong>:<br> 1）<strong>便于合理使用存储资源</strong>，每个Partition在一个Broker上存储，可以把海量的数据按照分区切割成一块一块数据存储在多台Broker上。合理控制分区的任务，可以实现负载均衡的效果。<br> 2）<strong>提高并行度</strong>，生产者可以以分区为单位发送数据:消费者可以以分区为单位进行消费数据</p> 
<p>主要有以下策略:</p> 
<ol><li>若指定partition 如果在创建ProducerRecord的时候，指定了分区号，那么就会存储到这个分区中去。</li><li>若指定Key 将key的hash值与topic的分区数取余得到最终的分区号。</li><li>若既没有key，也没有分区号，采用默认的RoundRobin轮询分配。(kafka2.4后使用StickyPartitioning Strategy分区)。</li></ol> 
<h3><a id="33__171"></a>3.3 生产经验——生产者如何提高吞吐量</h3> 
<h4><a id="331__172"></a>3.3.1 吞吐量</h4> 
<ul><li>batch .size :批次大小，默认16k</li><li>mslinger.ms :等待时间，修改为5-100</li><li>compression.type: 压缩snappy</li><li>RecordAccumulator: 缓冲区大小，修改为64m。</li></ul> 
<h3><a id="34__178"></a>3.4 生产经验——数据可靠性</h3> 
<p><img src="https://images2.imgbox.com/e6/cb/UTqo9cFq_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/94/14/M8XhaGE4_o.png" alt="在这里插入图片描述"><br> <strong>可靠性总结:</strong></p> 
<ul><li>acks=0，生产者发送过来数据就不管了，可靠性差，效率高;</li><li>acks=1，生产者发送过来数据Leaer应答，可靠性中等，效率中等;</li><li>acks= -1，生产者发送过来数据Leader和ISR队列里面所有Flwer应答，可靠性高，效率低; 
  <ul><li>在生产环境中，ack=0很少使用;</li><li>ack=1，一般用于传输普通日志，允许丢个别数据;</li><li>ack=-1，一般用于传输和钱相关的数据对可靠性要求比较高的场景。</li></ul> </li></ul> 
<h3><a id="35__189"></a>3.5 生产经验——数据去重</h3> 
<h4><a id="351__190"></a>3.5.1 数据传递语义</h4> 
<p><img src="https://images2.imgbox.com/2a/13/Nv2aLrj0_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="352__192"></a>3.5.2 幂等性</h4> 
<ol><li>幂等性原理：<br> <img src="https://images2.imgbox.com/83/6e/1Guaxbuq_o.png" alt="在这里插入图片描述"></li><li>开启幂等性<br> 在prudocer的配置对象中，添加参数<strong>enable.idempotence</strong>,参数值默认为true，设置为false就关闭了。</li></ol> 
<h4><a id="353__197"></a>3.5.3 生产者事务</h4> 
<p><strong>1. kafka事务原理</strong><br> <img src="https://images2.imgbox.com/67/99/g3FE9iA1_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="36__200"></a>3.6 生产经验——数据有序</h3> 
<p>单分区内，有序(有条件的，详见下节) ;<br> 多分区，分区与分区间无序:</p> 
<h3><a id="37_httpsimgblogcsdnimgcn574d1612f87446908c2a46f7efb9e938png_203"></a>3.7 生产经验——数据无序<img src="https://images2.imgbox.com/1b/01/jWP7TpSu_o.png" alt="在这里插入图片描述"></h3> 
<hr> 
<h2><a id="4_Kafka_Broker_206"></a>第4章 Kafka Broker</h2> 
<h3><a id="41_Kafka_Broker__207"></a>4.1 Kafka Broker 工作流程</h3> 
<ol><li>Kafka Broker工作流程图示<br> <img src="https://images2.imgbox.com/9f/be/oguKPpxI_o.png" alt="在这里插入图片描述"></li></ol> 
<h4><a id="412_Broker_210"></a>4.1.2 Broker重要参数</h4> 
<table><thead><tr><th>参数名称</th><th>描述</th></tr></thead><tbody><tr><td>replica.lag.time.max.ms</td><td>ISR中的Follower超过该事件阈值(默认30s)未向Leader发送同步数据，则该Follower将被踢出ISR。</td></tr><tr><td>auto.leader.rebalance.enable</td><td>默认是true。 自动Leader Partition 平衡。</td></tr><tr><td>leader.imbalance.per.broker.percentage</td><td>默认是10%。每个broker允许的不平衡的leader的比率。如果每个broker超过了这个值，控制器会触发leader的平衡。</td></tr><tr><td>leader.imbalance.check.interval.seconds</td><td>默认值300秒。检查leader负载是否平衡的间隔时间。</td></tr><tr><td>log.segment.bytes</td><td>Kafka中log日志是分成一块块存储的，此配置是指log日志划分 成块的大小，默认值1G。</td></tr><tr><td>log.index.interval.bytes</td><td>默认4kb，kafka里面每当写入了4kb大小的日志（.log），然后就往index文件里面记录一个索引。</td></tr><tr><td></td><td>log.retention.hours</td></tr><tr><td>log.retention.minutes</td><td>Kafka中数据保存的时间，分钟级别，默认关闭。</td></tr><tr><td>log.retention.ms</td><td>Kafka中数据保存的时间，毫秒级别，默认关闭。（优先级最高）</td></tr><tr><td>log.retention.check.interval.ms</td><td>检查数据是否保存超时的间隔，默认是5分钟。</td></tr><tr><td>log.retention.bytes</td><td>默认等于-1，表示无穷大。超过设置的所有日志总大小，删除最早的segment。</td></tr><tr><td>log.cleanup.policy</td><td>默认是delete，表示所有数据启用删除策略；如果设置值为compact，表示所有数据启用压缩策略。</td></tr><tr><td>num.io.threads</td><td>默认是8。负责写磁盘的线程数。整个参数值要占总核数的50%。</td></tr><tr><td>num.replica.fetchers</td><td>副本拉取线程数，这个参数占总核数的50%的1/3</td></tr><tr><td>num.network.threads</td><td>默认是3。数据传输线程数，这个参数占总核数的50%的2/3 。</td></tr><tr><td>log.flush.interval.messages</td><td>强制页缓存刷写到磁盘的条数，默认是Max(long) (9223372036854775807)。一般交给系统管理。</td></tr><tr><td>log.flush.interval.ms</td><td>每隔多久，刷数据到磁盘，默认是null。一般不建议修改，交给系统自己管理。</td></tr></tbody></table> 
<h3><a id="42_Kafka__231"></a>4.2 Kafka 副本</h3> 
<h4><a id="421_kafka__232"></a>4.2.1 kafka 副本的基本信息</h4> 
<table><thead><tr><th>kafka副本作用</th><th>提高数据可靠性</th></tr></thead><tbody><tr><td>kafka副本个数</td><td>默认1个，生产环境中一般配置为2个，保证数据可靠性；但是过多的副本会增加磁盘存储空间、增加网络数据传输、降低kafka效率。</td></tr><tr><td>kafka副本角色</td><td>副本角色分为Leader和Follower。kafka生产者只会把数据发送到Leader，follower会主动从Leader上同步数据。</td></tr><tr><td>kafka中的AR</td><td>是所有副本的统称（Assigned Repllicas）, AR = ISR + OSR；ISR:表示和Leader保持同步（默认30s）的follower集合。OSR：表示Follower与Leader副本同步时，延迟过多的副本。</td></tr></tbody></table> 
<h4><a id="422_Leader_238"></a>4.2.2 Leader选举过程</h4> 
<ol><li>kafka controller：<br> kafka集群中有<strong>一个broker的Controller会被选举为Controller Leader</strong>，<strong>负责管理集群broker的上下线、所有的topic的分区副本分配</strong>和<strong>Leader选举等工作</strong>。<strong>Controller的信息同步工作是依赖于Zookeeper的。</strong></li></ol> 
<h4><a id="423_leader_follower_241"></a>4.2.3 leader和 follower故障处理细节</h4> 
<h5><a id="1_followerHw_242"></a>1. follower故障处理细节**（被踢-重连-追上Hw-连接成功）**</h5> 
<p><img src="https://images2.imgbox.com/64/5a/VsD11qq9_o.png" alt="在这里插入图片描述"><br> <strong>LEO（Log End Offset）：每个副本的最后一个offset，LEO其实就是最新的offset + 1。<br> HW（High Watermark）：所有副本中最小的LEO 。</strong></p> 
<p><strong>1）Follower故障:</strong><br> （1） Follower发生故障后会被临时踢出ISR<br> （2） 这个期间Leader和Follower继续接收数据<br> （3）待该Follower恢复后，Follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向Leader进行同步。<br> 4）等该Follower的LEO大于等于该Partition的HW，即Follower追上Leader之后，就可以重新加入ISR了。</p> 
<h5><a id="2_leaderISRarleaderleaderfollowerleader_253"></a>2. leader故障处理细节（从ISR队列选取ar中靠前的节点选为leader，新leader短则follower“剪”，反之则向leader同步）</h5> 
<p><strong>1) Leader故障</strong><br> (1) Leader发生故障之后，会从ISR中选出一个新的Leader<br> (2) 为保证多个副本之间的数据一致性，其余的Follower会先将各自的log文件高于HW的部分截掉，然后从新的Leader同步数据。</p> 
<h3><a id="43__258"></a>4.3 文件存储</h3> 
<h4><a id="431__259"></a>4.3.1 文件存储机制</h4> 
<p><img src="https://images2.imgbox.com/22/0e/HhY99q3Y_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="432__261"></a>4.3.2 文件清理策略</h4> 
<ol><li>kafka数据文件保存时间：默认是7天</li><li>kafka数据文件保存可通过如下参数修改<br> ① log.retention.hours：最低优先级小时， 默认7天（168小时）<br> ② log.retention.minutes：分钟<br> ③ log.retention.ms：最高优先级毫秒<br> ④ log.retention.check.interval.ms：负责设置检查周期，默认5分钟。</li><li>那么一旦超过了设置的时间就会采取清理策略，清理策略有两种：<strong>delete和compact</strong></li></ol> 
<p><strong>1）delete策略</strong><br> delete日志删除：将过期数据删除。<br> 配置：log.cleanup.policy=delete<br> 基于时间：默认打开，以segment中所有记录中的最大时间戳作为文件时间戳<br> 基于大小：默认关闭，超过设置的所有日志大小，删除最早的segment。<br> log.retention.bytes，默认等于-1，表示无穷大。</p> 
<p><strong>2）compact日志策略</strong><br> <strong>compact日志策略：对于相同的key的不同value值，只保留最后一个版本。</strong></p> 
<h3><a id="44__279"></a>4.4 高效读写数据</h3> 
<p>1．Kafka本身是分布式集群，可以采用分区技术，并行度高<br> 2．读数据采用<strong>稀疏索引</strong>，可以快速定位要消费的数据<br> 3．顺序写磁盘<br> Kafka的producer生产数据，要写入到log文件中，写的过程是一直<strong>追加到文件末端，为顺序写</strong>。官网有数据表明，同样的磁盘，<strong>顺序写能到600M/s，而随机写只有100K/s</strong>。这与磁盘的机械机构有关，顺序写之所以快，是因为其<strong>省去了大量磁头寻址的时间</strong>。<br> 4. 页缓存+零拷贝技术:<br> <img src="https://images2.imgbox.com/e6/01/67doy6HQ_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="5_Kafka_286"></a>第5章 Kafka消费者</h2> 
<h3><a id="51_Kafka_287"></a>5.1 Kafka消费方式</h3> 
<p><strong>Kafka的consumer采用pull（拉）模式从broker中读取数据。</strong></p> 
<table><thead><tr><th>push（推）模式</th><th>很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。</th></tr></thead><tbody><tr><td>pull 模式</td><td>可以根据consumer的消费能力以适当的速率消费消息。不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直返回空数据。针对这一点，Kafka的消费者在消费数据时会传入一个时长参数timeout，如果当前没有数据可供消费，consumer会等待一段时间之后再返回，这段时长即为timeout。</td></tr></tbody></table> 
<h3><a id="52_Kakfa_293"></a>5.2 Kakfa消费者工作流程</h3> 
<h4><a id="521__294"></a>5.2.1 消费者总体工作流程</h4> 
<p><img src="https://images2.imgbox.com/b3/b0/EAjze8Oa_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="522__297"></a>5.2.2 消费者组原理</h4> 
<p><img src="https://images2.imgbox.com/ea/35/tHAWAX8K_o.png" alt="在这里插入图片描述"></p> 
<p>Consumer Group（CG）：消费者组，由多个consumer组成。形成一个消费者组的条件，是所有消费者的groupid相同。</p> 
<ul><li><strong>消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费。</strong></li><li><strong>消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。</strong></li></ul> 
<p><strong>消费者组初始化流程:</strong><br> <img src="https://images2.imgbox.com/1f/d5/Sxg5wrbV_o.png" alt="在这里插入图片描述"></p> 
<p><strong>消费者组详细消费流程:</strong><br> <img src="https://images2.imgbox.com/47/43/iWI5Sfqt_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="53__310"></a>5.3 生产经验——分区分配策略及再平衡</h3> 
<p>1、一个consumer group中有多个consumer组成，一个 topic有多个partition组成，现在的问题是，到底由哪个consumer来消费哪个partition的数据。<br> 2、Kafka有四种主流的分区分配策略： <strong>Range</strong>、<strong>RoundRobin</strong>、<strong>Sticky</strong>、<strong>CooperativeSticky</strong>。<br> 可以通过配置参数partition.assignment.strategy，修改分区的分配策略。默认策略是Range + CooperativeSticky。Kafka可以同时使用多个分区分配策略。</p> 
<ol><li> <p><strong>Range分区策略原理:</strong>===&gt; <strong>容易造成数据倾斜</strong><br> <img src="https://images2.imgbox.com/1a/ca/fBNpnIKX_o.png" alt="在这里插入图片描述"></p> </li><li> <p><strong>RoundRobin分区分配策略原理</strong><br> <img src="https://images2.imgbox.com/9c/b6/88IUvTXN_o.png" alt="在这里插入图片描述"></p> </li><li> <p><strong>Sticky分区分配策略原理</strong><br> 1.粘性分区定义：<br> 可以理解为分配的结果带有“粘性的”。即在执行一次新的分配之前，考虑上一次分配的结果，<strong>尽量少的调整分配的变动，可以节省大量的开销</strong>。粘性分区是Kafka从0.11.x版本开始引入这种分配策略，<strong>首先会尽量均衡的放置分区到消费者上面</strong>，在出现同一消费者组内消费者出现问题的时候，会<strong>尽量保持原有分配的分区不变化。</strong></p> </li></ol> 
<h3><a id="55_offset_322"></a>5.5 offset位移</h3> 
<h4><a id="551_offset_323"></a>5.5.1 offset的默认维护位置</h4> 
<p>由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要<strong>实时记录自己消费到了哪个offset</strong>，以便故障恢复后继续消费。<br> Kafka 0.9版本之前，consumer默认将offset保存在Zookeeper中，从0.9版本开始，consumer默认将offset保存在Kafka一个内置的topic中，该topic为**__consumer_offsets**。<br> 在__consumer_offsets主题里面采用<strong>key+value</strong>的方式存储数据。<strong>key是groupId+topic+分区号</strong>,<strong>value是当前offset的值</strong>。每隔一段时间，kafka内部就会对这个topic进行compact（压实），即每个groupId+topic+分区号就保留最新的数据。</p> 
<h4><a id="552_offset_329"></a>5.5.2 自动提交offset</h4> 
<p><strong>1. 自动提交offset图示</strong><br> <img src="https://images2.imgbox.com/be/25/KEAZ8v50_o.png" alt="在这里插入图片描述"><br> 2）消费者自动提交offset</p> 
<pre><code class="prism language-java"><span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>clients<span class="token punctuation">.</span>consumer<span class="token punctuation">.</span></span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>clients<span class="token punctuation">.</span>consumer<span class="token punctuation">.</span></span><span class="token class-name">ConsumerRecord</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>clients<span class="token punctuation">.</span>consumer<span class="token punctuation">.</span></span><span class="token class-name">ConsumerRecords</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>clients<span class="token punctuation">.</span>consumer<span class="token punctuation">.</span></span><span class="token class-name">KafkaConsumer</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>util<span class="token punctuation">.</span></span><span class="token class-name">Arrays</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>util<span class="token punctuation">.</span></span><span class="token class-name">Properties</span><span class="token punctuation">;</span>
 
<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">CustomConsumer</span> <span class="token punctuation">{<!-- --></span>
    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token class-name">String</span><span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token comment">// 1. 创建kafka消费者配置类</span>
        <span class="token class-name">Properties</span> properties <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Properties</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// 2. 添加配置参数</span>
        <span class="token comment">// 添加连接</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>BOOTSTRAP_SERVERS_CONFIG<span class="token punctuation">,</span> <span class="token string">"hadoop102:9092"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// 配置序列化 必须</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>KEY_DESERIALIZER_CLASS_CONFIG<span class="token punctuation">,</span> <span class="token string">"org.apache.kafka.common.serialization.StringDeserializer"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>VALUE_DESERIALIZER_CLASS_CONFIG<span class="token punctuation">,</span> <span class="token string">"org.apache.kafka.common.serialization.StringDeserializer"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// 配置消费者组</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>GROUP_ID_CONFIG<span class="token punctuation">,</span> <span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// 是否自动提交offset</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>ENABLE_AUTO_COMMIT_CONFIG<span class="token punctuation">,</span> <span class="token string">"true"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// 提交offset的时间周期，默认5s，</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>AUTO_COMMIT_INTERVAL_MS_CONFIG<span class="token punctuation">,</span> <span class="token string">"1000"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// 3. 创建kafka消费者</span>
        <span class="token class-name">KafkaConsumer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span></span> consumer <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">KafkaConsumer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token punctuation">&gt;</span></span><span class="token punctuation">(</span>properties<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// 4. 设置消费主题  形参是列表</span>
        consumer<span class="token punctuation">.</span><span class="token function">subscribe</span><span class="token punctuation">(</span><span class="token class-name">Arrays</span><span class="token punctuation">.</span><span class="token function">asList</span><span class="token punctuation">(</span><span class="token string">"first"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// 5. 消费数据</span>
        <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">{<!-- --></span>
            <span class="token comment">// 6. 读取消息</span>
            <span class="token class-name">ConsumerRecords</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span></span> consumerRecords <span class="token operator">=</span> consumer<span class="token punctuation">.</span><span class="token function">poll</span><span class="token punctuation">(</span><span class="token class-name">Duration</span><span class="token punctuation">.</span><span class="token function">ofSeconds</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

            <span class="token comment">// 7. 输出消息</span>
            <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token class-name">ConsumerRecord</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span></span> consumerRecord <span class="token operator">:</span> consumerRecords<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
                <span class="token class-name">System</span><span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>consumerRecord<span class="token punctuation">.</span><span class="token function">value</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token punctuation">}</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<h4><a id="553_offset_377"></a>5.5.3 手动提交offset</h4> 
<p><strong>1. 手动提交offset图示：</strong><br> <img src="https://images2.imgbox.com/a6/3e/kWh15nar_o.png" alt="在这里插入图片描述"><br> 1）同步提交offset<br> 由于同步提交offset有失败重试机制，故更加可靠，以下为同步提交offset的示例。</p> 
<pre><code class="prism language-java"><span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>clients<span class="token punctuation">.</span>consumer<span class="token punctuation">.</span></span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>clients<span class="token punctuation">.</span>consumer<span class="token punctuation">.</span></span><span class="token class-name">ConsumerRecord</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>clients<span class="token punctuation">.</span>consumer<span class="token punctuation">.</span></span><span class="token class-name">ConsumerRecords</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>clients<span class="token punctuation">.</span>consumer<span class="token punctuation">.</span></span><span class="token class-name">KafkaConsumer</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>util<span class="token punctuation">.</span></span><span class="token class-name">Arrays</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>util<span class="token punctuation">.</span></span><span class="token class-name">Properties</span><span class="token punctuation">;</span>

<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">CustomConsumerByHand</span> <span class="token punctuation">{<!-- --></span>
    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token class-name">String</span><span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token comment">// 1. 创建kafka消费者配置类</span>
        <span class="token class-name">Properties</span> properties <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Properties</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// 2. 添加配置参数</span>
        <span class="token comment">// 添加连接</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>BOOTSTRAP_SERVERS_CONFIG<span class="token punctuation">,</span> <span class="token string">"hadoop102:9092"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// 配置序列化 必须</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>KEY_DESERIALIZER_CLASS_CONFIG<span class="token punctuation">,</span> <span class="token string">"org.apache.kafka.common.serialization.StringDeserializer"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>VALUE_DESERIALIZER_CLASS_CONFIG<span class="token punctuation">,</span> <span class="token string">"org.apache.kafka.common.serialization.StringDeserializer"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// 配置消费者组</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>GROUP_ID_CONFIG<span class="token punctuation">,</span> <span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// 是否自动提交offset</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>ENABLE_AUTO_COMMIT_CONFIG<span class="token punctuation">,</span> <span class="token string">"false"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// 提交offset的时间周期</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>AUTO_COMMIT_INTERVAL_MS_CONFIG<span class="token punctuation">,</span> <span class="token string">"1000"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// 3. 创建kafka消费者</span>
        <span class="token class-name">KafkaConsumer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span></span> consumer <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">KafkaConsumer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token punctuation">&gt;</span></span><span class="token punctuation">(</span>properties<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// 4. 设置消费主题  形参是列表</span>
        consumer<span class="token punctuation">.</span><span class="token function">subscribe</span><span class="token punctuation">(</span><span class="token class-name">Arrays</span><span class="token punctuation">.</span><span class="token function">asList</span><span class="token punctuation">(</span><span class="token string">"first"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// 5. 消费数据</span>
        <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">{<!-- --></span>
            <span class="token comment">// 6. 读取消息</span>
            <span class="token class-name">ConsumerRecords</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span></span> consumerRecords <span class="token operator">=</span> consumer<span class="token punctuation">.</span><span class="token function">poll</span><span class="token punctuation">(</span><span class="token class-name">Duration</span><span class="token punctuation">.</span><span class="token function">ofSeconds</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

            <span class="token comment">// 7. 输出消息</span>
            <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token class-name">ConsumerRecord</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span></span> consumerRecord <span class="token operator">:</span> consumerRecords<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
                <span class="token class-name">System</span><span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>consumerRecord<span class="token punctuation">.</span><span class="token function">value</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token punctuation">}</span>
<span class="token comment">// 同步提交offset</span>
            consumer<span class="token punctuation">.</span><span class="token function">commitSync</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>

    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>2）异步提交offset<br> 虽然同步提交offset更可靠一些，但是由于其会阻塞当前线程，直到提交成功。因此吞吐量会受到很大的影响。<strong>因此更多的情况下，会选用异步提交offset的方式。</strong><br> 以下为异步提交offset的示例：</p> 
<pre><code class="prism language-java"><span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>clients<span class="token punctuation">.</span>consumer<span class="token punctuation">.</span></span><span class="token operator">*</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>common<span class="token punctuation">.</span></span><span class="token class-name">TopicPartition</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>util<span class="token punctuation">.</span></span><span class="token class-name">Arrays</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>util<span class="token punctuation">.</span></span><span class="token class-name">Map</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>util<span class="token punctuation">.</span></span><span class="token class-name">Properties</span><span class="token punctuation">;</span>

<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">CustomConsumerByHand</span> <span class="token punctuation">{<!-- --></span>
    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token class-name">String</span><span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
         <span class="token comment">// 1. 创建kafka消费者配置类</span>
        <span class="token class-name">Properties</span> properties <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Properties</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// 2. 添加配置参数</span>
        <span class="token comment">// 添加连接</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>BOOTSTRAP_SERVERS_CONFIG<span class="token punctuation">,</span> <span class="token string">"hadoop102:9092"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// 配置序列化 必须</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>KEY_DESERIALIZER_CLASS_CONFIG<span class="token punctuation">,</span> <span class="token string">"org.apache.kafka.common.serialization.StringDeserializer"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>VALUE_DESERIALIZER_CLASS_CONFIG<span class="token punctuation">,</span> <span class="token string">"org.apache.kafka.common.serialization.StringDeserializer"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// 配置消费者组</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>GROUP_ID_CONFIG<span class="token punctuation">,</span> <span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// 是否自动提交offset</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>ENABLE_AUTO_COMMIT_CONFIG<span class="token punctuation">,</span> <span class="token string">"false"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// 提交offset的时间周期</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>AUTO_COMMIT_INTERVAL_MS_CONFIG<span class="token punctuation">,</span> <span class="token string">"1000"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// 3. 创建kafka消费者</span>
        <span class="token class-name">KafkaConsumer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span></span> consumer <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">KafkaConsumer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token punctuation">&gt;</span></span><span class="token punctuation">(</span>properties<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// 4. 设置消费主题  形参是列表</span>
        consumer<span class="token punctuation">.</span><span class="token function">subscribe</span><span class="token punctuation">(</span><span class="token class-name">Arrays</span><span class="token punctuation">.</span><span class="token function">asList</span><span class="token punctuation">(</span><span class="token string">"first"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// 5. 消费数据</span>
        <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">{<!-- --></span>
            <span class="token comment">// 6. 读取消息</span>
            <span class="token class-name">ConsumerRecords</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span></span> consumerRecords <span class="token operator">=</span> consumer<span class="token punctuation">.</span><span class="token function">poll</span><span class="token punctuation">(</span><span class="token class-name">Duration</span><span class="token punctuation">.</span><span class="token function">ofSeconds</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

            <span class="token comment">// 7. 输出消息</span>
            <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token class-name">ConsumerRecord</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span></span> consumerRecord <span class="token operator">:</span> consumerRecords<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
                <span class="token class-name">System</span><span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>consumerRecord<span class="token punctuation">.</span><span class="token function">value</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token punctuation">}</span>

            <span class="token comment">// 异步提交offset</span>
            consumer<span class="token punctuation">.</span><span class="token function">commitAsync</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">OffsetCommitCallback</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
                <span class="token comment">/**
                 * 回调函数输出
                 * @param offsets   offset信息
                 * @param exception 异常
                 */</span>
                <span class="token annotation punctuation">@Override</span>
                <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">onComplete</span><span class="token punctuation">(</span><span class="token class-name">Map</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">TopicPartition</span><span class="token punctuation">,</span> <span class="token class-name">OffsetAndMetadata</span><span class="token punctuation">&gt;</span></span> offsets<span class="token punctuation">,</span> <span class="token class-name">Exception</span> exception<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
                    <span class="token comment">// 如果出现异常打印</span>
                    <span class="token keyword">if</span> <span class="token punctuation">(</span>exception <span class="token operator">!=</span> <span class="token keyword">null</span> <span class="token punctuation">)</span><span class="token punctuation">{<!-- --></span>
                        <span class="token class-name">System</span><span class="token punctuation">.</span>err<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"Commit failed for "</span> <span class="token operator">+</span> offsets<span class="token punctuation">)</span><span class="token punctuation">;</span>
                    <span class="token punctuation">}</span>
                <span class="token punctuation">}</span>
            <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>

    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<h4><a id="554_Offset_493"></a>5.5.4 指定Offset消费</h4> 
<pre><code>auto.offset.reset = earliest | latest | none |
</code></pre> 
<p>当Kafka中没有初始偏移量（消费者组第一次消费）或服务器上不再存在当前偏移量时（例如该数据已被删除），该怎么办?<br> (1）earliest：自动将偏移量重置为最早的偏移量<br> （2）latest(默认值)：自动将偏移量重置为最新偏移量<br> （3）none：如果未找到消费者组的先前偏移量，则向消费者抛出异常</p> 
<h4><a id="555__499"></a>5.5.5 数据漏消费和重复消费分析</h4> 
<ol><li><strong>问题</strong>：无论是同步提交还是异步提交offset，都有可能会造成数据的<strong>漏消费</strong>或者<strong>重复消费</strong>。</li><li><strong>漏消费</strong>：先提交offset后消费，有可能造成数据的漏消费；</li><li><strong>重复消费</strong>：而先消费后提交offset，有可能会造成数据的重复消费。<br> <img src="https://images2.imgbox.com/aa/74/LwLexDVs_o.png" alt="在这里插入图片描述"></li></ol> 
<h3><a id="56_Consumer_504"></a>5.6 生产经验之Consumer事务</h3> 
<p><strong>1. 消费者事务</strong><br> <img src="https://images2.imgbox.com/be/05/BBENKBWA_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="57__507"></a>5.7 生产经验——数据积压（消费者如何提高吞吐量）</h3> 
<p><img src="https://images2.imgbox.com/62/f1/wrjrM2sh_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="6_KafkaEagle_509"></a>第6章 Kafka-Eagle监控</h2> 
<p>Kafka-Eagle框架可以监控Kafka集群的整体运行情况，在生产环境中经常使用。<br> 后期更新</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/3ad570c66872e4780b5a5617ab13891e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">jsonp解析</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/76dd8978098915488695e64f2f76de8b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Git - 如何将git修改的文件导出和导入</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>