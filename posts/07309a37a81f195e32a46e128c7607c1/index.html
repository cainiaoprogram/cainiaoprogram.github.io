<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Docker快速搭建Clickhouse集群(3分片3副本) - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Docker快速搭建Clickhouse集群(3分片3副本)" />
<meta property="og:description" content="背景 前文简单介绍了下Clickhouse的安装和客户端使用，在实际生产环境中，Clickhouse常常是以集群模式部署的，由于很多系统不满足sse4.2指令，这里使用docker来搭建一个Clickhouse的集群。
1. 环境说明 1.1 机器列表 机器名
IP
配置
操作系统
部署的服务
备注
server01
192.168.21.21
8c8g
centos7.3
clickhouserver（cs01-01）和
clickhouserver（cs01-02）
clickhouse01-01: 实例1, 端口: tcp 9000, http 8123, 同步端口9009, 类型: 分片1, 副本1
clickhouse01-02: 实例2, 端口: tcp 9001, http 8124, 同步端口9010, 类型: 分片2, 副本2 (clickhouse2的副本)
server02
192.168.21.69
8c8g
centos7.3
clickhouserver（cs02-01）和
clickhouserver（cs02-02）
clickhouse02-01: 实例1, 端口: tcp 9000, http 8123, 同步端口9009, 类型: 分片2, 副本1
clickhouse02-02: 实例2, 端口: tcp 9001, http 8124, 同步端口9010, 类型: 分片3, 副本2 (clickhouse3的副本)" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/07309a37a81f195e32a46e128c7607c1/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-02-15T09:28:57+08:00" />
<meta property="article:modified_time" content="2021-02-15T09:28:57+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Docker快速搭建Clickhouse集群(3分片3副本)</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>背景</h2> 
<p>前文简单介绍了下Clickhouse的安装和客户端使用，在实际生产环境中，Clickhouse常常是以集群模式部署的，由于很多系统不满足sse4.2指令，这里使用docker来搭建一个Clickhouse的集群。</p> 
<h2 id="8240-1576734643032">1. 环境说明</h2> 
<h3 id="6796-1577159832892">1.1 机器列表</h3> 
<table><tbody><tr><td> <p>机器名</p> </td><td> <p>IP</p> </td><td> <p>配置</p> </td><td> <p>操作系统</p> </td><td> <p>部署的服务</p> </td><td> <p>备注</p> </td></tr><tr><td> <p>server01</p> </td><td> <p>192.168.21.21</p> </td><td> <p>8c8g</p> </td><td> <p>centos7.3</p> </td><td> <p>clickhouserver（cs01-01）和</p> <p>clickhouserver（cs01-02）</p> </td><td> <p>   clickhouse01-01: 实例1, 端口: tcp 9000, http 8123, 同步端口9009, 类型: 分片1, 副本1</p> <p>     clickhouse01-02: 实例2, 端口: tcp 9001, http 8124, 同步端口9010, 类型: 分片2, 副本2 (clickhouse2的副本)</p> </td></tr><tr><td> <p>server02</p> </td><td> <p>192.168.21.69</p> </td><td> <p>8c8g</p> </td><td> <p>centos7.3</p> </td><td> <p>clickhouserver（cs02-01）和</p> <p>clickhouserver（cs02-02）</p> </td><td> <p> clickhouse02-01: 实例1, 端口: tcp 9000, http 8123, 同步端口9009, 类型: 分片2, 副本1</p> <p>     clickhouse02-02: 实例2, 端口: tcp 9001, http 8124, 同步端口9010, 类型: 分片3, 副本2 (clickhouse3的副本)</p> </td></tr><tr><td> <p>server03</p> </td><td> <p>192.168.21.6</p> </td><td> <p>8c8g</p> </td><td> <p>centos7.3</p> </td><td> <p>clickhouserver（cs03-01）和</p> <p>clickhouserver（cs03-02）</p> </td><td> <p>clickhouse03-01: 实例1, 端口: tcp 9000, http 8123, 同步端口9009, 类型: 分片3, 副本1</p> <p>     clickhouse03-02: 实例2, 端口: tcp 9001, http 8124, 同步端口9010, 类型: 分片1, 副本2 (clickhouse1的副本)</p> </td></tr></tbody></table> 
<h3 id="9554-1577159848576">1.2 机器初始化</h3> 
<h4 id="7839-1577159849363">1.2.1 配置host</h4> 
<p id="5827-1577159923191">执行：<em><strong>vi /etc/hosts，</strong></em> 加入下面三行：</p> 
<blockquote> 
 <p>192.168.21.21 server01</p> 
 <p>192.168.21.69 server02</p> 
 <p>192.168.21.6 server03</p> 
</blockquote> 
<h4 id="7833-1577159919173">1.2.2 安装docker</h4> 
<p>每台机器上均安装相同版本的docker</p> 
<p>参照：<a href="https://blog.csdn.net/ccgshigao/article/details/109173649">docker环境搭建</a></p> 
<h4 id="3616-1577160100449">1.2.3 安装Zookeeper</h4> 
<p>参照：<a href="https://blog.csdn.net/ccgshigao/article/details/107627370?ops_request_misc=%25257B%252522request%25255Fid%252522%25253A%252522161335209216780271521506%252522%25252C%252522scm%252522%25253A%25252220140713.130102334.pc%25255Fblog.%252522%25257D&amp;request_id=161335209216780271521506&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_v2~rank_v29-2-107627370.pc_v2_rank_blog_default&amp;utm_term=zk">zookper集群搭建（3节点）安装</a></p> 
<h3 id="1855-1577168713547">1.3 目录初始化</h3> 
<h4 id="7320-1576734674595">1.3.1 创建对应本地路径</h4> 
<p>分别在三台服务器，创建数据存储目录： </p> 
<p><em><strong>mkdir /data/clickhouse</strong></em></p> 
<h4>1.3.2 获取clickhouse-server的配置</h4> 
<p>1）在server01服务器做如下操作</p> 
<p>先按照官方教程的docker命令启动Clickhouse-Server</p> 
<p><em><strong>docker run -d --name clickhouse-server --ulimit nofile=262144:262144 --volume=/data/clickhouse/:/var/lib/clickhouse yandex/clickhouse-server</strong></em></p> 
<p>2）启动完成后，复制容器内的配置文件到本机目录下</p> 
<pre><code>#拷贝容器内容的配置到/etc目录下
docker cp clickhouse-server:/etc/clickhouse-server/ /etc/

#将server01上的目录重命名
cp -rf /etc/clickhouse-server/ /etc/clickhouse-server01/
cp -rf /etc/clickhouse-server/ /etc/clickhouse-server02/</code></pre> 
<p>3）然后将/etc/clickhouse-server/ 分别拷贝到每个机器上</p> 
<pre><code>#拷贝配置到server02上
scp /etc/clickhouse-server/ server02:/etc/clickhouse-server01/
scp /etc/clickhouse-server/ server02:/etc/clickhouse-server02/
#拷贝配置到server03上
scp /etc/clickhouse-server/ server03:/etc/clickhouse-server01/
scp /etc/clickhouse-server/ server03:/etc/clickhouse-server02/</code></pre> 
<h2>2. 集群环境搭建</h2> 
<h3 id="6077-1576994445543">2.1 集群环境拓扑图</h3> 
<p><img alt="" height="778" src="https://images2.imgbox.com/9c/98/XAQuEjla_o.jpg" width="716"></p> 
<p id="5158-1576994464664">集群环境说明：</p> 
<blockquote> 
 <p>clickhouse01-01: 实例1, 端口: tcp 9000, http 8123, 同步端口9009, 类型: 分片1, 副本1</p> 
 <p>clickhouse01-02: 实例2, 端口: tcp 9001, http 8124, 同步端口9010, 类型: 分片2, 副本2 (clickhouse2的副本)</p> 
 <p>clickhouse02-01: 实例1, 端口: tcp 9000, http 8123, 同步端口9009, 类型: 分片2, 副本1</p> 
 <p>clickhouse02-02: 实例2, 端口: tcp 9001, http 8124, 同步端口9010, 类型: 分片3, 副本2 (clickhouse3的副本)</p> 
 <p>clickhouse03-01: 实例1, 端口: tcp 9000, http 8123, 同步端口9009, 类型: 分片3, 副本1</p> 
 <p>clickhouse03-02: 实例2, 端口: tcp 9001, http 8124, 同步端口9010, 类型: 分片1, 副本2 (clickhouse1的副本)</p> 
</blockquote> 
<h3 id="6065-1576993592401">2.2 配置集群</h3> 
<h4 id="8976-1576997932107">2.2.1 待修改的配置文件</h4> 
<p>需要修改的配置有两个(如果有需要也可以配置user.xml)：</p> 
<ul><li>/etc/clickhouse-server/config.xml</li><li>/etc/clickhouse-server/metrika.xml（新增文件）</li></ul> 
<h4>2.2.2 server1上配置clickhouse-server的实例</h4> 
<p>2.2.2.1 clickhouse-01-01的配置：</p> 
<p>1）/etc/clickhouse-server01/config.xml(其他实例此配置内容和这个一样就行）</p> 
<p>修改include from节点为实际的引用到的文件</p> 
<blockquote> 
 <pre><code class="language-html"> &lt;!-- If element has 'incl' attribute, then for it's value will be used corresponding substitution from another file.
         By default, path to file with substitutions is /etc/metrika.xml. It could be changed in config in 'include_from' element.
         Values for substitutions are specified in /yandex/name_of_substitution elements in that file.
      --&gt;
    &lt;include_from&gt;/etc/clickhouse-server/metrika.xml&lt;/include_from&gt;
    
    &lt;listen_host&gt;0.0.0.0&lt;/listen_host&gt;
    &lt;listen_host&gt;127.0.0.1&lt;/listen_host&gt;</code></pre> 
</blockquote> 
<p>2）/etc/clickhouse-server01/metrika.xml（所有实例的配置内容都和这个一样就行）</p> 
<blockquote> 
 <pre><code class="language-html">&lt;!--所有实例均使用这个集群配置，不用个性化 --&gt;
&lt;yandex&gt;

    &lt;!-- 集群配置 --&gt;
    &lt;!-- clickhouse_remote_servers所有实例配置都一样 --&gt;
    &lt;!-- 集群配置 --&gt;
      &lt;clickhouse_remote_servers&gt;
        &lt;cluster_3s_1r&gt;

            &lt;!-- 数据分片1  --&gt;
            &lt;shard&gt;
                &lt;internal_replication&gt;true&lt;/internal_replication&gt;
                &lt;replica&gt;
                    &lt;host&gt;server01&lt;/host&gt;
                    &lt;port&gt;9000&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
                &lt;replica&gt;
                    &lt;host&gt;server03&lt;/host&gt;
                    &lt;port&gt;9001&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
            &lt;/shard&gt;

            &lt;!-- 数据分片2  --&gt;
            &lt;shard&gt;
                &lt;internal_replication&gt;true&lt;/internal_replication&gt;
                &lt;replica&gt;
                    &lt;host&gt;server02&lt;/host&gt;
                    &lt;port&gt;9000&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
                &lt;replica&gt;
                    &lt;host&gt;server01&lt;/host&gt;
                    &lt;port&gt;9001&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
            &lt;/shard&gt;

            &lt;!-- 数据分片3  --&gt;
            &lt;shard&gt;
                &lt;internal_replication&gt;true&lt;/internal_replication&gt;
                &lt;replica&gt;
                    &lt;host&gt;server03&lt;/host&gt;
                    &lt;port&gt;9000&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
                &lt;replica&gt;
                    &lt;host&gt;server02&lt;/host&gt;
                    &lt;port&gt;9001&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
            &lt;/shard&gt;
        &lt;/cluster_3s_1r&gt;
    &lt;/clickhouse_remote_servers&gt;

    &lt;!-- ZK  --&gt;
    &lt;!-- zookeeper_servers所有实例配置都一样 --&gt;
    &lt;zookeeper-servers&gt;
        &lt;node index="1"&gt;
            &lt;host&gt;192.168.21.66&lt;/host&gt;
            &lt;port&gt;2181&lt;/port&gt;
        &lt;/node&gt;
        &lt;node index="2"&gt;
            &lt;host&gt;192.168.21.57&lt;/host&gt;
            &lt;port&gt;2181&lt;/port&gt;
        &lt;/node&gt;
        &lt;node index="3"&gt;
            &lt;host&gt;192.168.21.17&lt;/host&gt;
            &lt;port&gt;2181&lt;/port&gt;
        &lt;/node&gt;
    &lt;/zookeeper-servers&gt;
    
    &lt;!-- marcos每个实例配置不一样 分片1, 副本1 --&gt;   
    &lt;macros&gt;
        &lt;layer&gt;01&lt;/layer&gt;
        &lt;shard&gt;01&lt;/shard&gt;
        &lt;replica&gt;cluster01-01-1&lt;/replica&gt;
    &lt;/macros&gt;


    &lt;networks&gt;
        &lt;ip&gt;::/0&lt;/ip&gt;
    &lt;/networks&gt;

    &lt;!-- 数据压缩算法  --&gt;
    &lt;clickhouse_compression&gt;
        &lt;case&gt;
            &lt;min_part_size&gt;10000000000&lt;/min_part_size&gt;
            &lt;min_part_size_ratio&gt;0.01&lt;/min_part_size_ratio&gt;
            &lt;method&gt;lz4&lt;/method&gt;
        &lt;/case&gt;
    &lt;/clickhouse_compression&gt;

&lt;/yandex&gt;</code></pre> 
</blockquote> 
<p>2.2.2.2 clickhouse-01-02的配置：</p> 
<p>1）/etc/clickhouse-server02/metrika.xml（所有实例的配置内容都和这个一样就行）</p> 
<blockquote> 
 <pre><code class="language-html">&lt;!--所有实例均使用这个集群配置，不用个性化 --&gt;
&lt;yandex&gt;

    &lt;!-- 集群配置 --&gt;
    &lt;!-- clickhouse_remote_servers所有实例配置都一样 --&gt;
    &lt;clickhouse_remote_servers&gt;
        &lt;cluster_3s_1r&gt;

            &lt;!-- 数据分片1  --&gt;
            &lt;shard&gt;
                &lt;internal_replication&gt;true&lt;/internal_replication&gt;
                &lt;replica&gt;
                    &lt;host&gt;server01&lt;/host&gt;
                    &lt;port&gt;9000&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
                &lt;replica&gt;
                    &lt;host&gt;server03&lt;/host&gt;
                    &lt;port&gt;9001&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
            &lt;/shard&gt;

            &lt;!-- 数据分片2  --&gt;
            &lt;shard&gt;
                &lt;internal_replication&gt;true&lt;/internal_replication&gt;
                &lt;replica&gt;
                    &lt;host&gt;server02&lt;/host&gt;
                    &lt;port&gt;9000&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
                &lt;replica&gt;
                    &lt;host&gt;server01&lt;/host&gt;
                    &lt;port&gt;9001&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
            &lt;/shard&gt;

            &lt;!-- 数据分片3  --&gt;
            &lt;shard&gt;
                &lt;internal_replication&gt;true&lt;/internal_replication&gt;
                &lt;replica&gt;
                    &lt;host&gt;server03&lt;/host&gt;
                    &lt;port&gt;9000&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
                &lt;replica&gt;
                    &lt;host&gt;server02&lt;/host&gt;
                    &lt;port&gt;9001&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
            &lt;/shard&gt;
        &lt;/cluster_3s_1r&gt;
    &lt;/clickhouse_remote_servers&gt;

    &lt;!-- ZK  --&gt;
    &lt;!-- zookeeper_servers所有实例配置都一样 --&gt;
    &lt;zookeeper-servers&gt;
        &lt;node index="1"&gt;
            &lt;host&gt;192.168.21.66&lt;/host&gt;
            &lt;port&gt;2181&lt;/port&gt;
        &lt;/node&gt;
        &lt;node index="2"&gt;
            &lt;host&gt;192.168.21.57&lt;/host&gt;
            &lt;port&gt;2181&lt;/port&gt;
        &lt;/node&gt;
        &lt;node index="3"&gt;
            &lt;host&gt;192.168.21.17&lt;/host&gt;
            &lt;port&gt;2181&lt;/port&gt;
        &lt;/node&gt;
    &lt;/zookeeper-servers&gt;
    
    &lt;!-- marcos每个实例配置不一样 分片2, 副本2--&gt;   
    &lt;macros&gt;
        &lt;layer&gt;01&lt;/layer&gt;
        &lt;shard&gt;02&lt;/shard&gt;
        &lt;replica&gt;cluster01-02-2&lt;/replica&gt;
    &lt;/macros&gt;

    &lt;networks&gt;
        &lt;ip&gt;::/0&lt;/ip&gt;
    &lt;/networks&gt;

    &lt;!-- 数据压缩算法  --&gt;
    &lt;clickhouse_compression&gt;
        &lt;case&gt;
            &lt;min_part_size&gt;10000000000&lt;/min_part_size&gt;
            &lt;min_part_size_ratio&gt;0.01&lt;/min_part_size_ratio&gt;
            &lt;method&gt;lz4&lt;/method&gt;
        &lt;/case&gt;
    &lt;/clickhouse_compression&gt;

&lt;/yandex&gt;</code></pre> 
</blockquote> 
<h4>2.2.3 server2上配置clickhouse-server的实例</h4> 
<p>2.2.3.1 clickhouse-02-01的配置：</p> 
<p>/etc/clickhouse-server01/metrika.xml</p> 
<blockquote> 
 <pre><code class="language-html">&lt;!--所有实例均使用这个集群配置，不用个性化 --&gt;
&lt;yandex&gt;

    &lt;!-- 集群配置 --&gt;
    &lt;!-- clickhouse_remote_servers所有实例配置都一样 --&gt;
       &lt;clickhouse_remote_servers&gt;
        &lt;cluster_3s_1r&gt;

            &lt;!-- 数据分片1  --&gt;
            &lt;shard&gt;
                &lt;internal_replication&gt;true&lt;/internal_replication&gt;
                &lt;replica&gt;
                    &lt;host&gt;server01&lt;/host&gt;
                    &lt;port&gt;9000&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
                &lt;replica&gt;
                    &lt;host&gt;server03&lt;/host&gt;
                    &lt;port&gt;9001&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
            &lt;/shard&gt;

            &lt;!-- 数据分片2  --&gt;
            &lt;shard&gt;
                &lt;internal_replication&gt;true&lt;/internal_replication&gt;
                &lt;replica&gt;
                    &lt;host&gt;server02&lt;/host&gt;
                    &lt;port&gt;9000&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
                &lt;replica&gt;
                    &lt;host&gt;server01&lt;/host&gt;
                    &lt;port&gt;9001&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
            &lt;/shard&gt;

            &lt;!-- 数据分片3  --&gt;
            &lt;shard&gt;
                &lt;internal_replication&gt;true&lt;/internal_replication&gt;
                &lt;replica&gt;
                    &lt;host&gt;server03&lt;/host&gt;
                    &lt;port&gt;9000&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
                &lt;replica&gt;
                    &lt;host&gt;server02&lt;/host&gt;
                    &lt;port&gt;9001&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
            &lt;/shard&gt;
        &lt;/cluster_3s_1r&gt;
    &lt;/clickhouse_remote_servers&gt;


    &lt;!-- ZK  --&gt;
    &lt;!-- zookeeper_servers所有实例配置都一样 --&gt;
    &lt;zookeeper-servers&gt;
        &lt;node index="1"&gt;
            &lt;host&gt;192.168.21.66&lt;/host&gt;
            &lt;port&gt;2181&lt;/port&gt;
        &lt;/node&gt;
        &lt;node index="2"&gt;
            &lt;host&gt;192.168.21.57&lt;/host&gt;
            &lt;port&gt;2181&lt;/port&gt;
        &lt;/node&gt;
        &lt;node index="3"&gt;
            &lt;host&gt;192.168.21.17&lt;/host&gt;
            &lt;port&gt;2181&lt;/port&gt;
        &lt;/node&gt;
    &lt;/zookeeper-servers&gt;
    
    &lt;!-- marcos每个实例配置不一样 分片2, 副本1--&gt;   
    &lt;macros&gt;
        &lt;layer&gt;01&lt;/layer&gt;
        &lt;shard&gt;02&lt;/shard&gt;
        &lt;replica&gt;cluster01-02-1&lt;/replica&gt;
    &lt;/macros&gt;


    &lt;networks&gt;
        &lt;ip&gt;::/0&lt;/ip&gt;
    &lt;/networks&gt;

    &lt;!-- 数据压缩算法  --&gt;
    &lt;clickhouse_compression&gt;
        &lt;case&gt;
            &lt;min_part_size&gt;10000000000&lt;/min_part_size&gt;
            &lt;min_part_size_ratio&gt;0.01&lt;/min_part_size_ratio&gt;
            &lt;method&gt;lz4&lt;/method&gt;
        &lt;/case&gt;
    &lt;/clickhouse_compression&gt;

&lt;/yandex&gt;</code></pre> 
</blockquote> 
<p>2.2.3.2 clickhouse-02-02的配置：</p> 
<p>/etc/clickhouse-server02/metrika.xml</p> 
<blockquote> 
 <pre><code class="language-html">&lt;!--所有实例均使用这个集群配置，不用个性化 --&gt;
&lt;yandex&gt;

    &lt;!-- 集群配置 --&gt;
    &lt;!-- clickhouse_remote_servers所有实例配置都一样 --&gt;
     &lt;clickhouse_remote_servers&gt;
        &lt;cluster_3s_1r&gt;

            &lt;!-- 数据分片1  --&gt;
            &lt;shard&gt;
                &lt;internal_replication&gt;true&lt;/internal_replication&gt;
                &lt;replica&gt;
                    &lt;host&gt;server01&lt;/host&gt;
                    &lt;port&gt;9000&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
                &lt;replica&gt;
                    &lt;host&gt;server03&lt;/host&gt;
                    &lt;port&gt;9001&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
            &lt;/shard&gt;

            &lt;!-- 数据分片2  --&gt;
            &lt;shard&gt;
                &lt;internal_replication&gt;true&lt;/internal_replication&gt;
                &lt;replica&gt;
                    &lt;host&gt;server02&lt;/host&gt;
                    &lt;port&gt;9000&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
                &lt;replica&gt;
                    &lt;host&gt;server01&lt;/host&gt;
                    &lt;port&gt;9001&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
            &lt;/shard&gt;

            &lt;!-- 数据分片3  --&gt;
            &lt;shard&gt;
                &lt;internal_replication&gt;true&lt;/internal_replication&gt;
                &lt;replica&gt;
                    &lt;host&gt;server03&lt;/host&gt;
                    &lt;port&gt;9000&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
                &lt;replica&gt;
                    &lt;host&gt;server02&lt;/host&gt;
                    &lt;port&gt;9001&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
            &lt;/shard&gt;
        &lt;/cluster_3s_1r&gt;
    &lt;/clickhouse_remote_servers&gt;


    &lt;!-- ZK  --&gt;
    &lt;!-- zookeeper_servers所有实例配置都一样 --&gt;
    &lt;zookeeper-servers&gt;
        &lt;node index="1"&gt;
            &lt;host&gt;192.168.21.66&lt;/host&gt;
            &lt;port&gt;2181&lt;/port&gt;
        &lt;/node&gt;
        &lt;node index="2"&gt;
            &lt;host&gt;192.168.21.57&lt;/host&gt;
            &lt;port&gt;2181&lt;/port&gt;
        &lt;/node&gt;
        &lt;node index="3"&gt;
            &lt;host&gt;192.168.21.17&lt;/host&gt;
            &lt;port&gt;2181&lt;/port&gt;
        &lt;/node&gt;
    &lt;/zookeeper-servers&gt;
    
    &lt;!-- marcos每个实例配置不一样 分片3, 副本2--&gt;   
    &lt;macros&gt;
        &lt;layer&gt;01&lt;/layer&gt;
        &lt;shard&gt;03&lt;/shard&gt;
        &lt;replica&gt;cluster01-03-2&lt;/replica&gt;
    &lt;/macros&gt;

    &lt;networks&gt;
        &lt;ip&gt;::/0&lt;/ip&gt;
    &lt;/networks&gt;

    &lt;!-- 数据压缩算法  --&gt;
    &lt;clickhouse_compression&gt;
        &lt;case&gt;
            &lt;min_part_size&gt;10000000000&lt;/min_part_size&gt;
            &lt;min_part_size_ratio&gt;0.01&lt;/min_part_size_ratio&gt;
            &lt;method&gt;lz4&lt;/method&gt;
        &lt;/case&gt;
    &lt;/clickhouse_compression&gt;

&lt;/yandex&gt;</code></pre> 
</blockquote> 
<h4>2.2.4 server3上配置clickhouse-server的实例</h4> 
<p>2.2.4.1 clickhouse-03-01的配置：</p> 
<p>1）/etc/clickhouse-server01/metrika.xml</p> 
<blockquote> 
 <pre><code class="language-html">&lt;!--所有实例均使用这个集群配置，不用个性化 --&gt;
&lt;yandex&gt;

    &lt;!-- 集群配置 --&gt;
    &lt;!-- clickhouse_remote_servers所有实例配置都一样 --&gt;
       &lt;clickhouse_remote_servers&gt;
        &lt;cluster_3s_1r&gt;

            &lt;!-- 数据分片1  --&gt;
            &lt;shard&gt;
                &lt;internal_replication&gt;true&lt;/internal_replication&gt;
                &lt;replica&gt;
                    &lt;host&gt;server01&lt;/host&gt;
                    &lt;port&gt;9000&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
                &lt;replica&gt;
                    &lt;host&gt;server03&lt;/host&gt;
                    &lt;port&gt;9001&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
            &lt;/shard&gt;

            &lt;!-- 数据分片2  --&gt;
            &lt;shard&gt;
                &lt;internal_replication&gt;true&lt;/internal_replication&gt;
                &lt;replica&gt;
                    &lt;host&gt;server02&lt;/host&gt;
                    &lt;port&gt;9000&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
                &lt;replica&gt;
                    &lt;host&gt;server01&lt;/host&gt;
                    &lt;port&gt;9001&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
            &lt;/shard&gt;

            &lt;!-- 数据分片3  --&gt;
            &lt;shard&gt;
                &lt;internal_replication&gt;true&lt;/internal_replication&gt;
                &lt;replica&gt;
                    &lt;host&gt;server03&lt;/host&gt;
                    &lt;port&gt;9000&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
                &lt;replica&gt;
                    &lt;host&gt;server02&lt;/host&gt;
                    &lt;port&gt;9001&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
            &lt;/shard&gt;
        &lt;/cluster_3s_1r&gt;
    &lt;/clickhouse_remote_servers&gt;

    &lt;!-- ZK  --&gt;
    &lt;!-- zookeeper_servers所有实例配置都一样 --&gt;
    &lt;zookeeper-servers&gt;
        &lt;node index="1"&gt;
            &lt;host&gt;192.168.21.66&lt;/host&gt;
            &lt;port&gt;2181&lt;/port&gt;
        &lt;/node&gt;
        &lt;node index="2"&gt;
            &lt;host&gt;192.168.21.57&lt;/host&gt;
            &lt;port&gt;2181&lt;/port&gt;
        &lt;/node&gt;
        &lt;node index="3"&gt;
            &lt;host&gt;192.168.21.17&lt;/host&gt;
            &lt;port&gt;2181&lt;/port&gt;
        &lt;/node&gt;
    &lt;/zookeeper-servers&gt;
    
    &lt;!-- marcos每个实例配置不一样 分片3, 副本1--&gt;   
    &lt;macros&gt;
        &lt;layer&gt;01&lt;/layer&gt;
        &lt;shard&gt;03&lt;/shard&gt;
        &lt;replica&gt;cluster01-03-1&lt;/replica&gt;
    &lt;/macros&gt;


    &lt;networks&gt;
        &lt;ip&gt;::/0&lt;/ip&gt;
    &lt;/networks&gt;

    &lt;!-- 数据压缩算法  --&gt;
    &lt;clickhouse_compression&gt;
        &lt;case&gt;
            &lt;min_part_size&gt;10000000000&lt;/min_part_size&gt;
            &lt;min_part_size_ratio&gt;0.01&lt;/min_part_size_ratio&gt;
            &lt;method&gt;lz4&lt;/method&gt;
        &lt;/case&gt;
    &lt;/clickhouse_compression&gt;

&lt;/yandex&gt;</code></pre> 
</blockquote> 
<p>2.2.4.2 clickhouse-03-02的配置：</p> 
<p>1. /etc/clickhouse-server02/metrika.xml</p> 
<blockquote> 
 <pre><code class="language-html">&lt;!--所有实例均使用这个集群配置，不用个性化 --&gt;
&lt;yandex&gt;

    &lt;!-- 集群配置 --&gt;
    &lt;!-- clickhouse_remote_servers所有实例配置都一样 --&gt;
     &lt;clickhouse_remote_servers&gt;
        &lt;cluster_3s_1r&gt;

            &lt;!-- 数据分片1  --&gt;
            &lt;shard&gt;
                &lt;internal_replication&gt;true&lt;/internal_replication&gt;
                &lt;replica&gt;
                    &lt;host&gt;server01&lt;/host&gt;
                    &lt;port&gt;9000&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
                &lt;replica&gt;
                    &lt;host&gt;server03&lt;/host&gt;
                    &lt;port&gt;9001&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
            &lt;/shard&gt;

            &lt;!-- 数据分片2  --&gt;
            &lt;shard&gt;
                &lt;internal_replication&gt;true&lt;/internal_replication&gt;
                &lt;replica&gt;
                    &lt;host&gt;server02&lt;/host&gt;
                    &lt;port&gt;9000&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
                &lt;replica&gt;
                    &lt;host&gt;server01&lt;/host&gt;
                    &lt;port&gt;9001&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
            &lt;/shard&gt;

            &lt;!-- 数据分片3  --&gt;
            &lt;shard&gt;
                &lt;internal_replication&gt;true&lt;/internal_replication&gt;
                &lt;replica&gt;
                    &lt;host&gt;server03&lt;/host&gt;
                    &lt;port&gt;9000&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
                &lt;replica&gt;
                    &lt;host&gt;server02&lt;/host&gt;
                    &lt;port&gt;9001&lt;/port&gt;
                    &lt;user&gt;default&lt;/user&gt;
                    &lt;password&gt;&lt;/password&gt;
                &lt;/replica&gt;
            &lt;/shard&gt;
        &lt;/cluster_3s_1r&gt;
    &lt;/clickhouse_remote_servers&gt;


    &lt;!-- ZK  --&gt;
    &lt;!-- zookeeper_servers所有实例配置都一样 --&gt;
    &lt;zookeeper-servers&gt;
        &lt;node index="1"&gt;
            &lt;host&gt;192.168.21.66&lt;/host&gt;
            &lt;port&gt;2181&lt;/port&gt;
        &lt;/node&gt;
        &lt;node index="2"&gt;
            &lt;host&gt;192.168.21.57&lt;/host&gt;
            &lt;port&gt;2181&lt;/port&gt;
        &lt;/node&gt;
        &lt;node index="3"&gt;
            &lt;host&gt;192.168.21.17&lt;/host&gt;
            &lt;port&gt;2181&lt;/port&gt;
        &lt;/node&gt;
    &lt;/zookeeper-servers&gt;
    
    &lt;!-- marcos每个实例配置不一样 分片1, 副本2--&gt;   
    &lt;macros&gt;
        &lt;layer&gt;01&lt;/layer&gt;
        &lt;shard&gt;01&lt;/shard&gt;
        &lt;replica&gt;cluster01-01-2&lt;/replica&gt;
    &lt;/macros&gt;

    &lt;networks&gt;
        &lt;ip&gt;::/0&lt;/ip&gt;
    &lt;/networks&gt;

    &lt;!-- 数据压缩算法  --&gt;
    &lt;clickhouse_compression&gt;
        &lt;case&gt;
            &lt;min_part_size&gt;10000000000&lt;/min_part_size&gt;
            &lt;min_part_size_ratio&gt;0.01&lt;/min_part_size_ratio&gt;
            &lt;method&gt;lz4&lt;/method&gt;
        &lt;/case&gt;
    &lt;/clickhouse_compression&gt;

&lt;/yandex&gt;
  </code></pre> 
</blockquote> 
<h3 id="7339-1576997689761">2.3 运行clickhouse集群</h3> 
<h4 id="1990-1576997705369">2.3.1 运行clickhouse01-01实例</h4> 
<p>登陆到server01：<em><strong>ssh server01</strong></em></p> 
<pre><code>#先删除cs01-01容器
docker rm -f cs01-01

#运行Clickhouse-server
docker run -d \
--name cs01-01 \
--ulimit nofile=262144:262144 \
--volume=/data/clickhouse01/:/var/lib/clickhouse \
--volume=/etc/clickhouse-server01/:/etc/clickhouse-server/ \
--add-host server01:192.168.21.21 \
--add-host server02:192.168.21.69 \
--add-host server03:192.168.21.6 \
--hostname $(hostname) \
-p 9000:9000 \
-p 8123:8123 \
-p 9009:9009 \
yandex/clickhouse-server</code></pre> 
<p>说明： </p> 
<blockquote> 
 <p>--add-host参数：因为我们在配置文件中使用了hostname来指代我们的服务器，为了让容器能够识别，所以需要加此参数，对应的host配置会自动被添加到容器主机的/etc/hosts里面</p> 
 <p>--hostname参数：clickhouse中的system.clusters表会显示集群信息，其中is_local的属性如果不配置hostname的话clickhouse无法识别是否是当前本机。is_local都为0的话会影响集群操作，is_local通过clickhouse-client登录到任一clickhouse-server上查看：SELECT * FROM system.clusters;</p> 
 <p>--p参数：暴露容器中的端口到本机端口中。</p> 
</blockquote> 
<h4 id="7942-1576997785128">2.3.2 运行clickhouse01-02实例</h4> 
<p>登陆到server01<em><strong>：ssh server01;</strong></em></p> 
<pre><code>docker run -d \
--name cs01-02 \
--ulimit nofile=262144:262144 \
--volume=/data/clickhouse02/:/var/lib/clickhouse \
--volume=/etc/clickhouse-server02/:/etc/clickhouse-server/ \
--add-host server01:192.168.21.21 \
--add-host server02:192.168.21.69 \
--add-host server03:192.168.21.6 \
--hostname $(hostname) \
-p 9001:9000 \
-p 8124:8123 \
-p 9010:9009 \
yandex/clickhouse-server</code></pre> 
<h4 id="4261-1576997866950">2.3.3 运行clickhouse02-01实例</h4> 
<p>登陆到server02<em><strong>：</strong><strong>ssh server02</strong></em></p> 
<pre><code>docker run -d \
--name cs02-01 \
--ulimit nofile=262144:262144 \
--volume=/data/clickhouse01/:/var/lib/clickhouse \
--volume=/etc/clickhouse-server01/:/etc/clickhouse-server/ \
--add-host server01:192.168.21.21 \
--add-host server02:192.168.21.69 \
--add-host server03:192.168.21.6 \
--hostname $(hostname) \
-p 9000:9000 \
-p 8123:8123 \
-p 9009:9009 \
yandex/clickhouse-server</code></pre> 
<h4>2.3.4 运行clickhouse02-02实例</h4> 
<p>登陆到server02：<em><strong>ssh server02</strong></em></p> 
<pre><code>docker run -d \
--name cs02-02 \
--ulimit nofile=262144:262144 \
--volume=/data/clickhouse02/:/var/lib/clickhouse \
--volume=/etc/clickhouse-server02/:/etc/clickhouse-server/ \
--add-host server01:192.168.21.21 \
--add-host server02:192.168.21.69 \
--add-host server03:192.168.21.6 \
--hostname $(hostname) \
-p 9001:9000 \
-p 8124:8123 \
-p 9010:9009 \
yandex/clickhouse-server</code></pre> 
<h4>2.3.5 运行clickhouse03-01实例</h4> 
<p>登陆到server03：<em><strong>ssh server03</strong></em></p> 
<pre><code>docker run -d \
--name cs03-01 \
--ulimit nofile=262144:262144 \
--volume=/data/clickhouse01/:/var/lib/clickhouse \
--volume=/etc/clickhouse-server01/:/etc/clickhouse-server/ \
--add-host server01:192.168.21.21 \
--add-host server02:192.168.21.69 \
--add-host server03:192.168.21.6 \
--hostname $(hostname) \
-p 9000:9000 \
-p 8123:8123 \
-p 9009:9009 \
yandex/clickhouse-server</code></pre> 
<h4>2.3.6 运行clickhouse03-02实例</h4> 
<p>登陆到server03：<em><strong>ssh server03</strong></em></p> 
<pre><code>docker run -d \
--name cs03-02 \
--ulimit nofile=262144:262144 \
--volume=/data/clickhouse02/:/var/lib/clickhouse \
--volume=/etc/clickhouse-server02/:/etc/clickhouse-server/ \
--add-host server01:192.168.21.21 \
--add-host server02:192.168.21.69 \
--hostname $(hostname) \
-p 9001:9000 \
-p 8124:8123 \
-p 9010:9009 \
yandex/clickhouse-server</code></pre> 
<h3 id="4488-1577001333953">2.4 clickhouse集群的数据操作</h3> 
<h4 id="9085-1577001604224">2.4.1 运行客户端连接clickhouse server</h4> 
<p>1）随便在哪一台实例的机器上执行如下（连不同的clickhouse实例只需要改下host和port的值即可）</p> 
<p id="9664-1577001670808"><em><strong>docker run -it \ --rm \ --add-host server01:192.168.21.21 \ --add-host server02:192.168.21.69 \ --add-host server03:192.168.21.6 \ yandex/clickhouse-client \ --host server01 \ --port 9000</strong></em></p> 
<p>2）执行下如下命令查看下配置信息：</p> 
<blockquote> 
 <p id="7832-1577005821981">#需要看下，是否和metrika.xml配置的分片和副本信息一致，如果不一致，需要check下每个clickhouse-server实例的配置</p> 
 <p>#应该is_local显示为0，且分片和副本信息都正确</p> 
 <p>SELECT * FROM system.clusters;</p> 
</blockquote> 
<h4 id="2084-1577001758272">2.4.2 创建本地复制表和分布式表</h4> 
<p>所有实例配置完上面这些之后，分别执行启动命令启动，然后所有实例都执行下面语句创建数据库：</p> 
<p>例如在实例01-02上执行：</p> 
<pre><code>#执行Clickhouse client进入Clickhouse
docker run -it --rm --add-host server01:192.168.21.21 --add-host server02:192.168.21.69 --add-host server03:192.168.21.6 \  
yandex/clickhouse-client --host server01 --port 9001
ClickHouse client version 19.17.5.18 (official build).
Connecting to server01:9001 as user default.
Connected to ClickHouse server version 19.17.5 revision 54428.

i-r9es2e0q :) CREATE DATABASE test;

CREATE DATABASE test

Ok.</code></pre> 
<h4>2.4.3 创建复制表</h4> 
<p>然后对于所有实例分别创建对应的复制表，这里测试创建一个简单的表：</p> 
<pre><code>#在clickhouse01-01 9000上执行:
docker run -it --rm --add-host server01:192.168.21.21 --add-host server02:192.168.21.69 --add-host server03:192.168.21.6 \
yandex/clickhouse-client --host server01 --port 9000
#然后执行
CREATE TABLE test.device_thing_data (
                time                     UInt64,
                user_id                 String,
                device_id                 String,
                source_id                 String,
                thing_id                   String,
                identifier                String,
                value_int32                Int32,
                value_float                Float32,
                value_double            Float64,
                value_string            String,
                value_enum              Enum8('0'=0,'1'=1,'2'=2,'3'=3,'4'=4,'5'=5,'6'=6,'7'=7,'8'=8),
                value_string_ex         String,
                value_array_string         Array(String),
                value_array_int32         Array(Int32),
                value_array_float         Array(Float32),
                value_array_double         Array(Float64),
                action_date                Date,
                action_time             DateTime
            ) Engine= ReplicatedMergeTree('/clickhouse/tables/01-01/device_thing_data','cluster01-01-1') PARTITION BY toYYYYMM(action_date) ORDER BY (user_id,device_id,thing_id,identifier,time,intHash64(time)) SAMPLE BY intHash64(time) SETTINGS index_granularity=8192
   
#在clickhouse01-02 9001上执行:
docker run -it --rm --add-host server01:192.168.21.21 --add-host server02:192.168.21.69 --add-host server03:192.168.21.6 \
yandex/clickhouse-client --host server01 --port 9001
然后执行
CREATE TABLE test.device_thing_data (
                time                     UInt64,
                user_id                 String,
                device_id                 String,
                source_id                 String,
                thing_id                   String,
                identifier                String,
                value_int32                Int32,
                value_float                Float32,
                value_double            Float64,
                value_string            String,
                value_enum              Enum8('0'=0,'1'=1,'2'=2,'3'=3,'4'=4,'5'=5,'6'=6,'7'=7,'8'=8),
                value_string_ex         String,
                value_array_string         Array(String),
                value_array_int32         Array(Int32),
                value_array_float         Array(Float32),
                value_array_double         Array(Float64),
                action_date                Date,
                action_time             DateTime
            ) Engine= ReplicatedMergeTree('/clickhouse/tables/01-02/device_thing_data','cluster01-02-2') PARTITION BY toYYYYMM(action_date) ORDER BY (user_id,device_id,thing_id,identifier,time,intHash64(time)) SAMPLE BY intHash64(time) SETTINGS index_granularity=8192
    
#在clickhouse02-01 9000上执行:
docker run -it --rm --add-host server01:192.168.21.21 --add-host server02:192.168.21.69 --add-host server03:192.168.21.6 \
yandex/clickhouse-client --host server02 --port 9000
#然后执行
CREATE TABLE test.device_thing_data (
                time                     UInt64,
                user_id                 String,
                device_id                 String,
                source_id                 String,
                thing_id                   String,
                identifier                String,
                value_int32                Int32,
                value_float                Float32,
                value_double            Float64,
                value_string            String,
                value_enum              Enum8('0'=0,'1'=1,'2'=2,'3'=3,'4'=4,'5'=5,'6'=6,'7'=7,'8'=8),
                value_string_ex         String,
                value_array_string         Array(String),
                value_array_int32         Array(Int32),
                value_array_float         Array(Float32),
                value_array_double         Array(Float64),
                action_date                Date,
                action_time             DateTime
            ) Engine= ReplicatedMergeTree('/clickhouse/tables/01-02/device_thing_data','cluster01-02-1') PARTITION BY toYYYYMM(action_date) ORDER BY (user_id,device_id,thing_id,identifier,time,intHash64(time)) SAMPLE BY intHash64(time) SETTINGS index_granularity=8192
  
#在clickhouse02-02 9001上执行:
docker run -it --rm --add-host server01:192.168.21.21 --add-host server02:192.168.21.69 --add-host server03:192.168.21.6 \
yandex/clickhouse-client --host server02 --port 9001   
#然后执行
CREATE TABLE test.device_thing_data (
                time                     UInt64,
                user_id                 String,
                device_id                 String,
                source_id                 String,
                thing_id                   String,
                identifier                String,
                value_int32                Int32,
                value_float                Float32,
                value_double            Float64,
                value_string            String,
                value_enum              Enum8('0'=0,'1'=1,'2'=2,'3'=3,'4'=4,'5'=5,'6'=6,'7'=7,'8'=8),
                value_string_ex         String,
                value_array_string         Array(String),
                value_array_int32         Array(Int32),
                value_array_float         Array(Float32),
                value_array_double         Array(Float64),
                action_date                Date,
                action_time             DateTime
            ) Engine= ReplicatedMergeTree('/clickhouse/tables/01-03/device_thing_data','cluster01-03-2') PARTITION BY toYYYYMM(action_date) ORDER BY (user_id,device_id,thing_id,identifier,time,intHash64(time)) SAMPLE BY intHash64(time) SETTINGS index_granularity=8192
     
#在clickhouse03-01 9000上执行:
docker run -it --rm --add-host server01:192.168.21.21 --add-host server02:192.168.21.69 --add-host server03:192.168.21.6 \
yandex/clickhouse-client --host server03 --port 9000
#然后执行
CREATE TABLE test.device_thing_data (
                time                     UInt64,
                user_id                 String,
                device_id                 String,
                source_id                 String,
                thing_id                   String,
                identifier                String,
                value_int32                Int32,
                value_float                Float32,
                value_double            Float64,
                value_string            String,
                value_enum              Enum8('0'=0,'1'=1,'2'=2,'3'=3,'4'=4,'5'=5,'6'=6,'7'=7,'8'=8),
                value_string_ex         String,
                value_array_string         Array(String),
                value_array_int32         Array(Int32),
                value_array_float         Array(Float32),
                value_array_double         Array(Float64),
                action_date                Date,
                action_time             DateTime
            ) Engine= ReplicatedMergeTree('/clickhouse/tables/01-03/device_thing_data','cluster01-03-1') PARTITION BY toYYYYMM(action_date) ORDER BY (user_id,device_id,thing_id,identifier,time,intHash64(time)) SAMPLE BY intHash64(time) SETTINGS index_granularity=8192
     
 
#在clickhouse03-02 9001上执行:
docker run -it --rm --add-host server01:192.168.21.21 --add-host server02:192.168.21.69 --add-host server03:192.168.21.6 \
yandex/clickhouse-client --host server03 --port 9001   
然后执行    
CREATE TABLE test.device_thing_data (
                time                     UInt64,
                user_id                 String,
                device_id                 String,
                source_id                 String,
                thing_id                   String,
                identifier                String,
                value_int32                Int32,
                value_float                Float32,
                value_double            Float64,
                value_string            String,
                value_enum              Enum8('0'=0,'1'=1,'2'=2,'3'=3,'4'=4,'5'=5,'6'=6,'7'=7,'8'=8),
                value_string_ex         String,
                value_array_string         Array(String),
                value_array_int32         Array(Int32),
                value_array_float         Array(Float32),
                value_array_double         Array(Float64),
                action_date                Date,
                action_time             DateTime
            ) Engine= ReplicatedMergeTree('/clickhouse/tables/01-01/device_thing_data','cluster01-01-2') PARTITION BY toYYYYMM(action_date) ORDER BY (user_id,device_id,thing_id,identifier,time,intHash64(time)) SAMPLE BY intHash64(time) SETTINGS index_granularity=8192</code></pre> 
<h4>2.4.3 创建分布式表（用于查询）</h4> 
<p>然后创建完上面复制表之后，可以创建分布式表，分布式表只是作为一个查询引擎，本身不存储任何数据，查询时将sql发送到所有集群分片，然后进行进行处理和聚合后将结果返回给客户端，因此clickhouse限制聚合结果大小不能大于分布式表节点的内存，当然这个一般条件下都不会超过；分布式表可以所有实例都创建，也可以只在一部分实例创建，这个和业务代码中查询的示例一致，建议设置多个，当某个节点挂掉时可以查询其他节点上的表，分布式表的建表语句如下：</p> 
<pre><code>#在clickhouse-server集群上一次性创建所有的分布式表，操作卡主了。原因不明
CREATE TABLE device_thing_data_all ON CLUSTER cluster_3s_1r AS test.device_thing_data ENGINE = Distributed(cluster_3s_1r, default,  device_thing_data, rand())

#如下这个需要每个机器上都操作一遍
CREATE TABLE device_thing_data_all  AS test.device_thing_data ENGINE = Distributed(cluster_3s_1r, test,  device_thing_data, rand())</code></pre> 
<h4>2.4.4 测试可用性</h4> 
<pre><code>#客户端连接到某个clickhouse-server实例（例如cs01-01）
#查询分布式表，此时没有查询到数据
select * from  image_label_all;

#在cs01-01上执行如下查看是否有数据
#查询本地复制表，此时没有查询到数据
select * from test.device_thing_data;


#往复制表中表里插入一条数据
INSERT INTO test.device_thing_data;  (user_id) VALUES ('1')

#由于刚才在cs01-01上插入一条数据，所以应该有数据了
select * from test.device_thing_data;

#查询分布式表,也有数据了
select * from  image_label_all;

#在cs03-02上查询复制表的数据（由于cs03-02是cs01-01的副本，所以数据被自动同步过来了），所以应该有数据了
select * from test.device_thing_data;

#在cs03-01上插入一条数据，此时应该会把数据同步到cs02-02上
INSERT INTO test.device_thing_data  (user_id) VALUES ('2')

#再次查询分布式表，此时应该查到cs03-01和cs01-01上的两条数据
select * from  image_label_all;</code></pre> 
<p>传送门：<a href="https://blog.csdn.net/ccgshigao/article/details/114249577?spm=1001.2014.3001.5501">2021最新测试资料&amp;大厂职位</a></p> 
<blockquote> 
 <p>博主：测试生财（一个不为996而996的测开码农）</p> 
 <p>座右铭：专注测试开发与自动化运维，努力读书思考写作，为内卷的人生奠定财务自由。</p> 
 <p>内容范畴：技术提升，职场杂谈，事业发展，阅读写作，投资理财，健康人生。</p> 
 <p>csdn：<a href="https://blog.csdn.net/ccgshigao">https://blog.csdn.net/ccgshigao</a></p> 
 <p>博客园：<a href="https://www.cnblogs.com/qa-freeroad/" rel="nofollow">https://www.cnblogs.com/qa-freeroad/</a></p> 
 <p>51cto：<a href="https://blog.51cto.com/14900374" rel="nofollow">https://blog.51cto.com/14900374</a></p> 
 <p>微信公众号：<strong>测试生财</strong>（定期分享独家内容和资源）</p> 
 <p><img alt="" class="medium-zoom-image" height="75" src="https://images2.imgbox.com/f7/be/3U7DQyNO_o.png" width="205"></p> 
</blockquote>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/9855262295c47f5ba1b1f038fbe3bab6/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">OBS Studio安装教程以及录制等详细配置</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d8bfa0a01fc326823661dfc1809dffed/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Java http请求设置代理</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>