<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>MiniGPT-v2：多任务视觉模型新升级 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="MiniGPT-v2：多任务视觉模型新升级" />
<meta property="og:description" content="白交 发自 凹非寺
转自量子位 | 公众号 QbitAI GPT-4V来做目标检测？网友实测：还没有准备好。
虽然检测到的类别没问题，但大多数边界框都错放了。
没关系，有人会出手！
那个抢跑GPT-4看图能力几个月的迷你GPT-4升级啦——MiniGPT-v2。
△（左边为GPT-4V生成，右边为MiniGPT-v2生成） 而且只是一句简单指令：[grounding] describe this image in detail就实现的结果。
不仅如此，还轻松处理各类视觉任务。
圈出一个物体，提示词前面加个 [identify] 可让模型直接识别出来物体的名字。
当然也可以什么都不加，直接问~
MiniGPT-v2由来自MiniGPT-4的原班人马（KAUST沙特阿卜杜拉国王科技大学）以及Meta的五位研究员共同开发。
上次MiniGPT-4刚出来就引发巨大关注，一时间服务器被挤爆，如今GItHub项目已超22000&#43;星。
此番升级，已经有网友开始用上了~
多视觉任务的通用界面 大模型作为各文本应用的通用界面，大家已经司空见惯了。受此灵感，研究团队想要建立一个可用于多种视觉任务的统一界面，比如图像描述、视觉问题解答等。
「如何在单一模型的条件下，使用简单多模态指令来高效完成各类任务？」成为团队需要解决的难题。
简单来说，MiniGPT-v2由三个部分组成：视觉主干、线性层和大型语言模型。
该模型以ViT视觉主干为基础，所有训练阶段都保持不变。从ViT中归纳出四个相邻的视觉输出标记，并通过线性层将它们投影到 LLaMA-2语言模型空间中。
团队建议在训练模型为不同任务使用独特的标识符，这样一来大模型就能轻松分辨出每个任务指令，还能提高每个任务的学习效率。
训练主要分为三个阶段：预训练——多任务训练——多模式指令调整。
最终，MiniGPT-v2 在许多视觉问题解答和视觉接地基准测试中，成绩都优于其他视觉语言通用模型。
最终这个模型可以完成多种视觉任务，比如目标对象描述、视觉定位、图像说明、视觉问题解答以及从给定的输入文本中直接解析图片对象。
感兴趣的朋友，可戳下方Demo链接体验：
https://minigpt-v2.github.io/
https://huggingface.co/spaces/Vision-CAIR/MiniGPT-v2
论文链接：https://arxiv.o‍rg/abs/2310.09478
GitHub链接：https://github.com/Vision-CAIR/MiniGPT-4
参考链接：https://twitter.com/leoyerrrr" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/9d414653edfa5b474a37cd416016e596/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-19T12:33:08+08:00" />
<meta property="article:modified_time" content="2023-10-19T12:33:08+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">MiniGPT-v2：多任务视觉模型新升级</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <h6>白交 发自 凹非寺<br>转自量子位 | 公众号 QbitAI</h6> 
 <p style="text-align:left;">GPT-4V来做目标检测？网友实测：还没有准备好。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/bf/3e/3uVWDUvD_o.png" alt="95e36020f416a5b9161f6b08e2c03834.png"></p> 
 <p style="text-align:left;">虽然检测到的类别没问题，但大多数边界框都错放了。</p> 
 <p style="text-align:left;">没关系，有人会出手！</p> 
 <p style="text-align:left;">那个抢跑GPT-4看图能力几个月的迷你GPT-4升级啦——<strong>MiniGPT-v2</strong>。</p> 
 <h6><strong><img src="https://images2.imgbox.com/46/48/X2uiPuNb_o.png" alt="abb908748e577e8e6075e160a223a5aa.png"></strong></h6> 
 <h6><strong>△</strong>（左边为GPT-4V生成，右边为MiniGPT-v2生成）</h6> 
 <p style="text-align:left;">而且只是一句简单指令：<strong>[grounding] describe this image in detail</strong>就实现的结果。</p> 
 <p style="text-align:left;">不仅如此，还轻松处理各类视觉任务。</p> 
 <p style="text-align:left;">圈出一个物体，提示词前面加个 [identify] 可让模型直接识别出来物体的名字。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/4c/96/1A4rmZkp_o.png" alt="47fb4c36b17dd32dcdc087ba66ec8334.png"></p> 
 <p style="text-align:left;">当然也可以什么都不加，直接问~</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/40/a3/o2eIrtkq_o.png" alt="e0abe0608ff1fcfdf3af5c425c295d40.png"></p> 
 <p style="text-align:left;">MiniGPT-v2由来自MiniGPT-4的原班人马（KAUST沙特阿卜杜拉国王科技大学）以及Meta的五位研究员共同开发。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/ca/43/NzGjV39c_o.png" alt="9e31215103cc47839ef98bd5b449ca53.png"><br></p> 
 <p style="text-align:left;">上次MiniGPT-4刚出来就引发巨大关注，一时间服务器被挤爆，如今GItHub项目已超22000+星。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/a3/2d/XakjJySV_o.png" alt="c613bc18dd1e2b580ec4dc7a1829ecef.png"></p> 
 <p style="text-align:left;">此番升级，已经有网友开始用上了~</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/c5/dc/ezKR3dTg_o.jpg" alt="dd21918829394d715c6913e33e6c66d9.jpeg"></p> 
 <h3>多视觉任务的通用界面</h3> 
 <p style="text-align:left;">大模型作为各文本应用的通用界面，大家已经司空见惯了。受此灵感，研究团队想要建立一个可用于多种视觉任务的统一界面，比如图像描述、视觉问题解答等。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/a0/5d/OTcv75RC_o.png" alt="029b4d403502eff994abbe3fac1b9b3d.png"></p> 
 <p style="text-align:left;">「如何在单一模型的条件下，使用简单多模态指令来高效完成各类任务？」成为团队需要解决的难题。</p> 
 <p style="text-align:left;">简单来说，MiniGPT-v2由三个部分组成：视觉主干、线性层和大型语言模型。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/4b/5b/xv0R3nt9_o.png" alt="d6cf650af423e21c2519894f057018de.png"></p> 
 <p style="text-align:left;">该模型以ViT视觉主干为基础，所有训练阶段都保持不变。从ViT中归纳出四个相邻的视觉输出标记，并通过线性层将它们投影到 LLaMA-2语言模型空间中。</p> 
 <p style="text-align:left;">团队建议在训练模型为不同任务使用独特的标识符，这样一来大模型就能轻松分辨出每个任务指令，还能提高每个任务的学习效率。</p> 
 <p style="text-align:left;">训练主要分为三个阶段：预训练——多任务训练——多模式指令调整。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/6e/2b/r9JNWcXP_o.png" alt="5bbdfe8ca2c9b1c5f4fb89aaa09c31c9.png"></p> 
 <p style="text-align:left;">最终，MiniGPT-v2 在许多视觉问题解答和视觉接地基准测试中，成绩都优于其他视觉语言通用模型。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/cb/4f/Tv6oCcOG_o.png" alt="79fa93d01abb20c32a2b3a941c88854d.png"></p> 
 <p style="text-align:left;">最终这个模型可以完成多种视觉任务，比如目标对象描述、视觉定位、图像说明、视觉问题解答以及从给定的输入文本中直接解析图片对象。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/0d/5f/M1D7eZFU_o.png" alt="199b446e64909b409e6714d9fb0a42b8.png"></p> 
 <p style="text-align:left;">感兴趣的朋友，可戳下方Demo链接体验：</p> 
 <p style="text-align:left;">https://minigpt-v2.github.io/<br>https://huggingface.co/spaces/Vision-CAIR/MiniGPT-v2</p> 
 <p style="text-align:left;">论文链接：https://arxiv.o‍rg/abs/2310.09478</p> 
 <p style="text-align:left;">GitHub链接：https://github.com/Vision-CAIR/MiniGPT-4</p> 
 <p style="text-align:left;">参考链接：https://twitter.com/leoyerrrr</p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/94e115c0fe5122fdc04eda06d510505d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">eCongnition 图像分割分类初学者保姆教程含多种工具介绍</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/dd609180792e35824b395370d0cf8ae2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">FusionCompute 6.5.1重置Web端登录密码</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>