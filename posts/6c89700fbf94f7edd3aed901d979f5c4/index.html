<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>PyTorch教程（3）RNN - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="PyTorch教程（3）RNN" />
<meta property="og:description" content="如何使用MNIST数据集建立递归神经网络?
递归神经网络(RNN)被认为是一种记忆网络。我们使用epoch为1，每次使用64个样品的批量大小来建立输入和输出之间的联系。利用RNN模型，我们可以预测图像中存在的数字。
让我们看看下面的例子。递归神经网络在输入层取一个向量序列，在输出层产生一个向量序列。信息序列在递归层中通过内部状态转换进行处理。有时输出值长期依赖于过去的历史值。这是RNN模型的另一种变体:长短期记忆(LSTM)模型。这适用于以顺序方式使用信息的任何类型的领域 。例如，在一个时间序列中，当前股票价格由历史股票价格决定，依赖关系可以是短的或长的。类似地，使用文本输入向量的长范围和短范围的上下文预测。还有其他行业用例，如噪声分类，其中噪声也是一个信息序列。
分类问题 下面的代码片段解释了使用PyTorch模块执行RNN模型。
权值有三组:U、V、W。权值向量集合，用W表示，表示网络中存储单元之间的信息传递，显示隐藏状态之间的通信。RNN使用Word2vec表示的嵌入层。例如，如果你有20,000个单词和1000个隐藏单位，矩阵的嵌入层大小为20,000×1000。新的表示被传递给LSTM，LSTM输出经过sigmoid函数输出。
import torch import torchvision.datasets as dsets import torchvision.transforms as transforms import matplotlib.pyplot as plt import torch.nn as nn torch.manual_seed(1) # 超参数 EPOCH=1 BATCH_SIZE=64 TIME_STEP=28 INPUT_SIZE=28 LR=0.01 DOWNLOAD=True RNN模型具有超参数，如迭代次数(EPOCH);批大小取决于单个机器中可用的内存;记忆信息序列的时间步长;输入大小和学习率。这些值的选择是指示性的;我们不能在其他用例中依赖它们。超参数的取值选择是一个迭代过程;您可以选择多个参数并决定哪个参数有效，或者对模型进行并行训练并决定哪个参数有效。
train_data = dsets.MNIST(root=&#34;./mnist&#34;, train=True, transform=transforms.ToTensor(),download=DOWNLOAD) print(train_data.train_data.size()) print(train_data.train_labels.size()) plt.imshow(train_data.train_data[0].numpy(), cmap=&#34;gray&#34;) plt.title(&#34;%i&#34;%train_data.train_labels[0]) plt.show() 前面的脚本显示了示例图像数据集的样子。为了训练深度学习模型，我们需要将整个训练数据集转换成小批量，这有助于我们平均模型的最终精度。通过数据加载器，我们可以加载训练数据，并准备小批量数据。在小批量中进行shuffle选择的目的是确保模型捕获实际数据集中的所有变化。
# 数据加载器返回训练数据的mini-batch train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True) # 选择测试集中的前2000个元素 test_data = dsets.MNIST(root=&#34;./mnist&#34;, train=False, transform=transforms.ToTensor()) # shape(2000, 28, 28) test_x = test_data.test_data.type(torch.FloatTensor)[:2000]/255. # convert to numpy test_y = test_data." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/6c89700fbf94f7edd3aed901d979f5c4/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-08-03T13:33:59+08:00" />
<meta property="article:modified_time" content="2021-08-03T13:33:59+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">PyTorch教程（3）RNN</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>如何使用MNIST数据集建立递归神经网络?<br> 递归神经网络(RNN)被认为是一种记忆网络。我们使用epoch为1，每次使用64个样品的批量大小来建立输入和输出之间的联系。利用RNN模型，我们可以预测图像中存在的数字。</p> 
<p>让我们看看下面的例子。递归神经网络在输入层取一个向量序列，在输出层产生一个向量序列。信息序列在递归层中通过内部状态转换进行处理。有时输出值长期依赖于过去的历史值。这是RNN模型的另一种变体:长短期记忆(LSTM)模型。这适用于以顺序方式使用信息的任何类型的领域 。例如，在一个时间序列中，当前股票价格由历史股票价格决定，依赖关系可以是短的或长的。类似地，使用文本输入向量的长范围和短范围的上下文预测。还有其他行业用例，如噪声分类，其中噪声也是一个信息序列。</p> 
<h3><a id="_5"></a>分类问题</h3> 
<p>下面的代码片段解释了使用PyTorch模块执行RNN模型。</p> 
<p>权值有三组:U、V、W。权值向量集合，用W表示，表示网络中存储单元之间的信息传递，显示隐藏状态之间的通信。RNN使用Word2vec表示的嵌入层。例如，如果你有20,000个单词和1000个隐藏单位，矩阵的嵌入层大小为20,000×1000。新的表示被传递给LSTM，LSTM输出经过sigmoid函数输出。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>datasets <span class="token keyword">as</span> dsets
<span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">as</span> transforms
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token comment"># 超参数</span>
EPOCH<span class="token operator">=</span><span class="token number">1</span>
BATCH_SIZE<span class="token operator">=</span><span class="token number">64</span>
TIME_STEP<span class="token operator">=</span><span class="token number">28</span>
INPUT_SIZE<span class="token operator">=</span><span class="token number">28</span>
LR<span class="token operator">=</span><span class="token number">0.01</span>
DOWNLOAD<span class="token operator">=</span><span class="token boolean">True</span>
</code></pre> 
<p>RNN模型具有超参数，如迭代次数(EPOCH);批大小取决于单个机器中可用的内存;记忆信息序列的时间步长;输入大小和学习率。这些值的选择是指示性的;我们不能在其他用例中依赖它们。超参数的取值选择是一个迭代过程;您可以选择多个参数并决定哪个参数有效，或者对模型进行并行训练并决定哪个参数有效。</p> 
<pre><code class="prism language-python">train_data <span class="token operator">=</span> dsets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">"./mnist"</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
 			transform<span class="token operator">=</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>download<span class="token operator">=</span>DOWNLOAD<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>train_data<span class="token punctuation">.</span>train_data<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>train_data<span class="token punctuation">.</span>train_labels<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>train_data<span class="token punctuation">.</span>train_data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cmap<span class="token operator">=</span><span class="token string">"gray"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"%i"</span><span class="token operator">%</span>train_data<span class="token punctuation">.</span>train_labels<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/ab/6b/fv7BLtoW_o.png" alt="在这里插入图片描述"><br> 前面的脚本显示了示例图像数据集的样子。为了训练深度学习模型，我们需要将整个训练数据集转换成小批量，这有助于我们平均模型的最终精度。通过数据加载器，我们可以加载训练数据，并准备小批量数据。在小批量中进行shuffle选择的目的是确保模型捕获实际数据集中的所有变化。</p> 
<pre><code class="prism language-python"><span class="token comment"># 数据加载器返回训练数据的mini-batch</span>
train_loader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token operator">=</span>train_data<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>BATCH_SIZE<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token comment"># 选择测试集中的前2000个元素</span>
test_data <span class="token operator">=</span> dsets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">"./mnist"</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
 			transform<span class="token operator">=</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># shape(2000, 28, 28)</span>
test_x <span class="token operator">=</span> test_data<span class="token punctuation">.</span>test_data<span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2000</span><span class="token punctuation">]</span><span class="token operator">/</span><span class="token number">255</span><span class="token punctuation">.</span>
<span class="token comment"># convert to numpy</span>
test_y <span class="token operator">=</span> test_data<span class="token punctuation">.</span>test_labels<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2000</span><span class="token punctuation">]</span>
</code></pre> 
<p>上述脚本准备训练数据集。使用标志train=False捕获测试数据。使用测试数据中的2000个随机样本进行模型测试。测试标签向量以NumPy数组格式表示。</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">RNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
		<span class="token builtin">super</span><span class="token punctuation">(</span>RNN<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
		self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>  <span class="token comment"># 如果使用nn.RNN(),很难学习</span>
				input_size<span class="token operator">=</span>INPUT_SIZE<span class="token punctuation">,</span>  <span class="token comment"># </span>
				hidden_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token comment"># 隐藏神经元</span>
				num_layers<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token comment"># rnn 层的数目</span>
				batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token comment"># input &amp; output中batc size作为第一维</span>
				<span class="token punctuation">)</span>
		self<span class="token punctuation">.</span>out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
	<span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
		<span class="token comment"># x形状为(batch, time_step, input_size)</span>
		<span class="token comment"># r_out形状为(batch, time_step, output_size)</span>
		<span class="token comment"># h_n形状(n_layers, batch, hidden_size)</span>
		<span class="token comment"># h_c形状(n_layers, batch, hidden_size)</span>
		<span class="token comment"># None表示0初始化隐藏层</span>
		r_out<span class="token punctuation">,</span> <span class="token punctuation">(</span>h_n<span class="token punctuation">,</span> h_c<span class="token punctuation">)</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>
		<span class="token comment"># choose r_out at the last time step</span>
		out <span class="token operator">=</span> self<span class="token punctuation">.</span>out<span class="token punctuation">(</span>r_out<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
		<span class="token keyword">return</span> out
</code></pre> 
<p>在之前的RNN课程中，我们训练了一个LSTM网络，它被证明对长时间保持记忆有效，从而有助于学习。如果我们使用nn.RNN()模型，它很难学习参数，因为RNN的普通实现不能长时间保持或记住信息。在LSTM网络中，图像的宽度被认为是输入的大小，隐藏的大小被决定为隐藏层神经元的数量，num_layers表示网络中RNN层的数量。</p> 
<p>RNN模块，在LSTM模块中，产生的输出为64×10的向量大小，因为输出层的数字被分类为0到9。最后一个前向函数说明了如何在RNN网络中进行前向传播。</p> 
<p>下面的脚本显示了如何在RNN类下处理LSTM模型。在LSTM函数中，输入长度为28，隐层神经元数为64，从隐层的64个神经元到输出的10个神经元。</p> 
<pre><code class="prism language-python">rnn <span class="token operator">=</span> RNN<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>rnn<span class="token punctuation">)</span>
<span class="token comment"># RNN(</span>
<span class="token comment">#   (rnn): LSTM(28, 64, batch_first=True)</span>
<span class="token comment">#   (out): Linear(in_features=64, out_features=10, bias=True)</span>
<span class="token comment"># )</span>
optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>rnn<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr<span class="token operator">=</span>LR<span class="token punctuation">)</span>
loss_func <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 标签你不是独热编码</span>
</code></pre> 
<p>为了优化所有RNN参数，我们使用Adam优化器。在函数内部，我们也使用了学习率。本例中使用的损失函数是交叉熵损失函数。我们需要提供多个epoch来获得最好的参数。</p> 
<p>在下面的脚本中，我们打印了训练损失和测试准确性。经过一个epoch后，测试准确率提高到95%，训练损耗降低到0.19。</p> 
<pre><code class="prism language-python"><span class="token comment"># 训练和测试</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>EPOCH<span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">for</span> step<span class="token punctuation">,</span> <span class="token punctuation">(</span>x<span class="token punctuation">,</span>y<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>
		b_x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span>
		b_y <span class="token operator">=</span> y
		output <span class="token operator">=</span> rnn<span class="token punctuation">(</span>b_x<span class="token punctuation">)</span>
		loss <span class="token operator">=</span> loss_func<span class="token punctuation">(</span>output<span class="token punctuation">,</span> b_y<span class="token punctuation">)</span>
		optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
		loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
		optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
		<span class="token keyword">if</span> step <span class="token operator">%</span> <span class="token number">50</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
			test_output<span class="token operator">=</span>rnn<span class="token punctuation">(</span>test_x<span class="token punctuation">)</span>
			pred_y <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>test_output<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span>
			accuracy <span class="token operator">=</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>pred_y<span class="token operator">==</span>test_y<span class="token punctuation">)</span><span class="token operator">/</span><span class="token builtin">float</span><span class="token punctuation">(</span>test_y<span class="token punctuation">.</span>size<span class="token punctuation">)</span>
			<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Epoch:"</span><span class="token punctuation">,</span> epoch<span class="token punctuation">,</span> <span class="token string">"| train loss: %.4f"</span> <span class="token operator">%</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
									<span class="token string">"|test accuracy: %.4f"</span><span class="token operator">%</span> accuracy<span class="token punctuation">)</span>
<span class="token comment"># Epoch: 0 | train loss: 2.3178 |test accuracy: 0.0995</span>
<span class="token comment"># Epoch: 0 | train loss: 1.1102 |test accuracy: 0.5300</span>
<span class="token comment"># Epoch: 0 | train loss: 0.7031 |test accuracy: 0.7100</span>
<span class="token comment"># Epoch: 0 | train loss: 0.6014 |test accuracy: 0.7930</span>
<span class="token comment"># Epoch: 0 | train loss: 0.7127 |test accuracy: 0.8575</span>
<span class="token comment"># Epoch: 0 | train loss: 0.3254 |test accuracy: 0.8860</span>
<span class="token comment"># Epoch: 0 | train loss: 0.3258 |test accuracy: 0.8835</span>
<span class="token comment"># Epoch: 0 | train loss: 0.3760 |test accuracy: 0.9280</span>
<span class="token comment"># Epoch: 0 | train loss: 0.2738 |test accuracy: 0.9330</span>
<span class="token comment"># Epoch: 0 | train loss: 0.1623 |test accuracy: 0.9240</span>
<span class="token comment"># Epoch: 0 | train loss: 0.1836 |test accuracy: 0.9240</span>
<span class="token comment"># Epoch: 0 | train loss: 0.3307 |test accuracy: 0.9220</span>
<span class="token comment"># Epoch: 0 | train loss: 0.1129 |test accuracy: 0.9205</span>
<span class="token comment"># Epoch: 0 | train loss: 0.0855 |test accuracy: 0.9440</span>
<span class="token comment"># Epoch: 0 | train loss: 0.1522 |test accuracy: 0.9450</span>
<span class="token comment"># Epoch: 0 | train loss: 0.2400 |test accuracy: 0.9430</span>
<span class="token comment"># Epoch: 0 | train loss: 0.2307 |test accuracy: 0.9365</span>
<span class="token comment"># Epoch: 0 | train loss: 0.1162 |test accuracy: 0.9435</span>
<span class="token comment"># Epoch: 0 | train loss: 0.1968 |test accuracy: 0.9515</span>
</code></pre> 
<p>一旦模型训练完毕，下一步就是使用RNN模型进行预测。然后我们比较实际输出和实际输出，以评估模型的执行情况。</p> 
<pre><code class="prism language-python">test_output <span class="token operator">=</span> rnn<span class="token punctuation">(</span>test_x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
pred_y <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>test_output<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>pred_y<span class="token punctuation">,</span> <span class="token string">"prediction number"</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>test_y<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">"real number"</span><span class="token punctuation">)</span>
<span class="token comment"># [7 2 1 0 4 1 4 9 5 9] prediction number</span>
<span class="token comment"># [7 2 1 0 4 1 4 9 5 9] real number</span>
</code></pre> 
<h3><a id="_141"></a>回归问题</h3> 
<p>我们如何为基于回归的问题建立一个递归神经网络?<br> 回归模型需要一个目标函数和一个特征集，然后再用一个函数来建立输入和输出之间的关系。在本例中，我们将使用递归神经网络(RNN)进行回归任务。回归问题似乎很简单;它们确实效果最好，但仅限于显示明确线性关系的数据。当预测输入和输出之间的非线性关系时，它们是相当复杂的。</p> 
<p>让我们看一下下面的示例，它显示了输入和输出数据之间的非线性循环模式。在前面的教程中，我们研究了RNN用于分类相关问题的一般示例，预测输入图像的类别。然而，在回归中，RNN的结构会改变，因为目标是预测真实值输出。输出层将有一个神经元处于与回归相关的问题中。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token comment"># 超参数</span>
TIME_STEP<span class="token operator">=</span><span class="token number">10</span>
INPUT_SIZE<span class="token operator">=</span><span class="token number">1</span>
LR<span class="token operator">=</span><span class="token number">0.02</span>
</code></pre> 
<p>下面的脚本显示了一些样本序列，其中目标cos函数近似于sin函数.</p> 
<pre><code class="prism language-python"><span class="token comment"># 显示数据</span>
steps <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>pi<span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
x_np <span class="token operator">=</span> np<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>steps<span class="token punctuation">)</span>
y_np <span class="token operator">=</span> np<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>steps<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>steps<span class="token punctuation">,</span> y_np<span class="token punctuation">,</span> <span class="token string">"r-"</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">"target (cos)"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>steps<span class="token punctuation">,</span> x_np<span class="token punctuation">,</span> <span class="token string">"b-"</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">"input (sin)"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>loc<span class="token operator">=</span><span class="token string">"best"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/55/a1/wDj34RBO_o.png" alt="在这里插入图片描述"><br> 让我们看看下面的例子。PyTorch库中的神经网络模块包含RNN函数。在下面的脚本中，我们使用了输入矩阵的大小，隐含层神经元的数量，以及网络中隐藏层的数量。</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">RNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
		<span class="token builtin">super</span><span class="token punctuation">(</span>RNN<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
		self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>  <span class="token comment"># 如果使用nn.RNN(),很难学习</span>
				input_size<span class="token operator">=</span>INPUT_SIZE<span class="token punctuation">,</span>  <span class="token comment"># </span>
				hidden_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token comment"># 隐藏神经元</span>
				num_layers<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token comment"># rnn 层的数目</span>
				batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token comment"># input &amp; output中batc size作为第一维</span>
				<span class="token comment"># (batch, time_step, input_size)</span>
				<span class="token punctuation">)</span>
		self<span class="token punctuation">.</span>out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
	<span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> h_state<span class="token punctuation">)</span><span class="token punctuation">:</span>
		<span class="token comment"># x形状为(batch, time_step, input_size)</span>
		<span class="token comment"># h_state形状(n_layers, batch, hidden_size)</span>
		<span class="token comment"># r_out形状为(batch, time_step, hidden_size)</span>
		r_out<span class="token punctuation">,</span> h_state <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>x<span class="token punctuation">,</span> h_state<span class="token punctuation">)</span>
		
		outs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment"># save all predictions</span>
		<span class="token keyword">for</span> time_step <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>r_out<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
			<span class="token comment"># 为每一个time step计算output</span>
			outs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>out<span class="token punctuation">(</span>r_out<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span>time_step<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
		<span class="token keyword">return</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>outs<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> h_state
</code></pre> 
<p>在创建RNN类函数之后，我们需要提供优化函数，即Adam，这次的损失函数是均方损失函数。由于目标是对一个连续变量进行预测，所以我们在优化层使用MSELoss函数。</p> 
<pre><code class="prism language-python">rnn <span class="token operator">=</span> RNN<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>rnn<span class="token punctuation">)</span>
<span class="token comment"># RNN(</span>
<span class="token comment">#   (rnn): LSTM(28, 32, batch_first=True)</span>
<span class="token comment">#   (out): Linear(in_features=32, out_features=1, bias=True)</span>
<span class="token comment"># )</span>
optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>rnn<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>LR<span class="token punctuation">)</span>
loss_func <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
h_state <span class="token operator">=</span> <span class="token boolean">None</span>
plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ion<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> step <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
	start<span class="token punctuation">,</span> end <span class="token operator">=</span> step <span class="token operator">*</span> np<span class="token punctuation">.</span>pi<span class="token punctuation">,</span> <span class="token punctuation">(</span>step<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">*</span>np<span class="token punctuation">.</span>pi
	<span class="token comment"># 使用sin预测cos</span>
	steps <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span>start<span class="token punctuation">,</span> end<span class="token punctuation">,</span> TIME_STEP<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
	x_np <span class="token operator">=</span> np<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>steps<span class="token punctuation">)</span>
	y_np <span class="token operator">=</span> np<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>steps<span class="token punctuation">)</span>
	x <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>x_np<span class="token punctuation">[</span>np<span class="token punctuation">.</span>newaxis<span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span>np<span class="token punctuation">.</span>newaxis<span class="token punctuation">]</span><span class="token punctuation">)</span>
	<span class="token comment"># shape(batch, time_step, input_size)</span>
	y <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>y_np<span class="token punctuation">[</span>np<span class="token punctuation">.</span>newaxis<span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span>np<span class="token punctuation">.</span>newaxis<span class="token punctuation">]</span><span class="token punctuation">)</span>
	optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 清除梯度值</span>
	prediction<span class="token punctuation">,</span>_ <span class="token operator">=</span> rnn<span class="token punctuation">(</span>x<span class="token punctuation">,</span> h_state<span class="token punctuation">)</span>
	loss <span class="token operator">=</span> loss_func<span class="token punctuation">(</span>prediction<span class="token punctuation">,</span> y<span class="token punctuation">)</span>  <span class="token comment"># cross entropy loss</span>
	loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 反向传播，计算梯度</span>
	optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 更新参数</span>
	<span class="token comment"># 绘图</span>
	plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>steps<span class="token punctuation">,</span> y_np<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"r-"</span><span class="token punctuation">)</span>
	plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>steps<span class="token punctuation">,</span> prediction<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"b-"</span><span class="token punctuation">)</span>
	plt<span class="token punctuation">.</span>draw<span class="token punctuation">(</span><span class="token punctuation">)</span>
	plt<span class="token punctuation">.</span>pause<span class="token punctuation">(</span><span class="token number">0.05</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/00/32/jvKbVan0_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4ba2218b0ac390dfcbecc0b82eca0be9/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">服务器怎么使用融云发图片消息,融云发送文件和图片消息</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0b02a0f4a3cda552859374f51b6de643/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">求生之路怎么显示所有服务器,求生之路2怎么屏蔽rpg服务器 求生之路2屏蔽rpg服务器方法_游侠网...</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>