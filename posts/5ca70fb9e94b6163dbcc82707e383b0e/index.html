<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>基于树莓派4B的YOLOv5-Lite目标检测的移植与部署（含训练教程） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="基于树莓派4B的YOLOv5-Lite目标检测的移植与部署（含训练教程）" />
<meta property="og:description" content="前言：本文为手把手教学树莓派4B项目——YOLOv5-Lite目标检测，本次项目采用树莓派4B（Cortex-A72）作为核心 CPU 进行部署。该篇博客算是深度学习理论的初步实战，选择的网络模型为 YOLOv5 模型的变种 YOLOv5-Lite 模型。YOLOv5-Lite 与 YOLOv5 相比虽然牺牲了部分网络模型精度，但是缺极大的提升了模型的推理速度，该模型属性将更适合实战部署使用。该项目的实践将帮助大家成功进入 “嵌入式AI” 领域，后续将在该项目上加入嵌入式的 “传统控制” 属性，读者朋友可以期待一下！（文末有代码开源！）
硬件实物图：
效果图：
一、YOLOv5-Lite概述 1.1 YOLOv5概述 YOLOv5 网络模型算是 YOLO 系列迭代后特别经典的一代网络模型，作者为：Glenn Jocher。部分学者可能认为YOlOv5的创新性不足，其是否称得上 YOLOv5 而议论纷纷。作者认为 YOLOv5 可以算是对 YOLO 系列之前的一次集大成者的总结和突破，其属于非常优秀经典的网络模型框架，各种网络结构和 trick 是非常值得借鉴的！
代码地址：ultralytics/yolov5: YOLOv5 🚀 in PyTorch &gt; ONNX &gt; CoreML &gt; TFLite (github.com)
Yolov5 官方代码中，给出的目标检测网络中一共有4个版本，分别是Yolov5s、Yolov5m、Yolov5l、Yolov5x四个模型。作者仅以 Yolov5s 的网络结构为对象进行讲解，其他版本的读者朋友可以参考其他博客！
Yolov5s 网络是 Yolov5 系列中深度最小，特征图的宽度最小的网络。后面的3种都是在此基础上不断加深，不断加宽。Yolov5 的网络结构图如下（源于江大白大佬的结构图）：
上图即 Yolov5 的网络结构图，可以看出，还是分为输入端、Backbone、Neck、Prediction四个部分。
（1）输入端：Mosaic数据增强、自适应锚框计算、自适应图片缩放
（2）Backbone：Focus结构，CSP结构
（3）Neck：FPN&#43;PAN结构
（4）Prediction：GIOU_Loss
上述四部分都是属于如今很常见的模块与Trick了，受限于博客篇幅，各部分的详解就不与读者朋友好好分析和交流了。建议对 YOLO 系列陌生的朋友可以去好好看看其他博主的博客亦或是去B站看视频教学！
下面丢上 Yolov5 作者的算法性能测试图：
到现在为止，Yolov5 已经更新迭代到 v7.0 版本了，科研学术圈以 Yolov5 为基础框架进行魔改的论文数不胜数。通过上述作者的概述读者朋友可能对 Yolov5 有了一个大致的了解，不难发现 Yolov5 是非常优秀的神经网络模型。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/5ca70fb9e94b6163dbcc82707e383b0e/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-27T12:45:32+08:00" />
<meta property="article:modified_time" content="2023-06-27T12:45:32+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">基于树莓派4B的YOLOv5-Lite目标检测的移植与部署（含训练教程）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><strong>前言：</strong>本文为手把手教学<span style="color:#fe2c24;"><strong>树莓派4B项目</strong></span>——<span style="color:#fe2c24;"><strong>YOLOv5-Lite目标检测</strong></span>，本次项目采用树莓派4B（<span style="color:#fe2c24;"><strong>Cortex-A72</strong></span>）作为核心 CPU 进行部署。该篇博客算是深度学习理论的初步实战，选择的网络模型为 <strong>YOLOv5 </strong>模型的变种 <span style="color:#fe2c24;"><strong>YOLOv5-Lite</strong></span> 模型。<span style="color:#fe2c24;"><strong>YOLOv5-Lite </strong></span>与 <strong>YOLOv5</strong> 相比虽然牺牲了部分网络模型精度，但是缺极大的提升了模型的<strong><span style="color:#fe2c24;">推理速度</span></strong>，该模型属性将更适合实战部署使用。该项目的实践将帮助大家成功进入 <span style="color:#fe2c24;"><strong>“嵌入式AI”</strong></span> 领域，后续将在该项目上加入嵌入式的 <strong>“传统控制”</strong> 属性，读者朋友可以期待一下！（<strong>文末有代码开源！</strong>）</p> 
<p><strong>硬件实物图：</strong></p> 
<p class="img-center"><img alt="" height="669" src="https://images2.imgbox.com/a2/e4/8BljpQJ4_o.png" width="1080"></p> 
<p><img alt="" height="719" src="https://images2.imgbox.com/91/36/VgVdD3qb_o.png" width="1098"></p> 
<p><strong>效果图：</strong></p> 
<p class="img-center"><img alt="" height="661" src="https://images2.imgbox.com/b7/44/kkTcChbw_o.png" width="1055"></p> 
<p class="img-center"><img alt="" height="661" src="https://images2.imgbox.com/01/ae/FS5RvJOQ_o.png" width="1055"></p> 
<h2>一、YOLOv5-Lite概述</h2> 
<h3>1.1 YOLOv5概述</h3> 
<blockquote> 
 <p><strong>YOLOv5</strong> 网络模型算是 <strong>YOLO</strong> 系列迭代后特别经典的一代网络模型，作者为：<strong>Glenn Jocher</strong>。部分学者可能认为<strong>YOlOv5</strong>的<strong>创新性不足</strong>，其是否称得上 <strong>YOLOv5 </strong>而议论纷纷。作者认为 <strong>YOLOv5</strong> 可以算是对 <strong>YOLO</strong> 系列之前的一次<strong>集大成者的总结</strong>和<strong>突破</strong>，其属于非常优秀经典的网络模型框架，各种<strong>网络结构</strong>和 <strong>trick</strong> 是非常值得借鉴的！</p> 
 <p><strong>代码地址：</strong><a href="https://github.com/ultralytics/yolov5" title="ultralytics/yolov5: YOLOv5 🚀 in PyTorch &gt; ONNX &gt; CoreML &gt; TFLite (github.com)">ultralytics/yolov5: YOLOv5 🚀 in PyTorch &gt; ONNX &gt; CoreML &gt; TFLite (github.com)</a></p> 
</blockquote> 
<p><strong>Yolov5</strong> 官方代码中，给出的目标检测网络中一共有4个版本，分别是<strong>Yolov5s</strong>、<strong>Yolov5m</strong>、<strong>Yolov5l</strong>、<strong>Yolov5x</strong>四个模型。作者仅以 <strong>Yolov5s</strong> 的网络结构为对象进行讲解，其他版本的读者朋友可以参考其他博客！</p> 
<p><strong>Yolov5s</strong> 网络是 <strong>Yolov5</strong> 系列中<strong>深度最小</strong>，特征图的<strong>宽度最小</strong>的网络。后面的3种都是在此基础上不断加深，不断加宽。Yolov5 的网络结构图如下（源于江大白大佬的结构图）：</p> 
<p><img alt="" height="1198" src="https://images2.imgbox.com/e1/32/7sDr5CIm_o.png" width="1200"></p> 
<p>上图即 <strong>Yolov5 </strong>的网络结构图，可以看出，还是分为<strong>输入端、Backbone、Neck、Prediction</strong>四个部分。</p> 
<p><strong>（1）输入端：</strong>Mosaic数据增强、自适应锚框计算、自适应图片缩放<br><strong>（2）Backbone：</strong>Focus结构，CSP结构<br><strong>（3）Neck：</strong>FPN+PAN结构<br><strong>（4）Prediction：</strong>GIOU_Loss</p> 
<blockquote> 
 <p>上述四部分都是属于如今很常见的模块与Trick了，受限于博客篇幅，各部分的详解就不与读者朋友好好分析和交流了。建议对 YOLO 系列陌生的朋友可以去好好看看其他博主的博客亦或是去B站看视频教学！</p> 
</blockquote> 
<p>下面丢上 <strong>Yolov5</strong> 作者的算法性能测试图：</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/cc/75/Nf8SitI8_o.png" width="1200"></p> 
<p>到现在为止，<strong>Yolov5</strong> 已经更新迭代到 <strong>v7.0</strong> 版本了，科研学术圈以 <strong>Yolov5</strong> 为基础框架进行魔改的论文数不胜数。通过上述作者的概述读者朋友可能对 <strong>Yolov5</strong> 有了一个大致的了解，不难发现 <strong>Yolov5</strong> 是非常优秀的神经网络模型。</p> 
<blockquote> 
 <p>可考虑到实际情况，部署的本地机器通常并没有 PC 端那么计算能力强劲。这时候为了整体目标检测系统的稳定运行，往往需要牺牲掉网络模型的精度以换取足够快的检测速度。因此，轻量化部署的网络模型结构就此孕育而生！</p> 
</blockquote> 
<h3>1.2 YOLOv5-Lite详解</h3> 
<blockquote> 
 <p><strong>Yolov5-Lite</strong> 网络模型作为轻量化部署网络的代表作之一，深受算法部署工程师的偏爱（作者为中国<strong>ppogg</strong>大佬）！</p> 
 <p><strong>Yolov5-Lite地址：</strong><a href="https://github.com/ppogg/YOLOv5-Lite" title="GitHub - ppogg/YOLOv5-Lite: 🍅🍅🍅YOLOv5-Lite: lighter, faster and easier to deploy. Evolved from yolov5 and the size of model is only 930+kb (int8) and 1.7M (fp16). It can reach 10+ FPS on the Raspberry Pi 4B when the input size is 320×320~">GitHub - ppogg/YOLOv5-Lite: 🍅🍅🍅YOLOv5-Lite: lighter, faster and easier to deploy. Evolved from yolov5 and the size of model is only 930+kb (int8) and 1.7M (fp16). It can reach 10+ FPS on the Raspberry Pi 4B when the input size is 320×320~</a></p> 
</blockquote> 
<p><strong>Yolov5-Lite </strong>算法的模型结构如图下。该算法去除了 <strong>Focus</strong> 结构层，减小了模型体量，使模型变得更为轻便；同时，去除了 <strong>4</strong> 次 <strong>slice</strong> 操作，减少了对计算机芯片缓存的占用，降低了计算机的处理负担。与 <strong>Yolov5</strong> 算法相比，<strong>Yolov5-Lite</strong> 算法能避免反复使用 <strong>C3 Layer </strong>模块。<strong>C3 Layer</strong> 模块会占用计算机很多运行空间，从而降低计算机的处理速度。这种方式能使 <strong>Yolov5-Lite </strong>算法模型的精度控制在可靠范围内，从而使其更易部署。</p> 
<p class="img-center"><img alt="" height="614" src="https://images2.imgbox.com/9e/83/3SzvAKIY_o.png" width="1200"></p> 
<p>在图像识别领域，主干网络结构(Backbone)和检测头(Head)中往往有一段中间层，即特征增强融合网络层(Neck)，可更精准地提取融合特征。<strong>Yolov5-Lite</strong> 算法也采用 <strong>FPN＋PAN</strong> 结构，但其对输出端(<strong>Head</strong>)进行了通道剪枝，改进了 <strong>YOLOv4</strong> 算法和 <strong>YOLOv5</strong> 算法中的 <strong>FPN＋PAN</strong> 的结构，具体表现在以下2个方面：</p> 
<p><span style="color:#fe2c24;"><strong>(1)、</strong></span>与 <strong>YOLOv5</strong> 算法不同，<strong>Yolov5-Lite</strong> 算法自身各结构的通道数量相同，即模型特征增强网络通道网格数也是20×20×96，这样可优化对内存的访问和使用，提高模型的运行效率；</p> 
<p><span style="color:#fe2c24;"><strong>(2)、</strong></span><strong>Yolov5-Lite</strong> 算法采用 <strong>PANet </strong>结构，将 <strong>YOLOv5 </strong>算法的通道连接(cat)操作改进为叠加操作,这样可进一步优化对内存的使用，加快处理速度。如下图为 <strong>Yolov5-Lite</strong> 算法与 <strong>YOLOv4</strong> 和  <strong>YOLOv5</strong> 算法中的 <span style="color:#fe2c24;"><strong>PAN</strong></span> 结构对比：</p> 
<p class="img-center"><img alt="" height="540" src="https://images2.imgbox.com/a6/7a/2eLJtxNf_o.png" width="1200"></p> 
<blockquote> 
 <p><strong>作者总结：</strong></p> 
 <p><strong>Yolov5-Lite </strong>网络模型源于 <strong>YOLOv5</strong> 模型，随处可以可见 <strong>YOLOv5</strong> 网络模型的影子。但是，出于移植部署的目的性，<strong>Yolov5-Lite </strong>网络将工作重心放在如何进行快速推理，如何轻量化网络模型大小！</p> 
 <p><strong>Yolov5-Lite </strong>网络模型不仅通过直接改变网络模型的结构，偏向多使用计算量小的网络模型结构去提取和融合目标特征，同时侧重运用计算机的运行机制：通过降低计算机内存的存储和读取去变相提高网络推理速度！！！</p> 
 <p>作者这里仅对 <strong>Yolov5-Lite </strong>做初步概述，详情读者朋友可以参考学术论文！</p> 
</blockquote> 
<h2>二、YOLOv5-Lite训练</h2> 
<h3>2.1 数据集制作</h3> 
<p>★常规的神经网络模型训练是需要收集到大量语义丰富的数据集进行训练的。但是考虑实际工程下可能仅需要对已知场地且固定实物进行目标检测追踪等任务，这个时候我们可以采取偷懒的下方作者使用的方法！</p> 
<p><strong>1、</strong>作者使用树莓派4B的 <strong>Camera</strong> 直接在捕获需要识别目标物的图片信息（<span style="color:#fe2c24;"><strong>捕获期间转动待识别的目标物体</strong></span>）；</p> 
<p><img alt="" height="1002" src="https://images2.imgbox.com/39/2a/dYJU3hHd_o.png" width="1200"></p> 
<p>树莓派4B的 <strong>Camera </strong>定时捕获照片的python代码如下：</p> 
<pre><code class="language-python">import cv2
from threading import Thread
import uuid
import os
import time
count = 0
def image_collect(cap):
    global count
    while True:
        success, img = cap.read()
        if success:
            file_name = str(uuid.uuid4())+'.jpg'
            cv2.imwrite(os.path.join('images',file_name),img)
            count = count+1
            print("save %d %s"%(count,file_name))
        time.sleep(0.4)

if __name__ == "__main__":
    
    os.makedirs("images",exist_ok=True)
    
    # 打开摄像头
    cap = cv2.VideoCapture(0)

    m_thread = Thread(target=image_collect, args=([cap]),daemon=True)
    
    while True:

        # 读取一帧图像

        success, img = cap.read()

        if not success:

            continue

        cv2.imshow("video",img)

        key =  cv2.waitKey(1) &amp; 0xFF   

        # 按键 "q" 退出
        if key ==  ord('c'):
            m_thread.start()
            continue
        elif key ==  ord('q'):
            break

    cap.release() </code></pre> 
<p>按动 <strong>“c” </strong>开始采集待识别目标图像，按动 <strong>“q” </strong>退出摄像头 <strong>Camera</strong> 的图片采集；</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/76/80/iI8H3ckN_o.png" width="1200"></p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/ba/70/Kaq71Dma_o.png" width="1200"></p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/2c/59/5vDugcnS_o.png" width="1200"></p> 
<p><strong>2、</strong>将捕获到的待识别目标物照片传输到PC端，利用 <strong>Labelme</strong> 软件进行标注（<strong>Labelme不会使用的建议相关博客</strong>）；</p> 
<p><img alt="" height="1021" src="https://images2.imgbox.com/4a/b1/sVH7ZvGA_o.png" width="1200"></p> 
<p>作者的标注了 <strong>3</strong> 类目标：<strong>drug，prime，glue</strong>；读者朋友可以根据自己实际情况标注自己需要的数据集！由于我们标注的数据的标签 <strong>label </strong>默认是 JSON 格式的不能被 YOLO 系列的神经网络模型直接进行利用训练。</p> 
<p><strong>3、</strong>使用 JSON 转 txt 的 YOLO 格式 label 的python代码进行转换（<strong>可以直接使用作者提供的代码</strong>）：</p> 
<p><strong>dic_lab.py：</strong></p> 
<pre><code class="language-python">dic_labels= {'drug':0,
            'glue':1,
            'prime':2,
             'path_json':'labels',
             'ratio':0.9}</code></pre> 
<p><strong>lablemetoyolo.py：</strong></p> 
<pre><code class="language-python">import os
import json
import random
import base64
import shutil
import argparse
from pathlib import Path
from glob import glob
from dic_lab import dic_labels

def generate_labels(dic_labs):
    path_input_json = dic_labels['path_json']
    ratio = dic_labs['ratio']
    for index, labelme_annotation_path in enumerate(glob(f'{path_input_json}/*.json')):

        # 读取文件名
        image_id = os.path.basename(labelme_annotation_path).rstrip('.json')
        
        # 计算是train 还是 valid
        train_or_valid = 'train' if random.random() &lt; ratio else 'valid'

        # 读取labelme格式的json文件
        labelme_annotation_file = open(labelme_annotation_path, 'r')
        labelme_annotation = json.load(labelme_annotation_file)

        # yolo 格式的 lables
        yolo_annotation_path = os.path.join(train_or_valid, 'labels',image_id + '.txt')
        yolo_annotation_file = open(yolo_annotation_path, 'w')
        
        # yolo 格式的图像保存
        yolo_image = base64.decodebytes(labelme_annotation['imageData'].encode())
        yolo_image_path = os.path.join(train_or_valid, 'images', image_id + '.jpg')
        
        yolo_image_file = open(yolo_image_path, 'wb')
        yolo_image_file.write(yolo_image)
        yolo_image_file.close()
     

        # 获取位置信息
        for shape in labelme_annotation['shapes']:
            if shape['shape_type'] != 'rectangle':
                print(
                    f'Invalid type `{shape["shape_type"]}` in annotation `annotation_path`')
                continue
           

            points = shape['points']
            scale_width = 1.0 / labelme_annotation['imageWidth']
            scale_height = 1.0 / labelme_annotation['imageHeight']
            width = (points[1][0] - points[0][0]) * scale_width
            height = (points[1][1] - points[0][1]) * scale_height
            x = ((points[1][0] + points[0][0]) / 2) * scale_width
            y = ((points[1][1] + points[0][1]) / 2) * scale_height
            object_class = dic_labels[shape['label']]
            yolo_annotation_file.write(f'{object_class} {x} {y} {width} {height}\n')
        yolo_annotation_file.close()
        print("creat lab %d : %s"%(index,image_id))


if __name__ == "__main__":
    os.makedirs(os.path.join("train",'images'),exist_ok=True)
    os.makedirs(os.path.join("train",'labels'),exist_ok=True)
    os.makedirs(os.path.join("valid",'images'),exist_ok=True)
    os.makedirs(os.path.join("valid",'labels'),exist_ok=True)
    generate_labels(dic_labels)

</code></pre> 
<p>我们需要根据自己的需要自定义字典 <strong>dic_lab</strong>，字典中的<strong> ratio = 0.9 </strong>的作用是将数据集拆分成训练集和验证集<strong> 9：1</strong>。读者朋友可以根据自己的实际情况去修改字典的标签内容，成功执行 <strong>lablemetoyolo.py </strong>代码后效果如下：</p> 
<p><img alt="" height="520" src="https://images2.imgbox.com/a4/af/aEQWBXMB_o.png" width="1097"></p> 
<p><img alt="" height="674" src="https://images2.imgbox.com/00/6c/4pbq5sP9_o.png" width="1125"></p> 
<p><strong>labels</strong>文件夹下的标签成功转换了 <strong>YOLO</strong> 系列可以使用的 <strong>label</strong> 标签，到此时就已经成功准备好我们需要的训练集了！</p> 
<blockquote> 
 <p><strong>特别说明：</strong>该方法仅适用于上述作者所说的场景下，实际情况下，建议大家还是使用合格的数据集进行训练（<strong>即目标与背景语义丰富的数据集</strong>），使得训练出来的神经网络具有良好的泛化性与鲁棒性，否则训练出来的网络很容易过拟合！</p> 
</blockquote> 
<h3>2.2 YOLOv5-Lite训练</h3> 
<p><strong>Yolov5-Lite</strong> 训练就是常规的神经网络模型训练，我们从 <strong>GitHub</strong> 上下载 <strong>Yolov5-Lite </strong>的源代码，训练平台为：<strong>PyCharm 2020.1 x64</strong>，GPU：<strong>RTX3060 6G</strong>，CPU：<strong>AMD Ryzen 7 5800H  3.2GHZ</strong>。</p> 
<p>读者朋友可以使用  <strong>PyCharm </strong>或者<strong> VsCode </strong>打开 <strong>Yolov5-Lite</strong> 的源码（作者使用<strong>PyCharm 2020.1 x64</strong>）；</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/1a/ba/V5UYhcX8_o.png" width="1200"></p> 
<p>在 <strong>Yolov5-Lite </strong>的目录下找到 <strong>train.py </strong>(训练文件)的 <strong>main</strong> 函数入口，进行如下配置：</p> 
<p><img alt="" height="1167" src="https://images2.imgbox.com/58/c4/OOzUUk5Y_o.png" width="1200"></p> 
<blockquote> 
 <p><strong>我们设置如下几个核心配置：</strong></p> 
 <p><strong>--weights v5lite-s.pt</strong></p> 
 <p><strong>--cfg models/v5Lite-s.yaml</strong></p> 
 <p><span style="color:#fe2c24;"><strong>--data data/mydata.yaml</strong></span></p> 
 <p><strong>--img-size 320</strong></p> 
 <p><strong>--batch-size 16</strong></p> 
 <p><strong>--data data/mydata.yaml</strong></p> 
 <p><strong>device 0/cpu                        </strong>（可以不使用CUDA训练）</p> 
</blockquote> 
<p>读者朋友一定要将数据集存放的地址位置搞正确！！！</p> 
<p><img alt="" height="188" src="https://images2.imgbox.com/44/40/w6WfoUWB_o.png" width="1106"></p> 
<p><strong>mydata.yaml：</strong></p> 
<p><img alt="" height="1160" src="https://images2.imgbox.com/68/2e/Cg1XEQii_o.png" width="1200"></p> 
<p><img alt="" height="866" src="https://images2.imgbox.com/33/b0/yaYm16o1_o.png" width="1200"></p> 
<p><strong>Yolov5-Lite </strong>网络模型的训练可以不一定必须使用 <strong>CUDA</strong> 进行加速，但是 <strong>pytorch</strong> 架构等依赖库一定需要满足，模型训练依赖要求如下：</p> 
<pre><code class="language-python"># base ----------------------------------------
matplotlib&gt;=3.2.2
numpy&gt;=1.18.5
opencv-python&gt;=4.1.2
Pillow
PyYAML&gt;=5.3.1
scipy&gt;=1.4.1
torch&gt;=1.8.0
torchvision&gt;=0.9.0
tqdm&gt;=4.41.0

# logging -------------------------------------
tensorboard&gt;=2.4.1
# wandb

# plotting ------------------------------------
seaborn&gt;=0.11.0
pandas

# export --------------------------------------
# coremltools&gt;=4.1
# onnx&gt;=1.9.1
# scikit-learn==0.19.2  # for coreml quantization

# extras --------------------------------------
thop  # FLOPS computation
pycocotools&gt;=2.0  # COCO mAP</code></pre> 
<p>将训练环境与数据集都搞定之后，就可以点击运行按钮进行 <strong>Yolov5-Lite </strong>的模型训练了！</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/ec/cc/Sekid0MS_o.png" width="1200"></p> 
<p>训练成功之后，将会在当前目录下的 <strong>run</strong> 文件下的 <strong>trian </strong>文件下找到<strong> expx</strong> (x代表数字)，<strong>expx</strong> 则存放了第 <strong>x </strong>次训练时候的各种数据内容，<strong>包括：</strong>历史最优权重<span style="color:#fe2c24;"><strong>best_weight</strong></span>，当前权重<strong>last_weight</strong>，训练结果<strong>result</strong>等等；</p> 
<p><img alt="" height="1164" src="https://images2.imgbox.com/77/03/TcGZrcD0_o.png" width="1200"></p> 
<h2>三、树莓派4B部署YOLOv5-Lite</h2> 
<blockquote> 
 <p>树莓派4B运行 <strong>Yolov5-Lite</strong> 网络模型进行目标检测需要依赖 <strong>OpenCV</strong> 等视觉<strong>Lib</strong>，读者朋友可以直接使用作者第一篇博客的配置。</p> 
 <p><strong>博客地址：</strong><a href="http://t.csdn.cn/jbHQm" rel="nofollow" title="http://t.csdn.cn/jbHQm">http://t.csdn.cn/jbHQm</a></p> 
</blockquote> 
<h3>3.1 ONNX概述</h3> 
<p><strong>Open Neural Network Exchange</strong>(<strong>ONNX</strong>)是一个开放的生态系统，它使人工智能开发人员在推进项目时选择合适的工具，不用被框架或者生态系统所束缚。<strong>ONNX</strong>支持不同框架之间的互操作性，简化从研究到生产之间的道路。<strong>ONNX</strong>支持许多框架（<strong>TensorFlow, Pytorch, Keras, MxNet, MATLAB</strong>等等），这些框架中的模型都可以导出或者转换为标准<strong>ONNX</strong>格式。模型采用ONNX格式后，就可在各种平台和设备上运行。</p> 
<p class="img-center"><img alt="" height="682" src="https://images2.imgbox.com/2b/dc/jNQkNQe5_o.png" width="1200"></p> 
<p>开发者根据深度学习框架优劣选择某个框架，但是这些框架适应不同的开发阶段，由于必须进行转换，从而导致了研究和生产之间的重大延迟。<strong>ONNX</strong>格式一个通用的<strong>IR</strong>，能够使得开发人员在开发或者部署的任何阶段选择最适合他们项目的框架。<strong>ONNX</strong>通过提供计算图的通用表示，帮助开发人员为他们的任务选择合适的框架。</p> 
<p><strong>ONNX可视化：ONNX</strong> 模型可以通过<strong> netron</strong> 进行可视化。</p> 
<blockquote> 
 <p><strong>作者总结：</strong></p> 
 <p>ONNX<strong> </strong>顾名思义就是开放的神经网络模型转换，利用它可以轻松将模型更换框架，从而适配亦或是部署在各类设备上。</p> 
</blockquote> 
<h3>3.2 ONNX模型转换和移植</h3> 
<p>如今的开源 <strong>YOLO</strong> 系列神经网络模型的目录下作者都会预留 <strong>export.py</strong> 文件将该神经网络模型进行转换到 <strong>ONNX</strong> 模型，方便大家实际情况下部署使用！</p> 
<p><img alt="" height="1087" src="https://images2.imgbox.com/74/09/xlgaleJ5_o.png" width="1200"></p> 
<p>将我们训练好的最优训练权重 <strong>weights</strong> 存放到 <strong>YOLOv5-Lite</strong> 主目录下，之后运行如下代码：</p> 
<pre><code class="language-python">python export.py --weights best.pt</code></pre> 
<p>运行该指令后将会通过 <strong>best.pt </strong>文件，生成 <strong>best.onnx</strong> 的<strong> ONNX</strong> 模型的权重文件；</p> 
<p><img alt="" height="613" src="https://images2.imgbox.com/ef/aa/sHDfj8wr_o.png" width="1154"></p> 
<p>同时为了成功运行 <strong>ONNX</strong> 格式的 <strong>YOLOv5-Lite</strong> 网络模型，需要在树莓派4B中安装 <strong>onnxruntim </strong>（可以直接使用作者提供的安装包），当然值得注意的是 <strong>onnxruntim </strong>的安装需要依赖的 <strong>Numpy版本1.21</strong> 以上（这点需要大家注意，当然如果使用了作者的镜像完全没有问题！）。</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/7f/06/ET4jI7po_o.png" width="1200"></p> 
<pre><code class="language-python">pip install onnx     （tab补全安装包）</code></pre> 
<p>将转换成 <strong>ONNX</strong> 格式的权重文件导入到树莓派4B中，并于目标检测程序保持同一目录下：</p> 
<p><img alt="" height="1086" src="https://images2.imgbox.com/ae/2a/Sluuj0Ui_o.png" width="1200"></p> 
<p>到此 <strong>ONNX</strong> 模型的转换与移植工作就可以完成了！</p> 
<h2>四、YOLOv5-Lite目标检测</h2> 
<p><strong>YOLOv5-Lite </strong>的目标检测前向推理程序是很简单的，可以直接借鉴作者如下提供的代码：</p> 
<pre><code class="language-python">import cv2
import numpy as np
import onnxruntime as ort
import time

def plot_one_box(x, img, color=None, label=None, line_thickness=None):
    """
    description: Plots one bounding box on image img,
                 this function comes from YoLov5 project.
    param: 
        x:      a box likes [x1,y1,x2,y2]
        img:    a opencv image object
        color:  color to draw rectangle, such as (0,255,0)
        label:  str
        line_thickness: int
    return:
        no return
    """
    tl = (
        line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1
    )  # line/font thickness
    color = color or [random.randint(0, 255) for _ in range(3)]
    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))
    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)
    if label:
        tf = max(tl - 1, 1)  # font thickness
        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]
        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3
        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled
        cv2.putText(
            img,
            label,
            (c1[0], c1[1] - 2),
            0,
            tl / 3,
            [225, 255, 255],
            thickness=tf,
            lineType=cv2.LINE_AA,
        )

def _make_grid( nx, ny):
        xv, yv = np.meshgrid(np.arange(ny), np.arange(nx))
        return np.stack((xv, yv), 2).reshape((-1, 2)).astype(np.float32)

def cal_outputs(outs,nl,na,model_w,model_h,anchor_grid,stride):
    
    row_ind = 0
    grid = [np.zeros(1)] * nl
    for i in range(nl):
        h, w = int(model_w/ stride[i]), int(model_h / stride[i])
        length = int(na * h * w)
        if grid[i].shape[2:4] != (h, w):
            grid[i] = _make_grid(w, h)

        outs[row_ind:row_ind + length, 0:2] = (outs[row_ind:row_ind + length, 0:2] * 2. - 0.5 + np.tile(
            grid[i], (na, 1))) * int(stride[i])
        outs[row_ind:row_ind + length, 2:4] = (outs[row_ind:row_ind + length, 2:4] * 2) ** 2 * np.repeat(
            anchor_grid[i], h * w, axis=0)
        row_ind += length
    return outs



def post_process_opencv(outputs,model_h,model_w,img_h,img_w,thred_nms,thred_cond):
    conf = outputs[:,4].tolist()
    c_x = outputs[:,0]/model_w*img_w
    c_y = outputs[:,1]/model_h*img_h
    w  = outputs[:,2]/model_w*img_w
    h  = outputs[:,3]/model_h*img_h
    p_cls = outputs[:,5:]
    if len(p_cls.shape)==1:
        p_cls = np.expand_dims(p_cls,1)
    cls_id = np.argmax(p_cls,axis=1)

    p_x1 = np.expand_dims(c_x-w/2,-1)
    p_y1 = np.expand_dims(c_y-h/2,-1)
    p_x2 = np.expand_dims(c_x+w/2,-1)
    p_y2 = np.expand_dims(c_y+h/2,-1)
    areas = np.concatenate((p_x1,p_y1,p_x2,p_y2),axis=-1)
    
    areas = areas.tolist()
    ids = cv2.dnn.NMSBoxes(areas,conf,thred_cond,thred_nms)
    if len(ids)&gt;0:
        return  np.array(areas)[ids],np.array(conf)[ids],cls_id[ids]
    else:
        return [],[],[]
def infer_img(img0,net,model_h,model_w,nl,na,stride,anchor_grid,thred_nms=0.4,thred_cond=0.5):
    # 图像预处理
    img = cv2.resize(img0, [model_w,model_h], interpolation=cv2.INTER_AREA)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = img.astype(np.float32) / 255.0
    blob = np.expand_dims(np.transpose(img, (2, 0, 1)), axis=0)

    # 模型推理
    outs = net.run(None, {net.get_inputs()[0].name: blob})[0].squeeze(axis=0)

    # 输出坐标矫正
    outs = cal_outputs(outs,nl,na,model_w,model_h,anchor_grid,stride)

    # 检测框计算
    img_h,img_w,_ = np.shape(img0)
    boxes,confs,ids = post_process_opencv(outs,model_h,model_w,img_h,img_w,thred_nms,thred_cond)

    return  boxes,confs,ids




if __name__ == "__main__":

    # 模型加载
    model_pb_path = "best.onnx"
    so = ort.SessionOptions()
    net = ort.InferenceSession(model_pb_path, so)
    
    # 标签字典
    dic_labels= {0:'drug',
            1:'glue',
            2:'prime'}
    
    # 模型参数
    model_h = 320
    model_w = 320
    nl = 3
    na = 3
    stride=[8.,16.,32.]
    anchors = [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]]
    anchor_grid = np.asarray(anchors, dtype=np.float32).reshape(nl, -1, 2)
    
    video = 0
    cap = cv2.VideoCapture(video)
    flag_det = False
    while True:
        success, img0 = cap.read()
        if success:
            
            if flag_det:
                t1 = time.time()
                det_boxes,scores,ids = infer_img(img0,net,model_h,model_w,nl,na,stride,anchor_grid,thred_nms=0.4,thred_cond=0.5)
                t2 = time.time()
            
                
                for box,score,id in zip(det_boxes,scores,ids):
                    label = '%s:%.2f'%(dic_labels[id],score)
            
                    plot_one_box(box.astype(np.int16), img0, color=(255,0,0), label=label, line_thickness=None)
                    
                str_FPS = "FPS: %.2f"%(1./(t2-t1))
                
                cv2.putText(img0,str_FPS,(50,50),cv2.FONT_HERSHEY_COMPLEX,1,(0,255,0),3)
                
            
            cv2.imshow("video",img0)
        key=cv2.waitKey(1) &amp; 0xFF    
        if key == ord('q'):
        
            break
        elif key &amp; 0xFF == ord('s'):
            flag_det = not flag_det
            print(flag_det)
            
    cap.release() </code></pre> 
<p>就该目标检测算法作者简单给大家讲解一下：</p> 
<p><img alt="" height="1086" src="https://images2.imgbox.com/9a/88/qTeGFUwW_o.png" width="1200"></p> 
<blockquote> 
 <p>上方的 <strong>model_pb_path</strong> 和 <strong>dic_labels</strong> 是需要根据自己实际情况去改一下的，其余基本保持不变即可！</p> 
</blockquote> 
<p><img alt="" height="1086" src="https://images2.imgbox.com/79/55/OU1LILeT_o.png" width="1200"></p> 
<p>上述基本除了 <strong>anchor</strong> 锚框这个数据，其余都是不需要改动的，这里 <strong>anchor</strong> 直接使用了默认值。如果大家想在目标检测的时候可以更好地框选出自己地识别目标，可以用 <strong>K-means</strong> 聚类算法去自适应 <strong>anchor</strong> 的大小，从小聚类出符合自己数据类型的 <strong>anchor</strong> ，这样目标检测的时候可能效果更好！</p> 
<p>一切准备就绪，直接运行咱们的 <strong>test_video.py</strong> 程序进行目标检测：</p> 
<pre><code class="language-python">python3 test_video.py</code></pre> 
<p>按键 <strong>“s” </strong>开启目标检测功能，按键 <strong>“q”</strong> 退出当前目标检测程序！</p> 
<p><img alt="" height="1086" src="https://images2.imgbox.com/a7/92/cn9fvBje_o.png" width="1200"></p> 
<h2>五、项目效果</h2> 
<h3>5.1 实战视频</h3> 
<div class="csdn-video-box"> 
 <iframe id="iNL2Ffp3-1687840201053" frameborder="0" src="https://live.csdn.net/v/embed/307801" allowfullscreen="true" data-mediaembed="csdn"></iframe> 
 <p>基于树莓派4B的YOLOv5-Lite目标检测</p> 
</div> 
<h3>5.2 作者有话</h3> 
<p>作者本次仅使用了 <strong>ONNX</strong> 模型下直接跑 <strong>YOLOv5-Lite </strong>的网络模型，目前该状态下的<strong>FPS</strong>仅维持在<span style="color:#fe2c24;"><strong>5</strong></span>左右，效果其实比直接跑 <strong>YOLOv5</strong> 网络模型已经好很多了（<span style="color:#fe2c24;"><strong>YOLOv5的FPS在0.3FPS左右</strong></span>）。但是距离可以与控制结合感觉还是差了点，所以，后续作者将对目标检测进行加速处理（<strong>使用NCNN，MNN等模型加速</strong>）。</p> 
<p>当然，作者也会分享自己实验室提出的轻量化目标检测网络从网络模型出发加速推理。将计算机视觉与控制结合的嵌入式AI教学后续也会出博客分享给各位，希望给大家日常的工作或者是电赛提供些许帮助！</p> 
<h2>六、项目代码</h2> 
<p><strong>代码地址：</strong><a href="https://download.csdn.net/download/black_sneak/87952847" title="基于树莓派4B的YOLOv5-Lite目标检测的资源包资源-CSDN文库">基于树莓派4B的YOLOv5-Lite目标检测的资源包资源-CSDN文库</a></p> 
<p><strong>如果积分不够的朋友，<span style="color:#fe2c24;">点波关注</span>，<span style="color:#fe2c24;">评论区留下邮箱</span>，作者无偿提供源码和后续问题解答。求求啦关注一波吧 ！！！</strong></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/0b1420157e515c5a6c960e0df66b9b5f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Hadoop3.3.1完全分布式部署</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a161f40703bc92c4a2f73fa4edba261f/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">TCP和UDP</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>