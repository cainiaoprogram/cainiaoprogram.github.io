<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>如何绘制深度学习-目标检测评估指标P-R（precision-recall）曲线？如何计算AP（average-precision）？ - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="如何绘制深度学习-目标检测评估指标P-R（precision-recall）曲线？如何计算AP（average-precision）？" />
<meta property="og:description" content="参考文章：深度学习-目标检测评估指标P-R曲线、AP、mAP
文章目录 P-R曲线：AP计算： 下面通过具体例子说明。
首先用训练好的模型得到所有测试样本的confidence score，每一类（如car）的confidence score保存到一个文件中（如comp1_cls_test_car.txt）。假设共有20个测试样本，每个的id，confidence score和ground truth label如下：​​
ground truth label通过预测的bbox与ground truth的iou确定为正样本或负样本。
接下来对confidence score排序，得到：
P-R曲线： 我们得到top-5的结果，前score最高的前5个样本，预测label为1，即：
此例中采用top-5评估，也可采用其他评估，如AP50，即当预测框与真实框的IoU值大于这个阈值时，该预测框才被认定为真阳性（True Positive, TP），反之就是假阳性（False Positive，FP）。
在这个例子中，true positives就是指第4和第2张图片，false positives就是指第13，19，6张图片。是相对于方框内的元素而言，在这个例子中，confidence score排在top-5之外的元素为false negatives和true negatives，即：
其中，false negatives是指第9，16，7，20张图片，true negatives是指第1,18,5,15,10,17,12,14,8,11,3张图片。
那么，这个例子中Precision=2/5=40%，意思是对于car这一类别，我们选定了5个样本，其中正确的有2个，即准确率为40%；Recall=2/6=30%，意思是在所有测试样本中，共有6个car，但是因为我们只召回了2个，所以召回率为30%。此时为下图中第5个样本点。同理图中第一个样本点：P=1，R=1/6，第二个样本点，考虑前两个样本，P=1，R=2/6=1/3。。。
这个例子的precision-recall曲线如下：
实际多类别分类任务中，我们通常不满足只通过top-5来衡量一个模型的好坏，而是需要知道从top-1到top-N（N是所有测试样本个数，本文中为20）对应的precision和recall。显然随着我们选定的样本越来也多，recall一定会越来越高，而precision整体上会呈下降趋势。把recall当成横坐标，precision当成纵坐标，即可得到常用的precision-recall曲线。
AP计算： 接下来说说AP的计算，此处参考的是PASCAL VOC CHALLENGE的2010年之前计算方法。首先设定一组阈值，[0, 0.1, 0.2, …, 1]。然后对于recall大于每一个阈值（比如recall&gt;0.3），我们都会得到一个对应的最大precision。这样，我们就计算出了11个precision。AP即为这11个precision的平均值。这种方法英文叫做11-point interpolated average precision。​
当然PASCAL VOC CHALLENGE自2010年后就换了另一种计算方法。新的计算方法假设这N个样本中有M个正例，那么我们会得到M个recall值（1/M, 2/M, …, M/M）,对于每个recall值r，我们可以计算出对应（r’ &gt;= r）的最大precision，然后对这M个precision值取平均即得到最后的AP值。计算方法如下：​
相应的Precision-Recall曲线（这条曲线是单调递减的）如下：​
AP衡量的是学出来的模型在每个类别上的好坏，mAP衡量的是学出的模型在所有类别上的好坏，得到AP后mAP的计算就变得很简单了，就是取所有AP的平均值。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/187de3bca572f170e70bc85497651ae0/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-02-06T15:51:15+08:00" />
<meta property="article:modified_time" content="2020-02-06T15:51:15+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">如何绘制深度学习-目标检测评估指标P-R（precision-recall）曲线？如何计算AP（average-precision）？</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><a href="https://blog.csdn.net/qq_41994006/article/details/81051150">参考文章：深度学习-目标检测评估指标P-R曲线、AP、mAP</a><br> </p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#PR_11" rel="nofollow">P-R曲线：</a></li><li><a href="#AP_26" rel="nofollow">AP计算：</a></li></ul> 
</div> 
<p></p> 
<p>下面通过具体例子说明。</p> 
<p>首先用训练好的模型得到所有测试样本的confidence score，每一类（如car）的confidence score保存到一个文件中（如comp1_cls_test_car.txt）。假设共有20个测试样本，每个的id，confidence score和ground truth label如下：​​</p> 
<p>ground truth label通过预测的bbox与ground truth的iou确定为正样本或负样本。<br> <img src="https://images2.imgbox.com/15/f8/KXGmAY8G_o.png" alt="在这里插入图片描述"><br> 接下来对confidence score排序，得到：<br> <img src="https://images2.imgbox.com/ae/ef/d68QLAmK_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="PR_11"></a>P-R曲线：</h2> 
<p>我们得到top-5的结果，前score最高的前5个样本，预测label为1，即：<br> <img src="https://images2.imgbox.com/e0/d1/eNopn7cu_o.png" alt="在这里插入图片描述"><br> 此例中采用top-5评估，也可采用其他评估，如AP50，即当预测框与真实框的IoU值大于这个阈值时，该预测框才被认定为真阳性（True Positive, TP），反之就是假阳性（False Positive，FP）。</p> 
<p>在这个例子中，true positives就是指第4和第2张图片，false positives就是指第13，19，6张图片。是相对于方框内的元素而言，在这个例子中，confidence score排在top-5之外的元素为false negatives和true negatives，即：<br> <img src="https://images2.imgbox.com/1d/00/HDzennny_o.png" alt="在这里插入图片描述"><br> 其中，false negatives是指第9，16，7，20张图片，true negatives是指第1,18,5,15,10,17,12,14,8,11,3张图片。</p> 
<p>那么，这个例子中Precision=2/5=40%，意思是对于car这一类别，我们选定了5个样本，其中正确的有2个，即准确率为40%；Recall=2/6=30%，意思是在所有测试样本中，共有6个car，但是因为我们只召回了2个，所以召回率为30%。此时为下图中第5个样本点。同理图中第一个样本点：P=1，R=1/6，第二个样本点，考虑前两个样本，P=1，R=2/6=1/3。。。</p> 
<p>这个例子的precision-recall曲线如下：<br> <img src="https://images2.imgbox.com/bb/c3/194MxDvk_o.png" alt="在这里插入图片描述"><br> 实际多类别分类任务中，我们通常不满足只通过top-5来衡量一个模型的好坏，而是需要知道从top-1到top-N（N是所有测试样本个数，本文中为20）对应的precision和recall。显然随着我们选定的样本越来也多，recall一定会越来越高，而precision整体上会呈下降趋势。把recall当成横坐标，precision当成纵坐标，即可得到常用的precision-recall曲线。</p> 
<h2><a id="AP_26"></a>AP计算：</h2> 
<p>接下来说说AP的计算，此处参考的是PASCAL VOC CHALLENGE的2010年之前计算方法。首先设定一组阈值，[0, 0.1, 0.2, …, 1]。然后对于recall大于每一个阈值（比如recall&gt;0.3），我们都会得到一个对应的最大precision。这样，我们就计算出了11个precision。AP即为这11个precision的平均值。这种方法英文叫做11-point interpolated average precision。​</p> 
<p>当然PASCAL VOC CHALLENGE自2010年后就换了另一种计算方法。新的计算方法假设这N个样本中有M个正例，那么我们会得到M个recall值（1/M, 2/M, …, M/M）,对于每个recall值r，我们可以计算出对应（r’ &gt;= r）的最大precision，然后对这M个precision值取平均即得到最后的AP值。计算方法如下：​<br> <img src="https://images2.imgbox.com/01/c3/dsx7hYr6_o.png" alt="在这里插入图片描述"><br> 相应的Precision-Recall曲线（这条曲线是单调递减的）如下：​<br> <img src="https://images2.imgbox.com/4b/7d/0TXefbQE_o.png" alt="在这里插入图片描述"><br> AP衡量的是学出来的模型在每个类别上的好坏，mAP衡量的是学出的模型在所有类别上的好坏，得到AP后mAP的计算就变得很简单了，就是取所有AP的平均值。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4c2d4464864f4e6b39a3b96a54d866f2/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Python报“TypeError: a bytes-like object is required, not ‘str’ ”解决办法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/4343ca42edde0be57deb77e78d886562/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">LTE入网流程分析</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>