<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>当大型语言模型（LLM）遇上知识图谱：两大技术优势互补 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="当大型语言模型（LLM）遇上知识图谱：两大技术优势互补" />
<meta property="og:description" content="1 引言 大型语言模型（LLM）已经很强了，但还可以更强。通过结合知识图谱，LLM 有望解决缺乏事实知识、幻觉和可解释性等诸多问题；而反过来 LLM 也能助益知识图谱，让其具备强大的文本和语言理解能力。而如果能将两者充分融合，我们也许还能得到更加全能的人工智能。
今天我们将介绍一篇综述 LLM 与知识图谱联合相关研究的论文，其中既包含用知识图谱增强 LLM 的研究进展，也有用 LLM 增强知识图谱的研究成果，还有 LLM 与知识图谱协同的最近成果。文中概括性的框架展示非常方便读者参考。
论文链接：https://arxiv.org/abs/2306.08302
BERT、RoBERTA 和 T5 等在大规模语料库上预训练的大型语言模型（LLM）已经能非常优秀地应对多种自然语言处理（NLP）任务，比如问答、机器翻译和文本生成。近段时间，随着模型规模的急剧增长，LLM 还进一步获得了涌现能力，开拓了将 LLM 用作通用人工智能（AGI）的道路。ChatGPT 和 PaLM2 等先进的 LLM 具有数百上千亿个参数，它们已有潜力解决许多复杂的实际任务，比如教育、代码生成和推荐。
尽管 LLM 已有许多成功应用，但由于缺乏事实知识，它们还是备受诟病。具体来说，LLM 会记忆训练语料库中包含的事实和知识。但是，进一步的研究表明，LLM 无法回忆出事实，而且往往还会出现幻觉问题，即生成具有错误事实的表述。举个例子，如果向 LLM 提问：「爱因斯坦在什么时候发现了引力？」它可能会说：「爱因斯坦在 1687 年发现了引力。」但事实上，提出引力理论的人是艾萨克・牛顿。这种问题会严重损害 LLM 的可信度。
LLM 是黑箱模型，缺乏可解释性，因此备受批评。LLM 通过参数隐含地表示知识。因此，我们难以解释和验证 LLM 获得的知识。此外，LLM 是通过概率模型执行推理，而这是一个非决断性的过程。对于 LLM 用以得出预测结果和决策的具体模式和功能，人类难以直接获得详情和解释。
尽管通过使用思维链（chain-of-thought），某些 LLM 具备解释自身预测结果的功能，但它们推理出的解释依然存在幻觉问题。这会严重影响 LLM 在事关重大的场景中的应用，比如医疗诊断和法律评判。举个例子，在医疗诊断场景中，LLM 可能误诊并提供与医疗常识相悖的解释。这就引出了另一个问题：在一般语料库上训练的 LLM 由于缺乏特定领域的知识或新训练数据，可能无法很好地泛化到特定领域或新知识上。
为了解决上述问题，一个潜在的解决方案是将知识图谱（KG）整合进 LLM 中。知识图谱能以三元组的形式存储巨量事实，即 (头实体、关系、尾实体)，因此知识图谱是一种结构化和决断性的知识表征形式，例子包括 Wikidata、YAGO 和 NELL。
知识图谱对多种应用而言都至关重要，因为其能提供准确、明确的知识。此外众所周知，它们还具有很棒的符号推理能力，这能生成可解释的结果。知识图谱还能随着新知识的持续输入而积极演进。此外，通过让专家来构建特定领域的知识图谱，就能具备提供精确可靠的特定领域知识的能力。
然而，知识图谱很难构建，并且由于真实世界知识图谱往往是不完备的，还会动态变化，因此当前的知识图谱方法难以应对。这些方法无法有效建模未见过的实体以及表征新知识。此外，知识图谱中丰富的文本信息往往会被忽视。不仅如此，知识图谱的现有方法往往是针对特定知识图谱或任务定制的，泛化能力不足。因此，有必要使用 LLM 来解决知识图谱面临的挑战。图 1 总结了 LLM 和知识图谱的优缺点。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/b3f251bab4b0ffbbfb2783e58be48119/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-07T23:18:18+08:00" />
<meta property="article:modified_time" content="2024-01-07T23:18:18+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">当大型语言模型（LLM）遇上知识图谱：两大技术优势互补</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>1 引言</h2> 
<p>大型语言模型（LLM）已经很强了，但还可以更强。通过结合知识图谱，LLM 有望解决缺乏事实知识、幻觉和可解释性等诸多问题；而反过来 LLM 也能助益知识图谱，让其具备强大的文本和语言理解能力。而如果能将两者充分融合，我们也许还能得到更加全能的人工智能。</p> 
<p>今天我们将介绍一篇综述 LLM 与知识图谱联合相关研究的论文，其中既包含用知识图谱增强 LLM 的研究进展，也有用 LLM 增强知识图谱的研究成果，还有 LLM 与知识图谱协同的最近成果。文中概括性的框架展示非常方便读者参考。</p> 
<p><strong>论文链接：</strong><a class="link-info" href="https://arxiv.org/abs/2306.08302" rel="nofollow" title="https://arxiv.org/abs/2306.08302">https://arxiv.org/abs/2306.08302</a></p> 
<p>BERT、RoBERTA 和 T5 等在大规模语料库上预训练的大型语言模型（LLM）已经能非常优秀地应对多种自然语言处理（NLP）任务，比如问答、机器翻译和文本生成。近段时间，随着模型规模的急剧增长，LLM 还进一步获得了涌现能力，开拓了将 LLM 用作通用人工智能（AGI）的道路。ChatGPT 和 PaLM2 等先进的 LLM 具有数百上千亿个参数，它们已有潜力解决许多复杂的实际任务，比如教育、代码生成和推荐。</p> 
<p>尽管 LLM 已有许多成功应用，但由于缺乏事实知识，它们还是备受诟病。具体来说，LLM 会记忆训练语料库中包含的事实和知识。但是，进一步的研究表明，LLM 无法回忆出事实，而且往往还会出现幻觉问题，即生成具有错误事实的表述。举个例子，如果向 LLM 提问：「爱因斯坦在什么时候发现了引力？」它可能会说：「爱因斯坦在 1687 年发现了引力。」但事实上，提出引力理论的人是艾萨克・牛顿。这种问题会严重损害 LLM 的可信度。</p> 
<p>LLM 是黑箱模型，缺乏可解释性，因此备受批评。LLM 通过参数隐含地表示知识。因此，我们难以解释和验证 LLM 获得的知识。此外，LLM 是通过概率模型执行推理，而这是一个非决断性的过程。对于 LLM 用以得出预测结果和决策的具体模式和功能，人类难以直接获得详情和解释。</p> 
<p>尽管通过使用思维链（chain-of-thought），某些 LLM 具备解释自身预测结果的功能，但它们推理出的解释依然存在幻觉问题。这会严重影响 LLM 在事关重大的场景中的应用，比如医疗诊断和法律评判。举个例子，在医疗诊断场景中，LLM 可能误诊并提供与医疗常识相悖的解释。这就引出了另一个问题：在一般语料库上训练的 LLM 由于缺乏特定领域的知识或新训练数据，可能无法很好地泛化到特定领域或新知识上。</p> 
<p>为了解决上述问题，一个潜在的解决方案是将知识图谱（KG）整合进 LLM 中。知识图谱能以三元组的形式存储巨量事实，即 (头实体、关系、尾实体)，因此知识图谱是一种结构化和决断性的知识表征形式，例子包括 Wikidata、YAGO 和 NELL。</p> 
<p>知识图谱对多种应用而言都至关重要，因为其能提供准确、明确的知识。此外众所周知，它们还具有很棒的符号推理能力，这能生成可解释的结果。知识图谱还能随着新知识的持续输入而积极演进。此外，通过让专家来构建特定领域的知识图谱，就能具备提供精确可靠的特定领域知识的能力。</p> 
<p>然而，知识图谱很难构建，并且由于真实世界知识图谱往往是不完备的，还会动态变化，因此当前的知识图谱方法难以应对。这些方法无法有效建模未见过的实体以及表征新知识。此外，知识图谱中丰富的文本信息往往会被忽视。不仅如此，知识图谱的现有方法往往是针对特定知识图谱或任务定制的，泛化能力不足。因此，有必要使用 LLM 来解决知识图谱面临的挑战。图 1 总结了 LLM 和知识图谱的优缺点。</p> 
<p></p> 
<p class="img-center"><img alt="" height="691" src="https://images2.imgbox.com/b2/8b/DCCZYICD_o.png" width="1080"></p> 
<p>▲ 图1：LLM 和知识图谱的优缺点总结</p> 
<p>如图所示，LLM 的优点：一般知识、语言处理、泛化能力。LLM 的缺点：隐含知识、幻觉问题、无法决断问题、黑箱、缺乏特定领域的知识和新知识。知识图谱的优点：结构化的知识、准确度、决断能力、可解释性、特定领域的知识、知识演进。知识图谱的缺点：不完备性、缺乏语言理解、未见过的知识。</p> 
<p>近段时间，将 LLM 和知识图谱联合起来的可能性受到了越来越多研究者和实践者关注。LLM 和知识图谱本质上是互相关联的，并且能彼此互相强化。如果用知识图谱增强 LLM，那么知识图谱不仅能被集成到 LLM 的预训练和推理阶段，从而用来提供外部知识，还能被用来分析 LLM 以提供可解释性。</p> 
<p>而在用 LLM 来增强知识图谱方面，LLM 已被用于多种与知识图谱相关的应用，比如知识图谱嵌入、知识图谱补全、知识图谱构建、知识图谱到文本的生成、知识图谱问答。LLM 能够提升知识图谱的性能并助益其应用。在 LLM 与知识图谱协同的相关研究中，研究者将 LLM 和知识图谱的优点融合，让它们在知识表征和推理方面的能力得以互相促进。</p> 
<p>这篇论文将在联合 LLM 与知识图谱方面提供一个前瞻性的路线图，帮助读者了解如何针对不同的下游任务，利用它们各自的优势，克服各自的局限。其中包含详细的分类和全面的总结，并指出了这些快速发展的领域的新兴方向。本文的主要贡献包括：</p> 
<ul><li>路线图：文中提供了一份 LLM 和知识图谱整合方面的前瞻性路线图。这份路线图包含联合 LLM 与知识图谱的三个概括性框架：用知识图谱增强 LLM、用 LLM 增强知识图谱、LLM 与知识图谱协同。可为联合这两种截然不同但互补的技术提供指导方针。</li><li>分类和总结评估：对于该路线图中的每种整合模式，文中都提供了详细的分类和全新的分类法。对于每种类别，文中都从不同整合策略和任务角度总结评估了相关研究工作，从而能为每种框架提供更多见解。</li><li>涵盖了新进展：文中覆盖了 LLM 和知识图谱的先进技术。其中讨论了 ChatGPT 和 GPT-4 等当前最先进的 LLM 以及多模态知识图谱等知识图谱新技术。</li><li>4挑战和未来方向：文中也会给出当前研究面临的挑战并给出一些有潜力的未来研究方向。</li></ul> 
<h2><strong>2 LLM和知识图谱基础知识</strong></h2> 
<h3><strong>2.1 大型语言模型（LLM）</strong></h3> 
<p>在大规模语料库上预训练的 LLM 可以解决多种 NLP 任务，拥有巨大潜力。如图 3 所示，大多数 LLM 都源自 Transformer 设计，其中包含编码器和解码器模块，并采用了自注意力机制。LLM 可以根据架构不同而分为三大类别：仅编码器 LLM、编码器 - 解码器 LLM、仅解码器 LLM。图 2 总结了一些代表性 LLM，涉及不同架构、模型大小和是否开源。</p> 
<p></p> 
<p class="img-center"><img alt="" height="598" src="https://images2.imgbox.com/5b/be/5HZkjG1w_o.png" width="1080"></p> 
<p>▲ 图2：近些年有代表性的LLM。实心方框表示开源模型，空心方框则是闭源模型。</p> 
<p></p> 
<p class="img-center"><img alt="" height="598" src="https://images2.imgbox.com/de/cf/0ABWUd4G_o.png" width="1080"></p> 
<p>▲ 图3：基于Transformer并使用了自注意力机制的LLM的示意图</p> 
<h3><strong>2.2 prompt 工程设计</strong></h3> 
<p>prompt 工程设计是一个全新领域，其关注的是创建和优化 prompt，从而让 LLM 能最有效地应对各种不同应用和研究领域。如图 4 所示，prompt 是 LLM 的自然语言输入序列，需要针对具体任务（如情绪分类）创建。prompt 可包含多个元素，即：指示、背景信息、输入文本。指示是告知模型执行某特定任务的短句。背景信息为输入文本或少样本学习提供相关的信息。输入文本是需要模型处理的文本。</p> 
<p></p> 
<p class="img-center"><img alt="" height="668" src="https://images2.imgbox.com/26/0e/qnkZzuO5_o.png" width="801"></p> 
<p>▲ 图4：一个情绪分类prompt的示例</p> 
<p>prompt 工程设计的目标是提升 LLM 应对多样化复杂任务的能力，如问答、情绪分类和常识推理。思维链（CoT）prompt 是通过中间推理步骤来实现复杂推理。另一种方法则是通过整合外部知识来设计更好的知识增强型 prompt。自动化 prompt 工程（APE）则是一种可以提升 LLM 性能的 prompt 自动生成方法。prompt 让人无需对 LLM 进行微调就能利用 LLM 的潜力。掌握 prompt 工程设计能让人更好地理解 LLM 的优劣之处。</p> 
<h3><strong>2.3 知识图谱（KG）</strong></h3> 
<p>知识图谱则是以 (实体、关系、实体) 三元组集合的方式来存储结构化知识。根据所存储信息的不同，现有的知识图谱可分为四大类：百科知识型知识图谱、常识型知识图谱、特定领域型知识图谱、多模态知识图谱。图 5 展示了不同类别知识图谱的例子。</p> 
<p></p> 
<p class="img-center"><img alt="" height="897" src="https://images2.imgbox.com/fd/ce/jIIIjIYD_o.png" width="723"></p> 
<p>▲ 图5：不同类别知识图谱示例</p> 
<h2><strong>3 应用</strong></h2> 
<p>LLM 和知识图谱都有着广泛的应用。表 1 总结了 LLM 和知识图谱的一些代表性应用。</p> 
<p></p> 
<p class="img-center"><img alt="" height="371" src="https://images2.imgbox.com/61/55/29AgHzcK_o.png" width="919"></p> 
<p>▲ 表1：LLM和知识图谱的代表性应用</p> 
<h3><strong>3.1 路线图与分类</strong></h3> 
<p>下面会先给出一份路线图，展现将 LLM 和知识图谱联合起来的框架，然后将对相关研究进行分类。</p> 
<h3>3.2 路线图</h3> 
<p>图 6 展示了将 LLM 和知识图谱联合起来的路线图。这份路线图包含联合 LLM 与知识图谱的三个框架：用知识图谱增强 LLM、用 LLM 增强知识图谱、LLM 与知识图谱协同。</p> 
<p><img alt="" height="371" src="https://images2.imgbox.com/59/25/gPzcNXF5_o.jpg" width="919">▲ 图6：联合知识图谱和LLM的一般路线图</p> 
<p></p> 
<p class="img-center"><img alt="" height="827" src="https://images2.imgbox.com/05/67/EhmEGyBS_o.png" width="910"></p> 
<p>▲ 图7：LLM与知识图谱协同的一般框架，其中包含四层：数据、协同模型、技术、应用</p> 
<h2><strong>4 分类</strong></h2> 
<p>为了更好地理解联合 LLM 和知识图谱的研究，论文进一步提供了每种框架的细粒度分类。具体来说，这里关注的是整合 LLM 与知识图谱的不同方法，即：用知识图谱增强 LLM、用 LLM 增强知识图谱、LLM 与知识图谱协同。图 8 细粒度地展示了相关研究的分类情况。</p> 
<p></p> 
<p class="img-center"><img alt="" height="785" src="https://images2.imgbox.com/bf/1f/Xx8kgMFr_o.png" width="1057"></p> 
<p>▲ 图8：联合LLM与知识图谱的相关研究分类</p> 
<h3 style="background-color:transparent;"><strong>4.1 用知识图谱增强LLM</strong></h3> 
<p>大型语言模型在许多自然语言处理任务上都表现出色。但是，由于 LLM 缺乏实际知识而且常在推理时生成事实性错误，因此也饱受批评。解决该问题的一种方法是用知识图谱增强 LLM。</p> 
<p>具体的方式有几种，一是使用知识图谱增强 LLM 预训练，其目的是在预训练阶段将知识注入到 LLM 中。二是使用知识图谱增强 LLM 推理，这能让 LLM 在生成句子时考虑到最新知识。三是使用知识图谱增强 LLM 可解释性，从而让我们更好地理解 LLM 的行为。表 2 总结了用知识图谱增强 LLM 的典型方法。</p> 
<p></p> 
<p class="img-center"><img alt="" height="854" src="https://images2.imgbox.com/cf/a9/tihwSSBU_o.png" width="1019"></p> 
<p>▲ 表2：用知识图谱增强LLM的方法</p> 
<h3><strong>4.2 用知识图谱增强 LLM 预训练</strong></h3> 
<p>现有的 LLM 主要依靠在大规模语料库上执行无监督训练。尽管这些模型在下游任务上表现卓越，它们却缺少与现实世界相关的实际知识。在将知识图谱整合进 LLM 方面，之前的研究可以分为三类：将知识图谱整合进训练目标、将知识图谱整合进 LLM 的输入、将知识图谱整合进附加的融合模块。</p> 
<p></p> 
<p class="img-center"><img alt="" height="437" src="https://images2.imgbox.com/de/fe/1HoZCrAS_o.png" width="801"></p> 
<p>▲ 图9：通过文本 - 知识对齐损失将知识图谱信息注入到LLM的训练目标中，其中h表示LLM生成的隐含表征。</p> 
<p><img alt="" height="750" src="https://images2.imgbox.com/81/fe/ZorH97CR_o.jpg" width="758">▲ 图10：使用图结构将知识图谱信息注入到LLM的输入中</p> 
<p></p> 
<p class="img-center"><img alt="" height="561" src="https://images2.imgbox.com/93/24/fh2IOH0J_o.png" width="807"></p> 
<p>▲ 图11：通过附加的融合模块将知识图谱整合到LLM中</p> 
<h3><strong>4.3 用知识图谱增强 LLM 推理</strong></h3> 
<p>以上方法可以有效地将知识与 LLM 的文本表征融合到一起。但是，真实世界的知识会变化，这些方法的局限是它们不允许更新已整合的知识，除非对模型重新训练。因此在推理时，它们可能无法很好地泛化用于未见过的知识。</p> 
<p>一些研究关注的正是分离知识空间与文本空间以及在推理时注入知识。这些方法主要关注的是问答（QA）任务，因为问答既需要模型捕获文本语义，还需要捕获最新的现实世界知识。</p> 
<p></p> 
<p class="img-center"><img alt="" height="478" src="https://images2.imgbox.com/1b/6c/TpvhH1Lb_o.png" width="1015"></p> 
<p>▲ 图12：用于LLM推理的动态知识图谱融合</p> 
<p></p> 
<p class="img-center"><img alt="" height="396" src="https://images2.imgbox.com/9d/18/55jEmgHk_o.png" width="984"></p> 
<p>▲ 图13：通过检索外部知识来增强LLM生成</p> 
<h3><strong>4.4 用知识图谱增强 LLM 可解释性</strong></h3> 
<p>尽管 LLM 在许多 NLP 任务上都表现不凡，但由于缺乏可解释性，依然备受诟病。LLM 可解释性是指理解和解释大型语言模型的内部工作方式和决策过程。这能提升 LLM 的可信度并促进 LLM 在事关重大的场景中的应用，比如医疗诊断和法律评判。由于知识图谱是以结构化的方式表示知识，因此可为推理结果提供优良的可解释性。因此，研究者必然会尝试用知识图谱来提升 LLM 的可解释性；相关研究大致可分为两类：用于语言模型探测的知识图谱、用于语言模型分析的知识图谱。</p> 
<p></p> 
<p class="img-center"><img alt="" height="425" src="https://images2.imgbox.com/06/78/8u7QtcIX_o.png" width="713"></p> 
<p>▲ 图14：使用知识图谱进行语言模型探测的一般框架</p> 
<p></p> 
<p class="img-center"><img alt="" height="484" src="https://images2.imgbox.com/e7/69/iux2uMhi_o.png" width="708"></p> 
<p>▲ 图15：使用知识图谱进行语言模型分析的一般框架</p> 
<h3><strong>4.5 用LLM增强知识图谱</strong></h3> 
<p>知识图谱的显著特点就是结构化的知识表示。它们适用于许多下游任务，比如问答、推荐和网络搜索。但是，传统知识图谱往往不完备，并且已有方法往往不会考虑文本信息。</p> 
<p>为了解决这些问题，已有研究者考虑使用 LLM 来增强知识图谱，使其能考虑文本信息，从而提升在下游任务上的表现。表 3 总结了代表性的研究工作。这里会涉及到使用 LLM 对知识图谱进行不同增强的方法，包括知识图谱嵌入、知识图谱补全、知识图谱到文本生成、知识图谱问答。</p> 
<p></p> 
<p class="img-center"><img alt="" height="1008" src="https://images2.imgbox.com/df/93/AcpVttj3_o.png" width="725"></p> 
<p>▲ 表3：用LLM增强知识图谱的代表性方法</p> 
<h3><strong>4.6 用 LLM 增强知识图谱嵌入</strong></h3> 
<p>知识图谱嵌入（KGE）的目标是将每个实体和关系映射到低维的向量（嵌入）空间。这些嵌入包含知识图谱的语义和结构信息，可用于多种不同的任务，如问答、推理和推荐。传统的知识图谱嵌入方法主要依靠知识图谱的结构信息来优化一个定义在嵌入上的评分函数（如 TransE 和 DisMult）。但是，这些方法由于结构连接性有限，因此难以表示未曾见过的实体和长尾的关系。</p> 
<p>图 16 展示了近期的一项研究：为了解决这一问题，该方法使用 LLM 来编码实体和关系的文本描述，从而丰富知识图谱的表征。</p> 
<p></p> 
<p class="img-center"><img alt="" height="747" src="https://images2.imgbox.com/45/75/U73tHaK3_o.png" width="717"></p> 
<p>▲ 图16：将LLM用作知识图谱嵌入的文本编码器</p> 
<p><img alt="" height="500" src="https://images2.imgbox.com/fb/7c/EfRCvkNJ_o.jpg" width="883">▲ 图17：用于联合文本与知识图谱嵌入的LLM</p> 
<h3><strong>4.7 用 LLM 增强知识图谱补全</strong></h3> 
<p>知识图谱补全（KGC）任务的目标是推断给定知识图谱中缺失的事实。类似于 KGE，传统 KGC 方法主要关注的是知识图谱的结构，而不会考虑广泛的文本信息。</p> 
<p>但是，近期有研究将 LLM 整合到了 KGC 方法中来编码文本或生成事实，取得了更好的 KGC 表现。根据使用方式，这些方法分为两类：将 LLM 用作编码器（PaE）、将 LLM 用作生成器（PaG）。</p> 
<p></p> 
<p class="img-center"><img alt="" height="915" src="https://images2.imgbox.com/7f/aa/w9YKlFYT_o.png" width="757"></p> 
<p>▲ 图18：将LLM用作编码器（PaE）来补全知识图谱的一般框架</p> 
<p><img alt="" height="835" src="https://images2.imgbox.com/33/24/MeNPSO08_o.jpg" width="882">▲ 图19：将LLM用作生成器（PaG）来补全知识图谱的一般框架 En. 和 De. 分别表示编码器和解码器。</p> 
<p></p> 
<p class="img-center"><img alt="" height="715" src="https://images2.imgbox.com/cc/c2/62XG3lCv_o.png" width="788"></p> 
<p>▲ 图20：使用基于prompt的PaG来补全知识图谱的框架</p> 
<h3><strong>4.8 用 LLM 增强知识图谱构建</strong></h3> 
<p>知识图谱构建涉及到为特定领域内的知识创建结构化的表示。这包括识别实体以及实体之间的关系。知识图谱构建过程通常涉及多个阶段，包括：实体发现、共指消解和关系提取。图 21 展示了将 LLM 用于知识图谱构建各个阶段的一般框架。近期还有研究探索了端到端知识图谱构建（一步构建出完整的知识图谱）以及直接从 LLM 中蒸馏出知识图谱。</p> 
<p></p> 
<p class="img-center"><img alt="" height="870" src="https://images2.imgbox.com/6f/90/AH58R37Y_o.png" width="1080"></p> 
<p>▲ 图21：基于LLM的知识图谱构建的一般框架</p> 
<p></p> 
<p class="img-center"><img alt="" height="366" src="https://images2.imgbox.com/b9/a9/bLAbtEmW_o.png" width="1080"></p> 
<p>▲ 图22：从LLM中蒸馏出知识图谱的一般框架</p> 
<h3><strong>4.9 用 LLM 增强知识图谱到文本生成</strong></h3> 
<p>知识图谱到文本（KG-to-text）生成的目标是生成能准确一致地描述输入知识图谱信息的高质量文本。知识图谱到文本生成连接了知识图谱与文本，能显著提升知识图谱在更现实的自然语言生成场景中的可用性，包括故事创作和基于知识的对话。但是，收集大量知识图谱 - 文本平行数据难度很大，成本很高，这会导致训练不充分和生成质量差。</p> 
<p>因此，有许多研究致力于解决这些问题：如何利用 LLM 的知识？如何构建大规模的弱监督知识图谱 - 文本语料库来解决这个问题？</p> 
<p></p> 
<p class="img-center"><img alt="" height="439" src="https://images2.imgbox.com/8f/62/K5z8ZKGE_o.png" width="1080"></p> 
<p>▲ 图23：知识图谱到文本生成的一般框架</p> 
<h3><strong>4.10 用 LLM 增强知识图谱问答</strong></h3> 
<p>知识图谱问答（KGQA）的目标是根据知识图谱存储的结构化事实来寻找自然语言问题的答案。KGQA 有一个无可避免的挑战：检索相关事实并将知识图谱的推理优势扩展到问答任务上。因此，近期有研究采用 LLM 来填补自然语言问题与结构化知识图谱之间的空白。</p> 
<p>图 24 给出了将 LLM 用于 KGQA 的一般框架，其中 LLM 可用作实体 / 关系提取器和答案推理器。</p> 
<p></p> 
<p class="img-center"><img alt="" height="359" src="https://images2.imgbox.com/f5/a8/RzxaWDLB_o.png" width="1080"></p> 
<p>▲ 图24：将LLM用于知识图谱问答的一般框架</p> 
<p><strong>LLM与知识图谱协同</strong></p> 
<p>LLM 与知识图谱协同近年来赢得了不少关注，该方法能将 LLM 和知识图谱的优点融合，从而更好地应对各种下游任务。举个例子，LLM 可用于理解自然语言，同时知识图谱可作为提供事实知识的知识库。将 LLM 和知识图谱联合起来可以造就执行知识表征和推理的强大模型。</p> 
<p>这里从两个方面关注了 LLM 与知识图谱协同：知识表征、推理。表 4 总结了代表性的研究工作。</p> 
<p></p> 
<p class="img-center"><img alt="" height="476" src="https://images2.imgbox.com/79/3d/boFUjS4r_o.png" width="921"></p> 
<p>▲ 表4：LLM与知识图谱协同方法汇总</p> 
<h2><strong>5 知识表征</strong></h2> 
<p>文本语料库和知识图谱都包含大量知识。但是，文本语料库中的知识通常是隐式的和非结构化的，而知识图谱中的知识是显式的和结构化的。因此，想要以统一方式来表示这些知识，就必须对齐文本语料库和知识图谱中的知识。图 25 给出了针对知识表征任务统一 LLM 和知识图谱的一般框架。</p> 
<p></p> 
<p class="img-center"><img alt="" height="825" src="https://images2.imgbox.com/3b/db/FYAeZRgB_o.png" width="647"></p> 
<p>▲图25：针对知识表征任务统一LLM和知识图谱的一般框架</p> 
<p>KEPLER 是一种用于知识嵌入和预训练语言表征的统一模型。KEPLER 会使用 LLM 将文本实体描述编码成它们的嵌入，然后对知识嵌入和语言建模目标进行联合优化。JointGT 提出了一种知识图谱 - 文本联合表征学习模型，其中提出了三个预训练任务来对齐知识图谱和文本的表征。</p> 
<p>DRAGON 则给出了一种自监督方法，可以基于文本和知识图谱来预训练一个语言 - 知识的联合基础模型。其输入是文本片段和相关的知识图谱子图，并会双向融合来自这两种模式的信息。然后，DRAGON 会利用两个自监督推理任务（掩码语言建模和知识图谱链接预测）来优化该模型的参数。HKLM 则引入了一种联合 LLM，其整合了知识图谱来学习特定领域知识的表征。</p> 
<h2><strong>6 推理</strong></h2> 
<p>为了同时利用 LLM 和知识图谱的优势，研究者也通过 LLM 和知识图谱协同来执行多种应用的推理任务。在问答任务中，QA-GNN 首先会利用 LLM 来处理文本问题，再引导知识图谱的推理步骤。这样一来就构建了文本和结构化信息之间的桥梁，从而能为推理过程提供解释。</p> 
<p>在知识图谱推理任务中，LARK 提出了一种由 LLM 引导的逻辑推理方法。其首先会将传统的逻辑规则转换成语言序列，然后要求 LLM 推理出最终输出。此外，Siyuan et al. 通过一个统一框架统一了结构推理和语言模型预训练。给定一个文本输入，他们采用 LLM 来生成逻辑查询，其可在知识图谱上执行以获取结构化的上下文信息。最后，这个结构化的上下文会与文本信息融合以生成最终输出。</p> 
<p>RecInDial 则将知识图谱与 LLM 组合起来在对话系统中提供个性化推荐。KnowledgeDA 提出了一种统一的领域语言模型开发流程，可使用领域知识图谱增强针对特定任务的训练过程。</p> 
<h2><strong>7 未来方向</strong></h2> 
<p>在联合知识图谱和大型语言模型方面还有诸多挑战有待解决，下面简单给出了这一研究领域的一些未来研究方向：</p> 
<ul><li>将知识图谱用于检测 LLM 的幻觉；</li><li>将知识图谱用于编辑 LLM 中的知识；</li><li>将知识图谱用于黑箱 LLM 知识注入；</li><li>将多模态 LLM 用于知识图谱；</li><li>将 LLM 用于理解知识图谱的结构；</li><li>将 LLM 和知识图谱协同用于双向推理。</li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/0b27092ef02e904893795f779ed72101/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">PAT 乙级 1076 Wifi密码</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f927bca475936f1df629c495105eb4b3/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Copilot在Pycharm的应用和示例</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>