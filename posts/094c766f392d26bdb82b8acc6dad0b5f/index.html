<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Object Detection(目标检测神文)(二) - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Object Detection(目标检测神文)(二)" />
<meta property="og:description" content="文章目录 [CVPR2019] Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression anchor-free[CVPR2019] Region Proposal by Guided Anchoring[CVPR2019] Feature Selective Anchor-Free Module for Single-Shot Object Detection[CVPR2019]CenterNet: Keypoint Triplets for Object Detection[CVPR2019]Objects as Points[CVPR2019]CornerNet-Lite: Efficient Keypoint Based Object Detection[CVPR2019]FoveaBox: Beyond Anchor-based Object Detector[2019]DuBox: No-Prior Box Objection Detection via Residual Dual Scale Detectors YOLO[2019]Spiking-YOLO: Spiking Neural Network for Real-time Object Detection[CVPR2019]Gaussian YOLOv3: An Accurate and Fast Object Detector Using Localization Uncertainty for Autonomous Driving [AAAI2019]Gradient Harmonized Single-stage Detector[2019]Augmentation for small object detection[2019]SimpleDet: A Simple and Versatile Distributed Framework for Object Detection and Instance Recognition[2019]BayesOD: A Bayesian Approach for Uncertainty Estimation in Deep Object Detectors[2019]DetNAS: Neural Architecture Search on Object Detection[2019]ThunderNet: Towards Real-time Generic Object Detection[2019]Feature Intertwiner for Object Detection[CVPR2019]Few-shot Adaptive Faster R-CNN[2019]Improving Object Detection with Inverted Attention[2019]FCOS: Fully Convolutional One-Stage Object Detection[CVPR2019]Libra R-CNN: Towards Balanced Learning for Object Detection[2019]Complexer-YOLO: Real-Time 3D Object Detection and Tracking on Semantic Point Clouds[CVPR2019]What Object Should I Use?" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/094c766f392d26bdb82b8acc6dad0b5f/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-03-22T09:49:31+08:00" />
<meta property="article:modified_time" content="2019-03-22T09:49:31+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Object Detection(目标检测神文)(二)</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><ul><li><ul><li><a href="#CVPR2019_Generalized_Intersection_over_Union_A_Metric_and_A_Loss_for_Bounding_Box_Regression_3" rel="nofollow">[CVPR2019] Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression</a></li></ul> 
   </li><li><a href="#anchorfree_6" rel="nofollow">anchor-free</a></li><li><ul><li><a href="#CVPR2019_Region_Proposal_by_Guided_Anchoring_8" rel="nofollow">[CVPR2019] Region Proposal by Guided Anchoring</a></li><li><a href="#CVPR2019_Feature_Selective_AnchorFree_Module_for_SingleShot_Object_Detection_12" rel="nofollow">[CVPR2019] Feature Selective Anchor-Free Module for Single-Shot Object Detection</a></li><li><a href="#CVPR2019CenterNet_Keypoint_Triplets_for_Object_Detection_16" rel="nofollow">[CVPR2019]CenterNet: Keypoint Triplets for Object Detection</a></li><li><a href="#CVPR2019Objects_as_Points_21" rel="nofollow">[CVPR2019]Objects as Points</a></li><li><a href="#CVPR2019CornerNetLite_Efficient_Keypoint_Based_Object_Detection_27" rel="nofollow">[CVPR2019]CornerNet-Lite: Efficient Keypoint Based Object Detection</a></li><li><a href="#CVPR2019FoveaBox_Beyond_Anchorbased_Object_Detector_31" rel="nofollow">[CVPR2019]FoveaBox: Beyond Anchor-based Object Detector</a></li><li><a href="#2019DuBox_NoPrior_Box_Objection_Detection_via_Residual_Dual_Scale_Detectors_36" rel="nofollow">[2019]DuBox: No-Prior Box Objection Detection via Residual Dual Scale Detectors</a></li></ul> 
   </li><li><a href="#YOLO_40" rel="nofollow">YOLO</a></li><li><ul><li><a href="#2019SpikingYOLO_Spiking_Neural_Network_for_Realtime_Object_Detection_41" rel="nofollow">[2019]Spiking-YOLO: Spiking Neural Network for Real-time Object Detection</a></li><li><a href="#CVPR2019Gaussian_YOLOv3_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving_44" rel="nofollow">[CVPR2019]Gaussian YOLOv3: An Accurate and Fast Object Detector Using Localization Uncertainty for Autonomous Driving</a></li></ul> 
  </li></ul> 
  </li><li><a href="#_49" rel="nofollow"></a></li><li><ul><li><ul><li><a href="#AAAI2019Gradient_Harmonized_Singlestage_Detector_50" rel="nofollow">[AAAI2019]Gradient Harmonized Single-stage Detector</a></li><li><a href="#2019Augmentation_for_small_object_detection_55" rel="nofollow">[2019]Augmentation for small object detection</a></li><li><a href="#2019SimpleDet_A_Simple_and_Versatile_Distributed_Framework_for_Object_Detection_and_Instance_Recognition_59" rel="nofollow">[2019]SimpleDet: A Simple and Versatile Distributed Framework for Object Detection and Instance Recognition</a></li><li><a href="#2019BayesOD_A_Bayesian_Approach_for_Uncertainty_Estimation_in_Deep_Object_Detectors_64" rel="nofollow">[2019]BayesOD: A Bayesian Approach for Uncertainty Estimation in Deep Object Detectors</a></li><li><a href="#2019DetNAS_Neural_Architecture_Search_on_Object_Detection_68" rel="nofollow">[2019]DetNAS: Neural Architecture Search on Object Detection</a></li><li><a href="#2019ThunderNet_Towards_Realtime_Generic_Object_Detection_72" rel="nofollow">[2019]ThunderNet: Towards Real-time Generic Object Detection</a></li><li><a href="#2019Feature_Intertwiner_for_Object_Detection_76" rel="nofollow">[2019]Feature Intertwiner for Object Detection</a></li><li><a href="#CVPR2019Fewshot_Adaptive_Faster_RCNN_81" rel="nofollow">[CVPR2019]Few-shot Adaptive Faster R-CNN</a></li><li><a href="#2019Improving_Object_Detection_with_Inverted_Attention_85" rel="nofollow">[2019]Improving Object Detection with Inverted Attention</a></li><li><a href="#2019FCOS_Fully_Convolutional_OneStage_Object_Detection_88" rel="nofollow">[2019]FCOS: Fully Convolutional One-Stage Object Detection</a></li><li><a href="#CVPR2019Libra_RCNN_Towards_Balanced_Learning_for_Object_Detection_91" rel="nofollow">[CVPR2019]Libra R-CNN: Towards Balanced Learning for Object Detection</a></li><li><a href="#2019ComplexerYOLO_RealTime_3D_Object_Detection_and_Tracking_on_Semantic_Point_Clouds_95" rel="nofollow">[2019]Complexer-YOLO: Real-Time 3D Object Detection and Tracking on Semantic Point Clouds</a></li><li><a href="#CVPR2019What_Object_Should_I_Use__Task_Driven_Object_Detection_98" rel="nofollow">[CVPR2019]What Object Should I Use? - Task Driven Object Detection</a></li><li><a href="#CVPR2019Towards_Universal_Object_Detection_by_Domain_Attention_107" rel="nofollow">[CVPR2019]Towards Universal Object Detection by Domain Attention</a></li><li><a href="#2019Prime_Sample_Attention_in_Object_Detection_112" rel="nofollow">[2019]Prime Sample Attention in Object Detection</a></li><li><a href="#2019BAOD_BudgetAware_Object_Detection_115" rel="nofollow">[2019]BAOD: Budget-Aware Object Detection</a></li><li><a href="#2019An_Analysis_of_PreTraining_on_Object_Detection_118" rel="nofollow">[2019]An Analysis of Pre-Training on Object Detection</a></li><li><a href="#2019Rethinking_Classification_and_Localization_in_RCNN_122" rel="nofollow">[2019]Rethinking Classification and Localization in R-CNN</a></li><li><a href="#CVPR2019NASFPN_Learning_Scalable_Feature_Pyramid_Architecture_for_Object_Detection_128" rel="nofollow">[CVPR2019]NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection</a></li><li><a href="#2019Automated_Focal_Loss_for_Image_based_Object_Detection_132" rel="nofollow">[2019]Automated Focal Loss for Image based Object Detection</a></li><li><a href="#2019LFFD_A_Light_and_Fast_Face_Detector_for_Edge_Devices_136" rel="nofollow">[2019]LFFD: A Light and Fast Face Detector for Edge Devices</a></li><li><a href="#CVPR2019Exploring_Object_Relation_in_Mean_Teacher_for_CrossDomain_Detection_139" rel="nofollow">[CVPR2019]Exploring Object Relation in Mean Teacher for Cross-Domain Detection</a></li><li><a href="#2019HARNet_Joint_Learning_of_Hybrid_Attention_for_Singlestage_Object_Detection_142" rel="nofollow">[2019]HAR-Net: Joint Learning of Hybrid Attention for Single-stage Object Detection</a></li><li><a href="#2019An_Energy_and_GPUComputation_Efficient_Backbone_Network_for_RealTime_Object_Detection_145" rel="nofollow">[2019]An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection</a></li><li><a href="#2019RepPoints_Point_Set_Representation_for_Object_Detection_149" rel="nofollow">[2019]RepPoints: Point Set Representation for Object Detection</a></li><li><a href="#2019Object_Detection_in_20_Years_A_Survey_153" rel="nofollow">[2019]Object Detection in 20 Years: A Survey</a></li><li><a href="#AAAI2019SCNN_A_General_Distribution_based_Statistical_Convolutional_Neural_Network_with_Application_to_Video_Object_Detection_156" rel="nofollow">[AAAI2019]SCNN: A General Distribution based Statistical Convolutional Neural Network with Application to Video Object Detection</a></li><li><a href="#2019Looking_Fast_and_Slow_MemoryGuided_Mobile_Video_Object_Detection_160" rel="nofollow">[2019]Looking Fast and Slow: Memory-Guided Mobile Video Object Detection</a></li><li><a href="#2019Progressive_Sparse_Local_Attention_for_Video_object_detection_164" rel="nofollow">[2019]Progressive Sparse Local Attention for Video object detection</a></li><li><a href="#2019Exploring_the_Semantics_for_Visual_Relationship_Detection_168" rel="nofollow">[2019]Exploring the Semantics for Visual Relationship Detection</a></li><li><a href="#2019PyramidBox_High_Performance_Detector_for_Finding_Tiny_Face_171" rel="nofollow">[2019]PyramidBox++: High Performance Detector for Finding Tiny Face</a></li><li><a href="#ICPR2018MSFDMultiScale_Receptive_Field_Face_Detector_175" rel="nofollow">[ICPR2018]MSFD:Multi-Scale Receptive Field Face Detector</a></li><li><a href="#CVPR2018Improving_Occlusion_and_Hard_Negative_Handling_for_SingleStage_Pedestrian_Detectors_179" rel="nofollow">[CVPR2018]Improving Occlusion and Hard Negative Handling for Single-Stage Pedestrian Detectors</a></li><li><a href="#ECCV2018Bibox_Regression_for_Pedestrian_Detection_and_Occlusion_Estimation_183" rel="nofollow">[ECCV2018]Bi-box Regression for Pedestrian Detection and Occlusion Estimation</a></li><li><a href="#2019SSACNN_Semantic_SelfAttention_CNN_for_Pedestrian_Detection_188" rel="nofollow">[2019]SSA-CNN: Semantic Self-Attention CNN for Pedestrian Detection</a></li><li><a href="#2019Boxlevel_Segmentation_Supervised_Deep_Neural_Networks_for_Accurate_and_Realtime_Multispectral_Pedestrian_Detection_192" rel="nofollow">[2019]Box-level Segmentation Supervised Deep Neural Networks for Accurate and Real-time Multispectral Pedestrian Detection</a></li><li><a href="#2019GFDSSD_Gated_Fusion_Double_SSD_for_Multispectral_Pedestrian_Detection_195" rel="nofollow">[2019]GFD-SSD: Gated Fusion Double SSD for Multispectral Pedestrian Detection</a></li><li><a href="#CVPR2019Highlevel_Semantic_Feature_DetectionA_New_Perspective_for_Pedestrian_Detection_198" rel="nofollow">[CVPR2019]High-level Semantic Feature Detection:A New Perspective for Pedestrian Detection</a></li></ul> 
   </li><li><a href="#Pedestrian_Detection_in_a_Crowd_204" rel="nofollow">Pedestrian Detection in a Crowd</a></li><li><ul><li><a href="#CVPR2018Repulsion_Loss_Detecting_Pedestrians_in_a_Crowd_205" rel="nofollow">[CVPR2018]Repulsion Loss: Detecting Pedestrians in a Crowd</a></li><li><a href="#ECCV2018Occlusionaware_RCNN_Detecting_Pedestrians_in_a_Crowd_208" rel="nofollow">[ECCV2018]Occlusion-aware R-CNN: Detecting Pedestrians in a Crowd</a></li><li><a href="#CVPR2019Adaptive_NMS_Refining_Pedestrian_Detection_in_a_Crowd_212" rel="nofollow">[CVPR2019]Adaptive NMS: Refining Pedestrian Detection in a Crowd</a></li><li><a href="#2019Unsupervised_Domain_Adaptation_for_Multispectral_Pedestrian_Detection_216" rel="nofollow">[2019]Unsupervised Domain Adaptation for Multispectral Pedestrian Detection</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h4><a id="CVPR2019_Generalized_Intersection_over_Union_A_Metric_and_A_Loss_for_Bounding_Box_Regression_3"></a>[CVPR2019] Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression</h4> 
<ul><li>arxiv: <a href="https://arxiv.org/abs/1902.09630" rel="nofollow">https://arxiv.org/abs/1902.09630</a></li></ul> 
<h3><a id="anchorfree_6"></a>anchor-free</h3> 
<p>无锚框最近的热点，有机会研究下。</p> 
<h4><a id="CVPR2019_Region_Proposal_by_Guided_Anchoring_8"></a>[CVPR2019] Region Proposal by Guided Anchoring</h4> 
<ul><li>intro: CUHK - SenseTime Joint Lab &amp; Amazon Rekognition &amp; Nanyang Technological University</li><li>arxiv: <a href="https://arxiv.org/abs/1901.03278" rel="nofollow">https://arxiv.org/abs/1901.03278</a></li></ul> 
<h4><a id="CVPR2019_Feature_Selective_AnchorFree_Module_for_SingleShot_Object_Detection_12"></a>[CVPR2019] Feature Selective Anchor-Free Module for Single-Shot Object Detection</h4> 
<ul><li>intro: FSAF for Single-Shot Object Detection</li><li>arxiv: <a href="https://arxiv.org/abs/1903.00621" rel="nofollow">https://arxiv.org/abs/1903.00621</a></li></ul> 
<h4><a id="CVPR2019CenterNet_Keypoint_Triplets_for_Object_Detection_16"></a>[CVPR2019]CenterNet: Keypoint Triplets for Object Detection</h4> 
<ul><li>intro: CornerNet改进</li><li>arxiv: <a href="https://arxiv.org/abs/1904.08189" rel="nofollow">https://arxiv.org/abs/1904.08189</a></li><li>github: <a href="https://github.com/Duankaiwen/CenterNet">https://github.com/Duankaiwen/CenterNet</a></li></ul> 
<h4><a id="CVPR2019Objects_as_Points_21"></a>[CVPR2019]Objects as Points</h4> 
<ul><li>intro: CornerNet改进</li><li>arxiv: <a href="https://arxiv.org/pdf/1904.07850.pdf" rel="nofollow">https://arxiv.org/pdf/1904.07850.pdf</a></li><li>github: <a href="https://github.com/xingyizhou/CenterNet">https://github.com/xingyizhou/CenterNet</a></li></ul> 
<h4><a id="CVPR2019CornerNetLite_Efficient_Keypoint_Based_Object_Detection_27"></a>[CVPR2019]CornerNet-Lite: Efficient Keypoint Based Object Detection</h4> 
<ul><li>intro: CornerNet改进，mAP34.4%-34ms</li><li>arxiv: <a href="https://arxiv.org/abs/1904.08900" rel="nofollow">https://arxiv.org/abs/1904.08900</a></li><li>github: <a href="https://github.com/princeton-vl/CornerNet-Lite">https://github.com/princeton-vl/CornerNet-Lite</a></li></ul> 
<h4><a id="CVPR2019FoveaBox_Beyond_Anchorbased_Object_Detector_31"></a>[CVPR2019]FoveaBox: Beyond Anchor-based Object Detector</h4> 
<ul><li>intro:</li><li>arxiv: <a href="https://arxiv.org/abs/1904.03797" rel="nofollow">https://arxiv.org/abs/1904.03797</a></li><li>github:</li></ul> 
<h4><a id="2019DuBox_NoPrior_Box_Objection_Detection_via_Residual_Dual_Scale_Detectors_36"></a>[2019]DuBox: No-Prior Box Objection Detection via Residual Dual Scale Detectors</h4> 
<ul><li>intro: Baidu Inc.</li><li>arxiv: <a href="https://arxiv.org/abs/1904.06883" rel="nofollow">https://arxiv.org/abs/1904.06883</a></li></ul> 
<h3><a id="YOLO_40"></a>YOLO</h3> 
<h4><a id="2019SpikingYOLO_Spiking_Neural_Network_for_Realtime_Object_Detection_41"></a>[2019]Spiking-YOLO: Spiking Neural Network for Real-time Object Detection</h4> 
<ul><li>arxiv: <a href="https://arxiv.org/abs/1903.06530" rel="nofollow">https://arxiv.org/abs/1903.06530</a></li></ul> 
<h4><a id="CVPR2019Gaussian_YOLOv3_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving_44"></a>[CVPR2019]Gaussian YOLOv3: An Accurate and Fast Object Detector Using Localization Uncertainty for Autonomous Driving</h4> 
<ul><li>arxiv: <a href="https://arxiv.org/abs/1904.04620" rel="nofollow">https://arxiv.org/abs/1904.04620</a></li></ul> 
<h2><a id="_49"></a></h2> 
<h4><a id="AAAI2019Gradient_Harmonized_Singlestage_Detector_50"></a>[AAAI2019]Gradient Harmonized Single-stage Detector</h4> 
<ul><li>intro: AAAI 2019 Oral</li><li>arxiv: <a href="https://arxiv.org/abs/1811.05181" rel="nofollow">https://arxiv.org/abs/1811.05181</a></li><li>gihtub(official): <a href="https://github.com/libuyu/GHM_Detection">https://github.com/libuyu/GHM_Detection</a></li></ul> 
<h4><a id="2019Augmentation_for_small_object_detection_55"></a>[2019]Augmentation for small object detection</h4> 
<ul><li>arxiv: <a href="https://arxiv.org/abs/1902.07296" rel="nofollow">https://arxiv.org/abs/1902.07296</a></li></ul> 
<h4><a id="2019SimpleDet_A_Simple_and_Versatile_Distributed_Framework_for_Object_Detection_and_Instance_Recognition_59"></a>[2019]SimpleDet: A Simple and Versatile Distributed Framework for Object Detection and Instance Recognition</h4> 
<ul><li>intro: TuSimple</li><li>arxiv: <a href="https://arxiv.org/abs/1903.05831" rel="nofollow">https://arxiv.org/abs/1903.05831</a></li><li>github: <a href="https://github.com/tusimple/simpledet">https://github.com/tusimple/simpledet</a></li></ul> 
<h4><a id="2019BayesOD_A_Bayesian_Approach_for_Uncertainty_Estimation_in_Deep_Object_Detectors_64"></a>[2019]BayesOD: A Bayesian Approach for Uncertainty Estimation in Deep Object Detectors</h4> 
<ul><li>intro: University of Toronto</li><li>arxiv: <a href="https://arxiv.org/abs/1903.03838" rel="nofollow">https://arxiv.org/abs/1903.03838</a></li></ul> 
<h4><a id="2019DetNAS_Neural_Architecture_Search_on_Object_Detection_68"></a>[2019]DetNAS: Neural Architecture Search on Object Detection</h4> 
<ul><li>intro: Chinese Academy of Sciences &amp; Megvii Inc</li><li>arxiv: <a href="https://arxiv.org/abs/1903.10979" rel="nofollow">https://arxiv.org/abs/1903.10979</a></li></ul> 
<h4><a id="2019ThunderNet_Towards_Realtime_Generic_Object_Detection_72"></a>[2019]ThunderNet: Towards Real-time Generic Object Detection</h4> 
<p><a href="https://arxiv.org/abs/1903.11752" rel="nofollow">https://arxiv.org/abs/1903.11752</a></p> 
<h4><a id="2019Feature_Intertwiner_for_Object_Detection_76"></a>[2019]Feature Intertwiner for Object Detection</h4> 
<ul><li>intro: ICLR 2019</li><li>intro: CUHK &amp; SenseTime &amp; The University of Sydney</li><li>arxiv: <a href="https://arxiv.org/abs/1903.11851" rel="nofollow">https://arxiv.org/abs/1903.11851</a></li></ul> 
<h4><a id="CVPR2019Fewshot_Adaptive_Faster_RCNN_81"></a>[CVPR2019]Few-shot Adaptive Faster R-CNN</h4> 
<ul><li>intro: CVPR 2019</li><li>arxiv: <a href="https://arxiv.org/abs/1903.09372" rel="nofollow">https://arxiv.org/abs/1903.09372</a></li></ul> 
<h4><a id="2019Improving_Object_Detection_with_Inverted_Attention_85"></a>[2019]Improving Object Detection with Inverted Attention</h4> 
<ul><li>arxiv: <a href="https://arxiv.org/abs/1903.12255" rel="nofollow">https://arxiv.org/abs/1903.12255</a></li></ul> 
<h4><a id="2019FCOS_Fully_Convolutional_OneStage_Object_Detection_88"></a>[2019]FCOS: Fully Convolutional One-Stage Object Detection</h4> 
<ul><li>arxiv: <a href="https://arxiv.org/abs/1904.01355" rel="nofollow">https://arxiv.org/abs/1904.01355</a></li></ul> 
<h4><a id="CVPR2019Libra_RCNN_Towards_Balanced_Learning_for_Object_Detection_91"></a>[CVPR2019]Libra R-CNN: Towards Balanced Learning for Object Detection</h4> 
<ul><li>intro: CVPR 2019</li><li>arxiv: <a href="https://arxiv.org/abs/1904.02701" rel="nofollow">https://arxiv.org/abs/1904.02701</a></li></ul> 
<h4><a id="2019ComplexerYOLO_RealTime_3D_Object_Detection_and_Tracking_on_Semantic_Point_Clouds_95"></a>[2019]Complexer-YOLO: Real-Time 3D Object Detection and Tracking on Semantic Point Clouds</h4> 
<ul><li>arxiv: <a href="https://arxiv.org/abs/1904.07537" rel="nofollow">https://arxiv.org/abs/1904.07537</a></li><li></ul> 
<h4><a id="CVPR2019What_Object_Should_I_Use__Task_Driven_Object_Detection_98"></a>[CVPR2019]What Object Should I Use? - Task Driven Object Detection</h4> 
<p>intro: CVPR 2019<br> arxiv: <a href="https://arxiv.org/abs/1904.03000" rel="nofollow">https://arxiv.org/abs/1904.03000</a></p> 
<p>FoveaBox: Beyond Anchor-based Object Detector<br> intro: Tsinghua University &amp; BNRist &amp; ByteDance AI Lab &amp; University of Pennsylvania<br> arxiv: <a href="https://arxiv.org/abs/1904.03797" rel="nofollow">https://arxiv.org/abs/1904.03797</a></p> 
<h4><a id="CVPR2019Towards_Universal_Object_Detection_by_Domain_Attention_107"></a>[CVPR2019]Towards Universal Object Detection by Domain Attention</h4> 
<ul><li>intro: CVPR 2019</li><li>arxiv: <a href="https://arxiv.org/abs/1904.04402" rel="nofollow">https://arxiv.org/abs/1904.04402</a></li></ul> 
<h4><a id="2019Prime_Sample_Attention_in_Object_Detection_112"></a>[2019]Prime Sample Attention in Object Detection</h4> 
<ul><li>arxiv: <a href="https://arxiv.org/abs/1904.04821" rel="nofollow">https://arxiv.org/abs/1904.04821</a></li></ul> 
<h4><a id="2019BAOD_BudgetAware_Object_Detection_115"></a>[2019]BAOD: Budget-Aware Object Detection</h4> 
<ul><li>arxiv: <a href="https://arxiv.org/abs/1904.05443" rel="nofollow">https://arxiv.org/abs/1904.05443</a></li></ul> 
<h4><a id="2019An_Analysis_of_PreTraining_on_Object_Detection_118"></a>[2019]An Analysis of Pre-Training on Object Detection</h4> 
<ul><li>intro: University of Maryland</li><li>arxiv: <a href="https://arxiv.org/abs/1904.05871" rel="nofollow">https://arxiv.org/abs/1904.05871</a></li></ul> 
<h4><a id="2019Rethinking_Classification_and_Localization_in_RCNN_122"></a>[2019]Rethinking Classification and Localization in R-CNN</h4> 
<ul><li>intro: Northeastern University &amp; Microsoft</li><li>arxiv: <a href="https://arxiv.org/abs/1904.06493" rel="nofollow">https://arxiv.org/abs/1904.06493</a></li></ul> 
<h4><a id="CVPR2019NASFPN_Learning_Scalable_Feature_Pyramid_Architecture_for_Object_Detection_128"></a>[CVPR2019]NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection</h4> 
<ul><li>intro: CVPR 2019, Google Brain</li><li>arxiv: <a href="https://arxiv.org/abs/1904.07392" rel="nofollow">https://arxiv.org/abs/1904.07392</a></li></ul> 
<h4><a id="2019Automated_Focal_Loss_for_Image_based_Object_Detection_132"></a>[2019]Automated Focal Loss for Image based Object Detection</h4> 
<ul><li>arxiv: <a href="https://arxiv.org/abs/1904.09048" rel="nofollow">https://arxiv.org/abs/1904.09048</a></li></ul> 
<h4><a id="2019LFFD_A_Light_and_Fast_Face_Detector_for_Edge_Devices_136"></a>[2019]LFFD: A Light and Fast Face Detector for Edge Devices</h4> 
<ul><li>arxiv: <a href="https://arxiv.org/abs/1904.10633" rel="nofollow">https://arxiv.org/abs/1904.10633</a></li></ul> 
<h4><a id="CVPR2019Exploring_Object_Relation_in_Mean_Teacher_for_CrossDomain_Detection_139"></a>[CVPR2019]Exploring Object Relation in Mean Teacher for Cross-Domain Detection</h4> 
<ul><li>intro: CVPR 2019</li><li>arxiv: <a href="https://arxiv.org/abs/1904.11245" rel="nofollow">https://arxiv.org/abs/1904.11245</a></li></ul> 
<h4><a id="2019HARNet_Joint_Learning_of_Hybrid_Attention_for_Singlestage_Object_Detection_142"></a>[2019]HAR-Net: Joint Learning of Hybrid Attention for Single-stage Object Detection</h4> 
<ul><li>arxiv: <a href="https://arxiv.org/abs/1904.11141" rel="nofollow">https://arxiv.org/abs/1904.11141</a></li></ul> 
<h4><a id="2019An_Energy_and_GPUComputation_Efficient_Backbone_Network_for_RealTime_Object_Detection_145"></a>[2019]An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection</h4> 
<p>intro: CVPR 2019 CEFRL Workshop<br> arxiv: <a href="https://arxiv.org/abs/1904.09730" rel="nofollow">https://arxiv.org/abs/1904.09730</a></p> 
<h4><a id="2019RepPoints_Point_Set_Representation_for_Object_Detection_149"></a>[2019]RepPoints: Point Set Representation for Object Detection</h4> 
<ul><li>intro: Peking University &amp; Tsinghua University &amp; Microsoft Research Asia</li><li>arxiv: <a href="https://arxiv.org/abs/1904.11490" rel="nofollow">https://arxiv.org/abs/1904.11490</a></li></ul> 
<h4><a id="2019Object_Detection_in_20_Years_A_Survey_153"></a>[2019]Object Detection in 20 Years: A Survey</h4> 
<ul><li>arxiv: <a href="https://arxiv.org/abs/1905.05055" rel="nofollow">https://arxiv.org/abs/1905.05055</a></li></ul> 
<h4><a id="AAAI2019SCNN_A_General_Distribution_based_Statistical_Convolutional_Neural_Network_with_Application_to_Video_Object_Detection_156"></a>[AAAI2019]SCNN: A General Distribution based Statistical Convolutional Neural Network with Application to Video Object Detection</h4> 
<ul><li>intro: AAAI 2019</li><li>arxiv: <a href="https://arxiv.org/abs/1903.07663" rel="nofollow">https://arxiv.org/abs/1903.07663</a></li></ul> 
<h4><a id="2019Looking_Fast_and_Slow_MemoryGuided_Mobile_Video_Object_Detection_160"></a>[2019]Looking Fast and Slow: Memory-Guided Mobile Video Object Detection</h4> 
<ul><li>intro: Cornell University &amp; Google AI</li><li>arxiv: <a href="https://arxiv.org/abs/1903.10172" rel="nofollow">https://arxiv.org/abs/1903.10172</a></li></ul> 
<h4><a id="2019Progressive_Sparse_Local_Attention_for_Video_object_detection_164"></a>[2019]Progressive Sparse Local Attention for Video object detection</h4> 
<ul><li>intro: NLPR,CASIA &amp; Horizon Robotics</li><li>arxiv: <a href="https://arxiv.org/abs/1903.09126" rel="nofollow">https://arxiv.org/abs/1903.09126</a></li></ul> 
<h4><a id="2019Exploring_the_Semantics_for_Visual_Relationship_Detection_168"></a>[2019]Exploring the Semantics for Visual Relationship Detection</h4> 
<ul><li>arxiv: <a href="https://arxiv.org/abs/1904.02104" rel="nofollow">https://arxiv.org/abs/1904.02104</a></li></ul> 
<h4><a id="2019PyramidBox_High_Performance_Detector_for_Finding_Tiny_Face_171"></a>[2019]PyramidBox++: High Performance Detector for Finding Tiny Face</h4> 
<ul><li>intro: Chinese Academy of Sciences &amp; Baidu, Inc.</li><li>arxiv: <a href="https://arxiv.org/abs/1904.00386" rel="nofollow">https://arxiv.org/abs/1904.00386</a></li></ul> 
<h4><a id="ICPR2018MSFDMultiScale_Receptive_Field_Face_Detector_175"></a>[ICPR2018]MSFD:Multi-Scale Receptive Field Face Detector</h4> 
<ul><li>intro: ICPR 2018</li><li>arxiv: <a href="https://arxiv.org/abs/1903.04147" rel="nofollow">https://arxiv.org/abs/1903.04147</a></li></ul> 
<h4><a id="CVPR2018Improving_Occlusion_and_Hard_Negative_Handling_for_SingleStage_Pedestrian_Detectors_179"></a>[CVPR2018]Improving Occlusion and Hard Negative Handling for Single-Stage Pedestrian Detectors</h4> 
<ul><li>intro: CVPR 2018</li><li>paper: <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Noh_Improving_Occlusion_and_CVPR_2018_paper.pdf" rel="nofollow">http://openaccess.thecvf.com/content_cvpr_2018/papers/Noh_Improving_Occlusion_and_CVPR_2018_paper.pdf</a></li></ul> 
<h4><a id="ECCV2018Bibox_Regression_for_Pedestrian_Detection_and_Occlusion_Estimation_183"></a>[ECCV2018]Bi-box Regression for Pedestrian Detection and Occlusion Estimation</h4> 
<ul><li>intro: ECCV 2018</li><li>paper: <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/CHUNLUAN_ZHOU_Bi-box_Regression_for_ECCV_2018_paper.pdf" rel="nofollow">http://openaccess.thecvf.com/content_ECCV_2018/papers/CHUNLUAN_ZHOU_Bi-box_Regression_for_ECCV_2018_paper.pdf</a></li><li>github(Pytorch): <a href="https://github.com/rainofmine/Bi-box_Regression">https://github.com/rainofmine/Bi-box_Regression</a></li></ul> 
<h4><a id="2019SSACNN_Semantic_SelfAttention_CNN_for_Pedestrian_Detection_188"></a>[2019]SSA-CNN: Semantic Self-Attention CNN for Pedestrian Detection</h4> 
<ul><li>arxiv: <a href="https://arxiv.org/abs/1902.09080" rel="nofollow">https://arxiv.org/abs/1902.09080</a></li></ul> 
<h4><a id="2019Boxlevel_Segmentation_Supervised_Deep_Neural_Networks_for_Accurate_and_Realtime_Multispectral_Pedestrian_Detection_192"></a>[2019]Box-level Segmentation Supervised Deep Neural Networks for Accurate and Real-time Multispectral Pedestrian Detection</h4> 
<ul><li>arxiv: <a href="https://arxiv.org/abs/1902.05291" rel="nofollow">https://arxiv.org/abs/1902.05291</a></li></ul> 
<h4><a id="2019GFDSSD_Gated_Fusion_Double_SSD_for_Multispectral_Pedestrian_Detection_195"></a>[2019]GFD-SSD: Gated Fusion Double SSD for Multispectral Pedestrian Detection</h4> 
<ul><li>arxiv: <a href="https://arxiv.org/abs/1903.06999" rel="nofollow">https://arxiv.org/abs/1903.06999</a></li><li></ul> 
<h4><a id="CVPR2019Highlevel_Semantic_Feature_DetectionA_New_Perspective_for_Pedestrian_Detection_198"></a>[CVPR2019]High-level Semantic Feature Detection:A New Perspective for Pedestrian Detection</h4> 
<ul><li>intro: CVPR 2019</li><li>intro: National University of Defense Technology &amp; Chinese Academy of Sciences &amp; Inception Institute of Artificial Intelligence (IIAI) &amp; Horizon Robotics Inc.</li><li>arxiv: <a href="https://arxiv.org/abs/1904.02948" rel="nofollow">https://arxiv.org/abs/1904.02948</a></li><li>github(official, Keras): <a href="https://github.com/liuwei16/CSP">https://github.com/liuwei16/CSP</a></li></ul> 
<h3><a id="Pedestrian_Detection_in_a_Crowd_204"></a>Pedestrian Detection in a Crowd</h3> 
<h4><a id="CVPR2018Repulsion_Loss_Detecting_Pedestrians_in_a_Crowd_205"></a>[CVPR2018]Repulsion Loss: Detecting Pedestrians in a Crowd</h4> 
<ul><li>intro: CVPR 2018</li><li>arxiv: <a href="https://arxiv.org/abs/1711.07752" rel="nofollow">https://arxiv.org/abs/1711.07752</a></li></ul> 
<h4><a id="ECCV2018Occlusionaware_RCNN_Detecting_Pedestrians_in_a_Crowd_208"></a>[ECCV2018]Occlusion-aware R-CNN: Detecting Pedestrians in a Crowd</h4> 
<ul><li>intro: ECCV 2018</li><li>arxiv: <a href="https://arxiv.org/abs/1807.08407" rel="nofollow">https://arxiv.org/abs/1807.08407</a></li></ul> 
<h4><a id="CVPR2019Adaptive_NMS_Refining_Pedestrian_Detection_in_a_Crowd_212"></a>[CVPR2019]Adaptive NMS: Refining Pedestrian Detection in a Crowd</h4> 
<ul><li>intro: CVPR 2019 oral</li><li>arxiv: <a href="https://arxiv.org/abs/1904.03629" rel="nofollow">https://arxiv.org/abs/1904.03629</a></li></ul> 
<h4><a id="2019Unsupervised_Domain_Adaptation_for_Multispectral_Pedestrian_Detection_216"></a>[2019]Unsupervised Domain Adaptation for Multispectral Pedestrian Detection</h4> 
<ul><li>arxiv: <a href="https://arxiv.org/abs/1904.03692" rel="nofollow">https://arxiv.org/abs/1904.03692</a></li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/27bc44cb91555dc0752206f5b4a0ade7/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">计算机视觉算法岗面试</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/35a3db8981bb3493da0a91526e056513/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">非root权限scp免密传输文件</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>