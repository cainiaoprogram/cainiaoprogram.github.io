<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>SLAM常见问题汇总 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="SLAM常见问题汇总" />
<meta property="og:description" content="注：本文内容大多数来源于“计算机视觉life”微信公众号，因文章较多，笔者记忆又不强，所以整理成博客形式，感兴趣的伙伴可以移步微信公众号浏览更多精彩内容哦！
一.为什么要学SLAM？ 计算机视觉主要分为两大方向：基于学习的方法（代表：深度学习）和基于几何的方法（代表：视觉SLAM）
1.深度学习在检测、识别领域具有强大能力，但涉及多视角几何相关的SLAM领域，深度学习作用非常有限，SLAM需要清晰的理论基础保证，而深度学习[黑盒子]模型目前还不奏效。
2.SLAM技术门槛高。需要具备三维空间刚体变换、相机成像模型、特征点提取与匹配、多视角几何、捆集调整等内容。
3.消费级RGB-D相机快速发展催生了以三维视觉为基础的商业化应用。
4.目前据算计视觉领域主要还是通过二维图片来感知世界，而三维视觉才是人类感知理解世界的正确方式，因此以三维视觉为基础的SLAM技术是机器人、无人驾驶、AR等人工智能细分领域的核心技术。
5.SLAM需求公司：互联网公司如百度、腾讯、阿里、京东等，计算机视觉算法公司如旷世、虹软、商汤等，自动驾驶创业公司如图森、momenta、景驰、驭势、滴滴及各大汽车厂商等，无人机/机器人公司如大疆、思岚、高仙等，AR移动终端应用相关公司如三星、华为、悉见等。
二.SLAM到底做什么？ SLAM是指当某种移动设备（如机器人、无人机、手机等）从一个未知环境里的未知地点出发，在运动过程中通过传感器（如激光雷达、摄像头等）观测定位自身位置、姿态、运动轨迹，再根据自身位置进行增量式的地图构建，从而达到同时定位和地图构建的目的。定位和建图是两个相辅相成的过程，地图可以提供更好的定位，而定位也可以进一步扩建地图。定位和建图是SLAM的基本要求，而路径规划是在此基础上的高级功能，不属于SLAM的讨论范畴。
三.SLAM使用的传感器 SLAM使用的传感器主要分为激光雷达和视觉两大类。
1.早期SLAM研究几乎全使用激光雷达，优势精度高，解决方案相对成熟，但价格贵，体积大，信息少不直观。
2.视觉SLAM就是用摄像头做主传感器，用拍摄的视频流作为输入来实现同时定位于建图。广泛应用于AR，自动驾驶，机器 人（京东，阿里等大型电商已经配备了仓储机器人，根据任务需求进行路径规划），无人机等前沿领域。
四. 手机地图APP即可定位，为什么还要SLAM？ 地图类APP的定位主要应用GPS，民用GPS定位精度为米，导航时并不知道当前车在哪个车道上。且GPS只能在室外使 用，而在建筑物内，洞穴，海底等很多地方GPS失效，对于这些地方定位需求更强烈，目前最有效的就是SLAM技术。
五.建图相关应用有哪些？ 根据相机扫描图片进行三维重建，包括室内及室外。利用室内场景的三维重建可以玩增强现实游戏；室内机器人可以判断障碍物 距离，识别理解环境，进行导航等；还可以将二维图片和重建结果进行融合实现三维漫游等。
六.SLAM需要学哪些？ SLAM里涉及很多图像处理、计算机视觉知识，总结一下主要有：
相机相关：单目、双目、RGB-D等相机的物理参数意义、相机成像模型、相机的标定、去畸变等。双目的话还涉及到视差计算，RGB-D的话涉及到RGB和depth图像的对齐等。
图像处理相关。比如和特征点相关的有：特征点描述子、特征点提取、特征点匹配。图像梯度计算、边缘检测、直线检测等。
多视角几何相关。比如对极约束、本质矩阵、单应矩阵、光流估计、三角化等。
开源代码：
虽然SLAM比较难，但是令人欣慰的是，SLAM领域有很多优秀的开源代码可以学习。列举几个主流的如下：
稀疏法：
ORB-SLAM2：支持单目，双目，RGB-D相机 https://github.com/raulmur/ORB_SLAM2
半稠密法：
LSD-SLAM：支持单目，双目，RGB-D相机 https://vision.in.tum.de/research/vslam/lsdslam
DSO：单目https://vision.in.tum.de/research/vslam/dso
稠密法：
Elastic Fusion：RGB-D相机 https://github.com/mp3guy/ElasticFusion
BundleFusion：RGB-D相机 https://github.com/niessner/BundleFusion
RGB-D SLAM V2：RGB-D相机 https://github.com/felixendres/rgbdslam_v2
多传感器融合：
VINS：单目 &#43; IMU（惯性测量单元）https://github.com/HKUST-Aerial-Robotics/VINS-Mono
OKVIS：（单目、双目、四目）&#43; IMU https://wp.doc.ic.ac.uk/sleutene/2016/02/04/release-of-okvis-open-keyframe-based-visual-inertial-slam/
数据集：
１、TUM RGB-D SLAM Dataset and Benchmark
德国慕尼黑理工大学计算机视觉组制作的数据集，使用Kinect相机采集的数据集，包括IMU数据，并且用高精度运动采集系统提供了groundtruth（真值）。提供测试脚本，可以方便的实现量化评估。https://vision.in.tum.de/data/datasets/rgbd-dataset。
２、KITTI Vision Benchmark Suite
德国卡尔斯鲁厄理工学院和丰田工业大学芝加哥分校一起合作制作的用于自动驾驶的数据集。使用一辆改装的汽车采集，该车配备了两台高分辨率彩色和灰度摄像机，还有Velodyne激光扫描仪和GPS定位系统，用来提供精确的groundtruth。主要采集区域是卡尔斯鲁厄市区、农村地区和高速公路。提供测试脚本可以方便的实现量化评估。http://www.cvlibs.net/datasets/kitti/
３、EuRoC MAV Dataset" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/f4c7d670d31c7dfdafa27d85ab66616d/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-12-02T17:48:17+08:00" />
<meta property="article:modified_time" content="2019-12-02T17:48:17+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">SLAM常见问题汇总</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>注：本文内容大多数来源于“计算机视觉life”微信公众号，因文章较多，笔者记忆又不强，所以整理成博客形式，感兴趣的伙伴可以移步微信公众号浏览更多精彩内容哦！</p> 
<h4>一.为什么要学SLAM？</h4> 
<p>计算机视觉主要分为两大方向：基于学习的方法（代表：深度学习）和基于几何的方法（代表：视觉SLAM）<br> 1.深度学习在检测、识别领域具有强大能力，但涉及多视角几何相关的SLAM领域，深度学习作用非常有限，SLAM需要清晰的理论基础保证，而深度学习[黑盒子]模型目前还不奏效。<br> 2.SLAM技术门槛高。需要具备三维空间刚体变换、相机成像模型、特征点提取与匹配、多视角几何、捆集调整等内容。<br> 3.消费级RGB-D相机快速发展催生了以三维视觉为基础的商业化应用。<br> 4.目前据算计视觉领域主要还是通过二维图片来感知世界，而三维视觉才是人类感知理解世界的正确方式，因此以三维视觉为基础的SLAM技术是机器人、无人驾驶、AR等人工智能细分领域的核心技术。<br> 5.SLAM需求公司：互联网公司如百度、腾讯、阿里、京东等，计算机视觉算法公司如旷世、虹软、商汤等，自动驾驶创业公司如图森、momenta、景驰、驭势、滴滴及各大汽车厂商等，无人机/机器人公司如大疆、思岚、高仙等，AR移动终端应用相关公司如三星、华为、悉见等。</p> 
<h4>二.SLAM到底做什么？</h4> 
<p><br>     <strong>SLAM是指当某种移动设备（如机器人、无人机、手机等）从一个未知环境里的未知地点出发，在运动过程中通过传感器（如激光雷达、摄像头等）观测定位自身位置、姿态、运动轨迹，再根据自身位置进行增量式的地图构建，从而达到同时定位和地图构建的目的</strong>。定位和建图是两个相辅相成的过程，地图可以提供更好的定位，而定位也可以进一步扩建地图。定位和建图是SLAM的基本要求，而路径规划是在此基础上的高级功能，不属于SLAM的讨论范畴。</p> 
<h4>三.SLAM使用的传感器</h4> 
<p style="text-indent:0;"><br>      <strong>SLAM使用的传感器主要分为激光雷达和视觉两大类。</strong><br>      1.早期SLAM研究几乎全使用激光雷达，优势精度高，解决方案相对成熟，但价格贵，体积大，信息少不直观。<br>      2.视觉SLAM就是用摄像头做主传感器，用拍摄的视频流作为输入来实现同时定位于建图。广泛应用于AR，自动驾驶，机器                人（京东，阿里等大型电商已经配备了仓储机器人，根据任务需求进行路径规划），无人机等前沿领域。</p> 
<h4>四. <strong>手机地图APP即可定位，为什么还要SLAM？</strong> </h4> 
<p style="text-indent:0;">   <br>      地图类APP的定位主要应用GPS，民用GPS定位精度为米，导航时并不知道当前车在哪个车道上。且GPS只能在室外使                   用，而在建筑物内，洞穴，海底等很多地方GPS失效，对于这些地方定位需求更强烈，目前最有效的就是SLAM技术。</p> 
<h4>五.建图相关应用有哪些？</h4> 
<p style="text-indent:0;"><br>     根据相机扫描图片进行三维重建，包括室内及室外。利用室内场景的三维重建可以玩增强现实游戏；室内机器人可以判断障碍物      距离，识别理解环境，进行导航等；还可以将二维图片和重建结果进行融合实现三维漫游等。</p> 
<h4>六.SLAM需要学哪些？</h4> 
<p style="text-indent:0;"><img alt="" class="has" height="1184" src="https://images2.imgbox.com/b8/3e/INWeztHi_o.png" width="1200"><br>  </p> 
<p><strong>SLAM里涉及很多图像处理、计算机视觉知识，总结一下主要有：</strong></p> 
<p><strong>相机相关</strong>：单目、双目、RGB-D等相机的物理参数意义、相机成像模型、相机的标定、去畸变等。双目的话还涉及到视差计算，RGB-D的话涉及到RGB和depth图像的对齐等。</p> 
<p><strong>图像处理</strong>相关。比如和特征点相关的有：特征点描述子、特征点提取、特征点匹配。图像梯度计算、边缘检测、直线检测等。</p> 
<p><strong>多视角几何</strong>相关。比如对极约束、本质矩阵、单应矩阵、光流估计、三角化等。<br><br><strong>开源代码：</strong><br> 虽然SLAM比较难，但是令人欣慰的是，SLAM领域有很多<strong>优秀的开源代码</strong>可以学习。列举几个主流的如下：</p> 
<p><strong>稀疏法：</strong><br> ORB-SLAM2：支持单目，双目，RGB-D相机 https://github.com/raulmur/ORB_SLAM2<br><strong>半稠密法：</strong><br> LSD-SLAM：支持单目，双目，RGB-D相机 https://vision.in.tum.de/research/vslam/lsdslam<br> DSO：单目https://vision.in.tum.de/research/vslam/dso<br><strong>稠密法：</strong><br> Elastic Fusion：RGB-D相机 https://github.com/mp3guy/ElasticFusion<br> BundleFusion：RGB-D相机  https://github.com/niessner/BundleFusion<br> RGB-D SLAM V2：RGB-D相机 https://github.com/felixendres/rgbdslam_v2<br><strong>多传感器融合：</strong><br> VINS：单目 + IMU（惯性测量单元）https://github.com/HKUST-Aerial-Robotics/VINS-Mono<br> OKVIS：（单目、双目、四目）+ IMU https://wp.doc.ic.ac.uk/sleutene/2016/02/04/release-of-okvis-open-keyframe-based-visual-inertial-slam/</p> 
<p><strong>数据集：</strong><br> １、TUM RGB-D SLAM Dataset and Benchmark<br>     德国慕尼黑理工大学计算机视觉组制作的数据集，使用Kinect相机采集的数据集，包括IMU数据，并且用高精度运动采集系统提供了groundtruth（真值）。提供测试脚本，可以方便的实现量化评估。https://vision.in.tum.de/data/datasets/rgbd-dataset。<br> ２、KITTI Vision Benchmark Suite<br>     德国卡尔斯鲁厄理工学院和丰田工业大学芝加哥分校一起合作制作的用于自动驾驶的数据集。使用一辆改装的汽车采集，该车配备了两台高分辨率彩色和灰度摄像机，还有Velodyne激光扫描仪和GPS定位系统，用来提供精确的groundtruth。主要采集区域是卡尔斯鲁厄市区、农村地区和高速公路。提供测试脚本可以方便的实现量化评估。http://www.cvlibs.net/datasets/kitti/<br> ３、EuRoC MAV Dataset<br>     苏黎世联邦理工大学制作的数据集，采用装备了双目相机和IMU的四旋翼无人机采集数据，使用高精度运动采集系统提供了groundtruth。提供测试脚本，可以方便的实现量化评估。<br> https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b8b8010458b7561ef224ecc68e45422b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">java实现HTTP请求的三种方式</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/af208e830422f422b3c4b23f5fd84043/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">ncnn中Yolov3DetectionOutput层参数含义</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>