<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>CVPR2020 步态论文：Gait Recognition via Semi-supervised Disentangled Representation Learning to Identit - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="CVPR2020 步态论文：Gait Recognition via Semi-supervised Disentangled Representation Learning to Identit" />
<meta property="og:description" content="论文题目：Gait Recognition via Semi-supervised Disentangled Representation Learning to Identity and Covariate Features
通过半监督解耦表示 学习身份和协变特征的步态识别
摘要
现有的步态识别方法通常侧重于学习与协变量（如携带状态、服装、步行速度和视角）不变性的身份特征，很少涉及从协变量方面学习特征，当协变量引起的变化压倒同一性的变化时，可能导致失效。因此，我们提出了一种结合身份和协变量特征的解耦表示学习的步态识别方法。具体地说，我们首先对输入的步态模板进行编码，得到解耦的身份特征和协变量特征，然后对特征进行解码，以半监督的方式同时重建输入的步态模板和同一对象的无协变量的标准版本，以确保解耦成功。最后，我们将分离出的身份特征输入到对比/三重态丢失函数中，用于验证/识别任务。此外，我们发现，通过将协变量特征从一个对象转移到另一个对象，可以合成新的步态模板。在三个可公开获取的步态数据集上的实验结果证明了该方法的有效性。
介绍
步态是一种重要的生物特征，用于远距离的人类身份识别，因为与其他生物特征(如面部、指纹和虹膜)相比，它可以在不需要主体合作的情况下在远距离被记录。此外，步态是一种无意识的特征，一般不会被人伪装。基于步态的识别因此有许多潜在的应用，如监视系统、取证和刑事调查[6,20,31]。
以前的步态识别研究可大致分为以下几类：
根据提取的特征分类为基于模型的方法[47、53、27、5、10、54、2]和基于外观的方法[15、32、44、24、48、4、37、29、59]。基于外观的步态识别方法由于其有效性和高效性，在步态识别领域得到了广泛的应用。但由于携带身份、着装、姿势变化、视角等协变量较多，因此存在较大的主体内差异。
基于外观的步态识别需要提取不受协变量影响的身份特征。这种不变方法分为两大类:判别法[15,52,30,12,13,33,41,50,51,43,58,7,26]和生成法[23,32,34,38,35,1,11,55,56,16]。前者的目的是直接从原始的步态表示中提取不变的恒等特征子空间，而后者的目的是将不同协变条件下的步态表示生成相同协变条件下的步态表示。但是，它们都侧重于学习协变不变的特征子空间或图像空间，很少涉及协变方面的特征学习，当协变带来的变化超过了恒等带来的变化时，可能会导致失效模式。
为了解决上述问题，我们提出了一种基于外观的步态识别采用解耦表征学习（DRL）同时考虑身份和协变量特征。这个想法的灵感来自于先前的工作[60]（CVPR2019的文章，可从博客中寻找），其中姿势和外观特征从RGB图像中分离出来，并且基于LSTM的姿势特征随时间的整合被用于步态识别。尽管服装的颜色和质地等外观特征的影响被成功地消除了，但我们认为RGB图像的使用仍然存在不足。首先，张等[60]假设两个条件来分离姿势和外观特征，一个是在一个序列中外观特征是一致的，另一个是每个训练对象至少包含两个外观特征完全不同的序列。然而，这些条件可能并不总是满足的，从而导致外观因素污染姿势特征。一个例子，如果在一个序列中光照条件突然改变（包括通过肢体运动使体表法线相对于入射光方向改变的情况），则第一个条件可能不满足；第二，如果训练对象只是部分地更换衣服或换成颜色不同但质地相似的衣服。文献[60]将RGB输入的颜色和纹理信息视为一种协变量，可以像许多步态识别工作一样，简单地使用基于轮廓的表示来处理。 因此，我们将[60]的分离思想扩展到直接从基于轮廓的步态表示中分离身份和协变量特征。我们将协变量分为两类，它们对步态表征有不同的影响，可能需要不同的分离策略。具体地说，第一类包括在身体上改变受试者身体形状的携带状态和衣服，并且该类具有一类明确的规范条件，即没有协变量的步态模板。作为一个例子，我们将没有携带对象（COs）和足够紧的衣服（例如，穿着紧身衣的对象）的步态模板作为携带状态和衣服的标准条件。第二类包括视角，它引入了所有主体之间的共同变化，而这类视角并没有一个明确和合适的标准条件来适用于所有的视角。本文着重于第一类。同样对于步态表示，我们选择步态能量图像（GEI）[15]，这是步态识别领域中使用最广泛的步态表示。
具体地说，我们首先使用编码器将输入的GEI分解成低维身份和协变量特征。然后我们使用一个解码器执行两个重建；一种是从分离的同一性和协变量特征中重建输入GEI，另一种是从分离的同一性和零填充协变量特征重建另一个与输入GEI相同但没有协变量（规范条件）的GEI，其中我们给出了基本真实GEI没有协变量。通过这种设计，我们可以成功地将输入GEI分解成同一性和协变量特征。最后，我们将一对或三元组的身份特征输入到验证/识别任务的对比/三元组损失中。
为此：
（1）我们使用基于轮廓的步态表示来避免不必要的颜色和纹理协变量；
（2）我们探索DRL，以分离身份和更常见但更困难的协变量，如携带状态和衣服；（3）我们通过以半监督的方式同时重建输入GEI及其标准版本来克服污染问题（即，我们给没有协变量的GEI（例如，“无协变量”）而不是在训练阶段给其他有协变量的GEI添加协变量标签）。
此外，我们发现，给定来自一个被摄体1的解耦身份特征和来自另一个被摄体2的解耦协变量特征，解码器可以重构具有从1表征的相同身份和从2表征的相同协变量的新GEI样本，如图1所示。因此，我们可以自由地将协变量特征从一个主题转移到另一个主题，从而生成新的GEI，我们称之为GEI编辑。
图1：在给定两名受试者(S1和S2)不同携带状态的步态模板后，我们的方法隐式地分离他们的身份和协变量特征(fid和fcov)，并仅使用身份特征重建不携带任何身份的步态模板。此外，我们还可以交换研究对象的协变量特征，并根据各自的携带状态生成新的步态模板。
我们将我们的贡献总结如下。
1） 一种用于步态识别的基于身份和协变量特征的分离网络。ICDNet
我们首次引入半监督DRL来分离身份和协变量特征，用于步态识别。分离后的身份特征具有纯粹性和区分性，可用于步态识别。
2） GEI编辑：从一个主题到另一个主题的协变量转移。
我们可以通过将分离的协变量特征从一个主体转移到另一个主体来生成新的GEIＧＥＩ。这对今后步态识别中数据增强的研究具有一定的参考价值。
3） 最先进的表现。
我们在三个公开可用的步态数据库上取得了最先进的性能：具有真实COs的OU-ISIR大人口步态数据库（OU LP Bag）[46]、OU-ISIR步态数据库、带有Bag版本的大群体数据集（OU LP Bag）[33]和CASIA-B步态数据库[57]。
2.相关工作
基于外观的步态识别方法主要分为判别法和生成法。第一类旨在利用传统的度量学习技术(如线性判别分析[15]、张量表示的判别分析[52]、随机子空间方法[13]、联合强度和空间度量学习[33])或当前的深度神经网络提取一种针对协变量的判别子空间。特别是，基于深度学习的方法更受欢迎，因为它们具有更高的性能。例如Shiraga等人[41]提出了一种用于对具有交叉熵损失的单输入GEIs进行分类的轻型卷积神经网络(light convolutional neural network, CNN)。此后，一些研究[51,43,26]对具有对比或三元损失的输入GEIs对或三联体进行了相似性学习。Wolf et al.[50]和Chao et al.[7]直接为剪影帧设计了cnn，而不是将GEIs作为网络输入。
第二类旨在使用子空间分析技术[23、32、34、38、35、1]或生成对抗网络（GAN）[55、56、16]，将不同协变量条件下的步态表示生成为相同协变量条件下的步态表示。 例如，Makihara等。 [32]提出了一种视图转换模型，将步态特征从画廊视图条件转换为探针视图条件。 Yu等。 [55，56]提出了基于GAN的生成网络，称为GaitGAN和GaitGANv2，它们可以从任何带有协变量的输入GEI生成正常状态下侧视图的不变GEI。
然而，上述方法集中于学习不变地用于协变量的身份特征子空间或图像空间，并且很少考虑从协变量方面学习特征，当变化时可能导致失败。由于协变量而不是由于同一性。 相比之下，我们的方法同时考虑了身份和协变量特征学习，并获得了明显的身份认同特征与协变量特征的消融。
解耦表示学习
DRL被期望通过将数据的底层结构分离成不相交的有意义的变量来提供收益，这有助于澄清深层模型并确定实际学习到的隐藏特征类型。Zhang等人。[60]首次将DRL引入步态识别领域，从RGB图像中分离出受试者的姿势和外观特征。虽然DRL在步态研究中是一个新的领域，但在其他生物特征识别（如人脸识别）的研究中已经得到了很好的探索。例如，Tran等人。[45]彭等。从人脸识别中分离出40个姿势不变的人脸图像。[61]通过年龄变化的分解，生成年龄不变的人脸特征。
与文献[60]相比，我们的方法避免了RGB信息分离的困难，并为基于轮廓的步态表示提供了新的有意义的分离变量（即身份特征和协变量特征）。与人脸识别[45，40，61]中需要附加协变量标签（例如，姿势或年龄标签）的DRL相比，我们的方法针对的是没有明确标签（除了“无协变量”标签），因为我们在论文中针对的协变量（即携带状态和衣服）没有清晰的标签。例如，即使是同一个手提行李，其携带状态（例如形状和位置）也很大程度上取决于物品。在我们的半监督DRL中，只需要部分训练对象的典型条件（即“无协变量”）的标签。
3.提出的方法
3.1概述
我们提出了一种适用于步态识别的方法DRL从GEI中分离身份和协变量特征。在我们的问题设置中，我们假设每个训练对象都有一个没有协变量的步态模板（例如，一个没有COs的GEI），而我们没有其他步态模板的协变量条件标签（例如，一个受试者可能携带背包、公文包、手提箱甚至什么都没有，但我们从未事先知道，它还提供了一个测试用例）。因此，我们尽量使部分标签“无协变量”更好地分离。[1]
图2示出了所提议的概述。基本解缠模块由一个编码流和两个解码流组成，处理训练集中的所有gei。然后将一对（probe和gallery）或三元组（query、true和imposter）分别输入验证或身份丢失函数，以进行验证或身份识别培训。在一个测试案例中，只有解缠模块的编码器被用来分离每个输入GEI的身份和协变量特征。将两个被试的身份特征之间的欧氏距离计算为差异得分。最后，我们通过比较验证场景（一对一匹配）的接受阈值来判断受试者是否相同或不同，或者在识别场景的画廊中找到最小的差异分数。（一对多匹配）
图2：
（a） 解缠模块使用编码器从输入GEI中分离潜在身份和协变量特征，解码器执行两次重构；一种是原始输入GEI的自重构（用实线表示），另一种是无协变量（用虚线表示）的输入GEI的重构。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/1b19ec666325e9dd224105f48357332c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-09-07T21:51:12+08:00" />
<meta property="article:modified_time" content="2020-09-07T21:51:12+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">CVPR2020 步态论文：Gait Recognition via Semi-supervised Disentangled Representation Learning to Identit</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>论文题目：Gait Recognition via Semi-supervised Disentangled Representation Learning to Identity and Covariate Features<br> 通过半监督解耦表示 学习身份和协变特征的步态识别<br> <strong>摘要</strong><br> 现有的步态识别方法通常侧重于学习与协变量（如携带状态、服装、步行速度和视角）不变性的身份特征，很少涉及从协变量方面学习特征，当协变量引起的变化压倒同一性的变化时，可能导致失效。因此，我们提出了一种结合身份和协变量特征的解耦表示学习的步态识别方法。具体地说，我们首先对输入的步态模板进行编码，得到解耦的身份特征和协变量特征，然后对特征进行解码，以半监督的方式同时重建输入的步态模板和同一对象的无协变量的标准版本，以确保解耦成功。最后，我们将分离出的身份特征输入到对比/三重态丢失函数中，用于验证/识别任务。此外，我们发现，通过将协变量特征从一个对象转移到另一个对象，可以合成新的步态模板。在三个可公开获取的步态数据集上的实验结果证明了该方法的有效性。</p> 
<ol><li>介绍<br> 步态是一种重要的生物特征，用于远距离的人类身份识别，因为与其他生物特征(如面部、指纹和虹膜)相比，它可以在不需要主体合作的情况下在远距离被记录。此外，步态是一种无意识的特征，一般不会被人伪装。基于步态的识别因此有许多潜在的应用，如监视系统、取证和刑事调查[6,20,31]。<br> 以前的步态识别研究可大致分为以下几类：<br> 根据提取的特征分类为基于模型的方法[47、53、27、5、10、54、2]和基于外观的方法[15、32、44、24、48、4、37、29、59]。基于外观的步态识别方法由于其有效性和高效性，在步态识别领域得到了广泛的应用。但由于携带身份、着装、姿势变化、视角等协变量较多，因此存在较大的主体内差异。<br> 基于外观的步态识别需要提取不受协变量影响的身份特征。这种不变方法分为两大类:判别法[15,52,30,12,13,33,41,50,51,43,58,7,26]和生成法[23,32,34,38,35,1,11,55,56,16]。前者的目的是直接从原始的步态表示中提取不变的恒等特征子空间，而后者的目的是将不同协变条件下的步态表示生成相同协变条件下的步态表示。但是，它们都侧重于学习协变不变的特征子空间或图像空间，很少涉及协变方面的特征学习，当协变带来的变化超过了恒等带来的变化时，可能会导致失效模式。<br> 为了解决上述问题，我们提出了一种基于外观的步态识别采用解耦表征学习（DRL）同时考虑身份和协变量特征。这个想法的灵感来自于先前的工作[60]（CVPR2019的文章，可从博客中寻找），其中姿势和外观特征从RGB图像中分离出来，并且基于LSTM的姿势特征随时间的整合被用于步态识别。尽管服装的颜色和质地等外观特征的影响被成功地消除了，但我们认为RGB图像的使用仍然存在不足。首先，张等[60]假设两个条件来分离姿势和外观特征，一个是在一个序列中外观特征是一致的，另一个是每个训练对象至少包含两个外观特征完全不同的序列。然而，这些条件可能并不总是满足的，从而导致外观因素污染姿势特征。一个例子，如果在一个序列中光照条件突然改变（包括通过肢体运动使体表法线相对于入射光方向改变的情况），则第一个条件可能不满足；第二，如果训练对象只是部分地更换衣服或换成颜色不同但质地相似的衣服。文献[60]将RGB输入的颜色和纹理信息视为一种协变量，可以像许多步态识别工作一样，简单地使用基于轮廓的表示来处理。</li></ol> 
<p>因此，我们将[60]的分离思想扩展到直接从基于轮廓的步态表示中分离身份和协变量特征。我们将协变量分为两类，它们对步态表征有不同的影响，可能需要不同的分离策略。具体地说，第一类包括在身体上改变受试者身体形状的携带状态和衣服，并且该类具有一类明确的规范条件，即没有协变量的步态模板。作为一个例子，我们将没有携带对象（COs）和足够紧的衣服（例如，穿着紧身衣的对象）的步态模板作为携带状态和衣服的标准条件。第二类包括视角，它引入了所有主体之间的共同变化，而这类视角并没有一个明确和合适的标准条件来适用于所有的视角。本文着重于第一类。同样对于步态表示，我们选择步态能量图像（GEI）[15]，这是步态识别领域中使用最广泛的步态表示。</p> 
<p>具体地说，我们首先使用编码器将输入的GEI分解成低维身份和协变量特征。然后我们使用一个解码器执行两个重建；一种是从分离的同一性和协变量特征中重建输入GEI，另一种是从分离的同一性和零填充协变量特征重建另一个与输入GEI相同但没有协变量（规范条件）的GEI，其中我们给出了基本真实GEI没有协变量。通过这种设计，我们可以成功地将输入GEI分解成同一性和协变量特征。最后，我们将一对或三元组的身份特征输入到验证/识别任务的对比/三元组损失中。<br> 为此：<br> （1）我们使用基于轮廓的步态表示来避免不必要的颜色和纹理协变量；<br> （2）我们探索DRL，以分离身份和更常见但更困难的协变量，如携带状态和衣服；（3）我们通过以半监督的方式同时重建输入GEI及其标准版本来克服污染问题（即，我们给没有协变量的GEI（例如，“无协变量”）而不是在训练阶段给其他有协变量的GEI添加协变量标签）。</p> 
<p>此外，我们发现，给定来自一个被摄体1的解耦身份特征和来自另一个被摄体2的解耦协变量特征，解码器可以重构具有从1表征的相同身份和从2表征的相同协变量的新GEI样本，如图1所示。因此，我们可以自由地将协变量特征从一个主题转移到另一个主题，从而生成新的GEI，我们称之为GEI编辑。<br> <img src="https://images2.imgbox.com/34/97/s8I5v5Lk_o.png" alt="在这里插入图片描述"><br> 图1：在给定两名受试者(S1和S2)不同携带状态的步态模板后，我们的方法隐式地分离他们的身份和协变量特征(fid和fcov)，并仅使用身份特征重建不携带任何身份的步态模板。此外，我们还可以交换研究对象的协变量特征，并根据各自的携带状态生成新的步态模板。</p> 
<p>我们将我们的贡献总结如下。<br> 1） 一种用于步态识别的基于身份和协变量特征的分离网络。ICDNet<br> 我们首次引入半监督DRL来分离身份和协变量特征，用于步态识别。分离后的身份特征具有纯粹性和区分性，可用于步态识别。<br> 2） GEI编辑：从一个主题到另一个主题的协变量转移。<br> 我们可以通过将分离的协变量特征从一个主体转移到另一个主体来生成新的GEIＧＥＩ。这对今后步态识别中数据增强的研究具有一定的参考价值。<br> 3） 最先进的表现。<br> 我们在三个公开可用的步态数据库上取得了最先进的性能：具有真实COs的OU-ISIR大人口步态数据库（OU LP Bag）[46]、OU-ISIR步态数据库、带有Bag版本的大群体数据集（OU LP Bag）[33]和CASIA-B步态数据库[57]。</p> 
<p>2.相关工作</p> 
<p>基于外观的步态识别方法主要分为判别法和生成法。第一类旨在利用传统的度量学习技术(如线性判别分析[15]、张量表示的判别分析[52]、随机子空间方法[13]、联合强度和空间度量学习[33])或当前的深度神经网络提取一种针对协变量的判别子空间。特别是，基于深度学习的方法更受欢迎，因为它们具有更高的性能。例如Shiraga等人[41]提出了一种用于对具有交叉熵损失的单输入GEIs进行分类的轻型卷积神经网络(light convolutional neural network, CNN)。此后，一些研究[51,43,26]对具有对比或三元损失的输入GEIs对或三联体进行了相似性学习。Wolf et al.[50]和Chao et al.[7]直接为剪影帧设计了cnn，而不是将GEIs作为网络输入。<br> 第二类旨在使用子空间分析技术[23、32、34、38、35、1]或生成对抗网络（GAN）[55、56、16]，将不同协变量条件下的步态表示生成为相同协变量条件下的步态表示。 例如，Makihara等。 [32]提出了一种视图转换模型，将步态特征从画廊视图条件转换为探针视图条件。 Yu等。 [55，56]提出了基于GAN的生成网络，称为GaitGAN和GaitGANv2，它们可以从任何带有协变量的输入GEI生成正常状态下侧视图的不变GEI。<br> 然而，上述方法集中于学习不变地用于协变量的身份特征子空间或图像空间，并且很少考虑从协变量方面学习特征，当变化时可能导致失败。由于协变量而不是由于同一性。 相比之下，我们的方法同时考虑了身份和协变量特征学习，并获得了明显的身份认同特征与协变量特征的消融。<br> <strong>解耦表示学习</strong></p> 
<p>DRL被期望通过将数据的底层结构分离成不相交的有意义的变量来提供收益，这有助于澄清深层模型并确定实际学习到的隐藏特征类型。Zhang等人。[60]首次将DRL引入步态识别领域，从RGB图像中分离出受试者的姿势和外观特征。虽然DRL在步态研究中是一个新的领域，但在其他生物特征识别（如人脸识别）的研究中已经得到了很好的探索。例如，Tran等人。[45]彭等。从人脸识别中分离出40个姿势不变的人脸图像。[61]通过年龄变化的分解，生成年龄不变的人脸特征。</p> 
<p>与文献[60]相比，我们的方法避免了RGB信息分离的困难，并为基于轮廓的步态表示提供了新的有意义的分离变量（即身份特征和协变量特征）。与人脸识别[45，40，61]中需要附加协变量标签（例如，姿势或年龄标签）的DRL相比，我们的方法针对的是没有明确标签（除了“无协变量”标签），因为我们在论文中针对的协变量（即携带状态和衣服）没有清晰的标签。例如，即使是同一个手提行李，其携带状态（例如形状和位置）也很大程度上取决于物品。在我们的半监督DRL中，只需要部分训练对象的典型条件（即“无协变量”）的标签。<br> <strong>3.提出的方法</strong><br> 3.1概述<br> 我们提出了一种适用于步态识别的方法DRL从GEI中分离身份和协变量特征。在我们的问题设置中，我们假设每个训练对象都有一个没有协变量的步态模板（例如，一个没有COs的GEI），而我们没有其他步态模板的协变量条件标签（例如，一个受试者可能携带背包、公文包、手提箱甚至什么都没有，但我们从未事先知道，它还提供了一个测试用例）。因此，我们尽量使部分标签“无协变量”更好地分离。[1]</p> 
<p>图2示出了所提议的概述。基本解缠模块由一个编码流和两个解码流组成，处理训练集中的所有gei。然后将一对（probe和gallery）或三元组（query、true和imposter）分别输入验证或身份丢失函数，以进行验证或身份识别培训。在一个测试案例中，只有解缠模块的编码器被用来分离每个输入GEI的身份和协变量特征。将两个被试的身份特征之间的欧氏距离计算为差异得分。最后，我们通过比较验证场景（一对一匹配）的接受阈值来判断受试者是否相同或不同，或者在识别场景的画廊中找到最小的差异分数。（一对多匹配）<br> <img src="https://images2.imgbox.com/b0/6d/R57j49pl_o.png" alt="在这里插入图片描述"><br> 图2：<br> （a） 解缠模块使用编码器从输入GEI中分离潜在身份和协变量特征，解码器执行两次重构；一种是原始输入GEI的自重构（用实线表示），另一种是无协变量（用虚线表示）的输入GEI的重构。<br> （b） 验证场景：一对（探针和图库）首先通过解缠模块（a）的编码器，然后将解缠后的身份特征用于验证丢失。<br> （c） 识别场景：一个三元组（查询、正版和冒名顶替者）首先通过解纠缠模块（a）的编码器，然后使用分离后的身份特征进行识别丢失。</p> 
<p><strong>3.2解耦模块</strong><br> 解耦模块是ICDNet的重要组成部分。如图2（a）所示，模块具有编码器E和解码器D。编码器E有一个编码流，接收输入的GEI X并输出潜在身份特征fid和协变量特征f cov，可以表示为：<br> <img src="https://images2.imgbox.com/44/bf/BVsPLuiI_o.png" alt="在这里插入图片描述"><br> 同时，解码器D有两个解码流。一个接收到连接的身份和协变特征[fid,fconv]，与此重建输入GEI X本身。另一个接收到连接的身份和零填充的协变量特征[fid,f0]，用它重构一个GEI X0，与输入GEI中相同的训练对象，也不需要协变量。直观地说，通过协变量特征向量的零填充，我们可以敲除协变量特征或使其无效，以确保在没有协变量的情况下重构GEI X X0时，协变量因子不会污染身份因子也就是说：一种纯化的只包含识别因子的GEI，重构的两个GEIs可以表示为<br> <img src="https://images2.imgbox.com/da/50/OFqCVpB5_o.png" alt="在这里插入图片描述"><br> 其中，f0= 0是与fcov相同尺寸的零填充特征。注意输入GEI X是否包含协变量并不重要;也就是说，如果输入GEI不包含协变量，那么对于两个输出，网络只尝试输出不带协变量的GEI。重构的两个GEIs X和x0应该与其对应的ground truth GEIs X(输入GEI)和x0 (GEI与X相同但不含协变量)相似。为此，我们将重构损失定义为<br> <img src="https://images2.imgbox.com/f0/72/BMHqZvPA_o.png" alt="在这里插入图片描述"><br> 通过最小化Lreconst，我们确保被解耦的fid 和 fcov分别包含了输入GEI的身份和协变量信息，预定义的零填充f0表示不存在协变量。在这种半监督的方法中，我们保证了该方法的解耦性。</p> 
<p>3.3. 基于身份特征的步态识别<br> 我们使用解耦的身份特征进行步态识别。生物识别任务一般有两种类型:验证和识别。验证任务(即一对一匹配)的目的是判断给定的一对探针和图库是否来自同一主体。识别任务（即一对多匹配）的目标是在给定探测（即查询）的情况下，从多个注册的库中找到正确的匹配项。先前的研究[43]详细讨论了适用于两种不同生物识别任务的网络架构和损失函数。因此，我们为这两个任务设计了两种不同的网络和损耗函数，如下所示。<br> <strong>验证任务。</strong>(见图2 (b))我们首先从一对GEIs (P, G)及其对应的二值标记y(其中1和0分别表示这对GEIs来自同一对象和不同对象)中准备解纠缠的身份特征;然后我们把这些特征输入到一个对比损失函数[14]中。<br> 假设有N对身份特征：<img src="https://images2.imgbox.com/78/43/Q0nqdUYL_o.png" alt="在这里插入图片描述"><br> 以及相应的标签<img src="https://images2.imgbox.com/8d/cc/7E4f4kZe_o.png" alt="在这里插入图片描述"><br> 我们定义一个对比损失函数为：<br> <img src="https://images2.imgbox.com/a8/17/XlR28qLe_o.png" alt="在这里插入图片描述"><br> 其中<img src="https://images2.imgbox.com/4b/1b/Uyk7csuk_o.png" alt="在这里插入图片描述"><br> 是给定对(Pi和Gi)差异得分并且m是间距。我们通过最小化Lcont，使来自同一主体对的P和G的身份特征靠近，来自不同主体对的P和G的身份特征远离，这种方法更适合于验证任务而不是识别任务。<br> <strong>识别任务</strong><br> (见图2 ©)我们首先准备从GEIs (Q, G, I)的三元中解耦身份特征，其中Q和G来自同一个主体，Q和I来自两个不同的主体。然后，我们将这些特征输入到一个三元损失函数[49]中。<br> 假设有N个三联的身份特征<img src="https://images2.imgbox.com/a0/07/YoVliDGn_o.png" alt="在这里插入图片描述"><br> 我们将三重态损失函数定义为：<br> <img src="https://images2.imgbox.com/ed/e8/PRs8Ict1_o.png" alt="在这里插入图片描述"><br> 式中，<img src="https://images2.imgbox.com/f4/89/fiLibvYI_o.png" alt="在这里插入图片描述"></p> 
<p>为同一受试者对(Qi和Gi)的差异得分，di -为不同受试者对(Qi和 Ii)的差异得分，m为边界。我们通过最小化Ltrip使Q和G的身份特征比相同Q和其他I的更接近，这更适合于身份识别任务而不是验证任务。<br> Ltrip仅仅限制了同一主体和不同主体对之间的相对距离，而不是强迫同一主语对之间的距离绝对接近。考虑到来自同一主体的被解耦的身份特征应该相似的解耦特性，考虑到来自同一主体的被解耦的身份特征应该相似的解耦特性，我们定义另一个损失函数，称为恒等相似度损失Lsimto，使同一主体对(Q和G)的恒等特征相互接近:<br> <img src="https://images2.imgbox.com/22/44/CfJ4uZiG_o.png" alt="在这里插入图片描述"><br> <strong>三元组和对的抽样</strong><br> 我们采用批量全取样[17]来处理对比损失和三重损失。对于每一批，我们首先随机选择P个受试者，每个受试者随机选择K个样本。因此一批样品中总共有pk个样品。然后我们选择这批成对和三元组的所有组合，得到P K(P K−1)对和P K(P K−K)(K−1)三胞胎。考虑到同一受试者对数量严重不平衡。我们修改Eq.(4)以便将相同和不同的被试对的损失归一化。<br> <img src="https://images2.imgbox.com/13/59/X7DvK8JM_o.png" alt="在这里插入图片描述"><br> <strong>3.4. Joint loss functions联合损失函数</strong></p> 
<p>从解耦和识别两个方面考虑，我们用上述损失函数的加权求和来定义联合损失函数，并对其进行端到端的训练。具体来说，对于验证任务，将联合损失函数定义为<img src="https://images2.imgbox.com/ae/c5/IbMm63W9_o.png" alt="在这里插入图片描述"><br> 其中，有两个超参数。<br> 对于识别任务，将联合损失函数定义为：<br> <img src="https://images2.imgbox.com/75/64/98hRLrAZ_o.png" alt="在这里插入图片描述"><br> 其中，<br> <img src="https://images2.imgbox.com/c8/40/tPOQTbmX_o.png" alt="在这里插入图片描述"><br> 为三个超参数:最后通过最小化联合损失函数L(E, D)来优化E和D的参数。<br> <strong>4. Experiment实验</strong><br> <em>4.1. Data sets</em><br> 我们在三个公开的步态数据库上评估了该方法:OU-LP-Bag[46]、OU-LP-Bag[33]和CASIA-B[57]。<br> OU-LP-Bag拥有世界范围内最多的步态数据库(62,528名受试者)，并包含了真实COs的协变量。训练集与[46]协议相同，训练集包含29,097个被试，分别包含有COs和无COs两个序列，测试集包含另外29,102个不相交的被试。在合作设置和非合作设置下，测试集中准备了两种不同版本的探针和图库。对于协作设置，画廊集只包含不含COs的序列，而探测集包含带有携带位置注释的7种COs的序列;对于不合作的设置，库集和探测集随机交换序列，都有带COs和不带COs的序列。<br> OU-LP-Bag对2070名患有和不患有COs的受试者进行了4140个序列分析。训练集包含1034个subject，测试集包含其他1036个不相交的subject。图库集包含没有COs的序列，而探测集包含有COs的序列。<br> CASIA-B包含来自11个视图的124个主题。每个主题和视图有10个序列。其中6例为正常行走(NM)， 2例为背着包(BG)，其余2例为穿外套(CL)。在[51]之后，选取NM下的前四个序列作为gallery (NM # 1-4)。另外六个序列作为三个探针集:(1)Set-NM包含两个NM序列(NM # 5-6)， (2) Set-BG包含两个BG序列(BG #1 - 2)， (3) Set-CL包含两个CL序列(CL #1 - 2)。<br> <em>4.2. Implementation details</em><br> <strong>Network architectures.</strong><br> 所述编码器和解码器的详细结构如图3所示。该编码器输入GEI大小为1×64×64，输出潜同一性和协变量特征，实验将其分别设置为96维向量和32维向量。该编码器的主干是基于GoogLeNet[42]中的Inception模块设计的，用于多尺度提取特征。译码器以潜在身份和协变特征作为输入和输出重构GEI，该GEI采用反卷积层设计。<br> <img src="https://images2.imgbox.com/76/ff/EsRNXNAL_o.png" alt="在这里插入图片描述"><br> <strong>Training strategies.</strong><br> 我们采用了两种培训策略:一个是从头训练模型为每个数据集,另一个是训练的模型最大的数据集(例如,OU-LP-Bag),然后微调pre-trained模型在另外两个更小的数据集,利用更好的泛化能力的最大数据集。第一战略,我们应用一个额外的数据增加(例如,垂直轴和水平轴的步长均为2，从-5平移到5个像素)，考虑到OU-LP-Bag和CASIAB的样本相对较少。对于第二种策略，我们只使用原始数据集。<br> <strong>参数设置。</strong><br> 我们使用Adam优化器[21]以端到端的方式训练所提出的ICDNet。对于从头开始的策略，初始学习率设置为0.0002，动量项(mach1, mach2)设置为(0.5,0.999)。在100,000次迭代之后，我们将学习率降低到0.00002，并继续运行50,000次迭代。对于微调策略，使用OU-LP-Bag上150,000次迭代的模型进行初始化。我们将初始学习率设置为0.00002，并且只运行10,000次迭代。<br> 对于OU-LP-Bag、OU-LP-Bag、CASIA-B批次所有采样参数(P, K)分别设为(300,2)、(100,2)和(8,16)。eq .(5)(7)中的margin m被设为3。<br> <img src="https://images2.imgbox.com/7b/46/MYWwALR2_o.png" alt="在这里插入图片描述"><br> <strong>4.3 评价指标</strong><br> 根据生物识别性能标准[19]，对于验证任务，有假匹配率(FMR)和假非匹配率(FNMR)的等错误率(EER)和检测错误权衡(DET)曲线，该曲线描述了接受阈值变化时FNMR和FMR之间的权衡。<br> 对于识别任务，我们报告了排名1的识别率(用排名1表示)和描述每个等级内的识别率的累积匹配特征(CMC)曲线。<br> <strong>4.4重建GEIs的可视化</strong><br> 我们通过可视化的OU-LP-Bag上的重建结果来定性地评价所提出的方法。我们首先在图4中展示了几个自重构的例子。<br> <img src="https://images2.imgbox.com/a9/cb/GeS6FMrQ_o.png" alt="在这里插入图片描述"></p> 
<p>很明显，无论输入GEIs是否有COs，我们都可以通过编码特征[fid,fcov]成功地重构出输入GEIs(图4 (b)和(e))。我们还成功重构了相同受试者的GEIs(不含COs)(图4 ©和(f)，与输入GEIs相似在不考虑COs的情况下(图4 (d))，图4 ©和(f)的重构结果相似，这说明同一受试者无论输入GEIs是否具有COs，其得到的身份特征都是相似的。此外,我们发现不仅因为本身,而且一些姿势的变化(例如,手,身体弯曲)诱导的携带状态可以被视为协变量的特性,这是明显的事实,他们也消除了在重建没有COs的GEIs(见图4 ©)。<br> 接下来我们结合来自两个不同主体的恒等和协变特征，看看是否从一个主体的协变特征可以转移到另一个主体。结果如图5所示。按照我们的预期，重构的GEI样本每一行具有相似的身份特征，每一列具有相似的协变量特征。此外，我们进一步证实转移的协变量特征包含许多协变量，包括COs、姿势、服装。通过评价，我们验证了所提出的方法能够解耦身份和协变量特征。<br> <strong>4.5. Comparison with state-of-the-art approaches</strong><br> *OU-LP-Bag。*我们评估了在原始数据库研究[46]和目前最先进的方法[26]中报告的基准，以及通过遵循[46]的原始实验协议提出的方法。所有结果如图6和表1所示。对于每一个合作/非合作设置和识别任务，我们都匹配了最先进的性能，并以很大的优势超过了第二最佳基准[26](例如，EER比它低0.3%以上，而排名1的比率比它高12%。表明了该方法在识别性能方面的优势。<br> *OU-LP-Bag β.*现有方法采用两个不同的数据集训练策略。正如4.2节中提到的,一个是从头训练[33，58]采用,而另一个是微调pre-trained模型在更大的数据集(例如,OU-LP-Bag),首次采用[26]在考虑相对较少的主题OULP-Bagβ。为了进行公平的比较，我们相应地将我们的方法与使用每种策略的其他基准进行比较。所有结果如图7和表2所示。结果表明，我们的方法对两种策略都有较好的效果。<br> <em>CASIA-B</em>由于目前的方法并不针对视角的协变量，所以我们将CASIA-B的实验集中在侧视图(或近侧视图)上。我们在这些实验中使用两种协议。协议1取自[8]，其中以前24名受试者作为训练集，其余100名受试者作为测试集。本协议只使用侧视图。<br> 协议1取自[8]，其中以前24名受试者作为训练集，其余100名受试者作为测试集，本协议只使用侧视图。方案2取自[18,51,8,58,60]，同时考虑步行条件(BG或CL)和视角变化。前34名受试者被用作训练集，其余90名受试者作为测试集。在我们的案例中，我们限制观看角度的变化，以近侧面的视角((probe vs. gallery: 90◦vs. 72◦and 90◦vs. 108◦).)。<br> 表3和表4分别给出了协议1和协议2的结果。只有排名1的比率被报告，因为EERs报告几乎没有任何基准。结果表明，在列车从头开始的策略下，该方法的性能较差。我们认为，这可能是因为CASIA-B与其他两个数据集相比，协议1和协议2的训练集中的受试者数量有限，导致我们的方法缺乏泛化能力。<br> 然而，一旦我们应用了微调策略，我们就可以获得最佳性能。<br> <strong>4.6. Ablation study of loss functions</strong><br> 我们分析了每个损失函数如何影响我们的方法在OU-LP-Bag上的性能。在保持识别损失(Lcontor Ltrip)的同时，增加或消除解耦损失(Lreconstand Lsim)以评估各自的性能。表5显示了添加解纠缠损失可以提高性能，这表明了我们的解耦方法的有效性<br> <strong>5. Conclusion</strong><br> 我们提出了一种名为ICDNet的步态识别方法，该方法利用半监督的DRL来解耦身份和协变量特征。我们设计了一个自动编码器，将输入GEI编码为身份和协变特征，并使用协变上的部分标签重新构造输入GEI和同一受试者的输入GEI，而不需要协变。我们提出了定性和定量的评估，以显示成功的身份和协变量特征的解耦和性能的改善与解开。我们也证实了所提出的方法使交叉重建成为可能，这显示了潜在的步态数据增加在未来的工作。此外，由于我们目前排除了观察角度，设计一个更全面的处理所有协变量的网络将是另一项未来的工作。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/bf51daac4256ee60ce5a57bd2a02ef7e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Java43——泛型</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c44fc7257bf202624eb6973e0d5ea847/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">(error) WRONGTYPE Operation against a key holding the wrong kind of value</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>