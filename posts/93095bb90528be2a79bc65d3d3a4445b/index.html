<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>pytorch训练时指定显卡 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="pytorch训练时指定显卡" />
<meta property="og:description" content="1. 利用CUDA_VISIBLE_DEVICES设置可用显卡 在CUDA中设定可用显卡，一般有2种方式：
(1) 在代码中直接指定
import os os.environ[&#39;CUDA_VISIBLE_DEVICES&#39;] = gpu_ids (2) 在命令行中执行代码时指定
CUDA_VISIBLE_DEVICES=gpu_ids python3 train.py 如果使用sh脚本文件运行代码，则有3种方式可以设置
(3) 在命令行中执行脚本文件时指定：
CUDA_VISIBLE_DEVICES=gpu_ids sh run.sh (4) 在sh脚本中指定：
source bashrc export CUDA_VISIBLE_DEVICES=gpu_ids &amp;&amp; python3 train.py (5) 在sh脚本中指定
source bashrc CUDA_VISIBLE_DEVICES=gpu_ids python3 train.py 如果同时使用多个设定可用显卡的指令，比如
source bashrc export CUDA_VISIBLE_DEVICES=gpu_id1 &amp;&amp; CUDA_VISIBLE_DEVICES=gpu_id2 python3 train.py 那么高优先级的指令会覆盖第优先级的指令使其失效。
优先级顺序为：不使用sh脚本 (1)&gt;(2)； 使用sh脚本(1)&gt;(5)&gt;(4)&gt;(3)
个人感觉在炼丹时建议大家从(2)(3)(4)(5)中选择一个指定可用显卡，不要重复指定以防造成代码的混乱。方法(1)虽然优先级最高，但是需要修改源代码，所以不建议使用。
2 .cuda()方法和torch.cuda.set_device() 可以使用.cuda()[包括model.cuda()/loss.cuda()/tensor.cuda()]方法和torch.cuda.set_device()来把模型和数据加载到对应的gpu上。
(1) .cuda() 以model.cuda()为例，加载方法为：
model.cuda(gpu_id) # gpu_id为int类型变量，只能指定一张显卡 model.cuda(&#39;cuda:&#39;&#43;str(gpu_ids)) #输入参数为str类型，可指定多张显卡 model.cuda(&#39;cuda:1,2&#39;) #指定多张显卡的一个示例 (2) torch.cuda.set_device() 使用torch.cuda.set_device()可以更方便地将模型和数据加载到对应GPU上, 直接定义模型之前加入一行代码即可
torch.cuda.set_device(gpu_id) #单卡 torch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/93095bb90528be2a79bc65d3d3a4445b/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-04-28T20:32:29+08:00" />
<meta property="article:modified_time" content="2022-04-28T20:32:29+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">pytorch训练时指定显卡</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>1. 利用CUDA_VISIBLE_DEVICES设置可用显卡</h2> 
<p>在CUDA中设定可用显卡，一般有2种方式：</p> 
<p>(1) 在代码中直接指定</p> 
<pre><code class="language-python">import os 
os.environ['CUDA_VISIBLE_DEVICES'] = gpu_ids </code></pre> 
<p>(2) 在命令行中执行代码时指定</p> 
<pre><code class="language-python">CUDA_VISIBLE_DEVICES=gpu_ids python3 train.py</code></pre> 
<p>如果使用sh脚本文件运行代码，则有3种方式可以设置</p> 
<p>(3) 在命令行中执行脚本文件时指定：</p> 
<pre><code class="language-python">CUDA_VISIBLE_DEVICES=gpu_ids sh run.sh</code></pre> 
<p>(4) 在sh脚本中指定：</p> 
<pre><code class="language-python">source bashrc
export CUDA_VISIBLE_DEVICES=gpu_ids &amp;&amp; python3 train.py</code></pre> 
<p>(5) 在sh脚本中指定</p> 
<pre><code>source bashrc
CUDA_VISIBLE_DEVICES=gpu_ids python3 train.py</code></pre> 
<p>如果同时使用多个设定可用显卡的指令，比如</p> 
<pre><code class="language-python">source bashrc
export CUDA_VISIBLE_DEVICES=gpu_id1 &amp;&amp; CUDA_VISIBLE_DEVICES=gpu_id2 python3 train.py</code></pre> 
<p>那么高优先级的指令会覆盖第优先级的指令使其失效。</p> 
<p>优先级顺序为：不使用sh脚本 (1)&gt;(2)； 使用sh脚本(1)&gt;(5)&gt;(4)&gt;(3)</p> 
<p>个人感觉在炼丹时建议大家从(2)(3)(4)(5)中选择一个指定可用显卡，不要重复指定以防造成代码的混乱。方法(1)虽然优先级最高，但是需要修改源代码，所以不建议使用。</p> 
<p> </p> 
<h2>2 .cuda()方法和torch.cuda.set_device()</h2> 
<p>可以使用.cuda()[包括model.cuda()/loss.cuda()/tensor.cuda()]方法和torch.cuda.set_device()来把模型和数据加载到对应的gpu上。</p> 
<h3>(1) .cuda()</h3> 
<p>以model.cuda()为例，加载方法为：</p> 
<pre><code>model.cuda(gpu_id) # gpu_id为int类型变量，只能指定一张显卡
model.cuda('cuda:'+str(gpu_ids)) #输入参数为str类型，可指定多张显卡
model.cuda('cuda:1,2') #指定多张显卡的一个示例</code></pre> 
<h3>(2) torch.cuda.set_device()</h3> 
<p>使用torch.cuda.set_device()可以更方便地将模型和数据加载到对应GPU上, 直接定义模型之前加入一行代码即可</p> 
<pre><code class="language-python">torch.cuda.set_device(gpu_id) #单卡
torch.cuda.set_device('cuda:'+str(gpu_ids)) #可指定多卡</code></pre> 
<p>但是这种写法的优先级低，如果model.cuda()中指定了参数，那么torch.cuda.set_device()会失效，而且<strong>pytorch的官方文档中明确说明，不建议用户使用该方法。</strong></p> 
<p>第1节和第2节所说的方法同时使用是并不会冲突，而是会叠加。比如在运行代码时使用</p> 
<pre><code class="language-python">CUDA_VISIBLE_DEVICES=2,3,4,5 python3 train.py</code></pre> 
<p>而在代码内部又指定</p> 
<pre><code>model.cuda(1)
loss.cuda(1)
tensor.cuda(1)</code></pre> 
<p>那么代码会在GPU3上运行。原理是CUDA_VISIBLE_DEVICES使得只有GPU2,3,4,5可见，那么这4张显卡，程序就会把它们看成GPU0,1,2,3，.cuda(1)把模型/loss/数据都加载到了程序所以为的GPU1上，则实际使用的显卡是GPU3。</p> 
<p>如果利用.cuda()或torch.cuda.set_device()把模型加载到多个显卡上，而实际上只使用一张显卡运行程序的话，那么程序会把模型加载到第一个显卡上，比如如果在代码中指定了</p> 
<pre><code>model.cuda('cuda:2,1')</code></pre> 
<p>在运行代码时使用</p> 
<pre><code>CUDA_VISIBLE_DEVICES=2,3,4,5 python3 train.py</code></pre> 
<p>这一指令，那么程序最终会在GPU4上运行。</p> 
<p> </p> 
<h2>3.多卡数据并行torch.nn.DataParallel</h2> 
<p>多卡数据并行一般使用</p> 
<pre><code>torch.nn.DataParallel(model,device_ids)</code></pre> 
<p>其中model是需要运行的模型，device_ids指定部署模型的显卡，数据类型是list</p> 
<p>device_ids中的第一个GPU（即device_ids[0]）和model.cuda()或torch.cuda.set_device()中的第一个GPU序号应保持一致，否则会报错。此外如果两者的第一个GPU序号都不是0,比如设置为：</p> 
<pre><code class="language-python">model=torch.nn.DataParallel(model,device_ids=[2,3])
model.cuda(2)</code></pre> 
<p>那么程序可以在GPU2和GPU3上正常运行，但是还会占用GPU0的一部分显存（大约500M左右），这是由于pytorch本身的bug导致的（截止1.4.0，没有修复这个bug）。</p> 
<p>device_ids的默认值是使用可见的GPU，不设置model.cuda()或torch.cuda.set_device()等效于设置了model.cuda(0)</p> 
<p></p> 
<h3>4. 多卡多线程并行torch.nn.parallel.DistributedDataParallel</h3> 
<p>（这个我是真的没有搞懂,,,,）</p> 
<p>参考了<a href="https://zhuanlan.zhihu.com/p/98535650" rel="nofollow" title="这篇文章">这篇文章</a>和<a href="https://link.zhihu.com/?target=https%3A//github.com/tczhangzhi/pytorch-distributed/blob/master/distributed.py%23L98" rel="nofollow" title="这个代码">这个代码</a>，关于GPU的指定，多卡多线程中有2个地方需要设置</p> 
<pre><code class="language-python">torch.cuda.set_device(args.local_rank)
torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank])</code></pre> 
<p>模型/loss/tensor设置为.cuda()或.cuda(args.local_rank)均可，不影响正常运行。</p> 
<p> </p> 
<h3>5. 推荐设置方式：</h3> 
<h3>(1) 单卡</h3> 
<p>使用CUDA_VISIBLE_DEVICES指定GPU，不要使用torch.cuda.set_device()，不要给.cuda()赋值。</p> 
<h3>(2) 多卡数据并行</h3> 
<p>直接指定CUDA_VISIBLE_DEVICES，通过调整可见显卡的顺序指定加载模型对应的GPU,不要使用torch.cuda.set_device()，不要给.cuda()赋值，不要给torch.nn.DataParallel中的device_ids赋值。比如想在GPU1,2,3中运行，其中GPU2是存放模型的显卡，那么直接设置</p> 
<pre><code>CUDA_VISIBLE_DEVICES=2,1,3</code></pre> 
<h3>(3) 多卡多线程</h3> 
<p>推荐参考<a href="https://zhuanlan.zhihu.com/p/98535650" rel="nofollow" title="这篇文章">这篇文章</a>和<a href="https://link.zhihu.com/?target=https%3A//github.com/tczhangzhi/pytorch-distributed/blob/master/distributed.py%23L98" rel="nofollow" title="这个代码">这个代码</a></p> 
<p> </p> 
<h2>参考</h2> 
<p><a href="https://zhuanlan.zhihu.com/p/98535650" rel="nofollow" title="当代研究生应当掌握的并行训练方法（单机多卡） - 知乎 (zhihu.com)">当代研究生应当掌握的并行训练方法（单机多卡） - 知乎 (zhihu.com)</a> </p> 
<p><a href="https://github.com/tczhangzhi/pytorch-distributed/blob/master/distributed.py#L98" title="pytorch-distributed/distributed.py at master · tczhangzhi/pytorch-distributed · GitHub">pytorch-distributed/distributed.py at master · tczhangzhi/pytorch-distributed · GitHub</a> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/bfed60aa4daf1dddce82b66274f999dc/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">交通流预测爬坑记（三）：使用pytorch实现LSTM预测交通流</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/cad3d0edd27f7bf969a120e9c273507e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">C语言笔记（建议背下来）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>