<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>车牌识别部署瑞芯微RK3588 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="车牌识别部署瑞芯微RK3588" />
<meta property="og:description" content="因为国内关于rockchip方面的文档不是很多,所以在此记录一下车牌识别模型转化以及部署的过程:
数据集:
cpdd,github上寻找到数据集
​​​​​​​​​​​​​​链接: https://pan.baidu.com/s/1SeqcoBBDqOYVBvpOTZy7tQ 提取码: 46wk 模型:
yolov5,crnn(lprnet_rknn模型量化失败)
目录
车牌检测
车牌识别
模型部署
​​​​​​​
车牌检测 yolov5模型不过多赘述,网络上面开源的很多,转化过程我分两步,pytorch转onnx,再onnx转rknn.
yolov5开源代码中有export.py文件用于输出onnx模型,
python3 export.py --weights weights/yolov5s.pt --opset 12
在onnx转rknn模型时可能会出现报错,需要修改opset参数(目前没有遇到这种情况).
rknn_toolkit2官方提供了python3.6和python3.8的package,如果pc没有这两个版本,就需要按转对应版本.
官方有提供onnx转rknn模型的代码(rknn-toolkit2-1.4.0/examples/onnx/yolov5/):
# Create RKNN object rknn = RKNN(verbose=True) # pre-process config print(&#39;--&gt; Config model&#39;) rknn.config(mean_values=[[0, 0, 0]], std_values=[[1, 1, 1]], target_platform=&#39;rk3588&#39;) print(&#39;done&#39;) # Load ONNX model print(&#39;--&gt; Loading model&#39;) # ret = rknn.load_onnx(model=ONNX_MODEL, outputs=[&#39;326&#39;, &#39;384&#39;, &#39;442&#39;]) ret = rknn.load_onnx(model=ONNX_MODEL, outputs=[&#39;343&#39;, &#39;360&#39;, &#39;377&#39;]) if ret !" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/57a1959c919016a2b03802d737d947fa/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-25T10:19:21+08:00" />
<meta property="article:modified_time" content="2023-06-25T10:19:21+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">车牌识别部署瑞芯微RK3588</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <blockquote> 
 <p>因为国内关于rockchip方面的文档不是很多,所以在此记录一下车牌识别模型转化以及部署的过程:</p> 
</blockquote> 
<p><strong>数据集:</strong></p> 
<p>cpdd,github上寻找到数据集</p> 
<p>​​​​​​​​​​​​​​链接: https://pan.baidu.com/s/1SeqcoBBDqOYVBvpOTZy7tQ 提取码: 46wk </p> 
<p><strong>模型:</strong></p> 
<p>yolov5,crnn(lprnet_rknn模型量化失败)</p> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="%E8%BD%A6%E7%89%8C%E6%A3%80%E6%B5%8B%3Ayolov5-toc" style="margin-left:40px;"><a href="#%E8%BD%A6%E7%89%8C%E6%A3%80%E6%B5%8B%3Ayolov5" rel="nofollow">车牌检测</a></p> 
<p id="%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB%3Acrnn(lprnet_rknn%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E5%A4%B1%E8%B4%A5)-toc" style="margin-left:40px;"><a href="#%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB%3Acrnn%28lprnet_rknn%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E5%A4%B1%E8%B4%A5%29" rel="nofollow">车牌识别</a></p> 
<p id="%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2-toc" style="margin-left:40px;"><a href="#%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2" rel="nofollow">模型部署</a></p> 
<hr id="hr-toc"> 
<p>​​​​​​​</p> 
<h3 id="%E8%BD%A6%E7%89%8C%E6%A3%80%E6%B5%8B%3Ayolov5">车牌检测</h3> 
<p>yolov5模型不过多赘述,网络上面开源的很多,转化过程我分两步,pytorch转onnx,再onnx转rknn.</p> 
<p>yolov5开源代码中有export.py文件用于输出onnx模型,</p> 
<blockquote> 
 <p>python3 export.py --weights weights/yolov5s.pt --opset 12</p> 
</blockquote> 
<p>在onnx转rknn模型时可能会出现报错,需要修改opset参数(目前没有遇到这种情况).</p> 
<p>rknn_toolkit2官方提供了python3.6和python3.8的package,如果pc没有这两个版本,就需要按转对应版本.</p> 
<p>官方有提供onnx转rknn模型的代码(rknn-toolkit2-1.4.0/examples/onnx/yolov5/):</p> 
<pre><code class="language-python"># Create RKNN object
    rknn = RKNN(verbose=True)

    # pre-process config
    print('--&gt; Config model')
    rknn.config(mean_values=[[0, 0, 0]], std_values=[[1, 1, 1]], target_platform='rk3588')
    print('done')

    # Load ONNX model
    print('--&gt; Loading model')
    # ret = rknn.load_onnx(model=ONNX_MODEL, outputs=['326', '384', '442'])
    ret = rknn.load_onnx(model=ONNX_MODEL, outputs=['343', '360', '377'])
    if ret != 0:
        print('Load model failed!')
        exit(ret)
    print('done')

    # Build model
    print('--&gt; Building model')
    ret = rknn.build(do_quantization=QUANTIZE_ON, dataset=DATASET)
    if ret != 0:
        print('Build model failed!')
        exit(ret)
    print('done')

    # Export RKNN model
    print('--&gt; Export rknn model')
    ret = rknn.export_rknn(RKNN_MODEL)
    if ret != 0:
        print('Export rknn model failed!')
        exit(ret)
    print('done')

    # Init runtime environment
    print('--&gt; Init runtime environment')
    ret = rknn.init_runtime()
    # ret = rknn.init_runtime(target='rk3588',device_id='85b9790400346ffd', async_mode=True)
    if ret != 0:
        print('Init runtime environment failed!')
        exit(ret)
    print('done')
    img = cv2.imread(IMG_PATH)
    img, ratio, (dw, dh) = letterbox(img, new_shape=(IMG_SIZE, IMG_SIZE))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</code></pre> 
<blockquote> 
 <p>rknn的接口定义在rknn-toolkit2-1.4.0/doc/Rockchip_User_Guide_RKNN_Toolkit2_CN-1.4.0.pdf文件中.其中主要需要修改的部分一般有rknn.config,rknn.load_onnx,init_runtime,rknn.inference.</p> 
</blockquote> 
<p>load_onnx函数中outputs需要设置;可以通过<a href="https://netron.app" rel="nofollow" title="Netron">Netron</a>来查看onnx模型的输出节点,主要找到这三个输出节点:<img alt="" height="394" src="https://images2.imgbox.com/28/b6/zHZexkzB_o.png" width="940"></p> 
<p>init_runtime函数用来模型结果查看:</p> 
<p><img alt="" height="88" src="https://images2.imgbox.com/d6/5c/mvEtcjCY_o.png" width="1200"></p> 
<p>其中device_id通过<u>adb devices</u>命令查看添加,target为目标平台.</p> 
<p>inference函数得到模型输出.</p> 
<hr> 
<h3 id="%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB%3Acrnn(lprnet_rknn%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E5%A4%B1%E8%B4%A5)">车牌识别</h3> 
<p>之前使用的lprnet来做车牌识别的工作,,用了50000张图片,训练30个epochs能够达到92%的准确率.但在转换模型时出现了问题,由于torch.onnx.export()对于maxpool3d不支持,需要将maxpool3d转化为3个maxpool2d来适应onnx转换.</p> 
<pre><code class="language-python">
class maxpool_3d(nn.Module):
    def __init__(self, kernel_size, stride):
        super(maxpool_3d, self).__init__()
        assert(len(kernel_size)==3 and len(stride)==3)
        kernel_size2d1 = kernel_size[-2:]
        stride2d1 = stride[-2:]
        kernel_size2d2 = (kernel_size[0],kernel_size[0])
        stride2d2 = (kernel_size[0], stride[0])
        self.maxpool1 = nn.MaxPool2d(kernel_size=kernel_size2d1, stride=stride2d1)
        self.maxpool2 = nn.MaxPool2d(kernel_size=kernel_size2d2, stride=stride2d2)

    def forward(self,x):
        x = self.maxpool1(x)
        x = x.transpose(1,3)
        x = self.maxpool2(x)
        x = x.transpose(1,3)
        return x
</code></pre> 
<p>使用后在torch.onnx.export()步骤不会出现报错了,但是我在使用onnxruntime进行结果复现时出现了问题,结果不对等,准确率掉到了80%,同时在转化为rknn模型后准确率掉到了75%.</p> 
<p>在进行了一系列数据读取的不同操作之后,选择了放弃,结果始终无法提升(具体原因由于个人能力问题,无法解释).</p> 
<p>后续选择了更换模型,换成了crnn网络,使用同样方式进行了模型转换,此次结果onnx模型和pytorch模型结果对比一致(想想还是没道理,觉得应该是网络结构有问题,但是pytorch测试时识别是没有问题的).</p> 
<p>模型转换与yolov5是一样的方式:</p> 
<pre><code class="language-python">
    # Create RKNN object
    rknn = RKNN(verbose=True)

    # pre-process config
    print('--&gt; Config model')
    # rknn.config(mean_values=[[127.5, 127.5, 127.5]], std_values=[[255, 255, 255]], target_platform='rk3588')
    rknn.config(mean_values=[[0, 0, 0]], std_values=[[1, 1, 1]], target_platform='rk3588')
    print('done')

    # Load ONNX model
    print('--&gt; Loading model')
    ret = rknn.load_onnx(model=ONNX_MODEL)
    if ret != 0:
        print('Load model failed!')
        exit(ret)
    print('done')

    # Build model
    print('--&gt; Building model')
    ret = rknn.build(do_quantization=False)
    if ret != 0:
        print('Build model failed!')
        exit(ret)
    print('done')

    # Export RKNN model
    print('--&gt; Export rknn model')
    ret = rknn.export_rknn(RKNN_MODEL)
    if ret != 0:
        print('Export rknn model failed!')
        exit(ret)
    print('done')

    # Init runtime environment
    print('--&gt; Init runtime environment')
    ret = rknn.init_runtime()
    # ret = rknn.init_runtime(target='rk3588', device_id='85b9790400346ffd', async_mode=True)
    if ret != 0:
        print('Init runtime environment failed!')
        exit(ret)
    print('done')
</code></pre> 
<p>不过在rknn.inference函数上需要进行修改,如果想要与pytorch一样的前处理方式,在进行transpose()转换维度时,</p> 
<pre><code class="language-python">
    img = cv2.resize(img, (168, 48))
    img = img.astype(np.float32)
    img = (img / 255 - mean_value) / std_value  # 归一化 减均值 除标准差
    img = img.transpose(2, 0, 1)  # h,w,c 转为 c,h,w
    img = img.reshape(1, *img.shape)  # channel,height,width转为batch,channel,height,channel</code></pre> 
<p>需要将data_format参数修改为nchw模式,但其实没有必要进行transpose操作(后续会说)</p> 
<pre><code class="language-python">outputs = rknn.inference(inputs=[img], data_format='nchw')[0]</code></pre> 
<p>后续进行结果的解析就与yolov5不太一样了,因为crnn是单输出形式,与yolov5的三输出形式不一样,所以只需要进行argmax操作就可以了,然后就配合classes类别输出结果就可以了.</p> 
<p>到此部分都是在pc端进行,都是对结果的模拟,并不能直接放置到rk板子上运行.</p> 
<hr> 
<h3 id="%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2">模型部署</h3> 
<p>官方github下有rknpu2项目代码,此部分是关于模型部署.</p> 
<p>不过我是基于rknn_toolkit2下rknn_toolkit_lite2完成的部署(因为本人C++技术堪称没有,只能依靠python方式啦),rknn_toolkit_lite2需要python3.7或python3.9部署.(进行这些操作之前需要在板端进行rknn_server和librknnrt.so的更新,具体可以看rknpu2/rknn_server_proxy.md,依照自己方式更新,我的是linux.)</p> 
<p>rknn_toolkit_lite2与pc端rknn_toolkit2没什么太大区别,模型的家在初始化基本上是一样的,就是将rknn更换成rknn_lite,函数也都大差不差,更细节的可以看api接口文档.</p> 
<pre><code class="language-python">rknn_lite = RKNNLite()

    print('--&gt;loading model : ' + RKNN_MODEL)
    ret = rknn_lite.load_rknn(RKNN_MODEL)
    # Init runtime environment
    print('--&gt; Init runtime environment')
    # ret = rknn.init_runtime()
    ret = rknn_lite.init_runtime(core_mask=RKNNLite.NPU_CORE_AUTO)
    if ret != 0:
        print('Init runtime environment failed!')
        exit(ret)
    print('done')</code></pre> 
<p>结果预测与rknn也差不多,区别是在rknn_toolkit_lite下inference中data_format参数只有'nhwc'一种数据排列方式,所以在数据前处理部分只需要将transpose这一步操作取消掉就可以了.</p> 
<pre><code class="language-python">    outputs = rknn.inference(inputs=[img])[0]
</code></pre> 
<p>后续处理就没什么区别了,只需要结合自己的业务进行代码修改就可以了.</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/7c9e7c442c38aa5f281b04d4d36c36ef/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">redis中opsForList().range()的使用</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/2418ccb7d6f6ef2ca1031de824e3e271/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">spring中事务失效的情况（常见的5种）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>