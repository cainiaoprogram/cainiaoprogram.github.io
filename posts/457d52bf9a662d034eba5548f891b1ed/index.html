<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【论文精读】Grounded Language-Image Pre-training（GLIP） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【论文精读】Grounded Language-Image Pre-training（GLIP）" />
<meta property="og:description" content="一. 背景 https://arxiv.org/abs/2112.03857 https://github.com/microsoft/GLIP 这篇论文做的任务是phrase grounding，属于visual grounding的一种。phrase grounding的任务是输入句子和图片，将句子中提到的物体都框出来。visual grounding其他任务和细节可以参考
https://zhuanlan.zhihu.com/p/388504127
GLIP既可以做目标检测也可以做grounding，
目标检测：
在扩增目标检测领域为SOTA，zero-shot效果较好，也可以做zero-shot目标检测任务。
与常规目标检测任务相比语义丰富。grounding：
与常规grounding任务相比可以做目标检测任务。 二、贡献 贡献
将目标检测和phrase grounding任务统一起来进行预训练扩大视觉语义迁移学习能力强 性能
27M关联数据上训练。在目标识别任务上有很强的零样本和小样本迁移性能Zero-shot：coco val上49.8AP，LVIS val上26.9AP微调后：COCO val上60.8AP下游13个目标检测任务时，1个样本的GLIP可以与Dynamic Head相匹敌 三、方法 3.1 方法1：检测和grounding任务统一 1. background: 对于检测数据集：
训练时输入标签名(person、hairdryer）、框、图片。
测试时输入图片，预测出框和标签名。
训练过程如下：
2. background as grounding：
groudning模型的输入是短语、短语中名词的框和图片。
将object模型转为grounding的办法：通过prompt的方式将标签名转化为短语。
如coco有80个标签，将80个标签用逗号连接，短语前加“Detect：”，来组成短句。
公式2变成公式3的过程中，T的大小会变化，从Nc变成NM
构建token：上图流程图中，M(sub-word tokens)总是比短语格式c多，原因有四个1）一些短语占了多个toeken位置，比如 traffic light。2）一些短语被分开成sub words，比如toothbrush分成了 tooth#, #brush。3）一些是添加的token，如逗号，Deteckt等，4）结尾会添加[NoObj]的token。在训练的时候，phrase是正例的话，多个subwords都是正例。测试时多个token的平均pro作为短语的probability。
3. detection和grounding联动：由上面的方法，可以用grounding模型来预训练检测任务，从而可以迁移GLIP模型做zero-shot的检测
3.2 方法2: deep fusion，视觉和语言联合 fusion部分公式如下：
​​​​​​​​​​​​​​O0是视觉backbone的feature, P0 是文本backbone的feature
X-MHA（cross-modality multi-head attention module）
L是DyHead中DyHeadModules个数，BERT Layer为新增。
attention部分在多模态中比较常见，比如co-attention、guided attention等。可以参考多模态中attention其他优化。
DeepFusion优点：
提高了phrase grounding效果" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/457d52bf9a662d034eba5548f891b1ed/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-24T00:45:43+08:00" />
<meta property="article:modified_time" content="2022-07-24T00:45:43+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【论文精读】Grounded Language-Image Pre-training（GLIP）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="__0"></a>一. 背景</h2> 
<pre><code class="prism language-bash">https://arxiv.org/abs/2112.03857
https://github.com/microsoft/GLIP
</code></pre> 
<p><img src="https://images2.imgbox.com/75/88/nbVhdOcI_o.png" alt="在这里插入图片描述"><br> 这篇论文做的任务是phrase grounding，属于visual grounding的一种。phrase grounding的任务是输入句子和图片，将句子中提到的物体都框出来。visual grounding其他任务和细节可以参考<br> https://zhuanlan.zhihu.com/p/388504127</p> 
<p>GLIP既可以做目标检测也可以做grounding，</p> 
<ul><li>目标检测：<br>  在扩增目标检测领域为SOTA，zero-shot效果较好，也可以做zero-shot目标检测任务。<br>  与常规目标检测任务相比语义丰富。</li><li>grounding：<br>  与常规grounding任务相比可以做目标检测任务。</li></ul> 
<h2><a id="_18"></a>二、贡献</h2> 
<p>贡献</p> 
<ul><li>将目标检测和phrase grounding任务统一起来进行预训练</li><li>扩大视觉语义</li><li>迁移学习能力强</li></ul> 
<p>性能</p> 
<ul><li>27M关联数据上训练。在目标识别任务上有很强的零样本和小样本迁移性能</li><li>Zero-shot：coco val上49.8AP，LVIS val上26.9AP</li><li>微调后：COCO val上60.8AP</li><li>下游13个目标检测任务时，1个样本的GLIP可以与Dynamic Head相匹敌</li></ul> 
<h2><a id="_30"></a>三、方法</h2> 
<h3><a id="31_1grounding_31"></a>3.1 方法1：检测和grounding任务统一</h3> 
<p><img src="https://images2.imgbox.com/43/39/8equ3d1C_o.png" alt="在这里插入图片描述"><br> <font color="#FF0000">1. background: </font><br> 对于检测数据集：<br> 训练时输入标签名(person、hairdryer）、框、图片。<br> 测试时输入图片，预测出框和标签名。<br> 训练过程如下：<br> <img src="https://images2.imgbox.com/58/3d/N3MOr0M3_o.png" alt="在这里插入图片描述"><br> <font color="#FF0000">2. background as grounding：</font><br> groudning模型的输入是短语、短语中名词的框和图片。<br> 将object模型转为grounding的办法：通过prompt的方式将标签名转化为短语。<br> <img src="https://images2.imgbox.com/83/77/CI2xYCaC_o.png" alt="在这里插入图片描述"></p> 
<p>如coco有80个标签，将80个标签用逗号连接，短语前加“Detect：”，来组成短句。</p> 
<p>公式2变成公式3的过程中，T的大小会变化，从N<em>c变成N</em>M<br> 构建token：上图流程图中，M(sub-word tokens)总是比短语格式c多，原因有四个1）一些短语占了多个toeken位置，比如 traffic light。2）一些短语被分开成sub words，比如toothbrush分成了 tooth#, #brush。3）一些是添加的token，如逗号，Deteckt等，4）结尾会添加[NoObj]的token。在训练的时候，phrase是正例的话，多个subwords都是正例。测试时多个token的平均pro作为短语的probability。</p> 
<p><font color="#FF0000">3. detection和grounding联动：</font>由上面的方法，可以用grounding模型来预训练检测任务，从而可以迁移GLIP模型做zero-shot的检测</p> 
<h3><a id="32_2_deep__fusion_53"></a>3.2 方法2: deep fusion，视觉和语言联合</h3> 
<p><img src="https://images2.imgbox.com/04/e2/6EXNxjsi_o.png" alt="在这里插入图片描述"><br> fusion部分公式如下：<br> <img src="https://images2.imgbox.com/a9/d1/4XYdEBcB_o.png" alt="在这里插入图片描述"></p> 
<p>​​​​​​​​​​​​​​O<sup>0</sup>是视觉backbone的feature, P<sup>0</sup> 是文本backbone的feature<br> X-MHA（cross-modality multi-head attention module）<br> L是DyHead中DyHeadModules个数，BERT Layer为新增。</p> 
<p>attention部分在多模态中比较常见，比如co-attention、guided attention等。可以参考多模态中attention其他优化。<br> <font color="#FF0000">DeepFusion优点：</font><br> 提高了phrase grounding效果<br> 使得视觉特征language-aware</p> 
<h3><a id="33_3__67"></a>3.3 方法3: 用丰富的语义数据预训练</h3> 
<p>grounding数据集语义都很丰富，目标检测不超过2000个类别，但是grounding数据集如Flickr30K包括了4.4w不同的短语，量级不同。<br> <font color="#FF0000">如何扩增grounding数据：</font></p> 
<ol><li>在gold data（det+grounding）上训练教师GLIP</li><li>使用这个教师模型来预测24M web image-text数据，通过NLP解析名词短语，存在5840个不同名词短语</li><li>学生模型在gold data和伪标签grounding数据上训练<br> <font color="#FF0000">扩增效果：</font><br> 学生模型效果比教师模型效果好，比如对于部分词汇，vaccine教师模型可能预测不出来，但是可以预测出a small vial，subwords对的，整体phrase都会是对的。那在给学生模型无监督数据时，可以将a small vial of vaccine标签整体给到学生模型作为学习标签。<br> <img src="https://images2.imgbox.com/ce/e9/15AJFusZ_o.png" alt="在这里插入图片描述"></li></ol> 
<h2><a id="_76"></a>四、实验结果</h2> 
<p><img src="https://images2.imgbox.com/a3/bf/yoTfZ1iG_o.png" alt="在这里插入图片描述"><br> <strong>FourODs</strong>（2.66M数据）是4个检测数据集集合，包括objects365、OpenImages、VG数据集（除了coco）、ImageNetBox。<br> <strong>GoldG+</strong> 数据集包括1.3M数据集，包括Flickr30K、VG caption、GQA。<br> <strong>GoldG</strong> 数据集是GoldG+去除了coco数据集</p> 
<h3><a id="41__81"></a>4.1 迁移效果在检测数据集上</h3> 
<p>zero-shot在coco上：</p> 
<ol><li>图文数据集没有带来提升</li><li>C和B比提升较大</li><li>Objects365包括了coco的80个<br> <img src="https://images2.imgbox.com/85/99/j6L2guQb_o.png" alt="在这里插入图片描述"><br> 在LVIS上效果：<br> LVIS：大规模细粒度词汇级标记数据集，1000+类别，披萨里的菠萝丁也被标记<br> Gold grounding很有效（model C vs model B）<br> <img src="https://images2.imgbox.com/e9/ab/s4EyJu4r_o.png" alt="在这里插入图片描述"></li></ol> 
<h3><a id="42_grounding_92"></a>4.2 在grounding数据集上</h3> 
<p>Flick 30k：图文匹配grounding数据集，goldG中包含了该数据集<br> <img src="https://images2.imgbox.com/46/4f/S4NimZvs_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="43__96"></a>4.3 消融实验-检测数据集影响</h3> 
<p>O365: 0.66M<br> GoldG: 0.8M<br> FourODs: 2.66M<br> 但是不是O365+GoldG效果反而更好<br> <img src="https://images2.imgbox.com/f0/46/LNJeRCBZ_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="44__102"></a>4.4 其他</h3> 
<p>如果定位不好，可以添加提示词帮助更好定位，下图添加了flat and round<br> <img src="https://images2.imgbox.com/c2/2d/4OOe94bn_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/fd44308d10112412630a3c3cc63ae2a3/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">最简单理解并实现斐波那契数列函数（c语言）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b2fc821aa472f2b51ed785122fec5168/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【方向盘】使用IDEA的60&#43;个快捷键分享给你，权为了提效（Live Template&amp;Postfix Completion篇）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>