<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【预训练语言模型】RoBERTa: A Robustly Optimized BERT Pretraining Approach - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【预训练语言模型】RoBERTa: A Robustly Optimized BERT Pretraining Approach" />
<meta property="og:description" content="【预训练语言模型】RoBERTa: A Robustly Optimized BERT Pretraining Approach 作者发现BERT以及提供的预训练语言模型并没有得到充分的训练，因此本文提出RoBERTa以挖掘BERT模型，并提供充分的训练。作者认为，扩增训练语料、增大预训练的迭代次数、去掉Next Sentence Prediction、在更长的序列上训练、动态Masking等策略（Trick）可以大幅度提升BERT的性能。
简要信息：
序号属性值1模型名称RoBERTa2所属领域自然语言处理3研究内容预训练语言模型4核心内容BERT改进5GitHub源码https://github.com/pytorch/fairseq6论文PDFhttps://arxiv.org/pdf/1907.11692.pdf 一、动机 现有的基于self-training的语言模型（例如ELMo、GPT、BERT等）方法虽然达到了SOTA，但是很难判断那个部分对效果具有很大的促进作用。同时预训练成本很高，使用的provate data限制了模型扩展；我们发现BERT预训练模型并没有得到充分的训练，语义挖掘能力还有一定提升空间； 二、背景——BERT模型及实验设置 可直接参考BERT讲解。
三、RoBERTa——Robustly optimized BERT approach 3.1 More Data BERT只用了Wikipedia和BookCorpus，RoBERTa又额外扩增了训练语料。RoBERTa一共在5个语料上训练，包括Wikipedia、BookCorpus、CC-News、OpenWebText和Stories。后续的实验均在这5个语料上完成。
3.2 Dynamic Making Strategy Masked Language Modeling是BERT中非常重要的预训练目标，但是，在BERT训练过程中，带有随机Mask的语料是数据预处理阶段得到的，而在训练过程中则固定不变（Static Masking）。因此BERT在训练时，对于每一个句子，每次都将见到相同Mask。
因此RoBERTa提出动态地改变每次训练时Mask采样位置（Dynamic Masking）。即每迭代一次训练，重新对每个句子的Mask进行采样。该策略间接实现了数据增强，且提高了鲁棒性。通过改变Mask策略，在QA、NLI以及分类任务上有提升：
3.3 The necessary of NSP？ Next Sentence Prediction（NSP）通常对sentence-pair的输入进行训练，目标是预测两个句子是否存在前后关系。但RoBERTa发现去掉NSP效果反而更好：
3.4 Larger Batch Size 在BERT中，batch size设置为256，一个epoch需要训练超过1M步。RoBERTa训练过程中，增大了batch size。如下表：
不同的batch size以及对应的学习率。实验发现当batch size为2k时，效果可以达到最好。batch size设置大可以采用数据并行方法进行训练。
3.5 Text Encoding——BPE Byte-Pair Encoding（BPE）由Neural Machine Translation of Rare Words with Subword Units提出解决在机器翻译领域中出现的Out-of- Vocabulary（OOV）问题。主要通过wordpiece技术将word分解为更为细粒度的片段。RoBERTa采用BPE，获得了超过5w个token（BERT只有3w）。
BPE的详解可参考：BPE(Byte Pair Encoding)算法" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/44105450ec0008c66140bddea75dd1b9/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-11-19T16:06:07+08:00" />
<meta property="article:modified_time" content="2021-11-19T16:06:07+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【预训练语言模型】RoBERTa: A Robustly Optimized BERT Pretraining Approach</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="RoBERTa_A_Robustly_Optimized_BERT_Pretraining_Approach_0"></a>【预训练语言模型】RoBERTa: A Robustly Optimized BERT Pretraining Approach</h2> 
<p>  作者发现BERT以及提供的预训练语言模型并没有得到充分的训练，因此本文提出RoBERTa以挖掘BERT模型，并提供充分的训练。作者认为，<strong>扩增训练语料</strong>、<strong>增大预训练的迭代次数</strong>、<strong>去掉Next Sentence Prediction</strong>、<strong>在更长的序列上训练</strong>、<strong>动态Masking</strong>等策略（Trick）可以大幅度提升BERT的性能。</p> 
<p><strong>简要信息：</strong></p> 
<table><thead><tr><th>序号</th><th>属性</th><th>值</th></tr></thead><tbody><tr><td>1</td><td>模型名称</td><td>RoBERTa</td></tr><tr><td>2</td><td>所属领域</td><td>自然语言处理</td></tr><tr><td>3</td><td>研究内容</td><td>预训练语言模型</td></tr><tr><td>4</td><td>核心内容</td><td>BERT改进</td></tr><tr><td>5</td><td>GitHub源码</td><td><a href="https://github.com/pytorch/fairseq">https://github.com/pytorch/fairseq</a></td></tr><tr><td>6</td><td>论文PDF</td><td><a href="https://arxiv.org/pdf/1907.11692.pdf" rel="nofollow">https://arxiv.org/pdf/1907.11692.pdf</a></td></tr></tbody></table> 
<h3><a id="_16"></a>一、动机</h3> 
<ul><li>现有的基于self-training的语言模型（例如ELMo、GPT、BERT等）方法虽然达到了SOTA，但是很难判断那个部分对效果具有很大的促进作用。同时预训练成本很高，使用的provate data限制了模型扩展；</li><li>我们发现BERT预训练模型并没有得到充分的训练，语义挖掘能力还有一定提升空间；</li></ul> 
<h3><a id="BERT_20"></a>二、背景——BERT模型及实验设置</h3> 
<p>  可直接参考<a href="https://wjn1996.blog.csdn.net/article/details/112223838" rel="nofollow">BERT</a>讲解。</p> 
<h3><a id="RoBERTafont_colorredRfontobustly_font_colorredofontptimized_font_colorredBERTfont_font_colorredafontpproach_23"></a>三、RoBERTa——<font color="red">R</font>obustly <font color="red">o</font>ptimized <font color="red">BERT</font> <font color="red">a</font>pproach</h3> 
<h5><a id="31_More_Data_25"></a>3.1 More Data</h5> 
<p>  BERT只用了Wikipedia和BookCorpus，RoBERTa又额外扩增了训练语料。RoBERTa一共在5个语料上训练，包括Wikipedia、BookCorpus、CC-News、OpenWebText和Stories。后续的实验均在这5个语料上完成。</p> 
<h5><a id="32_Dynamic_Making_Strategy_28"></a>3.2 Dynamic Making Strategy</h5> 
<p>  Masked Language Modeling是BERT中非常重要的预训练目标，但是，在BERT训练过程中，带有随机Mask的语料是数据预处理阶段得到的，而在训练过程中则固定不变（Static Masking）。因此BERT在训练时，对于每一个句子，每次都将见到相同Mask。</p> 
<p>  因此RoBERTa提出动态地改变每次训练时Mask采样位置（Dynamic Masking）。即每迭代一次训练，重新对每个句子的Mask进行采样。该策略间接实现了数据增强，且提高了鲁棒性。通过改变Mask策略，在QA、NLI以及分类任务上有提升：<br> <img src="https://images2.imgbox.com/0f/68/qGBTl2Pm_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="33_The_necessary_of_NSP_34"></a>3.3 The necessary of NSP？</h5> 
<p>  Next Sentence Prediction（NSP）通常对sentence-pair的输入进行训练，目标是预测两个句子是否存在前后关系。但RoBERTa发现去掉NSP效果反而更好：<br> <img src="https://images2.imgbox.com/8a/d2/a8JjJdbZ_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="34_Larger_Batch_Size_38"></a>3.4 Larger Batch Size</h5> 
<p>  在BERT中，batch size设置为256，一个epoch需要训练超过1M步。RoBERTa训练过程中，增大了batch size。如下表：<br> <img src="https://images2.imgbox.com/16/e5/abTKrzMF_o.png" alt="在这里插入图片描述"><br> 不同的batch size以及对应的学习率。实验发现当batch size为2k时，效果可以达到最好。batch size设置大可以采用数据并行方法进行训练。</p> 
<h5><a id="35_Text_EncodingBPE_43"></a>3.5 Text Encoding——BPE</h5> 
<p>  Byte-Pair Encoding（BPE）由<a href="https://arxiv.org/pdf/1508.07909.pdf" rel="nofollow">Neural Machine Translation of Rare Words with Subword Units</a>提出解决在机器翻译领域中出现的Out-of- Vocabulary（OOV）问题。主要通过wordpiece技术将word分解为更为细粒度的片段。RoBERTa采用BPE，获得了超过5w个token（BERT只有3w）。</p> 
<blockquote> 
 <p>BPE的详解可参考：<a href="https://blog.csdn.net/foneone/article/details/103811328">BPE(Byte Pair Encoding)算法</a></p> 
</blockquote> 
<h3><a id="_48"></a>四、实验</h3> 
<p>  RoBERTa参与了SQuAD、RACE和GLUE的打榜，并与当时最好的模型XLNet进行比对，结果如下：</p> 
<h5><a id="SQuAD_50"></a>SQuAD</h5> 
<p><img src="https://images2.imgbox.com/0f/58/SHkBMSTV_o.png" alt="在这里插入图片描述"></p> 
<blockquote> 
 <p>其中SG-Net是一个抽取式问答的模型，博主做过论文解读，可参考：<a href="https://blog.csdn.net/qq_36426650/article/details/110309423?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163730887016780264027221%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&amp;request_id=163730887016780264027221&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_v2~rank_v29-2-110309423.pc_v2_rank_blog_default&amp;utm_term=SG&amp;spm=1018.2226.3001.4450#t6">机器阅读理解算法集锦</a></p> 
</blockquote> 
<h5><a id="RACE_54"></a>RACE</h5> 
<p><img src="https://images2.imgbox.com/68/e3/4ca2f7HG_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="GLUE_58"></a>GLUE</h5> 
<p><img src="https://images2.imgbox.com/39/c3/W6BNKp6c_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/71070812369d0ec92159fcab358e92c3/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Spring Data ElasticSearch analyzer 定义 @Filed失效 @Mapping失效 创建索引 无效 解决办法 ElasticsearchRestTemplate</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/4a50b3d618dd18c5876e5d5816a573d0/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">java项目poi插件导出Excel文件名中文乱码</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>