<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ResNets - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="ResNets" />
<meta property="og:description" content="ResNets 背景： 非常非常深的神经网络是很难训练的，因为存在梯度消失和梯度爆炸问题。
《转载&#43;更改》
https://blog.csdn.net/qq_29893385/article/details/81207203
ResNets是由残差块（Residual block）构建的
首先解释一下什么是残差块。 这是一个两层神经网络，在 relu层进行激活，得到a^(l&#43;1) ，再次进行激活，两层之后得到a^(l&#43;2) 。计算过程是从a^1 开始，首先进行线性激活，根据这个公式： ，通过 算出 ，即 乘以权重矩阵，再加上偏差因子。然后通过ReLU非线性激活函数得到 ， 计算得出。
接着再次进行线性激活，依据等式 ，最后根据这个等式再次进行ReLu非线性激活，即 ，这里的 是指ReLU非线性函数，得到的结果就是 。换句话说，信息流从 到 需要经过以上所有步骤，即这组网络层的主路径。
在残差网络中的变化： 前面的输入，将直接传向后，拷贝到神经网络的深层，在ReLU非线性激活函数前加上 ，形成一条捷径。
a^1 的信息直接到达神经网络的深层，不再沿着主路径传递，ReLU非线性函数，对 二个输入进行函数处理，即： ，产生一个残差块。
在上面这个图中，画一条捷径，直达第二层。
这条捷径在进行ReLU非线性激活函数之前加上的，而这里的每一个节点都执行了线性函数和ReLU激活函数。插入的时机是在线性激活之后，ReLU激活之前。
除了捷径，另一个术语“跳跃连接”，就是指跳过一层或者好几层，从而将信息传递到神经网络的更深层。
ResNet的发明者是何恺明（Kaiming He）、张翔宇（Xiangyu Zhang）、任少卿（Shaoqing Ren）和孙剑（Jiangxi Sun），他们发现使用残差块能够训练更深的神经网络。所以构建一个ResNet网络就是通过将很多这样的残差块堆积在一起，形成一个很深神经网络。
这并不是一个残差网络，而是一个普通网络（Plain network），这个术语来自ResNet论文。
把它变成ResNet的方法是加上所有跳跃连接，每两层增加一个捷径，构成一个残差块。如图所示，5个残差块连接在一起构成一个残差网络。
如果我们使用标准优化算法训练一个普通网络，比如说梯度下降法，或者其它热门的优化算法。如果没有残差，没有这些捷径或者跳跃连接，凭经验你会发现随着网络深度的加深，训练错误会先减少，然后增多。而理论上，随着网络深度的加深，应该训练得越来越好才对。也就是说，理论上网络深度越深越好。但实际上，如果没有残差网络，对于一个普通网络来说，深度越深意味着用优化算法越难训练。实际上，随着网络深度的加深，训练错误会越来越多。
但有了ResNets就不一样了，即使网络再深，训练的表现却不错，比如说训练误差减少，就算是训练深达100层的网络也不例外。有人甚至在1000多层的神经网络中做过实验，尽管目前我还没有看到太多实际应用。但是对 的激活，或者这些中间的激活能够到达网络的更深层。这种方式确实有助于解决梯度消失和梯度爆炸问题，让我们在训练更深网络的同时，又能保证良好的性能。也许从另外一个角度来看，随着网络越来深，网络连接会变得臃肿，但是ResNet确实在训练深度网络方面非常有效。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/a81676469672af29a8f0ccf754dcd9a0/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-10-19T11:01:13+08:00" />
<meta property="article:modified_time" content="2018-10-19T11:01:13+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ResNets</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="ResNets_1"></a>ResNets</h2> 
<p>背景： 非常非常深的神经网络是很难训练的，因为存在梯度消失和梯度爆炸问题。<br> 《转载+更改》<br> <a href="https://blog.csdn.net/qq_29893385/article/details/81207203">https://blog.csdn.net/qq_29893385/article/details/81207203</a></p> 
<ol><li>ResNets是由残差块（Residual block）构建的<br> 首先解释一下什么是残差块。</li></ol> 
<p><img src="https://images2.imgbox.com/48/bb/rJn480GA_o.png" alt="在这里插入图片描述"></p> 
<p>这是一个两层神经网络，在 relu层进行激活，得到a^(l+1) ，再次进行激活，两层之后得到a^(l+2) 。计算过程是从a^1 开始，首先进行线性激活，根据这个公式：<img src="https://images2.imgbox.com/b2/3d/b1zA2svp_o.png" alt="在这里插入图片描述"> ，通过 算出 ，即 乘以权重矩阵，再加上偏差因子。然后通过ReLU非线性激活函数得到 ， 计算得出。<br> 接着再次进行线性激活，依据等式<img src="https://images2.imgbox.com/f8/f2/xfv66SSW_o.png" alt="在这里插入图片描述"> ，最后根据这个等式再次进行ReLu非线性激活，即 ，这里的 是指ReLU非线性函数，得到的结果就是<img src="https://images2.imgbox.com/c1/3c/CaOpizGq_o.png" alt="在这里插入图片描述"> 。换句话说，信息流从 到 需要经过以上所有步骤，即这组网络层的主路径。</p> 
<p><img src="https://images2.imgbox.com/ee/a3/U33Ngt0t_o.png" alt="在这里插入图片描述"></p> 
<ol start="2"><li>在残差网络中的变化：</li></ol> 
<p>前面的输入，将直接传向后，拷贝到神经网络的深层，在ReLU非线性激活函数前加上 ，形成一条捷径。<br> a^1 的信息直接到达神经网络的深层，不再沿着主路径传递，ReLU非线性函数，对 二个输入进行函数处理，即：<img src="https://images2.imgbox.com/bb/36/VnMebpJU_o.png" alt="在这里插入图片描述">  ，产生一个残差块。</p> 
<p><img src="https://images2.imgbox.com/3d/9a/Y6Uc50VR_o.png" alt="在这里插入图片描述"></p> 
<p>在上面这个图中，画一条捷径，直达第二层。<br> 这条捷径在进行ReLU非线性激活函数之前加上的，而这里的每一个节点都执行了线性函数和ReLU激活函数。插入的时机是在线性激活之后，ReLU激活之前。<br> 除了捷径，另一个术语“跳跃连接”，就是指跳过一层或者好几层，从而将信息传递到神经网络的更深层。</p> 
<p>ResNet的发明者是何恺明（Kaiming He）、张翔宇（Xiangyu Zhang）、任少卿（Shaoqing Ren）和孙剑（Jiangxi Sun），他们发现使用残差块能够训练更深的神经网络。所以构建一个ResNet网络就是通过将很多这样的残差块堆积在一起，形成一个很深神经网络。</p> 
<p><img src="https://images2.imgbox.com/27/b3/DS9Z6Kxg_o.png" alt="在这里插入图片描述"></p> 
<p>这并不是一个残差网络，而是一个普通网络（Plain network），这个术语来自ResNet论文。</p> 
<p><img src="https://images2.imgbox.com/8a/30/qVgQvC9t_o.png" alt="在这里插入图片描述"></p> 
<p>把它变成ResNet的方法是加上所有跳跃连接，每两层增加一个捷径，构成一个残差块。如图所示，5个残差块连接在一起构成一个残差网络。</p> 
<p><img src="https://images2.imgbox.com/d3/09/AlcFwrAT_o.png" alt="在这里插入图片描述"></p> 
<p>如果我们使用标准优化算法训练一个普通网络，比如说梯度下降法，或者其它热门的优化算法。如果没有残差，没有这些捷径或者跳跃连接，凭经验你会发现随着网络深度的加深，训练错误会先减少，然后增多。而理论上，随着网络深度的加深，应该训练得越来越好才对。也就是说，理论上网络深度越深越好。但实际上，如果没有残差网络，对于一个普通网络来说，深度越深意味着用优化算法越难训练。实际上，随着网络深度的加深，训练错误会越来越多。</p> 
<p>但有了ResNets就不一样了，即使网络再深，训练的表现却不错，比如说训练误差减少，就算是训练深达100层的网络也不例外。有人甚至在1000多层的神经网络中做过实验，尽管目前我还没有看到太多实际应用。但是对 的激活，或者这些中间的激活能够到达网络的更深层。这种方式确实有助于解决梯度消失和梯度爆炸问题，让我们在训练更深网络的同时，又能保证良好的性能。也许从另外一个角度来看，随着网络越来深，网络连接会变得臃肿，但是ResNet确实在训练深度网络方面非常有效。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/7476551c2e2b8cc1a17642e609eb5385/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">IP地址自动封与解封的shell脚本</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/30f98dbda3ac464a1becfdb34d4962af/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">数据导入的几种方法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>