<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Python3《机器学习实战》学习笔记（五）：朴素贝叶斯实战篇之新浪新闻分类 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Python3《机器学习实战》学习笔记（五）：朴素贝叶斯实战篇之新浪新闻分类" />
<meta property="og:description" content="转载请注明作者和出处：http://blog.csdn.net/c406495762 Github代码获取：https://github.com/Jack-Cherish/Machine-Learning/ 机器学习知乎专栏：https://zhuanlan.zhihu.com/ml-jack Python版本： Python3.x 运行平台： Windows IDE： Sublime text3
一 前言二 朴素贝叶斯改进之拉普拉斯平滑三 朴素贝叶斯之过滤垃圾邮件 收集数据准备数据 四 朴素贝叶斯之新浪新闻分类Sklearn 中文语句切分文本特征选择使用Sklearn构建朴素贝叶斯分类器 五 总结 一 前言 上篇文章Python3《机器学习实战》学习笔记（四）：朴素贝叶斯基础篇之言论过滤器讲解了朴素贝叶斯的基础知识。本篇文章将在此基础上进行扩展，你将看到以下内容：
拉普拉斯平滑垃圾邮件过滤新浪新闻分类 二 朴素贝叶斯改进之拉普拉斯平滑 上篇文章提到过，算法存在一定的问题，需要进行改进。那么需要改进的地方在哪里呢？利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计算p(w0|1)p(w1|1)p(w2|1)。如果其中有一个概率值为0，那么最后的成绩也为0。我们拿出上一篇文章的截图。
从上图可以看出，在计算的时候已经出现了概率为0的情况。如果新实例文本，包含这种概率为0的分词，那么最终的文本属于某个类别的概率也就是0了。显然，这样是不合理的，为了降低这种影响，可以将所有词的出现数初始化为1，并将分母初始化为2。这种做法就叫做拉普拉斯平滑(Laplace Smoothing)又被称为加1平滑，是比较常用的平滑方法，它就是为了解决0概率问题。
除此之外，另外一个遇到的问题就是下溢出，这是由于太多很小的数相乘造成的。学过数学的人都知道，两个小数相乘，越乘越小，这样就造成了下溢出。在程序中，在相应小数位置进行四舍五入，计算结果可能就变成0了。为了解决这个问题，对乘积结果取自然对数。通过求对数可以避免下溢出或者浮点数舍入导致的错误。同时，采用自然对数进行处理不会有任何损失。下图给出函数f(x)和ln(f(x))的曲线。
检查这两条曲线，就会发现它们在相同区域内同时增加或者减少，并且在相同点上取到极值。它们的取值虽然不同，但不影响最终结果。因此我们可以对上篇文章的trainNB0(trainMatrix, trainCategory)函数进行更改，修改如下：
&#34;&#34;&#34; 函数说明:朴素贝叶斯分类器训练函数 Parameters: trainMatrix - 训练文档矩阵，即setOfWords2Vec返回的returnVec构成的矩阵 trainCategory - 训练类别标签向量，即loadDataSet返回的classVec Returns: p0Vect - 侮辱类的条件概率数组 p1Vect - 非侮辱类的条件概率数组 pAbusive - 文档属于侮辱类的概率 Author: Jack Cui Blog: http://blog.csdn.net/c406495762 Modify: 2017-08-12 &#34;&#34;&#34; def trainNB0(trainMatrix,trainCategory): numTrainDocs = len(trainMatrix) #计算训练的文档数目 numWords = len(trainMatrix[0]) #计算每篇文档的词条数 pAbusive = sum(trainCategory)/float(numTrainDocs) #文档属于侮辱类的概率 p0Num = np." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/89fcfbe7d6d1732d8f245f9bca66226c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2017-08-23T10:16:04+08:00" />
<meta property="article:modified_time" content="2017-08-23T10:16:04+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Python3《机器学习实战》学习笔记（五）：朴素贝叶斯实战篇之新浪新闻分类</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><strong>转载请注明作者和出处：</strong><a href="http://blog.csdn.net/c406495762" target="_blank" rel="noopener noreferrer">http://blog.csdn.net/c406495762</a> <br> <strong>Github代码获取：</strong><a href="https://github.com/Jack-Cherish/Machine-Learning/" target="_blank" rel="noopener noreferrer">https://github.com/Jack-Cherish/Machine-Learning/</a> <br> <strong>机器学习知乎专栏：</strong><a href="https://zhuanlan.zhihu.com/ml-jack" rel="nofollow noopener noreferrer" target="_blank">https://zhuanlan.zhihu.com/ml-jack</a> <br> <strong>Python版本：</strong> Python3.x <br> <strong>运行平台：</strong> Windows <br> <strong>IDE：</strong> Sublime text3</p> 
<p></p> 
<div class="toc"> 
 <div class="toc"> 
  <ul><li><a href="#%E4%B8%80-%E5%89%8D%E8%A8%80" rel="nofollow">一 前言</a></li><li><a href="#%E4%BA%8C-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%94%B9%E8%BF%9B%E4%B9%8B%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%B9%B3%E6%BB%91" rel="nofollow">二 朴素贝叶斯改进之拉普拉斯平滑</a></li><li><a href="#%E4%B8%89-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%B9%8B%E8%BF%87%E6%BB%A4%E5%9E%83%E5%9C%BE%E9%82%AE%E4%BB%B6" rel="nofollow">三 朴素贝叶斯之过滤垃圾邮件</a> 
    <ul><li><a href="#1-%E6%94%B6%E9%9B%86%E6%95%B0%E6%8D%AE" rel="nofollow">收集数据</a></li><li><a href="#2-%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE" rel="nofollow">准备数据</a></li></ul> </li><li><a href="#%E5%9B%9B-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%B9%8B%E6%96%B0%E6%B5%AA%E6%96%B0%E9%97%BB%E5%88%86%E7%B1%BBsklearn" rel="nofollow">四 朴素贝叶斯之新浪新闻分类Sklearn</a> 
    <ul><li><a href="#1-%E4%B8%AD%E6%96%87%E8%AF%AD%E5%8F%A5%E5%88%87%E5%88%86" rel="nofollow">中文语句切分</a></li><li><a href="#2-%E6%96%87%E6%9C%AC%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9" rel="nofollow">文本特征选择</a></li><li><a href="#3-%E4%BD%BF%E7%94%A8sklearn%E6%9E%84%E5%BB%BA%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8" rel="nofollow">使用Sklearn构建朴素贝叶斯分类器</a></li></ul> </li><li><a href="#%E4%BA%94-%E6%80%BB%E7%BB%93" rel="nofollow">五 总结</a></li></ul> 
 </div> 
</div> 
<p></p> 
<hr> 
<h2 id="一-前言">一 前言</h2> 
<p>上篇文章<a href="http://blog.csdn.net/c406495762/article/details/77341116" target="_blank" rel="noopener noreferrer">Python3《机器学习实战》学习笔记（四）：朴素贝叶斯基础篇之言论过滤器</a>讲解了朴素贝叶斯的基础知识。本篇文章将在此基础上进行扩展，你将看到以下内容：</p> 
<ul><li>拉普拉斯平滑</li><li>垃圾邮件过滤</li><li>新浪新闻分类</li></ul> 
<hr> 
<h2 id="二-朴素贝叶斯改进之拉普拉斯平滑">二 朴素贝叶斯改进之拉普拉斯平滑</h2> 
<p>上篇文章提到过，算法存在一定的问题，需要进行改进。那么需要改进的地方在哪里呢？利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计算p(w0|1)p(w1|1)p(w2|1)。如果其中有一个概率值为0，那么最后的成绩也为0。我们拿出上一篇文章的截图。</p> 
<p></p> 
<p></p> 
<div align="center"> 
 <img src="https://images2.imgbox.com/97/d0/GjQkUrOt_o.png"> 
</div> 
<p></p> 
<p></p> 
<p>从上图可以看出，在计算的时候已经出现了概率为0的情况。如果新实例文本，包含这种概率为0的分词，那么最终的文本属于某个类别的概率也就是0了。显然，这样是不合理的，为了降低这种影响，可以将所有词的出现数初始化为1，并将分母初始化为2。这种做法就叫做拉普拉斯平滑(Laplace Smoothing)又被称为加1平滑，是比较常用的平滑方法，它就是为了解决0概率问题。</p> 
<p>除此之外，另外一个遇到的问题就是下溢出，这是由于太多很小的数相乘造成的。学过数学的人都知道，两个小数相乘，越乘越小，这样就造成了下溢出。在程序中，在相应小数位置进行四舍五入，计算结果可能就变成0了。为了解决这个问题，对乘积结果取自然对数。通过求对数可以避免下溢出或者浮点数舍入导致的错误。同时，采用自然对数进行处理不会有任何损失。下图给出函数f(x)和ln(f(x))的曲线。</p> 
<p></p> 
<p></p> 
<div align="center"> 
 <img src="https://images2.imgbox.com/b9/91/KrA5pNJD_o.png"> 
</div> 
<p></p> 
<p></p> 
<p>检查这两条曲线，就会发现它们在相同区域内同时增加或者减少，并且在相同点上取到极值。它们的取值虽然不同，但不影响最终结果。因此我们可以对上篇文章的trainNB0(trainMatrix, trainCategory)函数进行更改，修改如下：</p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-string">"""
函数说明:朴素贝叶斯分类器训练函数

Parameters:
    trainMatrix - 训练文档矩阵，即setOfWords2Vec返回的returnVec构成的矩阵
    trainCategory - 训练类别标签向量，即loadDataSet返回的classVec
Returns:
    p0Vect - 侮辱类的条件概率数组
    p1Vect - 非侮辱类的条件概率数组
    pAbusive - 文档属于侮辱类的概率
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-12
"""</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">trainNB0</span><span class="hljs-params">(trainMatrix,trainCategory)</span>:</span>
    numTrainDocs = len(trainMatrix)                            <span class="hljs-comment">#计算训练的文档数目</span>
    numWords = len(trainMatrix[<span class="hljs-number">0</span>])                            <span class="hljs-comment">#计算每篇文档的词条数</span>
    pAbusive = sum(trainCategory)/float(numTrainDocs)        <span class="hljs-comment">#文档属于侮辱类的概率</span>
    p0Num = np.ones(numWords); p1Num = np.ones(numWords)    <span class="hljs-comment">#创建numpy.ones数组,词条出现数初始化为1，拉普拉斯平滑</span>
    p0Denom = <span class="hljs-number">2.0</span>; p1Denom = <span class="hljs-number">2.0</span>                            <span class="hljs-comment">#分母初始化为2,拉普拉斯平滑</span>
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(numTrainDocs):
        <span class="hljs-keyword">if</span> trainCategory[i] == <span class="hljs-number">1</span>:                            <span class="hljs-comment">#统计属于侮辱类的条件概率所需的数据，即P(w0|1),P(w1|1),P(w2|1)···</span>
            p1Num += trainMatrix[i]
            p1Denom += sum(trainMatrix[i])
        <span class="hljs-keyword">else</span>:                                                <span class="hljs-comment">#统计属于非侮辱类的条件概率所需的数据，即P(w0|0),P(w1|0),P(w2|0)···</span>
            p0Num += trainMatrix[i]
            p0Denom += sum(trainMatrix[i])
    p1Vect = np.log(p1Num/p1Denom)                            <span class="hljs-comment">#取对数，防止下溢出         </span>
    p0Vect = np.log(p0Num/p0Denom)         
    <span class="hljs-keyword">return</span> p0Vect,p1Vect,pAbusive                            <span class="hljs-comment">#返回属于侮辱类的条件概率数组，属于非侮辱类的条件概率数组，文档属于侮辱类的概率</span></code></pre> 
<p>运行代码，就可以得到如下结果：</p> 
<p></p> 
<p></p> 
<div align="center"> 
 <img src="https://images2.imgbox.com/56/46/HTtP9yvY_o.png"> 
</div> 
<p></p> 
<p></p> 
<p>瞧，这样我们得到的结果就没有问题了，不存在0概率。当然除此之外，我们还需要对代码进行修改classifyNB(vec2Classify, p0Vec, p1Vec, pClass1)函数，修改如下：</p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-string">"""
函数说明:朴素贝叶斯分类器分类函数

Parameters:
    vec2Classify - 待分类的词条数组
    p0Vec - 侮辱类的条件概率数组
    p1Vec -非侮辱类的条件概率数组
    pClass1 - 文档属于侮辱类的概率
Returns:
    0 - 属于非侮辱类
    1 - 属于侮辱类
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-12
"""</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">classifyNB</span><span class="hljs-params">(vec2Classify, p0Vec, p1Vec, pClass1)</span>:</span>
    p1 = sum(vec2Classify * p1Vec) + np.log(pClass1)        <span class="hljs-comment">#对应元素相乘。logA * B = logA + logB，所以这里加上log(pClass1)</span>
    p0 = sum(vec2Classify * p0Vec) + np.log(<span class="hljs-number">1.0</span> - pClass1)
    <span class="hljs-keyword">if</span> p1 &gt; p0:
        <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span></code></pre> 
<p>为啥这么改？因为取自然对数了。logab = loga + logb。</p> 
<p>这样，我们的朴素贝叶斯分类器就改进完毕了。</p> 
<hr> 
<h2 id="三-朴素贝叶斯之过滤垃圾邮件">三 朴素贝叶斯之过滤垃圾邮件</h2> 
<p>在上篇文章那个简单的例子中，我们引入了字符串列表。使用朴素贝叶斯解决一些现实生活中的问题时，需要先从文本内容得到字符串列表，然后生成词向量。下面这个例子中，我们将了解朴素贝叶斯的一个最著名的应用：电子邮件垃圾过滤。首先看一下使用朴素贝叶斯对电子邮件进行分类的步骤：</p> 
<ul><li>收集数据：提供文本文件。</li><li>准备数据：将文本文件解析成词条向量。</li><li>分析数据：检查词条确保解析的正确性。</li><li>训练算法：使用我们之前建立的trainNB0()函数。</li><li>测试算法：使用classifyNB()，并构建一个新的测试函数来计算文档集的错误率。</li><li>使用算法：构建一个完整的程序对一组文档进行分类，将错分的文档输出到屏幕上。</li></ul> 
<h3 id="1-收集数据">1 收集数据</h3> 
<p>数据我已经为大家准备好了，可以在我的Github上下载：<a href="https://github.com/Jack-Cherish/Machine-Learning/tree/master/Naive%20Bayes/email" target="_blank" rel="noopener noreferrer">https://github.com/Jack-Cherish/Machine-Learning/tree/master/Naive%20Bayes/email</a></p> 
<p>有两个文件夹ham和spam，spam文件下的txt文件为垃圾邮件。</p> 
<h3 id="2-准备数据">2 准备数据</h3> 
<p>对于英文文本，我们可以以非字母、非数字作为符号进行切分，使用split函数即可。编写代码如下：</p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-comment"># -*- coding: UTF-8 -*-</span>
<span class="hljs-keyword">import</span> re

<span class="hljs-string">"""
函数说明:接收一个大字符串并将其解析为字符串列表

Parameters:
    无
Returns:
    无
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-14
"""</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">textParse</span><span class="hljs-params">(bigString)</span>:</span>                                                   <span class="hljs-comment">#将字符串转换为字符列表</span>
    listOfTokens = re.split(<span class="hljs-string">r'\W*'</span>, bigString)                              <span class="hljs-comment">#将特殊符号作为切分标志进行字符串切分，即非字母、非数字</span>
    <span class="hljs-keyword">return</span> [tok.lower() <span class="hljs-keyword">for</span> tok <span class="hljs-keyword">in</span> listOfTokens <span class="hljs-keyword">if</span> len(tok) &gt; <span class="hljs-number">2</span>]            <span class="hljs-comment">#除了单个字母，例如大写的I，其它单词变成小写</span>

<span class="hljs-string">"""
函数说明:将切分的实验样本词条整理成不重复的词条列表，也就是词汇表

Parameters:
    dataSet - 整理的样本数据集
Returns:
    vocabSet - 返回不重复的词条列表，也就是词汇表
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-11
"""</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">createVocabList</span><span class="hljs-params">(dataSet)</span>:</span>
    vocabSet = set([])                      <span class="hljs-comment">#创建一个空的不重复列表</span>
    <span class="hljs-keyword">for</span> document <span class="hljs-keyword">in</span> dataSet:               
        vocabSet = vocabSet | set(document) <span class="hljs-comment">#取并集</span>
    <span class="hljs-keyword">return</span> list(vocabSet)

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
    docList = []; classList = []
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, <span class="hljs-number">26</span>):                                                  <span class="hljs-comment">#遍历25个txt文件</span>
        wordList = textParse(open(<span class="hljs-string">'email/spam/%d.txt'</span> % i, <span class="hljs-string">'r'</span>).read())     <span class="hljs-comment">#读取每个垃圾邮件，并字符串转换成字符串列表</span>
        docList.append(wordList)
        classList.append(<span class="hljs-number">1</span>)                                                 <span class="hljs-comment">#标记垃圾邮件，1表示垃圾文件</span>
        wordList = textParse(open(<span class="hljs-string">'email/ham/%d.txt'</span> % i, <span class="hljs-string">'r'</span>).read())      <span class="hljs-comment">#读取每个非垃圾邮件，并字符串转换成字符串列表</span>
        docList.append(wordList)
        classList.append(<span class="hljs-number">0</span>)                                                 <span class="hljs-comment">#标记非垃圾邮件，1表示垃圾文件   </span>
    vocabList = createVocabList(docList)                                    <span class="hljs-comment">#创建词汇表，不重复</span>
    print(vocabList)</code></pre> 
<p>这样我们就得到了词汇表，结果如下图所示：</p> 
<p></p> 
<p></p> 
<div align="center"> 
 <img src="https://images2.imgbox.com/9e/51/VX8YkNMc_o.png"> 
</div> 
<p></p> 
<p></p> 
<p>根据词汇表，我们就可以将每个文本向量化。我们将数据集分为训练集和测试集，使用交叉验证的方式测试朴素贝叶斯分类器的准确性。编写代码如下：</p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-comment"># -*- coding: UTF-8 -*-</span>
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> random
<span class="hljs-keyword">import</span> re

<span class="hljs-string">"""
函数说明:将切分的实验样本词条整理成不重复的词条列表，也就是词汇表

Parameters:
    dataSet - 整理的样本数据集
Returns:
    vocabSet - 返回不重复的词条列表，也就是词汇表
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-11
"""</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">createVocabList</span><span class="hljs-params">(dataSet)</span>:</span>
    vocabSet = set([])                      <span class="hljs-comment">#创建一个空的不重复列表</span>
    <span class="hljs-keyword">for</span> document <span class="hljs-keyword">in</span> dataSet:               
        vocabSet = vocabSet | set(document) <span class="hljs-comment">#取并集</span>
    <span class="hljs-keyword">return</span> list(vocabSet)

<span class="hljs-string">"""
函数说明:根据vocabList词汇表，将inputSet向量化，向量的每个元素为1或0

Parameters:
    vocabList - createVocabList返回的列表
    inputSet - 切分的词条列表
Returns:
    returnVec - 文档向量,词集模型
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-11
"""</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">setOfWords2Vec</span><span class="hljs-params">(vocabList, inputSet)</span>:</span>
    returnVec = [<span class="hljs-number">0</span>] * len(vocabList)                                    <span class="hljs-comment">#创建一个其中所含元素都为0的向量</span>
    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> inputSet:                                                <span class="hljs-comment">#遍历每个词条</span>
        <span class="hljs-keyword">if</span> word <span class="hljs-keyword">in</span> vocabList:                                            <span class="hljs-comment">#如果词条存在于词汇表中，则置1</span>
            returnVec[vocabList.index(word)] = <span class="hljs-number">1</span>
        <span class="hljs-keyword">else</span>: print(<span class="hljs-string">"the word: %s is not in my Vocabulary!"</span> % word)
    <span class="hljs-keyword">return</span> returnVec                                                    <span class="hljs-comment">#返回文档向量</span>


<span class="hljs-string">"""
函数说明:根据vocabList词汇表，构建词袋模型

Parameters:
    vocabList - createVocabList返回的列表
    inputSet - 切分的词条列表
Returns:
    returnVec - 文档向量,词袋模型
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-14
"""</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bagOfWords2VecMN</span><span class="hljs-params">(vocabList, inputSet)</span>:</span>
    returnVec = [<span class="hljs-number">0</span>]*len(vocabList)                                        <span class="hljs-comment">#创建一个其中所含元素都为0的向量</span>
    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> inputSet:                                                <span class="hljs-comment">#遍历每个词条</span>
        <span class="hljs-keyword">if</span> word <span class="hljs-keyword">in</span> vocabList:                                            <span class="hljs-comment">#如果词条存在于词汇表中，则计数加一</span>
            returnVec[vocabList.index(word)] += <span class="hljs-number">1</span>
    <span class="hljs-keyword">return</span> returnVec                                                    <span class="hljs-comment">#返回词袋模型</span>

<span class="hljs-string">"""
函数说明:朴素贝叶斯分类器训练函数

Parameters:
    trainMatrix - 训练文档矩阵，即setOfWords2Vec返回的returnVec构成的矩阵
    trainCategory - 训练类别标签向量，即loadDataSet返回的classVec
Returns:
    p0Vect - 侮辱类的条件概率数组
    p1Vect - 非侮辱类的条件概率数组
    pAbusive - 文档属于侮辱类的概率
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-12
"""</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">trainNB0</span><span class="hljs-params">(trainMatrix,trainCategory)</span>:</span>
    numTrainDocs = len(trainMatrix)                            <span class="hljs-comment">#计算训练的文档数目</span>
    numWords = len(trainMatrix[<span class="hljs-number">0</span>])                            <span class="hljs-comment">#计算每篇文档的词条数</span>
    pAbusive = sum(trainCategory)/float(numTrainDocs)        <span class="hljs-comment">#文档属于侮辱类的概率</span>
    p0Num = np.ones(numWords); p1Num = np.ones(numWords)    <span class="hljs-comment">#创建numpy.ones数组,词条出现数初始化为1，拉普拉斯平滑</span>
    p0Denom = <span class="hljs-number">2.0</span>; p1Denom = <span class="hljs-number">2.0</span>                            <span class="hljs-comment">#分母初始化为2,拉普拉斯平滑</span>
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(numTrainDocs):
        <span class="hljs-keyword">if</span> trainCategory[i] == <span class="hljs-number">1</span>:                            <span class="hljs-comment">#统计属于侮辱类的条件概率所需的数据，即P(w0|1),P(w1|1),P(w2|1)···</span>
            p1Num += trainMatrix[i]
            p1Denom += sum(trainMatrix[i])
        <span class="hljs-keyword">else</span>:                                                <span class="hljs-comment">#统计属于非侮辱类的条件概率所需的数据，即P(w0|0),P(w1|0),P(w2|0)···</span>
            p0Num += trainMatrix[i]
            p0Denom += sum(trainMatrix[i])
    p1Vect = np.log(p1Num/p1Denom)                            <span class="hljs-comment">#取对数，防止下溢出         </span>
    p0Vect = np.log(p0Num/p0Denom)         
    <span class="hljs-keyword">return</span> p0Vect,p1Vect,pAbusive                            <span class="hljs-comment">#返回属于侮辱类的条件概率数组，属于非侮辱类的条件概率数组，文档属于侮辱类的概率</span>

<span class="hljs-string">"""
函数说明:朴素贝叶斯分类器分类函数

Parameters:
    vec2Classify - 待分类的词条数组
    p0Vec - 侮辱类的条件概率数组
    p1Vec -非侮辱类的条件概率数组
    pClass1 - 文档属于侮辱类的概率
Returns:
    0 - 属于非侮辱类
    1 - 属于侮辱类
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-12
"""</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">classifyNB</span><span class="hljs-params">(vec2Classify, p0Vec, p1Vec, pClass1)</span>:</span>
    p1 = sum(vec2Classify * p1Vec) + np.log(pClass1)        <span class="hljs-comment">#对应元素相乘。logA * B = logA + logB，所以这里加上log(pClass1)</span>
    p0 = sum(vec2Classify * p0Vec) + np.log(<span class="hljs-number">1.0</span> - pClass1)
    <span class="hljs-keyword">if</span> p1 &gt; p0:
        <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>

<span class="hljs-string">"""
函数说明:朴素贝叶斯分类器训练函数

Parameters:
    trainMatrix - 训练文档矩阵，即setOfWords2Vec返回的returnVec构成的矩阵
    trainCategory - 训练类别标签向量，即loadDataSet返回的classVec
Returns:
    p0Vect - 侮辱类的条件概率数组
    p1Vect - 非侮辱类的条件概率数组
    pAbusive - 文档属于侮辱类的概率
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-12
"""</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">trainNB0</span><span class="hljs-params">(trainMatrix,trainCategory)</span>:</span>
    numTrainDocs = len(trainMatrix)                            <span class="hljs-comment">#计算训练的文档数目</span>
    numWords = len(trainMatrix[<span class="hljs-number">0</span>])                            <span class="hljs-comment">#计算每篇文档的词条数</span>
    pAbusive = sum(trainCategory)/float(numTrainDocs)        <span class="hljs-comment">#文档属于侮辱类的概率</span>
    p0Num = np.ones(numWords); p1Num = np.ones(numWords)    <span class="hljs-comment">#创建numpy.ones数组,词条出现数初始化为1，拉普拉斯平滑</span>
    p0Denom = <span class="hljs-number">2.0</span>; p1Denom = <span class="hljs-number">2.0</span>                            <span class="hljs-comment">#分母初始化为2,拉普拉斯平滑</span>
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(numTrainDocs):
        <span class="hljs-keyword">if</span> trainCategory[i] == <span class="hljs-number">1</span>:                            <span class="hljs-comment">#统计属于侮辱类的条件概率所需的数据，即P(w0|1),P(w1|1),P(w2|1)···</span>
            p1Num += trainMatrix[i]
            p1Denom += sum(trainMatrix[i])
        <span class="hljs-keyword">else</span>:                                                <span class="hljs-comment">#统计属于非侮辱类的条件概率所需的数据，即P(w0|0),P(w1|0),P(w2|0)···</span>
            p0Num += trainMatrix[i]
            p0Denom += sum(trainMatrix[i])
    p1Vect = np.log(p1Num/p1Denom)                            <span class="hljs-comment">#取对数，防止下溢出         </span>
    p0Vect = np.log(p0Num/p0Denom)         
    <span class="hljs-keyword">return</span> p0Vect,p1Vect,pAbusive                            <span class="hljs-comment">#返回属于侮辱类的条件概率数组，属于非侮辱类的条件概率数组，文档属于侮辱类的概率</span>


<span class="hljs-string">"""
函数说明:接收一个大字符串并将其解析为字符串列表

Parameters:
    无
Returns:
    无
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-14
"""</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">textParse</span><span class="hljs-params">(bigString)</span>:</span>                                                   <span class="hljs-comment">#将字符串转换为字符列表</span>
    listOfTokens = re.split(<span class="hljs-string">r'\W*'</span>, bigString)                              <span class="hljs-comment">#将特殊符号作为切分标志进行字符串切分，即非字母、非数字</span>
    <span class="hljs-keyword">return</span> [tok.lower() <span class="hljs-keyword">for</span> tok <span class="hljs-keyword">in</span> listOfTokens <span class="hljs-keyword">if</span> len(tok) &gt; <span class="hljs-number">2</span>]            <span class="hljs-comment">#除了单个字母，例如大写的I，其它单词变成小写</span>

<span class="hljs-string">"""
函数说明:测试朴素贝叶斯分类器

Parameters:
    无
Returns:
    无
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-14
"""</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">spamTest</span><span class="hljs-params">()</span>:</span>
    docList = []; classList = []; fullText = []
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, <span class="hljs-number">26</span>):                                                  <span class="hljs-comment">#遍历25个txt文件</span>
        wordList = textParse(open(<span class="hljs-string">'email/spam/%d.txt'</span> % i, <span class="hljs-string">'r'</span>).read())     <span class="hljs-comment">#读取每个垃圾邮件，并字符串转换成字符串列表</span>
        docList.append(wordList)
        fullText.append(wordList)
        classList.append(<span class="hljs-number">1</span>)                                                 <span class="hljs-comment">#标记垃圾邮件，1表示垃圾文件</span>
        wordList = textParse(open(<span class="hljs-string">'email/ham/%d.txt'</span> % i, <span class="hljs-string">'r'</span>).read())      <span class="hljs-comment">#读取每个非垃圾邮件，并字符串转换成字符串列表</span>
        docList.append(wordList)
        fullText.append(wordList)
        classList.append(<span class="hljs-number">0</span>)                                                 <span class="hljs-comment">#标记非垃圾邮件，1表示垃圾文件   </span>
    vocabList = createVocabList(docList)                                    <span class="hljs-comment">#创建词汇表，不重复</span>
    trainingSet = list(range(<span class="hljs-number">50</span>)); testSet = []                             <span class="hljs-comment">#创建存储训练集的索引值的列表和测试集的索引值的列表                       </span>
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>):                                                     <span class="hljs-comment">#从50个邮件中，随机挑选出40个作为训练集,10个做测试集</span>
        randIndex = int(random.uniform(<span class="hljs-number">0</span>, len(trainingSet)))                <span class="hljs-comment">#随机选取索索引值</span>
        testSet.append(trainingSet[randIndex])                              <span class="hljs-comment">#添加测试集的索引值</span>
        <span class="hljs-keyword">del</span>(trainingSet[randIndex])                                         <span class="hljs-comment">#在训练集列表中删除添加到测试集的索引值</span>
    trainMat = []; trainClasses = []                                        <span class="hljs-comment">#创建训练集矩阵和训练集类别标签系向量             </span>
    <span class="hljs-keyword">for</span> docIndex <span class="hljs-keyword">in</span> trainingSet:                                            <span class="hljs-comment">#遍历训练集</span>
        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))       <span class="hljs-comment">#将生成的词集模型添加到训练矩阵中</span>
        trainClasses.append(classList[docIndex])                            <span class="hljs-comment">#将类别添加到训练集类别标签系向量中</span>
    p0V, p1V, pSpam = trainNB0(np.array(trainMat), np.array(trainClasses))  <span class="hljs-comment">#训练朴素贝叶斯模型</span>
    errorCount = <span class="hljs-number">0</span>                                                          <span class="hljs-comment">#错误分类计数</span>
    <span class="hljs-keyword">for</span> docIndex <span class="hljs-keyword">in</span> testSet:                                                <span class="hljs-comment">#遍历测试集</span>
        wordVector = setOfWords2Vec(vocabList, docList[docIndex])           <span class="hljs-comment">#测试集的词集模型</span>
        <span class="hljs-keyword">if</span> classifyNB(np.array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:    <span class="hljs-comment">#如果分类错误</span>
            errorCount += <span class="hljs-number">1</span>                                                 <span class="hljs-comment">#错误计数加1</span>
            print(<span class="hljs-string">"分类错误的测试集："</span>,docList[docIndex])
    print(<span class="hljs-string">'错误率：%.2f%%'</span> % (float(errorCount) / len(testSet) * <span class="hljs-number">100</span>))


<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
    spamTest()</code></pre> 
<p>运行结果如下：</p> 
<p></p> 
<p></p> 
<div align="center"> 
 <img src="https://images2.imgbox.com/69/59/tgJSgpjv_o.png"> 
</div> 
<p></p> 
<p></p> 
<p>函数spamTest()会输出在10封随机选择的电子邮件上的分类错误概率。既然这些电子邮件是随机选择的，所以每次的输出结果可能有些差别。如果发现错误的话，函数会输出错误的文档的此表，这样就可以了解到底是哪篇文档发生了错误。如果想要更好地估计错误率，那么就应该将上述过程重复多次，比如说10次，然后求平均值。相比之下，将垃圾邮件误判为正常邮件要比将正常邮件归为垃圾邮件好。为了避免错误，有多种方式可以用来修正分类器，这些内容会在后续文章中进行讨论。</p> 
<p>这部分代码获取：<a href="https://github.com/Jack-Cherish/Machine-Learning/blob/master/Naive%20Bayes/bayes-modify.py" target="_blank" rel="noopener noreferrer">https://github.com/Jack-Cherish/Machine-Learning/blob/master/Naive%20Bayes/bayes-modify.py</a></p> 
<hr> 
<h2 id="四-朴素贝叶斯之新浪新闻分类sklearn">四 朴素贝叶斯之新浪新闻分类(Sklearn)</h2> 
<h3 id="1-中文语句切分">1 中文语句切分</h3> 
<p>考虑一个问题，英文的语句可以通过非字母和非数字进行切分，但是汉语句子呢？就比如我打的这一堆字，该如何进行切分呢？我们自己写个规则？</p> 
<p>幸运地是，这部分的工作不需要我们自己做了，可以直接使用第三方分词组件，即jieba，没错就是”结巴”。</p> 
<p>jieba已经兼容Python2和Python3，使用如下指令直接安装即可：</p> 
<pre class="prettyprint"><code class=" hljs cmake">pip3 <span class="hljs-keyword">install</span> jieba</code></pre> 
<p>Python中文分词组件使用简单：</p> 
<ul><li>官方教程：<a href="https://github.com/fxsjy/jieba" target="_blank" rel="noopener noreferrer">https://github.com/fxsjy/jieba</a></li><li>民间教程：<a href="https://www.oschina.net/p/jieba" rel="nofollow noopener noreferrer" target="_blank">https://www.oschina.net/p/jieba</a></li></ul> 
<p>新闻分类数据集我也已经准备好，可以到我的Github进行下载：<a href="https://github.com/Jack-Cherish/Machine-Learning/tree/master/Naive%20Bayes/SogouC" target="_blank" rel="noopener noreferrer">https://github.com/Jack-Cherish/Machine-Learning/tree/master/Naive%20Bayes/SogouC</a></p> 
<p>数据集已经做好分类，分文件夹保存，分类结果如下：</p> 
<p></p> 
<p></p> 
<div align="center"> 
 <img src="https://images2.imgbox.com/33/35/wznLNn9H_o.png"> 
</div> 
<p></p> 
<p></p> 
<p>数据集已经准备好，接下来，让我们直接进入正题。切分中文语句，编写如下代码：</p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-comment"># -*- coding: UTF-8 -*-</span>
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> jieba

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">TextProcessing</span><span class="hljs-params">(folder_path)</span>:</span>
    folder_list = os.listdir(folder_path)                        <span class="hljs-comment">#查看folder_path下的文件</span>
    data_list = []                                                <span class="hljs-comment">#训练集</span>
    class_list = []

    <span class="hljs-comment">#遍历每个子文件夹</span>
    <span class="hljs-keyword">for</span> folder <span class="hljs-keyword">in</span> folder_list:
        new_folder_path = os.path.join(folder_path, folder)        <span class="hljs-comment">#根据子文件夹，生成新的路径</span>
        files = os.listdir(new_folder_path)                        <span class="hljs-comment">#存放子文件夹下的txt文件的列表</span>

        j = <span class="hljs-number">1</span>
        <span class="hljs-comment">#遍历每个txt文件</span>
        <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> files:
            <span class="hljs-keyword">if</span> j &gt; <span class="hljs-number">100</span>:                                            <span class="hljs-comment">#每类txt样本数最多100个</span>
                <span class="hljs-keyword">break</span>
            <span class="hljs-keyword">with</span> open(os.path.join(new_folder_path, file), <span class="hljs-string">'r'</span>, encoding = <span class="hljs-string">'utf-8'</span>) <span class="hljs-keyword">as</span> f:    <span class="hljs-comment">#打开txt文件</span>
                raw = f.read()

            word_cut = jieba.cut(raw, cut_all = <span class="hljs-keyword">False</span>)            <span class="hljs-comment">#精简模式，返回一个可迭代的generator</span>
            word_list = list(word_cut)                            <span class="hljs-comment">#generator转换为list</span>

            data_list.append(word_list)
            class_list.append(folder)
            j += <span class="hljs-number">1</span>
        print(data_list)
        print(class_list)
<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
    <span class="hljs-comment">#文本预处理</span>
    folder_path = <span class="hljs-string">'./SogouC/Sample'</span>                <span class="hljs-comment">#训练集存放地址</span>
    TextProcessing(folder_path)</code></pre> 
<p>代码运行结果如下所示，可以看到，我们已经顺利将每个文本进行切分，并进行了类别标记。</p> 
<p></p> 
<p></p> 
<div align="center"> 
 <img src="https://images2.imgbox.com/95/eb/qPoexo4X_o.png"> 
</div> 
<p></p> 
<p></p> 
<h3 id="2-文本特征选择">2 文本特征选择</h3> 
<p>我们将所有文本分成训练集和测试集，并对训练集中的所有单词进行词频统计，并按降序排序。也就是将出现次数多的词语在前，出现次数少的词语在后进行排序。编写代码如下：</p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-comment"># -*- coding: UTF-8 -*-</span>
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> random
<span class="hljs-keyword">import</span> jieba

<span class="hljs-string">"""
函数说明:中文文本处理

Parameters:
    folder_path - 文本存放的路径
    test_size - 测试集占比，默认占所有数据集的百分之20
Returns:
    all_words_list - 按词频降序排序的训练集列表
    train_data_list - 训练集列表
    test_data_list - 测试集列表
    train_class_list - 训练集标签列表
    test_class_list - 测试集标签列表
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-22
"""</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">TextProcessing</span><span class="hljs-params">(folder_path, test_size = <span class="hljs-number">0.2</span>)</span>:</span>
    folder_list = os.listdir(folder_path)                        <span class="hljs-comment">#查看folder_path下的文件</span>
    data_list = []                                                <span class="hljs-comment">#数据集数据</span>
    class_list = []                                                <span class="hljs-comment">#数据集类别</span>

    <span class="hljs-comment">#遍历每个子文件夹</span>
    <span class="hljs-keyword">for</span> folder <span class="hljs-keyword">in</span> folder_list:
        new_folder_path = os.path.join(folder_path, folder)        <span class="hljs-comment">#根据子文件夹，生成新的路径</span>
        files = os.listdir(new_folder_path)                        <span class="hljs-comment">#存放子文件夹下的txt文件的列表</span>

        j = <span class="hljs-number">1</span>
        <span class="hljs-comment">#遍历每个txt文件</span>
        <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> files:
            <span class="hljs-keyword">if</span> j &gt; <span class="hljs-number">100</span>:                                            <span class="hljs-comment">#每类txt样本数最多100个</span>
                <span class="hljs-keyword">break</span>
            <span class="hljs-keyword">with</span> open(os.path.join(new_folder_path, file), <span class="hljs-string">'r'</span>, encoding = <span class="hljs-string">'utf-8'</span>) <span class="hljs-keyword">as</span> f:    <span class="hljs-comment">#打开txt文件</span>
                raw = f.read()

            word_cut = jieba.cut(raw, cut_all = <span class="hljs-keyword">False</span>)            <span class="hljs-comment">#精简模式，返回一个可迭代的generator</span>
            word_list = list(word_cut)                            <span class="hljs-comment">#generator转换为list</span>

            data_list.append(word_list)                            <span class="hljs-comment">#添加数据集数据</span>
            class_list.append(folder)                            <span class="hljs-comment">#添加数据集类别</span>
            j += <span class="hljs-number">1</span>

    data_class_list = list(zip(data_list, class_list))            <span class="hljs-comment">#zip压缩合并，将数据与标签对应压缩</span>
    random.shuffle(data_class_list)                                <span class="hljs-comment">#将data_class_list乱序</span>
    index = int(len(data_class_list) * test_size) + <span class="hljs-number">1</span>            <span class="hljs-comment">#训练集和测试集切分的索引值</span>
    train_list = data_class_list[index:]                        <span class="hljs-comment">#训练集</span>
    test_list = data_class_list[:index]                            <span class="hljs-comment">#测试集</span>
    train_data_list, train_class_list = zip(*train_list)        <span class="hljs-comment">#训练集解压缩</span>
    test_data_list, test_class_list = zip(*test_list)            <span class="hljs-comment">#测试集解压缩</span>

    all_words_dict = {}                                            <span class="hljs-comment">#统计训练集词频</span>
    <span class="hljs-keyword">for</span> word_list <span class="hljs-keyword">in</span> train_data_list:
        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> word_list:
            <span class="hljs-keyword">if</span> word <span class="hljs-keyword">in</span> all_words_dict.keys():
                all_words_dict[word] += <span class="hljs-number">1</span>
            <span class="hljs-keyword">else</span>:
                all_words_dict[word] = <span class="hljs-number">1</span>

    <span class="hljs-comment">#根据键的值倒序排序</span>
    all_words_tuple_list = sorted(all_words_dict.items(), key = <span class="hljs-keyword">lambda</span> f:f[<span class="hljs-number">1</span>], reverse = <span class="hljs-keyword">True</span>)
    all_words_list, all_words_nums = zip(*all_words_tuple_list)    <span class="hljs-comment">#解压缩</span>
    all_words_list = list(all_words_list)                        <span class="hljs-comment">#转换成列表</span>
    <span class="hljs-keyword">return</span> all_words_list, train_data_list, test_data_list, train_class_list, test_class_list

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
    <span class="hljs-comment">#文本预处理</span>
    folder_path = <span class="hljs-string">'./SogouC/Sample'</span>                <span class="hljs-comment">#训练集存放地址</span>
    all_words_list, train_data_list, test_data_list, train_class_list, test_class_list = TextProcessing(folder_path, test_size=<span class="hljs-number">0.2</span>)
    print(all_words_list)</code></pre> 
<p>all_words_list就是将所有训练集的切分结果通过词频降序排列构成的单词合集。观察一下打印结果，不难发现，这里包含了很多标点符号，很显然，这些标点符号是不能作为新闻分类的特征的。总不能说，应为这个文章逗号多，所以它是xx类新闻吧？为了降低这些高频的符号对分类结果的影响，我们应该怎么做呢？答曰：抛弃他们！ 除了这些，还有”在”，”了”这样对新闻分类无关痛痒的词。并且还有一些数字，数字显然也不能作为分类新闻的特征。所以要消除它们对分类结果的影响，我们可以定制一个规则。</p> 
<p></p> 
<p></p> 
<div align="center"> 
 <img src="https://images2.imgbox.com/64/a7/K5wN965U_o.png"> 
</div> 
<p></p> 
<p></p> 
<p>一个简单的规则可以这样制定：首先去掉高频词，至于去掉多少个高频词，我们可以通过观察去掉高频词个数和最终检测准确率的关系来确定。除此之外，去除数字，不把数字作为分类特征。同时，去除一些特定的词语，比如：”的”，”一”，”在”，”不”，”当然”,”怎么”这类的对新闻分类无影响的介词、代词、连词。怎么去除这些词呢？可以使用已经整理好的stopwords_cn.txt文本。下载地址：<a href="https://github.com/Jack-Cherish/Machine-Learning/blob/master/Naive%20Bayes/stopwords_cn.txt" target="_blank" rel="noopener noreferrer">https://github.com/Jack-Cherish/Machine-Learning/blob/master/Naive%20Bayes/stopwords_cn.txt</a></p> 
<p>这个文件是这个样子的：</p> 
<p></p> 
<p></p> 
<div align="center"> 
 <img src="https://images2.imgbox.com/1e/6a/mixafh6b_o.png"> 
</div> 
<p></p> 
<p></p> 
<p>所以我们可以根据这个文档，将这些单词去除，不作为分类的特征。我们先去除前100个高频词汇，然后编写代码如下：</p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-comment"># -*- coding: UTF-8 -*-</span>
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> random
<span class="hljs-keyword">import</span> jieba

<span class="hljs-string">"""
函数说明:中文文本处理

Parameters:
    folder_path - 文本存放的路径
    test_size - 测试集占比，默认占所有数据集的百分之20
Returns:
    all_words_list - 按词频降序排序的训练集列表
    train_data_list - 训练集列表
    test_data_list - 测试集列表
    train_class_list - 训练集标签列表
    test_class_list - 测试集标签列表
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-22
"""</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">TextProcessing</span><span class="hljs-params">(folder_path, test_size = <span class="hljs-number">0.2</span>)</span>:</span>
    folder_list = os.listdir(folder_path)                        <span class="hljs-comment">#查看folder_path下的文件</span>
    data_list = []                                                <span class="hljs-comment">#数据集数据</span>
    class_list = []                                                <span class="hljs-comment">#数据集类别</span>

    <span class="hljs-comment">#遍历每个子文件夹</span>
    <span class="hljs-keyword">for</span> folder <span class="hljs-keyword">in</span> folder_list:
        new_folder_path = os.path.join(folder_path, folder)        <span class="hljs-comment">#根据子文件夹，生成新的路径</span>
        files = os.listdir(new_folder_path)                        <span class="hljs-comment">#存放子文件夹下的txt文件的列表</span>

        j = <span class="hljs-number">1</span>
        <span class="hljs-comment">#遍历每个txt文件</span>
        <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> files:
            <span class="hljs-keyword">if</span> j &gt; <span class="hljs-number">100</span>:                                            <span class="hljs-comment">#每类txt样本数最多100个</span>
                <span class="hljs-keyword">break</span>
            <span class="hljs-keyword">with</span> open(os.path.join(new_folder_path, file), <span class="hljs-string">'r'</span>, encoding = <span class="hljs-string">'utf-8'</span>) <span class="hljs-keyword">as</span> f:    <span class="hljs-comment">#打开txt文件</span>
                raw = f.read()

            word_cut = jieba.cut(raw, cut_all = <span class="hljs-keyword">False</span>)            <span class="hljs-comment">#精简模式，返回一个可迭代的generator</span>
            word_list = list(word_cut)                            <span class="hljs-comment">#generator转换为list</span>

            data_list.append(word_list)                            <span class="hljs-comment">#添加数据集数据</span>
            class_list.append(folder)                            <span class="hljs-comment">#添加数据集类别</span>
            j += <span class="hljs-number">1</span>

    data_class_list = list(zip(data_list, class_list))            <span class="hljs-comment">#zip压缩合并，将数据与标签对应压缩</span>
    random.shuffle(data_class_list)                                <span class="hljs-comment">#将data_class_list乱序</span>
    index = int(len(data_class_list) * test_size) + <span class="hljs-number">1</span>            <span class="hljs-comment">#训练集和测试集切分的索引值</span>
    train_list = data_class_list[index:]                        <span class="hljs-comment">#训练集</span>
    test_list = data_class_list[:index]                            <span class="hljs-comment">#测试集</span>
    train_data_list, train_class_list = zip(*train_list)        <span class="hljs-comment">#训练集解压缩</span>
    test_data_list, test_class_list = zip(*test_list)            <span class="hljs-comment">#测试集解压缩</span>

    all_words_dict = {}                                            <span class="hljs-comment">#统计训练集词频</span>
    <span class="hljs-keyword">for</span> word_list <span class="hljs-keyword">in</span> train_data_list:
        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> word_list:
            <span class="hljs-keyword">if</span> word <span class="hljs-keyword">in</span> all_words_dict.keys():
                all_words_dict[word] += <span class="hljs-number">1</span>
            <span class="hljs-keyword">else</span>:
                all_words_dict[word] = <span class="hljs-number">1</span>

    <span class="hljs-comment">#根据键的值倒序排序</span>
    all_words_tuple_list = sorted(all_words_dict.items(), key = <span class="hljs-keyword">lambda</span> f:f[<span class="hljs-number">1</span>], reverse = <span class="hljs-keyword">True</span>)
    all_words_list, all_words_nums = zip(*all_words_tuple_list)    <span class="hljs-comment">#解压缩</span>
    all_words_list = list(all_words_list)                        <span class="hljs-comment">#转换成列表</span>
    <span class="hljs-keyword">return</span> all_words_list, train_data_list, test_data_list, train_class_list, test_class_list

<span class="hljs-string">"""
函数说明:读取文件里的内容，并去重

Parameters:
    words_file - 文件路径
Returns:
    words_set - 读取的内容的set集合
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-22
"""</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">MakeWordsSet</span><span class="hljs-params">(words_file)</span>:</span>
    words_set = set()                                            <span class="hljs-comment">#创建set集合</span>
    <span class="hljs-keyword">with</span> open(words_file, <span class="hljs-string">'r'</span>, encoding = <span class="hljs-string">'utf-8'</span>) <span class="hljs-keyword">as</span> f:        <span class="hljs-comment">#打开文件</span>
        <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f.readlines():                                <span class="hljs-comment">#一行一行读取</span>
            word = line.strip()                                    <span class="hljs-comment">#去回车</span>
            <span class="hljs-keyword">if</span> len(word) &gt; <span class="hljs-number">0</span>:                                    <span class="hljs-comment">#有文本，则添加到words_set中</span>
                words_set.add(word)                               
    <span class="hljs-keyword">return</span> words_set                                             <span class="hljs-comment">#返回处理结果</span>

<span class="hljs-string">"""
函数说明:文本特征选取

Parameters:
    all_words_list - 训练集所有文本列表
    deleteN - 删除词频最高的deleteN个词
    stopwords_set - 指定的结束语
Returns:
    feature_words - 特征集
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-22
"""</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">words_dict</span><span class="hljs-params">(all_words_list, deleteN, stopwords_set = set<span class="hljs-params">()</span>)</span>:</span>
    feature_words = []                            <span class="hljs-comment">#特征列表</span>
    n = <span class="hljs-number">1</span>
    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range(deleteN, len(all_words_list), <span class="hljs-number">1</span>):
        <span class="hljs-keyword">if</span> n &gt; <span class="hljs-number">1000</span>:                            <span class="hljs-comment">#feature_words的维度为1000</span>
            <span class="hljs-keyword">break</span>                               
        <span class="hljs-comment">#如果这个词不是数字，并且不是指定的结束语，并且单词长度大于1小于5，那么这个词就可以作为特征词</span>
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> all_words_list[t].isdigit() <span class="hljs-keyword">and</span> all_words_list[t] <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> stopwords_set <span class="hljs-keyword">and</span> <span class="hljs-number">1</span> &lt; len(all_words_list[t]) &lt; <span class="hljs-number">5</span>:
            feature_words.append(all_words_list[t])
        n += <span class="hljs-number">1</span>
    <span class="hljs-keyword">return</span> feature_words

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
    <span class="hljs-comment">#文本预处理</span>
    folder_path = <span class="hljs-string">'./SogouC/Sample'</span>                <span class="hljs-comment">#训练集存放地址</span>
    all_words_list, train_data_list, test_data_list, train_class_list, test_class_list = TextProcessing(folder_path, test_size=<span class="hljs-number">0.2</span>)

    <span class="hljs-comment">#生成stopwords_set</span>
    stopwords_file = <span class="hljs-string">'./stopwords_cn.txt'</span>
    stopwords_set = MakeWordsSet(stopwords_file)

    feature_words = words_dict(all_words_list, <span class="hljs-number">100</span>, stopwords_set)
    print(feature_words)</code></pre> 
<p>运行结果如下：</p> 
<p></p> 
<p></p> 
<div align="center"> 
 <img src="https://images2.imgbox.com/0a/76/8Alux0cy_o.png"> 
</div> 
<p></p> 
<p></p> 
<p>可以看到，我们已经滤除了那些没有用的词组，这个feature_words就是我们最终选出的用于新闻分类的特征。随后，我们就可以根据feature_words，将文本向量化，然后用于训练朴素贝叶斯分类器。这个向量化的思想和第三章的思想一致，因此不再累述。</p> 
<h3 id="3-使用sklearn构建朴素贝叶斯分类器">3 使用Sklearn构建朴素贝叶斯分类器</h3> 
<p>数据已经处理好了，接下来就可以使用sklearn构建朴素贝叶斯分类器了。</p> 
<p>官方英文文档地址：<a href="http://scikit-learn.org/dev/modules/generated/sklearn.naive_bayes.MultinomialNB.html" rel="nofollow noopener noreferrer" target="_blank">http://scikit-learn.org/dev/modules/generated/sklearn.naive_bayes.MultinomialNB.html</a></p> 
<p>朴素贝叶斯是一类比较简单的算法，scikit-learn中朴素贝叶斯类库的使用也比较简单。相对于决策树，KNN之类的算法，朴素贝叶斯需要关注的参数是比较少的，这样也比较容易掌握。在scikit-learn中，一共有3个朴素贝叶斯的分类算法类。分别是GaussianNB，MultinomialNB和BernoulliNB。其中GaussianNB就是先验为高斯分布的朴素贝叶斯，MultinomialNB就是先验为多项式分布的朴素贝叶斯，而BernoulliNB就是先验为伯努利分布的朴素贝叶斯。上篇文章讲解的先验概率模型就是先验概率为多项式分布的朴素贝叶斯。</p> 
<p></p> 
<p></p> 
<div align="center"> 
 <img src="https://images2.imgbox.com/05/43/K0Vjt2TE_o.png"> 
</div> 
<p></p> 
<p></p> 
<p>对于新闻分类，属于多分类问题。我们可以使用MultinamialNB()完成我们的新闻分类问题。另外两个函数的使用暂且不再进行扩展，可以自行学习。MultinomialNB假设特征的先验概率为多项式分布，即如下式：</p> 
<p></p> 
<p></p> 
<div align="center"> 
 <img src="https://images2.imgbox.com/d3/0f/8Wt4FcBD_o.png"> 
</div> 
<p></p> 
<p></p> 
<p>其中，P(Xj = Xjl | Y = Ck)是第k个类别的第j维特征的第l个取值条件概率。mk是训练集中输出为第k类的样本个数。λ为一个大于0的常数，尝尝取值为1，即拉普拉斯平滑，也可以取其他值。</p> 
<p>接下来，我们看下MultinamialNB这个函数，只有3个参数：</p> 
<p></p> 
<p></p> 
<div align="center"> 
 <img src="https://images2.imgbox.com/a0/80/4hUkTCTc_o.png"> 
</div> 
<p></p> 
<p></p> 
<p>参数说明如下：</p> 
<ul><li>alpha：浮点型可选参数，默认为1.0，其实就是添加拉普拉斯平滑，即为上述公式中的λ ，如果这个参数设置为0，就是不添加平滑；</li><li>fit_prior：布尔型可选参数，默认为True。布尔参数fit_prior表示是否要考虑先验概率，如果是false,则所有的样本类别输出都有相同的类别先验概率。否则可以自己用第三个参数class_prior输入先验概率，或者不输入第三个参数class_prior让MultinomialNB自己从训练集样本来计算先验概率，此时的先验概率为P(Y=Ck)=mk/m。其中m为训练集样本总数量，mk为输出为第k类别的训练集样本数。</li><li>class_prior：可选参数，默认为None。</li></ul> 
<p>总结如下：</p> 
<table><thead><tr><th align="left">fit_prior</th><th align="left">class_prior</th><th align="left">最终先验概率</th></tr></thead><tbody><tr><td align="left">False</td><td align="left">填或不填没有意义</td><td align="left">P(Y = Ck) = 1 / k</td></tr><tr><td align="left">True</td><td align="left">不填</td><td align="left">P(Y = Ck) = mk / m</td></tr><tr><td align="left">True</td><td align="left">填</td><td align="left">P(Y = Ck) = class_prior</td></tr></tbody></table> 
<p>除此之外，MultinamialNB也有一些方法供我们使用：</p> 
<p></p> 
<p></p> 
<div align="center"> 
 <img src="https://images2.imgbox.com/97/a5/Dr6KzS2r_o.png"> 
</div> 
<p></p> 
<p></p> 
<p>MultinomialNB一个重要的功能是有partial_fit方法，这个方法的一般用在如果训练集数据量非常大，一次不能全部载入内存的时候。这时我们可以把训练集分成若干等分，重复调用partial_fit来一步步的学习训练集，非常方便。GaussianNB和BernoulliNB也有类似的功能。 在使用MultinomialNB的fit方法或者partial_fit方法拟合数据后，我们可以进行预测。此时预测有三种方法，包括predict，predict_log_proba和predict_proba。predict方法就是我们最常用的预测方法，直接给出测试集的预测类别输出。predict_proba则不同，它会给出测试集样本在各个类别上预测的概率。容易理解，predict_proba预测出的各个类别概率里的最大值对应的类别，也就是predict方法得到类别。predict_log_proba和predict_proba类似，它会给出测试集样本在各个类别上预测的概率的一个对数转化。转化后predict_log_proba预测出的各个类别对数概率里的最大值对应的类别，也就是predict方法得到类别。具体细节不再讲解，可参照官网手册。</p> 
<p>了解了这些，我们就可以编写代码，通过观察取不同的去掉前deleteN个高频词的个数与最终检测准确率的关系，确定deleteN的取值：</p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-comment"># -*- coding: UTF-8 -*-</span>
<span class="hljs-keyword">from</span> sklearn.naive_bayes <span class="hljs-keyword">import</span> MultinomialNB
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> random
<span class="hljs-keyword">import</span> jieba

<span class="hljs-string">"""
函数说明:中文文本处理

Parameters:
    folder_path - 文本存放的路径
    test_size - 测试集占比，默认占所有数据集的百分之20
Returns:
    all_words_list - 按词频降序排序的训练集列表
    train_data_list - 训练集列表
    test_data_list - 测试集列表
    train_class_list - 训练集标签列表
    test_class_list - 测试集标签列表
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-22
"""</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">TextProcessing</span><span class="hljs-params">(folder_path, test_size = <span class="hljs-number">0.2</span>)</span>:</span>
    folder_list = os.listdir(folder_path)                        <span class="hljs-comment">#查看folder_path下的文件</span>
    data_list = []                                                <span class="hljs-comment">#数据集数据</span>
    class_list = []                                                <span class="hljs-comment">#数据集类别</span>

    <span class="hljs-comment">#遍历每个子文件夹</span>
    <span class="hljs-keyword">for</span> folder <span class="hljs-keyword">in</span> folder_list:
        new_folder_path = os.path.join(folder_path, folder)        <span class="hljs-comment">#根据子文件夹，生成新的路径</span>
        files = os.listdir(new_folder_path)                        <span class="hljs-comment">#存放子文件夹下的txt文件的列表</span>

        j = <span class="hljs-number">1</span>
        <span class="hljs-comment">#遍历每个txt文件</span>
        <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> files:
            <span class="hljs-keyword">if</span> j &gt; <span class="hljs-number">100</span>:                                            <span class="hljs-comment">#每类txt样本数最多100个</span>
                <span class="hljs-keyword">break</span>
            <span class="hljs-keyword">with</span> open(os.path.join(new_folder_path, file), <span class="hljs-string">'r'</span>, encoding = <span class="hljs-string">'utf-8'</span>) <span class="hljs-keyword">as</span> f:    <span class="hljs-comment">#打开txt文件</span>
                raw = f.read()

            word_cut = jieba.cut(raw, cut_all = <span class="hljs-keyword">False</span>)            <span class="hljs-comment">#精简模式，返回一个可迭代的generator</span>
            word_list = list(word_cut)                            <span class="hljs-comment">#generator转换为list</span>

            data_list.append(word_list)                            <span class="hljs-comment">#添加数据集数据</span>
            class_list.append(folder)                            <span class="hljs-comment">#添加数据集类别</span>
            j += <span class="hljs-number">1</span>

    data_class_list = list(zip(data_list, class_list))            <span class="hljs-comment">#zip压缩合并，将数据与标签对应压缩</span>
    random.shuffle(data_class_list)                                <span class="hljs-comment">#将data_class_list乱序</span>
    index = int(len(data_class_list) * test_size) + <span class="hljs-number">1</span>            <span class="hljs-comment">#训练集和测试集切分的索引值</span>
    train_list = data_class_list[index:]                        <span class="hljs-comment">#训练集</span>
    test_list = data_class_list[:index]                            <span class="hljs-comment">#测试集</span>
    train_data_list, train_class_list = zip(*train_list)        <span class="hljs-comment">#训练集解压缩</span>
    test_data_list, test_class_list = zip(*test_list)            <span class="hljs-comment">#测试集解压缩</span>

    all_words_dict = {}                                            <span class="hljs-comment">#统计训练集词频</span>
    <span class="hljs-keyword">for</span> word_list <span class="hljs-keyword">in</span> train_data_list:
        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> word_list:
            <span class="hljs-keyword">if</span> word <span class="hljs-keyword">in</span> all_words_dict.keys():
                all_words_dict[word] += <span class="hljs-number">1</span>
            <span class="hljs-keyword">else</span>:
                all_words_dict[word] = <span class="hljs-number">1</span>

    <span class="hljs-comment">#根据键的值倒序排序</span>
    all_words_tuple_list = sorted(all_words_dict.items(), key = <span class="hljs-keyword">lambda</span> f:f[<span class="hljs-number">1</span>], reverse = <span class="hljs-keyword">True</span>)
    all_words_list, all_words_nums = zip(*all_words_tuple_list)    <span class="hljs-comment">#解压缩</span>
    all_words_list = list(all_words_list)                        <span class="hljs-comment">#转换成列表</span>
    <span class="hljs-keyword">return</span> all_words_list, train_data_list, test_data_list, train_class_list, test_class_list

<span class="hljs-string">"""
函数说明:读取文件里的内容，并去重

Parameters:
    words_file - 文件路径
Returns:
    words_set - 读取的内容的set集合
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-22
"""</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">MakeWordsSet</span><span class="hljs-params">(words_file)</span>:</span>
    words_set = set()                                            <span class="hljs-comment">#创建set集合</span>
    <span class="hljs-keyword">with</span> open(words_file, <span class="hljs-string">'r'</span>, encoding = <span class="hljs-string">'utf-8'</span>) <span class="hljs-keyword">as</span> f:        <span class="hljs-comment">#打开文件</span>
        <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f.readlines():                                <span class="hljs-comment">#一行一行读取</span>
            word = line.strip()                                    <span class="hljs-comment">#去回车</span>
            <span class="hljs-keyword">if</span> len(word) &gt; <span class="hljs-number">0</span>:                                    <span class="hljs-comment">#有文本，则添加到words_set中</span>
                words_set.add(word)                               
    <span class="hljs-keyword">return</span> words_set                                             <span class="hljs-comment">#返回处理结果</span>

<span class="hljs-string">"""
函数说明:根据feature_words将文本向量化

Parameters:
    train_data_list - 训练集
    test_data_list - 测试集
    feature_words - 特征集
Returns:
    train_feature_list - 训练集向量化列表
    test_feature_list - 测试集向量化列表
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-22
"""</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">TextFeatures</span><span class="hljs-params">(train_data_list, test_data_list, feature_words)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">text_features</span><span class="hljs-params">(text, feature_words)</span>:</span>                        <span class="hljs-comment">#出现在特征集中，则置1                                               </span>
        text_words = set(text)
        features = [<span class="hljs-number">1</span> <span class="hljs-keyword">if</span> word <span class="hljs-keyword">in</span> text_words <span class="hljs-keyword">else</span> <span class="hljs-number">0</span> <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> feature_words]
        <span class="hljs-keyword">return</span> features
    train_feature_list = [text_features(text, feature_words) <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> train_data_list]
    test_feature_list = [text_features(text, feature_words) <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> test_data_list]
    <span class="hljs-keyword">return</span> train_feature_list, test_feature_list                <span class="hljs-comment">#返回结果</span>


<span class="hljs-string">"""
函数说明:文本特征选取

Parameters:
    all_words_list - 训练集所有文本列表
    deleteN - 删除词频最高的deleteN个词
    stopwords_set - 指定的结束语
Returns:
    feature_words - 特征集
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-22
"""</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">words_dict</span><span class="hljs-params">(all_words_list, deleteN, stopwords_set = set<span class="hljs-params">()</span>)</span>:</span>
    feature_words = []                            <span class="hljs-comment">#特征列表</span>
    n = <span class="hljs-number">1</span>
    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range(deleteN, len(all_words_list), <span class="hljs-number">1</span>):
        <span class="hljs-keyword">if</span> n &gt; <span class="hljs-number">1000</span>:                            <span class="hljs-comment">#feature_words的维度为1000</span>
            <span class="hljs-keyword">break</span>                               
        <span class="hljs-comment">#如果这个词不是数字，并且不是指定的结束语，并且单词长度大于1小于5，那么这个词就可以作为特征词</span>
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> all_words_list[t].isdigit() <span class="hljs-keyword">and</span> all_words_list[t] <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> stopwords_set <span class="hljs-keyword">and</span> <span class="hljs-number">1</span> &lt; len(all_words_list[t]) &lt; <span class="hljs-number">5</span>:
            feature_words.append(all_words_list[t])
        n += <span class="hljs-number">1</span>
    <span class="hljs-keyword">return</span> feature_words

<span class="hljs-string">"""
函数说明:新闻分类器

Parameters:
    train_feature_list - 训练集向量化的特征文本
    test_feature_list - 测试集向量化的特征文本
    train_class_list - 训练集分类标签
    test_class_list - 测试集分类标签
Returns:
    test_accuracy - 分类器精度
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-22
"""</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">TextClassifier</span><span class="hljs-params">(train_feature_list, test_feature_list, train_class_list, test_class_list)</span>:</span>
    classifier = MultinomialNB().fit(train_feature_list, train_class_list)
    test_accuracy = classifier.score(test_feature_list, test_class_list)
    <span class="hljs-keyword">return</span> test_accuracy

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
    <span class="hljs-comment">#文本预处理</span>
    folder_path = <span class="hljs-string">'./SogouC/Sample'</span>                <span class="hljs-comment">#训练集存放地址</span>
    all_words_list, train_data_list, test_data_list, train_class_list, test_class_list = TextProcessing(folder_path, test_size=<span class="hljs-number">0.2</span>)

    <span class="hljs-comment"># 生成stopwords_set</span>
    stopwords_file = <span class="hljs-string">'./stopwords_cn.txt'</span>
    stopwords_set = MakeWordsSet(stopwords_file)


    test_accuracy_list = []
    deleteNs = range(<span class="hljs-number">0</span>, <span class="hljs-number">1000</span>, <span class="hljs-number">20</span>)                <span class="hljs-comment">#0 20 40 60 ... 980</span>
    <span class="hljs-keyword">for</span> deleteN <span class="hljs-keyword">in</span> deleteNs:
        feature_words = words_dict(all_words_list, deleteN, stopwords_set)
        train_feature_list, test_feature_list = TextFeatures(train_data_list, test_data_list, feature_words)
        test_accuracy = TextClassifier(train_feature_list, test_feature_list, train_class_list, test_class_list)
        test_accuracy_list.append(test_accuracy)

    plt.figure()
    plt.plot(deleteNs, test_accuracy_list)
    plt.title(<span class="hljs-string">'Relationship of deleteNs and test_accuracy'</span>)
    plt.xlabel(<span class="hljs-string">'deleteNs'</span>)
    plt.ylabel(<span class="hljs-string">'test_accuracy'</span>)
    plt.show()</code></pre> 
<p>运行结果如下：</p> 
<p></p> 
<p></p> 
<div align="center"> 
 <img src="https://images2.imgbox.com/a7/8b/cwSJuLxa_o.png"> 
</div> 
<p></p> 
<p></p> 
<p>我们绘制出了deleteNs和test_accuracy的关系，这样我们就可以大致确定去掉前多少的高频词汇了。每次运行程序，绘制的图形可能不尽相同，我们可以通过多次测试，来决定这个deleteN的取值，然后确定这个参数，这样就可以顺利构建出用于新闻分类的朴素贝叶斯分类器了。我测试感觉450还不错，最差的分类准确率也可以达到百分之50以上。将<code>if __name__ == '__main__'</code>下的代码修改如下：</p> 
<pre class="prettyprint"><code class=" hljs bash"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
    <span class="hljs-comment">#文本预处理</span>
    folder_path = <span class="hljs-string">'./SogouC/Sample'</span>                <span class="hljs-comment">#训练集存放地址</span>
    all_words_list, train_data_list, test_data_list, train_class_list, test_class_list = TextProcessing(folder_path, test_size=<span class="hljs-number">0.2</span>)

    <span class="hljs-comment"># 生成stopwords_set</span>
    stopwords_file = <span class="hljs-string">'./stopwords_cn.txt'</span>
    stopwords_<span class="hljs-keyword">set</span> = MakeWordsSet(stopwords_file)


    test_accuracy_list = []
    feature_words = words_dict(all_words_list, <span class="hljs-number">450</span>, stopwords_<span class="hljs-keyword">set</span>)
    train_feature_list, test_feature_list = TextFeatures(train_data_list, test_data_list, feature_words)
    test_accuracy = TextClassifier(train_feature_list, test_feature_list, train_class_list, test_class_list)
    test_accuracy_list.append(test_accuracy)
    ave = lambda c: sum(c) / len(c)</code></pre> 
<p>运行结果：</p> 
<p></p> 
<p></p> 
<div align="center"> 
 <img src="https://images2.imgbox.com/6d/7f/Sv35UPV0_o.png"> 
</div> 
<p></p> 
<p></p> 
<hr> 
<h2 id="五-总结">五 总结</h2> 
<ul><li>在训练朴素贝叶斯分类器之前，要处理好训练集，文本的清洗还是有很多需要学习的东西。</li><li>根据提取的分类特征将文本向量化，然后训练朴素贝叶斯分类器。</li><li>去高频词汇数量的不同，对结果也是有影响的的。</li><li>拉普拉斯平滑对于改善朴素贝叶斯分类器的分类效果有着积极的作用。</li><li>知乎专栏正在跟进中，届时会同步更新，欢迎帮忙点波关注：<a href="https://zhuanlan.zhihu.com/ml-jack" rel="nofollow noopener noreferrer" target="_blank">https://zhuanlan.zhihu.com/ml-jack</a></li><li>如有问题，请留言。如有错误，还望指正，谢谢！</li></ul> 
<p><strong>PS： 如果觉得本篇本章对您有所帮助，欢迎关注、评论、顶！</strong></p> 
<p>本文出现的所有代码和数据集，均可在我的github上下载，欢迎Follow、Star：<a href="https://github.com/Jack-Cherish/Machine-Learning" target="_blank" rel="noopener noreferrer">https://github.com/Jack-Cherish/Machine-Learning</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d5753023bfd163204ee6e96fb9660c4c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">事务并发处理带来的问题</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/350d410fc13d0d31e12c3d1ff6eca6a2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">地址总线、字长和内存空间的关系</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>