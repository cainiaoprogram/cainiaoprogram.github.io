<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>大数据面试题 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="大数据面试题" />
<meta property="og:description" content="Big Data 面试题总结 JAVA相关 1-1）List 与set 的区别？ 老掉牙的问题了，还在这里老生常谈：List特点：元素有放入顺序，元素可重复 ，Set特点：元素无放入顺序，元素不可重复。
1-2）数据库的三大范式？ 原子性、一致性、唯一性
1-3）java 的io类的图解 1-4）对象与引用对象的区别 对象就是好没有初始化的对象，引用对象即使对这个对象进行了初始化，这个初始化可以使自己的直接new的也可以是直接其他的赋值的，那么背new或者背其他赋值的我们叫做是引用对象，最大的区别于
1-5）谈谈你对反射机制的理解及其用途？ 反射有三种获取的方式，分别是：forName / getClass / 直接使用class方式 使用反射可以获取类的实例
1-6）列出至少五种设计模式 设计方式有工厂法，懒加载，观察者模式，静态工厂，迭代器模式，外观模式、、、、
1-7）RPC 原理？ Rpc分为同步调用和一部调用，异步与同步的区别在于是否等待服务器端的返回值。Rpc的组件有RpcServer,RpcClick,RpcProxy,RpcConnection,RpcChannel,RpcProtocol,RpcInvoker等组件，
1-8）ArrayList、Vector、LinkedList 的区别及其优缺点？HashMap、HashTable 的区别及优缺点？ ArrayList 和 Vector 是采用数组方式存储数据的,是根据索引来访问元素的，都可以
根据需要自动扩展内部数据长度，以便增加和插入元素，都允许直接序号索引元素，但
是插入数据要涉及到数组元素移动等内存操作，所以索引数据快插入数据慢，他们最大
的区别就是 synchronized 同步的使用。
LinkedList 使用双向链表实现存储，按序号索引数据需要进行向前或向后遍历，但
是插入数据时只需要记录本项的前后项即可，所以插入数度较快！
如果只是查找特定位置的元素或只在集合的末端增加、移除元素，那么使用 Vector
或 ArrayList 都可以。如果是对其它指定位置的插入、删除操作，最好选择 LinkedList
HashMap、HashTable 的区别及其优缺点：
HashTable 中的方法是同步的 HashMap 的方法在缺省情况下是非同步的 因此在多线程环境下需要做额外的同步机制。
HashTable 不允许有 null 值 key 和 value 都不允许，而 HashMap 允许有 null 值 key和 value 都允许 因此 HashMap 使用 containKey（）来判断是否存在某个键。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/7e68b903847f6d16adb7aab3834f900a/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2016-09-12T16:27:19+08:00" />
<meta property="article:modified_time" content="2016-09-12T16:27:19+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">大数据面试题</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p style="text-align:center"> <img src="https://images2.imgbox.com/8c/fc/W3TOfzm2_o.jpg" alt=""></p> 
<p style="text-align:center"><br> </p> 
<h2><strong></strong></h2> 
<h2><strong>Big Data <span style="font-family:宋体">面试题总结 </span></strong></h2> 
<h3><strong>JAVA<span style="font-family:黑体">相关</span> </strong></h3> 
<h4><strong>1-1<span style="font-family:宋体">）</span><span style="font-family:Calibri">List </span><span style="font-family:宋体">与</span><span style="font-family:Calibri">set </span><span style="font-family:宋体">的区别？</span></strong></h4> 
<p><span style="color:rgb(12,12,12)">老掉牙的问题了，还在这里老生常谈：</span><span style="color:rgb(12,12,12)">List特点：元素有放入顺序，元素可重复 ，Set特点：元素无放入顺序，元素不可重复</span><span style="color:rgb(12,12,12)"><span style="font-family:宋体">。</span></span></p> 
<p><span style="color:rgb(12,12,12)"> </span></p> 
<h4><strong>1-2<span style="font-family:宋体">）数据库的三大范式？</span></strong></h4> 
<p>原子性、一致性、唯一性</p> 
<p> </p> 
<h4><strong>1-3<span style="font-family:宋体">）</span><span style="font-family:Calibri">java </span><span style="font-family:宋体">的</span><span style="font-family:Calibri">io</span><span style="font-family:宋体">类的图解</span></strong></h4> 
<p></p> 
<p> </p> 
<h4><strong>1-4<span style="font-family:宋体">）对象与引用对象的区别</span></strong></h4> 
<p><span style="font-family:宋体">对象就是好没有初始化的对象，引用对象即使对这个对象进行了初始化，这个初始化可以使自己的直接</span>new<span style="font-family:宋体">的也可以是直接其他的赋值的，那么背</span><span style="font-family:Calibri">new</span><span style="font-family:宋体">或者背其他赋值的我们叫做是引用对象，最大的区别于</span></p> 
<p> </p> 
<h4><strong>1-5<span style="font-family:宋体">）谈谈你对反射机制的理解及其用途？</span></strong></h4> 
<p>反射有三种获取的方式，分别是：<span style="color:rgb(75,75,75)">forName</span><span style="color:rgb(75,75,75)"> </span><span style="color:rgb(75,75,75)"> / getClass / <span style="font-family:宋体">直接使用</span><span style="font-family:Verdana">class</span><span style="font-family:宋体">方式</span></span><span style="color:rgb(75,75,75)"> </span><span style="color:rgb(12,12,12)"><span style="font-family:宋体">使用反射可以获取类的实例</span></span></p> 
<h4><strong>1-6<span style="font-family:宋体">）列出至少五种设计模式</span></strong></h4> 
<p>设计方式有工厂法，懒加载，观察者模式，静态工厂，迭代器模式，外观模式、、、、</p> 
<p> </p> 
<h4><strong>1-7<span style="font-family:宋体">）</span><span style="font-family:Calibri">RPC </span><span style="font-family:宋体">原理？</span></strong></h4> 
<p><span style="color:rgb(12,12,12)">R</span><span style="color:rgb(12,12,12)">pc<span style="font-family:宋体">分为同步调用和一部调用，异步与同步的区别在于是否等待服务器端的返回值。</span></span><span style="color:rgb(12,12,12)">R</span><span style="color:rgb(12,12,12)">pc<span style="font-family:宋体">的组件有</span><span style="font-family:Verdana">RpcServer,RpcClick,RpcProxy,RpcConnection,RpcChannel,RpcProtocol,RpcInvoker</span><span style="font-family:宋体">等组件，</span></span></p> 
<p><span style="color:rgb(12,12,12)"> </span></p> 
<h4><strong>1-8<span style="font-family:宋体">）</span><span style="font-family:Calibri">ArrayList</span><span style="font-family:宋体">、</span><span style="font-family:Calibri">Vector</span><span style="font-family:宋体">、</span><span style="font-family:Calibri">LinkedList </span><span style="font-family:宋体">的区别及其优缺点？</span><span style="font-family:Calibri">HashMap</span><span style="font-family:宋体">、</span><span style="font-family:Calibri">HashTable </span><span style="font-family:宋体">的区别及优缺点？</span></strong></h4> 
<p><span style="color:rgb(12,12,12)">    ArrayList <span style="font-family:宋体">和 </span><span style="font-family:Verdana">Vector </span><span style="font-family:宋体">是采用数组方式存储数据的</span><span style="font-family:Verdana">,</span><span style="font-family:宋体">是根据索引来访问元素的，都可以</span></span></p> 
<p><span style="color:rgb(12,12,12)"><span style="font-family:宋体">根据需要自动扩展内部数据长度，以便增加和插入元素，都允许直接序号索引元素，但</span></span></p> 
<p><span style="color:rgb(12,12,12)"><span style="font-family:宋体">是插入数据要涉及到数组元素移动等内存操作，所以索引数据快插入数据慢，他们最大</span></span></p> 
<p><span style="color:rgb(12,12,12)"><span style="font-family:宋体">的区别就是</span> synchronized <span style="font-family:宋体">同步的使用。</span></span></p> 
<p><span style="color:rgb(12,12,12)">    LinkedList <span style="font-family:宋体"> 使用双向链表实现存储，按序号索引数据需要进行向前或向后遍历，但</span></span></p> 
<p><span style="color:rgb(12,12,12)"><span style="font-family:宋体">是插入数据时只需要记录本项的前后项即可，所以插入数度较快！</span></span></p> 
<p><span style="color:rgb(12,12,12)"><span style="font-family:宋体">如果只是查找特定位置的元素或只在集合的末端增加、移除元素，那么使用</span> Vector</span></p> 
<p><span style="color:rgb(12,12,12)"><span style="font-family:宋体">或</span> ArrayList <span style="font-family:宋体">都可以。如果是对其它指定位置的插入、删除操作，最好选择 </span><span style="font-family:Verdana">LinkedList</span></span></p> 
<p><span style="color:rgb(12,12,12)">HashMap<span style="font-family:宋体">、</span><span style="font-family:Verdana">HashTable </span><span style="font-family:宋体">的区别及其优缺点：</span></span></p> 
<p><span style="color:rgb(12,12,12)">     HashTable <span style="font-family:宋体"> 中的方法是同步的 </span><span style="font-family:Verdana">HashMap </span><span style="font-family:宋体">的方法在缺省情况下是非同步的 因此在多线程环境下需要做额外的同步机制。</span></span></p> 
<p><span style="color:rgb(12,12,12)">    HashTable <span style="font-family:宋体">不允许有 </span><span style="font-family:Verdana">null </span><span style="font-family:宋体">值 </span><span style="font-family:Verdana">key </span><span style="font-family:宋体">和 </span><span style="font-family:Verdana">value </span><span style="font-family:宋体">都不允许，而 </span><span style="font-family:Verdana">HashMap </span><span style="font-family:宋体">允许有 </span><span style="font-family:Verdana">null </span><span style="font-family:宋体">值 </span><span style="font-family:Verdana">key</span><span style="font-family:宋体">和 </span><span style="font-family:Verdana">value </span><span style="font-family:宋体">都允许 因此 </span><span style="font-family:Verdana">HashMap </span><span style="font-family:宋体">使用 </span><span style="font-family:Verdana">containKey</span><span style="font-family:宋体">（）来判断是否存在某个键。</span></span></p> 
<p><span style="color:rgb(12,12,12)">HashTable <span style="font-family:宋体">使用 </span> <span style="font-family:Verdana">Enumeration </span><span style="font-family:宋体">，而 </span><span style="font-family:Verdana">HashMap </span><span style="font-family:宋体">使用 </span><span style="font-family:Verdana">iterator</span><span style="font-family:宋体">。</span></span></p> 
<p><span style="color:rgb(12,12,12)">     Hashtable <span style="font-family:宋体"> 是 </span><span style="font-family:Verdana">Dictionary </span><span style="font-family:宋体">的子类，</span><span style="font-family:Verdana">HashMap </span><span style="font-family:宋体">是 </span><span style="font-family:Verdana">Map </span><span style="font-family:宋体">接口的一个实现类。</span></span></p> 
<h4><strong>1-9<span style="font-family:宋体">）使用 </span><span style="font-family:Calibri">StringBuffer </span><span style="font-family:宋体">而不是 </span><span style="font-family:Calibri">String</span></strong></h4> 
<p><span style="color:rgb(12,12,12)"><span style="font-family:宋体">当需要对字符串进行操作时，使用</span> StringBuffer <span style="font-family:宋体">而不是 </span><span style="font-family:Verdana">String</span><span style="font-family:宋体">，</span><span style="font-family:Verdana">String </span><span style="font-family:宋体">是 </span><span style="font-family:Verdana">read-only </span><span style="font-family:宋体">的，如果对它进行修改，会产生临时对象，而 </span><span style="font-family:Verdana">StringBuffer </span><span style="font-family:宋体">是可修改的，不会产生临时对象。</span></span></p> 
<h4><strong>1-10<span style="font-family:宋体">）集合的扩充</span></strong></h4> 
<p><span style="color:rgb(12,12,12)">ArrayList  list = new ArrayList(90000); list<span style="font-family:宋体">扩充多少次？？</span></span></p> 
<p> <strong><span style="color:rgb(127,0,85)">public</span></strong> ArrayList() {<!-- --></p> 
<p>        <strong><span style="color:rgb(127,0,85)">this</span></strong>(10);</p> 
<p>}</p> 
<p><span style="font-family:Consolas">默认的扩充是</span>10由此计算</p> 
<p> </p> 
<h4><strong>1-11<span style="font-family:宋体">）</span><span style="font-family:Calibri">java</span><span style="font-family:宋体">的拆包与封包的问题</span></strong></h4> 
<p> </p> 
<p>System.<strong><em><span style="color:rgb(0,0,192)">out</span></em></strong>.println(<span style="color:rgb(42,0,255)">"5"</span> + 2);</p> 
<p>52</p> 
<p> </p> 
<h4><strong>1-12<span style="font-family:宋体">）</span>Java<span style="font-family:宋体">中</span><span style="font-family:Calibri">Class.forName</span><span style="font-family:宋体">和</span><span style="font-family:Calibri">ClassLoader.loadClass</span><span style="font-family:宋体">的区别</span></strong></h4> 
<p>Class.forName("xx.xx")<span style="font-family:宋体">等同于</span><span style="font-family:Consolas">Class.forName("xx.xx",true,CALLClass.class.getClassLoader())</span><span style="font-family:宋体">，第二个参数</span><span style="font-family:Consolas">(bool)</span><span style="font-family:宋体">表示装载类的时候是否初始化该类，即调用类的静态块的语句及初始化静态成员变量。</span></p> 
<p> </p> 
<p>ClassLoader loader = Thread.currentThread.getContextClassLoader(); //<span style="font-family:宋体">也可以用</span><span style="font-family:Consolas">(ClassLoader.getSystemClassLoader())</span></p> 
<p> </p> 
<p>Class cls = loader.loadClass("xx.xx"); //<span style="font-family:宋体">这句话没有执行初始化</span></p> 
<p> </p> 
<p>forName<span style="font-family:宋体">可以控制是否初始化类，而</span><span style="font-family:Consolas">loadClass</span><span style="font-family:宋体">加载时是没有初始化的。</span></p> 
<p> </p> 
<h4><strong>1-13<span style="font-family:宋体">）</span><span style="font-family:Calibri">hashMap</span><span style="font-family:宋体">与</span><span style="font-family:Calibri">hashTable</span><span style="font-family:宋体">的区别</span></strong></h4> 
<p>                   HashMap                Hashtable</p> 
<p> </p> 
<p><span style="font-family:宋体">父类</span>               AbstractMap          Dictiionary</p> 
<p> </p> 
<p><span style="font-family:宋体">是否同步</span>            <span style="font-family:宋体">否</span>                            <span style="font-family:宋体">是</span></p> 
<p> </p> 
<p>k<span style="font-family:宋体">，</span><span style="font-family:Consolas">v</span><span style="font-family:宋体">可否</span><span style="font-family:Consolas">null        </span><span style="font-family:宋体">是                            否</span></p> 
<p> </p> 
<p> </p> 
<p><span style="color:rgb(75,75,75)">Hashtable</span><span style="color:rgb(75,75,75)"><span style="font-family:宋体">和</span>HashMap采用的hash/rehash算法都大概一样，所以性能不会有很大的差异。</span></p> 
<p> </p> 
<h4><strong>1-14<span style="font-family:宋体">）怎样实现数组的反转</span></strong></h4> 
<p>ArrayList arrayList = new ArrayList();  </p> 
<p> arrayList.add("A");  </p> 
<p> arrayList.add("B");</p> 
<p> </p> 
<p>对数组进行反转</p> 
<p>Collections.reverse(arrayList);</p> 
<p> </p> 
<h4><strong>1-15<span style="font-family:宋体">）请使用</span><span style="font-family:Calibri">JAVA</span><span style="font-family:宋体">实现二分查找</span></strong></h4> 
<p>一般的面试者都是些向看看你的思路，所以一般答题时只需要把思路写出来即可。</p> 
<p>具体的实现如下：</p> 
<p>二分查找就是折半查找，要想折半就必须把原来的数据进行排序，才能方便的查找：</p> 
<p>实现代码如下：</p> 
<p> public static int binarySearch(int[] srcArray, int des){   </p> 
<p>        int low = 0;   </p> 
<p>        int high = srcArray.length-1;   </p> 
<p>        while(low &lt;= high) {   </p> 
<p>            int middle = (low + high)/2;   </p> 
<p>            if(des == srcArray[middle]) {   </p> 
<p>                return middle;   </p> 
<p>            }else if(des &lt;srcArray[middle]) {   </p> 
<p>                high = middle - 1;   </p> 
<p>            }else {   </p> 
<p>                low = middle + 1;   </p> 
<p>            }  </p> 
<p>        }  </p> 
<p>        return -1;  </p> 
<p>   }</p> 
<p> </p> 
<p> </p> 
<h4><strong>1-16<span style="font-family:宋体">）</span><span style="font-family:Calibri">java </span><span style="font-family:宋体">中有两个线程怎样等待一个线程执行完毕</span></strong></h4> 
<p><span style="font-family:宋体">可以使用</span>join<span style="font-family:宋体">关键字</span></p> 
<p> </p> 
<h4><strong>1-17<span style="font-family:宋体">）</span><span style="font-family:Calibri">hashmap hashtable currentHashMap</span><span style="font-family:宋体">的使用区别</span></strong></h4> 
<p>     hashmap hashtable <span style="font-family:宋体">的醉的的区别在于</span><span style="font-family:Calibri">hashtable </span><span style="font-family:宋体">是线程安全的，而</span><span style="font-family:Calibri">hashmap </span><span style="font-family:宋体">不是线程安全的，</span><span style="font-family:Calibri">currentHashMap</span><span style="font-family:宋体">也是线程安全的。</span></p> 
<p>     ConcurrentHashMap<span style="font-family:宋体">是使用了锁分段技术技术来保证线程安全的。</span><span style="font-family:宋体">所分段的技术是：讲数据分成一段一段的储存，给每一段的数据添加一把锁，当线程访问一个数据时，其他的数据可以被访问。</span></p> 
<p> </p> 
<h4><strong>1-18<span style="font-family:宋体">）简单描述一下</span><span style="font-family:Calibri">java</span><span style="font-family:宋体">的</span><span style="font-family:Calibri">gc</span><span style="font-family:宋体">机制，常用的</span><span style="font-family:Calibri">JAVA</span><span style="font-family:宋体">调优的方法，</span><span style="font-family:Calibri">OOM</span><span style="font-family:宋体">如何产生的，如何处理</span><span style="font-family:Calibri">OOM</span><span style="font-family:宋体">问题？？？</span></strong></h4> 
<p>1、<span style="font-family:宋体">程序在运行时会产生很多的对象的信息，当这些对象的信息没有用时，则会被</span>gc<span style="font-family:宋体">回收</span></p> 
<p>2、<span style="font-family:宋体">调优的方式主要是调节年轻代与老年代的内存的大小</span></p> 
<p>3、OOM<span style="font-family:宋体">是</span>OutOfMemory<span style="font-family:宋体">的缩写</span>(<span style="font-family:宋体">搞得跟多高大上似的</span><span style="font-family:Arial">)</span><span style="font-family:宋体">就是线程创建的多了，没有及时的回收过来所产生的，代码如下：</span></p> 
<p>public class JavaVMStackOOM {  </p> 
<p>    private void dontStop() {  </p> 
<p>        while (true) {  </p> 
<p>              </p> 
<p>        }  </p> 
<p>    }  </p> 
<p>      </p> 
<p>    public void stackLeakByThread() {  </p> 
<p>        while (true) {  </p> 
<p>            Thread thread = new Thread(new Runnable() {  </p> 
<p>                @Override  </p> 
<p>                public void run() {  </p> 
<p>                    dontStop();  </p> 
<p>                }  </p> 
<p>            });  </p> 
<p>            thread.start();  </p> 
<p>        }  </p> 
<p>    }  </p> 
<p>      </p> 
<p>    public static void main(String[] args) {  </p> 
<p>        JavaVMStackOOM oom = new JavaVMStackOOM();  </p> 
<p>        oom.stackLeakByThread();  </p> 
<p>}  </p> 
<p> </p> 
<p>4、<span style="font-family:宋体">既然知道以上的现象，在写代码时应该注意，不要过多的创建线程的数目。</span></p> 
<p> </p> 
<p> </p> 
<h3><strong>Linux <span style="font-family:黑体">相关</span></strong></h3> 
<h4><strong>2-1<span style="font-family:宋体">）关闭不必要的服务</span></strong></h4> 
<p>A、<span style="font-family:宋体">使用</span>ntsysv<span style="font-family:宋体">命令查看开启与关闭的服务</span></p> 
<p>B、停止打印服务</p> 
<p>[root@hadoop1 /]# /etc/init.d/cups stop</p> 
<p>[root@hadoop1 /]# chkconfig cups off</p> 
<h4><strong>2-2<span style="font-family:宋体">）关闭</span><span style="font-family:Calibri">IP6</span></strong></h4> 
<p>[root@hadoop1 /]# vim /etc/modprobe.conf</p> 
<p>在下面添加一下配置：</p> 
<p>alias net-pf-10 off</p> 
<p>alias ipv6 off</p> 
<h4><strong>2-3<span style="font-family:宋体">）调整文件的最大的打开数</span></strong></h4> 
<p><span style="font-family:宋体">查看当前的文件的数量：</span>[root@hadoop1 /]#ulimit -a</p> 
<p>修改配置：<br> [root@hadoop1 /]# vi /etc/security/limits.conf <span style="font-family:宋体">在文件最后加上：</span></p> 
<p>* soft nofile 65535</p> 
<p>* hard nofile 65535</p> 
<p>* soft nproc 65535</p> 
<p>* hard nproc 65535</p> 
<h4><strong>2-4<span style="font-family:宋体">）修改 </span><span style="font-family:Calibri">linux </span><span style="font-family:宋体">内核参数</span></strong></h4> 
<p>[root@hadoop1 /]# vi /etc/sysctl.conf</p> 
<p>在文本的最后追加一下内容：</p> 
<p>net.core.somaxconn = 32768</p> 
<p> </p> 
<p><span style="font-family:宋体">表示物理内存使用到</span> 90%<span style="font-family:宋体">（</span><span style="font-family:Calibri">100-10=90</span><span style="font-family:宋体">）的时候才使用 </span><span style="font-family:Calibri">swap </span><span style="font-family:宋体">交换区</span></p> 
<h4><strong>2-5<span style="font-family:宋体">）关闭 </span><span style="font-family:Calibri">noatime</span></strong></h4> 
<p>在最后追加一下内容</p> 
<p>/dev/sda2 /data ext3 noatime,nodiratime 0 0</p> 
<p> </p> 
<p> </p> 
<h4><strong>2-6)<span style="font-family:宋体">请用</span><span style="font-family:Calibri">shell</span><span style="font-family:宋体">命令把某一个文件下的所有的文件分发到其他的机器上</span></strong></h4> 
<p>Scp  -r  /user/local   hadoop2:/user/local</p> 
<p> </p> 
<h4><strong>2-7<span style="font-family:宋体">）</span><span style="font-family:Calibri">echo 1+1 &amp;&amp; echo "1+1" </span><span style="font-family:宋体">会输出什么</span></strong></h4> 
<p> </p> 
<p>[root@hadoop1 ~]# echo 1+1 &amp;&amp; echo "1+1"</p> 
<p>1+1</p> 
<p>1+1</p> 
<p> </p> 
<p>[root@hadoop1 ~]# echo 1+1 &amp;&amp; echo "1+1" &amp;&amp; echo "1+" 1</p> 
<p>1+1</p> 
<p>1+1</p> 
<p>1+ 1</p> 
<p> </p> 
<h4><strong>2-8<span style="font-family:宋体">）在当前的额目录下找出包含祖母</span><span style="font-family:Calibri">a</span><span style="font-family:宋体">并且文件的额大小大于</span><span style="font-family:Calibri">55K</span><span style="font-family:宋体">的文件</span></strong></h4> 
<p>[root@hadoop1 test]# find  .|  grep -ri "a" </p> 
<p>a.text:a</p> 
<p align="justify"> </p> 
<p align="justify">后半句没有写出来，有时间在搞</p> 
<p align="justify"> </p> 
<p> </p> 
<h4><strong>2-9<span style="font-family:宋体">）</span><span style="font-family:Calibri">linux</span><span style="font-family:宋体">用什么命令查看</span><span style="font-family:Calibri">cpu,</span><span style="font-family:宋体">硬盘，内存的信息？</span></strong></h4> 
<p align="justify">Top <span style="font-family:宋体">命令</span></p> 
<p align="justify"> </p> 
<h3><strong>Hadoop<span style="font-family:黑体">相关</span></strong></h3> 
<h4><strong>3-1<span style="font-family:宋体">）简单概述</span>hdfs<span style="font-family:宋体">原理，以及各个模块的职责</span></strong></h4> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p>1<span style="font-family:宋体">、客户端向 </span><span style="font-family:Calibri">nameNode </span><span style="font-family:宋体">发送要上传文件的请求</span></p> 
<p>2<span style="font-family:宋体">、</span><span style="font-family:Calibri">nameNode </span><span style="font-family:宋体">返回给用户是否能上传数据的状态</span></p> 
<p>3<span style="font-family:宋体">、加入用户端需要上传一个 </span><span style="font-family:Calibri">1024M </span><span style="font-family:宋体">的文件，客户端会通过 </span><span style="font-family:Calibri">Rpc </span><span style="font-family:宋体">请求 </span><span style="font-family:Calibri">NameNode</span><span style="font-family:宋体">，并返回需要上传给那些 </span><span style="font-family:Calibri">DataNode(</span><span style="font-family:宋体">分配机器的距离以及空间的大小等</span><span style="font-family:Calibri">),namonode</span><span style="font-family:宋体">会选择就近原则分配机器。</span></p> 
<p>4<span style="font-family:宋体">、客户端请求建立 </span><span style="font-family:Calibri">block </span><span style="font-family:宋体">传输管道 </span><span style="font-family:Calibri">chnnel </span><span style="font-family:宋体">上传数据</span></p> 
<p>5<span style="font-family:宋体">、在上传是 </span><span style="font-family:Calibri">datanode </span><span style="font-family:宋体">会与其他的机器建立连接并把数据块传送到其他的机器上</span></p> 
<p>6<span style="font-family:宋体">、</span><span style="font-family:Calibri">dataNode </span><span style="font-family:宋体">向 </span><span style="font-family:Calibri">namenode </span><span style="font-family:宋体">汇报自己的储存情况以及自己的信息</span></p> 
<p>7<span style="font-family:宋体">、档第一个快上传完后再去执行其他的复制的传送</span></p> 
<p> </p> 
<p> </p> 
<p> </p> 
<h4><strong>3-2<span style="font-family:宋体">）</span>mr<span style="font-family:宋体">的工作原理</span></strong></h4> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p>1、<span style="font-family:宋体">当执行</span>mr<span style="font-family:宋体">程序是，会执行一个</span><span style="font-family:Calibri">Job</span></p> 
<p>2、<span style="font-family:宋体">客户端的</span>jobClick<span style="font-family:宋体">会请求</span><span style="font-family:Calibri">namenode</span><span style="font-family:宋体">的</span><span style="font-family:Calibri">jobTracker</span><span style="font-family:宋体">要执行任务</span></p> 
<p>3、jobClick<span style="font-family:宋体">会去</span><span style="font-family:Calibri">HDFS</span><span style="font-family:宋体">端复制作业的资源文件</span></p> 
<p>4、<span style="font-family:宋体">客户端的</span>jobClick<span style="font-family:宋体">会向</span><span style="font-family:Calibri">namenode</span><span style="font-family:宋体">提交作业</span><span style="font-family:Calibri">,</span><span style="font-family:宋体">让</span><span style="font-family:Calibri">namenode</span><span style="font-family:宋体">做准备</span></p> 
<p>5、Namenode<span style="font-family:宋体">的</span><span style="font-family:Calibri">jobTracker</span><span style="font-family:宋体">会去初始化创建的对象</span></p> 
<p>6、Namenode<span style="font-family:宋体">会获取</span><span style="font-family:Calibri">hdfs</span><span style="font-family:宋体">的划分的分区</span></p> 
<p>7、Namenode<span style="font-family:宋体">去检查</span><span style="font-family:Calibri">TaskTracker</span><span style="font-family:宋体">的心跳信息，查看存活的机器</span></p> 
<p>8、<span style="font-family:宋体">当执行的</span>datenode<span style="font-family:宋体">执行任务时</span><span style="font-family:Calibri">Datenode</span><span style="font-family:宋体">会去</span><span style="font-family:Calibri">HDFS</span><span style="font-family:宋体">获取作业的资源的文件</span></p> 
<p>9、TaskTracker<span style="font-family:宋体">会去执行代码，并登陆</span><span style="font-family:Calibri">JVM</span><span style="font-family:宋体">的执行渠道</span></p> 
<p>10、JVM<span style="font-family:宋体">或执行</span><span style="font-family:Calibri">MapTask</span><span style="font-family:宋体">或者</span><span style="font-family:Calibri">ReduceTask</span></p> 
<p>11、执行终结</p> 
<p> </p> 
<p> </p> 
<h4><strong>3-3<span style="font-family:宋体">）怎样判断文件时候存在</span></strong></h4> 
<p><span style="font-family:宋体">这是</span>linux<span style="font-family:宋体">上的知识，只需要在</span><span style="font-family:Calibri">IF[ -f ] </span><span style="font-family:宋体">括号中加上</span><span style="font-family:Calibri">-f</span><span style="font-family:宋体">参数即可判断文件是否存在</span></p> 
<p> </p> 
<p> </p> 
<h4><strong>3-4<span style="font-family:宋体">）</span>fsimage<span style="font-family:宋体">和</span><span style="font-family:Calibri">edit</span><span style="font-family:宋体">的区别？</span></strong></h4> 
<p> </p> 
<p>大家都知道namenode与<span style="color:rgb(51,51,51)">secondary namenode</span><span style="color:rgb(51,51,51)"> <span style="font-family:宋体">的关系，当他们要进行数据同步时叫做</span>checkpoint时就用到了fsimage与edit，fsimage是保存最新的元数据的信息，当fsimage数据到一定的大小事会去生成一个新的文件来保存元数据的信息，这个新的文件就是edit，edit会回滚最新的数据。</span></p> 
<p><span style="color:rgb(51,51,51)"> </span></p> 
<h4><strong>3-5<span style="font-family:宋体">）</span>hdfs<span style="font-family:宋体">中的</span><span style="font-family:Calibri">block</span><span style="font-family:宋体">默认保存几份？</span></strong></h4> 
<p>不管是hadoop1.x <span style="font-family:宋体">还是</span><span style="font-family:Calibri">hadoop2.x </span><span style="font-family:宋体">都是默认的保存三份，可以通过参数</span><span style="font-family:Calibri">dfs.replication</span><span style="font-family:宋体">就行修改，副本的数目要根据机器的个数来确定。</span></p> 
<p> </p> 
<h4><strong>3-6<span style="font-family:宋体">）</span><span style="font-family:宋体">列举几个配置文件优化？</span></strong></h4> 
<p>Core-site.xml <span style="font-family:宋体">文件的优化</span></p> 
<p> </p> 
<p>fs.trash.interval</p> 
<p><span style="font-family:宋体">默认值：</span> 0</p> 
<p><span style="font-family:宋体">说明：</span> <span style="font-family:宋体">这个是开启</span>hdfs<span style="font-family:宋体">文件删除自动转移到垃圾箱的选项，值为垃圾箱文件清除时间。一般开启这个会比较好，以防错误删除重要文件。单位是分钟。</span></p> 
<p> </p> 
<p>dfs.namenode.handler.count</p> 
<p><span style="font-family:宋体">默认值：</span>10</p> 
<p><span style="font-family:宋体">说明：</span>hadoop<span style="font-family:宋体">系统里启动的任务线程数，这里改为</span><span style="font-family:Calibri">40</span><span style="font-family:宋体">，同样可以尝试该值大小对效率的影响变化进行最合适的值的设定。</span></p> 
<p> </p> 
<p>mapreduce.tasktracker.http.threads</p> 
<p><span style="font-family:宋体">默认值：</span>40</p> 
<p><span style="font-family:宋体">说明：</span>map<span style="font-family:宋体">和</span><span style="font-family:Calibri">reduce</span><span style="font-family:宋体">是通过</span><span style="font-family:Calibri">http</span><span style="font-family:宋体">进行数据传输的，这个是设置传输的并行线程数。</span></p> 
<p><span style="color:rgb(51,51,51)"> </span></p> 
<h4><strong>3-7) <span style="font-family:宋体">谈谈数据倾斜，如何发生的，并给出优化方案</span></strong></h4> 
<p><span style="color:rgb(51,51,51)">     <span style="font-family:宋体">数据的倾斜主要是两个的数据相差的数量不在一个级别上，在只想任务时就造成了数据的倾斜，可以通过分区的方法减少</span>reduce数据倾斜性能的方法，例如;抽样和范围的分区、自定义分区、数据大小倾斜的自定义侧咯</span></p> 
<p><span style="color:rgb(51,51,51)"> </span></p> 
<h4><strong>3-8<span style="font-family:宋体">）简单概括安装</span><span style="font-family:Calibri">hadoop</span><span style="font-family:宋体">的步骤</span></strong></h4> 
<p><span style="color:rgb(51,51,51)">1.创建 hadoop 帐户。</span></p> 
<p><span style="color:rgb(51,51,51)">2.setup.改 IP。</span></p> 
<p><span style="color:rgb(51,51,51)">3.安装 java，并修改/etc/profile 文件，配置 java 的环境变量。</span></p> 
<p><span style="color:rgb(51,51,51)">4.修改 Host 文件域名。</span></p> 
<p><span style="color:rgb(51,51,51)">5.安装 SSH，配置无密钥通信。</span></p> 
<p><span style="color:rgb(51,51,51)">6.解压 hadoop。</span></p> 
<p><span style="color:rgb(51,51,51)">7.配置 conf 文件下 hadoop-env.sh、core-site.sh、mapre-site.sh、hdfs-site.sh。</span></p> 
<p><span style="color:rgb(51,51,51)">8.配置 hadoop 的环境变量。</span></p> 
<p><span style="color:rgb(51,51,51)">9.Hadoop namenode -format</span></p> 
<p><span style="color:rgb(51,51,51)">10.Start-all.sh</span></p> 
<p><span style="color:rgb(51,51,51)"> </span></p> 
<h4><strong>3-9<span style="font-family:宋体">）简单概述</span><span style="font-family:Calibri">hadoop</span><span style="font-family:宋体">中的角色的分配以及功能</span></strong></h4> 
<p><span style="color:rgb(51,51,51)">N</span><span style="color:rgb(51,51,51)">amenode:负责管理元数据的信息</span></p> 
<p><span style="color:rgb(51,51,51)">SecondName:做namenode冷备份，对于namenode的机器当掉后能快速切换到制定的Secondname上</span></p> 
<p><span style="color:rgb(51,51,51)">DateNode:主要做储存数据的。</span></p> 
<p><span style="color:rgb(51,51,51)">JobTracker:管理任务，并把任务分配到taskTasker</span></p> 
<p><span style="color:rgb(51,51,51)">TaskTracker：执行任务的</span></p> 
<p><span style="color:rgb(51,51,51)"> </span></p> 
<h4><strong>3-10<span style="font-family:宋体">）怎样快速的杀死一个</span><span style="font-family:Calibri">job</span></strong></h4> 
<p><span style="color:rgb(51,51,51)">1、</span><span style="color:rgb(51,51,51)"><span style="font-family:宋体">执行</span>hadoop  job -list  拿到job-id</span></p> 
<p><span style="color:rgb(51,51,51)">2、</span><span style="color:rgb(51,51,51)">H</span><span style="color:rgb(51,51,51)">adoop job kill hadoop-id </span></p> 
<p> </p> 
<h4><strong>3-11)<span style="font-family:宋体">新增一个节点时怎样快速的启动</span></strong></h4> 
<p><span style="color:rgb(51,51,51)">Hadoop-daemon.sh start datanode</span></p> 
<p><span style="color:rgb(51,51,51)"> </span></p> 
<h4><strong>3-12)<span style="font-family:宋体">你认为用</span><span style="font-family:Calibri">java , streaming , pipe </span><span style="font-family:宋体">方式开发</span><span style="font-family:Calibri">map/reduce,</span><span style="font-family:宋体">各有什么优点</span></strong></h4> 
<p><span style="color:rgb(51,51,51)"><span style="font-family:宋体">开发</span>mapReduce只用过java与hive,不过使用java开发mapreduce显得笨拙，效率也慢，基于java慢的原因于是hive，这样就方便了查询与设计</span></p> 
<p><span style="color:rgb(51,51,51)"> </span></p> 
<h4><strong>3-13<span style="font-family:宋体">）简单概述</span><span style="font-family:Calibri">hadoop</span><span style="font-family:宋体">的</span><span style="font-family:Calibri">join</span><span style="font-family:宋体">的方法</span></strong></h4> 
<p><span style="color:rgb(51,51,51)">H</span><span style="color:rgb(51,51,51)">adoop 常用的jion有reduce side join  , map side  join ,  SemiJoin 不过reduce side join 与 map side join 比较常用，不过都是比较耗时的。</span></p> 
<p><span style="color:rgb(51,51,51)"> </span></p> 
<h4><strong>3-14<span style="font-family:宋体">）简单概述</span><span style="font-family:Calibri">hadoop</span><span style="font-family:宋体">的</span><span style="font-family:Calibri">combinet</span><span style="font-family:宋体">与</span><span style="font-family:Calibri">partition</span><span style="font-family:宋体">的区别</span></strong></h4> 
<p><span style="color:rgb(51,51,51)">combine<span style="font-family:宋体">和</span><span style="font-family:Arial">partition</span><span style="font-family:宋体">都是函数，中间的步骤应该只有</span><span style="font-family:Arial">shuffle</span><span style="font-family:宋体">！</span></span><span style="color:rgb(51,51,51)"> </span><span style="color:rgb(51,51,51)">combine<span style="font-family:宋体">分为</span><span style="font-family:Arial">map</span><span style="font-family:宋体">端和</span><span style="font-family:Arial">reduce</span><span style="font-family:宋体">端，</span></span><span style="color:rgb(255,0,0)"><span style="font-family:宋体">作用是把同一个</span>key<span style="font-family:宋体">的键值对合并在一起</span></span><span style="color:rgb(51,51,51)"><span style="font-family:宋体">，可以自定义的</span></span><span style="color:rgb(51,51,51)">,</span><span style="color:rgb(51,51,51)">partition<span style="font-family:宋体">是分割</span><span style="font-family:Arial">map</span><span style="font-family:宋体">每个节点的结果，</span></span><span style="color:rgb(255,0,0)"><span style="font-family:宋体">按照</span>key<span style="font-family:宋体">分别映射给不同的</span><span style="font-family:Arial">reduce</span></span><span style="color:rgb(51,51,51)"><span style="font-family:宋体">，也是可以自定义的。这里其实可以理解归类。</span></span></p> 
<h4><strong>3-15 ) hdfs <span style="font-family:宋体">的数据压缩算法</span></strong></h4> 
<p><span style="color:rgb(51,51,51)"> </span><span style="color:rgb(51,51,51)">H</span><span style="color:rgb(51,51,51)">adoop 的压缩算法有很多，其中比较常用的就是gzip算法与bzip2算法，都可以可通过</span><span style="color:rgb(51,51,51); background:rgb(250,250,252)">CompressionCodec</span><span style="color:rgb(51,51,51); background:rgb(250,250,252)"><span style="font-family:宋体">来实现</span></span></p> 
<p><span style="color:rgb(51,51,51)"> </span></p> 
<h4><strong>3-16<span style="font-family:宋体">）</span><span style="font-family:Calibri">hadoop</span><span style="font-family:宋体">的调度</span></strong></h4> 
<p><span style="color:rgb(51,51,51)">H</span><span style="color:rgb(51,51,51)">adoop 的调度有三种其中fifo的调度hadoop的默认的，这种方式是按照作业的优先级的高低与到达时间的先后执行的，还有公平调度器：名字见起意就是分配用户的公平获取共享集群呗!容量调度器:让程序都能货到执行的能力，在队列中获得资源。</span></p> 
<p><span style="color:rgb(51,51,51)"> </span></p> 
<h4><strong>3-17<span style="font-family:宋体">）</span><span style="font-family:Calibri">reduce </span><span style="font-family:宋体">后输出的数据量有多大？</span></strong></h4> 
<p><span style="font-family:宋体">输出的数据量还不是取决于</span>map<span style="font-family:宋体">端给他的数据量，没有数据</span><span style="font-family:Calibri">reduce</span><span style="font-family:宋体">也没法运算啊</span><span style="font-family:Calibri">!!</span></p> 
<p> </p> 
<p></p> 
<h4><strong>3-18) datanode <span style="font-family:宋体">在什么情况下不会备份？</span></strong></h4> 
<p>Hadoop<span style="font-family:宋体">保存的三个副本如果不算备份的话，那就是在正常运行的情况下不会备份，也是就是在设置副本为</span><span style="font-family:Calibri">1</span><span style="font-family:宋体">的时候不会备份，说白了就是单台机器呗！！还有</span><span style="font-family:Calibri">datanode </span><span style="font-family:宋体">在强制关闭或者非正常断电不会备份。</span></p> 
<h4><strong>3-19<span style="font-family:宋体">）</span><span style="font-family:Calibri">combine </span><span style="font-family:宋体">出现在那个过程？</span></strong></h4> 
<p>Hadoop<span style="font-family:宋体">的</span><span style="font-family:Calibri">map</span><span style="font-family:宋体">过程，根据意思就知道结合的意思吗，剩下的你们就懂了。想想</span><span style="font-family:Calibri">wordcound</span></p> 
<p> </p> 
<h4><strong>3-20) hdfs <span style="font-family:宋体">的体系结构？</span></strong></h4> 
<p>HDFS<span style="font-family:宋体">有 </span><span style="font-family:Calibri">namenode</span><span style="font-family:宋体">、</span><span style="font-family:Calibri">secondraynamenode</span><span style="font-family:宋体">、</span><span style="font-family:Calibri">datanode </span><span style="font-family:宋体">组成。</span></p> 
<p>namenode <span style="font-family:宋体">负责管理 </span><span style="font-family:Calibri">datanode </span><span style="font-family:宋体">和记录元数据</span></p> 
<p>secondraynamenode <span style="font-family:宋体">负责合并日志</span></p> 
<p>datanode <span style="font-family:宋体">负责存储数据</span></p> 
<h4><strong>3-21) hadoop flush <span style="font-family:宋体">的过程？</span></strong></h4> 
<p>Flush <span style="font-family:宋体">就是把数据落到磁盘，把数据保存起来呗</span><span style="font-family:Calibri">!</span></p> 
<p> </p> 
<h4><strong>3-22) <span style="font-family:宋体">什么是队列</span></strong></h4> 
<p>队列的实现是链表，消费的顺序是先进先出。</p> 
<p> </p> 
<h4><strong>3-23<span style="font-family:宋体">）三个 </span><span style="font-family:Calibri">datanode</span><span style="font-family:宋体">，当有一个 </span><span style="font-family:Calibri">datanode </span><span style="font-family:宋体">出现错误会怎样？</span></strong></h4> 
<p><span style="font-family:宋体">第一不会给储存带来影响，因为有其他的副本保存着，不过建议尽快修复，第二会影响运算的效率，机器少了，</span>reduce<span style="font-family:宋体">在保存数据时选择就少了，一个数据的块就大了所以就会慢。</span></p> 
<p> </p> 
<h4><strong>3-24<span style="font-family:宋体">）</span><span style="font-family:Calibri">mapReduce </span><span style="font-family:宋体">的执行过程</span></strong></h4> 
<p><span style="font-family:宋体">首先</span>map<span style="font-family:宋体">端会</span><span style="background:rgb(245,250,226)">Text </span><span style="font-family:宋体">接受到来自的数据，</span>text<span style="font-family:宋体">可以把数据进行操作，最后通过</span><span style="font-family:Calibri">context</span><span style="font-family:宋体">把</span><span style="font-family:Calibri">key</span><span style="font-family:宋体">与</span><span style="font-family:Calibri">value</span><span style="font-family:宋体">写入到下一步进行计算，一般的</span><span style="font-family:Calibri">reduce</span><span style="font-family:宋体">接受的</span><span style="font-family:Calibri">value</span><span style="font-family:宋体">是个集合可以运算，最后再通过</span><span style="font-family:Calibri">context</span><span style="font-family:宋体">把数据持久化出来。</span></p> 
<p> </p> 
<h4><strong>3-25<span style="font-family:宋体">）</span>Cloudera <span style="font-family:宋体"> 提供哪几种安装 </span><span style="font-family:Calibri">CDH </span><span style="font-family:宋体">的方法</span></strong></h4> 
<p><span style="color:rgb(68,68,68)">· </span><span style="color:rgb(68,68,68)">Cloudera manager</span></p> 
<p><span style="color:rgb(68,68,68)">· </span><span style="color:rgb(68,68,68)">Tarball</span></p> 
<p><span style="color:rgb(68,68,68)">· </span><span style="color:rgb(68,68,68)">Yum</span></p> 
<p><span style="color:rgb(68,68,68)">· </span><span style="color:rgb(68,68,68)">Rpm</span></p> 
<p> </p> 
<h4><strong>3-26<span style="font-family:宋体">）选择题与判断题</span></strong></h4> 
<p>http://blog.csdn.net/jiangheng0535/article/details/16800415</p> 
<p> </p> 
<h4><strong>3-27<span style="font-family:宋体">）</span><span style="font-family:Calibri">hadoop</span><span style="font-family:宋体">的</span><span style="font-family:Calibri">combinet</span><span style="font-family:宋体">与</span><span style="font-family:Calibri">partition</span><span style="font-family:宋体">效果图</span></strong></h4> 
<p></p> 
<p> </p> 
<p> </p> 
<h4><strong>3-28<span style="font-family:宋体">）</span><span style="font-family:Calibri">hadoop </span><span style="font-family:宋体">的机架感知（或者说是扩普）</span></strong></h4> 
<p>看图说话</p> 
<p></p> 
<p>数据块会优先储存在离namenode进的机器或者说成离namenode机架近的机器上，正好是验证了那句话不走网络就不走网络，不用磁盘就不用磁盘。</p> 
<p> </p> 
<h4><strong>3-29<span style="font-family:宋体">）文件大小默认为 </span><span style="font-family:Calibri">64M</span><span style="font-family:宋体">，改为 </span><span style="font-family:Calibri">128M </span><span style="font-family:宋体">有啥影响？</span></strong></h4> 
<p><span style="font-family:宋体">这样减少了</span>namenode<span style="font-family:宋体">的处理能力，数据的元数据保存在</span><span style="font-family:Calibri">namenode</span><span style="font-family:宋体">上，如果在网络不好的情况下会增到</span><span style="font-family:Calibri">datanode</span><span style="font-family:宋体">的储存速度。可以根据自己的网络来设置大小。</span></p> 
<p> </p> 
<h4><strong>3-30<span style="font-family:宋体">）</span><span style="font-family:Calibri">datanode </span><span style="font-family:宋体">首次加入 </span><span style="font-family:Calibri">cluster </span><span style="font-family:宋体">的时候，如果 </span><span style="font-family:Calibri">log </span><span style="font-family:宋体">报告不兼容文件版本，那需要</span><span style="font-family:Calibri">namenode </span><span style="font-family:宋体">执行格式化操作，这样处理的原因是？</span></strong></h4> 
<p>     <span style="font-family:宋体">这样处理是不合理的，因为那么</span> namenode <span style="font-family:宋体"> 格式化操作，是对文件系统进行格式</span></p> 
<p><span style="font-family:宋体">化，</span>namenode <span style="font-family:宋体">格式化时清空 </span><span style="font-family:Calibri">dfs/name </span><span style="font-family:宋体">下空两个目录下的所有文件，之后，会在目</span></p> 
<p><span style="font-family:宋体">录</span> dfs.name.dir <span style="font-family:宋体"> 下创建文件。</span></p> 
<p>     <span style="font-family:宋体">文本不兼容，有可能时</span> namenode <span style="font-family:宋体"> 与 </span><span style="font-family:Calibri">datanode </span><span style="font-family:宋体">的 数据里的 </span><span style="font-family:Calibri">namespaceID</span><span style="font-family:宋体">、</span></p> 
<p>clusterID <span style="font-family:宋体">不一致，找到两个 </span><span style="font-family:Calibri">ID </span><span style="font-family:宋体">位置，修改为一样即可解决。</span></p> 
<h4><strong>3-31<span style="font-family:宋体">）什么 </span><span style="font-family:Calibri">hadoop streaming</span><span style="font-family:宋体">？</span></strong></h4> 
<p>提示：指的是用其它语言处理</p> 
<p> </p> 
<h4><strong>3-32<span style="font-family:宋体">）</span><span style="font-family:Calibri">MapReduce </span><span style="font-family:宋体">中排序发生在哪几个阶段？这些排序是否可以避免？为什么？</span></strong></h4> 
<p>     <span style="font-family:宋体">一个</span> MapReduce <span style="font-family:宋体"> 作业由 </span><span style="font-family:Calibri">Map </span><span style="font-family:宋体">阶段和 </span><span style="font-family:Calibri">Reduce </span><span style="font-family:宋体">阶段两部分组成，这两阶段会对数</span></p> 
<p><span style="font-family:宋体">据排序，从这个意义上说，</span>MapReduce <span style="font-family:宋体"> 框架本质就是一个 </span><span style="font-family:Calibri">Distributed Sort</span><span style="font-family:宋体">。在 </span><span style="font-family:Calibri">Map</span></p> 
<p><span style="font-family:宋体">阶段，在</span> Map <span style="font-family:宋体">阶段，</span><span style="font-family:Calibri">Map Task </span><span style="font-family:宋体">会在本地磁盘输出一个按照 </span><span style="font-family:Calibri">key </span><span style="font-family:宋体">排序（采用的是快速</span></p> 
<p><span style="font-family:宋体">排序）的文件（中间可能产生多个文件，但最终会合并成一个），在</span> Reduce <span style="font-family:宋体"> 阶段，每</span></p> 
<p><span style="font-family:宋体">个</span> Reduce Task <span style="font-family:宋体"> 会对收到的数据排序，这样，数据便按照 </span><span style="font-family:Calibri">Key </span><span style="font-family:宋体">分成了若干组，之后以</span></p> 
<p><span style="font-family:宋体">组为单位交给</span> reduce<span style="font-family:宋体">（）处理。很多人的误解在 </span><span style="font-family:Calibri">Map </span><span style="font-family:宋体">阶段，如果不使用 </span><span style="font-family:Calibri">Combiner</span></p> 
<p><span style="font-family:宋体">便不会排序，这是错误的，不管你用不用</span> Combiner<span style="font-family:宋体">，</span><span style="font-family:Calibri">Map Task </span><span style="font-family:宋体">均会对产生的数据排</span></p> 
<p><span style="font-family:宋体">序（如果没有</span> Reduce Task<span style="font-family:宋体">，则不会排序，实际上 </span><span style="font-family:Calibri">Map </span><span style="font-family:宋体">阶段的排序就是为了减轻 </span><span style="font-family:Calibri">Reduce</span></p> 
<p><span style="font-family:宋体">端排序负载）。由于这些排序是</span> MapReduce <span style="font-family:宋体"> 自动完成的，用户无法控制，因此，在</span> </p> 
<p>hadoop 1.x <span style="font-family:宋体">中无法避免，也不可以关闭，但 </span><span style="font-family:Calibri">hadoop2.x </span><span style="font-family:宋体">是可以关闭的。</span></p> 
<p> </p> 
<h4><strong>3-33<span style="font-family:宋体">）</span><span style="font-family:Calibri">hadoop</span><span style="font-family:宋体">的</span><span style="font-family:Calibri">shuffer</span><span style="font-family:宋体">的概念</span></strong></h4> 
<p>Shuffer<span style="font-family:宋体">是一个过程，实在</span><span style="font-family:Calibri">map</span><span style="font-family:宋体">端到</span><span style="font-family:Calibri">reduce</span><span style="font-family:宋体">在调</span><span style="font-family:Calibri">reduce</span><span style="font-family:宋体">数据之前都叫</span><span style="font-family:Calibri">shuffer,</span><span style="font-family:宋体">主要是分区与排序，也就是内部的缓存分分区以及分发（是</span><span style="font-family:Calibri">reduce</span><span style="font-family:宋体">来拉数据的）和传输</span></p> 
<p> </p> 
<h4><strong>3-34<span style="font-family:宋体">）</span><span style="font-family:Calibri">hadoop</span><span style="font-family:宋体">的优化</span></strong></h4> 
<p>1、优化的思路可以从配置文件和系统以及代码的设计思路来优化</p> 
<p>2、配置文件的优化：调节适当的参数，在调参数时要进行测试</p> 
<p>3、<span style="font-family:宋体">代码的优化：</span>combiner<span style="font-family:宋体">的个数尽量与</span><span style="font-family:Calibri">reduce</span><span style="font-family:宋体">的个数相同，数据的类型保持一致，可以减少拆包与封包的进度</span></p> 
<p>4、<span style="font-family:宋体">系统的优化：可以设置</span>linux<span style="font-family:宋体">系统打开最大的文件数预计网络的带宽</span><span style="font-family:Calibri">MTU</span><span style="font-family:宋体">的配置</span></p> 
<p>5、<span style="font-family:宋体">为</span> job <span style="font-family:宋体">添加一个 </span> <span style="font-family:Calibri">Combiner</span><span style="font-family:宋体">，可以大大的减少</span><span style="font-family:Calibri">shuffer</span><span style="font-family:宋体">阶段的</span><span style="font-family:Calibri">maoTask</span><span style="font-family:宋体">拷贝过来给远程的   </span><span style="font-family:Calibri">reduce task</span><span style="font-family:宋体">的数据量，一般而言</span><span style="font-family:Calibri">combiner</span><span style="font-family:宋体">与</span><span style="font-family:Calibri">reduce</span><span style="font-family:宋体">相同。</span></p> 
<p>6、<span style="font-family:宋体">在开发中尽量使用</span>stringBuffer<span style="font-family:宋体">而不是</span><span style="font-family:Calibri">string</span><span style="font-family:宋体">，</span><span style="font-family:Calibri">string</span><span style="font-family:宋体">的模式是</span><span style="font-family:Calibri">read-only</span><span style="font-family:宋体">的，如果对它进行修改，会产生临时的对象，二</span><span style="font-family:Calibri">stringBuffer</span><span style="font-family:宋体">是可修改的，不会产生临时对象。</span></p> 
<p>7、修改一下配置：</p> 
<p><span style="font-family:宋体">一下是修改</span> mapred-site.xml <span style="font-family:宋体"> 文件</span></p> 
<p>修改最大槽位数</p> 
<p><span style="font-family:宋体">槽位数是在各个</span> tasktracker <span style="font-family:宋体"> 上的 </span><span style="font-family:Calibri">mapred-site.xml </span><span style="font-family:宋体">上设置的，默认都是 </span><span style="font-family:Calibri">2</span></p> 
<p>&lt;property&gt;</p> 
<p>&lt;name&gt;mapred.tasktracker.map.tasks.maximum&lt;/name&gt;</p> 
<p>task <span style="font-family:宋体">的最大数</span></p> 
<p>&lt;value&gt;2&lt;/value&gt;</p> 
<p>&lt;/property&gt;</p> 
<p>&lt;property&gt;</p> 
<p>&lt;name&gt;mapred.tasktracker.reduce.tasks.maximum&lt;/name&gt;</p> 
<p>ducetask <span style="font-family:宋体">的最大数</span></p> 
<p>&lt;value&gt;2&lt;/value&gt;</p> 
<p>&lt;/property&gt;</p> 
<p>调整心跳间隔</p> 
<p><span style="font-family:宋体">集群规模小于</span> 300 <span style="font-family:宋体">时，心跳间隔为 </span><span style="font-family:Calibri">300 </span><span style="font-family:宋体">毫秒</span></p> 
<p>mapreduce.jobtracker.heartbeat.interval.min <span style="font-family:宋体">心跳时间</span></p> 
<p><span style="font-family:宋体">北京市昌平区建材城西路金燕龙办公楼一层</span> <span style="font-family:宋体"> 电话：</span>400-618-9090</p> 
<p>mapred.heartbeats.in.second <span style="font-family:宋体">集群每增加多少节点，时间增加下面的值</span></p> 
<p>mapreduce.jobtracker.heartbeat.scaling.factor <span style="font-family:宋体">集群每增加上面的个数，心跳增多少</span></p> 
<p>启动带外心跳</p> 
<p>mapreduce.tasktracker.outofband.heartbeat <span style="font-family:宋体">默认是 </span> <span style="font-family:Calibri">false</span></p> 
<p>配置多块磁盘</p> 
<p>mapreduce.local.dir</p> 
<p><span style="font-family:宋体">配置</span> RPC hander <span style="font-family:宋体"> 数目</span></p> 
<p>mapred.job.tracker.handler.count <span style="font-family:宋体">默认是 </span><span style="font-family:Calibri">10</span><span style="font-family:宋体">，可以改成 </span><span style="font-family:Calibri">50</span><span style="font-family:宋体">，根据机器的能力</span></p> 
<p><span style="font-family:宋体">配置</span> HTTP <span style="font-family:宋体">线程数目</span></p> 
<p>tasktracker.http.threads <span style="font-family:宋体">默认是 </span><span style="font-family:Calibri">40</span><span style="font-family:宋体">，可以改成 </span><span style="font-family:Calibri">100 </span><span style="font-family:宋体">根据机器的能力</span></p> 
<p>选择合适的压缩方式</p> 
<p><span style="font-family:宋体">以</span> snappy <span style="font-family:宋体">为例：</span></p> 
<p>&lt;property&gt;</p> 
<p>&lt;name&gt;mapred.compress.map.output&lt;/name&gt;</p> 
<p>&lt;value&gt;true&lt;/value&gt;</p> 
<p>&lt;/property&gt;</p> 
<p>&lt;property&gt;</p> 
<p>&lt;name&gt;mapred.map.output.compression.codec&lt;/name&gt;</p> 
<p>&lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt;</p> 
<p>&lt;/property&gt;</p> 
<p> </p> 
<h4><strong>3-35)3 <span style="font-family:宋体">个 </span><span style="font-family:Calibri">datanode </span><span style="font-family:宋体">中有一个 个</span><span style="font-family:Calibri">datanode </span><span style="font-family:宋体">出现错误会怎样？</span></strong></h4> 
<p><span style="font-family:宋体">这个</span> datanode <span style="font-family:宋体">的数据会在其他的 </span><span style="font-family:Calibri">datanode </span><span style="font-family:宋体">上重新做备份。</span></p> 
<p> </p> 
<h4><strong>3-36<span style="font-family:宋体">）怎样决定</span><span style="font-family:Calibri">mapreduce</span><span style="font-family:宋体">的中的</span><span style="font-family:Calibri">map</span><span style="font-family:宋体">以及</span><span style="font-family:Calibri">reduce</span><span style="font-family:宋体">的数量</span></strong></h4> 
<p><span style="font-family:宋体">在</span>mapreduce<span style="font-family:宋体">中</span><span style="font-family:Calibri">map</span><span style="font-family:宋体">是有块的大小来决定的，</span><span style="font-family:Calibri">reduce</span><span style="font-family:宋体">的数量可以按照用户的业务来配置。</span></p> 
<p> </p> 
<h4><strong>3-37<span style="font-family:宋体">）两个文件合并的问题</span></strong></h4> 
<p><a target="_blank" href="http://www.cnblogs.com/zemliu/archive/2012/08/16/2641036.html" rel="nofollow noopener noreferrer"><strong><span style="color:rgb(12,12,12)"><span style="font-family:宋体">给定</span>a<span style="font-family:宋体">、</span><span style="font-family:Verdana">b</span><span style="font-family:宋体">两个文件，各存放</span><span style="font-family:Verdana">50</span><span style="font-family:宋体">亿个</span><span style="font-family:Verdana">url</span><span style="font-family:宋体">，每个</span><span style="font-family:Verdana">url</span><span style="font-family:宋体">各占用</span><span style="font-family:Verdana">64</span><span style="font-family:宋体">字节，内存限制是</span><span style="font-family:Verdana">4G</span><span style="font-family:宋体">，如何找出</span><span style="font-family:Verdana">a</span><span style="font-family:宋体">、</span><span style="font-family:Verdana">b</span><span style="font-family:宋体">文件共同的</span><span style="font-family:Verdana">url</span><span style="font-family:宋体">？</span></span></strong></a></p> 
<p> </p> 
<p>    主要的思想是把文件分开进行计算，在对每个文件进行对比，得出相同的URL,<span style="font-family:宋体">因为以上说是含有相同的</span><span style="font-family:Calibri">URL</span><span style="font-family:宋体">所以不用考虑数据倾斜的问题。详细的解题思路为：</span></p> 
<p> </p> 
<p><span style="color:rgb(35,35,35)"><span style="font-family:宋体">可以估计每个文件的大小为</span></span><span style="color:rgb(35,35,35)">5G*64=300G</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">，远大于</span></span><span style="color:rgb(35,35,35)">4G</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。</span></span><span style="color:rgb(35,35,35)"> </span><span style="color:rgb(35,35,35)"><br> </span><span style="color:rgb(35,35,35)">    </span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">遍历文件</span></span><span style="color:rgb(35,35,35)">a</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">，对每个</span></span><span style="color:rgb(35,35,35)">url</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">求取</span></span><span style="color:rgb(35,35,35)">hash(url)%1000</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">，然后根据所得值将</span></span><span style="color:rgb(35,35,35)">url</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">分别存储到</span></span><span style="color:rgb(35,35,35)">1000</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">个小文件（设为</span></span><span style="color:rgb(35,35,35)">a0,a1,...a999</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">）当中。这样每个小文件的大小约为</span></span><span style="color:rgb(35,35,35)">300M</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">。遍历文件</span></span><span style="color:rgb(35,35,35)">b</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">，采取和</span></span><span style="color:rgb(35,35,35)">a</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">相同的方法将</span></span><span style="color:rgb(35,35,35)">url</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">分别存储到</span></span><span style="color:rgb(35,35,35)">1000</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">个小文件</span></span><span style="color:rgb(35,35,35)">(b0,b1....b999)</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">中。这样处理后，所有可能相同的</span></span><span style="color:rgb(35,35,35)">url</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">都在对应的小文件</span></span><span style="color:rgb(35,35,35)">(a0 vs b0, a1 vs b1....a999 vs b999)</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">当中，不对应的小文件（比如</span></span><span style="color:rgb(35,35,35)">a0 vs b99</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">）不可能有相同的</span></span><span style="color:rgb(35,35,35)">url</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">。然后我们只要求出</span></span><span style="color:rgb(35,35,35)">1000</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">对小文件中相同的</span></span><span style="color:rgb(35,35,35)">url</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">即可。</span></span><span style="color:rgb(35,35,35)"> </span><span style="color:rgb(35,35,35)"><br> </span><span style="color:rgb(35,35,35)">    </span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">比如对于</span></span><span style="color:rgb(35,35,35)">a0 vs b0</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">，我们可以遍历</span></span><span style="color:rgb(35,35,35)">a0</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">，将其中的</span></span><span style="color:rgb(35,35,35)">url</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">存储到</span></span><span style="color:rgb(35,35,35)">hash_map</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">当中。然后遍历</span></span><span style="color:rgb(35,35,35)">b0</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">，如果</span></span><span style="color:rgb(35,35,35)">url</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">在</span></span><span style="color:rgb(35,35,35)">hash_map</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">中，则说明此</span></span><span style="color:rgb(35,35,35)">url</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">在</span></span><span style="color:rgb(35,35,35)">a</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">和</span></span><span style="color:rgb(35,35,35)">b</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">中同时存在，保存到文件中即可。</span></span><span style="color:rgb(35,35,35)"> </span><span style="color:rgb(35,35,35)"><br> </span><span style="color:rgb(35,35,35)">    </span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">如果分成的小文件不均匀，导致有些小文件太大（比如大于</span></span><span style="color:rgb(35,35,35)">2G</span><span style="color:rgb(35,35,35)"><span style="font-family:宋体">），可以考虑将这些太大的小文件再按类似的方法分成小小文件即可</span></span></p> 
<p><span style="color:rgb(35,35,35)"> </span></p> 
<h4><strong>3-38<span style="font-family:宋体">）怎样决定一个</span><span style="font-family:Calibri">job</span><span style="font-family:宋体">的</span><span style="font-family:Calibri">map</span><span style="font-family:宋体">和</span><span style="font-family:Calibri">reduce</span><span style="font-family:宋体">的数量</span></strong></h4> 
<p>map的数量通常是由hadoop集群的DFS块大小确定的，也就是输入文件的总块数<span style="font-family:宋体">，</span>reduce<span style="font-family:宋体">端是复制</span><span style="font-family:Tahoma">map</span><span style="font-family:宋体">端的数据，相对于</span><span style="font-family:Tahoma">map</span><span style="font-family:宋体">端的任务，</span><span style="font-family:Tahoma">reduce</span><span style="font-family:宋体">节点资源是相对于比较缺少的，同时运行的速度会变慢，争取的任务的个数应该是</span><span style="font-family:Tahoma">0.95</span><span style="font-family:宋体">过着</span><span style="font-family:Tahoma">1.75</span><span style="font-family:宋体">。</span></p> 
<p> </p> 
<h4><strong>3-39<span style="font-family:宋体">）</span><span style="font-family:Calibri">hadoop</span><span style="font-family:宋体">的</span><span style="font-family:Calibri">sequencefile</span><span style="font-family:宋体">的格式，并说明下什么是</span><span style="font-family:Calibri">JAVA</span><span style="font-family:宋体">的序列化，如何实现</span><span style="font-family:Calibri">JAVA</span><span style="font-family:宋体">的序列化</span></strong></h4> 
<p>1、hadoop<span style="font-family:宋体">的序列化</span><span style="font-family:Tahoma">(</span>sequencefile)<span style="font-family:宋体">是一二进制的形式来保存的</span></p> 
<p>2、Java<span style="font-family:宋体">的序列化是讲对象的内容进行流化</span></p> 
<p>3、<span style="font-family:宋体">实现序列化需要实现</span>Serializable<span style="font-family:宋体">接口便可以了</span></p> 
<p align="justify"> </p> 
<h4><strong>3-40<span style="font-family:宋体">）简单概述一下</span><span style="font-family:Calibri">hadoop1</span><span style="font-family:宋体">与</span><span style="font-family:Calibri">hadoop2</span><span style="font-family:宋体">的区别</span></strong></h4> 
<p>Hadoop2<span style="font-family:宋体">与</span><span style="font-family:Calibri">hadoop1</span><span style="font-family:宋体">最大的区别在于</span><span style="font-family:Calibri">HDFS</span><span style="font-family:宋体">的架构与</span><span style="font-family:Calibri">mapreduce</span><span style="font-family:宋体">的很大的区别，而且速度上有很大的提升，</span><span style="font-family:Calibri">hadoop2</span><span style="font-family:宋体">最主要的两个变化是：</span><span style="font-family:Calibri">namenode</span><span style="font-family:宋体">可以集群的部署了，</span><span style="font-family:Calibri">hadoop2</span><span style="font-family:宋体">中的</span><span style="font-family:Calibri">mapreduce</span><span style="font-family:宋体">中的</span><span style="font-family:Calibri">jobTracker</span><span style="font-family:宋体">中的资源调度器与生命周期管理拆分成两个独立的组件，并命名为</span><span style="font-family:Calibri">YARN</span></p> 
<p> </p> 
<h4><strong>3-41<span style="font-family:宋体">）</span><span style="font-family:Calibri">YARN</span><span style="font-family:宋体">的新特性</span></strong></h4> 
<p>YARN<span style="font-family:宋体">是</span><span style="font-family:Calibri">hadoop2.x</span><span style="font-family:宋体">之后才出的，主要是</span><span style="font-family:Calibri">hadoop</span><span style="font-family:宋体">的</span><span style="font-family:Calibri">HA(</span><span style="font-family:宋体">也就是集群</span><span style="font-family:Calibri">)</span><span style="font-family:宋体">，磁盘的容错，资源调度器 </span></p> 
<p> </p> 
<h4><strong>3-42<span style="font-family:宋体">）</span><span style="font-family:Calibri">hadoop join</span><span style="font-family:宋体">的原理</span></strong></h4> 
<p><span style="font-family:宋体">实现两个表的</span>join<span style="font-family:宋体">首先在</span><span style="font-family:Calibri">map</span><span style="font-family:宋体">端需要把表标示一下，把其中的一个表打标签，到</span><span style="font-family:Calibri">reduce</span><span style="font-family:宋体">端再进行笛卡尔积的运算，就是</span><span style="font-family:Calibri">reduce</span><span style="font-family:宋体">进行的实际的链接操作。</span></p> 
<p> </p> 
<h4><strong>3-43<span style="font-family:宋体">）</span><span style="font-family:Calibri">hadoop</span><span style="font-family:宋体">的二次排序</span></strong></h4> 
<p>Hadoop<span style="font-family:宋体">默认的是</span><span style="color:rgb(51,51,51)">HashPartitioner</span><span style="font-family:宋体">排序，当</span>map<span style="font-family:宋体">端一个文件非常大另外一个文件非常小时就会产生资源的分配不均匀，既可以使用</span><span style="font-family:Calibri">setPartitionerClass</span><span style="font-family:宋体">来设置分区，即形成了二次分区。</span></p> 
<p> </p> 
<h4><strong>3-44<span style="font-family:宋体">）</span><span style="font-family:Calibri">hadoop</span><span style="font-family:宋体">的</span><span style="font-family:Calibri">mapreduce</span><span style="font-family:宋体">的排序发生在几个阶段？</span></strong></h4> 
<p><span style="font-family:宋体">发生在两个阶段即使</span>map<span style="font-family:宋体">与</span><span style="font-family:Calibri">reduce</span><span style="font-family:宋体">阶段</span></p> 
<p> </p> 
<h4><strong>3-45<span style="font-family:宋体">）请描述</span><span style="font-family:Calibri">mapreduce</span><span style="font-family:宋体">中</span><span style="font-family:Calibri">shuffer</span><span style="font-family:宋体">阶段的工作流程，如何优化</span><span style="font-family:Calibri">shuffer</span><span style="font-family:宋体">阶段的？</span></strong></h4> 
<p> </p> 
<p>Mapreduce<span style="font-family:宋体">的</span><span style="font-family:Calibri">shuffer</span><span style="font-family:宋体">是出在</span><span style="font-family:Calibri">map task</span><span style="font-family:宋体">到</span><span style="font-family:Calibri">reduce task</span><span style="font-family:宋体">的这段过程中，首先会进入到</span><span style="font-family:Calibri">copy</span><span style="font-family:宋体">过程，会通过</span><span style="font-family:Calibri">http</span><span style="font-family:宋体">方式请求</span><span style="font-family:Calibri">map task</span><span style="font-family:宋体">所在的</span><span style="font-family:Calibri">task Tracker</span><span style="font-family:宋体">获取</span><span style="font-family:Calibri">map task </span><span style="font-family:宋体">的输出的文件，因此当</span><span style="font-family:Calibri">map  task</span><span style="font-family:宋体">结束，这些文件就会落到磁盘中，</span><span style="font-family:Calibri">merge</span><span style="font-family:宋体">实在</span><span style="font-family:Calibri">map</span><span style="font-family:宋体">端的动作，只是在</span><span style="font-family:Calibri">map</span><span style="font-family:宋体">拷贝过来的数值，会放到内存缓冲区中，给</span><span style="font-family:Calibri">shuffer</span><span style="font-family:宋体">使用，</span><span style="font-family:Calibri">reduce</span><span style="font-family:宋体">阶段，不断的</span><span style="font-family:Calibri">merge</span><span style="font-family:宋体">后最终会把文件放到磁盘中。</span></p> 
<p> </p> 
<p> </p> 
<h4><strong>3-46<span style="font-family:宋体">）</span><span style="font-family:Calibri">mapreduce</span><span style="font-family:宋体">的</span><span style="font-family:Calibri">combiner</span><span style="font-family:宋体">的作用是什么，什么时候不易使用？？</span></strong></h4> 
<p>Mapreduce<span style="font-family:宋体">中的</span><span style="font-family:Calibri">Combiner</span><span style="font-family:宋体">就是为了避免</span><span style="font-family:Calibri">map</span><span style="font-family:宋体">任务和</span><span style="font-family:Calibri">reduce</span><span style="font-family:宋体">任务之间的数据传输而设置的，</span><span style="font-family:Calibri">Hadoop</span><span style="font-family:宋体">允许用户针对</span><span style="font-family:Calibri">map task</span><span style="font-family:宋体">的输出指定一个合并函数。即为了减少传输到</span><span style="font-family:Calibri">Reduce</span><span style="font-family:宋体">中的数据量。它主要是为了削减</span><span style="font-family:Calibri">Mapper</span><span style="font-family:宋体">的输出从而减少网络带宽和</span><span style="font-family:Calibri">Reducer</span><span style="font-family:宋体">之上的负载。</span></p> 
<p>在数据量较少时不宜使用。</p> 
<p> </p> 
<p>3-47<span style="font-family:宋体">）</span></p> 
<h3><strong>Zookeeper <span style="font-family:黑体">相关</span> </strong></h3> 
<h4><strong>4-1<span style="font-family:宋体">）</span><span style="font-family:宋体">写出你对</span>zookeeper<span style="font-family:宋体">的理解</span></strong></h4> 
<p><span style="color:rgb(51,51,51)">     <span style="font-family:宋体">随着大数据的快速发展，多机器的协调工作，避免主要机器单点故障的问题，于是就引入管理机器的一个软件，他就是</span>zookeeper来协助机器正常的运行。</span></p> 
<p><span style="color:rgb(51,51,51)">     </span><span style="color:rgb(51,51,51)">Z</span><span style="color:rgb(51,51,51)">ookeeper有两个角色分别是leader与follower ，其中leader是主节点，其他的是副节点，在安装配置上一定要注意配置奇数个的机器上，便于zookeeper快速切换选举其他的机器。</span></p> 
<p><span style="color:rgb(51,51,51)"><span style="font-family:宋体">在其他的软件执行任务时在</span>zookeeper注册时会在zookeeper下生成相对应的目录，以便zookeeper去管理机器。</span></p> 
<p><span style="color:rgb(51,51,51)"> </span></p> 
<h4><strong>4-2<span style="font-family:宋体">）</span><span style="font-family:Calibri">zookeeper </span><span style="font-family:宋体">的搭建过程</span></strong></h4> 
<p><span style="font-family:宋体">主要是配置文件</span>zoo.cfg <span style="font-family:宋体"> 配置</span><span style="font-family:Calibri">dataDir </span><span style="font-family:宋体">的路径一句</span><span style="font-family:Calibri">dataLogDir </span><span style="font-family:宋体">的路径以及</span><span style="font-family:Calibri">myid</span><span style="font-family:宋体">的配置以及</span><span style="font-family:Calibri">server</span><span style="font-family:宋体">的配置，心跳端口与选举端口</span></p> 
<p> </p> 
<p> </p> 
<h3><strong>Hive <span style="font-family:黑体">相关</span></strong></h3> 
<h4><strong>5-1<span style="font-family:宋体">）</span><span style="font-family:Calibri">hive</span><span style="font-family:宋体">是怎样保存元数据的</span></strong></h4> 
<p><span style="font-family:宋体">保存元数据的方式有：内存数据库</span>rerdy<span style="font-family:宋体">，本地</span><span style="font-family:Calibri">mysql</span><span style="font-family:宋体">数据库，远程</span><span style="font-family:Calibri">mysql</span><span style="font-family:宋体">数据库，但是本地的</span><span style="font-family:Calibri">mysql</span><span style="font-family:宋体">数据用的比较多，因为本地读写速度都比较快</span></p> 
<p> </p> 
<h4><strong>5-2<span style="font-family:宋体">）外部表与内部表的区别</span></strong></h4> 
<p> </p> 
<p><span style="color:rgb(51,51,51)"><span style="font-family:宋体">先来说下</span>Hive中内部表与外部表的区别： </span></p> 
<p><span style="color:rgb(51,51,51)">Hive 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。这样外部表相对来说更加安全些，数据组织也更加灵活，方便共享源数据。</span></p> 
<p> </p> 
<h4><strong>5-3<span style="font-family:宋体">）对于 </span><span style="font-family:Calibri">hive</span><span style="font-family:宋体">，你写过哪些 </span><span style="font-family:Calibri">UDF </span><span style="font-family:宋体">函数，作用是什么</span></strong></h4> 
<p>UDF: user  <span style="color:rgb(51,51,51)">defined</span><span style="color:rgb(51,51,51)"> </span><span style="color:rgb(51,51,51)"> </span>function  <span style="font-family:宋体">的缩写，编写</span><span style="font-family:Calibri">hive udf</span><span style="font-family:宋体">的两种方式</span><span style="font-family:Calibri">extends UDF </span><span style="font-family:宋体">重写</span><span style="font-family:Calibri">evaluate</span><span style="font-family:宋体">第二种</span><span style="font-family:Calibri">extends GenericUDF</span><span style="font-family:宋体">重写</span><span style="color:rgb(51,51,51)">initialize<span style="font-family:宋体">、</span><span style="font-family:Arial">getDisplayString</span><span style="font-family:宋体">、</span><span style="font-family:Arial">evaluate</span><span style="font-family:宋体">方法</span></span></p> 
<p> </p> 
<h4><strong>5-4)Hive <span style="font-family:宋体">的 </span><span style="font-family:Calibri">sort by </span><span style="font-family:宋体">和 </span><span style="font-family:Calibri">order by </span><span style="font-family:宋体">的区别</span> </strong></h4> 
<p>  order by 会对输入做全局排序，因此只有一个reducer（多个reducer无法保证全局有序）只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。</p> 
<p>   sort by不是全局排序，其在数据进入reducer前完成排序.</p> 
<p><span style="font-family:宋体">因此，如果用</span>sort by进行排序，并且设置mapred.reduce.tasks&gt;1， 则sort by只保证每个reducer的输出有序，不保证全局有序。</p> 
<p> </p> 
<h4><strong>5-5<span style="font-family:宋体">）</span><span style="font-family:Calibri">hive</span><span style="font-family:宋体">保存元数据的方式以及各有什么特点？</span></strong></h4> 
<p>1、Hive有内存数据库derby数据库，特点是保存数据小，不稳定</p> 
<p>2、mysql数据库，储存方式可以自己设定，持久化好，一般企业开发都用mysql做支撑</p> 
<h4><strong>5-6<span style="font-family:宋体">）在开发中问什么建议使用外部表？</span></strong></h4> 
<p>1、<span style="font-family:宋体">外部表不会加载到</span>hive中只会有一个引用加入到元数据中</p> 
<p>2、<span style="font-family:宋体">在删除时不会删除表，只会删除元数据，所以不必担心数据的</span></p> 
<p> </p> 
<h4><strong>5-7<span style="font-family:宋体">）</span><span style="font-family:Calibri">hive partition </span><span style="font-family:宋体">分区</span></strong></h4> 
<p><span style="font-family:宋体">分区表，动态分区</span></p> 
<p> </p> 
<h4><strong>5-8<span style="font-family:宋体">）</span><span style="font-family:Calibri">insert into </span><span style="font-family:宋体">和 </span><span style="font-family:Calibri">override write </span><span style="font-family:宋体">区别？</span></strong></h4> 
<p>insert into<span style="font-family:宋体">：将某一张表中的数据写到另一张表中</span></p> 
<p>override write<span style="font-family:宋体">：覆盖之前的内容。</span></p> 
<p> </p> 
<h3><strong>Hbase <span style="font-family:黑体">相关</span></strong></h3> 
<h4><strong>6-1<span style="font-family:宋体">）</span><span style="font-family:Calibri">Hbase </span><span style="font-family:宋体">的 </span><span style="font-family:Calibri">rowkey </span><span style="font-family:宋体">怎么创建比较好？列族怎么创建比较好？</span></strong></h4> 
<p> </p> 
<p> </p> 
<p><span style="color:rgb(12,12,12)">Rowkey是一个二进制码流，Rowkey的长度被很多开发者建议说设计在10~100个字节，不过建议是越短越好，不要超过16个字节。</span><span style="color:rgb(12,12,12)"><span style="font-family:宋体">在查找时有索引会加快速度。</span></span></p> 
<p> </p> 
<p>Rowkey<span style="font-family:宋体">散列原则</span> <span style="font-family:宋体">、</span> Rowkey<span style="font-family:宋体">唯一原则</span> <span style="font-family:宋体">、</span> <span style="font-family:宋体">针对事务数据</span>Rowkey<span style="font-family:宋体">设计</span> <span style="font-family:宋体">、</span> <span style="font-family:宋体">针对统计数据的</span>Rowkey<span style="font-family:宋体">设计</span> <span style="font-family:宋体">、</span> <span style="font-family:宋体">针对通用数据的</span>Rowkey<span style="font-family:宋体">设计</span><span style="font-family:宋体">、</span> <span style="font-family:宋体">支持多条件查询的</span>RowKey<span style="font-family:宋体">设计</span>。 </p> 
<p> </p> 
<p> </p> 
<p>总结设计列族：</p> 
<p>1、一般不建议设计多个列族</p> 
<p>2、数据块的缓存的设计</p> 
<p>3、激进缓存设计</p> 
<p>4、<span style="font-family:宋体">布隆过滤器的设计</span>(<span style="font-family:宋体">可以提高随机读取的速度</span><span style="font-family:Calibri">)</span></p> 
<p>5、生产日期的设计</p> 
<p>6、列族压缩</p> 
<p>7、单元时间版本</p> 
<p> </p> 
<h4><strong>6-2<span style="font-family:宋体">）</span><span style="font-family:Calibri">Hbase </span><span style="font-family:宋体">的实现原理</span></strong></h4> 
<p>Hbase  <span style="font-family:宋体">的实现原理是</span><span style="font-family:Calibri">rpc </span>Protocol </p> 
<p> </p> 
<h4><strong>6-3) hbase <span style="font-family:宋体">过滤器实现原则</span></strong></h4> 
<p><span style="font-family:宋体">感觉这个问题有问题，过滤器多的是啦，说的是哪一个不知道</span>!!!!</p> 
<p>Hbase<span style="font-family:宋体">的过滤器有：</span><span style="color:rgb(51,51,51)">RowFilter</span><span style="color:rgb(51,51,51)"><span style="font-family:宋体">、</span></span><span style="color:rgb(51,51,51)">PrefixFilter</span><span style="color:rgb(51,51,51)"><span style="font-family:宋体">、</span></span><span style="color:rgb(51,51,51)">KeyOnlyFilter</span><span style="color:rgb(51,51,51)"><span style="font-family:宋体">、</span></span><span style="color:rgb(51,51,51)">RandomRowFilter</span><span style="color:rgb(51,51,51)"><span style="font-family:宋体">、</span></span><span style="color:rgb(51,51,51)">InclusiveStopFilter</span><span style="color:rgb(51,51,51)"><span style="font-family:宋体">、</span></span><span style="color:rgb(51,51,51)">FirstKeyOnlyFilter</span><span style="color:rgb(51,51,51)"><span style="font-family:宋体">、</span></span><span style="color:rgb(51,51,51)">ColumnPrefixFilter</span><span style="color:rgb(51,51,51)"><span style="font-family:宋体">、</span></span><span style="color:rgb(51,51,51)">ValueFilter</span><span style="color:rgb(51,51,51)"><span style="font-family:宋体">、</span></span><span style="color:rgb(51,51,51)">ColumnCountGetFilter</span><span style="color:rgb(51,51,51)"><span style="font-family:宋体">、</span></span><span style="color:rgb(51,51,51)">SingleColumnValueFilter</span><span style="color:rgb(51,51,51)"><span style="font-family:宋体">、</span></span><span style="color:rgb(51,51,51)">SingleColumnValueExcludeFilter</span><span style="color:rgb(51,51,51)"><span style="font-family:宋体">、</span></span><span style="color:rgb(51,51,51)">WhileMatchFilter</span><span style="color:rgb(51,51,51)"><span style="font-family:宋体">、</span></span><span style="color:rgb(51,51,51)">FilterList</span><span style="color:rgb(51,51,51)"> </span></p> 
<p><span style="color:rgb(51,51,51)">   <span style="font-family:宋体">你看这么多过滤波器呢，谁知道你问的那个啊！！</span></span></p> 
<p><span style="color:rgb(51,51,51)">   <span style="font-family:宋体">比较常用的过滤器有：</span></span><span style="color:rgb(51,51,51)">RowFilter</span><span style="color:rgb(51,51,51)"> <span style="font-family:宋体">一看就知道是行的过滤器，来过滤行的信息。</span></span><span style="color:rgb(51,51,51)">PrefixFilter</span><span style="color:rgb(51,51,51)"><span style="font-family:宋体">前缀的过滤器，就是把前缀作为参数来查找数据呗！剩下的不解释了看过滤器的直接意思就</span></span><span style="color:rgb(51,51,51)">OK<span style="font-family:宋体">了很简单。</span></span></p> 
<p><span style="color:rgb(51,51,51)"> </span></p> 
<h4><strong>6-4<span style="font-family:宋体">）描述 </span><span style="font-family:Calibri">HBase, zookeeper </span><span style="font-family:宋体">搭建过程</span></strong></h4> 
<p>Zookeeper <span style="font-family:宋体">的问题楼上爬爬有步骤，</span><span style="font-family:Calibri">hbase </span><span style="font-family:宋体">主要的配置文件有</span><span style="font-family:Calibri">hbase.env.sh </span><span style="font-family:宋体">主要配置的是</span><span style="font-family:Calibri">JDK</span><span style="font-family:宋体">的路径以及是否使用外部的</span><span style="font-family:Calibri">ZK</span><span style="font-family:宋体">，</span>hbase-site.xml <span style="font-family:宋体">主要配置的是与</span>HDFS<span style="font-family:宋体">的链接的路径以及</span><span style="font-family:Calibri">zk</span><span style="font-family:宋体">的信息，修改</span>regionservers的链接其他机器的配置。</p> 
<p> </p> 
<h4><strong>6-5<span style="font-family:宋体">）</span><span style="font-family:Calibri">hive </span><span style="font-family:宋体">如何调优？</span></strong></h4> 
<p><span style="font-family:宋体">在优化时要注意数据的问题，尽量减少数据倾斜的问题，减少</span>job<span style="font-family:宋体">的数量，同事对小的文件进行成大的文件，如果优化的设计那就更好了，因为</span><span style="font-family:Calibri">hive</span><span style="font-family:宋体">的运算就是</span><span style="font-family:Calibri">mapReduce</span><span style="font-family:宋体">所以调节</span><span style="font-family:Calibri">mapreduce</span><span style="font-family:宋体">的参数也会使性能提高，如调节</span><span style="font-family:Calibri">task</span><span style="font-family:宋体">的数目。</span></p> 
<p> </p> 
<h4><strong>6-6<span style="font-family:宋体">）</span><span style="font-family:Calibri">hive</span><span style="font-family:宋体">的权限的设置</span></strong></h4> 
<p><span style="color:rgb(12,12,12)">Hive的权限需要在</span><span style="color:rgb(12,12,12)">hive-site.xml</span><span style="color:rgb(12,12,12)"><span style="font-family:宋体">文件中设置才会起作用，配置默认的是</span></span><span style="color:rgb(12,12,12)">false，需要把</span><span style="color:rgb(12,12,12)">hive.security.authorization.enabled</span><span style="color:rgb(12,12,12)"><span style="font-family:宋体">设置为</span></span><span style="color:rgb(12,12,12)">true</span><span style="color:rgb(12,12,12)"><span style="font-family:宋体">，并对不同的用户设置不同的权限，例如</span>select ,drop等的操作。</span></p> 
<p><span style="color:rgb(12,12,12)"> </span></p> 
<h4><strong>6-7 ) hbase <span style="font-family:宋体">写数据的原理</span></strong></h4> 
<p></p> 
<p> </p> 
<p>1. <span style="font-family:宋体">首先，</span>Client通过访问ZK来请求目标数据的地址。</p> 
<p>2. ZK中保存了-ROOT-表的地址，所以ZK通过访问-ROOT-表来请求数据地址。</p> 
<p>3. <span style="font-family:宋体">同样，</span>-ROOT-表中保存的是.META.的信息，通过访问.META.表来获取具体的RS。</p> 
<p>4. .META.表查询到具体RS信息后返回具体RS地址给Client。</p> 
<p>5. Client端获取到目标地址后，然后直接向该地址发送数据请求。</p> 
<p> </p> 
<p> </p> 
<h4><strong>6-8<span style="font-family:宋体">）</span><span style="font-family:Calibri">hbase</span><span style="font-family:宋体">宕机了如何处理？</span></strong></h4> 
<p><span style="color:rgb(51,51,51)">HBase<span style="font-family:宋体">的</span><span style="font-family:Arial">RegionServer</span><span style="font-family:宋体">宕机超过一定时间后，</span><span style="font-family:Arial">HMaster</span><span style="font-family:宋体">会将其所管理的</span><span style="font-family:Arial">region</span><span style="font-family:宋体">重新分布到其他活动的</span><span style="font-family:Arial">RegionServer</span><span style="font-family:宋体">上，由于数据和日志都持久在</span><span style="font-family:Arial">HDFS</span><span style="font-family:宋体">中，</span></span><span style="color:rgb(51,51,51)"><br> </span><span style="color:rgb(51,51,51)"><span style="font-family:宋体">该操作不会导致数据丢失。</span></span><span style="color:rgb(12,12,12)"><span style="font-family:宋体">所以数据的一致性和安全性是有保障的。</span></span><span style="color:rgb(51,51,51)"><br> </span><span style="color:rgb(51,51,51)"><span style="font-family:宋体">但是重新分配的</span>region<span style="font-family:宋体">需要根据日志恢复原</span><span style="font-family:Arial">RegionServer</span><span style="font-family:宋体">中的内存</span><span style="font-family:Arial">MemoryStore</span><span style="font-family:宋体">表，这会导致宕机的</span><span style="font-family:Arial">region</span><span style="font-family:宋体">在这段时间内无法对外提供服务。</span></span><span style="color:rgb(51,51,51)"><br> </span><span style="color:rgb(51,51,51)"><span style="font-family:宋体">而一旦重分布，宕机的节点重新启动后就相当于一个新的</span>RegionServer<span style="font-family:宋体">加入集群，为了平衡，需要再次将某些</span><span style="font-family:Arial">region</span><span style="font-family:宋体">分布到该</span><span style="font-family:Arial">server</span><span style="font-family:宋体">。</span><span style="font-family:Arial"> </span></span><span style="color:rgb(51,51,51)"><br> </span><span style="color:rgb(51,51,51)"><span style="font-family:宋体">因此，</span>Region Server<span style="font-family:宋体">的内存表</span><span style="font-family:Arial">memstore</span><span style="font-family:宋体">如何在节点间做到更高的可用，是</span><span style="font-family:Arial">HBase</span><span style="font-family:宋体">的一个较大的挑战。</span></span></p> 
<p><span style="color:rgb(51,51,51)"> </span></p> 
<h4><strong>6-9<span style="font-family:宋体">）</span><span style="font-family:Calibri">Hbase </span><span style="font-family:宋体">中的 </span><span style="font-family:Calibri">metastore </span><span style="font-family:宋体">用来做什么的？</span></strong></h4> 
<p>Hbase<span style="font-family:宋体">的</span><span style="font-family:Calibri">metastore</span><span style="font-family:宋体">是用来保存数据的，其中保存数据的方式有有三种第一种于第二种是本地储存，第二种是远程储存这一种企业用的比较多</span></p> 
<p> </p> 
<h4><strong>6-10)hbase<span style="font-family:宋体">客户端在客户端怎样优化？</span></strong></h4> 
<p>Hbase<span style="font-family:宋体">使用</span><span style="font-family:Calibri">JAVA</span><span style="font-family:宋体">来运算的，索引</span><span style="font-family:Calibri">Java</span><span style="font-family:宋体">的优化也适用于</span><span style="font-family:Calibri">hbase</span><span style="font-family:宋体">，在使用过滤器事记得开启</span><span style="font-family:Calibri">bloomfilter</span><span style="font-family:宋体">可以是性能提高</span><span style="font-family:Calibri">3-4</span><span style="font-family:宋体">倍，设置</span><span style="color:rgb(12,12,12)">HBASE_HEAPSIZE</span><span style="color:rgb(12,12,12)"><span style="font-family:宋体">设置大一些</span></span></p> 
<p> </p> 
<h4><strong>6-11<span style="font-family:宋体">）</span><span style="font-family:Calibri">hbase</span><span style="font-family:宋体">是怎样预分区的？</span></strong></h4> 
<p><span style="color:rgb(12,12,12)"><span style="font-family:宋体">如何去进行预分区，可以采用下面三步：</span></span><span style="color:rgb(12,12,12)"><br> </span><span style="color:rgb(12,12,12)">  1.取样，先随机生成一定数量的rowkey,将取样数据按升序排序放到一个集合里</span><span style="color:rgb(12,12,12)"><br> </span><span style="color:rgb(12,12,12)">  2.根据预分区的region个数，对整个集合平均分割，即是相关的splitKeys.</span><span style="color:rgb(12,12,12)"><br> </span><span style="color:rgb(12,12,12)"> </span><span style="color:rgb(12,12,12)"> </span><span style="color:rgb(12,12,12)">3.HBaseAdmin.createTable(HTableDescriptor tableDescriptor,byte[][] splitkeys)可以指定预分区的splitKey，即是指定region间的rowkey临界值</span></p> 
<h4><strong>6-12<span style="font-family:宋体">）怎样将 </span><span style="font-family:Calibri">mysql </span><span style="font-family:宋体">的数据导入到 </span><span style="font-family:Calibri">hbase </span><span style="font-family:宋体">中？</span></strong></h4> 
<p><span style="color:rgb(12,12,12)"><span style="font-family:宋体">不能使用</span> sqoop，速度太慢了，提示如下：</span></p> 
<p><span style="color:rgb(12,12,12)">A、一种可以加快批量写入速度的方法是通过预先创建一些空的 regions，这样当</span></p> 
<p><span style="color:rgb(12,12,12)"><span style="font-family:宋体">数据写入</span> HBase 时，会按照 region 分区情况，在集群内做数据的负载均衡。</span></p> 
<p><span style="color:rgb(12,12,12)">B、hbase 里面有这样一个 </span><span style="color:rgb(255,0,0)">hfileoutputformat </span><span style="color:rgb(12,12,12)"><span style="font-family:宋体">类，他的实现可以将数据转换成</span> hfile</span></p> 
<p><span style="color:rgb(12,12,12)"><span style="font-family:宋体">格式，通过</span> new 一个这个类，进行相关配置,这样会在 hdfs 下面产生一个文件，这个</span></p> 
<p><span style="color:rgb(12,12,12)"><span style="font-family:宋体">时候利用</span> hbase 提供的 jruby 的 loadtable.rb 脚本就可以进行批量导入。</span></p> 
<p><span style="color:rgb(12,12,12)"> </span></p> 
<h4><strong>6-13<span style="font-family:宋体">）谈谈 </span><span style="font-family:Calibri">HBase </span><span style="font-family:宋体">集群安装注意事项？</span></strong></h4> 
<p>     <span style="font-family:宋体">需要注意的地方是</span> ZooKeeper <span style="font-family:宋体"> 的配置。这与 </span><span style="font-family:Calibri">hbase-env.sh </span><span style="font-family:宋体">文件相关，文件中</span></p> 
<p>HBASE_MANAGES_ZK <span style="font-family:宋体">环境变量用来设置是使用 </span><span style="font-family:Calibri">hbase </span><span style="font-family:宋体">默认自带的 </span><span style="font-family:Calibri">Zookeeper </span><span style="font-family:宋体">还</span></p> 
<p><span style="font-family:宋体">是使用独立的</span> ZooKeeper<span style="font-family:宋体">。</span><span style="font-family:Calibri">HBASE_MANAGES_ZK=false </span><span style="font-family:宋体">时使用独立的，为 </span><span style="font-family:Calibri">true </span><span style="font-family:宋体">时</span></p> 
<p>使用默认自带的。</p> 
<p>    <span style="font-family:宋体">某个节点的</span> HRegionServer <span style="font-family:宋体"> 启动失败，这是由于这 </span><span style="font-family:Calibri">3 </span><span style="font-family:宋体">个节点的系统时间不一致相</span></p> 
<p><span style="font-family:宋体">差超过集群的检查时间</span> 30s<span style="font-family:宋体">。</span></p> 
<h4><strong>6-14)<span style="font-family:宋体">简述 </span><span style="font-family:Calibri">HBase </span><span style="font-family:宋体">的瓶颈</span></strong></h4> 
<p>Hbase<span style="font-family:宋体">主要的瓶颈就是传输问题，在操作时大部分的操作都是需要对磁盘操作的</span></p> 
<p> </p> 
<h4><strong>6-15<span style="font-family:宋体">）</span><span style="font-family:Calibri">Redis, </span><span style="font-family:宋体">传统数据库</span><span style="font-family:Calibri">,hbase,hive  </span><span style="font-family:宋体">每个之间的区别</span></strong></h4> 
<p>Redis <span style="font-family:宋体">是基于内存的数据库，注重实用内存的计算，</span><span style="font-family:Calibri">hbase</span><span style="font-family:宋体">是列式数据库，无法创建主键，地从是基于</span><span style="font-family:Calibri">HDFS</span><span style="font-family:宋体">的，每一行可以保存很多的列，</span><span style="font-family:Calibri">hive</span><span style="font-family:宋体">是数据的仓库，是为了减轻</span><span style="font-family:Calibri">mapreduce</span><span style="font-family:宋体">而设计的，不是数据库是用来与红薯做交互的。</span></p> 
<p> </p> 
<h4><strong>6-16<span style="font-family:宋体">）</span><span style="font-family:Calibri">Hbase </span><span style="font-family:宋体">的特性</span><span style="font-family:Calibri">,</span><span style="font-family:宋体">以及你怎么去设计 </span><span style="font-family:Calibri">rowkey </span><span style="font-family:宋体">和 </span><span style="font-family:Calibri">columnFamily ,</span><span style="font-family:宋体">怎么去建一个 </span><span style="font-family:Calibri">table</span></strong></h4> 
<p><span style="font-family:宋体">因为</span>hbase<span style="font-family:宋体">是列式数据库，列非表</span><span style="font-family:Calibri">schema</span><span style="font-family:宋体">的一部分，所以只需要考虑</span><span style="font-family:Calibri">rowkey</span><span style="font-family:宋体">和</span><span style="font-family:Calibri">columnFamily </span><span style="font-family:宋体">即可，</span><span style="font-family:Calibri">rowkey</span><span style="font-family:宋体">有为的相关性，最好数据库添加一个前缀，文件越小，查询速度越快，再设计列是有一个列簇，但是列簇不宜过多。</span></p> 
<p> </p> 
<p> </p> 
<h4><strong>6-17<span style="font-family:宋体">）</span><span style="font-family:Calibri">Hhase</span><span style="font-family:宋体">与</span><span style="font-family:Calibri">hive</span><span style="font-family:宋体">的区别</span></strong></h4> 
<p>     Apache HBase<span style="font-family:宋体">是一种</span><span style="font-family:Calibri">Key/Value</span><span style="font-family:宋体">系统，它运行在</span><span style="font-family:Calibri">HDFS</span><span style="font-family:宋体">之上。和</span><span style="font-family:Calibri">Hive</span><span style="font-family:宋体">不一样，</span><span style="font-family:Calibri">Hbase</span><span style="font-family:宋体">的能够在它的数据库上实时运行，而不是运行</span><span style="font-family:Calibri">MapReduce</span><span style="font-family:宋体">任务。</span><span style="font-family:Calibri">Hive</span><span style="font-family:宋体">被分区为表格，表格又被进一步分割为列簇。列簇必须使用</span><span style="font-family:Calibri">schema</span><span style="font-family:宋体">定义，列簇将某一类型列集合起来（列不要求</span><span style="font-family:Calibri">schema</span><span style="font-family:宋体">定义）。例如，“</span><span style="font-family:Calibri">message</span><span style="font-family:宋体">”列簇可能包含：“</span><span style="font-family:Calibri">to</span><span style="font-family:宋体">”</span><span style="font-family:Calibri">, </span><span style="font-family:宋体">”</span><span style="font-family:Calibri">from</span><span style="font-family:宋体">” “</span><span style="font-family:Calibri">date</span><span style="font-family:宋体">”</span><span style="font-family:Calibri">, </span><span style="font-family:宋体">“</span><span style="font-family:Calibri">subject</span><span style="font-family:宋体">”</span><span style="font-family:Calibri">, </span><span style="font-family:宋体">和”</span><span style="font-family:Calibri">body</span><span style="font-family:宋体">”</span><span style="font-family:Calibri">. </span><span style="font-family:宋体">每一个 </span><span style="font-family:Calibri">key/value</span><span style="font-family:宋体">对在</span><span style="font-family:Calibri">Hbase</span><span style="font-family:宋体">中被定义为一个</span><span style="font-family:Calibri">cell</span><span style="font-family:宋体">，每一个</span><span style="font-family:Calibri">key</span><span style="font-family:宋体">由</span><span style="font-family:Calibri">row-key</span><span style="font-family:宋体">，列簇、列和时间戳。在</span><span style="font-family:Calibri">Hbase</span><span style="font-family:宋体">中，行是</span><span style="font-family:Calibri">key/value</span><span style="font-family:宋体">映射的集合，这个映射通过</span><span style="font-family:Calibri">row-key</span><span style="font-family:宋体">来唯一标识。</span><span style="font-family:Calibri">Hbase</span><span style="font-family:宋体">利用</span><span style="font-family:Calibri">Hadoop</span><span style="font-family:宋体">的基础设施，可以利用通用的设备进行水平的扩展。</span></p> 
<p><span style="color:rgb(12,12,12)"> </span></p> 
<h4><strong>6-18<span style="font-family:宋体">）</span><span style="font-family:宋体">描述</span>hbase<span style="font-family:宋体">的</span><span style="font-family:Calibri">scan</span><span style="font-family:宋体">和</span><span style="font-family:Calibri">get</span><span style="font-family:宋体">功能以及实现的异同</span></strong></h4> 
<p> </p> 
<p>      HBase<span style="font-family:宋体">的查询实现只提供两种方式： </span><span style="font-family:Calibri">1</span><span style="font-family:宋体">、按指定</span><span style="font-family:Calibri">RowKey</span><span style="font-family:宋体">获取唯一一条记录，</span><span style="font-family:Calibri">get</span><span style="font-family:宋体">方法（</span><span style="font-family:Calibri">org.apache.hadoop.hbase.client.Get</span><span style="font-family:宋体">） </span><span style="font-family:Calibri">2</span><span style="font-family:宋体">、按指定的条件获取一批记录，</span><span style="font-family:Calibri">scan</span><span style="font-family:宋体">方法（</span><span style="font-family:Calibri">org.apache.hadoop.hbase.client.Scan</span><span style="font-family:宋体">） 实现条件查询功能使用的就是</span><span style="font-family:Calibri">scan</span><span style="font-family:宋体">方式</span></p> 
<p> </p> 
<h4><strong>6-19<span style="font-family:宋体">）</span></strong><a target="_blank" href="http://blog.csdn.net/caoli98033/article/details/44650497" rel="noopener noreferrer"><span style="color:rgb(0,0,0)">HBase scan setBatch和setCaching的区别</span></a><strong></strong></h4> 
<p>      can可以通过setCaching与setBatch方法提高速度（以空间换时间），</p> 
<p>setCaching设置的值为每次rpc的请求记录数，默认是1；cache大可以优化性能，但是太大了会花费很长的时间进行一次传输。</p> 
<p>   setBatch设置每次取的column size；有些row特别大，所以需要分开传给client，就是一次传一个row的几个column。</p> 
<h4><strong>6-20<span style="font-family:宋体">）</span><span style="font-family:Calibri">hbase </span><span style="font-family:宋体">中</span><span style="font-family:Calibri">cell</span><span style="font-family:宋体">的结构</span></strong></h4> 
<p><span style="color:rgb(51,51,51)">cell<span style="font-family:宋体">中的数据是没有类型的，全部是字节码形式存贮。</span></span></p> 
<p><span style="color:rgb(51,51,51)"> </span></p> 
<h4><strong>6-21<span style="font-family:宋体">）</span><span style="font-family:Calibri">hbase </span><span style="font-family:宋体">中</span><span style="font-family:Calibri">region</span><span style="font-family:宋体">太多和</span><span style="font-family:Calibri">region</span><span style="font-family:宋体">太大带来的冲突</span></strong></h4> 
<p><span style="color:rgb(51,51,51)">H</span><span style="color:rgb(51,51,51)">base<span style="font-family:宋体">的</span><span style="font-family:Arial">region</span><span style="font-family:宋体">会自动</span><span style="font-family:Arial">split</span><span style="font-family:宋体">，当</span><span style="font-family:Arial">region</span><span style="font-family:宋体">太时，</span><span style="font-family:Arial">regio</span><span style="font-family:宋体">太大时</span></span><span style="color:rgb(51,51,51)"><span style="font-family:sans-serif">分布</span></span><span style="color:rgb(51,51,51)"><span style="font-family:宋体">会</span></span><span style="color:rgb(51,51,51)"><span style="font-family:sans-serif">不均衡</span></span><span style="color:rgb(51,51,51)"><span style="font-family:宋体">，同时对于大批量的代入数据建议如下：</span></span></p> 
<p><span style="color:rgb(51,51,51)">1、</span><span style="color:rgb(51,51,51)"><span style="font-family:宋体">还是必须让业务方对</span>rowkey<span style="font-family:宋体">进行预分片，对业务数据</span><span style="font-family:sans-serif">rowkey</span><span style="font-family:宋体">进行</span><span style="font-family:sans-serif">md5</span><span style="font-family:宋体">或者其他的</span><span style="font-family:sans-serif">hash</span><span style="font-family:宋体">策略，让数据尽量随机分布而不是顺序写入。</span></span></p> 
<p><span style="color:rgb(51,51,51)">2<span style="font-family:宋体">、随时观察</span><span style="font-family:sans-serif">region</span><span style="font-family:宋体">的大小，是否出现大</span><span style="font-family:sans-serif">region</span><span style="font-family:宋体">的情况。</span></span></p> 
<h3><strong>Flume<span style="font-family:黑体">相关</span></strong></h3> 
<h4><strong>7-1<span style="font-family:宋体">）</span><span style="font-family:Calibri">flume </span><span style="font-family:宋体">不采集 </span><span style="font-family:Calibri">Nginx </span><span style="font-family:宋体">日志，通过 </span><span style="font-family:Calibri">Logger4j </span><span style="font-family:宋体">采集日志，优缺点是什么？</span></strong></h4> 
<p><span style="font-family:宋体">在</span>nginx<span style="font-family:宋体">采集日志时无法获取</span><span style="font-family:Calibri">session</span><span style="font-family:宋体">的信息，然而</span><span style="font-family:Calibri">logger4j</span><span style="font-family:宋体">则可以获取</span><span style="font-family:Calibri">session</span><span style="font-family:宋体">的信息，</span><span style="font-family:Calibri">logger4j</span><span style="font-family:宋体">的方式比较稳定，不会宕机。缺点：不够灵活，</span><span style="font-family:Calibri">logger4j</span><span style="font-family:宋体">的方式和项目结合过滤紧密，二</span><span style="font-family:Calibri">flume</span><span style="font-family:宋体">的方式就比较灵活，便于插拔式比较好，不会影响项目的性能。</span></p> 
<p> </p> 
<h4><strong>7-2<span style="font-family:宋体">）</span><span style="font-family:Calibri">flume </span><span style="font-family:宋体">和 </span><span style="font-family:Calibri">kafka </span><span style="font-family:宋体">采集日志区别，采集日志时中间停了，怎么记录之前的日志。</span></strong></h4> 
<p>Flume <span style="font-family:宋体">采集日志是通过流的方式直接将日志收集到存储层，而 </span><span style="font-family:Calibri">kafka </span><span style="font-family:宋体">试讲日志缓存在 </span><span style="font-family:Calibri">kafka</span></p> 
<p><span style="font-family:宋体">集群，待后期可以采集到存储层。</span>Flume <span style="font-family:宋体"> 采集中间停了，可以采用文件的方式记录之前的日志，而 </span><span style="font-family:Calibri">kafka </span><span style="font-family:宋体">是采用 </span><span style="font-family:Calibri">offset(</span><span style="font-family:宋体">偏移量</span><span style="font-family:Calibri">) </span><span style="font-family:宋体">的方式记录之前的日志。</span></p> 
<p> </p> 
<h3><strong>Kafka <span style="font-family:黑体">相关</span></strong></h3> 
<h4><strong>8-1<span style="font-family:宋体">）</span><span style="font-family:Calibri">kafka </span><span style="font-family:宋体">中怎样储存数据，哟及结构的，</span><span style="font-family:Calibri">data.....</span><span style="font-family:宋体">目录下有多少个分区，每个分区的存储格式是什么样的？</span></strong></h4> 
<p>1<span style="font-family:宋体">、</span><span style="font-family:Calibri">topic </span> <span style="font-family:宋体">是按照“主题名</span><span style="font-family:Calibri">-</span><span style="font-family:宋体">分区”存储的</span></p> 
<p>2<span style="font-family:宋体">、分区个数由配置文件决定</span></p> 
<p>3<span style="font-family:宋体">、每个分区下最重要的两个文件是 </span><span style="font-family:Calibri">0000000000.log </span><span style="font-family:宋体">和 </span><span style="font-family:Calibri">000000.index</span><span style="font-family:宋体">，</span><span style="font-family:Calibri">0000000.log</span></p> 
<p><span style="font-family:宋体">以默认</span> 1G <span style="font-family:宋体">大小回滚。</span></p> 
<p> </p> 
<p> </p> 
<h3><strong>Spark <span style="font-family:黑体">相关</span></strong></h3> 
<h4><strong>9-1<span style="font-family:宋体">）</span><span style="font-family:Calibri">mr </span><span style="font-family:宋体">和 </span><span style="font-family:Calibri">spark </span><span style="font-family:宋体">区别，怎么理解 </span><span style="font-family:Calibri">spark-rdd</span></strong></h4> 
<p>     Mr <span style="font-family:宋体">是文件方式的分布式计算框架，是将中间结果和最终结果记录在文件中，</span><span style="font-family:Calibri">map </span><span style="font-family:宋体">和 </span><span style="font-family:Calibri">reduce</span><span style="font-family:宋体">的数据分发也是在文件中。</span></p> 
<p> </p> 
<p>spark <span style="font-family:宋体">是内存迭代式的计算框架，计算的中间结果可以缓存内存，也可以缓存硬盘，但是不是每一步计算都需要缓存的。</span></p> 
<p> </p> 
<p>Spark-rdd <span style="font-family:宋体">是一个数据的分区记录集合，是利用内存来计算的，</span><span style="font-family:Calibri">spark</span><span style="font-family:宋体">之所以快是因为有内存的模式</span></p> 
<p> </p> 
<h4><strong>9-2<span style="font-family:宋体">）简单描述</span><span style="font-family:Calibri">spark</span><span style="font-family:宋体">的</span><span style="font-family:Calibri">wordCount</span><span style="font-family:宋体">的执行过程</span></strong></h4> 
<p>scala&gt; sc.textFile("/usr/local/words.txt")</p> 
<p>res0: org.apache.spark.rdd.RDD[String] = /usr/local/words.txt MapPartitionsRDD[1] at textFile at &lt;console&gt;:22</p> 
<p> </p> 
<p>scala&gt; sc.textFile("/usr/local/words.txt").flatMap(_.split(" "))</p> 
<p>res2: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at flatMap at &lt;console&gt;:22</p> 
<p> </p> 
<p>scala&gt; sc.textFile("/usr/local/words.txt").flatMap(_.split(" ")).map((_,1))</p> 
<p>res3: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[8] at map at &lt;console&gt;:22</p> 
<p> </p> 
<p>scala&gt; sc.textFile("/usr/local/words.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_)</p> 
<p>res5: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[17] at reduceByKey at &lt;console&gt;:22</p> 
<p> </p> 
<p>scala&gt; sc.textFile("/usr/local/words.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect</p> 
<p>res6: Array[(String, Int)] = Array((dageda,1), (xiaoli,1), (hellow,4), (xisdsd,1), (xiaozhang,1))</p> 
<p> </p> 
<p> </p> 
<h4><strong>9-3<span style="font-family:宋体">）按照需求使用</span><span style="font-family:Calibri">spark</span><span style="font-family:宋体">编写一下程序</span></strong></h4> 
<p>A<span style="font-family:宋体">、当前文件</span><span style="font-family:Calibri">a.text</span><span style="font-family:宋体">的格式为，请统计每个单词出现的个数</span></p> 
<p>A,b,c,d</p> 
<p>B,b,f,e</p> 
<p>A,a,c,f</p> 
<p> </p> 
<p>sc.textFile(“/user/local/a.text”).flatMap(_.split(“,”)).map((_,1)).ReduceByKey(_+_).collect()</p> 
<p>或：</p> 
<p><strong><span style="color:rgb(127,0,85)">package</span></strong> cn.<span style="background:rgb(192,192,192)">bigdata</span></p> 
<p> </p> 
<p><strong><span style="color:rgb(127,0,85)">import</span></strong> org.apache.spark.SparkConf</p> 
<p><strong><span style="color:rgb(127,0,85)">import</span></strong> org.apache.spark.SparkContext</p> 
<p><strong><span style="color:rgb(127,0,85)">import</span></strong> org.apache.spark.rdd.RDD</p> 
<p> </p> 
<p><strong><span style="color:rgb(127,0,85)">object</span></strong> <span style="color:rgb(50,147,153)">Demo</span> {<!-- --></p> 
<p> </p> 
<p>  <span style="color:rgb(63,127,95)">/*</span></p> 
<p><span style="color:rgb(63,127,95)">a,b,c,d</span></p> 
<p><span style="color:rgb(63,127,95)">b,b,f,e</span></p> 
<p><span style="color:rgb(63,127,95)">a,a,c,f</span></p> 
<p><span style="color:rgb(63,127,95)">c,c,a,d</span></p> 
<p><span style="color:rgb(63,127,95)">   * 计算第四列每个元素出现的个数</span></p> 
<p><span style="color:rgb(63,127,95)">   */</span></p> 
<p>  <strong><span style="color:rgb(127,0,85)">def</span></strong> <span style="color:rgb(76,76,76)">main</span>(<span style="color:rgb(100,0,103)">args</span>: Array[<em><span style="color:rgb(50,147,153)">String</span></em>]): Unit = {<!-- --></p> 
<p>    <strong><span style="color:rgb(127,0,85)">val</span></strong> <span style="color:rgb(94,94,255)">conf</span>: SparkConf = <strong><span style="color:rgb(127,0,85)">new</span></strong> SparkConf().<span style="color:rgb(76,76,76)">setAppName</span>(<span style="color:rgb(42,0,255)">"demo"</span>).<span style="color:rgb(76,76,76)">setMaster</span>(<span style="color:rgb(42,0,255)">"local"</span>)</p> 
<p>    <strong><span style="color:rgb(127,0,85)">val</span></strong> <span style="color:rgb(94,94,255)">sc</span>: SparkContext = <strong><span style="color:rgb(127,0,85)">new</span></strong> SparkContext(<span style="color:rgb(94,94,255)">conf</span>)</p> 
<p>    <strong><span style="color:rgb(127,0,85)">val</span></strong> <span style="color:rgb(94,94,255)">data</span>: RDD[<em><span style="color:rgb(50,147,153)">String</span></em>] = <span style="color:rgb(94,94,255)">sc</span>.<span style="color:rgb(76,76,76)">textFile</span>(<span style="color:rgb(42,0,255)">"f://demo.txt"</span>)</p> 
<p>    <span style="color:rgb(63,127,95)">//数据切分</span></p> 
<p>    <strong><span style="color:rgb(127,0,85)">val</span></strong> <span style="color:rgb(94,94,255)">fourthData</span>: RDD[(<em><span style="color:rgb(50,147,153)">String</span></em>, Int)] = <span style="color:rgb(94,94,255)">data</span>.<span style="color:rgb(76,76,76)">map</span> { <span style="color:rgb(100,0,103)">x</span> =&gt;</p> 
<p>      <strong><span style="color:rgb(127,0,85)">val</span></strong> <span style="color:rgb(94,94,255)">arr</span>: Array[<em><span style="color:rgb(50,147,153)">String</span></em>] = <span style="color:rgb(100,0,103)">x</span>.<span style="color:rgb(76,76,76)">split</span>(<span style="color:rgb(42,0,255)">","</span>)</p> 
<p>      <strong><span style="color:rgb(127,0,85)">val</span></strong> <span style="color:rgb(94,94,255)">fourth</span>: <em><span style="color:rgb(50,147,153)">String</span></em> = <span style="color:rgb(94,94,255)"> arr</span>(<span style="color:rgb(196,140,255)">3</span>)</p> 
<p>      (<span style="color:rgb(94,94,255)">fourth</span>, <span style="color:rgb(196,140,255)"> 1</span>)</p> 
<p>    }</p> 
<p>    <strong><span style="color:rgb(127,0,85)">val</span></strong> <span style="color:rgb(94,94,255)">result</span>: RDD[(<em><span style="color:rgb(50,147,153)">String</span></em>, Int)] = <u><span style="color:rgb(94,94,255)">fourthData</span></u>.<span style="color:rgb(76,76,76)">reduceByKey</span>(_ <span style="color:rgb(76,76,76)">+</span> _);</p> 
<p>    <span style="color:rgb(76,76,76)">println</span>(<u><span style="color:rgb(94,94,255)">result</span>.<span style="color:rgb(76,76,76)">collect</span>()</u>.<span style="color:rgb(76,76,76)">toBuffer</span>)</p> 
<p>  }</p> 
<p>}</p> 
<p> </p> 
<p>B<span style="font-family:宋体">、</span><span style="font-family:Calibri">HDFS</span><span style="font-family:宋体">中有两个文件</span><span style="font-family:Calibri">a.text</span><span style="font-family:宋体">与</span><span style="font-family:Calibri">b.text,</span><span style="font-family:宋体">文件的格式为</span><span style="font-family:Calibri">(ip,username),</span><span style="font-family:宋体">如：</span><span style="font-family:Calibri">a.text,b.text</span></p> 
<p>a.text</p> 
<p>127.0.0.1  xiaozhang</p> 
<p>127.0.0.1  xiaoli</p> 
<p>127.0.0.2  wangwu</p> 
<p>127.0.0.3  lisi</p> 
<p> </p> 
<p>B.text</p> 
<p>127.0.0.4  lixiaolu</p> 
<p>127.0.0.5  lisi</p> 
<p> </p> 
<p><span style="font-family:宋体">每个文件至少有</span>1000<span style="font-family:宋体">万行，请用程序完成一下工作，</span></p> 
<p>1）<span style="font-family:宋体">每个文件的个子的</span>IP</p> 
<p>2)<span style="font-family:宋体">出现在</span>b.text<span style="font-family:宋体">而没有出现在</span><span style="font-family:Calibri">a.text</span><span style="font-family:宋体">的</span><span style="font-family:Calibri">IP</span></p> 
<p>3)<span style="font-family:宋体">每个</span>user<span style="font-family:宋体">出现的次数以及每个</span><span style="font-family:Calibri">user</span><span style="font-family:宋体">对应的</span><span style="font-family:Calibri">IP</span><span style="font-family:宋体">的个数</span></p> 
<p> </p> 
<p><strong>代码如下：</strong></p> 
<p>1<span style="font-family:宋体">）各个文件的</span><span style="font-family:Calibri">ip</span><span style="font-family:宋体">数</span></p> 
<p>package cn.bigdata</p> 
<p> </p> 
<p>import java.util.concurrent.Executors</p> 
<p> </p> 
<p>import org.apache.hadoop.conf.Configuration</p> 
<p>import org.apache.hadoop.fs.FileSystem</p> 
<p>import org.apache.hadoop.fs.LocatedFileStatus</p> 
<p>import org.apache.hadoop.fs.Path</p> 
<p>import org.apache.hadoop.fs.RemoteIterator</p> 
<p>import org.apache.spark.SparkConf</p> 
<p>import org.apache.spark.SparkContext</p> 
<p>import org.apache.spark.rdd.RDD</p> 
<p>import org.apache.spark.rdd.RDD.rddToPairRDDFunctions</p> 
<p> </p> 
<p>//<span style="font-family:宋体">各个文件的</span><span style="font-family:Calibri">ip</span><span style="font-family:宋体">数</span></p> 
<p>object Demo2 {<!-- --></p> 
<p> </p> 
<p>  val cachedThreadPool = Executors.newCachedThreadPool()</p> 
<p> </p> 
<p>  def main(args: Array[String]): Unit = {<!-- --></p> 
<p>    val conf: SparkConf = new SparkConf().setAppName("demo2").setMaster("local")</p> 
<p>    val sc: SparkContext = new SparkContext(conf)</p> 
<p>    val hdpConf: Configuration = new Configuration</p> 
<p>    val fs: FileSystem = FileSystem.get(hdpConf)</p> 
<p>    val listFiles: RemoteIterator[LocatedFileStatus] = fs.listFiles(new Path("f://txt/2/"), true)</p> 
<p>    while (listFiles.hasNext) {<!-- --></p> 
<p>      val fileStatus = listFiles.next</p> 
<p>      val pathName = fileStatus.getPath.getName</p> 
<p>      cachedThreadPool.execute(new Runnable() {<!-- --></p> 
<p>        override def run(): Unit = {<!-- --></p> 
<p>          println("=======================" + pathName)</p> 
<p>          analyseData(pathName, sc)</p> 
<p>        }</p> 
<p>      })</p> 
<p>    }</p> 
<p>  }</p> 
<p> </p> 
<p>  def analyseData(pathName: String, sc: SparkContext): Unit = {<!-- --></p> 
<p>    val data: RDD[String] = sc.textFile("f://txt/2/" + pathName)</p> 
<p>    val dataArr: RDD[Array[String]] = data.map(_.split(" "))</p> 
<p>    val ipAndOne: RDD[(String, Int)] = dataArr.map(x =&gt; {<!-- --></p> 
<p>      val ip = x(0)</p> 
<p>      (ip, 1)</p> 
<p>    })</p> 
<p>    val counts: RDD[(String, Int)] = ipAndOne.reduceByKey(_ + _)</p> 
<p>    val sortedSort: RDD[(String, Int)] = counts.sortBy(_._2, false)</p> 
<p>    sortedSort.saveAsTextFile("f://txt/3/" + pathName)</p> 
<p>  }</p> 
<p>}</p> 
<p> </p> 
<p>2<span style="font-family:宋体">）出现在</span><span style="font-family:Calibri">b.txt</span><span style="font-family:宋体">而没有出现在</span><span style="font-family:Calibri">a.txt</span><span style="font-family:宋体">的</span><span style="font-family:Calibri">ip</span></p> 
<p>package cn.bigdata</p> 
<p> </p> 
<p>import java.util.concurrent.Executors</p> 
<p> </p> 
<p>import org.apache.spark.SparkConf</p> 
<p>import org.apache.spark.SparkContext</p> 
<p>import org.apache.spark.rdd.RDD</p> 
<p> </p> 
<p>/*</p> 
<p> * <span style="font-family:宋体">出现在</span><span style="font-family:Calibri">b.txt</span><span style="font-family:宋体">而没有出现在</span><span style="font-family:Calibri">a.txt</span><span style="font-family:宋体">的</span><span style="font-family:Calibri">ip</span></p> 
<p> */</p> 
<p>object Demo3 {<!-- --></p> 
<p>  </p> 
<p>  val cachedThreadPool = Executors.newCachedThreadPool()</p> 
<p>  </p> 
<p>  def main(args: Array[String]): Unit = {<!-- --></p> 
<p>    val conf = new SparkConf().setAppName("Demo3").setMaster("local")</p> 
<p>    val sc = new SparkContext(conf)</p> 
<p>    val data_a = sc.textFile("f://txt/2/a.txt")</p> 
<p>    val data_b = sc.textFile("f://txt/2/b.txt")</p> 
<p>    val splitArr_a = data_a.map(_.split(" "))</p> 
<p>    val ip_a: RDD[String] = splitArr_a.map(x =&gt; x(0))</p> 
<p>    val splitArr_b = data_b.map(_.split(" "))</p> 
<p>    val ip_b: RDD[String] = splitArr_b.map(x =&gt; x(0))</p> 
<p>    val subRdd: RDD[String] = ip_b.subtract(ip_a)</p> 
<p>    subRdd.saveAsTextFile("f://txt/4/")</p> 
<p>  }</p> 
<p>}</p> 
<p> </p> 
<p>3<span style="font-family:宋体">）</span></p> 
<p>package cn.bigdata</p> 
<p> </p> 
<p>import org.apache.spark.SparkConf</p> 
<p>import org.apache.spark.SparkContext</p> 
<p>import org.apache.spark.rdd.RDD</p> 
<p>import scala.collection.mutable.Set</p> 
<p> </p> 
<p>/*</p> 
<p> * <span style="font-family:宋体">每个</span><span style="font-family:Calibri">user</span><span style="font-family:宋体">出现的次数以及每个</span><span style="font-family:Calibri">user</span><span style="font-family:宋体">对应的</span><span style="font-family:Calibri">ip</span><span style="font-family:宋体">数</span></p> 
<p> */</p> 
<p>object Demo4 {<!-- --></p> 
<p>  def main(args: Array[String]): Unit = {<!-- --></p> 
<p>    val conf = new SparkConf().setAppName("Demo4").setMaster("local")</p> 
<p>    val sc = new SparkContext(conf)</p> 
<p>    val data: RDD[String] = sc.textFile("f://txt/5/")</p> 
<p>    val lines = data.map(_.split(" "))</p> 
<p>    val userIpOne = lines.map(x =&gt; {<!-- --></p> 
<p>      val ip = x(0)</p> 
<p>      val user = x(1)</p> 
<p>      (user, (ip, 1))</p> 
<p>    })</p> 
<p> </p> 
<p>    val userListIpCount: RDD[(String, (Set[String], Int))] = userIpOne.combineByKey(</p> 
<p>      x =&gt; (Set(x._1), x._2),</p> 
<p>      (a: (Set[String], Int), b: (String, Int)) =&gt; {<!-- --></p> 
<p>        (a._1 + b._1, a._2 + b._2)</p> 
<p>      },</p> 
<p>      (m: (Set[String], Int), n: (Set[String], Int)) =&gt; {<!-- --></p> 
<p>        (m._1 ++ n._1, m._2 + n._2)</p> 
<p>      })</p> 
<p> </p> 
<p>    val result: RDD[String] = userListIpCount.map(x =&gt; {<!-- --></p> 
<p>      x._1 + ":userCount:" + x._2._2 + ",ipCount:" + x._2._1.size</p> 
<p>    })</p> 
<p> </p> 
<p>    println(result.collect().toBuffer)</p> 
<p> </p> 
<p>  }</p> 
<p>}</p> 
<p> </p> 
<h3><strong>Sqoop <span style="font-family:黑体">相关</span></strong></h3> 
<p>10-1<span style="font-family:宋体">）</span><span style="font-family:Calibri">sqoop</span><span style="font-family:宋体">在导入到</span><span style="font-family:Calibri">MySql</span><span style="font-family:宋体">数据库是怎样保证数据重复，如果重复了该怎么办？？</span></p> 
<p>在导入时在语句的后面加上一下命令作为节点：</p> 
<p><span style="color:rgb(192,0,0)">--incremental append \</span></p> 
<p><span style="color:rgb(192,0,0)">--check-column id \</span></p> 
<p><span style="color:rgb(192,0,0)">--last-value 1208</span></p> 
<p><span style="color:rgb(192,0,0)"> </span></p> 
<h3><strong>Redis <span style="font-family:黑体">相关</span></strong></h3> 
<h4><strong>10-1<span style="font-family:宋体">）</span><span style="font-family:Calibri">redis</span><span style="font-family:宋体">保存磁盘的时间</span></strong></h4> 
<p>#   Note: you can disable saving at all commenting all the "save" lines.</p> 
<p>#</p> 
<p>#   It is also possible to remove all the previously configured save</p> 
<p>#   points by adding a save directive with a single empty string argument</p> 
<p>#   like in the following example:</p> 
<p>#</p> 
<p>#   save ""</p> 
<p> </p> 
<p>save 900 1</p> 
<p>save 300 10</p> 
<p>save 60 10000</p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p><span style="color:rgb(192,0,0)"> </span></p> 
<h3><strong>环境配置</strong></h3> 
<h4><strong>1<span style="font-family:宋体">）你们的集群规模？</span></strong></h4> 
<p>这个得看个人在公司的规模，下面介绍一下我们公司的一些配置：</p> 
<p> </p> 
<p><span style="font-family:宋体">联想</span>System x3750  <span style="font-family:宋体">服务器，价格</span>3.5<span style="font-family:宋体">万，内存容量</span><span style="font-family:Calibri">32G</span><span style="font-family:宋体">，产品类型机架式，硬盘接口</span><span style="font-family:Calibri">SSD,CPU</span><span style="font-family:宋体">频率</span><span style="font-family:Calibri">2.6GH,CPU</span><span style="font-family:宋体">数量</span><span style="font-family:Calibri">2</span><span style="font-family:宋体">颗，三级缓存</span><span style="font-family:Calibri">15MB,cpu</span><span style="font-family:宋体">核心</span><span style="font-family:Calibri">6</span><span style="font-family:宋体">核，</span><span style="font-family:Calibri">cpu</span><span style="font-family:宋体">线程数</span><span style="font-family:Calibri">12</span><span style="font-family:宋体">线程</span><span style="font-family:Calibri">,</span><span style="font-family:宋体">最大内存支持</span><span style="font-family:Calibri">1.5T,</span><span style="font-family:宋体">网络是千兆网卡</span><span style="font-family:Calibri">,</span><span style="font-family:宋体">可插拔时硬盘接口</span><span style="font-family:Calibri">12</span><span style="font-family:宋体">个卡槽</span><span style="font-family:Calibri">,</span><span style="font-family:宋体">配置</span><span style="font-family:Calibri">1T</span><span style="font-family:宋体">的容量</span></p> 
<p> </p> 
<p><span style="font-family:宋体">详细：</span>http://detail.zol.com.cn/server/index1101243.shtml</p> 
<p></p> 
<p> </p> 
<p><span style="font-family:宋体">名字</span>                               <span style="font-family:宋体">软件</span>                     <span style="font-family:宋体">运行管理</span></p> 
<p>Hadoop1                           JDK,hadoop                namenode</p> 
<p>Hadoop2                           JDK,hadoop                namenode</p> 
<p>Hadoop3                           JDK,hadoop                secondaryNamenode</p> 
<p>Hadoop4                           JDK,hadoop                secondaryNamenode</p> 
<p>Hadoop5                           JDK,hadoop                datanode</p> 
<p>Hadoop6                           JDK,hadoop                datanode</p> 
<p>Hadoop7                           JDK,hadoop                datanode</p> 
<p>Hadoop8                           JDK,hadoop                datanode</p> 
<p>Hadoop9                           JDK,hadoop                datanode</p> 
<p>Hadoop10                          JDK,zookeeper,tomcat,mvn,kafka leader</p> 
<p>Hadoop11                          JDK,zookeeper,tomcat,mvn,kafka  follower</p> 
<p>Hadoop12                          JDK,zookeeper,tomcat,mvn,kafka  follower</p> 
<p>Hadoop13                          JDK,hive,mysql,svn,logstarh    hive,mysql,svn</p> 
<p>Hadoop14                          JDK,hbase,mysql<span style="font-family:宋体">备份        </span><span style="font-family:Calibri">datanode</span></p> 
<p>Hadoop15                          JDK,nginx,Log<span style="font-family:宋体">日志手机       </span><span style="font-family:Calibri">datanode</span></p> 
<p> </p> 
<p> </p> 
<p><span style="font-family:宋体">数据就是每天访问的</span>Log<span style="font-family:宋体">日志不是很大，有的时候大有的时候小的可怜</span></p> 
<p> </p> 
<h4><strong>2)<span style="font-family:宋体">你在项目中遇到了哪些难题，是怎么解决的？</span></strong></h4> 
<p>1、在执行任务时发现副本的个数不对，经过一番的查找发现是超时的原因，修改了配置文件<span style="color:rgb(51,51,51)">hdfs-site.xml<span style="font-family:宋体">：</span></span><span style="color:rgb(51,51,51)"><span style="font-family:宋体">中修改了超时时间。</span></span></p> 
<p><span style="color:rgb(51,51,51)">2、</span><span style="color:rgb(51,51,51)"><span style="font-family:宋体">由于当时在分配各个目录空间大小时，没有很好的分配导致有的目录的空间浪费，于是整体商量后把储存的空间调大了一些。</span></span></p> 
<p align="justify"><span style="color:rgb(51,51,51)"> </span></p> 
<p align="justify"><span style="color:rgb(51,51,51)"> </span></p> 
<h3><strong>设计题</strong></h3> 
<p>1-1<span style="font-family:宋体">）采集</span><span style="font-family:Calibri">nginx</span><span style="font-family:宋体">产生的日志，日志的格式为</span><span style="font-family:Calibri">user  ip   time  url  </span> htmlId  <span style="font-family:宋体">每天产生的文件的数据量上亿条，请设计方案把数据保存到</span><span style="font-family:Calibri">HDFS</span><span style="font-family:宋体">上，并提供一下实时查询的功能（响应时间小于</span><span style="font-family:Calibri">3s</span><span style="font-family:宋体">）</span></p> 
<p>A、<span style="font-family:宋体">某个用户某天访问某个</span>URL<span style="font-family:宋体">的次数</span></p> 
<p>B、<span style="font-family:宋体">某个</span>URL<span style="font-family:宋体">某天被访问的总次数</span></p> 
<p align="justify"> </p> 
<p align="justify"><span style="font-family:宋体">实时思路是：使用</span>Logstash + Kafka + Spark-streaming + Redis + <span style="font-family:宋体">报表展示平台</span></p> 
<p align="justify"><span style="font-family:宋体">离线的思路是：</span>Logstash + Kafka + Elasticsearch +  Spark-streaming + <span style="font-family:宋体">关系型数据库</span></p> 
<p align="justify"> </p> 
<p align="justify">A、B<span style="font-family:宋体">、数据在进入到</span><span style="font-family:Calibri">Spark-streaming </span><span style="font-family:宋体">中进行过滤，把符合要求的数据保存到</span><span style="font-family:Calibri">Redis</span><span style="font-family:宋体">中</span></p> 
<p align="justify"> </p> 
<p align="justify"><span style="font-family:宋体"></span></p> 
<p align="justify"><span style="font-family:宋体"></span></p> 
<p align="justify"><span style="color:rgb(51,51,51)"></span></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/804eb50d3b507197c776d2a557c2b536/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Android Multimedia框架总结（八）Stagefright框架之AwesomePlayer及数据解析器</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/112e9e201dabc8d67ce6585a64136788/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【随记】数据库还原失败System.Data.SqlClient.SqlError: 无法执行 BACKUP LOG，因为当前没有数据库备份...</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>