<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>深度学习模型压缩方法综述（三） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="深度学习模型压缩方法综述（三）" />
<meta property="og:description" content="版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。 深度学习模型压缩方法综述（一） 深度学习模型压缩方法综述（二） 深度学习模型压缩方法综述（三）
前言 在前两章，我们介绍了一些在已有的深度学习模型的基础上，直接对其进行压缩的方法，包括核的稀疏化，和模型的裁剪两个方面的内容，其中核的稀疏化可能需要一些稀疏计算库的支持，其加速的效果可能受到带宽、稀疏度等很多因素的制约；而模型的裁剪方法则比较简单明了，直接在原有的模型上剔除掉不重要的filter，虽然这种压缩方式比较粗糙，但是神经网络的自适应能力很强，加上大的模型往往冗余比较多，将一些参数剔除之后，通过一些retraining的手段可以将由剔除参数而降低的性能恢复回来，因此只需要挑选一种合适的裁剪手段以及retraining方式，就能够有效的在已有模型的基础上对其进行很大程度的压缩，是目前使用最普遍的方法。然而除了这两种方法以外，本文还将为大家介绍另外两种方法：基于教师——学生网络、以及精细模型设计的方法。
基于教师——学生网络的方法 基于教师——学生网络的方法，属于迁移学习的一种。迁移学习也就是将一个模型的性能迁移到另一个模型上，而对于教师——学生网络，教师网络往往是一个更加复杂的网络，具有非常好的性能和泛化能力，可以用这个网络来作为一个soft target来指导另外一个更加简单的学生网络来学习，使得更加简单、参数运算量更少的学生模型也能够具有和教师网络相近的性能，也算是一种模型压缩的方式。
Distilling the Knowledge in a Neural Network 论文地址 较大、较复杂的网络虽然通常具有很好的性能，但是也存在很多的冗余信息，因此运算量以及资源的消耗都非常多。而所谓的Distilling就是将复杂网络中的有用信息提取出来迁移到一个更小的网络上，这样学习来的小网络可以具备和大的复杂网络想接近的性能效果，并且也大大的节省了计算资源。这个复杂的网络可以看成一个教师，而小的网络则可以看成是一个学生。 这个复杂的网络是提前训练好具有很好性能的网络，学生网络的训练含有两个目标：一个是hard target，即原始的目标函数，为小模型的类别概率输出与label真值的交叉熵；另一个为soft target，为小模型的类别概率输出与大模型的类别概率输出的交叉熵，在soft target中，概率输出的公式调整如下，这样当T值很大时，可以产生一个类别概率分布较缓和的输出： 作者认为，由于soft target具有更高的熵，它能比hard target提供更加多的信息，因此可以使用较少的数据以及较大的学习率。将hard和soft的target通过加权平均来作为学生网络的目标函数，soft target所占的权重更大一些。 作者同时还指出，T值取一个中间值时，效果更好，而soft target所分配的权重应该为T^2，hard target的权重为1。 这样训练得到的小模型也就具有与复杂模型近似的性能效果，但是复杂度和计算量却要小很多。 对于distilling而言，复杂模型的作用事实上是为了提高label包含的信息量。通过这种方法，可以把模型压缩到一个非常小的规模。模型压缩对模型的准确率没有造成太大影响，而且还可以应付部分信息缺失的情况。
Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer 论文地址 作者借鉴Distilling的思想，使用复杂网络中能够提供视觉相关位置信息的Attention map来监督小网络的学习，并且结合了低、中、高三个层次的特征，示意图如下： 教师网络从三个层次的Attention Transfer对学生网络进行监督。其中三个层次对应了ResNet中三组Residual Block的输出。在其他网络中可以借鉴。 这三个层次的Attention Transfer基于Activation，Activation Attention为feature map在各个通道上的值求和，基于Activation的Attention Transfer的损失函数如下： 其中Qs和Qt分别是学生网络和教师网络在不同层次的Activation向量，作者提出在这里在这里对Q进行标准化对于学生网络的训练非常重要。 除了基于Activation的Attention Transfer，作者还提出了一种Gradient Attention，它的损失函数如下： 但是就需要两次反向传播的过程，实现起来较困难并且效果提升不明显。 基于Activation的Attention Transfer效果较好，而且可以和Hinton的Distilling结合。 目前已有基于Activation的实现源码：https://github.com/szagoruyko/attention-transfer" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/1f37fc1bb12ff3a06881252cca7eed0c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-09-24T10:19:05+08:00" />
<meta property="article:modified_time" content="2019-09-24T10:19:05+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深度学习模型压缩方法综述（三）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-kimbie-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <article class="baidu_pl"> 
 <div id="article_content" class="article_content clearfix"> 
  <div class="article-copyright"> 
   <span class="creativecommons"> <a rel="nofollow" href="http://creativecommons.org/licenses/by-sa/4.0/"> </a>  版权声明：本文为博主原创文章，遵循<a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="noopener noopener noreferrer" target="_blank"> CC 4.0 BY-SA </a>版权协议，转载请附上原文出处链接和本声明。  </span> 
   <div class="article-source-link2222"> 
    <p><a href="http://blog.csdn.net/wspba/article/details/75671573" rel="nofollow">深度学习模型压缩方法综述（一）</a> <br> <a href="http://blog.csdn.net/wspba/article/details/75675554" rel="nofollow">深度学习模型压缩方法综述（二）</a> <br> <a href="http://blog.csdn.net/wspba/article/details/76039135" rel="nofollow">深度学习模型压缩方法综述（三）</a></p> 
   </div> 
  </div> 
 </div> 
</article> 
<h3 id="前言">前言</h3> 
<p>在前两章，我们介绍了一些在已有的深度学习模型的基础上，直接对其进行压缩的方法，包括核的稀疏化，和模型的裁剪两个方面的内容，其中核的稀疏化可能需要一些稀疏计算库的支持，其加速的效果可能受到带宽、稀疏度等很多因素的制约；而模型的裁剪方法则比较简单明了，直接在原有的模型上剔除掉不重要的filter，虽然这种压缩方式比较粗糙，但是神经网络的自适应能力很强，加上大的模型往往冗余比较多，将一些参数剔除之后，通过一些retraining的手段可以将由剔除参数而降低的性能恢复回来，因此只需要挑选一种合适的裁剪手段以及retraining方式，就能够有效的在已有模型的基础上对其进行很大程度的压缩，是目前使用最普遍的方法。然而除了这两种方法以外，本文还将为大家介绍另外两种方法：基于教师——学生网络、以及精细模型设计的方法。</p> 
<h3 id="基于教师学生网络的方法">基于教师——学生网络的方法</h3> 
<p>基于教师——学生网络的方法，属于迁移学习的一种。迁移学习也就是将一个模型的性能迁移到另一个模型上，而对于教师——学生网络，教师网络往往是一个更加复杂的网络，具有非常好的性能和泛化能力，可以用这个网络来作为一个soft target来指导另外一个更加简单的学生网络来学习，使得更加简单、参数运算量更少的学生模型也能够具有和教师网络相近的性能，也算是一种模型压缩的方式。</p> 
<ul><li><p>Distilling the Knowledge in a Neural Network <a href="https://arxiv.org/pdf/1503.02531.pdf" rel="nofollow">论文地址</a> <br> 较大、较复杂的网络虽然通常具有很好的性能，但是也存在很多的冗余信息，因此运算量以及资源的消耗都非常多。而所谓的Distilling就是将复杂网络中的有用信息提取出来迁移到一个更小的网络上，这样学习来的小网络可以具备和大的复杂网络想接近的性能效果，并且也大大的节省了计算资源。这个复杂的网络可以看成一个教师，而小的网络则可以看成是一个学生。 <br> <img src="https://images2.imgbox.com/f1/14/ByY21ZSX_o.png" alt="这里写图片描述" title=""> <br> 这个复杂的网络是提前训练好具有很好性能的网络，学生网络的训练含有两个目标：一个是hard target，即原始的目标函数，为小模型的类别概率输出与label真值的交叉熵；另一个为soft target，为小模型的类别概率输出与大模型的类别概率输出的交叉熵，在soft target中，概率输出的公式调整如下，这样当T值很大时，可以产生一个类别概率分布较缓和的输出： <br> <img src="https://images2.imgbox.com/27/6b/GMF3Z3YL_o.png" alt="这里写图片描述" title=""> <br> 作者认为，由于soft target具有更高的熵，它能比hard target提供更加多的信息，因此可以使用较少的数据以及较大的学习率。将hard和soft的target通过加权平均来作为学生网络的目标函数，soft target所占的权重更大一些。 作者同时还指出，T值取一个中间值时，效果更好，而soft target所分配的权重应该为T^2，hard target的权重为1。 这样训练得到的小模型也就具有与复杂模型近似的性能效果，但是复杂度和计算量却要小很多。 <br> 对于distilling而言，复杂模型的作用事实上是为了提高label包含的信息量。通过这种方法，可以把模型压缩到一个非常小的规模。模型压缩对模型的准确率没有造成太大影响，而且还可以应付部分信息缺失的情况。</p></li><li><p>Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer <a href="https://arxiv.org/abs/1612.03928" rel="nofollow">论文地址</a> <br> 作者借鉴Distilling的思想，使用复杂网络中能够提供视觉相关位置信息的Attention map来监督小网络的学习，并且结合了低、中、高三个层次的特征，示意图如下： <br> <img src="https://images2.imgbox.com/70/b2/yA1DssLE_o.png" alt="这里写图片描述" title=""> <br> 教师网络从三个层次的Attention Transfer对学生网络进行监督。其中三个层次对应了ResNet中三组Residual Block的输出。在其他网络中可以借鉴。 这三个层次的Attention Transfer基于Activation，Activation Attention为feature map在各个通道上的值求和，基于Activation的Attention Transfer的损失函数如下： <br> <img src="https://images2.imgbox.com/77/9b/rP2L7skK_o.png" alt="这里写图片描述" title=""> <br> 其中Qs和Qt分别是学生网络和教师网络在不同层次的Activation向量，作者提出在这里在这里对Q进行标准化对于学生网络的训练非常重要。 <br> 除了基于Activation的Attention Transfer，作者还提出了一种Gradient Attention，它的损失函数如下： <br> <img src="https://images2.imgbox.com/c7/f1/fMPevHLq_o.png" alt="这里写图片描述" title=""> <br> 但是就需要两次反向传播的过程，实现起来较困难并且效果提升不明显。 基于Activation的Attention Transfer效果较好，而且可以和Hinton的Distilling结合。 <br> 目前已有基于Activation的实现源码：<a href="https://github.com/szagoruyko/attention-transfer" rel="nofollow">https://github.com/szagoruyko/attention-transfer</a></p></li></ul> 
<h3 id="小结">小结</h3> 
<p>教师学生网络的方法，利用一个性能较好的教师网络，在神经元的级别上，来监督学生网络的训练，相当于提高了模型参数的利用率。其实可以这么来理解，我们一般训练一个神经网络就像是要爬一座山，你的target就是山顶的终点线，给了你一个目标，但是需要你自己去摸索如何找到通往终点的路，你就需要不断学习不断尝试，假如你的体力有效，可能就难以达到目标；但是如果这个时候有一个经验丰富的老司机，他以及达到过终点，那么他就可以为你指明一条上山的路，或者在路上给你立很多路标，你就只要沿着路标上山就好了，这样你也能够很容易的到达山顶，这就是教师——学生网络的意义。</p> 
<h3 id="基于精细模型设计的方法">基于精细模型设计的方法</h3> 
<p>上述几种方法都是在已有的性能较好模型的基础上，在保证模型性能的前提下尽可能的降低模型的复杂度以及运算量。除此之外，还有很多工作将注意力放在更小、更高效、更精细的网络模块设计上，如SqueezeNet的fire module，ResNet的Residual module，GoogLenet的Inception Module，它们基本都是由很小的卷积（1*1和3*3）组成，不仅参数运算量小，同时还具备了很好的性能效果。</p> 
<ul><li><p>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications <a href="https://arxiv.org/abs/1704.04861" rel="nofollow">论文地址</a> <br> 这篇论文是Google针对手机等嵌入式设备提出的一种轻量级的深层神经网络，取名为MobileNets。核心思想就是卷积核的巧妙分解，可以有效减少网络参数。所谓的卷积核分解，实际上就是将a × a × c分解成一个a × a × 1的卷积和一个1 ×1 × c的卷积，，其中a是卷积核大小，c是卷积核的通道数。其中第一个a × a × 1的卷积称为Depthwise Separable Convolutions，它对前一层输出的feature map的每一个channel单独进行a × a 的卷积来提取空间特征，然后再使用1 ×1 的卷积将多个通道的信息线性组合起来，称为Pointwise Convolutions，如下图： <br> <img src="https://images2.imgbox.com/d5/19/ClEj6JK2_o.png" alt="这里写图片描述" title=""> <br> 这样可以很大程度的压缩计算量： <br> <img src="https://images2.imgbox.com/93/b6/XszLiOBj_o.png" alt="这里写图片描述" title=""> <br> 其中DK为原始卷积核的大小，DF为输入feature map的尺寸， 这样相当于将运算量降低DK^2倍左右。 <br> MobileNet中Depthwise实际上是通过卷积中的group来实现的，其实在后面也会发现，这些精细模型的设计都是和group有关。本文的源码：<a href="https://github.com/shicai/MobileNet-Caffe" rel="nofollow">https://github.com/shicai/MobileNet-Caffe</a></p></li><li><p>Aggregated Residual Transformations for Deep Neural Networks <a href="https://arxiv.org/pdf/1611.05431.pdf" rel="nofollow">论文地址</a> <br> 作者提出，在传统的ResNet的基础上，以往的方法只往两个方向进行研究，一个深度，一个宽度，但是深度加深，模型训练难度更大，宽度加宽，模型复杂度更高，计算量更大，都在不同的程度上增加了资源的损耗，因此作者从一个新的维度：Cardinality（本文中应该为path的数量）来对模型进行考量，作者在ResNet的基础上提出了一种新的结构，ResNeXt： <br> <img src="https://images2.imgbox.com/64/97/FSL4StUG_o.png" alt="这里写图片描述" title=""> <br> 上图中两种结构计算量相近，但是右边结构的性能更胜一筹（Cardinality更大）。 <br> <img src="https://images2.imgbox.com/ce/c3/YTl62kZS_o.png" alt="这里写图片描述" title=""> <br> 以上三种结构等价，因此可以通过group的形式来实现ResNeXt。其实ResNeXt和mobilenet等结构性质很相近，都是通过group的操作，在维度相同时降低复杂度，或者在复杂度相同时增加维度，然后再通过1*1的卷积将所有通道的信息再融合起来。因此全文看下来，作者的核心创新点就在于提出了 aggregrated transformations，用一种平行堆叠相同拓扑结构的blocks代替原来 ResNet 的三层卷积的block，在不明显增加参数量级的情况下提升了模型的准确率，同时由于拓扑结构相同，超参数也减少了，便于模型移植。本文的源码：<a href="https://github.com/facebookresearch/ResNeXt" rel="nofollow">https://github.com/facebookresearch/ResNeXt</a></p></li><li><p>ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices <a href="https://arxiv.org/abs/1707.01083?context=cs.CV" rel="nofollow">论文地址</a> <br> 作者提出，虽然MobileNet、ResNeXt等网络能够大大的降低模型的复杂度，并且也能保持不错的性能，但是1 ×1卷积的计算消耗还是比较大的，比如在ResNeXt中，一个模块中1 ×1卷积就占据了93%的运算量，而在MobileNet中更是占到了94.86%，因此作者希望在这个上面进一步降低计算量：即在1 ×1的卷积上也采用group的操作，但是本来1 ×1本来是为了整合所有通道的信息，如果使用group的操作就无法达到这个效果，因此作者就想出了一种channel shuffle的方法，如下图： <br> <img src="https://images2.imgbox.com/81/2b/KjKebHcp_o.png" alt="这里写图片描述" title=""> <br> 如上图b和c，虽然对1 ×1的卷积使用了group的操作，但是在中间的feature map增加了一个channel shuffle的操作，这样每个group都可以接受到上一层不同group的feature，这样就可以很好的解决之前提到的问题，同时还降低了模型的计算量，ShuffleNet的模块如下： <br> <img src="https://images2.imgbox.com/f4/03/IYqHY1Xm_o.png" alt="这里写图片描述" title=""> <br> 作者使用了不同的group数进行实验，发现越小的模型，group数量越多性能越好。这是因为在模型大小一样的情况下，group数量越多，feature map的channel数越多，对于小的模型，channel数量对于性能提升更加重要。 <br> 最后作者将shufflenet的方法和mobilenet的方法进行了比较，性能似乎更胜一筹： <br> <img src="https://images2.imgbox.com/89/4f/2GEHhngn_o.png" alt="这里写图片描述" title=""> </p></li></ul> 
<h3 id="小结-1">小结</h3> 
<p>本节的三篇论文都是对精细模型的设计方法，直接搭建出参数运算量小的模型，适用于嵌入式平台，基本上都是通过group的方式来实现，确实能够很大程度的将模型的计算量降到最低，尤其是最后一篇shufflenet，实际上利用了一个特征融合的思路，在一个下模型中，分为更细的模型（group），再将这些更细的模型有效的融合起来，能够充分的利用参数，而达到更好的压缩效果。</p> 
<h3 id="结论">结论</h3> 
<p>目前关于深度学习模型压缩的方法有很多，本系列博文从四个角度来对模型压缩的方法进行了介绍，总的来说，所列出的文章和方法都具有非常强的借鉴性，值得我们去学习，效果也较明显。其中基于核稀疏化的方法，主要是在参数更新时增加额外的惩罚项，来诱导核的稀疏化，然后就可以利用裁剪或者稀疏矩阵的相关操作来实现模型的压缩；基于模型裁剪的方法，主要是对已训练好的网络进行压缩，往往就是寻找一种更加有效的评价方式，将不重要的参数剔除，以达到模型压缩的目的，这种压缩方法实现简单，尤其是regular的方式，裁剪效率最高，但是如何寻找一个最有效的评价方式是最重要的。基于迁移学习的方法，利用一个性能好的教师网络来监督学生网络进行学习，大大降低了简单网络学习到不重要信息的比例，提高了参数的利用效率，也是目前用的较多的方法。基于精细模型设计的方法，模型本身体积小，运行速度快，性能也不错，目前小的高效模型也开始广泛运用在各种嵌入式平台中。 <br> 总的来说，以上几种方法可以结合使用，比如说先对参数进行结构化的限制，使得参数裁剪起来更加容易，然后再选择合适的裁剪方法，考虑不同的评价标准以及裁剪策略，并在裁剪过程中充分考虑参数量、计算量、带宽等需求，以及不同硬件平台特性，在模型的性能、压缩、以及平台上的加速很好的进行权衡，才能够达到更好的效果。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/5bbb7f6a1340123c6870f06c75e9dfe7/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">深度学习模型压缩方法综述（二）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/8800956319185b67333ac04080fe04d6/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">css布局 三个按钮一排显示如何居中对齐</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>