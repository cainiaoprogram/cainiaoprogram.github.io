<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>CS224N_2019_Assignment3: Dependency Parsing (Solution) - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="CS224N_2019_Assignment3: Dependency Parsing (Solution)" />
<meta property="og:description" content="前言 A3作業讓你學會建立neural dependency parser的同時也能熟悉Pytorch的用法。
Written part是關於Adam和Dropout的解答與思考，這部分教授在課上解釋的比較少，但屬於neural network的重點之一，建議閱讀相關文獻加深這部分的理解。
Coding part是關於運用wrriten part的optimizer trick建立一個完整的simple neural net，並進行模型訓練。
題目詳情 – Written Part – #1. Machine Learning &amp; Neural Networks (8 points) Answer：
( a )
i. Using m updates the gradient by multiplying it by α(1-β) times, reducing the gradient even further than SGD.
ii. v will get larger updates since its calculation contains the power of the gradients. If v is larger than 1, the updated v will be larger; if v is smaller than 1, the updated v will become smaller." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/e191f93f10fe3b7b3e60af140a97e555/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-12-21T12:45:39+08:00" />
<meta property="article:modified_time" content="2019-12-21T12:45:39+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">CS224N_2019_Assignment3: Dependency Parsing (Solution)</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="_2"></a>前言</h3> 
<p>A3作業讓你學會建立neural dependency parser的同時也能熟悉Pytorch的用法。<br> Written part是關於Adam和Dropout的解答與思考，這部分教授在課上解釋的比較少，但屬於neural network的重點之一，建議閱讀相關文獻加深這部分的理解。<br> Coding part是關於運用wrriten part的optimizer trick建立一個完整的simple neural net，並進行模型訓練。</p> 
<h3><a id="_8"></a>題目詳情</h3> 
<h5><a id="_Written_Part__10"></a>– Written Part –</h5> 
<h4><a id="1_Machine_Learning__Neural_Networks_8_points_12"></a>#1. Machine Learning &amp; Neural Networks (8 points)</h4> 
<p>Answer：</p> 
<p>( a )<br> i. Using m updates the gradient by multiplying it by α(1-β) times, reducing the gradient even further than SGD.</p> 
<p>ii. v will get larger updates since its calculation contains the power of the gradients. If v is larger than 1, the updated v will be larger; if v is smaller than 1, the updated v will become smaller. This can help with learning by avoiding the learning rate being too large(exploding) or too small(vanishing) through the calculation of the division (√v).</p> 
<p>( b )<br> i. <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         γ 
        
       
         = 
        
        
        
          1 
         
         
         
           1 
          
         
           − 
          
          
          
            p 
           
           
           
             d 
            
           
             r 
            
           
             o 
            
           
             p 
            
           
          
         
        
       
      
        γ = \frac{1}{1-p_{drop}} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span style="margin-right: 0.05556em;" class="mord mathdefault">γ</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.39319em; vertical-align: -0.54808em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.845108em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3448em;"><span class="" style="top: -2.34877em; margin-left: 0em; margin-right: 0.0714286em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span style="margin-right: 0.02778em;" class="mord mathdefault mtight">r</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">p</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.290114em;"><span class=""></span></span></span></span></span></span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.54808em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>.<br> Since<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           h 
          
          
          
            d 
           
          
            r 
           
          
            o 
           
          
            p 
           
          
         
        
          = 
         
        
          γ 
         
        
          d 
         
        
          ⊙ 
         
        
          h 
         
        
       
         h_{drop} = γd⊙h 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.980548em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span style="margin-right: 0.02778em;" class="mord mathdefault mtight">r</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">p</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span style="margin-right: 0.05556em;" class="mord mathdefault">γ</span><span class="mord mathdefault">d</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">⊙</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathdefault">h</span></span></span></span></span></span><br> ∵<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          h 
         
         
         
           d 
          
         
           r 
          
         
           o 
          
         
           p 
          
         
        
       
         = 
        
       
         γ 
        
       
         ( 
        
       
         1 
        
       
         − 
        
        
        
          p 
         
         
         
           d 
          
         
           r 
          
         
           o 
          
         
           p 
          
         
        
       
         ) 
        
       
         ⊙ 
        
       
         h 
        
       
         = 
        
       
         h 
        
       
      
        h_{drop}=γ(1-p_{drop})⊙h=h 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.980548em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span style="margin-right: 0.02778em;" class="mord mathdefault mtight">r</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">p</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span style="margin-right: 0.05556em;" class="mord mathdefault">γ</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1.03611em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span style="margin-right: 0.02778em;" class="mord mathdefault mtight">r</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">p</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">⊙</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathdefault">h</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathdefault">h</span></span></span></span></span><br> ∴<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         γ 
        
       
         ( 
        
       
         1 
        
       
         − 
        
        
        
          p 
         
         
         
           d 
          
         
           r 
          
         
           o 
          
         
           p 
          
         
        
       
         ) 
        
       
         = 
        
       
         1 
        
       
      
        γ(1-p_{drop})=1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span style="margin-right: 0.05556em;" class="mord mathdefault">γ</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1.03611em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span style="margin-right: 0.02778em;" class="mord mathdefault mtight">r</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">p</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span></span></span></span></span><br> and we get <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         γ 
        
       
         = 
        
        
        
          1 
         
         
         
           1 
          
         
           − 
          
          
          
            p 
           
           
           
             d 
            
           
             r 
            
           
             o 
            
           
             p 
            
           
          
         
        
       
      
        γ = \frac{1}{1-p_{drop}} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span style="margin-right: 0.05556em;" class="mord mathdefault">γ</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.39319em; vertical-align: -0.54808em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.845108em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3448em;"><span class="" style="top: -2.34877em; margin-left: 0em; margin-right: 0.0714286em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span style="margin-right: 0.02778em;" class="mord mathdefault mtight">r</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">p</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.290114em;"><span class=""></span></span></span></span></span></span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.54808em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>.</p> 
<p>ii. Because dropout is a regularization form and used to prevent overfitting in order to achieve better generalization of the model.</p> 
<h4><a id="2_Neural_TransitionBased_Dependency_Parsing_42_points_35"></a>#2. Neural Transition-Based Dependency Parsing (42 points)</h4> 
<p>( a )</p> 
<table><thead><tr><th>Stack</th><th>Buffer</th><th>New Dependency</th><th>Transition</th></tr></thead><tbody><tr><td>[ROOT,parsed,this]</td><td>[sentence,correctly]</td><td></td><td>SHIFT</td></tr><tr><td>[ROOT,parsed,this,sentence]</td><td>[correctly]</td><td></td><td>SHIFT</td></tr><tr><td>[ROOT,parsed,sentence]</td><td>[correctly]</td><td>sentence-&gt;this</td><td>LEFT ARC</td></tr><tr><td>[ROOT,parsed]</td><td>[correctly]</td><td>parse-&gt;sentence</td><td>RIGHT ARC</td></tr><tr><td>[ROOT,parsed,correctly]</td><td></td><td></td><td>SHIFT</td></tr><tr><td>[ROOT,parsed]</td><td></td><td>parsed-&gt;correctly</td><td>RIGHT ARC</td></tr><tr><td>[ROOT]</td><td></td><td>ROOT-&gt;paresed</td><td>RIGHT ARC</td></tr></tbody></table> 
<p>( b )<br> 2n steps. For each dependency, there are two steps of transitions(SHIFT and LEFT ARC or RIGHT ARC)</p> 
<h5><a id="_Coding_Part__52"></a>– Coding Part –</h5> 
<p>( c )寫parser的transition機制。這裡需要注意的是sentence不允許有改動，sentence是list，所以要用到slicing/list()/copy function去copy list。關於不同的copy方式對比的參考：<a href="https://stackoverflow.com/questions/2612802/how-to-clone-or-copy-a-list" rel="nofollow">How to clone or copy a list? - stackoverflow</a></p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">PartialParse</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Initializes this partial parse.

        @param sentence (list of str): The sentence to be parsed as a list of words.
                                        Your code should not modify the sentence.
        """</span>
        <span class="token comment"># The sentence being parsed is kept for bookkeeping purposes. Do not alter it in your code.</span>
        self<span class="token punctuation">.</span>sentence <span class="token operator">=</span> sentence

        <span class="token comment">### YOUR CODE HERE (3 Lines)</span>

        self<span class="token punctuation">.</span>stack <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'ROOT'</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span><span class="token builtin">buffer</span> <span class="token operator">=</span> sentence<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span>  <span class="token comment">#Use slicing to copy lists. With self.buffer = sentence, the assignment just copies the reference to the list, not the actual list.</span>
        self<span class="token punctuation">.</span>dependencies <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

        <span class="token comment">### END YOUR CODE</span>


    <span class="token keyword">def</span> <span class="token function">parse_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> transition<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Performs a single parse step by applying the given transition to this partial parse

        @param transition (str): A string that equals "S", "LA", or "RA" representing the shift,
                                left-arc, and right-arc transitions. You can assume the provided
                                transition is a legal transition.
        """</span>
        <span class="token comment">### YOUR CODE HERE (~7-10 Lines)</span>

        <span class="token keyword">if</span> transition <span class="token operator">==</span> <span class="token string">'S'</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>stack<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span><span class="token builtin">buffer</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span><span class="token builtin">buffer</span><span class="token punctuation">.</span>remove<span class="token punctuation">(</span>self<span class="token punctuation">.</span><span class="token builtin">buffer</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">elif</span> transition <span class="token operator">==</span> <span class="token string">'LA'</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>dependencies<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>stack<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>stack<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>stack<span class="token punctuation">.</span>remove<span class="token punctuation">(</span>self<span class="token punctuation">.</span>stack<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">elif</span> transition <span class="token operator">==</span> <span class="token string">'RA'</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>dependencies<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>stack<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>stack<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>stack<span class="token punctuation">.</span>remove<span class="token punctuation">(</span>self<span class="token punctuation">.</span>stack<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        <span class="token comment">### END YOUR CODE</span>
</code></pre> 
<p>檢驗結果: <img src="https://images2.imgbox.com/bb/df/XNui2Cqq_o.png" alt="在这里插入图片描述">( d ) 根據PDF的algorithm1架構，寫一個parse sentence in minibatches的function。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">minibatch_parse</span><span class="token punctuation">(</span>sentences<span class="token punctuation">,</span> model<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Parses a list of sentences in minibatches using a model.

    @param sentences (list of list of str): A list of sentences to be parsed
                                            (each sentence is a list of words and each word is of type string)
    @param model (ParserModel): The model that makes parsing decisions. It is assumed to have a function
                                model.predict(partial_parses) that takes in a list of PartialParses as input and
                                returns a list of transitions predicted for each parse. That is, after calling
                                    transitions = model.predict(partial_parses)
                                transitions[i] will be the next transition to apply to partial_parses[i].
    @param batch_size (int): The number of PartialParses to include in each minibatch


    @return dependencies (list of dependency lists): A list where each element is the dependencies
                                                    list for a parsed sentence. Ordering should be the
                                                    same as in sentences (i.e., dependencies[i] should
                                                    contain the parse for sentences[i]).
    """</span>
    dependencies <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

    <span class="token comment">### YOUR CODE HERE (~8-10 Lines)</span>

    partial_parses <span class="token operator">=</span> <span class="token punctuation">[</span>PartialParse<span class="token punctuation">(</span>s<span class="token punctuation">)</span> <span class="token keyword">for</span> s <span class="token keyword">in</span> sentences<span class="token punctuation">]</span>  <span class="token comment"># for each sentence in sentences, apply PartialParse() to get partial_parses</span>
    unfinished_parses <span class="token operator">=</span> partial_parses<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">while</span> <span class="token builtin">len</span><span class="token punctuation">(</span>unfinished_parses<span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>
        minibatch <span class="token operator">=</span> unfinished_parses<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span>batch_size<span class="token punctuation">]</span>  <span class="token comment"># extract mini-batch</span>
        transitions <span class="token operator">=</span> model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>minibatch<span class="token punctuation">)</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>minibatch<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            minibatch<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>parse_step<span class="token punctuation">(</span>transitions<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># for each single unit in minibatch, apply parse_step() to its transition</span>
            <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>minibatch<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">buffer</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token operator">and</span> <span class="token builtin">len</span><span class="token punctuation">(</span>minibatch<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>stack<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
                unfinished_parses<span class="token punctuation">.</span>remove<span class="token punctuation">(</span>minibatch<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># remove the unit in mini batch after completion</span>

    dependencies<span class="token punctuation">.</span>extend<span class="token punctuation">(</span>p<span class="token punctuation">.</span>dependencies <span class="token keyword">for</span> p <span class="token keyword">in</span> partial_parses<span class="token punctuation">)</span>  <span class="token comment"># use dependencies of partial_parses directly because the minibatch is the shallow copy of partial_parses and both contains reference of the same objects.</span>

    <span class="token comment">### END YOUR CODE</span>

    <span class="token keyword">return</span> dependencies
</code></pre> 
<p>檢驗結果：<br> <img src="https://images2.imgbox.com/22/c6/rLA84UiE_o.png" alt="在这里插入图片描述">( e ) 搭nn的不同Layer，照著document做沒什麼難度。</p> 
<p>需要注意的是input size，根據PDF的解釋，先有parser提取出m個由a list of integer token構成的features，然後再找到每個詞對應的embedding，連接成一個 input vector： <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
         = 
        
       
         [ 
        
        
        
          E 
         
         
         
           w 
          
         
           1 
          
         
        
       
         , 
        
       
         . 
        
       
         . 
        
       
         . 
        
       
         , 
        
        
        
          E 
         
         
         
           w 
          
         
           m 
          
         
        
       
         ] 
        
       
      
        x = [E_{w1},...,E_{wm}] 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">[</span><span class="mord"><span style="margin-right: 0.05764em;" class="mord mathdefault">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.05764em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span style="margin-right: 0.02691em;" class="mord mathdefault mtight">w</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span style="margin-right: 0.05764em;" class="mord mathdefault">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: -0.05764em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span style="margin-right: 0.02691em;" class="mord mathdefault mtight">w</span><span class="mord mathdefault mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">]</span></span></span></span></span>，所以<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
      
        x 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault">x</span></span></span></span></span>的shape等於number of features(which is <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         m 
        
       
      
        m 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault">m</span></span></span></span></span>)*embedding size(which is the size of each <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          E 
         
        
          w 
         
        
       
      
        E_{w} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span style="margin-right: 0.05764em;" class="mord mathdefault">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: -0.05764em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span style="margin-right: 0.02691em;" class="mord mathdefault mtight">w</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>)。</p> 
<p>先寫好parser model.py：</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">ParserModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">""" Feedforward neural network with an embedding layer and single hidden layer.
    The ParserModel will predict which transition should be applied to a
    given partial parse configuration.

    PyTorch Notes:
        - Note that "ParserModel" is a subclass of the "nn.Module" class. In PyTorch all neural networks
            are a subclass of this "nn.Module".
        - The "__init__" method is where you define all the layers and their respective parameters
            (embedding layers, linear layers, dropout layers, etc.).
        - "__init__" gets automatically called when you create a new instance of your class, e.g.
            when you write "m = ParserModel()".
        - Other methods of ParserModel can access variables that have "self." prefix. Thus,
            you should add the "self." prefix layers, values, etc. that you want to utilize
            in other ParserModel methods.
        - For further documentation on "nn.Module" please see https://pytorch.org/docs/stable/nn.html.
    """</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> embeddings<span class="token punctuation">,</span> n_features<span class="token operator">=</span><span class="token number">36</span><span class="token punctuation">,</span>
        hidden_size<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">,</span> n_classes<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> dropout_prob<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">""" Initialize the parser model.

        @param embeddings (Tensor): word embeddings (num_words, embedding_size)
        @param n_features (int): number of input features
        @param hidden_size (int): number of hidden units
        @param n_classes (int): number of output classes
        @param dropout_prob (float): dropout probability
        """</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>ParserModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>n_features <span class="token operator">=</span> n_features
        self<span class="token punctuation">.</span>n_classes <span class="token operator">=</span> n_classes
        self<span class="token punctuation">.</span>dropout_prob <span class="token operator">=</span> dropout_prob
        self<span class="token punctuation">.</span>embed_size <span class="token operator">=</span> embeddings<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size
        self<span class="token punctuation">.</span>pretrained_embeddings <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>embeddings<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>embed_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>pretrained_embeddings<span class="token punctuation">.</span>weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>embeddings<span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token comment">### YOUR CODE HERE (~5 Lines)</span>

        self<span class="token punctuation">.</span>embed_to_hidden <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_features<span class="token operator">*</span>self<span class="token punctuation">.</span>embed_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span> <span class="token comment"># According to PDF, input shape equals number of features multiplies by embedding size.</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>embed_to_hidden<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> gain<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dropout_prob<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>hidden_to_logits <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_classes<span class="token punctuation">)</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_to_logits<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> gain<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token comment">### END YOUR CODE</span>

    <span class="token keyword">def</span> <span class="token function">embedding_lookup</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">""" Utilize `self.pretrained_embeddings` to map input `t` from input tokens (integers)
            to embedding vectors.

            PyTorch Notes:
                - `self.pretrained_embeddings` is a torch.nn.Embedding object that we defined in __init__
                - Here `t` is a tensor where each row represents a list of features. Each feature is represented by an integer (input token).
                - In PyTorch the Embedding object, e.g. `self.pretrained_embeddings`, allows you to
                    go from an index to embedding. Please see the documentation (https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding)
                    to learn how to use `self.pretrained_embeddings` to extract the embeddings for your tensor `t`.

            @param t (Tensor): input tensor of tokens (batch_size, n_features)

            @return x (Tensor): tensor of embeddings for words represented in t
                                (batch_size, n_features * embed_size)
        """</span>
        <span class="token comment">### YOUR CODE HERE (~1-3 Lines)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pretrained_embeddings<span class="token punctuation">(</span>t<span class="token punctuation">)</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># resize x into 2 dimensions.</span>

        <span class="token comment">### END YOUR CODE</span>
        <span class="token keyword">return</span> x


    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">""" Run the model forward.

            Note that we will not apply the softmax function here because it is included in the loss function nn.CrossEntropyLoss

            PyTorch Notes:
                - Every nn.Module object (PyTorch model) has a `forward` function.
                - When you apply your nn.Module to an input tensor `t` this function is applied to the tensor.
                    For example, if you created an instance of your ParserModel and applied it to some `t` as follows,
                    the `forward` function would called on `t` and the result would be stored in the `output` variable:
                        model = ParserModel()
                        output = model(t) # this calls the forward function
                - For more details checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward

        @param t (Tensor): input tensor of tokens (batch_size, n_features)

        @return logits (Tensor): tensor of predictions (output after applying the layers of the network)
                                 without applying softmax (batch_size, n_classes)
        """</span>
        <span class="token comment">###  YOUR CODE HERE (~3-5 lines)</span>

        e <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding_lookup<span class="token punctuation">(</span>t<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>embed_to_hidden<span class="token punctuation">(</span>e<span class="token punctuation">)</span>
        h <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        d <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>h<span class="token punctuation">)</span>
        logits <span class="token operator">=</span> self<span class="token punctuation">.</span>hidden_to_logits<span class="token punctuation">(</span>d<span class="token punctuation">)</span>

        <span class="token comment">### END YOUR CODE</span>
        <span class="token keyword">return</span> logits
</code></pre> 
<p>再寫好run.py：</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>parser<span class="token punctuation">,</span> train_data<span class="token punctuation">,</span> dev_data<span class="token punctuation">,</span> output_path<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">,</span> n_epochs<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.0005</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">""" Train the neural dependency parser.

    @param parser (Parser): Neural Dependency Parser
    @param train_data ():
    @param dev_data ():
    @param output_path (str): Path to which model weights and results are written.
    @param batch_size (int): Number of examples in a single batch
    @param n_epochs (int): Number of training epochs
    @param lr (float): Learning rate
    """</span>
    best_dev_UAS <span class="token operator">=</span> <span class="token number">0</span>


    <span class="token comment">### YOUR CODE HERE (~2-7 lines)</span>

    optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>parser<span class="token punctuation">.</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token punctuation">)</span>
    loss_func <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment">### END YOUR CODE</span>

<span class="token keyword">def</span> <span class="token function">train_for_epoch</span><span class="token punctuation">(</span>parser<span class="token punctuation">,</span> train_data<span class="token punctuation">,</span> dev_data<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> loss_func<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">""" Train the neural dependency parser for single epoch.

    Note: In PyTorch we can signify train versus test and automatically have
    the Dropout Layer applied and removed, accordingly, by specifying
    whether we are training, `model.train()`, or evaluating, `model.eval()`

    @param parser (Parser): Neural Dependency Parser
    @param train_data ():
    @param dev_data ():
    @param optimizer (nn.Optimizer): Adam Optimizer
    @param loss_func (nn.CrossEntropyLoss): Cross Entropy Loss Function
    @param batch_size (int): batch size
    @param lr (float): learning rate

    @return dev_UAS (float): Unlabeled Attachment Score (UAS) for dev data
    """</span>
    parser<span class="token punctuation">.</span>model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># Places model in "train" mode, i.e. apply dropout layer</span>
    n_minibatches <span class="token operator">=</span> math<span class="token punctuation">.</span>ceil<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>train_data<span class="token punctuation">)</span> <span class="token operator">/</span> batch_size<span class="token punctuation">)</span>
    loss_meter <span class="token operator">=</span> AverageMeter<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">with</span> tqdm<span class="token punctuation">(</span>total<span class="token operator">=</span><span class="token punctuation">(</span>n_minibatches<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">as</span> prog<span class="token punctuation">:</span>
        <span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>train_x<span class="token punctuation">,</span> train_y<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>minibatches<span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>   <span class="token comment"># remove any baggage in the optimizer</span>
            loss <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token comment"># store loss for this batch here</span>
            train_x <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>train_x<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
            train_y <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>train_y<span class="token punctuation">.</span>nonzero<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

            <span class="token comment">### YOUR CODE HERE (~5-10 lines)</span>
            
            logits <span class="token operator">=</span> parser<span class="token punctuation">.</span>model<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>train_x<span class="token punctuation">)</span>
            loss <span class="token operator">+=</span> loss_func<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> train_y<span class="token punctuation">)</span>
            loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

            <span class="token comment">### END YOUR CODE</span>
            prog<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
            loss_meter<span class="token punctuation">.</span>update<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

</code></pre> 
<p>用debug=True檢驗結果：averge loss=0.14, UAS=72.5<br> <img src="https://images2.imgbox.com/22/6e/TqjMhBlW_o.png" alt="在这里插入图片描述"></p> 
<p>– END –</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/15e6d2a06e4a827cf9d29d232fbaf4df/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">python 简单全自动WORD合并（加程序链接）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/123665711ce3a0543c0225bc17be8630/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Oracle11g与Oracle11gxe有什么区别</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>