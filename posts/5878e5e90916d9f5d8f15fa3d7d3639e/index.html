<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Paperreading之三Simple Baselines for Human Pose Estimation - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Paperreading之三Simple Baselines for Human Pose Estimation" />
<meta property="og:description" content="本次paper是coco2018关键点检测项目的亚军方案，方法非常的简洁明了，但是效果很惊艳，达到了state of the art，paper的标题也是写了simple baseline。整篇paper包含一个sota的姿态估计和姿态跟踪，本篇博客只涉及到paper里面的姿态估计部分。
前言 人体姿态估计是MSCOCO数据集(http://cocodataset.org/)上面的一项比赛，人体关键点检测，目前主流的做法都是深度学习来做。本文的出发点：
作者认为目前的姿态估计方法都太过于复杂，并且有显著的差异，比如hourglass，open pose，cpn等等，比较这些工作的差异性，更多体现在系统层面而不是信息层面。
作者在本文提出了一个既精确（sota水平），又简单（网络结构非常简单，见下文）的姿态估计方法，作为一个baseline，希望能激发一些新的ideas和简化评估方式。 2. 网络结构部分 网络结构一句话就可以表达清楚，就是一个普通的backbone（用resnet50就很好）加一些转置卷积层（作为一个head network）。作者认为这可能是得到heatmaps的最简单方式，并且包含了从深到浅的特征。
先看图：
上图c是不是非常简单，论文的整个网络就是这样。三个网络最大的区别就是在head network（头部网络）是如何得到高分辨率的feature map的，前两个方法都是上采样得到heatmap，但是simple baseline的方法是使用deconv ，deconv相当于同时做了卷积和上采样。
看起来似乎是得到高分辨率的feature maps是很重要的，但是你是怎么获取的并不是那么的关键，不同的方法获取的都会有不错的效果。
看一下forward函数
就一个resnet50去掉全连接层，加3个deconv layers和一个1*1的卷积就完事了。Deconv layers全部都是一样的kernel=4，channels=256，最后加一个1*1的卷积，输出得到关键点的heatmaps，求一个最大响应值就可以得到最后坐标点了。
Loss的设计：就是普通L2 loss，只在最后的输出算loss，并没有中继监督。
3.Ablation experiments 消融实验对比：
作者从heatmaps的尺寸，deconv的卷积核尺寸，backbone结构，输入图像尺寸等4个方面分别作了对比：
结论是：heatmaps尺寸最好是64*48，三层deconv，kernel的size最好是4，backbone是越大越好，图像尺寸越大越好，但是后两者会极大增加计算量和显存。要做好精度和速度的平衡。
4. 源码和结果对比 源码是Pytorch版本(https://github.com/Microsoft/human-pose-estimation.pytorch)，微软亚洲研究院官方出品，写的很好很通用，值得一读。
下面是各种效果对比图，训练几乎没有任何trick，网络里面也没有任何其他的骚操作，比如各种ohem，ohkm等等，也没有中继监督，相同条件下（主要是输入尺寸和bakcbone）效果都是领先水平。在backbone是resnet152和输入是384*288的条件下，各项指标都是排名第一，都是单模型对比的。
5.结论 论文给出的simple baseline是真的很simple，效果真的好，但是其实没有很特别的创新地方，只是把deconv用在了姿态估计的网络里面。
就像论文里面的第二章节一个标题Pose Estimation Using A Deconvolution Head Network。整个网络就是一个普通backbone&#43;一个使用deconv的head network。
参考文献 [1]Simple Baselines for Human Pose Estimation and Tracking(https://arxiv.org/abs/1804.06208)
[2]https://github.com/Microsoft/human-pose-estimation.pytorch" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/5878e5e90916d9f5d8f15fa3d7d3639e/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-11-14T22:37:16+08:00" />
<meta property="article:modified_time" content="2018-11-14T22:37:16+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Paperreading之三Simple Baselines for Human Pose Estimation</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p style="margin-left:0cm;">  本次paper是coco2018关键点检测项目的亚军方案，方法非常的简洁明了，但是效果很惊艳，达到了state of the art，paper的标题也是写了simple baseline。整篇paper包含一个sota的姿态估计和姿态跟踪，本篇博客只涉及到paper里面的姿态估计部分。</p> 
<ol><li> <h4>前言</h4> </li></ol> 
<p style="margin-left:0cm;">人体姿态估计是MSCOCO数据集(http://cocodataset.org/)上面的一项比赛，人体关键点检测，目前主流的做法都是深度学习来做。本文的出发点：</p> 
<p style="margin-left:0cm;"><img alt="" class="has" height="68" src="https://images2.imgbox.com/78/99/GFA6juI1_o.png" width="840"></p> 
<p style="margin-left:0cm;">作者认为目前的姿态估计方法都太过于复杂，并且有显著的差异，比如hourglass，open pose，cpn等等，比较这些工作的差异性，更多体现在系统层面而不是信息层面。</p> 
<p style="margin-left:0cm;">作者在本文提出了一个既精确（sota水平），又简单（网络结构非常简单，见下文）的姿态估计方法，作为一个baseline，希望能激发一些新的ideas和简化评估方式。  </p> 
<h4>2. 网络结构部分</h4> 
<p style="margin-left:0cm;"><img alt="" class="has" height="160" src="https://images2.imgbox.com/f3/65/9Xvx2dSd_o.png" width="839"></p> 
<p style="margin-left:0cm;">网络结构一句话就可以表达清楚，就是一个普通的backbone（用resnet50就很好）加一些转置卷积层（作为一个head network）。作者认为这可能是得到heatmaps的最简单方式，并且包含了从深到浅的特征。</p> 
<p style="margin-left:0cm;">先看图：</p> 
<p style="margin-left:0cm;"><img alt="" class="has" height="609" src="https://images2.imgbox.com/91/b7/aeiKzVpX_o.png" width="846"></p> 
<p style="margin-left:0cm;">上图c是不是非常简单，论文的整个网络就是这样。三个网络最大的区别就是在head network（头部网络）是如何得到高分辨率的feature map的，前两个方法都是上采样得到heatmap，但是simple baseline的方法是使用deconv ，deconv相当于同时做了卷积和上采样。</p> 
<p style="margin-left:0cm;">看起来似乎是得到高分辨率的feature maps是很重要的，但是你是怎么获取的并不是那么的关键，不同的方法获取的都会有不错的效果。</p> 
<p style="margin-left:0cm;"> </p> 
<p style="margin-left:0cm;">看一下forward函数</p> 
<p style="margin-left:0cm;"><img alt="" class="has" height="372" src="https://images2.imgbox.com/6d/83/Xj0nAbKS_o.png" width="582"></p> 
<p style="margin-left:0cm;"><span style="color:#f33b45;">就一个resnet50去掉全连接层，加3个deconv layers和一个1*1的卷积就完事了。Deconv layers全部都是一样的kernel=4，channels=256，最后加一个1*1的卷积，输出得到关键点的heatmaps，求一个最大响应值就可以得到最后坐标点了</span>。</p> 
<p style="margin-left:0cm;"> </p> 
<p style="margin-left:0cm;"><strong>Loss的设计</strong>：就是普通L2 loss，只在最后的输出算loss，并没有中继监督。</p> 
<p style="margin-left:0cm;"><img alt="" class="has" height="217" src="https://images2.imgbox.com/92/23/pa0iHI8r_o.png" width="803"></p> 
<p style="margin-left:0cm;"> </p> 
<h4>3.Ablation experiments</h4> 
<p style="margin-left:0cm;"><img alt="" class="has" height="657" src="https://images2.imgbox.com/bf/a7/mI0cJjy2_o.png" width="835">   <img alt="" class="has" height="736" src="https://images2.imgbox.com/24/e2/7dyTKdQZ_o.png" width="850"></p> 
<p style="margin-left:0cm;">消融实验对比：</p> 
<p style="margin-left:0cm;">作者从heatmaps的尺寸，deconv的卷积核尺寸，backbone结构，输入图像尺寸等4个方面分别作了对比：</p> 
<p style="margin-left:0cm;">结论是：<span style="color:#f33b45;">heatmaps尺寸最好是64*48，三层deconv，kernel的size最好是4，backbone是越大越好，图像尺寸越大越好，但是后两者会极大增加计算量和显存。要做好精度和速度的平衡</span>。</p> 
<h4 style="margin-left:0cm;">4. 源码和结果对比</h4> 
<p style="margin-left:0cm;">源码是<a href="https://github.com/Microsoft/human-pose-estimation.pytorch">Pytorch版本(</a>https://github.com/Microsoft/human-pose-estimation.pytorch)，微软亚洲研究院官方出品，写的很好很通用，值得一读。</p> 
<p style="margin-left:0cm;">下面是各种效果对比图，训练几乎没有任何trick，网络里面也没有任何其他的骚操作，<span style="color:#f33b45;">比如各种ohem，ohkm等等，也没有中继监督，相同条件下（主要是输入尺寸和bakcbone）效果都是领先水平。在backbone是resnet152和输入是384*288的条件下，各项指标都是排名第一，都是单模型对比的。</span></p> 
<p style="margin-left:0cm;"><img alt="" class="has" height="371" src="https://images2.imgbox.com/0a/f9/jOtrQ68C_o.png" width="792"></p> 
<p style="margin-left:0cm;"><img alt="" class="has" height="522" src="https://images2.imgbox.com/95/51/TslGzRNs_o.png" width="738"></p> 
<h4 style="margin-left:0cm;">5.结论</h4> 
<p style="margin-left:0cm;">论文给出的simple baseline是真的很simple，效果真的好，但是其实没有很特别的创新地方，只是把deconv用在了姿态估计的网络里面。</p> 
<p style="margin-left:0cm;">就像论文里面的第二章节一个标题Pose Estimation Using A Deconvolution Head Network。整个网络就是一个普通backbone+一个使用deconv的head network。</p> 
<p style="margin-left:0cm;"> </p> 
<h4 style="margin-left:0cm;">参考文献</h4> 
<p style="margin-left:0cm;">[1]<em>Simple Baselines for Human Pose Estimation and Tracking</em>(https://arxiv.org/abs/1804.06208)</p> 
<p style="margin-left:0cm;">[2]https://github.com/Microsoft/human-pose-estimation.pytorch</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/831a8073361f73ba9e4b92862acdcfdf/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Visual Studio 2017 修改 文件 编码为 UTF-8</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/03e02a07818ec2e5bcac885047318b2e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">使用Feign后service无法注入问题可能原因之一</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>