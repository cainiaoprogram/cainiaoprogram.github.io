<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>MapReduce之WordCount程序详解及常见错误汇总 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="MapReduce之WordCount程序详解及常见错误汇总" />
<meta property="og:description" content="前言：
在之前的笔记中，我们已经成功的关联了eclipse和hadoop，对FileSystem的使用进行了简单了解。
下面就是Hadoop中的重点MapReduce程序的开发。作为MapReduce（以下使用MR来代替）开发中的入门程序WordCount，基本是每个学习MapReduce的同学都必知必会的。有关于WordCount的概念笔者就不再赘述，网上有N多文章讲解。
本次博客主要是记录笔者在Windows环境下使用eclipse进行WordCount程序编写过程中所遇到的问题及解决方案。
准备工作：
* Windows环境下Eclipse工具的准备（需要使用插件关联hadoop，更多细节请参考笔者另一篇文章https://blog.csdn.net/qq_26323323/article/details/82936098 ）
* 创建maven项目，命名为hadoop，将Linux环境下hadoop的配置文件core-site.xml/mapred-site.xml/hdfs-site.xml/yarn-site.xml放入hadoop/src/main/resources中（主要是因为MR程序需要加载这些配置文件中的配置内容）
* 在hadoop/src/main/resources中创建log4j.properties文件，内容如下
log4j.rootLogger=DEBUG, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n log4j.appender.logfile=org.apache.log4j.FileAppender log4j.appender.logfile.File=target/spring.log log4j.appender.logfile.layout=org.apache.log4j.PatternLayout log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 注意：之所以要创建该文件，是因为在eclipse中启动MR程序时，默认是没有日志的，我们加载log4j的配置后，root设置为DEBUG级别，那么程序的每一步操作我们都可以通过日志来观察到，有利于我们定位问题
* 使用用户hxw（笔者）来启动hadoop
HADOOP_HOME/sbin/start-dfs.sh HADOOP_HOME/sbin/start-yarn.sh 1.WordCount程序的编写
具体内容如下，笔者不再详述
package hadoop.mr; import java.io.IOException; import java.net.URI; import java.util.StringTokenizer; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; public class WordCount { public static class WCMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;{ private final static IntWritable ONE = new IntWritable(1); private Text word = new Text(); /** * map程序，进行切割转换 */ @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, IntWritable&gt;." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/a65fba982981c8bf5a67f7ea3ea49b1f/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-10-06T15:48:15+08:00" />
<meta property="article:modified_time" content="2018-10-06T15:48:15+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">MapReduce之WordCount程序详解及常见错误汇总</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><strong>前言：</strong></p> 
<p>    在之前的笔记中，我们已经成功的关联了eclipse和hadoop，对FileSystem的使用进行了简单了解。</p> 
<p>    下面就是Hadoop中的重点MapReduce程序的开发。作为MapReduce（<strong>以下使用MR来代替</strong>）开发中的入门程序WordCount，基本是每个学习MapReduce的同学都必知必会的。有关于WordCount的概念笔者就不再赘述，网上有N多文章讲解。</p> 
<p>    本次博客主要是记录笔者在Windows环境下使用eclipse进行WordCount程序编写过程中所遇到的问题及解决方案。</p> 
<p> </p> 
<p><strong>准备工作：</strong></p> 
<p>    * Windows环境下Eclipse工具的准备（需要使用插件关联hadoop，更多细节请参考笔者另一篇文章<a href="https://blog.csdn.net/qq_26323323/article/details/82936098">https://blog.csdn.net/qq_26323323/article/details/82936098</a> ）</p> 
<p> </p> 
<p>    * 创建maven项目，命名为hadoop，将Linux环境下hadoop的配置文件core-site.xml/mapred-site.xml/hdfs-site.xml/yarn-site.xml放入hadoop/src/main/resources中（<strong>主要是因为MR程序需要加载这些配置文件中的配置内容</strong>）</p> 
<p> </p> 
<p><strong>    * 在hadoop/src/main/resources中创建log4j.properties文件，内容如下</strong></p> 
<pre class="has"><code class="language-java">log4j.rootLogger=DEBUG, stdout
log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n
log4j.appender.logfile=org.apache.log4j.FileAppender
log4j.appender.logfile.File=target/spring.log
log4j.appender.logfile.layout=org.apache.log4j.PatternLayout
log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n</code></pre> 
<p><strong>    注意：之所以要创建该文件，是因为在eclipse中启动MR程序时，默认是没有日志的，我们加载log4j的配置后，root设置为DEBUG级别，那么程序的每一步操作我们都可以通过日志来观察到，有利于我们定位问题</strong></p> 
<p> </p> 
<p>    * 使用用户hxw（笔者）来启动hadoop</p> 
<pre class="has"><code class="language-java">HADOOP_HOME/sbin/start-dfs.sh
HADOOP_HOME/sbin/start-yarn.sh</code></pre> 
<p> </p> 
<p><strong>1.WordCount程序的编写</strong></p> 
<p>    具体内容如下，笔者不再详述</p> 
<pre class="has"><code class="language-java">package hadoop.mr;

import java.io.IOException;
import java.net.URI;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCount {

	public static class WCMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;{
		private final static IntWritable ONE = new IntWritable(1);
		private Text word = new Text();
		 
		/**
		 * map程序，进行切割转换
		 */
		@Override
		protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, IntWritable&gt;.Context context)
				throws IOException, InterruptedException {
			
			// 1.解析value为token，默认会按照空格进行分割
			StringTokenizer token = new StringTokenizer(value.toString());
			while(token.hasMoreTokens()){
				// 2.将分割后的字符放入Word
				word.set(token.nextToken());
				
				// 3.输出k-v格式	类似(hadoop,1)
				context.write(word, ONE);
			}
		}
	}
	
	public static class WCReduce extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{
		private IntWritable result = new IntWritable();
		
		/**
		 * reduce程序，对map的结果进行合并
		 */
		@Override
		protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,
				Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context) throws IOException, InterruptedException {
			
			// 1.计算总和
			int sum = 0;
			for (IntWritable intWritable : values) {
				sum += intWritable.get();
			}
			
			result.set(sum);
			// 2.输出结果
			context.write(key, result);
		}
	}
	
	private static String INPUT_PATH = "/user/hadoop/mapreduce/input/";
	private static String OUTPUT_PATH = "/user/hadoop/mapreduce/output/";
	private static String HDFS_URI = "hdfs://hadoop:9000";// 对应于core-site.xml中的FS.default
	
	public static void main(String[] args) {
		
		try {
			// 1.如果已经有output_path，则先进行删除
			deleteOutputFile(OUTPUT_PATH);
			
			// 2.创建job，设置基本属性
			Job job = Job.getInstance();
			job.setJarByClass(WordCount.class);
			job.setJobName("wordcount");
			
			// 3.设置Mapper、Reducer
			job.setMapperClass(WCMapper.class);
			job.setReducerClass(WCReduce.class);
			
			job.setOutputKeyClass(Text.class);
			job.setOutputValueClass(IntWritable.class);
			
			// 4.设置输入路径和输出路径
			FileInputFormat.addInputPath(job, new Path(INPUT_PATH));
			FileOutputFormat.setOutputPath(job, new Path(OUTPUT_PATH));
			
			// 5.执行，执行完成后退出程序
			System.exit(job.waitForCompletion(true) ? 0 : 1);
		} catch (IOException e) {
			e.printStackTrace();
		} catch (Exception e) {
			e.printStackTrace();
		}
	}
	
	static void deleteOutputFile(String path) throws Exception{
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(new URI(HDFS_URI),conf,"hxw");
        if(fs.exists(new Path(path))){
            fs.delete(new Path(path),true);
        }
    }
}</code></pre> 
<p><strong>2.遇到的问题汇总</strong></p> 
<p> </p> 
<p><strong>    1）访问HDFS无权限</strong></p> 
<p>    报错内容一般如下：</p> 
<pre class="has"><code class="language-java">org.apache.hadoop.security.AccessControlException: Permission denied: user=root, access=WRITE, inode="":nutch:supergroup:rwxr-xr-x</code></pre> 
<p>    <strong>报错原因</strong>：主要是由于HDFS的文件系统都是有用户和权限的，如果当前用户无权限则在使用该文件或文件夹的时候会报错。</p> 
<p>    <strong>解决方案</strong>：</p> 
<p>        * 使用hdfs dfs -chmod 命令来修改相关文件或文件夹权限；</p> 
<p>        * <strong>如果在测试环境</strong>，用户不想这么麻烦来修改权限的话，也可使用配置来禁用hdfs的权限管理，可以在hdfs-site.xml中配置以下内容</p> 
<pre class="has"><code class="language-java">	&lt;property&gt;
		&lt;name&gt;dfs.permissions&lt;/name&gt;
		&lt;value&gt;false&lt;/value&gt;
	&lt;/property&gt;</code></pre> 
<p><strong>    2）运行过程中，无报错无日志，在<a href="http://hadoop:8088" rel="nofollow">http://hadoop:8088</a> <a href="http://hadoop:8088" rel="nofollow">界面中也无任务进度</a></strong></p> 
<p>    笔者在运行的过程中，比较烦恼的一件事就是，运行的时候没有任何报错，任务调度界面中也没有任务显示</p> 
<p>    将项目打成jar包放入Linux环境下，也是可以运行的，很奇怪。</p> 
<p>    后来，就添加了log4j.properties文件，将root设置为DEBUG级别（内容如上所示），就看到了其中的报错。</p> 
<p>    所以，<strong>我们在运行项目的时候，一定要添加日志文件</strong></p> 
<p> </p> 
<p><strong>    3）创建对ResourceManager连接的时候报错</strong></p> 
<pre class="has"><code class="language-java">DEBUG [org.apache.hadoop.ipc.Client] - closing ipc connection to 0.0.0.0/0.0.0.0:8032: Connection refused: no further information
java.net.ConnectException: Connection refused: no further information</code></pre> 
<p>    <strong>报错原因</strong><span style="color:#000000;">：看报错信息，我们知道是在创建对0.0.0.0:8032的Connection时候失败。为什么会失败？应该是无法连接到0.0.0.0这个IP。我们没有在配置文件中配置这个IP和端口，那么这个应该是默认配置。我们去hadoop官网的core-default.xml、yarn-default.xml等默认配置文件进行查看的时候，发现在yarn-site.xml中</span>发现以下内容</p> 
<pre class="has"><code class="language-java">yarn.resourcemanager.hostname	0.0.0.0	The hostname of the RM.
yarn.resourcemanager.address	${yarn.resourcemanager.hostname}:8032</code></pre> 
<p>    那么可以确定这个IP：port是对ResourceManager的连接失败</p> 
<p>    我们知道ResourceManager负责集群的资源分配，所有NodeManager都需要与ResourceManager进行通信交换信息，yarn.resourcemanager.hostname默认为0.0.0.0，我们将这个内容修改为hadoop，对应着当前本机地址即可</p> 
<p> </p> 
<p><strong>    解决方案</strong>：在yarn-site.xml中添加</p> 
<pre class="has"><code class="language-java">&lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
    &lt;value&gt;hadoop&lt;/value&gt;
&lt;/property&gt;</code></pre> 
<p><strong>    4）no job control</strong></p> 
<p>    报错信息如下所示：</p> 
<pre class="has"><code class="language-java">Exception message: /bin/bash: line 0: fg: no job control
Stack trace: ExitCodeException exitCode=1: /bin/bash: line 0: fg: no job control

	at org.apache.hadoop.util.Shell.runCommand(Shell.java:545)
	at org.apache.hadoop.util.Shell.run(Shell.java:456)
    ...</code></pre> 
<p>    <strong>报错原因</strong>：由于我们使用Windows平台进行开发并添加MR任务，而hadoop部署在Linux平台上，故针对跨平台的job会报该错</p> 
<p>    <strong>解决方案</strong>：在mapred-site.xml中添加以下配置</p> 
<p><strong>    设置为job提交允许跨平台</strong></p> 
<pre class="has"><code class="language-java">	&lt;property&gt;
		&lt;name&gt;mapreduce.app-submission.cross-platform&lt;/name&gt;
		&lt;value&gt;true&lt;/value&gt;
	&lt;/property&gt;</code></pre> 
<p> </p> 
<p><strong>    5）ClassNotFoundException</strong></p> 
<pre class="has"><code class="language-java">Error: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class hadoop.mr.WordCount$WCMapper not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195)
	at org.apache.hadoop.mapreduce.task.JobContextImpl.getMapperClass(JobContextImpl.java:186)
...</code></pre> 
<p>     <strong>报错原因</strong>：这个还是比较难解释的，需要对hadoop的运行原理有一定的了解。具体可参考这篇文章 <a href="https://blog.csdn.net/qq_19648191/article/details/56684268">https://blog.csdn.net/qq_19648191/article/details/56684268</a> </p> 
<p>          <strong>解决方案</strong>：在core-site.xml中设置如下配置：</p> 
<pre class="has"><code class="language-java">&lt;property&gt;
    &lt;name&gt;mapred.jar&lt;/name&gt;
    &lt;value&gt;C:/Users/lucky/Desktop/wc.jar&lt;/value&gt;
&lt;/property&gt;</code></pre> 
<p>    然后每次运行WordCount任务的时候，先将当前项目导出为一个jar包，命名为wc.jar，然后位置也要与我们配置的位置一致，这样再运行的时候就不会报错了</p> 
<p> </p> 
<p> </p> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e668794c7c49620af82d3e9a3b43eccc/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">web页面编写过程中的中文乱码问题解决</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/78b7ec875f6fbfdd3f5d6f1046ea55a2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">C语言ascII与数字转化的问题，值得新手看看</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>