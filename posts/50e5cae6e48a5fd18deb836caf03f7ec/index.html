<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>文生图一致性角色生成！谷歌最新文本到图片扩散模型工作 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="文生图一致性角色生成！谷歌最新文本到图片扩散模型工作" />
<meta property="og:description" content="大家好，我是阿潘，今天和大家分享一篇谷歌最新的文生图的工作。主要解决在文生图中无法生成一致性角色的难题！如果有帮助，欢迎大家帮忙转发分享，感谢！
推荐 微信交流群现已有2000&#43;从业人员交流群，欢迎进群交流学习，微信：nvshenj125
B站最新成果demo分享地址：https://space.bilibili.com/288489574
顶会工作整理Github repo：https://github.com/DWCTOD/CVPR2023-Papers-with-Code-Demo
The Chosen One: Consistent Characters in Text-to-Image Diffusion Models 论文：https://arxiv.org/pdf/2311.10093.pdf
项目：https://omriavrahami.com/the-chosen-one/
摘要: 文本到图像生成模型的最新进展释放了视觉创造力的巨大潜力。然而，这些模型在生成一致的角色方面遇到了困难，而这对于故事可视化、游戏开发资产设计、广告等众多现实应用程序来说是一个至关重要的方面。当前的方法通常依赖于目标角色的多个预先存在的图像或涉及劳动密集型的手动过程。在这项工作中，我们提出了一种用于一致角色生成的全自动解决方案，唯一的输入是文本提示。我们引入了一个迭代过程，在每个阶段，识别一组具有相似身份的连贯图像，并从该组中提取更一致的身份。我们的定量分析表明，与基线方法相比，我们的方法在即时对齐和身份一致性之间取得了更好的平衡，并且这些发现得到了用户研究的证实。
例如，考虑一位插画师正在创作橡皮泥猫角色。如图 2 所示，SOTA的文本到图像模型根据描述角色的prompt，会产生各种结果，很可能缺乏一致性（第一行）。相比之下，在这项工作中，展示了如何提取一致的猫表征（第二行），然后可以使用它在多种不同的上下文中描绘同一角色。
在这项工作中，认为在许多应用中，目标是生成一些一致的角色，而不是在视觉上匹配特定的外观。因此，作者提出了一种新的设置，目标是自动提取仅需要符合单一自然语言描述的角色的一致表示。我们的方法不需要目标角色的任何图像作为输入；因此，它能够创建一个新颖的一致角色，不一定类似于任何现有的视觉描绘。
一致角色生成任务是基于以下假设：对于特定提示，足够大的生成图像集将包含具有共享特征的图像组。给定这样一个集群，我们可以提取一种表示，捕获其图像之间的“共同点”。使用这种表示重复该过程，我们可以增加生成图像之间的一致性，同时仍然忠实于原始输入提示。 1、首先根据提供的文本提示生成图像库，并使用预先训练的特征提取器将它们embedding到欧几里得空间中。
2、接下来，对这些mbedding进行聚类，并选择最具”凝聚力“的簇作为尝试提取一致身份的个性化方法的输入。
3、然后，我们使用生成的模型生成下一个图像库，该图像应该表现出更高的一致性，同时仍然描述输入提示。迭代重复该过程直至收敛。
主要贡献是：（1）我们形式化了一致角色生成的任务，（2）为该任务提出了一种新颖的解决方案，（3）除了用户研究之外，我们还定量和定性地评估我们的方法，以 证明其有效性。
详细方案 如前所述，这项工作的目标是基于文本描述生成一致的角色图像（或另一种视觉主题）。通过迭代定制预训练的文本到图像模型来实现这一目标，使用模型本身生成的图像集作为训练数据。直观上，我们通过反复将模型的输出汇集到一致的身份来细化目标角色的表示。一旦该过程收敛，所得模型就可以用于在新颖的上下文中生成目标角色的一致图像。
具体方法如算法1和图3所示，以下是每一步的细节：
3.1. Identity Clustering
使用特征提取器 F 将每个图像嵌入到高维语义embedding空间中，这里使用 DINOv2 作为特征提取器 F。
接下来使用 K-MEANS&#43;&#43; 算法根据embedding空间中的余弦相似度对生成图像的embedding进行聚类。然后使用预设的阈值来过滤掉簇的大小过小的情况，并在剩余的集群中，选择最具“凝聚力”的簇作为身份提取阶段的输入。
在图 4 中，我们展示了 DINOv2 嵌入空间的可视化，其中高维嵌入 S 使用 t-SNE 投影到 2D 中，并根据其 K-MEANS&#43;&#43; 集群进行着色。一些嵌入比其他嵌入更紧密地聚集在一起，并且选择黑色集群作为最具“凝聚力”的簇。
3.2. Identity Extraction
根据当前迭代中生成的图像集的多样性，最具内聚性的聚类内聚性可能仍然表现出不一致的身份，如图 3 所示。因此，表征 θ 尚未准备好进行一致生成，我们进一步细化 它通过对图像进行内聚训练来提取更一致的身份。这种细化是使用文本到图像个性化方法进行的，其目的是从给定的一组已经描述了一致身份的图像中提取角色。虽然我们将它们应用于一组不完全一致的图像，但这些图像是根据彼此的语义相似性来选择的，使得这些方法能够从中提取出共同的身份
作者的解决方案基于预训练的稳定扩散 XL (SDXL) 模型，该模型使用两个文本编码器：CLIP 和 OpenCLIP 。执行 textual inversion 以添加一对新的文本标记 τ ，每个文本标记对应两个文本编码器。然而，作者发现这个参数空间的表达能力不够，因此还通过自注意力层和交叉注意力层的低秩适应（LoRA）来更新模型权重。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/50e5cae6e48a5fd18deb836caf03f7ec/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-11-20T23:30:06+08:00" />
<meta property="article:modified_time" content="2023-11-20T23:30:06+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">文生图一致性角色生成！谷歌最新文本到图片扩散模型工作</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p>大家好，我是阿潘，今天和大家分享一篇谷歌最新的文生图的工作。主要解决在文生图中无法生成一致性角色的难题！如果有帮助，欢迎大家帮忙转发分享，感谢！</p> 
 <h3> 推荐</h3> 
 <p style="text-align:left;">微信交流群现已有2000+从业人员交流群，欢迎进群交流学习，微信：nvshenj125</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/44/d6/dJdLfGm3_o.jpg" alt="72b11192571f9547eee2855fe7270848.jpeg"></p> 
 <p style="text-align:left;">B站最新成果demo分享地址：https://space.bilibili.com/288489574</p> 
 <p>顶会工作整理Github repo：https://github.com/DWCTOD/CVPR2023-Papers-with-Code-Demo</p> 
 <h2>The Chosen One: Consistent Characters in Text-to-Image Diffusion Models</h2> 
 <p>论文：https://arxiv.org/pdf/2311.10093.pdf</p> 
 <p>项目：https://omriavrahami.com/the-chosen-one/</p> 
 <h3> 摘要:</h3> 
 <p>文本到图像生成模型的最新进展释放了视觉创造力的巨大潜力。然而，<strong>这些模型在生成一致的角色方面遇到了困难，而这对于故事可视化、游戏开发资产设计、广告等众多现实应用程序来说是一个至关重要的方面</strong>。当前的方法通常依赖于目标角色的多个预先存在的图像或涉及劳动密集型的手动过程。在这项工作中，我们提出了一种用于一致角色生成的全自动解决方案，唯一的输入是文本提示。我们引入了一个迭代过程，在每个阶段，识别一组具有相似身份的连贯图像，并从该组中提取更一致的身份。我们的定量分析表明，与基线方法相比，我们的方法在即时对齐和身份一致性之间取得了更好的平衡，并且这些发现得到了用户研究的证实。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/80/69/CJHyf9Da_o.png" alt="bb7f817fdea49944a93bd2d87e99e38b.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/e6/83/nfxrNmp5_o.png" alt="8fa4bc8499efa01614d1544fdc8ff6a6.png"></p> 
 <p>例如，考虑一位插画师正在创作橡皮泥猫角色。<strong>如图 2 所示，SOTA的文本到图像模型根据描述角色的prompt，会产生各种结果，很可能缺乏一致性</strong>（第一行）。相比之下，在这项工作中，展示了如何提取一致的猫表征（第二行），然后可以使用它在多种不同的上下文中描绘同一角色。</p> 
 <p>在这项工作中，认为在许多应用中，目标是生成一些一致的角色，而不是在视觉上匹配特定的外观。因此，作者提出了一种新的设置，目标是自动提取仅需要符合单一自然语言描述的角色的一致表示。我们的方法不需要目标角色的任何图像作为输入；因此，它能够创建一个新颖的一致角色，不一定类似于任何现有的视觉描绘。<br></p> 
 <p><strong>一致角色生成任务是基于以下假设：对于特定提示，足够大的生成图像集将包含具有共享特征的图像组。给定这样一个集群，我们可以提取一种表示，捕获其图像之间的“共同点”。使用这种表示重复该过程，我们可以增加生成图像之间的一致性，同时仍然忠实于原始输入提示。 </strong></p> 
 <p>1、首先根据提供的文本提示生成图像库，并使用预先训练的特征提取器将它们embedding到欧几里得空间中。</p> 
 <p>2、接下来，对这些mbedding进行聚类，并选择最具”凝聚力“的簇作为尝试提取一致身份的个性化方法的输入。</p> 
 <p>3、然后，我们使用生成的模型生成下一个图像库，该图像应该表现出更高的一致性，同时仍然描述输入提示。迭代重复该过程直至收敛。</p> 
 <p>主要贡献是：（1）我们形式化了一致角色生成的任务，（2）为该任务提出了一种新颖的解决方案，（3）除了用户研究之外，我们还定量和定性地评估我们的方法，以 证明其有效性。</p> 
 <h3> 详细方案</h3> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/fd/a9/WlL4SF7j_o.png" alt="10b0f79e31fdfa0a5bbfec8b2d7572d6.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/3e/39/omi1Pgbd_o.png" alt="3925cd5fcac1908e8edb37494cddf05c.png"></p> 
 <p>如前所述，这项工作的目标是基于文本描述生成一致的角色图像（或另一种视觉主题）。通过迭代定制预训练的文本到图像模型来实现这一目标，使用模型本身生成的图像集作为训练数据。直观上，我们通过反复将模型的输出汇集到一致的身份来细化目标角色的表示。一旦该过程收敛，所得模型就可以用于在新颖的上下文中生成目标角色的一致图像。</p> 
 <p>具体方法如算法1和图3所示，以下是每一步的细节：</p> 
 <p><strong>3.1. Identity Clustering</strong></p> 
 <p>使用特征提取器 F 将每个图像嵌入到高维语义embedding空间中，这里使用 <strong>DINOv2 作为特征提取器 F</strong>。</p> 
 <p>接下来使用 K<strong>-MEANS++ 算法根据embedding空间中的余弦相似度对生成图像的embedding进行聚类</strong>。然后使用预设的阈值来过滤掉簇的大小过小的情况，并在剩余的集群中，选择最具“凝聚力”的簇作为身份提取阶段的输入。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/2f/92/KAaGAdxO_o.png" alt="dc6607250517b65f5c1186314bdf6a37.png"></p> 
 <p>在图 4 中，我们展示了 DINOv2 嵌入空间的可视化，其中高维嵌入 S 使用 t-SNE 投影到 2D 中，并根据其 K-MEANS++ 集群进行着色。一些嵌入比其他嵌入更紧密地聚集在一起，并且选择黑色集群作为最具“凝聚力”的簇。</p> 
 <p><strong>3.2. Identity Extraction</strong></p> 
 <p>根据当前迭代中生成的图像集的多样性，最具内聚性的聚类内聚性可能仍然表现出不一致的身份，如图 3 所示。因此，表征 θ 尚未准备好进行一致生成，我们进一步细化 它通过对图像进行内聚训练来提取更一致的身份。这种细化是使用文本到图像个性化方法进行的，其目的是从给定的一组已经描述了一致身份的图像中提取角色。虽然我们将它们应用于一组不完全一致的图像，但这些图像是根据彼此的语义相似性来选择的，使得这些方法能够从中提取出共同的身份</p> 
 <p>作者的解决方案基于预训练的稳定扩散 XL (SDXL) 模型，该模型使用两个文本编码器：CLIP 和 OpenCLIP 。执行 textual inversion 以添加一对新的文本标记 τ ，每个文本标记对应两个文本编码器。然而，作者发现这个参数空间的表达能力不够，因此还通过自注意力层和交叉注意力层的低秩适应（LoRA）来更新模型权重。</p> 
 <p><strong>3.3. Convergence</strong></p> 
 <p>如前所述（算法 1 和图 3），上述过程是迭代执行的。请注意，每次迭代中提取的表示 θ 是用于生成下一次迭代的 N 个图像集的表示。因此，生成的图像被汇集到一致的身份中。</p> 
 <p>我们不使用固定次数的迭代，而是应用能够提前停止的收敛标准。每次迭代后，计算新生成图像的所有 N 个embedding之间的每对embedding组平均欧几里得距离，并在该距离小于预定义阈值 dconv 时停止。</p> 
 <p>最后，应该注意的是，我们的方法是不确定的，即，当多次运行我们的方法时，在相同的输入提示 p 上，将生成不同的一致角色。这与我们任务的一对多性质是一致的。</p> 
 <h3> 效果和局限性</h3> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/19/ae/OAvc7YG5_o.png" alt="371c6f4a4da4a7d1b1915afca5298327.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/e3/d3/0NkG1qPn_o.png" alt="f41f053a5f9bd3f53d1258fda70c23a5.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/78/08/xbD1zcid_o.png" alt="d36c24605934add49bd7b7159f981c8d.png"></p> 
 <p>今天的分享就到这里，大家喜欢的话，可以多多支持，感谢！</p> 
 <h3> 推荐阅读</h3> 
 <p><a href="" rel="nofollow">Arxiv论文这样子下载！是真的快！</a><br></p> 
 <p><a href="" rel="nofollow">用 AI 做了一个体感游戏！真人拳皇搞起！</a><br></p> 
 <p><a href="" rel="nofollow">杀疯了！本科大神毕设：3D虚拟主播软件！独创AR直播！支持OBS！开源！</a></p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4e0a33911d7825d099bd505e75acefa7/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">STM32:OLED屏幕开发</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/99ac748321be5fea0ecd32f4bd233a50/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">什么是一阶逻辑？</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>