<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>CUDA编程之CUDA流 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="CUDA编程之CUDA流" />
<meta property="og:description" content="文章目录 前言CUDA流在默认流中重叠主机与设备用非默认CUDA流重叠多个核函数的执行重叠多个核函数的例子 用非默认CUDA流重叠核函数的执行与数据传递不可分页主机内存与异步的数据传输函数 总结参考 前言 CUDA程序的并行层次主要有两个，一个是核函数内部的并行，一个是核函数外部的。合理使用CUDA流能实现核函数外部的并行。
CUDA流 核函数外部并行：
（1）核函数计算与数据传输之间的并行
（2）主机计算与数据传输之间的并行
（3）不同数据传输之间的并行
（4）核函数计算与主机计算之间的并行
（5）不同核函数之间的并行
CUDA流：一个CUDA流指的是由主机发出的在一个设备中执行的CUDA操作序列。CUDA流各个操作的次序是由主机控制的，按照主机发布的次序执行。两个不同的CUDA流中的操作不一定按照顺序执行，和线程一样，要么并发要么交错地执行。
任何CUDA操作都存在于某个CUDA流中，要么是默认流和空流，要么是指定地非空流。前面的CUDA程序没有指定的都是默认和空流中执行的。
非默认的CUDA流实在主机端产生与销毁的。
// 创建 cudaError_r cudaStreamCream_t(cudaStream_t*); // 在yolov5_trt中使用过 // 销毁 cudaError_r cudaStreamDestroy(cudaStream_t); 由创建的代码可知，输入参数是cudaStream_t类型指针，返回值类型是一个错误代码。
不同CUDA流之间的并发
主机向某个CUDA流中发布一系列命令后必须马上获得程序的控制权，不用等待该CUDA流中的命令在设备中执行完毕。这样，就可以通过主机生成多个独立的CUDA流。
同时，CUDA运行时API还提供了两个函数：
// 这两个函数和等待和守护线程有点像 // 会强制阻塞主机，知道cuda流stream中的所有操作都执行完毕。 cudaError_r cudaStreamSynchronize(cudaStream_t stream); // 不阻塞主机，只是检查CUDA流stream中的所有操作是否都执行完毕。 cudaError_r cudaStreamQuery(cudaStream_t stream); 在默认流中重叠主机与设备 合理利用主机与设备之间的执行次序。如前面的简单的数组求和的核函数：
// 将某些数据从主机复制到设备上 cudaMemcpy(d_x,h_x,M,cudaMemcpyHostToDevice); cudaMemcpy(d_y,h_y,M,cudaMemcpyHostToDevice); // 调用核函数在设备中进行计算,数组求和 const int block_size = 128; // 不同型号的GPU有线程限制，开普勒到图灵最大为1024 const int gride_size = N/block_size; add&lt;&lt;&lt;gride_size,block_size&gt;&gt;&gt;(d_x,d_y,d_z); // 将某些数据从设备复制到主机上,这个数据传输函数隐式的起到了同步主机与设备的作用,所以后面用不用cudaDeviceSynchronize都可以 cudaMemcpy(h_z,d_z,M,cudaMemcpyDeviceToHost); 从主机看，数据传输是同步的。在主机执行核函数之前的CUDA操作语句将在默认的CUDA流中按代码的顺序执行。在进行数据传输时，主机是闲置的，不能进行其他操作。
不同的是，在执行核函数时，核函数的启动是异步的。主机发出执行的命令，不会等核函数执行完毕，而会立刻的到程序的控制权，往下执行。执行到从设备到主机传输数据这条语句，该语句不会立即执行。因为这是默认流中的CUDA操作，必须等待前一个CUDA操作执行完毕才会开始执行。
从上面的分析可知，核函数启动异步，对默认流中CUDA的操作会阻塞，但是对主机中的程序不会进行阻塞。如果调用核函数下一句是主机中的某个计算任务，那么主机就会在设备执行核函数的同时去进行一些计算。这样主机和设备就可以同时进行计算。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/4a1cb03cd2719583d0b3e5f9d271b399/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-02-03T23:59:10+08:00" />
<meta property="article:modified_time" content="2023-02-03T23:59:10+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">CUDA编程之CUDA流</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#_6" rel="nofollow">前言</a></li><li><a href="#CUDA_9" rel="nofollow">CUDA流</a></li><li><ul><li><a href="#_42" rel="nofollow">在默认流中重叠主机与设备</a></li><li><a href="#CUDA_59" rel="nofollow">用非默认CUDA流重叠多个核函数的执行</a></li><li><ul><li><a href="#_61" rel="nofollow">重叠多个核函数的例子</a></li></ul> 
   </li><li><a href="#CUDA_100" rel="nofollow">用非默认CUDA流重叠核函数的执行与数据传递</a></li><li><ul><li><a href="#_101" rel="nofollow">不可分页主机内存与异步的数据传输函数</a></li></ul> 
  </li></ul> 
  </li><li><a href="#_175" rel="nofollow">总结</a></li><li><ul><li><a href="#_177" rel="nofollow">参考</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<hr> 
<h2><a id="_6"></a>前言</h2> 
<p>    CUDA程序的并行层次主要有两个，一个是核函数内部的并行，一个是核函数外部的。合理使用CUDA流能实现核函数外部的并行。</p> 
<h2><a id="CUDA_9"></a>CUDA流</h2> 
<p>核函数外部并行：<br> （1）核函数计算与数据传输之间的并行<br> （2）主机计算与数据传输之间的并行<br> （3）不同数据传输之间的并行<br> （4）核函数计算与主机计算之间的并行<br> （5）不同核函数之间的并行</p> 
<p>    CUDA流：一个CUDA流指的是由主机发出的在一个设备中执行的CUDA操作序列。CUDA流各个操作的次序是由主机控制的，按照主机发布的次序执行。两个不同的CUDA流中的操作不一定按照顺序执行，和线程一样，要么并发要么交错地执行。<br>     任何CUDA操作都存在于某个CUDA流中，要么是默认流和空流，要么是指定地非空流。前面的CUDA程序没有指定的都是默认和空流中执行的。<br> 非默认的CUDA流实在主机端产生与销毁的。</p> 
<pre><code class="prism language-cpp"><span class="token comment">// 创建</span>
cudaError_r <span class="token function">cudaStreamCream_t</span><span class="token punctuation">(</span>cudaStream_t<span class="token operator">*</span><span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token comment">// 在yolov5_trt中使用过</span>
<span class="token comment">// 销毁</span>
cudaError_r <span class="token function">cudaStreamDestroy</span><span class="token punctuation">(</span>cudaStream_t<span class="token punctuation">)</span><span class="token punctuation">;</span> 
</code></pre> 
<p>由创建的代码可知，输入参数是cudaStream_t类型指针，返回值类型是一个错误代码。</p> 
<p><strong>不同CUDA流之间的并发</strong></p> 
<p>    主机向某个CUDA流中发布一系列命令后必须马上获得程序的控制权，不用等待该CUDA流中的命令在设备中执行完毕。这样，就可以通过主机生成多个独立的CUDA流。<br> 同时，CUDA运行时API还提供了两个函数：</p> 
<pre><code class="prism language-cpp"><span class="token comment">// 这两个函数和等待和守护线程有点像</span>
<span class="token comment">// 会强制阻塞主机，知道cuda流stream中的所有操作都执行完毕。</span>
cudaError_r <span class="token function">cudaStreamSynchronize</span><span class="token punctuation">(</span>cudaStream_t stream<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment">// 不阻塞主机，只是检查CUDA流stream中的所有操作是否都执行完毕。</span>
cudaError_r <span class="token function">cudaStreamQuery</span><span class="token punctuation">(</span>cudaStream_t stream<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<h3><a id="_42"></a>在默认流中重叠主机与设备</h3> 
<p>合理利用主机与设备之间的执行次序。如前面的简单的数组求和的核函数：</p> 
<pre><code class="prism language-cpp">   <span class="token comment">// 将某些数据从主机复制到设备上</span>
    <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>d_x<span class="token punctuation">,</span>h_x<span class="token punctuation">,</span>M<span class="token punctuation">,</span>cudaMemcpyHostToDevice<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>d_y<span class="token punctuation">,</span>h_y<span class="token punctuation">,</span>M<span class="token punctuation">,</span>cudaMemcpyHostToDevice<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">// 调用核函数在设备中进行计算,数组求和</span>
    <span class="token keyword">const</span> <span class="token keyword">int</span> block_size <span class="token operator">=</span> <span class="token number">128</span><span class="token punctuation">;</span>  <span class="token comment">// 不同型号的GPU有线程限制，开普勒到图灵最大为1024</span>
    <span class="token keyword">const</span> <span class="token keyword">int</span> gride_size <span class="token operator">=</span> N<span class="token operator">/</span>block_size<span class="token punctuation">;</span>
    add<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>gride_size<span class="token punctuation">,</span>block_size<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>d_x<span class="token punctuation">,</span>d_y<span class="token punctuation">,</span>d_z<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">// 将某些数据从设备复制到主机上,这个数据传输函数隐式的起到了同步主机与设备的作用,所以后面用不用cudaDeviceSynchronize都可以</span>
    <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>h_z<span class="token punctuation">,</span>d_z<span class="token punctuation">,</span>M<span class="token punctuation">,</span>cudaMemcpyDeviceToHost<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<p>    从主机看，数据传输是同步的。在主机执行核函数之前的CUDA操作语句将在默认的CUDA流中按代码的顺序执行。在进行数据传输时，主机是闲置的，不能进行其他操作。<br>     不同的是，在执行核函数时，核函数的启动是异步的。主机发出执行的命令，不会等核函数执行完毕，而会立刻的到程序的控制权，往下执行。执行到从设备到主机传输数据这条语句，该语句不会立即执行。因为这是默认流中的CUDA操作，必须等待前一个CUDA操作执行完毕才会开始执行。<br>     从上面的分析可知，核函数启动异步，对默认流中CUDA的操作会阻塞，但是对主机中的程序不会进行阻塞。如果调用核函数下一句是主机中的某个计算任务，那么主机就会在设备执行核函数的同时去进行一些计算。这样主机和设备就可以同时进行计算。<br>     当然，重叠主机与设备的计算要考虑两者计算的时间。一般来说设备函数的计算速度是主机函数的10倍左右。当设备函数执行完的时间与接下来的主机函数执行完的时间差不多时，两者擦汗不读时间结束，加速效果最好。当主机函数与设备函数的计算时间相差很多的情况下，设备函数占主要计算时间或者主机函数占主要时间，加速效果就差了。</p> 
<h3><a id="CUDA_59"></a>用非默认CUDA流重叠多个核函数的执行</h3> 
<p>    一个默认流可以实现主机计算与设备计算的并行，但是多个核函数之间的并行必须使用多个CUDA流。</p> 
<h4><a id="_61"></a>重叠多个核函数的例子</h4> 
<p>使用非默认流时，核函数的执行配置中必须包含一个流对象。例如，核函数有如下三种调用方式：</p> 
<pre><code class="prism language-cpp">add<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>gride_size<span class="token punctuation">,</span>block_size<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
add<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>gride_size<span class="token punctuation">,</span>block_size<span class="token punctuation">,</span>shared<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
add<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>N_gride<span class="token punctuation">,</span>N_block_size<span class="token punctuation">,</span>N_shared<span class="token punctuation">,</span>stream_id<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<p>    其中，第一个调用方式在默认流中执行。第二个是使用了动态共享内存的，同样在默认流中执行。只有第三个调用方式，说明核函数在编号为stream_id的CUDA流中执行，而且可以使用动态共享内存，不使用动态共享内存的情况下，动态共享字节参数不能省略，必须为。<br> 如修改原来的核函数中数组相加的程序：</p> 
<pre><code class="prism language-cpp"><span class="token comment">// 多个非默认流</span>
 <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> n <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> n <span class="token operator">&lt;</span> num<span class="token punctuation">;</span> <span class="token operator">++</span>n<span class="token punctuation">)</span>
        <span class="token punctuation">{<!-- --></span>
            <span class="token keyword">int</span> offset <span class="token operator">=</span> n <span class="token operator">*</span> N1<span class="token punctuation">;</span>
            add<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid_size<span class="token punctuation">,</span> block_size<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> streams<span class="token punctuation">[</span>n<span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>
            <span class="token punctuation">(</span>d_x <span class="token operator">+</span> offset<span class="token punctuation">,</span> d_y <span class="token operator">+</span> offset<span class="token punctuation">,</span> d_z <span class="token operator">+</span> offset<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
<span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">-</span>
<span class="token keyword">void</span> __global__ <span class="token function">add</span><span class="token punctuation">(</span><span class="token keyword">const</span> real <span class="token operator">*</span>d_x<span class="token punctuation">,</span> <span class="token keyword">const</span> real <span class="token operator">*</span>d_y<span class="token punctuation">,</span> real <span class="token operator">*</span>d_z<span class="token punctuation">)</span>
<span class="token punctuation">{<!-- --></span>
    <span class="token keyword">const</span> <span class="token keyword">int</span> n <span class="token operator">=</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">*</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>n <span class="token operator">&lt;</span> N1<span class="token punctuation">)</span>
    <span class="token punctuation">{<!-- --></span>
    	<span class="token comment">// 使用了for循环进行数组元素相加，这是为了后面的计时方便，不使用也行。</span>
        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> <span class="token number">1000000</span><span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span>
        <span class="token punctuation">{<!-- --></span>
            d_z<span class="token punctuation">[</span>n<span class="token punctuation">]</span> <span class="token operator">=</span> d_x<span class="token punctuation">[</span>n<span class="token punctuation">]</span> <span class="token operator">+</span> d_y<span class="token punctuation">[</span>n<span class="token punctuation">]</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>和最初的代码，有两个区别：<br> （1）在调用核函数的时候，使用了for循环，表示使用非默认流中重叠多个核函数的执行。n，表示CUDA流的索引。offset 表示多个核函数中，每个核函数使用N1个线程。<br> （2）在每个核函数计算时，使用了for循环进行数组元素相加，这是为了后面的计时方便，不使用也行。<br> 在测试中发现，使用多个流相对于使用一个流有了加速。但是，存在加速比的限制：<br> （1）当所有CUDA流中对应核函数的线程总数和超过某个值时，再增加流的数目就不会带来更高的加速比了。<br> （2）单个GPU中能够并发执行的核函数个数的上限，不同的GPU架构有不同的上限。</p> 
<h3><a id="CUDA_100"></a>用非默认CUDA流重叠核函数的执行与数据传递</h3> 
<h4><a id="_101"></a>不可分页主机内存与异步的数据传输函数</h4> 
<p>    要实现核函数执行与数据传输的并发（重叠），必须让这两个操作处于不同的非默认流，而且数据传输必须使用cudaMemcpy()函数的异步版本，即cudaMemcpyAsync()函数（前面tensorrt部署中有使用）。异步传输由GPU中的DMA直接实现，不需要主机参与。如果用同步的数据传输，主机无法再一个流中进行数据传输时，去另一个流调用核函数。这样核函数执行与数据传输的并发也就无法实现。<br> <img src="https://images2.imgbox.com/3d/a5/4p2I54vA_o.png" alt="在这里插入图片描述"><br>     异步的数据传输函数，只比同步的多了一个参数，表示所在流的变量。在使用异步数据传输函数时，需要将主机内存定义为不可分页内存或者固定内存。不可分页主机内存分分配可以由以下两个函数中任何一个实现：<br> <img src="https://images2.imgbox.com/5d/5c/pUUMA6ex_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/14/d8/DnOvjZnd_o.png" alt="在这里插入图片描述"></p> 
<p><strong>尝试使用不同的流执行不同的操作来提升性能。</strong></p> 
<p>    一般一个cuda程序要有主机向设备进行数据传输（H2D），核函数的调用（KER），设备向主机进行数据传输（D2H）。这三个操作在一个CUDA流中执行的顺序：<br> <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
        
        
       
         s 
        
       
         t 
        
       
         r 
        
       
         e 
        
       
         a 
        
       
         m 
        
       
         0 
        
       
         : 
        
       
         H 
        
       
         2 
        
       
         D 
        
       
         − 
        
       
         &gt; 
        
       
         K 
        
       
         E 
        
       
         R 
        
       
         − 
        
       
         &gt; 
        
       
         D 
        
       
         2 
        
       
         H 
        
       
      
        \qquad \qquad \qquad \qquad stream0 : H2D -&gt; KER -&gt; D2H 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mspace" style="margin-right: 2em;"></span><span class="mspace" style="margin-right: 2em;"></span><span class="mspace" style="margin-right: 2em;"></span><span class="mspace" style="margin-right: 2em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mord mathnormal">re</span><span class="mord mathnormal">am</span><span class="mord">0</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.7667em; vertical-align: -0.0833em;"></span><span class="mord mathnormal" style="margin-right: 0.0813em;">H</span><span class="mord">2</span><span class="mord mathnormal" style="margin-right: 0.0278em;">D</span><span class="mord">−</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.7667em; vertical-align: -0.0833em;"></span><span class="mord mathnormal" style="margin-right: 0.0715em;">K</span><span class="mord mathnormal" style="margin-right: 0.0077em;">ER</span><span class="mord">−</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">D</span><span class="mord">2</span><span class="mord mathnormal" style="margin-right: 0.0813em;">H</span></span></span></span></span><br> 如果简单的将3个操作放入3个不同的流中：<br> <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
        
        
       
         s 
        
       
         t 
        
       
         r 
        
       
         e 
        
       
         a 
        
       
         m 
        
       
         0 
        
       
         : 
        
       
         H 
        
       
         2 
        
       
         D 
        
       
      
        \qquad \qquad \qquad \qquad stream0 : H2D 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mspace" style="margin-right: 2em;"></span><span class="mspace" style="margin-right: 2em;"></span><span class="mspace" style="margin-right: 2em;"></span><span class="mspace" style="margin-right: 2em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mord mathnormal">re</span><span class="mord mathnormal">am</span><span class="mord">0</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.0813em;">H</span><span class="mord">2</span><span class="mord mathnormal" style="margin-right: 0.0278em;">D</span></span></span></span></span><br> <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
        
        
       
         s 
        
       
         t 
        
       
         r 
        
       
         e 
        
       
         a 
        
       
         m 
        
       
         1 
        
       
         : 
        
        
        
       
         − 
        
       
         &gt; 
        
       
         K 
        
       
         E 
        
       
         R 
        
       
      
        \qquad \qquad \qquad \qquad stream1 : \quad \qquad -&gt; KER 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mspace" style="margin-right: 2em;"></span><span class="mspace" style="margin-right: 2em;"></span><span class="mspace" style="margin-right: 2em;"></span><span class="mspace" style="margin-right: 2em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mord mathnormal">re</span><span class="mord mathnormal">am</span><span class="mord">1</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right: 1em;"></span><span class="mspace" style="margin-right: 2em;"></span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6667em; vertical-align: -0.0833em;"></span><span class="mord">−</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.0715em;">K</span><span class="mord mathnormal" style="margin-right: 0.0077em;">ER</span></span></span></span></span><br> <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
        
        
       
         s 
        
       
         t 
        
       
         r 
        
       
         e 
        
       
         a 
        
       
         m 
        
       
         2 
        
       
         : 
        
        
        
        
        
       
         − 
        
       
         &gt; 
        
       
         D 
        
       
         2 
        
       
         H 
        
       
      
        \qquad \qquad \qquad \qquad stream2: \qquad \qquad \qquad \qquad -&gt; D2H 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mspace" style="margin-right: 2em;"></span><span class="mspace" style="margin-right: 2em;"></span><span class="mspace" style="margin-right: 2em;"></span><span class="mspace" style="margin-right: 2em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mord mathnormal">re</span><span class="mord mathnormal">am</span><span class="mord">2</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right: 2em;"></span><span class="mspace" style="margin-right: 2em;"></span><span class="mspace" style="margin-right: 2em;"></span><span class="mspace" style="margin-right: 2em;"></span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6667em; vertical-align: -0.0833em;"></span><span class="mord">−</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">D</span><span class="mord">2</span><span class="mord mathnormal" style="margin-right: 0.0813em;">H</span></span></span></span></span><br>     这样操作并不能带来性能提升。必须创造出在逻辑上可以并发执行的CUDA操作，可以将以上3个CUDA操作都分成若干等份，然后在每一个流中发布一个CUDA操作序列。<br> <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
        
        
       
         s 
        
       
         t 
        
       
         r 
        
       
         e 
        
       
         a 
        
       
         m 
        
       
         0 
        
       
         : 
        
       
         H 
        
       
         2 
        
       
         D 
        
       
         − 
        
       
         &gt; 
        
       
         K 
        
       
         E 
        
       
         R 
        
       
         − 
        
       
         &gt; 
        
       
         D 
        
       
         2 
        
       
         H 
        
       
      
        \qquad \qquad \qquad \qquad stream0 : H2D -&gt; KER -&gt; D2H 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mspace" style="margin-right: 2em;"></span><span class="mspace" style="margin-right: 2em;"></span><span class="mspace" style="margin-right: 2em;"></span><span class="mspace" style="margin-right: 2em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mord mathnormal">re</span><span class="mord mathnormal">am</span><span class="mord">0</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.7667em; vertical-align: -0.0833em;"></span><span class="mord mathnormal" style="margin-right: 0.0813em;">H</span><span class="mord">2</span><span class="mord mathnormal" style="margin-right: 0.0278em;">D</span><span class="mord">−</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.7667em; vertical-align: -0.0833em;"></span><span class="mord mathnormal" style="margin-right: 0.0715em;">K</span><span class="mord mathnormal" style="margin-right: 0.0077em;">ER</span><span class="mord">−</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">D</span><span class="mord">2</span><span class="mord mathnormal" style="margin-right: 0.0813em;">H</span></span></span></span></span><br> <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
        
        
       
         s 
        
       
         t 
        
       
         r 
        
       
         e 
        
       
         a 
        
       
         m 
        
       
         1 
        
       
         : 
        
        
        
       
         H 
        
       
         2 
        
       
         D 
        
       
         − 
        
       
         &gt; 
        
       
         K 
        
       
         E 
        
       
         R 
        
       
         − 
        
       
         &gt; 
        
       
         D 
        
       
         2 
        
       
         H 
        
       
      
        \qquad \qquad \qquad \qquad stream1 : \qquad \qquad H2D -&gt; KER -&gt; D2H 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mspace" style="margin-right: 2em;"></span><span class="mspace" style="margin-right: 2em;"></span><span class="mspace" style="margin-right: 2em;"></span><span class="mspace" style="margin-right: 2em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mord mathnormal">re</span><span class="mord mathnormal">am</span><span class="mord">1</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right: 2em;"></span><span class="mspace" style="margin-right: 2em;"></span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.7667em; vertical-align: -0.0833em;"></span><span class="mord mathnormal" style="margin-right: 0.0813em;">H</span><span class="mord">2</span><span class="mord mathnormal" style="margin-right: 0.0278em;">D</span><span class="mord">−</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.7667em; vertical-align: -0.0833em;"></span><span class="mord mathnormal" style="margin-right: 0.0715em;">K</span><span class="mord mathnormal" style="margin-right: 0.0077em;">ER</span><span class="mord">−</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">D</span><span class="mord">2</span><span class="mord mathnormal" style="margin-right: 0.0813em;">H</span></span></span></span></span><br>     如上面使用两个流的情况，每个CUDA操作所处理的数据量只有使用一个CUDA流时的一半。其中H2D这里不能并发的执行，是受硬件资源的限制。如果H2D ，KER ，D2H这三个操作执行的时间都相同，那么可以有效的隐藏两个CUDA操作，使得总的执行效率相比使用单个CUDA流的情况提升<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         6 
        
       
         / 
        
       
         4 
        
       
         = 
        
       
         1.5 
        
       
      
        6/4=1.5 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord">6/4</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">1.5</span></span></span></span></span>倍。如下给出使用非默认CUDA流重叠核函数的执行与数据传递的示例：</p> 
<pre><code class="prism language-cpp"><span class="token comment">// 部分代码</span>
<span class="token keyword">void</span> __global__ <span class="token function">add</span><span class="token punctuation">(</span><span class="token keyword">const</span> real <span class="token operator">*</span>x<span class="token punctuation">,</span> <span class="token keyword">const</span> real <span class="token operator">*</span>y<span class="token punctuation">,</span> real <span class="token operator">*</span>z<span class="token punctuation">,</span> <span class="token keyword">int</span> N<span class="token punctuation">)</span>
<span class="token punctuation">{<!-- --></span>
    <span class="token keyword">const</span> <span class="token keyword">int</span> n <span class="token operator">=</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">*</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>n <span class="token operator">&lt;</span> N<span class="token punctuation">)</span>
    <span class="token punctuation">{<!-- --></span>
     	<span class="token comment">// 和上面一样故意让核函数求和操作重复40次，让核函数和数据传输所用时间相当。</span>
        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> <span class="token number">40</span><span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span>
        <span class="token punctuation">{<!-- --></span>
            z<span class="token punctuation">[</span>n<span class="token punctuation">]</span> <span class="token operator">=</span> x<span class="token punctuation">[</span>n<span class="token punctuation">]</span> <span class="token operator">+</span> y<span class="token punctuation">[</span>n<span class="token punctuation">]</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>

<span class="token keyword">void</span> <span class="token function">timing</span>
<span class="token punctuation">(</span>
    <span class="token keyword">const</span> real <span class="token operator">*</span>h_x<span class="token punctuation">,</span> <span class="token keyword">const</span> real <span class="token operator">*</span>h_y<span class="token punctuation">,</span> real <span class="token operator">*</span>h_z<span class="token punctuation">,</span>
    real <span class="token operator">*</span>d_x<span class="token punctuation">,</span> real <span class="token operator">*</span>d_y<span class="token punctuation">,</span> real <span class="token operator">*</span>d_z<span class="token punctuation">,</span>
    <span class="token keyword">const</span> <span class="token keyword">int</span> num
<span class="token punctuation">)</span>
<span class="token punctuation">{<!-- --></span>
    <span class="token keyword">int</span> N1 <span class="token operator">=</span> N <span class="token operator">/</span> num<span class="token punctuation">;</span>
    <span class="token keyword">int</span> M1 <span class="token operator">=</span> M <span class="token operator">/</span> num<span class="token punctuation">;</span>

    <span class="token keyword">float</span> t_sum <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
    <span class="token keyword">float</span> t2_sum <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>

    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> repeat <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> repeat <span class="token operator">&lt;=</span> NUM_REPEATS<span class="token punctuation">;</span> <span class="token operator">++</span>repeat<span class="token punctuation">)</span>
    <span class="token punctuation">{<!-- --></span>
        cudaEvent_t start<span class="token punctuation">,</span> stop<span class="token punctuation">;</span>
        <span class="token function">CHECK</span><span class="token punctuation">(</span><span class="token function">cudaEventCreate</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>start<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token function">CHECK</span><span class="token punctuation">(</span><span class="token function">cudaEventCreate</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>stop<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token function">CHECK</span><span class="token punctuation">(</span><span class="token function">cudaEventRecord</span><span class="token punctuation">(</span>start<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token function">cudaEventQuery</span><span class="token punctuation">(</span>start<span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token comment">// num个CUDA流</span>
        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> num<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span>
        <span class="token punctuation">{<!-- --></span>
            <span class="token keyword">int</span> offset <span class="token operator">=</span> i <span class="token operator">*</span> N1<span class="token punctuation">;</span>
            <span class="token comment">// 使用异步数据传输</span>
            <span class="token function">CHECK</span><span class="token punctuation">(</span><span class="token function">cudaMemcpyAsync</span><span class="token punctuation">(</span>d_x <span class="token operator">+</span> offset<span class="token punctuation">,</span> h_x <span class="token operator">+</span> offset<span class="token punctuation">,</span> M1<span class="token punctuation">,</span> 
                cudaMemcpyHostToDevice<span class="token punctuation">,</span> streams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token function">CHECK</span><span class="token punctuation">(</span><span class="token function">cudaMemcpyAsync</span><span class="token punctuation">(</span>d_y <span class="token operator">+</span> offset<span class="token punctuation">,</span> h_y <span class="token operator">+</span> offset<span class="token punctuation">,</span> M1<span class="token punctuation">,</span> 
                cudaMemcpyHostToDevice<span class="token punctuation">,</span> streams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token keyword">int</span> block_size <span class="token operator">=</span> <span class="token number">128</span><span class="token punctuation">;</span>
            <span class="token keyword">int</span> grid_size <span class="token operator">=</span> <span class="token punctuation">(</span>N1 <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> block_size <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span>
            <span class="token comment">// 非默认CUDA流</span>
            add<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid_size<span class="token punctuation">,</span> block_size<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> streams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>
            <span class="token punctuation">(</span>d_x <span class="token operator">+</span> offset<span class="token punctuation">,</span> d_y <span class="token operator">+</span> offset<span class="token punctuation">,</span> d_z <span class="token operator">+</span> offset<span class="token punctuation">,</span> N1<span class="token punctuation">)</span><span class="token punctuation">;</span>

            <span class="token function">CHECK</span><span class="token punctuation">(</span><span class="token function">cudaMemcpyAsync</span><span class="token punctuation">(</span>h_z <span class="token operator">+</span> offset<span class="token punctuation">,</span> d_z <span class="token operator">+</span> offset<span class="token punctuation">,</span> M1<span class="token punctuation">,</span> 
                cudaMemcpyDeviceToHost<span class="token punctuation">,</span> streams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
</code></pre> 
<h2><a id="_175"></a>总结</h2> 
<p>CUDA流的基础知识总结</p> 
<h3><a id="_177"></a>参考</h3> 
<p>如博客内容有侵权行为，可及时联系删除！<br> CUDA 编程：基础与实践<br> <a href="https://docs.nvidia.com/cuda/" rel="nofollow">https://docs.nvidia.com/cuda/</a><br> <a href="https://docs.nvidia.com/cuda/cuda-runtime-api" rel="nofollow">https://docs.nvidia.com/cuda/cuda-runtime-api</a><br> <a href="https://github.com/brucefan1983/CUDA-Programming">https://github.com/brucefan1983/CUDA-Programming</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/665936a4f61991c0ea18fbda707570ed/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">DeepLabV3&#43;：ASPP加强特征提取网络的搭建</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/adcc0196643cb68582b20592135dfb99/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">yarn安装失败解决方案之一</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>