<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>循环神经网络系列RNN、LSTM及变种 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="循环神经网络系列RNN、LSTM及变种" />
<meta property="og:description" content="摘要
循环神经网络（Recurrent Neural Network, RNN）是一类以序列（sequence）数据为输入，在序列的演进方向进行递归（recursion）且所有节点（循环单元）按链式连接的递归神经网络（recursive neural network）。
人脑记忆原理：
对信息的预测和记忆功能 语言模型对当前输入信息的先后顺序 人脑的存储方式 人脑的记忆算法功能 序列
自然语言处理问题。x1可以看做是第一个单词，x2可以看做是第二个单词，依次类推。 语音处理。此时，每个元素是每帧的声音信号。 时间序列问题。例如每天的股票价格等。 RNN原理
运行过程：
其中每个圆圈可以看作是一个单元，而且每个单元做的事情也是一样的， 因此可以折叠呈左半图的样子。用一句话解释RNN，就是一个单元结构重复使用。 RNN结构的变种
N 对 1
应用：
判断处理序列分类问题，如输入一段文字判别它所属的类别， 输入一个句子判断其情感倾向，输入一段视频并判断它的类别等。 结构图：
1 对 N
应用：
从类别生成语音或音乐，从图像生成文字（image caption）， 此时输入的X就是图像的特征，而输出的y序列就是一段句子。 结构图：
N 对 M
同时也是一种Encoder-Decoder模型。
应用：
机器翻译：Encoder-Decoder的最经典应用，事实上这一结构就是在机器翻译领域最先提出的 文本摘要：输入是一段文本序列，输出是这段文本序列的摘要序列。 阅读理解：将输入的文章和问题分别编码，再对其进行解码得到问题的答案。 语音识别：输入是语音信号序列，输出是文字序列。 结构图：
RNN的训练方法
CNN的训练过程是一个损失递减优化的过程而RNN不同于CNN。
BPTT （back-propagation through time)
沿着需要优化的参数的负梯度方向不断寻找更优的点直至收敛。
sigmoid函数的缺点：
导数值范围为(0,0.25]，反向传播时会导致“梯度消失“。 tanh函数导数值范围更大，相对好一点。 sigmoid函数不是0中心对称，tanh函数是，可以使网络收敛的更好。 激活函数问题用RELU函数解决。 LSTM
LSTM网络通过精妙的门控制将短期记忆与长期记忆结合起来， 并且一定程度上解决了梯度消失的问题。 解决长期依赖问题
例如在语言模型中，需要预测的词本应该在上下文位置较近时， RNN可以使用先前的信息并正确预测，而位置较远时，反之。 举例： LSTM网络结构
原始结构：
使用tanh激活函数对上层细胞和本层输入进行激活后更新细胞状态。 整体上除了h在随时间流动，细胞状态c也在随时间流动，细胞状态c就代表着长期记忆。 LSTM结构" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/ae9980d184408f51a8fba80ab0f0a0c8/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-06-22T14:44:22+08:00" />
<meta property="article:modified_time" content="2020-06-22T14:44:22+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">循环神经网络系列RNN、LSTM及变种</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-github-gist">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><strong>摘要</strong></p> 
<p>循环神经网络（Recurrent Neural Network, RNN）是一类以序列（sequence）数据为输入，在序列的演进方向进行递归（recursion）且所有节点（循环单元）按链式连接的递归神经网络（recursive neural network）。</p> 
<p><strong>人脑记忆原理：</strong></p> 
<pre><code> 对信息的预测和记忆功能
 语言模型对当前输入信息的先后顺序
人脑的存储方式
人脑的记忆算法功能
</code></pre> 
<p><strong>序列</strong></p> 
<pre><code>自然语言处理问题。x1可以看做是第一个单词，x2可以看做是第二个单词，依次类推。
语音处理。此时，每个元素是每帧的声音信号。
时间序列问题。例如每天的股票价格等。
</code></pre> 
<p><strong>RNN原理</strong><br> <img src="https://images2.imgbox.com/93/94/KX2kdOnI_o.png" alt="1"><br> 运行过程：</p> 
<pre><code>其中每个圆圈可以看作是一个单元，而且每个单元做的事情也是一样的，
因此可以折叠呈左半图的样子。用一句话解释RNN，就是一个单元结构重复使用。 
</code></pre> 
<p><img src="https://images2.imgbox.com/f8/26/LCx9habS_o.png" alt="2"><br> <strong>RNN结构的变种</strong></p> 
<p><strong>N 对 1</strong></p> 
<p>应用：</p> 
<pre><code>判断处理序列分类问题，如输入一段文字判别它所属的类别，
输入一个句子判断其情感倾向，输入一段视频并判断它的类别等。
</code></pre> 
<p>结构图：<br> <img src="https://images2.imgbox.com/2b/78/NDnGFW18_o.png" alt="2"></p> 
<p><strong>1 对 N</strong></p> 
<p>应用：</p> 
<pre><code>从类别生成语音或音乐，从图像生成文字（image caption），
此时输入的X就是图像的特征，而输出的y序列就是一段句子。
</code></pre> 
<p>结构图：<br> <img src="https://images2.imgbox.com/1f/b8/rNKI3FMO_o.png" alt="1"><br> <strong>N 对 M</strong></p> 
<p>同时也是一种Encoder-Decoder模型。</p> 
<p>应用：</p> 
<pre><code>机器翻译：Encoder-Decoder的最经典应用，事实上这一结构就是在机器翻译领域最先提出的
文本摘要：输入是一段文本序列，输出是这段文本序列的摘要序列。
阅读理解：将输入的文章和问题分别编码，再对其进行解码得到问题的答案。
语音识别：输入是语音信号序列，输出是文字序列。
</code></pre> 
<p>结构图：<img src="https://images2.imgbox.com/fd/b1/gnpVOScy_o.png" alt="2"><br> <img src="https://images2.imgbox.com/66/c9/wwhpe225_o.png" alt="2"></p> 
<p><strong>RNN的训练方法</strong></p> 
<p>CNN的训练过程是一个损失递减优化的过程而RNN不同于CNN。</p> 
<p><strong>BPTT</strong> （back-propagation through time)</p> 
<p>沿着需要优化的参数的负梯度方向不断寻找更优的点直至收敛。</p> 
<p>sigmoid函数的缺点：</p> 
<pre><code>导数值范围为(0,0.25]，反向传播时会导致“梯度消失“。
tanh函数导数值范围更大，相对好一点。
sigmoid函数不是0中心对称，tanh函数是，可以使网络收敛的更好。
激活函数问题用RELU函数解决。
</code></pre> 
<p><strong>LSTM</strong></p> 
<pre><code>LSTM网络通过精妙的门控制将短期记忆与长期记忆结合起来，
并且一定程度上解决了梯度消失的问题。
</code></pre> 
<p><strong>解决长期依赖问题</strong></p> 
<pre><code>例如在语言模型中，需要预测的词本应该在上下文位置较近时，
RNN可以使用先前的信息并正确预测，而位置较远时，反之。
举例：
</code></pre> 
<p><img src="https://images2.imgbox.com/cb/2f/ghJTLU5B_o.png" alt="1"><br> <img src="https://images2.imgbox.com/42/76/yhJRFddf_o.png" alt="2"><br> <strong>LSTM网络结构</strong></p> 
<p>原始结构：</p> 
<pre><code>使用tanh激活函数对上层细胞和本层输入进行激活后更新细胞状态。
整体上除了h在随时间流动，细胞状态c也在随时间流动，细胞状态c就代表着长期记忆。
</code></pre> 
<p><img src="https://images2.imgbox.com/a0/a3/Iw7GBo8z_o.png" alt="2"><br> <strong>LSTM结构</strong><br> <img src="https://images2.imgbox.com/6e/62/T9paCNsI_o.png" alt="2"><br> <img src="https://images2.imgbox.com/bd/2a/JA9DQcYg_o.png" alt="2"><br> 上图中符合意义：</p> 
<pre><code>黄色的矩形是学习得到的神经网络层
粉色的圆形表示一些运算操作，诸如加法乘法
黑色的单箭头表示向量的传输
两个箭头合成一个表示向量的连接
一个箭头分开表示向量的复制
</code></pre> 
<p>LSTM 的关键就是细胞状态，水平线在图上方贯穿运行。</p> 
<pre><code>细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。
信息在上面流传保持不变会很容易。
</code></pre> 
<p><img src="https://images2.imgbox.com/15/85/jCVloHFt_o.png" alt="2"></p> 
<p>下图这种结构表示一个 sigmoid 神经网络层和一个 pointwise 乘法操作。</p> 
<pre><code>Sigmoid 层输出 0 到 1 之间的数值，描述每个部分有多少量可以通过。
0 代表“不许任何量通过”，1 就指“允许任意量通过”！
</code></pre> 
<p><img src="https://images2.imgbox.com/9a/3e/8frwqKyc_o.png" alt="2"><br> <strong>门的概念</strong></p> 
<p><strong>遗忘门</strong></p> 
<p>第一步，从上层获得的细胞状态我们使用了一个遗忘门Sigmoid函数来读取ht-1和xt，输出0~1的值给每个在细胞状态Ct-1中的数字，1表示完全保留，0表示舍弃。<br> <img src="https://images2.imgbox.com/28/9b/1N92L4Yv_o.png" alt="2"><br> <strong>输入门</strong></p> 
<p>第二步，新的信息的确定：首先、使用Sigmoid层（输入门层）去决定更新什么值it；然后使用tanh层创建一个新的候选值向量Ct，这个Ct被加入到新的细胞状态中。<br> <img src="https://images2.imgbox.com/36/1b/4MjvHbgf_o.png" alt="2"><br> 第三步，更新细胞状态，将Ct-1更新为真正的Ct，我们把旧状态Ct-1与ft相乘，丢弃掉我们确定需要丢弃的信息。接着加上 it*Ct。这就是新的候选值真正的Ct，根据我们决定更新每个状态的程度进行变化。</p> 
<p>到此，我们已经根据输入ht-1和xt和Ct-1完成对细胞状态的一次更新。</p> 
<p>在语言模型的例子中，这就是我们实际根据前面确定的目标，丢弃旧代词的性别信息并添加新的信息的地方。<br> <img src="https://images2.imgbox.com/d1/38/3y6eOrsa_o.png" alt="2"><br> <strong>输出门</strong></p> 
<p>完成了细胞状态更新的同时，还要基于本次更新的细胞状态给到下一个结构一个输出。这里用输出门来完成。<br> 首先，ht-1和xt经过Sigmoid输出为ot，确定哪些输出。<br> 新的细胞状态使用tanh激活后，乘以ot得到最终的输出ht。<br> <img src="https://images2.imgbox.com/62/cb/vzhO0TV4_o.png" alt="2"><br> <strong>LSTM的变体</strong></p> 
<p><strong>第一个：窥视孔连接peephole connection</strong></p> 
<p>与LSTM不同处在于，每一个sigmoid的输入都增加了当前的细胞状态。<br> <img src="https://images2.imgbox.com/5c/29/EjqdTj6H_o.png" alt="2"><br> <strong>第二个：对偶coupled 忘记和输入门</strong></p> 
<p>不同于之前是分开确定什么忘记和需要添加什么新的信息，这里是一同做出决定。我们仅仅会当我们将要输入在当前位置时忘记。我们仅仅输入新的值到那些我们已经忘记旧的信息的那些状态 。<br> <img src="https://images2.imgbox.com/fc/e6/fcLmmayi_o.png" alt="2"></p> 
<p><strong>第三个： Gated Recurrent Unit (GRU)</strong></p> 
<p><a href="https://blog.csdn.net/Uwr44UOuQcNsUQb60zk2/article/details/78888834?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase">参考博客</a></p> 
<p>它将忘记门和输入门合成了一个单一的 更新门。同样还混合了细胞状态和隐藏状态，和其他一些改动。最终的模型比标准的 LSTM 模型要简单，也是非常流行的变体。<br> emmm看起来要复杂多了，慢慢梳理一下也不是很复杂。<br> <img src="https://images2.imgbox.com/50/b8/O9hue99D_o.png" alt="2"></p> 
<p><strong>第四个：SRU</strong></p> 
<p>xt 代表 t 时刻的输入；<br> W、b 代表权重和偏置；<br> ft 代表 t 时刻的遗忘门（forget gate）；<br> rt 代表 t 时刻的重置门（reset gate）；<br> ct 和 ht 分别代表 t 时刻的状态和最终的输出；<br> σ 和 g 分别代表Sigmoid函数和激活函数（tanh、relu）；<br> 公式中的 ⊙ 代表矩阵对应元素间的操作.<br> <img src="https://images2.imgbox.com/26/7a/ktOcbmp5_o.png" alt="2"><br> Github链接：<a href="https://github.com/bamtercelboo/pytorch_SRU">https://github.com/bamtercelboo/pytorch_SRU</a></p> 
<p><strong>双向RNN\LSTM\GRU\SRU</strong></p> 
<p>在经典的循环神经网络中，状态的传输是从前往后单向的。然而，在有些问题中，当前时刻的输出不仅和之前的状态有关系，也和之后的状态相关。这时就需要双向RNN（BiRNN）来解决这类问题。例如预测一个语句中缺失的单词不仅需要根据前文来判断，也需要根据后面的内容，这时双向RNN就可以发挥它的作用。<br> 双向RNN的主题结构就是两个单向RNN的结合。在每一个时刻t，输入会同时提供给这两个方向相反的RNN，而输出则是由这两个单向RNN共同决定（可以拼接或者求和等）。<br> 同样地，将双向RNN中的RNN替换成LSTM或者GRU结构，则组成了BiLSTM和BiGRU。<br> <img src="https://images2.imgbox.com/d0/bb/XqK7NEkg_o.png" alt="2"><br> <strong>两个问题</strong></p> 
<p>RNN与CNN区别？</p> 
<p>相同点：</p> 
<pre><code>都有参数共享
传统的神经网络的扩展
通过前向传播反向更新并参数共享
</code></pre> 
<p>不同：</p> 
<pre><code>RNN的wuv参数共享，cnn参数共享多少跟卷积核大小有关
cnn是空间的扩展，rnn是序列的扩展
cnn没有记忆
</code></pre> 
<p>RNN与LSTM区别？</p> 
<pre><code>lstm在rnn的结构上增加了对过去状态的过滤
rnn没有门的概念
lstm可以处理较长期的问题
rnn只能处理短期问题
</code></pre> 
<p>本文参考：<a href="https://www.jianshu.com/p/9dc9f41f0b29" rel="nofollow">https://www.jianshu.com/p/9dc9f41f0b29</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e45cd3a39842a95c4086d0a33eb1d204/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Javase基础（二）——数据类型</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/037f1ae297ee1e8479e39e909111872f/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Qt图片交互——QGraphicsView&#43;鼠标选点&#43;放大缩小&#43;OpenCV</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>