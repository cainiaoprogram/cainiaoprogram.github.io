<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>NLP 中的Mask全解 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="NLP 中的Mask全解" />
<meta property="og:description" content="文章目录 Mask的作用处理非定长序列RNN中的MaskAttention中Mask防止标签泄露Transformer中的MaskBERT中的MaskXLNet中的Mask 转载来源：https://zhuanlan.zhihu.com/p/139595546 Mask 在NLP中是一个很常规的操作，也有多种应用的场景和形式，下面尝试从以下几个方面去全（用了夸张的修辞手法）解Mask，并尽可能地辅以图片说明和代码解释： Mask的作用： 处理非定长序列 RNN中的MaskAttention中Mask 防止标签泄露 Transformer中的MaskBERT中的MaskXLNet中的Mask Mask的作用 对于NLP中mask的作用，先上结论：
1、padding mask：处理非定长序列，区分padding和非padding部分，如在RNN等模型和Attention机制中的应用等
2、sequence mask：防止标签泄露，如：Transformer decoder中的mask矩阵，BERT中的[Mask]位，XLNet中的mask矩阵等
PS：padding mask 和 sequence mask非官方命名
处理非定长序列 在NLP中，文本一般是不定长的，所以在进行 batch训练之前，要先进行长度的统一，过长的句子可以通过truncating 截断到固定的长度，过短的句子可以通过 padding 增加到固定的长度，但是 padding 对应的字符只是为了统一长度，并没有实际的价值，因此希望在之后的计算中屏蔽它们，这时候就需要 Mask。
图片参考
上图为中文场景下，一个 batch=5 的，以字为单位的输入矩阵（也可以在分词后以词为单位）和 mask 矩阵，左图已经将文本 padding 到统一长度了，右图中的1表示有效字，0代表无效字。
RNN中的Mask 对于RNN等模型，本身是可以直接处理不定长数据的，因此它不需要提前告知 sequence length，如下是pytorch下的LSTM定义：
nn.LSTM(input_size, hidden_size, *args, **kwargs) 但是在实践中，为了 batch 训练，一般会把不定长的序列 padding 到相同长度，再用 mask 去区分非 padding 部分和 padding 部分。
区分的目的是使得RNN只作用到它实际长度的句子，而不会处理无用的 padding 部分，这样RNN的输出和隐状态都会是对应句子实际的最后一位。另外，对于token级别的任务，也可以通过mask去忽略 padding 部分对应的loss。
不过，在 pytorch 中，对 mask 的具体实现形式不是mask矩阵，而是通过一个句子长度列表来实现的，但本质一样。实现如下，sentence_lens 表示的是这个batch中每一个句子的实际长度。参考" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/f8e8c94357962b0ecff4d7380e7b9427/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-06-08T23:02:06+08:00" />
<meta property="article:modified_time" content="2020-06-08T23:02:06+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">NLP 中的Mask全解</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-dracula">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#Mask_12" rel="nofollow">Mask的作用</a></li><li><a href="#_17" rel="nofollow">处理非定长序列</a></li><li><a href="#RNNMask_22" rel="nofollow">RNN中的Mask</a></li><li><a href="#AttentionMask_47" rel="nofollow">Attention中Mask</a></li><li><a href="#_69" rel="nofollow">防止标签泄露</a></li><li><a href="#TransformerMask_71" rel="nofollow">Transformer中的Mask</a></li><li><a href="#BERTMask_125" rel="nofollow">BERT中的Mask</a></li><li><a href="#XLNetMask_133" rel="nofollow">XLNet中的Mask</a></li></ul> 
</div> 
<br> 转载来源：https://zhuanlan.zhihu.com/p/139595546 
<br> 
<img src="https://images2.imgbox.com/b9/de/49SXG30M_o.png" alt="alt"> 
<br> Mask 在NLP中是一个很常规的操作，也有多种应用的场景和形式，下面尝试从以下几个方面去全（用了夸张的修辞手法）解Mask，并尽可能地辅以图片说明和代码解释： 
<p></p> 
<ul><li>Mask的作用： 
  <ul><li>处理非定长序列 
    <ul><li>RNN中的Mask</li><li>Attention中Mask</li></ul> </li><li>防止标签泄露 
    <ul><li>Transformer中的Mask</li><li>BERT中的Mask</li><li>XLNet中的Mask</li></ul> </li></ul> </li></ul> 
<h2><a id="Mask_12"></a>Mask的作用</h2> 
<p>对于NLP中mask的作用，先上结论：</p> 
<blockquote> 
 <p>1、padding mask：处理非定长序列，区分padding和非padding部分，如在RNN等模型和Attention机制中的应用等<br> 2、sequence mask：防止标签泄露，如：Transformer decoder中的mask矩阵，BERT中的[Mask]位，XLNet中的mask矩阵等<br> PS：padding mask 和 sequence mask非官方命名</p> 
</blockquote> 
<h2><a id="_17"></a>处理非定长序列</h2> 
<p>在NLP中，文本一般是不定长的，所以在进行 batch训练之前，要先进行长度的统一，过长的句子可以通过truncating 截断到固定的长度，过短的句子可以通过 padding 增加到固定的长度，但是 padding 对应的字符只是为了统一长度，并没有实际的价值，因此希望在之后的计算中屏蔽它们，这时候就需要 Mask。<br> <img src="https://images2.imgbox.com/1a/49/f9Io2N3f_o.png" alt="alt"><br> <a href="https://link.zhihu.com/?target=https%3A//www.cnblogs.com/neopenx/p/4806006.html" rel="nofollow">图片参考</a><br> 上图为中文场景下，一个 batch=5 的，以字为单位的输入矩阵（也可以在分词后以词为单位）和 mask 矩阵，左图已经将文本 padding 到统一长度了，右图中的1表示有效字，0代表无效字。</p> 
<h2><a id="RNNMask_22"></a>RNN中的Mask</h2> 
<p>对于RNN等模型，本身是可以直接处理不定长数据的，因此它不需要提前告知 sequence length，如下是pytorch下的LSTM定义：</p> 
<pre><code class="prism language-python">nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
</code></pre> 
<p>但是在实践中，为了 batch 训练，一般会把不定长的序列 padding 到相同长度，再用 mask 去区分非 padding 部分和 padding 部分。</p> 
<p>区分的目的是使得RNN只作用到它实际长度的句子，而不会处理无用的 padding 部分，这样RNN的输出和隐状态都会是对应句子实际的最后一位。另外，对于token级别的任务，也可以通过mask去忽略 padding 部分对应的loss。</p> 
<p>不过，在 pytorch 中，对 mask 的具体实现形式不是mask矩阵，而是通过一个句子长度列表来实现的，但本质一样。实现如下，sentence_lens 表示的是这个batch中每一个句子的实际长度。<a href="https://zhuanlan.zhihu.com/p/34418001" rel="nofollow">参考</a></p> 
<pre><code class="prism language-python">embed_input_x_packed <span class="token operator">=</span> pack_padded_sequence<span class="token punctuation">(</span>embed_input_x<span class="token punctuation">,</span> sentence_lens<span class="token punctuation">,</span> batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
encoder_outputs_packed<span class="token punctuation">,</span> <span class="token punctuation">(</span>h_last<span class="token punctuation">,</span> c_last<span class="token punctuation">)</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>lstm<span class="token punctuation">(</span>embed_input_x_packed<span class="token punctuation">)</span>
encoder_outputs<span class="token punctuation">,</span> _ <span class="token operator">=</span> pad_packed_sequence<span class="token punctuation">(</span>encoder_outputs_packed<span class="token punctuation">,</span> batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre> 
<p>btw，在 pytorch 的 Embedding 和 Loss 中也有对 padding 值的设置：</p> 
<pre><code class="prism language-python"><span class="token comment"># padding_idx (int, optional): If given, pads the output with the embedding vector at </span>
<span class="token comment"># `padding_idx` (initialized to zeros) whenever it encounters the index.</span>
embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embed_dim<span class="token punctuation">,</span> padding_idx<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

<span class="token comment"># ignore_index (int, optional): Specifies a target value that is ignored</span>
<span class="token comment"># and does not contribute to the input gradient.</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span>ignore_index<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
</code></pre> 
<h2><a id="AttentionMask_47"></a>Attention中Mask</h2> 
<p>在 Attention 机制中，同样需要忽略 padding 部分的影响，这里以transformer encoder中的self-attention为例：</p> 
<p>self-attention中，Q和K在点积之后，需要先经过mask再进行softmax，因此，对于要屏蔽的部分，mask之后的输出需要为负无穷，这样softmax之后输出才为0。<br> <img src="https://images2.imgbox.com/9b/fc/iKBcs04I_o.png" alt="alt"><br> <a href="https://link.zhihu.com/?target=https%3A//jalammar.github.io/illustrated-transformer/" rel="nofollow">图片参考</a></p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">attention</span><span class="token punctuation">(</span>query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token string">"Compute 'Scaled Dot Product Attention'"</span>
    d_k <span class="token operator">=</span> query<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>query<span class="token punctuation">,</span> key<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> \
             <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>d_k<span class="token punctuation">)</span>
    <span class="token keyword">if</span> mask <span class="token keyword">is</span> <span class="token operator">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        scores <span class="token operator">=</span> scores<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>mask <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1e9</span><span class="token punctuation">)</span> <span class="token comment"># mask步骤，用 -1e9 代表负无穷</span>
    p_attn <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>scores<span class="token punctuation">,</span> dim <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> dropout <span class="token keyword">is</span> <span class="token operator">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        p_attn <span class="token operator">=</span> dropout<span class="token punctuation">(</span>p_attn<span class="token punctuation">)</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>p_attn<span class="token punctuation">,</span> value<span class="token punctuation">)</span><span class="token punctuation">,</span> p_attn
</code></pre> 
<p><a href="https://link.zhihu.com/?target=http%3A//nlp.seas.harvard.edu/2018/04/03/attention.html%23attention" rel="nofollow">代码参考</a></p> 
<p>PS：上述两个参考都是非常好的 transformer 介绍文章，参考1，图片与文字相得益彰，参考2，代码与讲解相辅相成。</p> 
<h2><a id="_69"></a>防止标签泄露</h2> 
<p>在语言模型中，常常需要从上一个词预测下一个词，但如果要在LM中应用 self attention 或者是同时使用上下文的信息，要想不泄露要预测的标签信息，就需要 mask 来“遮盖”它。不同的mask方式，也对应了一篇篇的paper，这里选取典型的几个。</p> 
<h2><a id="TransformerMask_71"></a>Transformer中的Mask</h2> 
<p>Transformer 是包括 Encoder和 Decoder的，Encoder中 self-attention 的 padding mask 如上，而 Decoder 还需要防止标签泄露，即在 t 时刻不能看到 t 时刻之后的信息，因此在上述 padding mask的基础上，还要加上 sequence mask。</p> 
<p>sequence mask 一般是通过生成一个上三角矩阵来实现的，上三角区域对应要mask的部分。</p> 
<p>在Transformer 的 Decoder中，先不考虑 padding mask，一个包括四个词的句子[A,B,C,D]在计算了相似度scores之后，得到下面第一幅图，将scores的上三角区域mask掉，即替换为负无穷，再做softmax得到第三幅图。这样，比如输入 B 在self-attention之后，也只和A，B有关，而与后序信息无关。</p> 
<p>因为在softmax之后的加权平均中: B’ = 0.48<em>A+0.52</em>B，而 C，D 对 B’不做贡献。<br> <img src="https://images2.imgbox.com/74/34/n9WmRYYH_o.png" alt="alt"><br> <a href="https://link.zhihu.com/?target=https%3A//baijiahao.baidu.com/s%3Fid%3D1652093322137148754%26wfr%3Dspider%26for%3Dpc" rel="nofollow">图片参考</a></p> 
<p>实际应用中，Decoder 需要结合 padding mask 和 sequence mask，下面在pytorch框架下以一个很简化的例子展示 Transformer 中 的两种 Mask。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch

<span class="token keyword">def</span> <span class="token function">padding_mask</span><span class="token punctuation">(</span>seq<span class="token punctuation">,</span> pad_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span>seq <span class="token operator">!=</span> pad_idx<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span>   <span class="token comment"># [B, 1, L]</span>

<span class="token keyword">def</span> <span class="token function">sequence_mask</span><span class="token punctuation">(</span>seq<span class="token punctuation">)</span><span class="token punctuation">:</span>
    batch_size<span class="token punctuation">,</span> seq_len <span class="token operator">=</span> seq<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
    mask <span class="token operator">=</span> <span class="token number">1</span><span class="token operator">-</span> torch<span class="token punctuation">.</span>triu<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span>seq_len<span class="token punctuation">,</span> seq_len<span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span><span class="token punctuation">,</span>diagonal<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
    mask <span class="token operator">=</span> mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [B, L, L]</span>
    <span class="token keyword">return</span> mask

<span class="token keyword">def</span> <span class="token function">test</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 以最简化的形式测试Transformer的两种mask</span>
    seq <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># batch_size=1, seq_len=3，padding_idx=0</span>
    embedding <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>num_embeddings<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> embedding_dim<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> padding_idx<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
    query<span class="token punctuation">,</span> key <span class="token operator">=</span> embedding<span class="token punctuation">(</span>seq<span class="token punctuation">)</span><span class="token punctuation">,</span> embedding<span class="token punctuation">(</span>seq<span class="token punctuation">)</span>
    scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>query<span class="token punctuation">,</span> key<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    mask_p <span class="token operator">=</span> padding_mask<span class="token punctuation">(</span>seq<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
    mask_s <span class="token operator">=</span> sequence_mask<span class="token punctuation">(</span>seq<span class="token punctuation">)</span>
    mask_decoder <span class="token operator">=</span> mask_p <span class="token operator">&amp;</span> mask_s <span class="token comment"># 结合 padding mask 和 sequence mask</span>

    scores_encoder <span class="token operator">=</span> scores<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>mask_p<span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1e9</span><span class="token punctuation">)</span> <span class="token comment"># 对于scores，在mask==0的位置填充</span>
    scores_decoder <span class="token operator">=</span> scores<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>mask_decoder<span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1e9</span><span class="token punctuation">)</span>

test<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>对应的各mask值为：</p> 
<pre><code class="prism language-python"><span class="token comment"># mask_p</span>
<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span> <span class="token number">1</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
<span class="token comment"># mask_s</span>
<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span> <span class="token number">0</span> <span class="token number">0</span><span class="token punctuation">]</span>
  <span class="token punctuation">[</span><span class="token number">1</span> <span class="token number">1</span> <span class="token number">0</span><span class="token punctuation">]</span>
  <span class="token punctuation">[</span><span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
<span class="token comment"># mask_decoder</span>
<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span> <span class="token number">0</span> <span class="token number">0</span><span class="token punctuation">]</span>
  <span class="token punctuation">[</span><span class="token number">1</span> <span class="token number">1</span> <span class="token number">0</span><span class="token punctuation">]</span>
  <span class="token punctuation">[</span><span class="token number">1</span> <span class="token number">1</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
</code></pre> 
<p>可以看到 mask_decoder 的第三列为0 ，对应padding mask，上三角为0，对应sequence mask。</p> 
<h2><a id="BERTMask_125"></a>BERT中的Mask</h2> 
<p>BERT实际上是Transformer的Encoder，为了在语言模型的训练中，使用上下文信息又不泄露标签信息，采用了Masked LM，简单来说就是随机的选择序列的部分token用 [Mask] 标记代替。</p> 
<p>这波Mask操作，思想很直接，实现很简单，效果很惊人。<br> <img src="https://images2.imgbox.com/a9/6e/hwrzD9az_o.png" alt="alt"><br> BERT之后，也有不少在Mask的范围和形式上做文章的，比如：ERNIE，但大同小异，不多赘述。</p> 
<p>而XLNet的Mask操作非常的巧(nan)妙(dong)，如下。</p> 
<h2><a id="XLNetMask_133"></a>XLNet中的Mask</h2> 
<p>XLNet通过Permutation Language Modeling实现了不在输入中加[Mask]，同样可以利用上下文信息，关键的操作就是下面所示的 Attention Mask 机制。<br> <img src="https://images2.imgbox.com/f6/f9/WvYJkrZM_o.png" alt="alt"><br> 但并不是那么好理解，要理解XLNet中的Mask，一定要先看张俊林老师的：<a href="https://zhuanlan.zhihu.com/p/70257427" rel="nofollow">XLNet:运行机制及和Bert的异同比较</a>，再来看下面的内容。上图也是引自该文，这里再引用一句我认为非常关键的一段话：</p> 
<blockquote> 
 <p>在Transformer内部，通过Attention掩码，从X的输入单词里面，也就是Ti的上文和下文单词中，随机选择i-1个，放到Ti的上文位置中，把其它单词的输入通过Attention掩码隐藏掉，于是就能够达成我们期望的目标（当然这个所谓放到Ti的上文位置，只是一种形象的说法，其实在内部，就是通过Attention Mask，把其它没有被选到的单词Mask掉，不让它们在预测单词Ti的时候发生作用，如此而已。看着就类似于把这些被选中的单词放到了上文Context_before的位置了）</p> 
</blockquote> 
<p>对于排列序列：3-&gt;2-&gt;4-&gt;1，通过 Attention Mask，在 self-attention 的加权平均计算中，以上图中的<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          E 
         
         
         
           2 
          
          
           
          
            ′ 
           
          
         
        
       
      
        E_{2{}'} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32798em;"><span class="" style="top: -2.55em; margin-left: -0.05764em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight"><span class="mord mtight"></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.682829em;"><span class="" style="top: -2.786em; margin-right: 0.0714286em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>为例：</p> 
<p>self-attention 计算之后Content stream中的 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          E 
         
         
         
           2 
          
         
           ′ 
          
         
        
       
         = 
        
        
        
          a 
         
        
          2 
         
        
        
        
          E 
         
        
          2 
         
        
       
         + 
        
        
        
          a 
         
        
          3 
         
        
        
        
          E 
         
        
          3 
         
        
       
      
        E_{2'}=a_{2}E_{2}+a_{3}E_{3} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32798em;"><span class="" style="top: -2.55em; margin-left: -0.05764em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.682829em;"><span class="" style="top: -2.786em; margin-right: 0.0714286em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.05764em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.05764em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>，其中<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          E 
         
        
          2 
         
        
       
      
        E_{2} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.05764em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>表示第2个词对应的向量，其他同理。这样在<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          E 
         
         
         
           2 
          
         
           ′ 
          
         
        
       
      
        E_{2'} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32798em;"><span class="" style="top: -2.55em; margin-left: -0.05764em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.682829em;"><span class="" style="top: -2.786em; margin-right: 0.0714286em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>中就看到了它的下文<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          E 
         
        
          3 
         
        
       
      
        E_{3} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.05764em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>，就好像是把<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          E 
         
        
          3 
         
        
       
      
        E_{3} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.05764em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>放到了它的上文位置一样，但实际顺序并没有改变。</p> 
<p>对序列进行排列的目的是为了生成这个 Attention Mask，再加上双流注意力去解决预测歧义的问题，可以说就是Permutation Language Modeling 的全部了。</p> 
<p>在评论中有一个非常好的问题：为什么不直接在attention掩码矩阵中只把当前的单词掩盖住来获取上下文的特征呢？直接mask住左上到右下的对角线构建双向语言模型不行吗？</p> 
<p>XLNet实际上仍然遵循语言模型的预测模式，即从左往右依次预测，如对于排列序列：3-&gt;2-&gt;4-&gt;1，预测 2 时用到了 3 信息，预测 4 时用到了3、2信息.……因此，它本质还是需要一个上三角/下三角mask矩阵，在上图的content stream矩阵中，把第4行的左边两个红点移到第4列中去，就变成了一个三角矩阵了，和Transformer decoder中的mask矩阵一样。</p> 
<p>那为什么要这样的三角矩阵呢？直接mask住左上到右下的对角线可以嘛？答案是不行，mask掉对角线在预测时可以不看到当前词，但是会提前看到后面的词，这样在预测后面词的时候就相当于提前发生了泄露。</p> 
<p>另外，要提及的一点是，XLNet的从左到右和一般语言模型（如GPT）的从左到右稍微有一点区别，GPT的输入和输出是错位一个时间步的，即读入 1,2,3，然后在 3 的位置预测 4；而 XLNet 的输入和输出时间步数是相同的（这一点类似于 BERT），输入 1,2,3,4 并在 4 的位置预测 4。当然，XLNet 不会读取第 4 个时间步的单词（否则有信息泄露），仅仅利用位置 4 的 position embedding，告诉模型现在想要预测第 4 个位置，所以最终预测是用的query stream，不含当前时间步信息。这一小点<a href="https://www.zhihu.com/question/330307904/answer/721986216" rel="nofollow">参考</a></p> 
<p>到这里，就是本文全部的Mask，但这肯定不是NLP中Mask的全部，但希望能帮助你去更好地理解Mask。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f175d4d76eb9940460eb389aaac66320/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">使用Python和Numpy进行波士顿房价预测任务（二）【深度学习入门_学习笔记】</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/34bac1346a041256ae888631f80b9bf4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">flex&#43;bison运行测试一个小例子</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>