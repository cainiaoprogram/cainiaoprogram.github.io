<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【深度学习】 基于Keras的Attention机制代码实现及剖析——Dense&#43;Attention - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【深度学习】 基于Keras的Attention机制代码实现及剖析——Dense&#43;Attention" />
<meta property="og:description" content="说明 大部分代码来源于网上，但网上的代码一下子可能难以入门或因版本原因报错，此处整理后进行详细分析。
参考的代码来源1：Attention mechanism Implementation for Keras.网上大部分代码都源于此，直接使用时注意Keras版本，若版本不对应，在merge处会报错，解决办法为：导入Multiply层并将attention_dense.py第17行的：
attention_mul = merge([inputs, attention_probs], output_shape=32, name=‘attention_mul’, mode=‘mul’)，改为：attention_mul = Multiply()([inputs, attention_probs])即可。
参考的代码来源2：[深度应用]·Keras极简实现Attention结构。这相当于来源1的简化版本，其将注意力层还做了封装，可直接使用。但此方法运用了两个注意力层，使我有些不太理解，这个问题在后面会进行讨论。
本文主体将在来源1的基础上进行分析探讨。Attention机制大致过程就是分配权重，所有用到权重的地方都可以考虑使用它，另外它是一种思路，不局限于深度学习的实现方法，此处仅代码上分析，且为深度学习的实现版本。更多理论请看解读大牛文章深度学习中的注意力机制(2017版)，还可以看解读这篇文章的大牛文章：[深度概念]·Attention机制实践解读。此处仅介绍Dense&#43;Attention，进阶篇LSTM&#43;Attention请看【深度学习】 基于Keras的Attention机制代码实现及剖析——LSTM&#43;Attention。 如果你对本系列感兴趣，可接以下传送门：
目录 说明实验目的数据集构造模型搭建模型训练及验证拓展多分类问题时，Attention效果如何？拓展总结 实验目的 在简单的分类模型(如最简的全连接网络)基础上实现Attention机制的运用。检验Attention是否真的捕捉到了关键特征，即被Attention分配的关键特征的权重是否更高。在已有的模型基础上适当做些变化，如调参或新加层，看看Attention的稳定性如何。 数据集构造 因为是在分类问题上进行应用，所以需要构造特征(X)和标签(Y)，此处数据随机产生，但为了进行Attention机制的有效性验证，我们将特征X的某一列置成和标签完全相同，如果Attention有效，那么模型学出来，自然这一列的权重就要最高。
默认设置：attention_column=1，即将 第“1”列(从0开始数) 与标签置成相同。
同时为了简化问题，将分类设置为二分类问题，即randint的参数high设置为2。(注意randint是左闭右开，所以当high=2时，y要么为0，要么为1)
def get_data(n, input_dim, attention_column=1): &#34;&#34;&#34; Data generation. x is purely random except that it&#39;s first value equals the target y. In practice, the network should learn that the target = x[attention_column]. Therefore, most of its attention should be focused on the value addressed by attention_column." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/e6f1181b159989a9dd7357e78244d400/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-09-21T08:18:51+08:00" />
<meta property="article:modified_time" content="2022-09-21T08:18:51+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【深度学习】 基于Keras的Attention机制代码实现及剖析——Dense&#43;Attention</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night-eighties">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_0"></a>说明</h2> 
<ol><li>大部分代码来源于网上，但网上的代码一下子可能难以入门或因版本原因报错，此处整理后进行详细分析。<br>   参考的<strong>代码来源1</strong>：<a href="https://github.com/philipperemy/keras-attention-mechanism">Attention mechanism Implementation for Keras.</a>网上大部分代码都源于此，直接使用时注意Keras版本，若版本不对应，在merge处会报错，解决办法为：导入Multiply层并将attention_dense.py第17行的：<br> attention_mul = merge([inputs, attention_probs], output_shape=32, name=‘attention_mul’, mode=‘mul’)，改为：attention_mul = Multiply()([inputs, attention_probs])即可。<br>   参考的<strong>代码来源2</strong>：<a href="https://blog.csdn.net/xiaosongshine/article/details/90579679">[深度应用]·Keras极简实现Attention结构</a>。这相当于来源1的简化版本，其将注意力层还做了封装，可直接使用。但此方法运用了两个注意力层，使我有些不太理解，这个问题在后面会进行讨论。<br>   <strong>本文主体将在来源1的基础上进行分析探讨。</strong></li><li>Attention机制大致过程就是分配权重，所有用到权重的地方都可以考虑使用它，另外它是一种思路，不局限于深度学习的实现方法，此处仅代码上分析，且为深度学习的实现版本。更多理论请看解读大牛文章<a href="https://blog.csdn.net/malefactor/article/details/78767781">深度学习中的注意力机制(2017版)</a>，还可以看解读这篇文章的大牛文章：<a href="https://blog.csdn.net/xiaosongshine/article/details/90573585">[深度概念]·Attention机制实践解读</a>。</li><li>此处仅介绍Dense+Attention，进阶篇LSTM+Attention请看<a href="https://blog.csdn.net/qq_34862636/article/details/103472650">【深度学习】 基于Keras的Attention机制代码实现及剖析——LSTM+Attention</a>。</li></ol> 
<p>  如果你对本系列感兴趣，可接以下传送门：<br> </p> 
<div class="toc"> 
 <h4>目录</h4> 
 <ul><li><a href="#_0" rel="nofollow">说明</a></li><li><a href="#_15" rel="nofollow">实验目的</a></li><li><a href="#_20" rel="nofollow">数据集构造</a></li><li><a href="#_42" rel="nofollow">模型搭建</a></li><li><a href="#_64" rel="nofollow">模型训练及验证</a></li><li><a href="#_225" rel="nofollow">拓展</a></li><li><ul><li><a href="#Attention_227" rel="nofollow">多分类问题时，Attention效果如何？</a></li><li><a href="#_257" rel="nofollow">拓展总结</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="_15"></a>实验目的</h2> 
<ol><li>在简单的分类模型(如最简的全连接网络)基础上实现Attention机制的运用。</li><li>检验Attention是否真的捕捉到了关键特征，即被Attention分配的关键特征的权重是否更高。</li><li>在已有的模型基础上适当做些变化，如调参或新加层，看看Attention的稳定性如何。</li></ol> 
<h2><a id="_20"></a>数据集构造</h2> 
<p>  因为是在分类问题上进行应用，所以需要构造特征(X)和标签(Y)，此处数据随机产生，但为了进行Attention机制的有效性验证，我们将特征X的某一列置成和标签完全相同，如果Attention有效，那么模型学出来，自然这一列的权重就要最高。<br>   默认设置：attention_column=1，即将 <strong>第“1”列(从0开始数)</strong> 与标签置成相同。<br>   同时为了简化问题，将分类设置为<strong>二分类问题</strong>，即randint的参数high设置为2。(注意randint是左闭右开，所以当high=2时，y要么为0，要么为1)</p> 
<pre><code class="prism language-py"><span class="token keyword">def</span> <span class="token function">get_data</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> input_dim<span class="token punctuation">,</span> attention_column<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Data generation. x is purely random except that it's first value equals the target y.
    In practice, the network should learn that the target = x[attention_column].
    Therefore, most of its attention should be focused on the value addressed by attention_column.
    :param n: the number of samples to retrieve.
    :param input_dim: the number of dimensions of each element in the series.
    :param attention_column: the column linked to the target. Everything else is purely random.
    :return: x: model inputs, y: model targets
    """</span>
    x <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>standard_normal<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> input_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
    y <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span>low<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> high<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> attention_column<span class="token punctuation">]</span> <span class="token operator">=</span> y<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>
    <span class="token keyword">return</span> x<span class="token punctuation">,</span> y
</code></pre> 
<p>   我们输出X,Y的前三行，看看是不是和我们想要的一致。可以看到每一个x∈X的“第1列”都等于标签号，一致了。<br> <img src="https://images2.imgbox.com/fa/10/6H0igwyN_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_42"></a>模型搭建</h2> 
<p>  下面开始在单隐层全连接网络的基础上用keras搭建注意力层。</p> 
<pre><code class="prism language-py"><span class="token keyword">def</span> <span class="token function">build_model</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    K<span class="token punctuation">.</span>clear_session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#清除之前的模型，省得压满内存</span>
    inputs <span class="token operator">=</span> Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">#输入层</span>

    <span class="token comment"># ATTENTION PART STARTS HERE 注意力层</span>
    attention_probs <span class="token operator">=</span> Dense<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'attention_vec'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
    attention_mul <span class="token operator">=</span>  Multiply<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">[</span>inputs<span class="token punctuation">,</span> attention_probs<span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token comment"># ATTENTION PART FINISHES HERE</span>

    attention_mul <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">(</span>attention_mul<span class="token punctuation">)</span> <span class="token comment">#原始的全连接</span>
    output <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>attention_mul<span class="token punctuation">)</span> <span class="token comment">#输出层</span>
    model <span class="token operator">=</span> Model<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span><span class="token punctuation">[</span>inputs<span class="token punctuation">]</span><span class="token punctuation">,</span> output<span class="token operator">=</span>output<span class="token punctuation">)</span>
    <span class="token keyword">return</span> model
</code></pre> 
<p>  可以看到注意力层就两行代码，分别是一个Dense(全连接)层和一个Multiply操作，注意Multiply是对应元素相乘。<br> <img src="https://images2.imgbox.com/9b/33/HzqAYjer_o.png" alt="在这里插入图片描述"><br>   如果画出加Attention前后的结构图，可如下图所示：<br> <img src="https://images2.imgbox.com/71/d9/mPLPgbg4_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_64"></a>模型训练及验证</h2> 
<p>  设置随机种子可以调试对比每次结果的异同，输入必要参数，调用模型就可以开始训练了，由于是二分类问题，所以损失函数用二分类交叉熵。训练集测试集8:2进行验证。</p> 
<pre><code class="prism language-py"><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">1337</span><span class="token punctuation">)</span>  <span class="token comment"># for reproducibility</span>
    input_dim <span class="token operator">=</span> <span class="token number">32</span> <span class="token comment">#特征数</span>
    N <span class="token operator">=</span> <span class="token number">10000</span> <span class="token comment">#数据集总记录数</span>
    inputs_1<span class="token punctuation">,</span> outputs <span class="token operator">=</span> get_data<span class="token punctuation">(</span>N<span class="token punctuation">,</span> input_dim<span class="token punctuation">)</span> <span class="token comment">#构造数据集</span>

    m <span class="token operator">=</span> build_model<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#构造模型</span>
    m<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer<span class="token operator">=</span><span class="token string">'adam'</span><span class="token punctuation">,</span> loss<span class="token operator">=</span><span class="token string">'binary_crossentropy'</span><span class="token punctuation">,</span> metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    m<span class="token punctuation">.</span>summary<span class="token punctuation">(</span><span class="token punctuation">)</span>

    m<span class="token punctuation">.</span>fit<span class="token punctuation">(</span><span class="token punctuation">[</span>inputs_1<span class="token punctuation">]</span><span class="token punctuation">,</span> outputs<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> validation_split<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span>
</code></pre> 
<p>  后台会输出我们的模型架构，看看是不是和设计的一样：<br> <img src="https://images2.imgbox.com/a2/0d/y2qEZJVb_o.png" alt="在这里插入图片描述"><br>   之前设置的是20个Epoch，看看训练结果：<br> <img src="https://images2.imgbox.com/d8/17/CMVYLKnY_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/f6/70/3JBaBKxl_o.png" alt="在这里插入图片描述"><br>   有一列特征与标签完全相同，这么简单的规律神经网络显然轻而易举地学出来了，训练集和测试集正确率都到了100%，那么注意力机制发挥了什么作用呢？我们来进行可视化看看。<br>   可视化的思路为：将注意力中学到的α绘制成柱状图，根据我们之前的设置，特征“第1列”的权重要更高乃至最高。这涉及到神经网络中间层的输出，具体在下面这个函数：</p> 
<pre><code class="prism language-py"><span class="token keyword">def</span> <span class="token function">get_activations</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> print_shape_only<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> layer_name<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># Documentation is available online on Github at the address below.</span>
    <span class="token comment"># From: https://github.com/philipperemy/keras-visualize-activations</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'----- activations -----'</span><span class="token punctuation">)</span>
    activations <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    inp <span class="token operator">=</span> model<span class="token punctuation">.</span><span class="token builtin">input</span>
    <span class="token keyword">if</span> layer_name <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        outputs <span class="token operator">=</span> <span class="token punctuation">[</span>layer<span class="token punctuation">.</span>output <span class="token keyword">for</span> layer <span class="token keyword">in</span> model<span class="token punctuation">.</span>layers<span class="token punctuation">]</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        outputs <span class="token operator">=</span> <span class="token punctuation">[</span>layer<span class="token punctuation">.</span>output <span class="token keyword">for</span> layer <span class="token keyword">in</span> model<span class="token punctuation">.</span>layers <span class="token keyword">if</span> layer<span class="token punctuation">.</span>name <span class="token operator">==</span> layer_name<span class="token punctuation">]</span>  <span class="token comment"># all layer outputs</span>
    funcs <span class="token operator">=</span> <span class="token punctuation">[</span>K<span class="token punctuation">.</span>function<span class="token punctuation">(</span><span class="token punctuation">[</span>inp<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span>K<span class="token punctuation">.</span>learning_phase<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>out<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> out <span class="token keyword">in</span> outputs<span class="token punctuation">]</span>  <span class="token comment"># evaluation functions</span>
    layer_outputs <span class="token operator">=</span> <span class="token punctuation">[</span>func<span class="token punctuation">(</span><span class="token punctuation">[</span>inputs<span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">for</span> func <span class="token keyword">in</span> funcs<span class="token punctuation">]</span>
    <span class="token keyword">for</span> layer_activations <span class="token keyword">in</span> layer_outputs<span class="token punctuation">:</span>
        activations<span class="token punctuation">.</span>append<span class="token punctuation">(</span>layer_activations<span class="token punctuation">)</span>
        <span class="token keyword">if</span> print_shape_only<span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span>layer_activations<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span>layer_activations<span class="token punctuation">)</span>
    <span class="token keyword">return</span> activations
</code></pre> 
<p>  这个函数有些复杂，但只需要知道它的功能就行，下面我们在main函数里续写如下代码进行调用：</p> 
<pre><code class="prism language-py">    testing_inputs_1<span class="token punctuation">,</span> testing_outputs <span class="token operator">=</span> get_data<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> input_dim<span class="token punctuation">)</span>

    <span class="token comment"># Attention vector corresponds to the second matrix.</span>
    <span class="token comment"># The first one is the Inputs output.</span>
    attention_vector <span class="token operator">=</span> get_activations<span class="token punctuation">(</span>m<span class="token punctuation">,</span> testing_inputs_1<span class="token punctuation">,</span>
                                       print_shape_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                       layer_name<span class="token operator">=</span><span class="token string">'attention_vec'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'attention ='</span><span class="token punctuation">,</span> attention_vector<span class="token punctuation">)</span>

    <span class="token comment"># plot part.</span>


    pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>attention_vector<span class="token punctuation">,</span> columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'attention (%)'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>kind<span class="token operator">=</span><span class="token string">'bar'</span><span class="token punctuation">,</span>
                                                                   title<span class="token operator">=</span><span class="token string">'Attention Mechanism as '</span>
                                                                         <span class="token string">'a function of input'</span>
                                                                         <span class="token string">' dimensions.'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre> 
<p>  看看结果：<br> <img src="https://images2.imgbox.com/e8/97/RXCqdFdV_o.png" alt="在这里插入图片描述"><br>   可以看到，果然是“第1列”权重最高，与理论完全吻合。<br>   完整代码如下(一个文件)：</p> 
<pre><code class="prism language-py"><span class="token keyword">from</span> keras<span class="token punctuation">.</span>models <span class="token keyword">import</span> <span class="token operator">*</span>
<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Input<span class="token punctuation">,</span> Dense<span class="token punctuation">,</span> Multiply
<span class="token keyword">import</span> keras<span class="token punctuation">.</span>backend <span class="token keyword">as</span> K
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd

<span class="token keyword">def</span> <span class="token function">get_activations</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> print_shape_only<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> layer_name<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># Documentation is available online on Github at the address below.</span>
    <span class="token comment"># From: https://github.com/philipperemy/keras-visualize-activations</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'----- activations -----'</span><span class="token punctuation">)</span>
    activations <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    inp <span class="token operator">=</span> model<span class="token punctuation">.</span><span class="token builtin">input</span>
    <span class="token keyword">if</span> layer_name <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        outputs <span class="token operator">=</span> <span class="token punctuation">[</span>layer<span class="token punctuation">.</span>output <span class="token keyword">for</span> layer <span class="token keyword">in</span> model<span class="token punctuation">.</span>layers<span class="token punctuation">]</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        outputs <span class="token operator">=</span> <span class="token punctuation">[</span>layer<span class="token punctuation">.</span>output <span class="token keyword">for</span> layer <span class="token keyword">in</span> model<span class="token punctuation">.</span>layers <span class="token keyword">if</span> layer<span class="token punctuation">.</span>name <span class="token operator">==</span> layer_name<span class="token punctuation">]</span>  <span class="token comment"># all layer outputs</span>
    funcs <span class="token operator">=</span> <span class="token punctuation">[</span>K<span class="token punctuation">.</span>function<span class="token punctuation">(</span><span class="token punctuation">[</span>inp<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span>K<span class="token punctuation">.</span>learning_phase<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>out<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> out <span class="token keyword">in</span> outputs<span class="token punctuation">]</span>  <span class="token comment"># evaluation functions</span>
    layer_outputs <span class="token operator">=</span> <span class="token punctuation">[</span>func<span class="token punctuation">(</span><span class="token punctuation">[</span>inputs<span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">for</span> func <span class="token keyword">in</span> funcs<span class="token punctuation">]</span>
    <span class="token keyword">for</span> layer_activations <span class="token keyword">in</span> layer_outputs<span class="token punctuation">:</span>
        activations<span class="token punctuation">.</span>append<span class="token punctuation">(</span>layer_activations<span class="token punctuation">)</span>
        <span class="token keyword">if</span> print_shape_only<span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span>layer_activations<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span>layer_activations<span class="token punctuation">)</span>
    <span class="token keyword">return</span> activations


<span class="token keyword">def</span> <span class="token function">get_data</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> input_dim<span class="token punctuation">,</span> attention_column<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Data generation. x is purely random except that it's first value equals the target y.
    In practice, the network should learn that the target = x[attention_column].
    Therefore, most of its attention should be focused on the value addressed by attention_column.
    :param n: the number of samples to retrieve.
    :param input_dim: the number of dimensions of each element in the series.
    :param attention_column: the column linked to the target. Everything else is purely random.
    :return: x: model inputs, y: model targets
    """</span>
    x <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>standard_normal<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> input_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
    y <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span>low<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> high<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> attention_column<span class="token punctuation">]</span> <span class="token operator">=</span> y<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>
    <span class="token keyword">return</span> x<span class="token punctuation">,</span> y

<span class="token keyword">def</span> <span class="token function">build_model</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    K<span class="token punctuation">.</span>clear_session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#清除之前的模型，省得压满内存</span>
    inputs <span class="token operator">=</span> Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">#输入层</span>

    <span class="token comment"># ATTENTION PART STARTS HERE 注意力层</span>
    attention_probs <span class="token operator">=</span> Dense<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'attention_vec'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
    attention_mul <span class="token operator">=</span>  Multiply<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">[</span>inputs<span class="token punctuation">,</span> attention_probs<span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token comment"># ATTENTION PART FINISHES HERE</span>

    attention_mul <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">(</span>attention_mul<span class="token punctuation">)</span> <span class="token comment">#原始的全连接</span>
    output <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>attention_mul<span class="token punctuation">)</span> <span class="token comment">#输出层</span>
    model <span class="token operator">=</span> Model<span class="token punctuation">(</span>inputs<span class="token operator">=</span><span class="token punctuation">[</span>inputs<span class="token punctuation">]</span><span class="token punctuation">,</span> outputs<span class="token operator">=</span>output<span class="token punctuation">)</span>
    <span class="token keyword">return</span> model


<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">1337</span><span class="token punctuation">)</span>  <span class="token comment"># for reproducibility</span>
    input_dim <span class="token operator">=</span> <span class="token number">32</span> <span class="token comment">#特征数</span>
    N <span class="token operator">=</span> <span class="token number">10000</span> <span class="token comment">#数据集总记录数</span>
    inputs_1<span class="token punctuation">,</span> outputs <span class="token operator">=</span> get_data<span class="token punctuation">(</span>N<span class="token punctuation">,</span> input_dim<span class="token punctuation">)</span> <span class="token comment">#构造数据集</span>

    m <span class="token operator">=</span> build_model<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#构造模型</span>
    m<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer<span class="token operator">=</span><span class="token string">'adam'</span><span class="token punctuation">,</span> loss<span class="token operator">=</span><span class="token string">'binary_crossentropy'</span><span class="token punctuation">,</span> metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    m<span class="token punctuation">.</span>summary<span class="token punctuation">(</span><span class="token punctuation">)</span>

    m<span class="token punctuation">.</span>fit<span class="token punctuation">(</span><span class="token punctuation">[</span>inputs_1<span class="token punctuation">]</span><span class="token punctuation">,</span> outputs<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> validation_split<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span>

    testing_inputs_1<span class="token punctuation">,</span> testing_outputs <span class="token operator">=</span> get_data<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> input_dim<span class="token punctuation">)</span>

    <span class="token comment"># Attention vector corresponds to the second matrix.</span>
    <span class="token comment"># The first one is the Inputs output.</span>
    attention_vector <span class="token operator">=</span> get_activations<span class="token punctuation">(</span>m<span class="token punctuation">,</span> testing_inputs_1<span class="token punctuation">,</span>
                                       print_shape_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                       layer_name<span class="token operator">=</span><span class="token string">'attention_vec'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'attention ='</span><span class="token punctuation">,</span> attention_vector<span class="token punctuation">)</span>

    <span class="token comment"># plot part.</span>


    pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>attention_vector<span class="token punctuation">,</span> columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'attention (%)'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>kind<span class="token operator">=</span><span class="token string">'bar'</span><span class="token punctuation">,</span>
                                                                   title<span class="token operator">=</span><span class="token string">'Attention Mechanism as '</span>
                                                                         <span class="token string">'a function of input'</span>
                                                                         <span class="token string">' dimensions.'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre> 
<h2><a id="_225"></a>拓展</h2> 
<p>  以上是对别人的代码的学习理解，接下来做一些小改动，看看Attention表现如何。</p> 
<h3><a id="Attention_227"></a>多分类问题时，Attention效果如何？</h3> 
<p>  以上代码改为多分类，需要注意一下几点：</p> 
<ol><li>构造数据集时，randint的high置为类别个数。</li><li>将随机完毕的y由十进制数改为二进制one-hot形式，以待模型输入。</li><li>模型最后一层的结点个数置为类别个数，同时激活函数改为softmax。</li><li>损失函数改为：loss=‘categorical_crossentropy’</li></ol> 
<p>  我们将类别数设置为5，先看看结果：</p> 
<p><img src="https://images2.imgbox.com/b2/e7/keYvGBls_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/26/a2/XeSLBEBB_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ff/33/NRzahAkd_o.png" alt="在这里插入图片描述"><br>   可以看到还是很轻松就学出了规律，再看看可视化的权重：<br> <img src="https://images2.imgbox.com/dc/9c/Ul3cTTpN_o.png" alt="在这里插入图片描述"><br>   对比可以发现，尽管“第1列”的权重仍是最高的，但这个优势已经不明显了，注意力机制的健壮性如何？是否因为是多分类，效果就下降了呢？那么增大类别个数来看看，我们将类别个数置为20，直接看图：<br> <img src="https://images2.imgbox.com/41/16/IdtCU5Xh_o.png" alt="在这里插入图片描述"><br>   可以观察到，“第1列”仍是最高的权重，并且比5分类时还要高，说明注意力机制确实非常强大，可我还是不死心，那么继续调，30个类的时候如何呢？<br> <img src="https://images2.imgbox.com/f9/bb/knvI1olV_o.png" alt="在这里插入图片描述"><br>   综合这两张图，我们惊奇地发现，<strong>准确率下降了，注意力紊乱了</strong>！注意力没有集中在本来人工设置最特别的特征“第1列”上，而集中出现在“第17列”上，这是为什么呢？继续尝试看看，设为50，100，图片分别为：<br> <img src="https://images2.imgbox.com/20/82/P6LeTIuA_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/98/c7/JIgJ1jyz_o.png" alt="在这里插入图片描述"><br>   可以发现，<strong>“注意力紊乱”</strong> 的情况仍存在，即使注意力没有向我们预设的“焦点”集中，<strong>分类准确率降低到了16.9%</strong> 这种“注意力紊乱”的表现是因为什么呢？<br>   我们可以发现，原模型设置的特征数是32个，当分类数接近或者超过特征数时，注意力才发生紊乱，而特征数对应的就是权重数，也就是注意力层的“算力”。因此我们可以有如下猜测：<strong>注意力紊乱是因为问题规模变大，导致原先的学习能力不足，学的不好</strong>。<br>   对于做DL的人来讲，很容易能想到，学得不够好怎么办？加层里的结点数量！加网络的深度！我们将分类数设置为50，特征数设置为128看看效果：<br> <img src="https://images2.imgbox.com/96/fe/leAnYuwd_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/a3/8d/7NgDLayC_o.png" alt="在这里插入图片描述"><br>   果然，<strong>注意力又能“集中”了，但分类效果依旧很差</strong>。那么再试试加深，我们将注意力里的全连接层，多增加一层，分类数设置为50，特征数设置为32，再看看效果：<br> <img src="https://images2.imgbox.com/5e/53/qQ8ilSqL_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/6d/a6/EUXlnWpL_o.png" alt="在这里插入图片描述"><br>   加深注意力网络后不仅注意力回归了，准确率也上升了！(重大发现啊！那是不是可以发文章了呢？)，然鹅已经有大佬在2016年就已经发表了，还给了一个好听的名字：多层注意力网络(Hierarchical Attention Networks)，论文名字是<a href="https://www.researchgate.net/publication/305334401_Hierarchical_Attention_Networks_for_Document_Classification" rel="nofollow">Hierarchical Attention Networks for Document Classification</a>。还有谷歌大佬也提出了一个方法来增加算力，叫多头注意力机制(multi-headed self-attention)，论文名字为：<a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" rel="nofollow">Attention Is All You Need</a>。</p> 
<h3><a id="_257"></a>拓展总结</h3> 
<p>  在拓展中，我们可以发现，注意力机制的效果<strong>在算力充足的情况下</strong>，是能很好捕捉重点特征的，而针对注意力算力的不足，可以使用加结点和加层级的方法，但加结点会增加特征，这与现实中客观任务不符(即分类的数据集特征一般是固定的)，且准确率没有提升，而<strong>加层级</strong>已有人进行应用并证实有效，因此可以作为我们搭建自己网络，提高自己指标的一个小技巧。<br>   此次扩展的完整代码(仅到多分类)：</p> 
<pre><code class="prism language-py"><span class="token keyword">from</span> keras<span class="token punctuation">.</span>models <span class="token keyword">import</span> <span class="token operator">*</span>
<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Input<span class="token punctuation">,</span> Dense<span class="token punctuation">,</span> Multiply
<span class="token keyword">import</span> keras<span class="token punctuation">.</span>backend <span class="token keyword">as</span> K
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">from</span> keras<span class="token punctuation">.</span>utils <span class="token keyword">import</span> to_categorical

<span class="token keyword">def</span> <span class="token function">get_activations</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> print_shape_only<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> layer_name<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># Documentation is available online on Github at the address below.</span>
    <span class="token comment"># From: https://github.com/philipperemy/keras-visualize-activations</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'----- activations -----'</span><span class="token punctuation">)</span>
    activations <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    inp <span class="token operator">=</span> model<span class="token punctuation">.</span><span class="token builtin">input</span>
    <span class="token keyword">if</span> layer_name <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        outputs <span class="token operator">=</span> <span class="token punctuation">[</span>layer<span class="token punctuation">.</span>output <span class="token keyword">for</span> layer <span class="token keyword">in</span> model<span class="token punctuation">.</span>layers<span class="token punctuation">]</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        outputs <span class="token operator">=</span> <span class="token punctuation">[</span>layer<span class="token punctuation">.</span>output <span class="token keyword">for</span> layer <span class="token keyword">in</span> model<span class="token punctuation">.</span>layers <span class="token keyword">if</span> layer<span class="token punctuation">.</span>name <span class="token operator">==</span> layer_name<span class="token punctuation">]</span>  <span class="token comment"># all layer outputs</span>
    funcs <span class="token operator">=</span> <span class="token punctuation">[</span>K<span class="token punctuation">.</span>function<span class="token punctuation">(</span><span class="token punctuation">[</span>inp<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span>K<span class="token punctuation">.</span>learning_phase<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>out<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> out <span class="token keyword">in</span> outputs<span class="token punctuation">]</span>  <span class="token comment"># evaluation functions</span>
    layer_outputs <span class="token operator">=</span> <span class="token punctuation">[</span>func<span class="token punctuation">(</span><span class="token punctuation">[</span>inputs<span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">for</span> func <span class="token keyword">in</span> funcs<span class="token punctuation">]</span>
    <span class="token keyword">for</span> layer_activations <span class="token keyword">in</span> layer_outputs<span class="token punctuation">:</span>
        activations<span class="token punctuation">.</span>append<span class="token punctuation">(</span>layer_activations<span class="token punctuation">)</span>
        <span class="token keyword">if</span> print_shape_only<span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span>layer_activations<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span>layer_activations<span class="token punctuation">)</span>
    <span class="token keyword">return</span> activations


<span class="token keyword">def</span> <span class="token function">get_data</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> input_dim<span class="token punctuation">,</span> class_num<span class="token punctuation">,</span> attention_column<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Data generation. x is purely random except that it's first value equals the target y.
    In practice, the network should learn that the target = x[attention_column].
    Therefore, most of its attention should be focused on the value addressed by attention_column.
    :param n: the number of samples to retrieve.
    :param input_dim: the number of dimensions of each element in the series.
    :param attention_column: the column linked to the target. Everything else is purely random.
    :return: x: model inputs, y: model targets
    """</span>
    
    x <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>standard_normal<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> input_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
    y <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span>low<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> high<span class="token operator">=</span>class_num<span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> attention_column<span class="token punctuation">]</span> <span class="token operator">=</span> y<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>
    y <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>to_categorical<span class="token punctuation">(</span>yy<span class="token punctuation">,</span>class_num<span class="token punctuation">)</span> <span class="token keyword">for</span> yy <span class="token keyword">in</span> y<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>n<span class="token punctuation">,</span>class_num<span class="token punctuation">)</span>
    <span class="token keyword">return</span> x<span class="token punctuation">,</span> y

<span class="token keyword">def</span> <span class="token function">build_model</span><span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span>class_num<span class="token punctuation">)</span><span class="token punctuation">:</span>
    K<span class="token punctuation">.</span>clear_session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#清除之前的模型，省得压满内存</span>
    inputs <span class="token operator">=</span> Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">#输入层</span>

    <span class="token comment"># ATTENTION PART STARTS HERE 注意力层</span>
    attention_probs <span class="token operator">=</span> Dense<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'attention_vec'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
    attention_mul <span class="token operator">=</span>  Multiply<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">[</span>inputs<span class="token punctuation">,</span> attention_probs<span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token comment"># ATTENTION PART FINISHES HERE</span>

    attention_mul <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">(</span>attention_mul<span class="token punctuation">)</span> <span class="token comment">#原始的全连接</span>
    output <span class="token operator">=</span> Dense<span class="token punctuation">(</span>class_num<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>attention_mul<span class="token punctuation">)</span> <span class="token comment">#输出层</span>
    model <span class="token operator">=</span> Model<span class="token punctuation">(</span>inputs<span class="token operator">=</span><span class="token punctuation">[</span>inputs<span class="token punctuation">]</span><span class="token punctuation">,</span> outputs<span class="token operator">=</span>output<span class="token punctuation">)</span>
    <span class="token keyword">return</span> model


<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">1337</span><span class="token punctuation">)</span>  <span class="token comment"># for reproducibility</span>
    input_dim <span class="token operator">=</span> <span class="token number">32</span> <span class="token comment">#特征数</span>
    N <span class="token operator">=</span> <span class="token number">10000</span> <span class="token comment">#数据集总记录数</span>
    class_num <span class="token operator">=</span> <span class="token number">20</span> <span class="token comment">#类别数</span>
    inputs_1<span class="token punctuation">,</span> outputs <span class="token operator">=</span> get_data<span class="token punctuation">(</span>N<span class="token punctuation">,</span> input_dim<span class="token punctuation">,</span> class_num<span class="token punctuation">)</span> <span class="token comment">#构造数据集</span>
    
    m <span class="token operator">=</span> build_model<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span>class_num<span class="token punctuation">)</span> <span class="token comment">#构造模型</span>
    m<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer<span class="token operator">=</span><span class="token string">'adam'</span><span class="token punctuation">,</span> loss<span class="token operator">=</span><span class="token string">'categorical_crossentropy'</span><span class="token punctuation">,</span> metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    m<span class="token punctuation">.</span>summary<span class="token punctuation">(</span><span class="token punctuation">)</span>

    m<span class="token punctuation">.</span>fit<span class="token punctuation">(</span><span class="token punctuation">[</span>inputs_1<span class="token punctuation">]</span><span class="token punctuation">,</span> outputs<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> validation_split<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span>

    testing_inputs_1<span class="token punctuation">,</span> testing_outputs <span class="token operator">=</span> get_data<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> input_dim<span class="token punctuation">,</span> class_num<span class="token punctuation">)</span>

    <span class="token comment"># Attention vector corresponds to the second matrix.</span>
    <span class="token comment"># The first one is the Inputs output.</span>
    attention_vector <span class="token operator">=</span> get_activations<span class="token punctuation">(</span>m<span class="token punctuation">,</span> testing_inputs_1<span class="token punctuation">,</span>
                                       print_shape_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                       layer_name<span class="token operator">=</span><span class="token string">'attention_vec'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'attention ='</span><span class="token punctuation">,</span> attention_vector<span class="token punctuation">)</span>

    <span class="token comment"># plot part.</span>


    pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>attention_vector<span class="token punctuation">,</span> columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'attention (%)'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>kind<span class="token operator">=</span><span class="token string">'bar'</span><span class="token punctuation">,</span>
                                                                   title<span class="token operator">=</span><span class="token string">'Attention Mechanism as '</span>
                                                                         <span class="token string">'a function of input'</span>
                                                                         <span class="token string">' dimensions.'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4d09d67b80111d3a8c546747ff0ef130/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【深度学习】 基于Keras的Attention机制代码实现及剖析——LSTM&#43;Attention</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b28cebb43f522760655a7fac8a405d8d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">shell双重循环之小实验</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>