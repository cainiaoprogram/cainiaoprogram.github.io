<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>BERT模型—6.对抗训练原理与代码实现 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="BERT模型—6.对抗训练原理与代码实现" />
<meta property="og:description" content="文章目录 引言一、对抗训练一般原理1.对抗样本 二、对抗训练的经典算法三、对抗训练代码实现1.FGM2.PGD 引言 对抗训练对于NLP来说，是一种非常好的上分利器，所以，非常有必要加深对对抗训练的认识。
一、对抗训练一般原理 小学语文课上，我们都学习过《矛与盾》这篇课文，
从辩证唯物史观角度来看，矛与盾并没有严格意义上的谁更厉害，谁一直占优。矛盾着的双方又同一又斗争，双方力量此长彼消，不断前进，从而推动事物发展。这也就是对抗训练。
1.对抗样本 两句话，只是部分英文单词发生了改变，但是在我们看来，含义还是几乎不变的，但是就是这种变化，基于BERT的文本分类模型对句子的情感分类居然是完全相反的。
根据研究，我们发现InferSent MultiNLI模型、ESIM MultiNLI模型、BERT MultiNLI模型在SNLI数据集上面准确率为84%以上，但是在对抗文本上，不管是传统的深度网络还是BERT模型，准确率都只有个位数。尽管BERT模型经过微调可以在不同的下游任务得到好的结果，但仍然是非常容易受到攻击的（模型不鲁棒）。
那么，如何使得模型更加鲁棒呢？比如：
我们可以对数据进行处理，将对攻击的防御放到数据处理过程中我们也可以对模型输出的向量表征做一些转换，将防御放在模型的输出端我们也可以将防守放在模型本身上来 二、对抗训练的经典算法 lan Goodfellow提出对抗训练方法，它的思想是在训练时，在原始的输入样本中加上扰动（对抗样本），我们用对抗样本来进行模型的训练，使得模型更加鲁棒
扰动的计算方式定义为：求得模型损失，对于 x x x求梯度，对所求的梯度经过符号函数处理，在乘以一个系数 ϵ \epsilon ϵ
这种对抗训练的方法叫做Fast Gradient Sign Method (FGSM)。
对抗训练其实可以当做正则化，减少模型的过拟合，提升模型的泛化性能。FGSM对抗训练方法应用与CV，cv的输入都是图像，图像输入模型时为RGB三个通道上的像素值，它的输入本身就是连续的，但是NLP的输入是离散的单词序列。那么，我们应该如何在NLP模型上定义对抗训练？
虽然NLP的输入都是离散的单词序列，但是会经过embedding转变成低维空间上的向量表征，我们可以将embedding后的向量表征当成上述对抗训练模型中的 x x x。lan Goodfellow在2017年提出了在连续的embedding上做扰动。
扰动的计算方式为：
x x x表示文本序列的embedding vectors 这种对抗训练的方法叫做Fast Gradient Method (FGM)。FGM通过一步，就移动到对抗样本上，如果梯度太大，可能会导致扰动过大,对模型造成误导。
FGM对抗训练方法一步得到对抗样本，容易导致扰动过大。Projected Gradient Descent
(PGD)方法限定了扰动的范围，对抗样本并不是一步就得到了，而是通过沿着不同点的梯度走了多步之后再去得到。
其中，
小步走:如果沿着梯度走的比较远，则通过投影的方式，投影到球面s上；多步走:生成一个对抗样本是走多步得到的； 这种对抗训练的方法叫做Projected Gradient Descent(PGD)。
三、对抗训练代码实现 1.FGM class FGM(): &#34;&#34;&#34; 定义对抗训练方法FGM,对模型embedding参数进行扰动 &#34;&#34;&#34; def __init__(self, model, epsilon=0.25,): # BERT模型 self.model = model # 求干扰时的系数值 self." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/1c85f14efff718b26717c45c8bec6c00/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-07-29T16:25:58+08:00" />
<meta property="article:modified_time" content="2021-07-29T16:25:58+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">BERT模型—6.对抗训练原理与代码实现</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><ul><li><ul><li><a href="#_1" rel="nofollow">引言</a></li><li><a href="#_3" rel="nofollow">一、对抗训练一般原理</a></li><li><ul><li><a href="#1_7" rel="nofollow">1.对抗样本</a></li></ul> 
    </li><li><a href="#_18" rel="nofollow">二、对抗训练的经典算法</a></li><li><a href="#_43" rel="nofollow">三、对抗训练代码实现</a></li><li><ul><li><a href="#1FGM_44" rel="nofollow">1.FGM</a></li><li><a href="#2PGD_118" rel="nofollow">2.PGD</a></li></ul> 
   </li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h4><a id="_1"></a>引言</h4> 
<p>  对抗训练对于NLP来说，是一种非常好的上分利器，所以，非常有必要加深对对抗训练的认识。</p> 
<h4><a id="_3"></a>一、对抗训练一般原理</h4> 
<p>  小学语文课上，我们都学习过《矛与盾》这篇课文，<br> <img src="https://images2.imgbox.com/6c/ef/Vdbntz7J_o.png" alt="在这里插入图片描述"><br> 从辩证唯物史观角度来看，矛与盾并没有严格意义上的谁更厉害，谁一直占优。矛盾着的双方又同一又斗争，双方力量此长彼消，不断前进，从而推动事物发展。这也就是对抗训练。</p> 
<h5><a id="1_7"></a>1.对抗样本</h5> 
<p>  两句话，只是部分英文单词发生了改变，但是在我们看来，含义还是几乎不变的，但是就是这种变化，基于BERT的文本分类模型对句子的情感分类居然是完全相反的。<br> <img src="https://images2.imgbox.com/8d/9e/3sWU2uY1_o.png" alt="在这里插入图片描述"><br>   根据研究，我们发现<code>InferSent MultiNLI</code>模型、<code>ESIM MultiNLI</code>模型、<code>BERT MultiNLI</code>模型在<code>SNLI</code>数据集上面准确率为84%以上，但是在对抗文本上，不管是传统的深度网络还是BERT模型，准确率都只有个位数。尽管BERT模型经过微调可以在不同的下游任务得到好的结果，但仍然是非常容易受到攻击的（模型不鲁棒）。<br> <img src="https://images2.imgbox.com/ea/73/LGq17BRW_o.png" alt="在这里插入图片描述"><br>   那么，如何使得模型更加鲁棒呢？比如：</p> 
<ul><li>我们可以对数据进行处理，将对攻击的防御放到数据处理过程中</li><li>我们也可以对模型输出的向量表征做一些转换，将防御放在模型的输出端</li><li>我们也可以将防守放在模型本身上来</li></ul> 
<h4><a id="_18"></a>二、对抗训练的经典算法</h4> 
<p>  lan Goodfellow提出对抗训练方法，它的思想是在训练时，在原始的输入样本中加上扰动（对抗样本），我们用对抗样本来进行模型的训练，使得模型更加鲁棒<br> <img src="https://images2.imgbox.com/1e/92/EAvw3RIp_o.png" alt="在这里插入图片描述"><br> 扰动的计算方式定义为：求得模型损失，对于<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
      
        x 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault">x</span></span></span></span></span>求梯度，对所求的梯度经过符号函数处理，在乘以一个系数<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ϵ 
        
       
      
        \epsilon 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault">ϵ</span></span></span></span></span><br> <img src="https://images2.imgbox.com/8e/d4/68eZxXl9_o.png" alt="在这里插入图片描述"><br> 这种对抗训练的方法叫做Fast Gradient Sign Method (FGSM)。<br>   对抗训练其实可以当做正则化，减少模型的过拟合，提升模型的泛化性能。FGSM对抗训练方法应用与CV，cv的输入都是图像，图像输入模型时为RGB三个通道上的像素值，它的输入本身就是连续的，但是NLP的输入是离散的单词序列。那么，我们应该<strong>如何在NLP模型上定义对抗训练？</strong><br>   虽然NLP的输入都是离散的单词序列，但是会经过embedding转变成低维空间上的向量表征，我们可以将embedding后的向量表征当成上述对抗训练模型中的<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
      
        x 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault">x</span></span></span></span></span>。lan Goodfellow在2017年提出了在连续的embedding上做扰动。<br> <img src="https://images2.imgbox.com/18/36/N01hGPDh_o.png" alt="在这里插入图片描述"><br> 扰动的计算方式为：<br> <img src="https://images2.imgbox.com/c6/d3/LyIZ2533_o.png" alt="在这里插入图片描述"></p> 
<ul><li><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          x 
         
        
       
         x 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault">x</span></span></span></span></span>表示文本序列的embedding vectors</li></ul> 
<p>这种对抗训练的方法叫做Fast Gradient Method (FGM)。FGM通过一步，就移动到对抗样本上，如果梯度太大，可能会导致扰动过大,对模型造成误导。<br>   FGM对抗训练方法一步得到对抗样本，容易导致扰动过大。Projected Gradient Descent<br> (PGD)方法限定了扰动的范围，对抗样本并不是一步就得到了，而是通过沿着不同点的梯度走了多步之后再去得到。<br> <img src="https://images2.imgbox.com/1b/28/H5Ck34Ug_o.png" alt="在这里插入图片描述"><br> 其中，<br> <img src="https://images2.imgbox.com/59/55/GMV7g2BL_o.png" alt="在这里插入图片描述"></p> 
<ul><li>小步走:如果沿着梯度走的比较远，则通过投影的方式，投影到球面s上；</li><li>多步走:生成一个对抗样本是走多步得到的；</li></ul> 
<p>这种对抗训练的方法叫做Projected Gradient Descent(PGD)。</p> 
<h4><a id="_43"></a>三、对抗训练代码实现</h4> 
<h5><a id="1FGM_44"></a>1.FGM</h5> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">FGM</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    定义对抗训练方法FGM,对模型embedding参数进行扰动
    """</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model<span class="token punctuation">,</span> epsilon<span class="token operator">=</span><span class="token number">0.25</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># BERT模型</span>
        self<span class="token punctuation">.</span>model <span class="token operator">=</span> model
        <span class="token comment"># 求干扰时的系数值</span>
        self<span class="token punctuation">.</span>epsilon <span class="token operator">=</span> epsilon

        self<span class="token punctuation">.</span>backup <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>

    <span class="token keyword">def</span> <span class="token function">attack</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> emb_name<span class="token operator">=</span><span class="token string">'word_embeddings'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        得到对抗样本
        :param emb_name:模型中embedding的参数名
        :return:
        """</span>
        <span class="token comment"># 循环遍历模型所有参数</span>
        <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># 如果当前参数在计算中保留了对应的梯度信息，并且包含了模型中embedding的参数名</span>
            <span class="token keyword">if</span> param<span class="token punctuation">.</span>requires_grad <span class="token keyword">and</span> emb_name <span class="token keyword">in</span> name<span class="token punctuation">:</span>
                <span class="token comment"># 把真实参数保存起来</span>
                self<span class="token punctuation">.</span>backup<span class="token punctuation">[</span>name<span class="token punctuation">]</span> <span class="token operator">=</span> param<span class="token punctuation">.</span>data<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>
                <span class="token comment"># 对参数的梯度求范数</span>
                norm <span class="token operator">=</span> torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>param<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>
                <span class="token comment"># 如果范数不等于0并且norm中没有缺失值</span>
                <span class="token keyword">if</span> norm <span class="token operator">!=</span> <span class="token number">0</span> <span class="token keyword">and</span> <span class="token keyword">not</span> torch<span class="token punctuation">.</span>isnan<span class="token punctuation">(</span>norm<span class="token punctuation">)</span><span class="token punctuation">:</span>
                    <span class="token comment"># 计算扰动，param.grad / norm=单位向量，起到了sgn(param.grad)一样的作用</span>
                    r_at <span class="token operator">=</span> self<span class="token punctuation">.</span>epsilon <span class="token operator">*</span> param<span class="token punctuation">.</span>grad <span class="token operator">/</span> norm
                    <span class="token comment"># 在原参数的基础上添加扰动</span>
                    param<span class="token punctuation">.</span>data<span class="token punctuation">.</span>add_<span class="token punctuation">(</span>r_at<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">restore</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> emb_name<span class="token operator">=</span><span class="token string">'word_embeddings'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        将模型原本的参数复原
        :param emb_name:模型中embedding的参数名
        """</span>
        <span class="token comment"># 循环遍历模型所有参数</span>
        <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># 如果当前参数在计算中保留了对应的梯度信息，并且包含了模型中embedding的参数名</span>
            <span class="token keyword">if</span> param<span class="token punctuation">.</span>requires_grad <span class="token keyword">and</span> emb_name <span class="token keyword">in</span> name<span class="token punctuation">:</span>
                <span class="token comment"># 断言</span>
                <span class="token keyword">assert</span> name <span class="token keyword">in</span> self<span class="token punctuation">.</span>backup
                <span class="token comment"># 取出模型真实参数</span>
                param<span class="token punctuation">.</span>data <span class="token operator">=</span> self<span class="token punctuation">.</span>backup<span class="token punctuation">[</span>name<span class="token punctuation">]</span>
        <span class="token comment"># 清空self.backup</span>
        self<span class="token punctuation">.</span>backup <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>
</code></pre> 
<pre><code class="prism language-python"><span class="token comment"># 实例初始化</span>
fgm <span class="token operator">=</span> FGM<span class="token punctuation">(</span>model<span class="token punctuation">)</span>
<span class="token keyword">for</span> batch_input<span class="token punctuation">,</span> batch_label <span class="token keyword">in</span> data<span class="token punctuation">:</span>
    <span class="token comment"># 正常训练</span>
    loss <span class="token operator">=</span> model<span class="token punctuation">(</span>batch_input<span class="token punctuation">,</span> batch_label<span class="token punctuation">)</span>
    <span class="token comment"># 反向传播，得到正常的grad</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span> 
    <span class="token comment"># 对抗训练，在embedding上添加对抗扰动</span>
    fgm<span class="token punctuation">.</span>attack<span class="token punctuation">(</span><span class="token punctuation">)</span> 
    <span class="token comment"># embedding参数被修改，此时，输入序列得到的embedding表征不一样                     </span>
    loss_adv <span class="token operator">=</span> model<span class="token punctuation">(</span>batch_input<span class="token punctuation">,</span> batch_label<span class="token punctuation">)</span>
    <span class="token comment"># 反向传播，并在正常的grad基础上，累加对抗训练的梯度</span>
    loss_adv<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span> 
    <span class="token comment"># 恢复embedding参数</span>
    fgm<span class="token punctuation">.</span>restore<span class="token punctuation">(</span><span class="token punctuation">)</span> 
    <span class="token comment"># 梯度下降，更新参数</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 将梯度清零</span>
    model<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h5><a id="2PGD_118"></a>2.PGD</h5> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">PGD</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    定义对抗训练方法PGD
    """</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model<span class="token punctuation">,</span> epsilon<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># BERT模型</span>
        self<span class="token punctuation">.</span>model <span class="token operator">=</span> model
        <span class="token comment"># 两个计算参数</span>
        self<span class="token punctuation">.</span>epsilon <span class="token operator">=</span> epsilon
        self<span class="token punctuation">.</span>alpha <span class="token operator">=</span> alpha
        <span class="token comment"># 用于存储embedding参数</span>
        self<span class="token punctuation">.</span>emb_backup <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>
        <span class="token comment"># 用于存储梯度，与多步走相关</span>
        self<span class="token punctuation">.</span>grad_backup <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>

    <span class="token keyword">def</span> <span class="token function">attack</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> emb_name<span class="token operator">=</span><span class="token string">'word_embeddings'</span><span class="token punctuation">,</span> is_first_attack<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        对抗
        :param emb_name: 模型中embedding的参数名
        :param is_first_attack: 是否是第一次攻击
        """</span>
        <span class="token comment"># 循环遍历模型的每一个参数</span>
        <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># 如果当前参数在计算中保留了对应的梯度信息，并且包含了模型中embedding的参数名</span>
            <span class="token keyword">if</span> param<span class="token punctuation">.</span>requires_grad <span class="token keyword">and</span> emb_name <span class="token keyword">in</span> name<span class="token punctuation">:</span>
                <span class="token comment"># 如果是第一次攻击</span>
                <span class="token keyword">if</span> is_first_attack<span class="token punctuation">:</span>
                    <span class="token comment"># 存储embedding参数</span>
                    self<span class="token punctuation">.</span>emb_backup<span class="token punctuation">[</span>name<span class="token punctuation">]</span> <span class="token operator">=</span> param<span class="token punctuation">.</span>data<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>
                <span class="token comment"># 求梯度的范数</span>
                norm <span class="token operator">=</span> torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>param<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>
                <span class="token comment"># 如果范数不等于0</span>
                <span class="token keyword">if</span> norm <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>
                    <span class="token comment"># 计算扰动,param.grad / norm=单位向量相当于sgn符号函数</span>
                    r_at <span class="token operator">=</span> self<span class="token punctuation">.</span>alpha <span class="token operator">*</span> param<span class="token punctuation">.</span>grad <span class="token operator">/</span> norm
                    <span class="token comment"># 在原参数的基础上添加扰动</span>
                    param<span class="token punctuation">.</span>data<span class="token punctuation">.</span>add_<span class="token punctuation">(</span>r_at<span class="token punctuation">)</span>
                    <span class="token comment"># 控制扰动后的模型参数值</span>
                    <span class="token comment"># 投影到以原参数为原点，epsilon大小为半径的球上面</span>
                    param<span class="token punctuation">.</span>data <span class="token operator">=</span> self<span class="token punctuation">.</span>project<span class="token punctuation">(</span>name<span class="token punctuation">,</span> param<span class="token punctuation">.</span>data<span class="token punctuation">,</span> self<span class="token punctuation">.</span>epsilon<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">restore</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> emb_name<span class="token operator">=</span><span class="token string">'word_embeddings'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        将模型原本参数复原
        :param emb_name: 模型中embedding的参数名
        """</span>
        <span class="token comment"># 循环遍历每一个参数</span>
        <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># 如果当前参数在计算中保留了对应的梯度信息，并且包含了模型中embedding的参数名</span>
            <span class="token keyword">if</span> param<span class="token punctuation">.</span>requires_grad <span class="token keyword">and</span> emb_name <span class="token keyword">in</span> name<span class="token punctuation">:</span>
                <span class="token keyword">assert</span> name <span class="token keyword">in</span> self<span class="token punctuation">.</span>emb_backup
                <span class="token comment"># 取出模型真实参数</span>
                param<span class="token punctuation">.</span>data <span class="token operator">=</span> self<span class="token punctuation">.</span>emb_backup<span class="token punctuation">[</span>name<span class="token punctuation">]</span>
        <span class="token comment"># 清空emb_backup</span>
        self<span class="token punctuation">.</span>emb_backup <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>

    <span class="token keyword">def</span> <span class="token function">project</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> param_name<span class="token punctuation">,</span> param_data<span class="token punctuation">,</span> epsilon<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        控制扰动后的模型参数值
        :param param_name:
        :param param_data:
        :param epsilon:
        """</span>
        <span class="token comment"># 计算加了扰动后的参数值与原始参数的差值</span>
        r <span class="token operator">=</span> param_data <span class="token operator">-</span> self<span class="token punctuation">.</span>emb_backup<span class="token punctuation">[</span>param_name<span class="token punctuation">]</span>
        <span class="token comment"># 如果差值的范数大于epsilon</span>
        <span class="token keyword">if</span> torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>r<span class="token punctuation">)</span> <span class="token operator">&gt;</span> epsilon<span class="token punctuation">:</span>
            <span class="token comment"># 对差值进行截断</span>
            r <span class="token operator">=</span> epsilon <span class="token operator">*</span> r <span class="token operator">/</span> torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>r<span class="token punctuation">)</span>
        <span class="token comment"># 返回新的加了扰动后的参数值</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>emb_backup<span class="token punctuation">[</span>param_name<span class="token punctuation">]</span> <span class="token operator">+</span> r

    <span class="token keyword">def</span> <span class="token function">backup_grad</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        对梯度进行备份
        """</span>
        <span class="token comment"># 循环遍历每一个参数</span>
        <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># 如果当前参数在计算中保留了对应的梯度信息</span>
            <span class="token keyword">if</span> param<span class="token punctuation">.</span>requires_grad<span class="token punctuation">:</span>
                <span class="token comment"># 如果参数没有梯度</span>
                <span class="token keyword">if</span> param<span class="token punctuation">.</span>grad <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"{} param has no grad !!!"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>name<span class="token punctuation">)</span><span class="token punctuation">)</span>
                    <span class="token keyword">continue</span>
                <span class="token comment"># 将参数梯度进行备份</span>
                self<span class="token punctuation">.</span>grad_backup<span class="token punctuation">[</span>name<span class="token punctuation">]</span> <span class="token operator">=</span> param<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">restore_grad</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        将梯度进行复原
        """</span>
        <span class="token comment"># 循环遍历每一个参数</span>
        <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># 如果当前参数在计算中保留了对应的梯度信息</span>
            <span class="token keyword">if</span> param<span class="token punctuation">.</span>requires_grad<span class="token punctuation">:</span>
                <span class="token comment"># 如果没有备份</span>
                <span class="token keyword">if</span> name <span class="token keyword">not</span> <span class="token keyword">in</span> self<span class="token punctuation">.</span>grad_backup<span class="token punctuation">:</span>
                    <span class="token keyword">continue</span>
                <span class="token comment"># 如果备份了，就将原始模型参数梯度取出</span>
                param<span class="token punctuation">.</span>grad <span class="token operator">=</span> self<span class="token punctuation">.</span>grad_backup<span class="token punctuation">[</span>name<span class="token punctuation">]</span>
</code></pre> 
<pre><code class="prism language-python"><span class="token comment"># 实例初始化</span>
pgd <span class="token operator">=</span> PGD<span class="token punctuation">(</span>model<span class="token punctuation">)</span>
steps_for_at <span class="token operator">=</span> <span class="token number">3</span>
<span class="token keyword">for</span> batch_input<span class="token punctuation">,</span> batch_label <span class="token keyword">in</span> data<span class="token punctuation">:</span>
    <span class="token comment"># 正常训练</span>
    loss <span class="token operator">=</span> model<span class="token punctuation">(</span>batch_input<span class="token punctuation">,</span> batch_label<span class="token punctuation">)</span>
    <span class="token comment"># 反向传播，得到正常的grad</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span> 
    <span class="token comment"># 保存正常的梯度</span>
    pgd<span class="token punctuation">.</span>backup_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># PGD要走多步，迭代走多步</span>
    <span class="token keyword">for</span> t <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>steps_for_at<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 在embedding上添加对抗扰动, first attack时备份param.data</span>
        pgd<span class="token punctuation">.</span>attack<span class="token punctuation">(</span>is_first_attack<span class="token operator">=</span><span class="token punctuation">(</span>t <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 中间过程，梯度清零</span>
        <span class="token keyword">if</span> t <span class="token operator">!=</span> steps_for_at <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">:</span>
            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 最后一步，恢复正常的grad</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            pgd<span class="token punctuation">.</span>restore_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># embedding参数被修改，此时，输入序列得到的embedding表征不一样                     </span>
    	loss_at <span class="token operator">=</span> model<span class="token punctuation">(</span>batch_input<span class="token punctuation">,</span> batch_label<span class="token punctuation">)</span>
        <span class="token comment"># 对抗样本上的损失</span>
        loss_at <span class="token operator">=</span> outputs_at<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        <span class="token comment"># 反向传播，并在正常的grad基础上，累加对抗训练的梯度</span>
        loss_at<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 恢复embedding参数</span>
   	pgd<span class="token punctuation">.</span>restore<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 梯度下降，更新参数</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 将梯度清零</span>
    model<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<hr> 
<p>如果对您有帮助，麻烦点赞关注，这真的对我很重要！！！如果需要互关，请评论或者私信！<br> <img src="https://images2.imgbox.com/ed/97/ZpD5numP_o.jpg" alt="在这里插入图片描述"></p> 
<hr>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ca2603eec2649cd89e29180158b25eb1/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">希望之春为何一直正在连接服务器,春之希望</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/88918ae1d596b57835a45b91a7a1fbdd/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">win10计算机未连接到网络适配器,win10未检测到正确的适配器怎么办_win10检测不到正确的适配器解决方法...</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>