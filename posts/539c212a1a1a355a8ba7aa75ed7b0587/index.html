<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Deep Residual Learning for Image Recognition - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Deep Residual Learning for Image Recognition" />
<meta property="og:description" content="Deep Residual Learning for Image Recognition中提出的152层网络结构在ILSRC 2015获得第一名，集成后的网络在ImageNet分类错误率3.57%，作者微软研究院的何凯明等。
上图中两条曲线分别为20层和56层的“plain”网络在训练集和测试集上的错误率，这里的“plain”是指一般的无环的“直流”卷积网络。按照以往的经验理论，网络越深应该有更好的分类效果，加大网络的深度是一种很有效的提高准确率的方法。但从上图中的曲线很明显是与这一结论相矛盾的，20层的网络无论是在训练集还是测试集上的错误率都比56层的要低。为了说明出现这一现象并不是因为梯度弥散或是膨胀导致的，所以在每一个卷积之后，ReLU之前 会使用bn层。对于深层的网络，有这样一个现象，算法开始是收敛的，但随着网络加深，准确略逐渐的下降，但并没有出现过拟合。所以说明出现这样的现象是因为优化出现问题。
这篇文章提出一种deep residual learning framework的结构，解决对于很深的网络的优化问题。这种结构主要受启发于LSTM和HighWay
这种结构有两个优点（1）比较容易优化（2）网络层数加深可以有更高的准确率。
这种short connection 实际上是卷积之后的output与input的逐点加和，这句就需要二者具有相同的维度，但是由于采样的原因使得很多时候并不能满足，通常有三种方法可以解决上述问题：
（A）使用zero-padding shortcuts增加维度，并且所有的shortcut是参数自由的（B）维度不同时使用projection shortcuts，相同时使用恒等变换x（C）完全使用projection shortcuts
实验验证三种方法的错误率为：
从结果可以看出B方法相较于A方法有一定的提升，但是C相对于B提升极少，但是计算量却增加很大，所以并不使用C方法。
对于上图中的两种结构identity shortcut有相似的时间复杂度。但是对于右侧的结构来说如果使用projection shortcuts计算量和模型大小相当于identity shortcut的二倍。所以identity shortcut对于这种bottleneck的设计来说是更好地。
这个实验说明resnet确实可以提高深度网络的准确率，使得网络优化的更好。
【注】这里需要注意一点的是，在这个网络结构中pool只出现了一次，使用的2步长的方法进行下采样。二者的最大的不同在于2步长的卷积虽然会使得output size 减小，但是由于卷积核大小为3，实际上每个像素点的信息都是利用到的，相较于pool来说带来的信息损失会比较小。并且在Google的rethink文章中也指出，在网络的前期尽量不要制造bottleneck，但频繁使用pool必然会增加这种信息的减少而出现bottleneck。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/539c212a1a1a355a8ba7aa75ed7b0587/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2016-01-08T18:57:25+08:00" />
<meta property="article:modified_time" content="2016-01-08T18:57:25+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Deep Residual Learning for Image Recognition</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><span style="font-size:18px">        Deep Residual Learning for Image Recognition中提出的152层网络结构在ILSRC 2015获得第一名，集成后的网络在ImageNet分类错误率3.57%，作者微软研究院的何凯明等。</span></p> 
<p style="text-align:center"><span style="font-size:18px"><img src="" alt=""><br> </span></p> 
<p><span style="font-size:18px">        上图中两条曲线分别为20层和56层的“plain”网络在训练集和测试集上的错误率，这里的“plain”是指一般的无环的“直流”卷积网络。按照以往的经验理论，网络越深应该有更好的分类效果，加大网络的深度是一种很有效的提高准确率的方法。但从上图中的曲线很明显是与这一结论相矛盾的，20层的网络无论是在训练集还是测试集上的错误率都比56层的要低。为了说明出现这一现象并不是因为梯度弥散或是膨胀导致的，所以在每一个卷积之后，ReLU之前 会使用bn层。对于深层的网络，有这样一个现象，算法开始是收敛的，但随着网络加深，准确略逐渐的下降，但并没有出现过拟合。所以说明出现这样的现象是因为优化出现问题。</span></p> 
<p><span style="font-size:18px"><span style="white-space:pre"></span>这篇文章提出一种deep residual learning framework的结构，解决对于很深的网络的优化问题。这种结构主要受启发于LSTM和HighWay</span></p> 
<p style="text-align:center"><span style="font-size:18px"><img src="" alt=""><br> </span></p> 
<p><span style="font-size:18px">这种结构有两个优点（1）比较容易优化（2）网络层数加深可以有更高的准确率。</span></p> 
<p style="text-align:center"><img src="" alt=""><br> </p> 
<p><span style="font-size:18px">这种short connection 实际上是卷积之后的output与input的逐点加和，这句就需要二者具有相同的维度，但是由于采样的原因使得很多时候并不能满足，通常有三种方法可以解决上述问题：</span></p> 
<p><span style="font-size:18px">（A）使用zero-padding shortcuts增加维度，并且所有的shortcut是参数自由的（B）维度不同时使用projection shortcuts，相同时使用恒等变换x（C）完全使用<span style="font-size:18px">projection shortcuts</span></span></p> 
<p><span style="font-size:18px"><span style="font-size:18px">实验验证三种方法的错误率为：</span></span></p> 
<p style="text-align:center"><span style="font-size:18px"><span style="font-size:18px"><img src="" alt=""><br> </span></span></p> 
<p><span style="font-size:18px">从结果可以看出B方法相较于A方法有一定的提升，但是C相对于B提升极少，但是计算量却增加很大，所以并不使用C方法。</span></p> 
<p style="text-align:center"><span style="font-size:18px"><img src="" alt=""><br> </span></p> 
<p><span style="font-size:18px">对于上图中的两种结构<span style="font-size:18px">identity shortcut有</span>相似的时间复杂度。但是对于右侧的结构来说如果使用</span><span style="font-size:18px">projection shortcuts计算量和模型大小相当于identity shortcut的二倍。所以<span style="font-size:18px">identity shortcut对于这种bottleneck的设计来说是更好地。</span></span></p> 
<p style="text-align:center"><span style="font-size:18px"><span style="font-size:18px"><img src="" alt=""><br> </span></span></p> 
<p><span style="font-size:18px"><span style="font-size:18px"></span></span></p> 
<p style="text-align:center"><span style="font-size:18px"><span style="font-size:18px"><img src="" alt=""><br> </span></span></p> 
<p style="text-align:center"><span style="font-size:18px"><span style="font-size:18px"><img src="" alt=""><br> </span></span></p> 
<p><span style="font-size:18px">这个实验说明resnet确实可以提高深度网络的准确率，使得网络优化的更好。</span></p> 
<p><span style="font-size:18px">【注】这里需要注意一点的是，在这个网络结构中pool只出现了一次，使用的2步长的方法进行下采样。二者的最大的不同在于2步长的卷积虽然会使得output size 减小，但是由于卷积核大小为3，实际上每个像素点的信息都是利用到的，相较于pool来说带来的信息损失会比较小。并且在Google的rethink文章中也指出，在网络的前期尽量不要制造bottleneck，但频繁使用pool必然会增加这种信息的减少而出现bottleneck。</span></p> 
<p><span style="font-size:18px"><br> </span></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4bb7e7475ee154c74ecb021c71e2d26a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">判断两个矩形是否重叠</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/63324b8417a8a52ee1f827ac3fbdb533/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Training Very Deep Networks</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>