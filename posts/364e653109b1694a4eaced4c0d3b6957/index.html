<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>FFmpeg视频播放(音视频解码) - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="FFmpeg视频播放(音视频解码)" />
<meta property="og:description" content="上一篇中FFmpeg解封装中在TinaFFmpeg中的prepare方法里把解码器上下文AVCodecContext交给VideoChannel,AudioChannel后，解码工作就交给它们来处理了，这节我们来看它们是如何处理的。
解码入口
prepare完成后会调用Java中TinaPlayer的onPrepare的方法，然后回调 PlayActivity的start方法，然后进入native层的start方法:
LOGE(&#34;native prepare流程准备完毕&#34;); // 准备完了 通知java 你随时可以开始播放 callHelper-&gt;onPrepare(THREAD_CHILD); //prepare完成后会调用Java中TinaPlayer的onPrepare的方法，然后回调 PlayActivity的start方法，然后进入native层的start方法 //TinaPlayer public void onPrepare(){ if (null != listener){ listener.onPrepare(); } } //PlayActivity tinaPlayer.setOnPrepareListener(new TinaPlayer.OnPrepareListener() { @Override public void onPrepare() { runOnUiThread(new Runnable() { @Override public void run() { Toast.makeText(&#34;开始播放&#34;).show(); } }); //调用native_start tinaPlayer.start(); } }); //TinaPlayer public void start(){ native_start(); } native层：调用ffmpeg中的start方法，然后分别调用videoChannel、audioChannel的play()方法
extern &#34;C&#34; JNIEXPORT void JNICALL Java_tina_com_player_TinaPlayer_native_1start(JNIEnv *env, jobject instance) { ffmpeg-&gt;start(); } //TinaFFmpeg void TinaFFmpeg::start() { //重新开线程 isPlaying = 1; if (audioChannel) { //设置为工作状态 audioChannel-&gt;play(); } if (videoChannel) { //设置为工作状态 videoChannel-&gt;setAudioChannel(audioChannel); videoChannel-&gt;play(); } pthread_create(&amp;pid_play, 0, play, this); } 兵马未动，粮草先行。把解码需要的数据源packet放入到SafeQueue的同步队列中去：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/364e653109b1694a4eaced4c0d3b6957/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-20T19:00:00+08:00" />
<meta property="article:modified_time" content="2023-10-20T19:00:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">FFmpeg视频播放(音视频解码)</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>上一篇中<a href="https://link.zhihu.com/?target=https%3A//juejin.cn/post/6844903703795728397" rel="nofollow" title="FFmpeg解封装">FFmpeg解封装</a>中在TinaFFmpeg中的prepare方法里把解码器上下文AVCodecContext交给VideoChannel,AudioChannel后，解码工作就交给它们来处理了，这节我们来看它们是如何处理的。</p> 
<p>解码入口</p> 
<p>prepare完成后会调用Java中TinaPlayer的onPrepare的方法，然后回调 PlayActivity的start方法，然后进入native层的start方法:<br>  </p> 
<pre><code> LOGE("native prepare流程准备完毕");
    // 准备完了 通知java 你随时可以开始播放
 callHelper-&gt;onPrepare(THREAD_CHILD);
//prepare完成后会调用Java中TinaPlayer的onPrepare的方法，然后回调 PlayActivity的start方法，然后进入native层的start方法</code></pre> 
<p></p> 
<pre><code>//TinaPlayer
public void onPrepare(){
        if (null != listener){
            listener.onPrepare();
        }
 }

//PlayActivity
tinaPlayer.setOnPrepareListener(new TinaPlayer.OnPrepareListener() {
  @Override
  public void onPrepare() {
    runOnUiThread(new Runnable() {
      @Override
      public void run() {
        Toast.makeText("开始播放").show();
      }
    });
    //调用native_start
    tinaPlayer.start();
  }
 });

//TinaPlayer
public void start(){
  native_start();
}</code></pre> 
<p>native层：调用ffmpeg中的start方法，然后分别调用videoChannel、audioChannel的play()方法</p> 
<pre><code>extern "C"
JNIEXPORT void JNICALL
Java_tina_com_player_TinaPlayer_native_1start(JNIEnv *env, jobject instance) {
    ffmpeg-&gt;start();
}

//TinaFFmpeg
void TinaFFmpeg::start() {
    //重新开线程
    isPlaying = 1;
    if (audioChannel) {
        //设置为工作状态
        audioChannel-&gt;play();
    }
    if (videoChannel) {
        //设置为工作状态
        videoChannel-&gt;setAudioChannel(audioChannel);
        videoChannel-&gt;play();
    }
    pthread_create(&amp;pid_play, 0, play, this);
}</code></pre> 
<p>兵马未动，粮草先行。把解码需要的数据源packet放入到SafeQueue的同步队列中去：</p> 
<pre><code>void *play(void *args) {
    TinaFFmpeg *fFmpeg = static_cast&lt;TinaFFmpeg *&gt;(args);
    fFmpeg-&gt;_start();
    return 0;
}
void TinaFFmpeg::_start() {
    int ret;
    //1.读取媒体数据包（音频、视频数据）
    while (isPlaying) {
        //读取文件的时候没有网络请求，一下子读完了，可能导致oom
        //特别是读本地文件的时候 一下子就读完了
        if (audioChannel &amp;&amp; audioChannel-&gt;packets.size() &gt; 100) {
            //10ms
            av_usleep(1000 * 10);
            continue;
        }
        if (videoChannel &amp;&amp; videoChannel-&gt;packets.size() &gt; 100) {
            av_usleep(1000 * 10);
            continue;
        }
        AVPacket *avPacket = av_packet_alloc();
        ret = av_read_frame(formatContext, avPacket);
        //等于0成功，其它失败
        if (ret == 0) {
            if (audioChannel &amp;&amp; avPacket-&gt;stream_index == audioChannel-&gt;id) {
                //todo 音频
                //在audioChannel中执行 解码工作
                audioChannel-&gt;packets.push(avPacket);
            } else if (videoChannel &amp;&amp; avPacket-&gt;stream_index == videoChannel-&gt;id) {
                //在videoChannel中执行 解码工作
                videoChannel-&gt;packets.push(avPacket);
            }
        } else if (ret == AVERROR_EOF) {
            //读取完成，但是可能还没有播放完成
            if (audioChannel-&gt;packets.empty() &amp;&amp; audioChannel-&gt;frames.empty()
                &amp;&amp; videoChannel-&gt;packets.empty() &amp;&amp; videoChannel-&gt;frames.empty()){
                av_packet_free(&amp;avPacket);//这里不释放会有内存泄漏，崩溃
                break;
            }
        } else {
            av_packet_free(&amp;avPacket);
            break;
        }
    }
    isPlaying = 0;
    audioChannel-&gt;stop();
    videoChannel-&gt;stop();
}</code></pre> 
<p>其实push队列跟audioChannel、videoChannel的play()的过程是同步的，是基于SafeQueue对象packets的生产者与消费者的问题处理，我们先来看视频解码。</p> 
<p>视频解码</p> 
<p>解码分为decode，render（渲染），创建两个线程，分别在各自的线程中处理。<br>  </p> 
<pre><code>void VideoChannel::play() {
    isPlaying = 1;
    packets.setWork(1);
    frames.setWork(1);
    //1.解码
    pthread_create(&amp;pid_decode, 0, decode_task, this);
    //2. 播放
    pthread_create(&amp;pid_render, 0, render_task, this);
}


//解码
void VideoChannel::decode() {
    AVPacket *packet = 0;
    while (isPlaying) {
        int ret = packets.pop(packet);
        if (!isPlaying) {
            break;
        }
        if (!ret) {
            continue;
        }
        //把包丢给解码器
        ret = avcodec_send_packet(avCodecContext, packet);
        releaseAvPacket(&amp;packet);
        while (ret != 0) {
            break;
        }
        AVFrame *frame = av_frame_alloc();

        avcodec_receive_frame(avCodecContext, frame);
        if (ret == AVERROR(EAGAIN)) {
            continue;
        } else if (ret != 0) {
            break;
        }
        //再开一个线程 播放。
        frames.push(frame);
    }
    releaseAvPacket(&amp;packet);
}</code></pre> 
<p>decode的过程其实很简单，就是把packet交给 AVCodec处理，然后拿到frame，放入到frames队列中去，而frames就是我们render的数据源，下面来看render：</p> 
<pre><code>//渲染
void VideoChannel::render() {
    //目标： RGBA, 没有缩放
    swsContext = sws_getContext(avCodecContext-&gt;width, avCodecContext-&gt;height,
                                avCodecContext-&gt;pix_fmt,
                                avCodecContext-&gt;width, avCodecContext-&gt;height, AV_PIX_FMT_RGBA,
                                SWS_BILINEAR, 0, 0, 0);
    //每个画面 刷新的间隔 单位：秒
    double frame_delays = 1.0 / fps;
    AVFrame *frame = 0;

    uint8_t *dst_data[4];
    int dst_linesize[4];
    av_image_alloc(dst_data, dst_linesize, avCodecContext-&gt;width, avCodecContext-&gt;height,
                   AV_PIX_FMT_RGBA, 1);
    while (isPlaying) {
        int ret = frames.pop(frame);
        if (!isPlaying) {
            break;
        }
        //src_lines: 每一行存放的 字节长度
        sws_scale(swsContext, reinterpret_cast&lt;const uint8_t *const *&gt;(frame-&gt;data),
                  frame-&gt;linesize, 0,
                  avCodecContext-&gt;height,
                  dst_data, dst_linesize);

        //获取当前 一个画面播放的时间
   ------------- 处理音视频同步(后面再讲)---------------
        //把解出来的数据给到 window的Surface，回调出去进行播放
callback(dst_data[0], dst_linesize[0], avCodecContext-&gt;width, avCodecContext&gt;height);
        releaseAvFrame(&amp;frame);
    }
    av_freep(&amp;dst_data[0]);
    releaseAvFrame(&amp;frame);
    isPlaying = 0;
    sws_freeContext(swsContext);
    swsContext = 0;
}</code></pre> 
<p><strong>相关学习资料推荐，点击下方链接免费报名，先码住不迷路~】</strong></p> 
<p><a href="https://link.zhihu.com/?target=https%3A//ke.qq.com/course/3202131%3FflowToken%3D1042316" rel="nofollow" title="音视频免费学习地址：">音视频免费学习地址：</a><a href="https://link.zhihu.com/?target=https%3A//xxetb.xet.tech/s/2cGd0" rel="nofollow" title="https://xxetb.xet.tech/s/2cGd0">https://xxetb.xet.tech/s/2cGd0</a></p> 
<p></p> 
<p>看render的过程其实没有真正的处理渲染，只是将frame的数据data通过回调接口传出去了，TinaFFmpeg调用了这个回调，而这个回调的具体实现在JNI的入口函数里native-lib.cpp中：</p> 
<pre><code>//VideoChannel
void VideoChannel::setRenderFrameCallback(RenderFrameCallback callback) {
    this-&gt;callback = callback;
}

//TinaFFmpeg
void TinaFFmpeg::setRenderFrameCallback(RenderFrameCallback callback) {
    this-&gt;callback = callback;
}

extern "C"
JNIEXPORT void JNICALL
Java_tina_com_player_TinaPlayer_native_1prepare(JNIEnv *env, jobject instance,
                                                jstring dataSource_) {
	....
    ffmpeg-&gt;setRenderFrameCallback(render);
    .....
}</code></pre> 
<p>那么这个render的回调函数究竟是如何处理的呢，如何把datas给到SurfaceView中显示呢？</p> 
<pre><code>void render(uint8_t *data, int linesize, int w, int h) {
    pthread_mutex_lock(&amp;mutex);
    if (!window) {
        pthread_mutex_unlock(&amp;mutex);
        return;
    }
    //设置窗口属性
    ANativeWindow_setBuffersGeometry(window, w, h,
                                     WINDOW_FORMAT_RGBA_8888);
    ANativeWindow_Buffer window_buffer;
    if (ANativeWindow_lock(window, &amp;window_buffer, 0)) {
        ANativeWindow_release(window);
        window = 0;
        pthread_mutex_unlock(&amp;mutex);
        return;
    }
    //填充rgb数据给dst_data
    uint8_t *dst_data = static_cast&lt;uint8_t *&gt;(window_buffer.bits);
    //stride : 一行多少个数据 （RGBA） * 4
    int dst_linesize = window_buffer.stride * 4;
    //一行一行拷贝
    for (int i = 0; i &lt; window_buffer.height; ++i) {
        memcpy(dst_data + i * dst_linesize, data + i * linesize, dst_linesize);
    }
    ANativeWindow_unlockAndPost(window);
    pthread_mutex_unlock(&amp;mutex);
}</code></pre> 
<p>通过memcpy逐行拷贝dst_data中的数据，我们在看一遍render中的传参：</p> 
<pre><code>//把解出来的数据给到 window的Surface，回调出去进行播放
callback(dst_data[0], dst_linesize[0], avCodecContext-&gt;width, avCodecContext-&gt;height);</code></pre> 
<p>这就把 videoChannel中解压的数据传给了上面的 NativeWindow中的buffer，NativeWindow作为Native层与Java层视频数据展现的承接容器，那具体的数据传输的介质是在TinaPlayer中通过setSurface传过来的SurfaceHolder中的Surface<br><br>  </p> 
<pre><code>/**
 * 画布创建OK
*/
@Override
public void surfaceCreated(SurfaceHolder holder) {
  native_setSurface(holder.getSurface());
}
/**
* 画布发生变化（横竖屏）
*/
@Override
 public void surfaceChanged(SurfaceHolder holder, int format, int width, int height) {
  native_setSurface(holder.getSurface());
}</code></pre> 
<p>至此，整个VideoChannal的解码、渲染工作完成，可以在PlayActivity中看到视频了，接下来处理声音：</p> 
<h4>音频解码</h4> 
<p>同样的两个线程，一个解码，一个播放</p> 
<pre><code>void *audio_decode(void *args) {
    AudioChannel *audioChannel = static_cast&lt;AudioChannel *&gt;(args);
    audioChannel-&gt;decode();
    return 0;
}

void *audio_play(void *args) {
    AudioChannel *audioChannel = static_cast&lt;AudioChannel *&gt;(args);
    audioChannel-&gt;_play();
    return 0;
}

void AudioChannel::play() {
    packets.setWork(1);
    frames.setWork(1);
......
 // 省略代码后续再讲 初始化音视频同步的Context
    isPlaying = 1;
    //1. 解码
    pthread_create(&amp;pid_audio_decode, 0, audio_decode, this);
    //2. 播放
    pthread_create(&amp;pid_audio_player, 0, audio_play, this);
}</code></pre> 
<p>解码过程一样：</p> 
<pre><code>//音频解码， 跟视频代码一样
void AudioChannel::decode() {
    AVPacket *packet = 0;
    while (isPlaying) {
        int ret = packets.pop(packet);
        if (!isPlaying) {
            break;
        }
        if (!ret) {
            continue;
        }
        //把包丢给解码器
        ret = avcodec_send_packet(avCodecContext, packet);
        releaseAvPacket(&amp;packet);
        while (ret != 0) {
            break;
        }
        AVFrame *frame = av_frame_alloc();

        avcodec_receive_frame(avCodecContext, frame);
        if (ret == AVERROR(EAGAIN)) {
            continue;
        } else if (ret != 0) {
            break;
        }
        //再开一个线程 播放。
        frames.push(frame);
    }
    releaseAvPacket(&amp;packet);
}</code></pre> 
<p>decode的工作任务是把frame放入到frames队列中就完成了，下面就是播放声音了。</p> 
<p>播放音频<br> ​</p> 
<p>关于OpenSL ES的使用可以进入ndk-sample查看native-audio工程:<a href="https://link.zhihu.com/?target=https%3A//link.juejin.cn/%3Ftarget%3Dhttps%253A%252F%252Fgithub.com%252Fgooglesamples%252Fandroid-ndk%252Fblob%252Fmaster%252Fnative-audio%252Fapp%252Fsrc%252Fmain%252Fcpp%252Fnative-audio-jni.c" rel="nofollow" title="github.com/googlesampl…">github.com/googlesampl…</a><br> OpenSL ES的开发流程主要有如下7个步骤：</p> 
<ol><li>创建引擎与接口</li><li>设置混音器</li><li>创建播放器</li><li>设置播放回调函数</li><li>设置播放状态</li><li>启动回调函数</li><li>释放</li></ol> 
<p>初始化相关的类<br><br> 作者：cxy107750<br> 链接：<a href="https://link.zhihu.com/?target=https%3A//juejin.cn/post/6844903703804117006" rel="nofollow" title="https://juejin.cn/post/6844903703804117006">https://juejin.cn/post/6844903703804117006</a><br> 来源：稀土掘金<br> 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 Android播放声音（声音格式PCM），有两种方式，方式一是通过Java SDK中的AudioTrack（AudioRecorder录音），另一种是通过NDK的OpenSL ES来播放，这里的数据在Native层，所以避免JNI跨层的数据调用以及反射等相关处理，这里我们选择OpenSL ES来播放音频，OpenSL ES是Android NDK中的包，专门为嵌入式设备处理音频的库，引入库需要修改CmakeLists.txt文件（FFmpeg只是编解码库，不支持处理输入输出设备）<br>  </p> 
<pre><code>class AudioChannel : public BaseChannel {
public:
    AudioChannel(int id, AVCodecContext *avCodecContext, AVRational time_base);

    ~AudioChannel();

private:
     /**
     * OpenSL ES
     */
    //引擎
    SLObjectItf engineObject = 0;
    //引擎接口
    SLEngineItf engineInterface = 0;
    //混音器
    SLObjectItf outputMixObject = 0;
    //播放器
    SLObjectItf bqPlayerObject = 0;
    //播放器接口
    SLPlayItf bqPlayerInterface = 0;
    //队列结构
    SLAndroidSimpleBufferQueueItf bqPlayerBufferQueueInterface = 0;
};</code></pre> 
<p>创建引擎与接口</p> 
<pre><code>SLresult result;
    // 创建引擎 SLObjectItf engineObject
    result = slCreateEngine(&amp;engineObject, 0, NULL, 0, NULL, NULL);
    if (SL_RESULT_SUCCESS != result) {
        return;
    }
    // 初始化引擎(init)
    result = (*engineObject)-&gt;Realize(engineObject, SL_BOOLEAN_FALSE);
    if (SL_RESULT_SUCCESS != result) {
        return;
    }

 // 获取引擎接口SLEngineItf engineInterface
result = (*engineObject)-&gt;GetInterface(engineObject, SL_IID_ENGINE, &amp;engineInterface);
    if (SL_RESULT_SUCCESS != result) {
        return;
    }</code></pre> 
<p>设置混音器</p> 
<pre><code>result = (*engineInterface)-&gt;CreateOutputMix(engineInterface,&amp;outputMixObject, 0, 0, 0);
    if (SL_RESULT_SUCCESS != result) {
        return;
    }
    // 初始化混音器outputMixObject
    result = (*outputMixObject)-&gt;Realize(outputMixObject, SL_BOOLEAN_FALSE);
    if (SL_RESULT_SUCCESS != result) {
        return;
    }

 //3.2 配置音轨（输出）
    //设置混音器
    SLDataLocator_OutputMix outputMix = {SL_DATALOCATOR_OUTPUTMIX, outputMixObject};
    SLDataSink audioSnk = {&amp;outputMix, NULL};
    //需要的接口， 操作队列的接口，可以添加混音接口
    const SLInterfaceID ids[1] = {SL_IID_BUFFERQUEUE};
    const SLboolean req[1] = {SL_BOOLEAN_TRUE};</code></pre> 
<p>创建播放器</p> 
<pre><code>//创建buffer缓冲类型的队列 2个队列
SLDataLocator_AndroidSimpleBufferQueue android_queue ={SL_DATALOCATOR_ANDROIDSIMPLEBUFFERQUEUE,2};
//pcm数据格式
    //pcm +2（双声道）+ 44100（采样率）+ 16（采样位）+ LEFT|RIGHT（双声道）+ 小端数据
SLDataFormat_PCM pcm = {SL_DATAFORMAT_PCM, 2, SL_SAMPLINGRATE_44_1, SL_PCMSAMPLEFORMAT_FIXED_16,SL_PCMSAMPLEFORMAT_FIXED_16,
SL_SPEAKER_FRONT_LEFT | SL_SPEAKER_FRONT_RIGHT,
SL_BYTEORDER_LITTLEENDIAN};

//数据源 将上述配置信息放到这个数据源中
 SLDataSource slDataSource = {&amp;android_queue, &amp;pcm};
//创建播放器
(*engineInterface)-&gt;CreateAudioPlayer(engineInterface, &amp;bqPlayerObject, &amp;slDataSource,&amp;audioSnk, 1,ids, req);
//初始化播放器
(*bqPlayerObject)-&gt;Realize(bqPlayerObject, SL_BOOLEAN_FALSE);</code></pre> 
<p>设置播放回调</p> 
<pre><code>(*bqPlayerObject)-&gt;GetInterface(bqPlayerObject, SL_IID_BUFFERQUEUE,
                                    &amp;bqPlayerBufferQueueInterface);
 //设置回调
(*bqPlayerBufferQueueInterface)-&gt;RegisterCallback(bqPlayerBufferQueueInterface, bqPlayerCallback, this);</code></pre> 
<p>设置播放状态</p> 
<pre><code>(*bqPlayerInterface)-&gt;SetPlayState(bqPlayerInterface, SL_PLAYSTATE_PLAYING);</code></pre> 
<p>启动激活函数</p> 
<pre><code> //6. 手动激活启动回调
bqPlayerCallback(bqPlayerBufferQueueInterface, this);</code></pre> 
<p>回调接口中处理转码PCM的过程</p> 
<pre><code>void bqPlayerCallback(SLAndroidSimpleBufferQueueItf bq, void *context) {
    AudioChannel *audioChannel = static_cast&lt;AudioChannel *&gt;(context);
    //获取pcm数据
    int dataSize = audioChannel-&gt;getPcm();
    if(dataSize &gt; 0){
        (*bq)-&gt; Enqueue(bq, audioChannel-&gt;data, dataSize);//这里取 16位数据
    }
}


//获取Pcm 数据
int AudioChannel::getPcm() {
    int data_size = 0;
    AVFrame *frame;
    int ret = frames.pop(frame);

    if (!isPlaying) {
        if (ret) {
            releaseAvFrame(&amp;frame);
        }
        return data_size;
    }
    //48000HZ  8位 =》 44100 16位
    //重采样
    //假设我们输入了10个数据， swrContext转码器，这一次处理了8个数据
    int64_t delays = swr_get_delay(swrContext, frame-&gt;sample_rate);

    //将nb_samples个数据由 sample_rate 采样率转成 44100后，返回多少个数据
    // 10 个 48000 = nb个 44100
    //AV_ROUND_UP : 向上取整 1.1 = 2
    int64_t max_samples = av_rescale_rnd(delays + frame-&gt;nb_samples, out_sample_rate, frame-&gt;sample_rate, AV_ROUND_UP);
    //上下文 + 输入缓冲区 + 输出缓冲区能接受的最大数据量 + 输入数据 + 输入数据个数
    // 返回每一个声道的数据
    int samples = swr_convert(swrContext, &amp;data, max_samples,
                              (const uint8_t **)frame-&gt;data, frame-&gt;nb_samples);
    //获得 samples个2字节（16位） * 2声道
    data_size = samples *  out_samplesize * out_channels;

    //获取一个frame的一个相对播放时间
    //获得播放这段数据的秒速（时间机）
    clock =  frame-&gt;pts * av_q2d(time_base);
    releaseAvFrame(&amp;frame);
    return data_size;
}</code></pre> 
<p>播放流程处理完成后，记得释放OpenSL中的对象。</p> 
<pre><code>void AudioChannel::stop() {
    isPlaying = 0;
    packets.setWork(0);
    frames.setWork(0);
    //释放播放器
    if(bqPlayerObject){
        (*bqPlayerObject)-&gt;Destroy(bqPlayerObject);
        bqPlayerObject = 0;
        bqPlayerInterface = 0;
        bqPlayerBufferQueueInterface = 0;
    }

    //释放混音器
    if(outputMixObject){
        (*outputMixObject)-&gt;Destroy(outputMixObject);
        outputMixObject = 0;
    }

    //释放引擎
    if(engineObject){
        (*engineObject)-&gt;Destroy(engineObject);
        engineObject = 0;
        engineInterface = 0;
    }
}</code></pre> 
<p>以上就是音频、视频解码及播放的整个流程，但是会存在的问题是音视频他们在各自的线程里播放，互不干扰，但是从视频源来的视频、音频在某一时间点是相互对应的，而我们的播放端是从队列里拿数据进行播放，队列有同步，等待，阻塞等因素，会造成音视频两者不同步，大概率上甚至说基本是不同步的。所以需要处理二者的同步关系，如何来处理二者的同步呢？下一篇详解.<br><br> 需要源码的，可前往<a href="https://link.zhihu.com/?target=https%3A//link.juejin.cn/%3Ftarget%3Dhttps%253A%252F%252Fgithub.com%252Fyinxiucheng%252FTinaPlayer" rel="nofollow" title="GitHub">GitHub</a>获取， 顺便点个star呗。<br><br> 作者：cxy107750<br> 原文 <a href="https://link.zhihu.com/?target=https%3A//juejin.cn/post/6844903703804117006%3FsearchId%3D20231018170723D5737DF4999CF6B571D9" rel="nofollow" title="FFmpeg视频播放(音视频解码) - 掘金">FFmpeg视频播放(音视频解码) - 掘金</a></p> 
<p><span style="color:#fe2c24;"><strong>★文末名片可以免费领取音视频开发学习资料，内容包括（FFmpeg ，webRTC ，rtmp ，hls ，rtsp ，ffplay ，srs）以及音视频学习路线图等等。</strong></span></p> 
<p><span style="color:#fe2c24;"><strong>见下方!↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓</strong></span></p> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f3ff11a8e8c687113341d63f5941ac4b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">解决提交到App Store时的ITMS-90478和ITMS-90062错误</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1b618dc5384fb3a6d1a5095543845723/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">大河弯弯：CSS 文档流与三大元素显示模式</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>