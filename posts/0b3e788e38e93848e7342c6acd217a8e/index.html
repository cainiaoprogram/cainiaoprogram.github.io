<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>自动驾驶预测-决策-规划-控制学习（4）：预测分析文献阅读 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="自动驾驶预测-决策-规划-控制学习（4）：预测分析文献阅读" />
<meta property="og:description" content="提示：文章写完后，目录可以自动生成，如何生成可参考右边的帮助文档
文章目录 前言一、摘要分析1.Transformer模型是什么？什么是自注意力机制？ 2.数据集是什么？3.预测车辆行驶轨迹和车辆换道意图4. LSTM 网络 二、神经网络概述1.人工智能是什么？2.机器学习是什么？3.深度学习是什么？4.神经网络是什么？一个最简单的神经网络模型神经网络可以理解为一个分类器，通过训练后，能够记住我们人为需要的一类数据特征，那么对以后输入的任何数据都会按照这个记忆来分类，这种记忆力与神经网络每一层的神经元个数以及深度有关。数学思维下，深度学习的神经网络好比一个可以自主调节系数的公式。 5.RNN和CNN是什么？各自有什么优缺点？RNN：时序序列数据的处理CNN:网格化图像数据处理RNN和CNN各自有以下优缺点： 二、绪论分析三、简要解释基于transformer的车辆未来时刻轨迹预测四、简要分析基于transformer的车辆换道意图预测 前言 题目：《基于 Transformer 的车辆轨迹预测及变道意图识别方法研究》-吉林大学都业铭
一、摘要分析 本文改进了 Transformer 模型， 在 Argoverse 数据集、NGSIM 数据集上分别设 计了新的模型结构， 可以分别完成预测车辆行驶轨迹和车辆换道意图识别任务。在 车辆行驶轨迹预测任务中，本文在 Argoverse 数据集上使用最小平均位移误差 （minADE）、最小终点位移误差（minFDE）、错过率（MR）等指标与多种轨迹预 测方法进行了比较与评估， 结果显示本文基于 Transformer 改进的模型在轨迹预测 任务上具有更高的准确性。除此之外，本文还在 Transformer 模型基础上进行改进， 使其能够完成车辆换道意图识别任务。同时本文与不同隐藏层数的 LSTM 网络进 行了对比实验， 结果表明在准确率与召回率指标上， 本文基于 Transformer 的车辆换道意图识别模型更加优秀。
1.Transformer模型是什么？ Transformer模型是一种基于自注意力机制的神经网络模型，最初由Google在2017年提出，用于自然语言处理任务，如机器翻译、文本生成等。与传统的循环神经网络（RNN）和卷积神经网络（CNN）不同，Transformer模型不需要按照时间顺序或空间位置顺序处理输入数据，而是通过自注意力机制来计算输入数据中不同位置之间的依赖关系，从而实现对输入数据的全局理解和处理。
Transformer模型由编码器和解码器两部分组成，其中编码器用于将输入序列转换为一系列特征表示，解码器则用于根据编码器的输出和上下文信息生成目标序列。Transformer模型的主要优点是能够处理长序列数据，同时具有较好的并行计算能力，因此在自然语言处理、图像处理等领域得到了广泛应用。
什么是自注意力机制？ 当谈到Transformer时，我们通常会提到它在自然语言处理中的应用，比如机器翻译。让我们以机器翻译为例来解释Transformer的原理。
假设我们要将一句英文句子翻译成法文。在传统的神经网络中，我们会使用循环神经网络（RNN）或卷积神经网络（CNN）来处理输入序列。但是，这些模型在处理长句子时可能会遇到一些问题，比如长期依赖关系的捕捉和并行计算的困难。
Transformer通过引入自注意力机制来解决这些问题。它的核心思想是将输入序列中的每个单词都看作是一个向量，并通过计算这些向量之间的相似度来确定它们之间的关系。
首先，Transformer将输入的英文句子中的每个单词转换为向量表示，这些向量称为&#34;词嵌入&#34;。然后，它引入了一个叫做&#34;注意力机制&#34;的概念。注意力机制允许模型在生成输出的过程中，根据输入序列中的不同单词的重要性来分配不同的权重。
在编码器部分，Transformer使用多层的自注意力机制来对输入序列进行编码。每一层都会计算每个单词与其他单词之间的相似度，并根据相似度分配权重。这样，模型可以同时考虑到整个输入序列的信息，而不仅仅是局部上下文。
在解码器部分，Transformer使用类似的自注意力机制来生成目标序列。它会根据已经生成的部分目标序列来预测下一个单词，并不断迭代生成整个目标序列。
通过使用自注意力机制，Transformer能够更好地捕捉长期依赖关系，并且可以并行计算，提高了模型的训练和推理效率。这使得Transformer在机器翻译等自然语言处理任务中取得了很大的成功。
总结起来，Transformer通过引入自注意力机制，将输入序列中的每个单词都看作是一个向量，并根据它们之间的相似度来确定它们之间的关系。这种机制使得Transformer能够更好地处理长句子和捕捉长期依赖关系，提高了模型的性能和效率。
想象一下你在学习一门新语言，比如学习法语。你需要将一句英文句子翻译成法语。传统的方法是逐个单词地翻译，但这可能会导致一些问题，比如长句子的处理困难和长期依赖关系的捕捉问题。
Transformer是一种新的方法，它使用了一种叫做&#34;自注意力&#34;的机制来解决这些问题。自注意力机制可以帮助模型在处理输入时，更好地关注不同单词之间的关。
在这个例子中，我们可以将输入的英文句子中的每个单词看作是一个人，而Transformer就像是一个聪明的观察者。这个观察者会同时观察所有的人，并注意到每个人与其他人之间的相似度。
然后，观察者会根据相似度来决定每个人的重要性。比如，如果一个人在句子中出现多次并且对整个句子的理解很重要，那么他的重要性就会更高。
在编码器部分，观察者会通过多次观察和计算相似度来对整个句子进行编码。每次观察都会给每个人分配不同的权重，以便更好地捕捉句子中的关系。
在解码器部分，观察者会使用类似的方法来生成目标语言的句子。他会根据已经生成的部分句子来预测下一个单词，并不断重复这个过程，直到生成整个目标句子。
通过使用自注意力机制，Transformer可以更好地处理长句子和捕捉长期依赖关系。这种方法使得模型在机器翻译等任务中表现出色。
2.数据集是什么？ 数据集是指在进行机器学习或深度学习任务时所使用的数据的集合。它是从现实世界中收集、整理和标注的一组样本数据，用于训练、验证和测试机器学习模型的性能。
数据集通常包含输入数据和对应的标签或目标值。输入数据可以是图像、文本、音频等不同形式的数据，而标签或目标值则是对输入数据的描述、分类或预测结果。在训练过程中，机器学习模型通过对数据集进行学习和优化，从而能够对新的未见过的数据进行准确的预测或分类。
数据集的质量和多样性对于机器学习模型的性能至关重要。一个好的数据集应该具有代表性、多样性和充分的样本数量，以确保模型能够泛化到不同的情况和场景中。同时，数据集的标注质量也需要高，以保证模型能够学习到正确的模式和规律。
在论文中，作者使用了Argoverse数据集和NGSIM数据集作为训练和评估模型的基准数据集，用于车辆行驶轨迹预测和车辆换道意图识别任务。这些数据集是从真实的交通场景中收集而来，并经过标注和处理，以供研究人员进行相关任务的研究和性能评估。
3.预测车辆行驶轨迹和车辆换道意图 预测车辆行驶轨迹和车辆换道意图是自动驾驶系统中的两个重要任务。在自动驾驶系统中，车辆需要能够准确地预测其他车辆的行驶轨迹和换道意图，以便做出相应的决策和规划行驶路线。
预测车辆行驶轨迹是指根据车辆当前的位置、速度和方向等信息，预测其未来一段时间内的行驶轨迹这个任务的难点在于需要考虑到其他车辆、行人、路标等因素对车辆行驶轨迹的影响，同时需要考虑到车辆自身的动力学特性和环境因素，如路况、天气等。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/0b3e788e38e93848e7342c6acd217a8e/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-06T18:03:56+08:00" />
<meta property="article:modified_time" content="2024-01-06T18:03:56+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">自动驾驶预测-决策-规划-控制学习（4）：预测分析文献阅读</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <blockquote> 
 <p>提示：文章写完后，目录可以自动生成，如何生成可参考右边的帮助文档</p> 
</blockquote> 
<p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#_7" rel="nofollow">前言</a></li><li><a href="#_10" rel="nofollow">一、摘要分析</a></li><li><ul><li><a href="#1Transformer_13" rel="nofollow">1.Transformer模型是什么？</a></li><li><ul><li><a href="#_18" rel="nofollow">什么是自注意力机制？</a></li></ul> 
   </li><li><a href="#2_42" rel="nofollow">2.数据集是什么？</a></li><li><a href="#3_50" rel="nofollow">3.预测车辆行驶轨迹和车辆换道意图</a></li><li><a href="#4_LSTM__56" rel="nofollow">4. LSTM 网络</a></li></ul> 
  </li><li><a href="#_64" rel="nofollow">二、神经网络概述</a></li><li><ul><li><a href="#1_66" rel="nofollow">1.人工智能是什么？</a></li><li><a href="#2_69" rel="nofollow">2.机器学习是什么？</a></li><li><a href="#3_75" rel="nofollow">3.深度学习是什么？</a></li><li><a href="#4_85" rel="nofollow">4.神经网络是什么？</a></li><li><ul><li><a href="#_88" rel="nofollow">一个最简单的神经网络模型</a></li><li><a href="#_102" rel="nofollow">神经网络可以理解为一个分类器，通过训练后，能够记住我们人为需要的一类数据特征，那么对以后输入的任何数据都会按照这个记忆来分类，这种记忆力与神经网络每一层的神经元个数以及深度有关。数学思维下，深度学习的神经网络好比一个可以自主调节系数的公式。</a></li></ul> 
   </li><li><a href="#5RNNCNN_108" rel="nofollow">5.RNN和CNN是什么？各自有什么优缺点？</a></li><li><ul><li><a href="#RNN_109" rel="nofollow">RNN：时序序列数据的处理</a></li><li><a href="#CNN_111" rel="nofollow">CNN:网格化图像数据处理</a></li><li><a href="#RNNCNN_114" rel="nofollow">RNN和CNN各自有以下优缺点：</a></li></ul> 
  </li></ul> 
  </li><li><a href="#_137" rel="nofollow">二、绪论分析</a></li><li><a href="#transformer_147" rel="nofollow">三、简要解释基于transformer的车辆未来时刻轨迹预测</a></li><li><a href="#transformer_159" rel="nofollow">四、简要分析基于transformer的车辆换道意图预测</a></li></ul> 
</div> 
<p></p> 
<hr> 
<h2><a id="_7"></a>前言</h2> 
<p>题目：《基于 Transformer 的车辆轨迹预测及变道意图识别方法研究》-吉林大学都业铭</p> 
<h2><a id="_10"></a>一、摘要分析</h2> 
<blockquote> 
 <p>本文改进了 Transformer 模型， 在 Argoverse 数据集、NGSIM 数据集上分别设 计了新的模型结构， 可以分别完成预测车辆行驶轨迹和车辆换道意图识别任务。在 车辆行驶轨迹预测任务中，本文在 Argoverse 数据集上使用最小平均位移误差 （minADE）、最小终点位移误差（minFDE）、错过率（MR）等指标与多种轨迹预 测方法进行了比较与评估， 结果显示本文基于 Transformer 改进的模型在轨迹预测 任务上具有更高的准确性。除此之外，本文还在 Transformer 模型基础上进行改进， 使其能够完成车辆换道意图识别任务。同时本文与不同隐藏层数的 LSTM 网络进 行了对比实验， 结果表明在准确率与召回率指标上， 本文基于 Transformer 的车辆换道意图识别模型更加优秀。</p> 
</blockquote> 
<h3><a id="1Transformer_13"></a>1.Transformer模型是什么？</h3> 
<p>Transformer模型是一种<strong>基于自注意力机制的神经网络模型</strong>，最初由Google在2017年提出，用于自然语言处理任务，如机器翻译、文本生成等。<strong>与传统的循环神经网络（RNN）和卷积神经网络（CNN）不同</strong>，Transformer模型不需要按照时间顺序或空间位置顺序处理输入数据，而是<strong>通过自注意力机制来计算输入数据中不同位置之间的依赖关系，从而实现对输入数据的全局理解和处理</strong>。</p> 
<p>Transformer模型由编码器和解码器两部分组成，其中编码器用于将输入序列转换为一系列特征表示，解码器则用于根据编码器的输出和上下文信息生成目标序列。Transformer模型的主要优点是能够处理长序列数据，同时具有较好的并行计算能力，因此在自然语言处理、图像处理等领域得到了广泛应用。</p> 
<h4><a id="_18"></a>什么是自注意力机制？</h4> 
<p>当谈到Transformer时，我们通常会提到它在自然语言处理中的应用，比如机器翻译。让我们以机器翻译为例来解释Transformer的原理。</p> 
<p>假设我们要将一句英文句子翻译成法文。在传统的神经网络中，我们会使用循环神经网络（RNN）或卷积神经网络（CNN）来处理输入序列。但是，这些模型在处理长句子时可能会遇到一些问题，比如长期依赖关系的捕捉和并行计算的困难。</p> 
<p>Transformer通过引入自注意力机制来解决这些问题。它的核心思想是将输入序列中的每个单词都看作是一个向量，并通过计算这些向量之间的相似度来确定它们之间的关系。</p> 
<p>首先，Transformer将输入的英文句子中的每个单词转换为向量表示，这些向量称为"词嵌入"。然后，它引入了一个叫做"注意力机制"的概念。注意力机制允许模型在生成输出的过程中，根据输入序列中的不同单词的重要性来分配不同的权重。</p> 
<p>在编码器部分，Transformer使用多层的自注意力机制来对输入序列进行编码。每一层都会计算每个单词与其他单词之间的相似度，并根据相似度分配权重。这样，模型可以同时考虑到整个输入序列的信息，而不仅仅是局部上下文。</p> 
<p>在解码器部分，Transformer使用类似的自注意力机制来生成目标序列。它会根据已经生成的部分目标序列来预测下一个单词，并不断迭代生成整个目标序列。</p> 
<p>通过使用自注意力机制，Transformer能够更好地捕捉长期依赖关系，并且可以并行计算，提高了模型的训练和推理效率。这使得Transformer在机器翻译等自然语言处理任务中取得了很大的成功。</p> 
<p>总结起来，Transformer通过引入自注意力机制，将输入序列中的每个单词都看作是一个向量，并根据它们之间的相似度来确定它们之间的关系。这种机制使得Transformer能够更好地处理长句子和捕捉长期依赖关系，提高了模型的性能和效率。</p> 
<blockquote> 
 <p>想象一下你在学习一门新语言，比如学习法语。你需要将一句英文句子翻译成法语。传统的方法是逐个单词地翻译，但这可能会导致一些问题，比如长句子的处理困难和长期依赖关系的捕捉问题。<br> Transformer是一种新的方法，它使用了一种叫做"自注意力"的机制来解决这些问题。自注意力机制可以帮助模型在处理输入时，更好地关注不同单词之间的关。<br> 在这个例子中，我们可以将输入的英文句子中的每个单词看作是一个人，而Transformer就像是一个聪明的观察者。这个观察者会同时观察所有的人，并注意到每个人与其他人之间的相似度。<br> 然后，观察者会根据相似度来决定每个人的重要性。比如，如果一个人在句子中出现多次并且对整个句子的理解很重要，那么他的重要性就会更高。<br> 在编码器部分，观察者会通过多次观察和计算相似度来对整个句子进行编码。每次观察都会给每个人分配不同的权重，以便更好地捕捉句子中的关系。<br> 在解码器部分，观察者会使用类似的方法来生成目标语言的句子。他会根据已经生成的部分句子来预测下一个单词，并不断重复这个过程，直到生成整个目标句子。<br> 通过使用自注意力机制，Transformer可以更好地处理长句子和捕捉长期依赖关系。这种方法使得模型在机器翻译等任务中表现出色。</p> 
</blockquote> 
<h3><a id="2_42"></a>2.数据集是什么？</h3> 
<p><strong>数据集是指在进行机器学习或深度学习任务时所使用的数据的集合。它是从现实世界中收集、整理和标注的一组样本数据，用于训练、验证和测试机器学习模型的性能。</strong></p> 
<p>数据集通常包含输入数据和对应的标签或目标值。输入数据可以是图像、文本、音频等不同形式的数据，而标签或目标值则是对输入数据的描述、分类或预测结果。在训练过程中，机器学习模型通过对数据集进行学习和优化，从而能够对新的未见过的数据进行准确的预测或分类。</p> 
<p>数据集的质量和多样性对于机器学习模型的性能至关重要。一个好的数据集应该具有代表性、多样性和充分的样本数量，以确保模型能够泛化到不同的情况和场景中。同时，数据集的标注质量也需要高，以保证模型能够学习到正确的模式和规律。</p> 
<p>在论文中，作者使用了Argoverse数据集和NGSIM数据集作为训练和评估模型的基准数据集，用于车辆行驶轨迹预测和车辆换道意图识别任务。这些数据集是从真实的交通场景中收集而来，并经过标注和处理，以供研究人员进行相关任务的研究和性能评估。</p> 
<h3><a id="3_50"></a>3.预测车辆行驶轨迹和车辆换道意图</h3> 
<p>预测车辆行驶轨迹和车辆换道意图是自动驾驶系统中的两个重要任务。在自动驾驶系统中，车辆需要能够准确地预测其他车辆的行驶轨迹和换道意图，以便做出相应的决策和规划行驶路线。</p> 
<p><strong>预测车辆行驶轨迹是指根据车辆当前的位置、速度和方向等信息，预测其未来一段时间内的行驶轨迹</strong>这个任务的难点在于需要考虑到其他车辆、行人、路标等因素对车辆行驶轨迹的影响，同时需要考虑到车辆自身的动力学特性和环境因素，如路况、天气等。</p> 
<p><strong>车辆换道意图识别是指根据车辆当前的位置、速度和方向等信息，判断其是否有换道的意图，并预测其可能的换道方向和时间</strong>这个任务的难点在于需要考虑到其他车辆的行驶轨迹、速度和方向等信息，同时需要考虑到车辆自身的动力学特性和环境因素，如路况、车道宽度等。</p> 
<h3><a id="4_LSTM__56"></a>4. LSTM 网络</h3> 
<p>LSTM（Long Short-Term Memory）网络是一种循环神经网络（RNN）的变体，用于处理序列数据和时间序列数据的建模和预测任务。相比于传统的RNN，LSTM网络<strong>通过引入门控机制，能够更好地捕捉和记忆长期依赖关系</strong>，从而在处理长序列数据时表现更好。</p> 
<p><strong>LSTM网络的核心是LSTM单元，它由输入门、遗忘门和输出门组成。每个门控制着信息的流动和记忆的更新，从而实现对序列数据的建模和预测。具体来说，LSTM单元通过输入门来决定哪些信息需要被记忆，通过遗忘门来决定哪些信息需要被遗忘，通过输出门来决定哪些信息需要输出。</strong></p> 
<p>LSTM网络的输入是一个序列数据，如文本、音频或时间序列数据。每个时间步的输入都会经过LSTM单元进行处理，并输出一个隐藏状态和一个输出。隐藏状态可以传递到下一个时间步，用于捕捉序列数据中的长期依赖关系，而输出则可以用于预测任务，如分类、回归或生成。</p> 
<p>LSTM网络在自然语言处理、语音识别、机器翻译等任务中得到了广泛应用，特别是在需要处理长序列数据和长期依赖关系的场景下。它的门控机制使得网络能够更好地处理梯度消失和梯度爆炸等问题，从而提高了模型的性能和训练效果。</p> 
<h2><a id="_64"></a>二、神经网络概述</h2> 
<p><img src="https://images2.imgbox.com/a1/2e/xQbRgFAt_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="1_66"></a>1.人工智能是什么？</h3> 
<p>就是让电脑具备人类的智慧与思考方式，从而以模仿人类的思维来处理问题。</p> 
<h3><a id="2_69"></a>2.机器学习是什么？</h3> 
<p>机器学习（Machine Learning）是一种人工智能的分支，旨在通过让计算机从数据中学习和改进，而不是显式地编程规则，来实现任务的自动化。机器学习算法可以从数据中发现模式和规律，并用于预测、分类、聚类等任务。</p> 
<p>人类一般建立算法是通过我们对规律的总结后编写的，比如冒泡排序的模型，计算机只需要获取输入，以及我们人类写好的算法模型，就能得出结果。但是机器学习要求的是我们人类提供的算法模型信息变少，需要计算机自己完成模型的构建。<br> 比如车牌号识别，我们人类提前提供正确的数字与字母信息，然后输入车票图片，让机器根据这些信息来建立起一套识别的模型，并进行验证。</p> 
<h3><a id="3_75"></a>3.深度学习是什么？</h3> 
<p>深度学习（Deep Learning）是机器学习的一种特殊形式，它基于人工神经网络（Artificial Neural Networks）的概念和结构。深度学习通过多层次的神经网络模型，可以学习和表示更加复杂的特征和模式，从而在图像识别、语音识别、自然语言处理等领域取得了重大突破。</p> 
<p>也就是说，<strong>深度学习把大量的我们采集到的数据，利用其多层次的结构来逐步的提取出需要的特征信息，从而捕捉到特定的规律。</strong></p> 
<p>举一个实际的例子：<br> 比如我们人类很容易就可以分辨出来猫与狗，而如何让电脑分辨出来呢？传统的方法可能是图像处理，借助一些算法来找出猫狗在特定部位的差异，从而得出结果。</p> 
<p><strong>而深度学习呢？他摒弃了对具体画面的处理，只从输入的数据和输出的结果来看，中间的处理与分析作为隐藏层，我们输入大量的数据，比如一幅猫的图片，那么每个像素点我们都可以人为的给他打上一个权重参数，意思就是我们人类告诉机器，你需要输出的正确的猫图像是什么样子的。权重参数代表的就是符合猫特征的程度。如果像素点A的权重参数高，B的权重参数小，那么在神经网络每一次的迭代过程中，就会遗弃低权重数据，将高权重数据作为特征进行提取。经过成千上万次迭代，最终留下的就是最契合猫的特征。</strong></p> 
<h3><a id="4_85"></a>4.神经网络是什么？</h3> 
<p>神经网络是一种模拟人脑的神经网络以期能够实现类人工智能的机器学习技术。人脑中的神经网络是一个非常复杂的组织。成人的大脑中估计有1000亿个神经元之多。<br> <img src="https://images2.imgbox.com/5f/c3/Wu6i983Z_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="_88"></a>一个最简单的神经网络模型</h4> 
<p>让我们来看一个经典的神经网络。这是一个包含三个层次的神经网络。红色的是输入层，绿色的是输出层，紫色的是中间层（也叫隐藏层）。输入层有3个输入单元，隐藏层有4个单元，输出层有2个单元。后文中，我们统一使用这种颜色来表达神经网络的结构。<br> <img src="https://images2.imgbox.com/c6/98/a5CmW3mu_o.png" alt="在这里插入图片描述"><br> ①设计一个神经网络时，输入层与输出层的节点数往往是固定的，中间层则可以自由指定；<br> ②神经网络结构图中的拓扑与箭头代表着预测过程时数据的流向，跟训练时的数据流有一定的区别；<br> ③<strong>结构图里的关键不是圆圈（代表“神经元”），而是连接线（代表“神经元”之间的连接）。每个连接线对应一个不同的权重（其值称为权值），这是需要训练得到的</strong>。<br> <img src="https://images2.imgbox.com/0e/fd/Gqzq9LUh_o.png" alt="在这里插入图片描述"><br> 　我们有一个数据，称之为样本。样本有四个属性，其中三个属性已知，一个属性未知。我们需要做的就是通过三个已知属性预测未知属性。</p> 
<p>具体办法就是使用神经元的公式进行计算。三个已知属性的值是a1，a2，a3，未知属性的值是z。z可以通过公式计算出来。</p> 
<p>这里，已知的属性称之为特征，未知的属性称之为目标。假设特征与目标之间确实是线性关系，并且我们已经得到表示这个关系的权值w1，w2，w3。那么，我们就可以通过神经元模型预测新样本的目标。</p> 
<h4><a id="_102"></a>神经网络可以理解为一个分类器，通过训练后，能够记住我们人为需要的一类数据特征，那么对以后输入的任何数据都会按照这个记忆来分类，这种记忆力与神经网络每一层的神经元个数以及深度有关。数学思维下，深度学习的神经网络好比一个可以自主调节系数的公式。</h4> 
<p>①对特定事物的数据化<br> ②对数据进行特征提取<br> ③有效性的过滤<br> ④分类器进行特征的分类</p> 
<h3><a id="5RNNCNN_108"></a>5.RNN和CNN是什么？各自有什么优缺点？</h3> 
<h4><a id="RNN_109"></a>RNN：时序序列数据的处理</h4> 
<p>RNN是一种具有循环连接的神经网络，它可以处理序列数据，如时间序列数据或自然语言文本。RNN的关键特点是它可以通过记忆之前的信息来影响当前的输出。这使得RNN在处理具有时序关系的数据时非常有效。RNN的一个常见变体是LSTM（长短期记忆网络），它通过引入门控机制来更好地捕捉长期依赖关系。</p> 
<h4><a id="CNN_111"></a>CNN:网格化图像数据处理</h4> 
<p>CNN是一种专门用于处理网格化数据（如图像）的神经网络结构。CNN通过卷积层和池化层来提取图像中的局部特征，并通过全连接层进行分类或回归。CNN的关键特点是它可以共享权重，从而减少了模型的参数量，提高了模型的效率和泛化能力。CNN在图像识别、目标检测和图像生成等任务中表现出色。</p> 
<h4><a id="RNNCNN_114"></a>RNN和CNN各自有以下优缺点：</h4> 
<p>RNN的优点：<br> 能够处理序列数据，具有记忆能力，可以捕捉长期依赖关系。<br> 在自然语言处理、语音识别等任务中表现出色。<br> 可以处理变长输入序列。</p> 
<p>RNN的缺点：<br> 训练RNN模型时，梯度消失和梯度爆炸问题可能会出现，导致难以训练深层网络。<br> 难以并行化处理，限制了其在大规模数据上的训练效率。<br> 对于长序列的处理效果可能不佳。</p> 
<p>CNN的优点：<br> 能够有效地提取图像中的局部特征，具有平移不变性。<br> 参数共享使得模型更加高效，减少了模型的复杂性。<br> 可以通过堆叠多个卷积层来提取更高级别的特征。</p> 
<p>CNN的缺点：<br> 对于处理序列数据的效果可能不如RNN。<br> 对于处理变长输入序列的任务，需要进行填充或截断操作。<br> 在处理大规模数据时，需要较大的计算资源。</p> 
<h2><a id="_137"></a>二、绪论分析</h2> 
<p><img src="https://images2.imgbox.com/25/41/VXqWRGYr_o.png" alt="在这里插入图片描述"></p> 
<blockquote> 
 <p>近年来，基于注意力机制的 Transformer 模型在自然语言处理、计算机视觉、 序列预测各个研究领域大放异彩。 <strong>Transformer 模型</strong>具有长序列处理能力强、并行 计算能力强、捕捉全局信息能力强、可解释性强、适用于不同任务等优点， <strong>十分适合于进行车辆轨迹预测与车辆换道意图识别任务</strong>。其注意力机制可以更好地捕捉序列中的长期依赖关系， 避免了梯度消失和梯度爆炸等问题， 这意味着模型可以考虑更长时间范围内的车辆行驶情况，从而提高预测的准确性。<br> 目前自动驾驶技术飞速发展， 若能够利用大数据与各种先进的深度学习算法、 模型等， 同时<strong>考虑到附近车辆之间的相互影响， 并且若能以此为依据准确预测每辆 车的行驶轨迹</strong>， 那么对于自动驾驶车辆来说， 就可以更好地规避交通事故和减少事 故风险， 避免交通拥堵和减少行驶时间， 这将大大地提高人们的通行效率、出行舒适度， 并且降低燃油消耗和提高驾驶体验， 为人们的出行带来更多的便利和舒适。<br> 本文将重点研究车辆行驶轨迹的预测与车辆换道意图的识别。<strong>通过对数据集 的预处理， 考虑不同车辆之间的影响， 使用改进的 Transformer 模型在不同的时间 尺度上进行车辆的轨迹预测； 使用改进的 Transformer 模型对车辆轨迹数据进行分类来判断识别车辆的换道意图</strong>。本文的主要组织结构如图 1.1 所示。<br> 本文第二章主要介绍了几种应用广泛的自动驾驶数据集， 并<strong>研究了本文实验 使用的公开免费 Argoverse 数据集以及 NGSIM 数据集的预处理方法，通过使用 Python 脚本语言过滤与筛选剔除无用信息，使用特定的规则抽取相关数据</strong>，保留可用于模型训练与验证的感兴趣数据。<br> 本文第三章主要研究了基于<strong>改进 Transformer 模型进行车辆轨迹预测的方法</strong>。 通过将图卷积神经网络处理车辆之间的相互关系， 与 Transformer 的注意力机制相 结合， 在同时考虑当前车辆历史运动轨迹、周围车辆的影响以及使用注意力机制计 算不同时间点自身位置及周围车辆位置的重要性得分， 从而更好地预测未来轨迹。 通过改变模型参数， 调整模型结构等方式优化模型， 最后与不同的方法通过实验数 据结果比较进行定量分析， 并给出了轨迹预测结果的可视化， 以此对本文模型进行定性分析。<br> 本文第四章主要研究了<strong>基于改进 Transformer 模型的车辆换道意图识别方法</strong>。 通过对传统 Transformer 模型结构的分析与数据特点、任务特点的分析， 使用图结 构建模车辆之间的相互影响， 在数据集上对改进模型进行训练与验证。同时， 建立了 LSTM 模型作为对比试验，在 NGSIM 数据集上用各样指标来比较两种模型的准确率。</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/40/6c/WPbdPtPf_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="transformer_147"></a>三、简要解释基于transformer的车辆未来时刻轨迹预测</h2> 
<blockquote> 
 <p>在 Transformer 模型的基础上，改进了模型结构，引入了图卷积神经网 络来建模车辆之间的交互影响，并且因为没有使用图学习神经网络学习图结构邻 接矩阵， 而是采用第二个多头注意力层，使网络在预测当前车辆轨迹时， 能够更加 注意当前车辆附近的车辆的轨迹对当前车辆轨迹的影响。</p> 
</blockquote> 
<p>分析：<br> <strong>传统的Transformer模型在车辆轨迹预测中可能无法充分考虑车辆之间的交互影响，因为它主要关注序列中的位置点之间的关系。而GCN可以通过图结构来捕捉车辆之间的交互关系。</strong></p> 
<p>在这个改进中，首先构建一个车辆之间的图结构，其中<strong>每个车辆是图中的一个节点。然后，通过GCN层来学习节点之间的信息传递和交互</strong>。</p> 
<p>在传统的GCN中，通常需要定义一个邻接矩阵来表示节点之间的连接关系。但在这个改进中，采用了第二个多头注意力层来代替邻接矩阵的学习。这样做的好处是可以让网络在预测当前车辆轨迹时更加关注当前车辆附近的车辆的轨迹对当前车辆轨迹的影响。</p> 
<p>通过引入GCN和多头注意力层，改进后的模型可以更好地建模车辆之间的交互影响，从而提高车辆轨迹预测的准确性和可靠性。</p> 
<h2><a id="transformer_159"></a>四、简要分析基于transformer的车辆换道意图预测</h2> 
<blockquote> 
 <p>对 NGSIM 数据集进行处理，提取换道车辆周围最近 6 辆车的相对于当前车辆的 XY 坐标偏 移量、相对于当前车辆的速度、相对于当前车辆的加速度信息， 在进行归一化后共 同构成了输入矩阵。同时， 本文还使用基于 LSTM 的模型在 NGSIM 数据集上同样 进行了车辆换道意图识别任务的训练与测试。测试结果考虑了多种时间长度序列<br> 输入情况下(1s、2s、3s)本文基于 Transformer 的车辆换道意图识别模型和不同隐藏层数量下的基于 LSTM 的模型在 NGSIM 测试集上对于车辆换道意图识别分类任 务的准确率、召回率、和 F1 分数，并通过分析实验数据结果，证明了本文基于Transformer 改进的车辆意图识别模型的优势</p> 
</blockquote> 
<p>分析：也就是说，<strong>输入的数据就是当前车辆周边6辆车的相对坐标偏移量、速度与加速度信息，构成输入矩阵。将归一化后的特征组合成输入序列。可以根据需要选择不同的时间长度，例如，选择前1秒、2秒或3秒的特征作为输入序列。使用Transformer模型来进行车辆换道意图预测</strong>。Transformer模型由多个编码器层和解码器层组成。编码器层用于学习输入序列的表示，解码器层用于生成预测结果。</p> 
<p>使用准备好的数据集对Transformer模型进行训练。训练过程中，通过最小化损失函数来优化模型参数，使其能够准确地预测车辆的换道意图。使用测试集对训练好的模型进行评估。计算准确率、召回率、F1分数等指标来评估模型的性能。使用训练好的模型对新的车辆轨迹数据进行预测，以识别车辆的换道意图。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/194c11821aef66de62c3f398292bcd30/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">18.Linux Shell中的mktemp命令使用</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/75ec2b03a629531fbc163afd39a5e29e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">C语言常量详解</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>