<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【论文解读】BERT和ALBERT - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【论文解读】BERT和ALBERT" />
<meta property="og:description" content="文章目录 1.前言2. BERT2.1 引入2.2 以前的工作2.2.1 feature-based 方法2.2.2 fine-tuning 方法2.2.3 迁移学习方法 2.3 BERT架构2.3.1 MLM2.3.2 NSP 2.4 实验2.4.1 BERT模型的效果2.4.2 验证性实验 3.ALBERT3.1 引入3.2 相关工作3.2.1 cross-layer parameter sharing（交叉层的参数共享）3.2.2 sentence-order prediction (SOP，句子顺序预测) 3.3 ALBERT的模型3.3.1 factorized embedding parameterization3.3.2 Cross-layer parameter sharing3.3.3 Inter-sentence coherence loss. 3.4 实验3.4.1 BERT和ALBERT的对比3.4.2 交叉层参数共享实验3.4.2 SOP 4. 参考 1.前言 最近重新阅读了BERT和ALBERT文章，所以写下自己的一些感悟。这两篇文章都是Google发出来的。其中BERT是2018年，在Transformer的基础上进行扩展；而ALBERT发表在2020年ICLR上，它是基础BERT来进行改进。
BERT论文 ALBERT论文 2. BERT BERT全称是Bidirectional Encoder Representations from Transformers，它通过连接从左到右和从右到左的文本，设计了一个预处理的深度双向表达模型。在fine-tuned阶段，只需要增加简单的输出层，就可以在BERT模型基础上达到SOTA的效果。
BERT在GLUE数据集上能够达到80.4%，在MultiNLI上则有86.7%，在SQuAD v1.1则有93.2%。
2.1 引入 在NLP场景中，预处理的模型往往能够提升下游任务的效果。在natural language inference（语言推断）、relation classification、NER和QA任务中，预处理的语言模型都是有效果的。
预处理的语言模型到下游任务中，主要有两种策略方法：
feature-based方法：类似于ELMO那样，设计了一个比较精细的结构，作为额外的特征进行输入。fine-tuning 方法：类似于Transformer和GPT那样的模型，它们可以在下游任务中进行简单的调节参数，使得模型适应于下游任务。 这些方法都是单方向（unidirectional）的语言模型。
BERT引入了两个预处理时的学习目标，包括：Masked language model（MLM）、next sentence predicton（NSP）。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/b7445ba1ebb65e2d28d8ff8e303eca91/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-03-12T14:52:20+08:00" />
<meta property="article:modified_time" content="2020-03-12T14:52:20+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【论文解读】BERT和ALBERT</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#1_1" rel="nofollow">1.前言</a></li><li><a href="#2_BERT_7" rel="nofollow">2. BERT</a></li><li><ul><li><a href="#21__12" rel="nofollow">2.1 引入</a></li><li><a href="#22__25" rel="nofollow">2.2 以前的工作</a></li><li><ul><li><a href="#221_featurebased__26" rel="nofollow">2.2.1 feature-based 方法</a></li><li><a href="#222_finetuning__31" rel="nofollow">2.2.2 fine-tuning 方法</a></li><li><a href="#223__34" rel="nofollow">2.2.3 迁移学习方法</a></li></ul> 
   </li><li><a href="#23_BERT_37" rel="nofollow">2.3 BERT架构</a></li><li><ul><li><a href="#231_MLM_55" rel="nofollow">2.3.1 MLM</a></li><li><a href="#232_NSP_62" rel="nofollow">2.3.2 NSP</a></li></ul> 
   </li><li><a href="#24__77" rel="nofollow">2.4 实验</a></li><li><ul><li><a href="#241_BERT_85" rel="nofollow">2.4.1 BERT模型的效果</a></li><li><a href="#242__94" rel="nofollow">2.4.2 验证性实验</a></li></ul> 
  </li></ul> 
  </li><li><a href="#3ALBERT_115" rel="nofollow">3.ALBERT</a></li><li><ul><li><a href="#31__118" rel="nofollow">3.1 引入</a></li><li><a href="#32__134" rel="nofollow">3.2 相关工作</a></li><li><ul><li><a href="#321_crosslayer_parameter_sharing_135" rel="nofollow">3.2.1 cross-layer parameter sharing（交叉层的参数共享）</a></li><li><a href="#322_sentenceorder_prediction_SOP_138" rel="nofollow">3.2.2 sentence-order prediction (SOP，句子顺序预测)</a></li></ul> 
   </li><li><a href="#33_ALBERT_141" rel="nofollow">3.3 ALBERT的模型</a></li><li><ul><li><a href="#331_factorized_embedding_parameterization_144" rel="nofollow">3.3.1 factorized embedding parameterization</a></li><li><a href="#332_Crosslayer_parameter_sharing_155" rel="nofollow">3.3.2 Cross-layer parameter sharing</a></li><li><a href="#333_Intersentence_coherence_loss_159" rel="nofollow">3.3.3 Inter-sentence coherence loss.</a></li></ul> 
   </li><li><a href="#34__164" rel="nofollow">3.4 实验</a></li><li><ul><li><a href="#341_BERTALBERT_167" rel="nofollow">3.4.1 BERT和ALBERT的对比</a></li><li><a href="#342__172" rel="nofollow">3.4.2 交叉层参数共享实验</a></li><li><a href="#342_SOP_176" rel="nofollow">3.4.2 SOP</a></li></ul> 
  </li></ul> 
  </li><li><a href="#4__180" rel="nofollow">4. 参考</a></li></ul> 
</div> 
<p></p> 
<h2><a id="1_1"></a>1.前言</h2> 
<p>最近重新阅读了BERT和ALBERT文章，所以写下自己的一些感悟。这两篇文章都是Google发出来的。其中BERT是2018年，在Transformer的基础上进行扩展；而ALBERT发表在2020年ICLR上，它是基础BERT来进行改进。</p> 
<ul><li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" checked disabled> BERT论文</li><li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" checked disabled> ALBERT论文</li></ul> 
<h2><a id="2_BERT_7"></a>2. BERT</h2> 
<p>BERT全称是<strong>Bidirectional Encoder Representations from Transformers</strong>，它通过连接从左到右和从右到左的文本，设计了一个预处理的深度双向表达模型。在fine-tuned阶段，只需要增加简单的输出层，就可以在BERT模型基础上达到SOTA的效果。</p> 
<p>BERT在GLUE数据集上能够达到80.4%，在MultiNLI上则有86.7%，在SQuAD v1.1则有93.2%。</p> 
<h3><a id="21__12"></a>2.1 引入</h3> 
<p>在NLP场景中，预处理的模型往往能够提升下游任务的效果。在natural language inference（语言推断）、relation classification、NER和QA任务中，预处理的语言模型都是有效果的。</p> 
<p>预处理的语言模型到下游任务中，主要有两种策略方法：</p> 
<ul><li><strong>feature-based方法</strong>：类似于ELMO那样，设计了一个比较精细的结构，作为额外的特征进行输入。</li><li><strong>fine-tuning 方法</strong>：类似于Transformer和GPT那样的模型，它们可以在下游任务中进行简单的调节参数，使得模型适应于下游任务。</li></ul> 
<p>这些方法都是单方向（unidirectional）的语言模型。</p> 
<p>BERT引入了两个预处理时的学习目标，包括：<strong>Masked language model（MLM）、next sentence predicton（NSP）。</strong></p> 
<ul><li><strong>MLM</strong>：随机掩盖一部分的tokens，然后对这些词语进行预测。这可以使得模型因为不知道是要预测哪些词语，所以必须要学习“<strong>从左到右</strong>”和“<strong>从右到左</strong>”的文本信息。</li><li><strong>NSP</strong>：输入两个句子，判断这两个句子是否是连贯性的句子。</li></ul> 
<h3><a id="22__25"></a>2.2 以前的工作</h3> 
<h4><a id="221_featurebased__26"></a>2.2.1 feature-based 方法</h4> 
<p>预处理得到的词向量能够作为模型的输入，这会带来大量的特征信息，提升之后的任务效果。</p> 
<p>在ELMO中，生成了比较传统的词向量。它们提出上下文敏感的特征，能够表征复杂语境下的词语语义。</p> 
<h4><a id="222_finetuning__31"></a>2.2.2 fine-tuning 方法</h4> 
<p>在有监督的下游任务微调模型之前，预先训练一些关于LM目标的模型架构。这种方法就是提前训练好了一个模型（<strong>提前训练的模型通常是用无监督的方法训练的</strong>），在下游任务中几乎不需要从头开始学习模型参数。GPT模型就是这种做法。</p> 
<h4><a id="223__34"></a>2.2.3 迁移学习方法</h4> 
<p>在有监督的数据集中训练一个模型，然后把训练好的模型迁移到另一个数据集中进行训练学习。这种方法也不需要模型从新训练，但是需要大量的有监督数据集。</p> 
<h3><a id="23_BERT_37"></a>2.3 BERT架构</h3> 
<p>论文中训练了两个基础的BERT：</p> 
<table><thead><tr><th>名称</th><th>Transformer的层数<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
        
         
          
          
            L 
           
          
         
           L 
          
         
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault">L</span></span></span></span></span></th><th>隐藏层大小<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
        
         
          
          
            H 
           
          
         
           H 
          
         
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.08125em;">H</span></span></span></span></span></th><th>self-attention head的数量<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
        
         
          
          
            A 
           
          
         
           A 
          
         
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault">A</span></span></span></span></span></th><th>总参数数量</th></tr></thead><tbody><tr><td>BERT_base</td><td>12</td><td>768</td><td>12</td><td>110M</td></tr><tr><td>BERT_large</td><td>24</td><td>1024</td><td>16</td><td>340M</td></tr></tbody></table> 
<p>可以看到BERT模型的参数量是很大的。它在输入表达中构建了<strong>三层</strong>的词语嵌入，同时设计了<strong>两个</strong>预训练时的任务（MLM，NSP）</p> 
<ul><li>BERT的输入：<br> <img src="https://images2.imgbox.com/90/29/PJFf1kzJ_o.png" alt="输入表达"><br> （1）<strong>Token embedding</strong>：句子中每个词语的词向量输入，使用了wordpiece方法。[CLS]和[SEP]代表的是输入句子的开头和结尾；[SEP]则分割了两个输入句子<br> （2）<strong>Segment embedding</strong>：句子向量，在代码实现中，把第一个句子设计成0，第二个句子设计成1，然后随机初始化<br> （3）<strong>Position embedding</strong>：词语的位置向量。</li></ul> 
<p>接下来介绍两个预训练的任务：MLM，SNP</p> 
<h4><a id="231_MLM_55"></a>2.3.1 MLM</h4> 
<p>为了实现真正的深度双向模型，所以使用了随机掩码。使用mask的原因是为了防止模型在双向循环训练的过程中“预见自身”。于是，文章中选取的策略是对输入序列中15%的词使用[MASK]标记掩盖掉，然后通过上下文去预测这些被mask的词语。但是为了防止模型过拟合地学习到【MASK】这个标记，对15%mask掉的词进一步优化：</p> 
<ul><li>以80%的概率用[MASK]替换：my dog is hairy ----&gt; my dog is [MASK]</li><li>以10%的概率随机替换：my dog is hairy ----&gt; my dog is apple</li><li>以10%的概率不进行替换：my dog is hairy ----&gt; my dog is hairy</li></ul> 
<h4><a id="232_NSP_62"></a>2.3.2 NSP</h4> 
<p>为了让模型能够学习到句子之间关系，则预训练任务中加入了“预测下一句”的任务，这是一个二值分类任务。也即是说输入句子中包括了两个句子A和B。</p> 
<ul><li>50%的B句子是A句子的下一个句子</li><li>另外50%的句子则是从数据集中随机抽取出来的。</li></ul> 
<p>例如例子：</p> 
<pre><code class="prism language-python">Input <span class="token operator">=</span> <span class="token punctuation">[</span>CLS<span class="token punctuation">]</span> the man went to <span class="token punctuation">[</span>MASK<span class="token punctuation">]</span> store <span class="token punctuation">[</span>SEP<span class="token punctuation">]</span> he bought a gallon <span class="token punctuation">[</span>MASK<span class="token punctuation">]</span> milk <span class="token punctuation">[</span>SEP<span class="token punctuation">]</span>
Label <span class="token operator">=</span> IsNext

Input <span class="token operator">=</span> <span class="token punctuation">[</span>CLS<span class="token punctuation">]</span> the man <span class="token punctuation">[</span>MASK<span class="token punctuation">]</span> to the store <span class="token punctuation">[</span>SEP<span class="token punctuation">]</span> penguin <span class="token punctuation">[</span>MASK<span class="token punctuation">]</span> are flight <span class="token comment">##less birds [SEP]</span>
Label <span class="token operator">=</span> NotNext
</code></pre> 
<h3><a id="24__77"></a>2.4 实验</h3> 
<p>接下来看一下实验部分。首先需要说明BERT在不同数据集中输出的部分都不一致。<br> <img src="https://images2.imgbox.com/74/3b/f5xOKfd1_o.png" alt="BERT输出部分"></p> 
<ul><li>（a）图主要用来适应两个句子的输入。获取[CLS]位置的输出，把它作为句子向量，然后预测类别。（<strong>适合句子推断等任务</strong>）</li><li>（b）图主要用来适应单个句子的输入。获取[CLS]位置的输出，把它作为句子向量，然后预测类别。（<strong>适合句子分类等任务</strong>）</li><li>（c）图主要用来适应QA任务，输入的是question和paragraph，输出找到的answer。</li><li>（d）图主要用来做序列标注。</li></ul> 
<h4><a id="241_BERT_85"></a>2.4.1 BERT模型的效果</h4> 
<p><img src="https://images2.imgbox.com/66/4a/cbPGoDCC_o.png" alt="BERT模型的效果"><br> 上面可以看出，对比几个模型，BERT的效果都大于现有的模型</p> 
<p><img src="https://images2.imgbox.com/9c/c5/DIRpEXdb_o.png" alt="NER任务"><br> 在CoNLL-2003的NER任务中，也比传统的LSTM+CRF要高。</p> 
<h4><a id="242__94"></a>2.4.2 验证性实验</h4> 
<p>（1）主要验证MLM的效果。</p> 
<ul><li>no NSP：没有使用NSP任务训练，仅仅使用MLM任务</li><li>LTR &amp; NO NSP：把MLM任务换成left to right（LTR）任务，也就是放弃了随机mask，改成按照顺序预测每一个词语</li><li>+BiLSTM：在LTR &amp; NO NSP任务上，输出层加了BiLSTM。<br> <img src="https://images2.imgbox.com/fa/c6/2kxf0UfL_o.png" alt="验证MLM"><br> 从图上的实验效果可以看出，使用MLM的模型会比LTR模型的效果要好很多。这侧面证实了MLM可以让模型保持双向的上下文内容的学习。</li></ul> 
<p>（2）参数实验<br> <img src="https://images2.imgbox.com/76/9b/a48wYgsQ_o.png" alt="参数实验"><br> BERT的层数L越多，隐藏层大小H越多，效果越好</p> 
<p>（3）MNLI数据集上的收敛速度<br> <img src="https://images2.imgbox.com/d3/36/QRArVYKH_o.png" alt="MNLI数据集上的收敛速度"><br> MLM的收敛速度确实比LTR模式稍慢。然而，就绝对准确度而言，MLM几乎立刻就开始超越LTR模式</p> 
<p>（4）BERT当feature-based来用（在Conll-2003数据集上）<br> <img src="https://images2.imgbox.com/5d/36/cw0mbPyS_o.png" alt="BERT当feature-based来用"><br> 实验中在BERT输出之后加一层BLSTM进行分类。如果把所有层的输出加起来，得到的效果最好。</p> 
<h2><a id="3ALBERT_115"></a>3.ALBERT</h2> 
<p>ALBERT模型是在BERT模型的基础上进行改进的。它设计了参数减少的方法，用来降低内存消耗，同时加快BERT的训练速度。同时使用了self-supervised loss（自监督损失函数）关注构建句子中的内在连贯性（coherence ）。实验在<strong>GLUE、RACE、SQuAD数据集</strong>上取得了最好的效果。</p> 
<h3><a id="31__118"></a>3.1 引入</h3> 
<p>在NLP任务中，一个好的预训练模型能够提升模型的效果。当前一个SOTA的模型，它有几百万或者十亿以上的参数，如果要扩大模型规模，就会遇到这些计算机内存上的限制，同时训练速度会受到限制。目前存在的解决方法有两种：<strong>模型并行化、好的内存管理机制</strong>。但这两种方法都会存在通信开销。因此本论文设计了一个<strong>A lite BERT（ALBERT）</strong>，它比BERT使用更少的参数。</p> 
<p>ALBERT使用了<strong>两种参数减少的方法</strong>：</p> 
<ul><li><strong>factorized embedding parameterization（词嵌入的因式分解）</strong>：把词嵌入矩阵进行分解，分解成更少的两个矩阵。</li><li><strong>cross-layer parameter sharing（交叉层的参数共享）</strong>：这种技术在深层的网络中更能减少参数。</li></ul> 
<p>总的来说，参数减少技术像是一种正则化方法。ALBERT会比BERT-large的参数量少18倍，同时训练速度快1.7倍。</p> 
<p>ALBERT还提出了另一种方法，用来代替NSP技术，这种新技术叫<strong>sentence-order prediction (SOP)</strong>。SOP是一种self-supervised loss。</p> 
<p>因此ALBERT利用了三种技术：</p> 
<ul><li><strong>factorized embedding parameterization（词嵌入的因式分解）</strong></li><li><strong>cross-layer parameter sharing（交叉层的参数共享）</strong></li><li><strong>sentence-order prediction (SOP，句子顺序预测)</strong></li></ul> 
<h3><a id="32__134"></a>3.2 相关工作</h3> 
<h4><a id="321_crosslayer_parameter_sharing_135"></a>3.2.1 cross-layer parameter sharing（交叉层的参数共享）</h4> 
<p>参数共享的思想来自于Transformer模型中。在多个研究工作中，发现交叉层的参数共享确实能够提升模型的效果，且模型层与层之间通常是震荡的，而这种做法也能够使得层与层之间收敛。</p> 
<h4><a id="322_sentenceorder_prediction_SOP_138"></a>3.2.2 sentence-order prediction (SOP，句子顺序预测)</h4> 
<p>ALBERT设计两个一个新的loss，来预测两个连续的文本段的顺序。语篇中的连贯性（coherence）和衔接性（cohesion）已得到广泛研究，并且已发现许多现象将相邻的文本片段连接在一起。比如：Skip-thought和FastSent利用句子向量预测相邻句子的词语。</p> 
<h3><a id="33_ALBERT_141"></a>3.3 ALBERT的模型</h3> 
<p>ALBERT的骨干结构是BERT模型，同时使用了GELU激活函数。定义词典的大小为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         E 
        
       
      
        E 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.05764em;">E</span></span></span></span></span>，encoder的层数为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         L 
        
       
      
        L 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault">L</span></span></span></span></span>，隐藏层的大小为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         H 
        
       
      
        H 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.08125em;">H</span></span></span></span></span>，attention head为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         H 
        
       
         / 
        
       
         64 
        
       
      
        H/64 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.08125em;">H</span><span class="mord">/</span><span class="mord">6</span><span class="mord">4</span></span></span></span></span>。</p> 
<h4><a id="331_factorized_embedding_parameterization_144"></a>3.3.1 factorized embedding parameterization</h4> 
<p>在BERT、XLBERT和RoBERTa中wordpiece embedding的大小为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         E 
        
       
      
        E 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.05764em;">E</span></span></span></span></span>，同时隐藏层大小为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         H 
        
       
      
        H 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.08125em;">H</span></span></span></span></span>，它们两个之间是相等的：<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         E 
        
       
         = 
        
       
         H 
        
       
      
        E=H 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.05764em;">E</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.08125em;">H</span></span></span></span></span>。</p> 
<p>从建模的观点来看，wordpiece embedding主要学习上下文相关的表征，而隐藏层则学习上下文无关的表征。。显然后者更加复杂，需要更多的参数，也就是说模型应当增大隐层大小 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         H 
        
       
      
        H 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.08125em;">H</span></span></span></span></span>，或者说满足 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         H 
        
       
         ≫ 
        
       
         E 
        
       
      
        H \gg E 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72243em; vertical-align: -0.0391em;"></span><span class="mord mathdefault" style="margin-right: 0.08125em;">H</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">≫</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.05764em;">E</span></span></span></span></span>.</p> 
<p>从实际角度来看，通常词典大小<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         V 
        
       
      
        V 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.22222em;">V</span></span></span></span></span>是非常大的，如果<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         E 
        
       
         = 
        
       
         H 
        
       
      
        E=H 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.05764em;">E</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.08125em;">H</span></span></span></span></span>，增加<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         H 
        
       
      
        H 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.08125em;">H</span></span></span></span></span>的大小，会使得<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         V 
        
       
         × 
        
       
         E 
        
       
      
        V \times E 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.76666em; vertical-align: -0.08333em;"></span><span class="mord mathdefault" style="margin-right: 0.22222em;">V</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.05764em;">E</span></span></span></span></span>的矩阵非常大，从而造成模型参数过大，训练速度减慢。</p> 
<p>因此，ALBERT使用了隐式分解的方法，把embedding矩阵参数分解成两个矩阵。也即是把参数量<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         O 
        
       
         ( 
        
       
         V 
        
       
         × 
        
       
         E 
        
       
         ) 
        
       
      
        O(V \times E) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.22222em;">V</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.05764em;">E</span><span class="mclose">)</span></span></span></span></span>变为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         O 
        
       
         ( 
        
       
         V 
        
       
         × 
        
       
         E 
        
       
         + 
        
       
         E 
        
       
         × 
        
       
         H 
        
       
         ) 
        
       
      
        O(V \times E + E \times H) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.22222em;">V</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.76666em; vertical-align: -0.08333em;"></span><span class="mord mathdefault" style="margin-right: 0.05764em;">E</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.76666em; vertical-align: -0.08333em;"></span><span class="mord mathdefault" style="margin-right: 0.05764em;">E</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.08125em;">H</span><span class="mclose">)</span></span></span></span></span></p> 
<p>在实现时，随机初始化<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         V 
        
       
         × 
        
       
         E 
        
       
      
        V \times E 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.76666em; vertical-align: -0.08333em;"></span><span class="mord mathdefault" style="margin-right: 0.22222em;">V</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.05764em;">E</span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         E 
        
       
         × 
        
       
         H 
        
       
      
        E \times H 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.76666em; vertical-align: -0.08333em;"></span><span class="mord mathdefault" style="margin-right: 0.05764em;">E</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.08125em;">H</span></span></span></span></span>的矩阵，计算某个单词的表示需用一个单词的one-hot向量乘以<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         V 
        
       
         × 
        
       
         E 
        
       
      
        V \times E 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.76666em; vertical-align: -0.08333em;"></span><span class="mord mathdefault" style="margin-right: 0.22222em;">V</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.05764em;">E</span></span></span></span></span>维的矩阵（也就是lookup），再用得到的结果乘 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         E 
        
       
         × 
        
       
         H 
        
       
      
        E \times H 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.76666em; vertical-align: -0.08333em;"></span><span class="mord mathdefault" style="margin-right: 0.05764em;">E</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.08125em;">H</span></span></span></span></span>维的矩阵即可。两个矩阵的参数通过模型学习。</p> 
<h4><a id="332_Crosslayer_parameter_sharing_155"></a>3.3.2 Cross-layer parameter sharing</h4> 
<p><img src="https://images2.imgbox.com/d0/23/dpePRn2Z_o.png" alt="层与层之间的距离"><br> 在交叉层中使用了参数共享操作，可以看到层与层之间的距离相似。在上图中ALBERT的层间距离是收敛的，不是震荡状态。</p> 
<h4><a id="333_Intersentence_coherence_loss_159"></a>3.3.3 Inter-sentence coherence loss.</h4> 
<p>在BERT模型中，NSP是一个binary分类，用来预测两段文字是否是连续的。然而，随后的研究（Yang等人，2019；Liu等人，2019）发现NSP的影响不可靠，并决定消除它，这一决定得到了几个任务下游任务绩效改进的支持。NSP不可靠的原因在于：对比于MLM任务，它是一项缺乏难度的任务。NSP包含了两种任务：<strong>主题预测、连贯性预测</strong>。然而主题预测比连贯性预测要容易，所以NSP是缺乏难度的任务。</p> 
<p>而ALBERT使用的sentence-order prediction（SOP），避免主题预测（topic prediction）并且专注于构建句子之间的连贯性。<strong>SOP loss使用了和BERT一样的技术来抽取positive examples（在同一个文档中抽取两个连续的句子），同时把这两个句子的顺序进行交换，用来作为negative 样本。</strong> SOP能够合理的是：一定程度上解决NSP任务，因为NSP大概率是基于没有对齐连续性线索</p> 
<h3><a id="34__164"></a>3.4 实验</h3> 
<p>使用了BOOKCORPUS (Zhu et al., 2015) and English Wikipedia数据用来作为原始测词典。限制了最大句子长度为512。同时也使用了<strong>MLM</strong>预测任务和<strong>SOP</strong>任务。</p> 
<h4><a id="341_BERTALBERT_167"></a>3.4.1 BERT和ALBERT的对比</h4> 
<p><img src="https://images2.imgbox.com/4f/70/33dKhPcK_o.png" alt="BERT和ALBERT的对比"></p> 
<ul><li>ALBERT-xxlarge的参数量比BERT-large参数量要少，同时它的效果也比BERT的好。</li><li>BERT-large和ALBERT-large使用了相同的层数，和相同的embedding大小。参数量确实是ALBERT的少，并且运行速度要更快。在效果上对比中BERT-large会比ALBERT-large要好</li></ul> 
<h4><a id="342__172"></a>3.4.2 交叉层参数共享实验</h4> 
<p><img src="https://images2.imgbox.com/5a/d5/Tb3Bqsys_o.png" alt="交叉层参数共享实验"><br> －如果全部的层都进行参数共享，参数量会变得很少，但模型的效果不会降低太多。</p> 
<h4><a id="342_SOP_176"></a>3.4.2 SOP</h4> 
<p><img src="https://images2.imgbox.com/75/85/UDb0gg8y_o.png" alt="SOP"></p> 
<ul><li>SOP在下游任务中表现很出色，比NSP要好。</li></ul> 
<h2><a id="4__180"></a>4. 参考</h2> 
<p>（1）BERT源码分析PART II：<a href="https://blog.csdn.net/Kaiyuan_sjtu/article/details/90288178">https://blog.csdn.net/Kaiyuan_sjtu/article/details/90288178</a></p> 
<p>（2）BERT论文：《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》<br> （3）ALBERT论文：《ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS》</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/7092fd015f88786cb51dc454db1d3fb4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">PostGIS实现空间数据转GeoJSON对象 （1）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/eb60f7fe21d2435c581ad7dd5b3cd849/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">zabbix安装及配置</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>