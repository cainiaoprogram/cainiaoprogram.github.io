<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>BERT、ALBERT、RoBerta、ERNIE模型对比和改进点总结 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="BERT、ALBERT、RoBerta、ERNIE模型对比和改进点总结" />
<meta property="og:description" content="1. BERT总结
MLM (Masked language model)NSP (Next Sentence Prediction) MLM ：
在一句话中随机选择 15% 的词汇用于预测。对于在原句中被抹去的词汇， 80% 情况下采用一个特殊符号 [MASK] 替换， 10% 情况下采用一个任意词替换，剩余 10% 情况下保持原词汇不变。这么做的主要原因是：在后续微调任务中语句中并不会出现 [MASK] 标记，而且这么做的另一个好处是：预测一个词汇时，模型并不知道输入对应位置的词汇是否为正确的词汇（ 10% 概率），这就迫使模型更多地依赖于上下文信息去预测词汇，并且赋予了模型一定的纠错能力。上述提到了这样做的一个缺点，其实这样做还有另外一个缺点，就是每批次数据中只有 15% 的标记被预测，这意味着模型可能需要更多的预训练步骤来收敛。
bert 缺点：
MLM直接对单个token进行随机mask，丢失了短语和实体信息，这一点对中文尤其明显，百度ERNIE对其进行了改进MLM仅预测被mask的token，其他token没有参与到预测中。而GPT等语言模型可以对每个token进行预测。导致MLM训练效率偏低，训练时间过长。这一点BERT论文自己也提到了，ELECTRA围绕这一点进行了改进。NSP任务可以学到sequence level信息，但仅为一个二分类，且负样本构造过于简单，导致模型不能充分训练。之前BERT的消融分析中也看到了，NSP对下游任务的作用比MLM要小。SpanBERT、ALBERT、Roberta均提到了这一点，并进行了相关改进，或者干脆弃用NSP。 2. ALBERT
嵌入层矩阵分解 跨层参数共享SOP(Sentence Order Prediciton) 正例与NSP相同，但负例是通过选择一篇文档中的两个连续的句子并将它们的顺序交换构造的。这样两个句子就会有相同的话题，模型学习到的就更多是句子间的连贯性。用于句子级别的预测（SOP）。SOP 主要聚焦于句间连贯，用于解决原版 BERT 中下一句预测（NSP）损失低效的问题。
3. RoBERT
4. ERNIE
增加 短语 和 实体 级别的 mask 方式， 融合外部知识添加优质中文语料DLM, Dialog 语料 总结：
模型总结改进点BERT中文以字为单位表示的通过两个训练任务采用多层transformer的encoder部分（核心是多头attention）的预训练模型比word2vec更能够动态得到词语在不同语境下的向量表示，能够解决word2vec上下文无关问题ALbert精简版的BERT，参数量减小了，克服扩展模型困难的问题1、因式分解:将词嵌入层先映射到一个低维空间，再映射到隐藏层 2、跨层参数共享 3、SOP：提升性能Roberta更为精细的bert调优版本，采用更大数据，更大batch_size，动态mask（中文采用全词mask），去掉NSP，采用全句子策略训练1、改进优化函数，更大的模型参数量
2、动态mask（全词mask）
3、去掉下一句预测任务
4、采用更大的batchERINIE融合了更多知识，mask的时候采用短语和实体级别的mask，添加了更多优质语料融合了更多知识，mask的时候采用短语和实体级别的mask，添加了更多优质语料 参考：
BERT、ALBERT、RoBerta、ERNIE模型对比和改进点总结 - 知乎" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/ee958890e8651ef219803e1bc03111cf/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-08-01T14:29:00+08:00" />
<meta property="article:modified_time" content="2022-08-01T14:29:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">BERT、ALBERT、RoBerta、ERNIE模型对比和改进点总结</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>1. BERT总结</p> 
<ul><li>MLM (Masked language model)</li><li>NSP (Next Sentence Prediction)</li></ul> 
<p>MLM ：</p> 
<blockquote> 
 <p> 在一句话中随机选择 15% 的词汇用于预测。对于在原句中被抹去的词汇， 80% 情况下采用一个特殊符号 [MASK] 替换， 10% 情况下采用一个任意词替换，剩余 10% 情况下保持原词汇不变。这么做的主要原因是：在后续微调任务中语句中并不会出现 [MASK] 标记，而且这么做的另一个好处是：预测一个词汇时，模型并不知道输入对应位置的词汇是否为正确的词汇（ 10% 概率），这就迫使模型更多地依赖于上下文信息去预测词汇，并且赋予了模型一定的纠错能力。上述提到了这样做的一个缺点，其实这样做还有另外一个缺点，就是每批次数据中只有 15% 的标记被预测，这意味着模型可能需要更多的预训练步骤来收敛。</p> 
</blockquote> 
<p></p> 
<p>bert 缺点：</p> 
<blockquote> 
 <ol><li>MLM直接对单个token进行随机mask，丢失了短语和实体信息，这一点对中文尤其明显，百度ERNIE对其进行了改进</li><li>MLM仅预测被mask的token，其他token没有参与到预测中。而GPT等语言模型可以对每个token进行预测。导致MLM训练效率偏低，训练时间过长。这一点BERT论文自己也提到了，ELECTRA围绕这一点进行了改进。</li><li>NSP任务可以学到sequence level信息，但仅为一个二分类，且负样本构造过于简单，导致模型不能充分训练。之前BERT的消融分析中也看到了，NSP对下游任务的作用比MLM要小。SpanBERT、ALBERT、Roberta均提到了这一点，并进行了相关改进，或者干脆弃用NSP。</li></ol> 
</blockquote> 
<p>2. ALBERT</p> 
<ul><li>嵌入层矩阵分解 <img alt="O(V\times H) -&gt; O(V\times E + E\times H)" class="mathcode" src="https://images2.imgbox.com/a0/63/29gTzkx3_o.png"></li><li>跨层参数共享</li><li>SOP(Sentence Order Prediciton) 
  <blockquote> 
   <p>正例与NSP相同，但负例是通过选择一篇文档中的两个连续的句子并将它们的顺序交换构造的。这样两个句子就会有相同的话题，模型学习到的就更多是句子间的连贯性。用于句子级别的预测（SOP）。SOP 主要聚焦于句间连贯，用于解决原版 BERT 中下一句预测（NSP）损失低效的问题。</p> 
  </blockquote> </li></ul> 
<p>3. RoBERT</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/4a/e0/bv46QaCY_o.jpg"></p> 
<p>4. ERNIE</p> 
<ul><li>增加 短语 和 实体 级别的 mask 方式， 融合外部知识</li><li>添加优质中文语料</li><li>DLM, Dialog 语料</li></ul> 
<p></p> 
<p>总结：</p> 
<table><tbody><tr><th>模型</th><th>总结</th><th>改进点</th></tr><tr><td>BERT</td><td>中文以字为单位表示的通过两个训练任务采用多层transformer的encoder部分（核心是多头attention）的预训练模型</td><td>比word2vec更能够动态得到词语在不同语境下的向量表示，能够解决word2vec上下文无关问题</td></tr><tr><td>ALbert</td><td>精简版的BERT，参数量减小了，克服扩展模型困难的问题</td><td>1、因式分解:将词嵌入层先映射到一个低维空间，再映射到隐藏层 2、跨层参数共享 3、SOP：提升性能</td></tr><tr><td>Roberta</td><td>更为精细的bert调优版本，采用更大数据，更大batch_size，动态mask（中文采用全词mask），去掉NSP，采用全句子策略训练</td><td>1、改进优化函数，更大的模型参数量<br> 2、动态mask（全词mask）<br> 3、去掉下一句预测任务<br> 4、采用更大的batch</td></tr><tr><td>ERINIE</td><td>融合了更多知识，mask的时候采用短语和实体级别的mask，添加了更多优质语料</td><td>融合了更多知识，mask的时候采用短语和实体级别的mask，添加了更多优质语料</td></tr></tbody></table> 
<p></p> 
<p></p> 
<p>参考：</p> 
<p><a href="https://zhuanlan.zhihu.com/p/347846720" rel="nofollow" title="BERT、ALBERT、RoBerta、ERNIE模型对比和改进点总结 - 知乎">BERT、ALBERT、RoBerta、ERNIE模型对比和改进点总结 - 知乎</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/9bb5cdfc5ede2c857e3ee7b0033f007e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">这个用PHP开发的全开源商城系统可免费商用</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c7a17fcf615f883253cea0938376c863/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">MachineLearning 11.  机器学习之随机森林生存分析（randomForestSRC）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>