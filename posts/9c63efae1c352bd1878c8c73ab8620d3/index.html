<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>PyTorch实现前馈神经网络（torch.nn） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="PyTorch实现前馈神经网络（torch.nn）" />
<meta property="og:description" content="PyTorch实现前馈神经网络（torch.nn) 1 回归任务 1.1 导入所需要的包1.2 自定义数据集1.3 构造数据迭代器1.4 模型构建1.5 参数初始化1.6 损失函数和优化器1.7 训练 1.7.1 定义训练函数1.7.2 开始训练模型1.7.3 绘制loss曲线2 二分类任务 2.1 导入所需要的包2.2 自定义数据集2.3 构造数据迭代器2.4 模型构建2.5 参数初始化2.6 损失函数和优化器2.7 训练 2.7.1 定义训练函数2.7.2 开始训练模型2.7.3 绘制loss曲线3 多分类任务 3.1 导入包3.2 下载MNIST数据集3.3 定义数据迭代器3.4 模型构建3.5 参数初始化3.6 损失函数和优化器3.7 定义准确率检验器3.8 训练 3.8.1 定义训练函数3.8.2 开始训练模型3.8.3 绘制loss、acc曲线 回归任务 该回归问题为高维回归问题，涉及的自变量过多，因此不适合用之前的线性回归模型，因为模型过于简单，导致训练的效率太低。因此我们考虑利用前馈神经网络，增加隐藏层，并且为了避免梯度消失问题，在隐藏层采用relu激活函数。
导入所需要的包 import torch from torch import nn from torch.nn import init import numpy as np from IPython import display import torch.utils.data as Data 自定义数据集 num_inputs = 500 num_examples = 10000 # true_w = torch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/9c63efae1c352bd1878c8c73ab8620d3/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-06T11:16:14+08:00" />
<meta property="article:modified_time" content="2023-04-06T11:16:14+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">PyTorch实现前馈神经网络（torch.nn）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2>PyTorch实现前馈神经网络（torch.nn)<span class="tocSkip"></span></h2> 
<div class="toc"> 
 <ul class="toc-item"><li><a href="#%E5%9B%9E%E5%BD%92%E4%BB%BB%E5%8A%A1" rel="nofollow"><span class="toc-item-num">1  </span>回归任务</a> 
   <ul class="toc-item"><li><a href="#%E5%AF%BC%E5%85%A5%E6%89%80%E9%9C%80%E8%A6%81%E7%9A%84%E5%8C%85" rel="nofollow"><span class="toc-item-num">1.1  </span>导入所需要的包</a></li><li><a href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E9%9B%86" rel="nofollow"><span class="toc-item-num">1.2  </span>自定义数据集</a></li><li><a href="#%E6%9E%84%E9%80%A0%E6%95%B0%E6%8D%AE%E8%BF%AD%E4%BB%A3%E5%99%A8" rel="nofollow"><span class="toc-item-num">1.3  </span>构造数据迭代器</a></li><li><a href="#%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA" rel="nofollow"><span class="toc-item-num">1.4  </span>模型构建</a></li><li><a href="#%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96" rel="nofollow"><span class="toc-item-num">1.5  </span>参数初始化</a></li><li><a href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E4%BC%98%E5%8C%96%E5%99%A8" rel="nofollow"><span class="toc-item-num">1.6  </span>损失函数和优化器</a></li><li><a href="#%E8%AE%AD%E7%BB%83" rel="nofollow"><span class="toc-item-num">1.7  </span>训练</a> 
     <ul class="toc-item"><li><a href="#%E5%AE%9A%E4%B9%89%E8%AE%AD%E7%BB%83%E5%87%BD%E6%95%B0" rel="nofollow"><span class="toc-item-num">1.7.1  </span>定义训练函数</a></li><li><a href="#%E5%BC%80%E5%A7%8B%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B" rel="nofollow"><span class="toc-item-num">1.7.2  </span>开始训练模型</a></li><li><a href="#%E7%BB%98%E5%88%B6loss%E6%9B%B2%E7%BA%BF" rel="nofollow"><span class="toc-item-num">1.7.3  </span>绘制loss曲线</a></li></ul></li></ul></li><li><a href="#%E4%BA%8C%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1" rel="nofollow"><span class="toc-item-num">2  </span>二分类任务</a> 
   <ul class="toc-item"><li><a href="#%E5%AF%BC%E5%85%A5%E6%89%80%E9%9C%80%E8%A6%81%E7%9A%84%E5%8C%85" rel="nofollow"><span class="toc-item-num">2.1  </span>导入所需要的包</a></li><li><a href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E9%9B%86" rel="nofollow"><span class="toc-item-num">2.2  </span>自定义数据集</a></li><li><a href="#%E6%9E%84%E9%80%A0%E6%95%B0%E6%8D%AE%E8%BF%AD%E4%BB%A3%E5%99%A8" rel="nofollow"><span class="toc-item-num">2.3  </span>构造数据迭代器</a></li><li><a href="#%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA" rel="nofollow"><span class="toc-item-num">2.4  </span>模型构建</a></li><li><a href="#%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96" rel="nofollow"><span class="toc-item-num">2.5  </span>参数初始化</a></li><li><a href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E4%BC%98%E5%8C%96%E5%99%A8" rel="nofollow"><span class="toc-item-num">2.6  </span>损失函数和优化器</a></li><li><a href="#%E8%AE%AD%E7%BB%83" rel="nofollow"><span class="toc-item-num">2.7  </span>训练</a> 
     <ul class="toc-item"><li><a href="#%E5%AE%9A%E4%B9%89%E8%AE%AD%E7%BB%83%E5%87%BD%E6%95%B0" rel="nofollow"><span class="toc-item-num">2.7.1  </span>定义训练函数</a></li><li><a href="#%E5%BC%80%E5%A7%8B%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B" rel="nofollow"><span class="toc-item-num">2.7.2  </span>开始训练模型</a></li><li><a href="#%E7%BB%98%E5%88%B6loss%E6%9B%B2%E7%BA%BF" rel="nofollow"><span class="toc-item-num">2.7.3  </span>绘制loss曲线</a></li></ul></li></ul></li><li><a href="#%E5%A4%9A%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1" rel="nofollow"><span class="toc-item-num">3  </span>多分类任务</a> 
   <ul class="toc-item"><li><a href="#%E5%AF%BC%E5%85%A5%E5%8C%85" rel="nofollow"><span class="toc-item-num">3.1  </span>导入包</a></li><li><a href="#%E4%B8%8B%E8%BD%BDMNIST%E6%95%B0%E6%8D%AE%E9%9B%86" rel="nofollow"><span class="toc-item-num">3.2  </span>下载MNIST数据集</a></li><li><a href="#%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E8%BF%AD%E4%BB%A3%E5%99%A8" rel="nofollow"><span class="toc-item-num">3.3  </span>定义数据迭代器</a></li><li><a href="#%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA" rel="nofollow"><span class="toc-item-num">3.4  </span>模型构建</a></li><li><a href="#%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96" rel="nofollow"><span class="toc-item-num">3.5  </span>参数初始化</a></li><li><a href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E4%BC%98%E5%8C%96%E5%99%A8" rel="nofollow"><span class="toc-item-num">3.6  </span>损失函数和优化器</a></li><li><a href="#%E5%AE%9A%E4%B9%89%E5%87%86%E7%A1%AE%E7%8E%87%E6%A3%80%E9%AA%8C%E5%99%A8" rel="nofollow"><span class="toc-item-num">3.7  </span>定义准确率检验器</a></li><li><a href="#%E8%AE%AD%E7%BB%83" rel="nofollow"><span class="toc-item-num">3.8  </span>训练</a> 
     <ul class="toc-item"><li><a href="#%E5%AE%9A%E4%B9%89%E8%AE%AD%E7%BB%83%E5%87%BD%E6%95%B0" rel="nofollow"><span class="toc-item-num">3.8.1  </span>定义训练函数</a></li><li><a href="#%E5%BC%80%E5%A7%8B%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B" rel="nofollow"><span class="toc-item-num">3.8.2  </span>开始训练模型</a></li><li><a href="#%E7%BB%98%E5%88%B6loss%E3%80%81acc%E6%9B%B2%E7%BA%BF" rel="nofollow"><span class="toc-item-num">3.8.3  </span>绘制loss、acc曲线</a></li></ul></li></ul></li></ul> 
</div> 
<h2><a id="_3"></a>回归任务</h2> 
<p>该回归问题为高维回归问题，涉及的自变量过多，因此不适合用之前的线性回归模型，因为模型过于简单，导致训练的效率太低。因此我们考虑利用前馈神经网络，增加隐藏层，并且为了避免梯度消失问题，在隐藏层采用relu激活函数。</p> 
<h3><a id="_6"></a>导入所需要的包</h3> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> init
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> IPython <span class="token keyword">import</span> display
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">as</span> Data
</code></pre> 
<h3><a id="_18"></a>自定义数据集</h3> 
<pre><code class="prism language-python">num_inputs <span class="token operator">=</span> <span class="token number">500</span>
num_examples <span class="token operator">=</span> <span class="token number">10000</span>
<span class="token comment">#</span>
true_w <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">500</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">0.0056</span>
true_b <span class="token operator">=</span> <span class="token number">0.028</span>
<span class="token comment">#随机生成的数据样本</span>
features <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>num_examples<span class="token punctuation">,</span> num_inputs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token comment">#行*列=10000*500</span>
labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>features<span class="token punctuation">,</span>true_w<span class="token punctuation">)</span> <span class="token operator">+</span> true_b
labels <span class="token operator">+=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.01</span><span class="token punctuation">,</span> size<span class="token operator">=</span>labels<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span> <span class="token comment">#扰动项</span>
<span class="token comment">#训练集和测试集上的样本&amp;标签数----真实的特征和样本</span>
trainfeatures <span class="token operator">=</span> features<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">7000</span><span class="token punctuation">]</span>
trainlabels <span class="token operator">=</span> labels<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">7000</span><span class="token punctuation">]</span>
testfeatures <span class="token operator">=</span> features<span class="token punctuation">[</span><span class="token number">7000</span><span class="token punctuation">:</span><span class="token punctuation">]</span>  
testlabels <span class="token operator">=</span> labels<span class="token punctuation">[</span><span class="token number">7000</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>trainfeatures<span class="token punctuation">.</span>shape<span class="token punctuation">,</span>trainlabels<span class="token punctuation">.</span>shape<span class="token punctuation">,</span>testfeatures<span class="token punctuation">.</span>shape<span class="token punctuation">,</span>testlabels<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
</code></pre> 
<pre><code>torch.Size([7000, 500]) torch.Size([7000, 1]) torch.Size([3000, 500]) torch.Size([3000, 1])
</code></pre> 
<h3><a id="_42"></a>构造数据迭代器</h3> 
<p>采用<code>torch.utils.data.DataLoader</code>读取小批量数据，分别定义关于训练集数据和测试集数据的迭代器iterate</p> 
<ul><li><code>batch_size</code>是超参数，表示一轮放入多少个样本进行训练</li><li><code>shuffle</code>是否打乱数据，True表示打乱数据</li><li><code>num_workers=0</code>表示不开启多线程读取数据</li></ul> 
<p>注：利用Data.TensorDataset获取数据集，Data.DataLoader构建数据迭代器，从而实现数据的批量读取。</p> 
<pre><code class="prism language-python"><span class="token comment">#获得数据迭代器</span>
batch_size <span class="token operator">=</span> <span class="token number">50</span> <span class="token comment"># 设置小批量大小</span>
<span class="token keyword">def</span> <span class="token function">load_array</span><span class="token punctuation">(</span>data_arrays<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> is_train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#自定义函数</span>
    <span class="token triple-quoted-string string">"""构造一个PyTorch数据迭代器。"""</span>
    dataset <span class="token operator">=</span> Data<span class="token punctuation">.</span>TensorDataset<span class="token punctuation">(</span><span class="token operator">*</span>data_arrays<span class="token punctuation">)</span><span class="token comment">#features 和 labels作为list传入，得到PyTorch的一个数据集</span>
    <span class="token keyword">return</span> Data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span>is_train<span class="token punctuation">,</span>num_workers<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token comment">#返回的是实例化后的DataLoader</span>
train_iter <span class="token operator">=</span> load_array<span class="token punctuation">(</span><span class="token punctuation">[</span>trainfeatures<span class="token punctuation">,</span>trainlabels<span class="token punctuation">]</span><span class="token punctuation">,</span>batch_size<span class="token punctuation">)</span>
test_iter <span class="token operator">=</span> load_array<span class="token punctuation">(</span><span class="token punctuation">[</span>testfeatures<span class="token punctuation">,</span>testlabels<span class="token punctuation">]</span><span class="token punctuation">,</span>batch_size<span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python"><span class="token comment"># 测试:利用python内置函数next，从迭代器中获取第一项。</span>
<span class="token comment"># next(iter(train_iter))</span>
</code></pre> 
<pre><code>[tensor([[ 0.1302, -0.7244,  0.8331,  ..., -0.0395, -1.7288,  0.5882],
         [ 0.3992,  0.6459,  0.5747,  ...,  2.7026, -0.2066,  0.0475],
         [ 1.1758,  0.8725,  0.0554,  ...,  1.3374,  0.4322, -0.1942],
         ...,
         [-1.3619,  1.0502, -1.2361,  ..., -1.2441, -1.1378, -0.1940],
         [ 0.7635,  2.2856, -0.1588,  ...,  0.4710,  1.3731,  0.6452],
         [ 0.1177, -0.8012, -0.1541,  ...,  0.5184,  0.1925, -0.1466]]),
 tensor([[ 0.1838],
         [ 0.0256],
         [ 0.1098],
  			...
         [-0.0794],
         [ 0.1535],
         [-0.0316],
         [-0.1934],
         [ 0.0309],
         [ 0.1449],
         [-0.3216],
         [ 0.0894],
         [-0.0281]])]
</code></pre> 
<h3><a id="_94"></a>模型构建</h3> 
<p>其实，<code>PyTorch</code>提供了⼤量预定义的层，这使我们只需关注使⽤哪些层来构造模型。<br> ⾸先，导⼊ <code>torch.nn</code> 模块。实际上，“nn”是neural networks（神经⽹络）的缩写。顾名思义，该模块定义了⼤量神经⽹的层。之前我们已经用过了<code>autograd</code> ，⽽ nn 就是利⽤ <code>autograd</code> 来定义模型。 nn 的核⼼数据结构是 <code>Module</code> ，它是⼀个抽象概念，既可以表示神经⽹络中的某个层（layer），也可以表示⼀个包含很多层的神经⽹络。</p> 
<ul><li>最常⻅的做法是继承 <code>nn.Module</code> ，撰写⾃⼰的⽹络/层。⼀个 <code>nn.Module</code> 实例应该包含⼀些层以及返回输出的前向传播（forward）⽅法。</li><li>事实上我们还可以⽤ <code>nn.Sequential</code> 来更加⽅便地搭建⽹络， <code>Sequential</code> 是⼀个有序的容器，⽹络层将按照在传⼊ <code>Sequential</code> 的顺序依次被添加到计算图中。</li></ul> 
<p>此模型较简单，因此使用<code>nn.Sequential</code>可以更加简洁方便构建前馈神经网络。</p> 
<pre><code class="prism language-python"><span class="token comment">#实现FlattenLayer层：将数据展平</span>
<span class="token keyword">class</span> <span class="token class-name">FlattenLayer</span> <span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">_init_</span> <span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>FlattenLayer<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>_init_<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span> <span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    
<span class="token comment">#模型定义和参数初始化</span>
<span class="token comment"># num_inputs = 500</span>
num_hiddens <span class="token operator">=</span> <span class="token number">256</span>
num_outputs <span class="token operator">=</span> <span class="token number">1</span>
net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
        FlattenLayer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token comment">#输入层</span>
        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span>num_hiddens<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token comment">#隐藏层</span>
        nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token comment">#隐藏层激活函数Relu</span>
        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span>num_outputs<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token comment">#输出层</span>
        <span class="token punctuation">)</span>
</code></pre> 
<h3><a id="_123"></a>参数初始化</h3> 
<p>由print输出的结果可知，param分别为[W1,b1,W2,b2]</p> 
<ul><li>W1.shape =num_hiddens * num_inputs</li><li>W2.shape =num_outputs * num_hiddens<br> 因此，X与W做矩阵乘法时，需要对W进行转置。</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">for</span> param <span class="token keyword">in</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span> <span class="token punctuation">(</span>param<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
    init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>param<span class="token punctuation">,</span>mean<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>std<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>
net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code>torch.Size([256, 500])
torch.Size([256])
torch.Size([1, 256])
torch.Size([1])
&lt;generator object Module.parameters at 0x000001EBC0702270&gt;
</code></pre> 
<h3><a id="_145"></a>损失函数和优化器</h3> 
<p>回归问题决定了使用最小化均方误差，对于优化器，使用torch模块中的小批量梯度下降。</p> 
<pre><code class="prism language-python">lr<span class="token operator">=</span><span class="token number">0.01</span>
loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="_155"></a>训练</h3> 
<h4><a id="_156"></a>定义训练函数</h4> 
<p>对于每一轮次的训练;</p> 
<ul><li>step1:在训练集上，进行小批量梯度下降更新参数</li><li>step2 每经过一个轮次的训练， 记录训练集和测试集上的loss</li></ul> 
<pre><code class="prism language-python"><span class="token comment">#记录列表（list），存储训练集和测试集上经过每一轮次，loss的变化</span>
<span class="token keyword">def</span> <span class="token function">train1</span> <span class="token punctuation">(</span>net<span class="token punctuation">,</span>train_iter<span class="token punctuation">,</span>test_iter<span class="token punctuation">,</span>loss<span class="token punctuation">,</span>num_epochs<span class="token punctuation">,</span>batch_size<span class="token punctuation">,</span>params <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>lr<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>optimizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    train_loss<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>
    test_loss<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment">#外循环控制循环轮次</span>
        <span class="token comment">#step1在训练集上，进行小批量梯度下降更新参数</span>
        <span class="token keyword">for</span> X<span class="token punctuation">,</span>y <span class="token keyword">in</span> train_iter<span class="token punctuation">:</span><span class="token comment">#内循环控制训练批次</span>
            y_hat <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
            l <span class="token operator">=</span> loss<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span>y<span class="token punctuation">)</span><span class="token comment">#l.size = torch.Size([]),即说明loss为表示*标量*的tensor`</span>
            <span class="token comment">#梯度清零</span>
            <span class="token keyword">if</span> optimizer <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">elif</span> params <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">and</span> params<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>grad <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                <span class="token keyword">for</span> param <span class="token keyword">in</span> params<span class="token punctuation">:</span>
                    param<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>
            l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> optimizer <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                SGD<span class="token punctuation">(</span>params<span class="token punctuation">,</span>lr<span class="token punctuation">,</span>batch_size<span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment">#step2 每经过一个轮次的训练， 记录训练集和测试集上的loss</span>
        train_labels <span class="token operator">=</span> trainlabels<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>  
        test_labels <span class="token operator">=</span> testlabels<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span> 
        train_loss<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span>loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>trainfeatures<span class="token punctuation">)</span><span class="token punctuation">,</span>train_labels<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">#loss本身就默认了取平均值！</span>
        test_loss<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span>loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>testfeatures<span class="token punctuation">)</span><span class="token punctuation">,</span>test_labels<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"epoch %d,train_loss %.6f,test_loss %.6f"</span><span class="token operator">%</span><span class="token punctuation">(</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span>train_loss<span class="token punctuation">[</span>epoch<span class="token punctuation">]</span><span class="token punctuation">,</span>test_loss<span class="token punctuation">[</span>epoch<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> 
    <span class="token keyword">return</span> train_loss<span class="token punctuation">,</span> test_loss
</code></pre> 
<h4><a id="_192"></a>开始训练模型</h4> 
<pre><code class="prism language-python">lr<span class="token operator">=</span><span class="token number">0.01</span>
num_epochs <span class="token operator">=</span> <span class="token number">50</span>
<span class="token comment">#batch_size、params epc已经定义</span>
train_loss<span class="token punctuation">,</span> test_loss <span class="token operator">=</span> train1 <span class="token punctuation">(</span>net<span class="token punctuation">,</span>train_iter<span class="token punctuation">,</span>test_iter<span class="token punctuation">,</span>loss<span class="token punctuation">,</span>num_epochs<span class="token punctuation">,</span>batch_size<span class="token punctuation">,</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr<span class="token punctuation">,</span>optimizer<span class="token punctuation">)</span><span class="token comment">#每一给optimizer,默认None</span>
</code></pre> 
<pre><code>epoch 1,train_loss 0.015031,test_loss 0.015377
epoch 2,train_loss 0.013855,test_loss 0.014311
epoch 3,train_loss 0.012677,test_loss 0.013246
epoch 4,train_loss 0.011430,test_loss 0.012137
epoch 5,train_loss 0.010103,test_loss 0.010897
epoch 6,train_loss 0.008683,test_loss 0.009595
...
epoch 48,train_loss 0.000411,test_loss 0.001012
epoch 49,train_loss 0.000404,test_loss 0.001008
epoch 50,train_loss 0.000396,test_loss 0.001003
</code></pre> 
<h4><a id="loss_214"></a>绘制loss曲线</h4> 
<pre><code class="prism language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
x<span class="token operator">=</span>np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token builtin">len</span><span class="token punctuation">(</span>train_loss<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token builtin">len</span><span class="token punctuation">(</span>train_loss<span class="token punctuation">)</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span>train_loss<span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">"train_loss"</span><span class="token punctuation">,</span>linewidth<span class="token operator">=</span><span class="token number">1.5</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span>test_loss<span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">"test_loss"</span><span class="token punctuation">,</span>linewidth<span class="token operator">=</span><span class="token number">1.5</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">"epoch"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"loss"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>​<br> <img src="https://images2.imgbox.com/ea/72/t0kK85be_o.png" alt="在这里插入图片描述"></p> 
<p>​</p> 
<h2><a id="_235"></a>二分类任务</h2> 
<pre><code class="prism language-python"><span class="token comment">#在执行一个新的任务之前，先将之前的中间变量的结果全部清空，即之前的编译不作数</span>
<span class="token operator">%</span>reset
</code></pre> 
<pre><code>Once deleted, variables cannot be recovered. Proceed (y/[n])? y
</code></pre> 
<h3><a id="_246"></a>导入所需要的包</h3> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> random
<span class="token keyword">from</span> IPython <span class="token keyword">import</span> display
<span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot <span class="token keyword">as</span> plt
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">as</span> Data
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> init
</code></pre> 
<h3><a id="_261"></a>自定义数据集</h3> 
<p>共生成两个数据集。</p> 
<ul><li>两个数据集的大小均为10000且训练集大小为7000，测试集大小为3000。</li><li>两个数据集的样本特征x的维度均为200，且分别服从均值互为相反数且方差相同的正态分布。</li><li>两个数据集的样本标签分别为0和1。</li></ul> 
<p>然后利用torch.cat()操作将两类的训练集和测试集分别合并，从而训练集的大小为14000，测试集的大小为6000。</p> 
<pre><code class="prism language-python">num_inputs <span class="token operator">=</span> <span class="token number">200</span>
<span class="token comment">#1类</span>
x1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">,</span> num_inputs<span class="token punctuation">)</span><span class="token punctuation">)</span>
y1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># 标签1 </span>
x1_train <span class="token operator">=</span> x1<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">7000</span><span class="token punctuation">]</span>
x1_test <span class="token operator">=</span> x1<span class="token punctuation">[</span><span class="token number">7000</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
<span class="token comment">#0类</span>
x2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">,</span> num_inputs<span class="token punctuation">)</span><span class="token punctuation">)</span>
y2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># 标签0</span>
x2_train <span class="token operator">=</span> x2<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">7000</span><span class="token punctuation">]</span>
x2_test <span class="token operator">=</span> x2<span class="token punctuation">[</span><span class="token number">7000</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
<span class="token comment">#合并训练集</span>
trainfeatures <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>x1_train<span class="token punctuation">,</span>x2_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">)</span>
trainlabels <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>y1<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">7000</span><span class="token punctuation">]</span><span class="token punctuation">,</span> y2<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">7000</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">)</span>
<span class="token comment">#合并测试集</span>
testfeatures <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>x1_test<span class="token punctuation">,</span>x2_test<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">)</span>
testlabels <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>y1<span class="token punctuation">[</span><span class="token number">7000</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> y2<span class="token punctuation">[</span><span class="token number">7000</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>trainfeatures<span class="token punctuation">.</span>shape<span class="token punctuation">,</span>trainlabels<span class="token punctuation">.</span>shape<span class="token punctuation">,</span>testfeatures<span class="token punctuation">.</span>shape<span class="token punctuation">,</span>testlabels<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
</code></pre> 
<pre><code>torch.Size([14000, 200]) torch.Size([14000, 1]) torch.Size([6000, 200]) torch.Size([6000, 1])
</code></pre> 
<h3><a id="_294"></a>构造数据迭代器</h3> 
<pre><code class="prism language-python">batch_size <span class="token operator">=</span> <span class="token number">50</span>
<span class="token comment"># 将训练数据的特征和标签组合</span>
dataset <span class="token operator">=</span> Data<span class="token punctuation">.</span>TensorDataset<span class="token punctuation">(</span>trainfeatures<span class="token punctuation">,</span> trainlabels<span class="token punctuation">)</span>
<span class="token comment"># 把 dataset 放入 DataLoader</span>
train_iter <span class="token operator">=</span> Data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>
    dataset<span class="token operator">=</span>dataset<span class="token punctuation">,</span> <span class="token comment"># torch TensorDataset format</span>
    batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span> <span class="token comment"># mini batch size</span>
    shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token comment"># 是否打乱数据 (训练集一般需要进行打乱)</span>
    num_workers<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token comment"># 多线程来读数据， 注意在Windows下需要设置为0</span>
<span class="token punctuation">)</span>
<span class="token comment"># 将测试数据的特征和标签组合</span>
dataset <span class="token operator">=</span> Data<span class="token punctuation">.</span>TensorDataset<span class="token punctuation">(</span>testfeatures<span class="token punctuation">,</span> testlabels<span class="token punctuation">)</span>
<span class="token comment"># 把 dataset 放入 DataLoader</span>
test_iter <span class="token operator">=</span> Data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>
    dataset<span class="token operator">=</span>dataset<span class="token punctuation">,</span> <span class="token comment"># torch TensorDataset format</span>
    batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span> <span class="token comment"># mini batch size</span>
    shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token comment"># 是否打乱数据 (训练集一般需要进行打乱)</span>
    num_workers<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token comment"># 多线程来读数据， 注意在Windows下需要设置为0</span>
<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="_319"></a>模型构建</h3> 
<pre><code class="prism language-python"><span class="token comment">#实现FlattenLayer层</span>
<span class="token comment">#完成将数据集展平的操作，保证一个样本的数据变成一个数组</span>
<span class="token keyword">class</span> <span class="token class-name">FlattenLayer</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
         <span class="token builtin">super</span><span class="token punctuation">(</span>FlattenLayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token comment"># #模型构建</span>
num_hiddens<span class="token punctuation">,</span>num_outputs <span class="token operator">=</span> <span class="token number">256</span><span class="token punctuation">,</span><span class="token number">1</span>
net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
        FlattenLayer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span>num_hiddens<span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span>num_outputs<span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
</code></pre> 
<h3><a id="_340"></a>参数初始化</h3> 
<pre><code class="prism language-python"><span class="token keyword">for</span> params <span class="token keyword">in</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>params<span class="token punctuation">,</span>mean<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>std<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="_348"></a>损失函数和优化器</h3> 
<pre><code class="prism language-python"><span class="token comment"># 定义二分类交叉熵损失函数</span>
loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>BCEWithLogitsLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 定义sgd优化器</span>
optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="_358"></a>训练</h3> 
<h4><a id="_359"></a>定义训练函数</h4> 
<pre><code class="prism language-python"><span class="token comment">#定义模型训练函数</span>
<span class="token keyword">def</span> <span class="token function">train2</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span>train_iter<span class="token punctuation">,</span>test_iter<span class="token punctuation">,</span>loss<span class="token punctuation">,</span>num_epochs<span class="token punctuation">,</span>batch_size<span class="token punctuation">,</span>params<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>lr<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>optimizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    train_ls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    test_ls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># 训练模型一共需要num_epochs个迭代周期</span>
        train_l_sum<span class="token punctuation">,</span> train_acc_num<span class="token punctuation">,</span>n <span class="token operator">=</span> <span class="token number">0.0</span><span class="token punctuation">,</span><span class="token number">0.0</span><span class="token punctuation">,</span><span class="token number">0</span>
        <span class="token comment"># 在每一个迭代周期中，会使用训练数据集中所有样本一次</span>
        <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> train_iter<span class="token punctuation">:</span> <span class="token comment"># x和y分别是小批量样本的特征和标签</span>
            y_hat <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
            l <span class="token operator">=</span> loss<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># l是有关小批量X和y的损失</span>
            <span class="token comment">#梯度清零</span>
            <span class="token keyword">if</span> optimizer <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">elif</span> params <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">and</span> params<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>grad <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                <span class="token keyword">for</span> param <span class="token keyword">in</span> params<span class="token punctuation">:</span>
                    param<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>
            l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 小批量的损失对模型参数求梯度</span>
            <span class="token keyword">if</span> optimizer <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                SGD<span class="token punctuation">(</span>params<span class="token punctuation">,</span>lr<span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token comment">#计算每个epoch的loss</span>
            train_l_sum <span class="token operator">+=</span> l<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">*</span>y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
            <span class="token comment">#每一个epoch的所有样本数</span>
            n<span class="token operator">+=</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        train_labels <span class="token operator">=</span> trainlabels<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
        test_labels <span class="token operator">=</span> testlabels<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
        train_ls<span class="token punctuation">.</span>append<span class="token punctuation">(</span>train_l_sum<span class="token operator">/</span>n<span class="token punctuation">)</span>
        test_ls<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>testfeatures<span class="token punctuation">)</span><span class="token punctuation">,</span>test_labels<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"epoch %d,train_loss %.6f,test_loss %.6f"</span><span class="token operator">%</span><span class="token punctuation">(</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span>train_loss<span class="token punctuation">[</span>epoch<span class="token punctuation">]</span><span class="token punctuation">,</span>test_loss<span class="token punctuation">[</span>epoch<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> 
    <span class="token keyword">return</span> train_ls<span class="token punctuation">,</span>test_ls
</code></pre> 
<h4><a id="_396"></a>开始训练模型</h4> 
<p><strong>注</strong>：为什么重复运行train函数时，结果不一样，往往是第一次的结果较为合理。因为运行完一次train中，params已经更新，第二次运行时，计算得到的误差就会非常小！<br> 因此，每次执行train之前，务必重新进行一次参数的初始化。</p> 
<pre><code class="prism language-python">lr <span class="token operator">=</span> <span class="token number">0.0001</span>
num_epochs <span class="token operator">=</span> <span class="token number">50</span>
train_loss<span class="token punctuation">,</span>test_loss <span class="token operator">=</span> train2<span class="token punctuation">(</span>net<span class="token punctuation">,</span>train_iter<span class="token punctuation">,</span>test_iter<span class="token punctuation">,</span>loss<span class="token punctuation">,</span>num_epochs<span class="token punctuation">,</span>batch_size<span class="token punctuation">,</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">,</span>lr<span class="token punctuation">,</span>optimizer<span class="token punctuation">)</span>
</code></pre> 
<pre><code>epoch 1,train_loss 0.668196,test_loss 0.635509
epoch 2,train_loss 0.603642,test_loss 0.571374
epoch 3,train_loss 0.538563,test_loss 0.505013
epoch 4,train_loss 0.470947,test_loss 0.436439
epoch 5,train_loss 0.402410,test_loss 0.368597
epoch 6,train_loss 0.336602,test_loss 0.305513
epoch 7,train_loss 0.277270,test_loss 0.250374
...
epoch 42,train_loss 0.010663,test_loss 0.010482
epoch 43,train_loss 0.010274,test_loss 0.010105
epoch 44,train_loss 0.009910,test_loss 0.009752
epoch 45,train_loss 0.009568,test_loss 0.009420
epoch 46,train_loss 0.009246,test_loss 0.009108
epoch 47,train_loss 0.008943,test_loss 0.008814
epoch 48,train_loss 0.008658,test_loss 0.008536
epoch 49,train_loss 0.008388,test_loss 0.008273
epoch 50,train_loss 0.008133,test_loss 0.008025
</code></pre> 
<h4><a id="loss_427"></a>绘制loss曲线</h4> 
<pre><code class="prism language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
x<span class="token operator">=</span>np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token builtin">len</span><span class="token punctuation">(</span>train_loss<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token builtin">len</span><span class="token punctuation">(</span>train_loss<span class="token punctuation">)</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span>train_loss<span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">"train_loss"</span><span class="token punctuation">,</span>linewidth<span class="token operator">=</span><span class="token number">1.5</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span>test_loss<span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">"test_loss"</span><span class="token punctuation">,</span>linewidth<span class="token operator">=</span><span class="token number">1.5</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">"epoch"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"loss"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>​<br> <img src="https://images2.imgbox.com/b4/5c/O3C3SFdc_o.png" alt="在这里插入图片描述"></p> 
<p>​</p> 
<h2><a id="_448"></a>多分类任务</h2> 
<pre><code class="prism language-python"><span class="token comment">#在执行一个新的任务之前，先将之前的中间变量的结果全部清空，即之前的编译不作数</span>
<span class="token operator">%</span>reset
</code></pre> 
<pre><code>Once deleted, variables cannot be recovered. Proceed (y/[n])? y
</code></pre> 
<h3><a id="_459"></a>导入包</h3> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">as</span> Data
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> init
<span class="token keyword">import</span> torchvision
<span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">as</span> transforms
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> random
<span class="token keyword">from</span> IPython <span class="token keyword">import</span> display
<span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot <span class="token keyword">as</span> plt
</code></pre> 
<h3><a id="MNIST_476"></a>下载MNIST数据集</h3> 
<p>按实验要求，这里利用torchvision模块下载了数字手写数据集:</p> 
<ul><li>其中训练集为60000张图片，测试集为10000张图片，其每个图片对应的标签是0-9之间，分别代表手写数字0,1,2,3,4,5,6,7,8,9.</li><li>图像是固定大小(28x28像素)，其值为0到1。为每个图像都被平展并转换为784(28 * 28)个特征的一维numpy数组。</li></ul> 
<pre><code class="prism language-python"><span class="token comment">#下载MNIST手写数据集 :包括训练集和测试集</span>
train_dataset <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'./Datasets/MNIST'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>  download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  
test_dataset <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'./Datasets/MNIST'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>  download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  
</code></pre> 
<h3><a id="_488"></a>定义数据迭代器</h3> 
<p>通过测试，输出X.shape为torch.Size([32, 1, 28, 28]) ：</p> 
<ul><li>32个图像</li><li>1代表图像为黑白，只有一个通道</li><li>28*28为图像的大小</li></ul> 
<p>torch.Size([32])，则代表32个图像，每个图像有一个对应的标签</p> 
<pre><code class="prism language-python">batch_size <span class="token operator">=</span> <span class="token number">32</span>  
train_iter <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>train_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>  num_workers<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>  
test_iter <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>test_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>  num_workers<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span> 
<span class="token comment">#测试：</span>
<span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> train_iter<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">,</span>y<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
    <span class="token keyword">break</span>
</code></pre> 
<pre><code>torch.Size([32, 1, 28, 28]) torch.Size([32])
</code></pre> 
<h3><a id="_511"></a>模型构建</h3> 
<pre><code class="prism language-python"><span class="token comment">#超参数初始化</span>
num_inputs<span class="token operator">=</span><span class="token number">784</span> <span class="token comment">#28*28</span>
num_hiddens<span class="token operator">=</span><span class="token number">256</span>
num_outputs<span class="token operator">=</span><span class="token number">10</span>
<span class="token comment">#实现FlattenLayer层</span>
<span class="token keyword">class</span> <span class="token class-name">FlattenLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">_init_</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>FlattenLayer<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>_init_<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment">#不管是否有展平的需要，都要加上，在这个例子中，显然有。</span>
<span class="token comment">#模型定义</span>
net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
        FlattenLayer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span>num_hiddens<span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span>num_outputs<span class="token punctuation">)</span>
    <span class="token punctuation">)</span>
</code></pre> 
<h3><a id="_534"></a>参数初始化</h3> 
<pre><code class="prism language-python"><span class="token keyword">for</span> param <span class="token keyword">in</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>param<span class="token punctuation">,</span>mean<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>std<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="_542"></a>损失函数和优化器</h3> 
<pre><code class="prism language-python">lr<span class="token operator">=</span><span class="token number">0.01</span>
loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="_551"></a>定义准确率检验器</h3> 
<p>加入了对测试集的准确率的判断，利用测试集的预测值和真实值进行比较，计算测试准确的概率。<br> FUN<br> 其中<code>y_hat.argmax(dim=1)</code>返回矩阵<code>y_hat</code>每行中最大元素的索引，且返回结果与变量y形状相同。相等条件判断式<code>(y_hat.argmax(dim=1) == y)</code>是一个类型为<code>ByteTensor</code>的Tensor,即tensor.bool，我们用<code>float()</code>将其转换为值为0（相等为假）或1（相等为真）的浮点型Tensor,即tensor.float。或者通过sum()直接转为为torch.int类型，再通item转换为int类型。</p> 
<p>对于本次的检验器，给定一个数据迭代器，两个功能：</p> 
<ul><li>计算准确率</li><li>计算loss</li></ul> 
<pre><code class="prism language-python"><span class="token comment">#返回准确率以及loss</span>
flag<span class="token operator">=</span><span class="token number">0</span>
<span class="token keyword">def</span> <span class="token function">evaluate_accuracy_loss</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> data_iter<span class="token punctuation">)</span><span class="token punctuation">:</span>
    acc_sum<span class="token operator">=</span><span class="token number">0.0</span>
    loss_sum<span class="token operator">=</span><span class="token number">0.0</span>
    n<span class="token operator">=</span><span class="token number">0</span>
    <span class="token keyword">global</span> flag
    <span class="token keyword">for</span> X<span class="token punctuation">,</span>y <span class="token keyword">in</span> data_iter<span class="token punctuation">:</span>
        y_hat <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
        <span class="token keyword">if</span> flag<span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token keyword">print</span> <span class="token punctuation">(</span>y_hat<span class="token punctuation">)</span><span class="token comment">#测试一下y_hat是否已经softmax激活</span>
        flag <span class="token operator">=</span> <span class="token number">1</span>
        acc_sum <span class="token operator">+=</span> <span class="token punctuation">(</span>y_hat<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">==</span>y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
        l <span class="token operator">=</span> loss<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span>y<span class="token punctuation">)</span>
        loss_sum <span class="token operator">+=</span> l<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">*</span>y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token comment">#由于loss(y_hat,y)默认为求平均，因此*y.shape[0]意味着求和。</span>
        n<span class="token operator">+=</span>y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    <span class="token keyword">return</span> acc_sum<span class="token operator">/</span>n<span class="token punctuation">,</span>loss_sum<span class="token operator">/</span>n
</code></pre> 
<pre><code class="prism language-python"><span class="token comment">#测试输出层是否被softmax函数激活</span>
<span class="token comment"># for X,y in train_iter:</span>
<span class="token comment">#         y_hat = net(X)</span>
<span class="token comment">#         print (y_hat)#测试一下y_hat是否已经softmax激活</span>
<span class="token comment">#         break</span>
</code></pre> 
<h3><a id="_589"></a>训练</h3> 
<h4><a id="_590"></a>定义训练函数</h4> 
<pre><code class="prism language-python"><span class="token comment">#记录列表（list），存储训练集和测试集上经过每一轮次，loss的变化</span>
<span class="token keyword">def</span> <span class="token function">train3</span> <span class="token punctuation">(</span>net<span class="token punctuation">,</span>train_iter<span class="token punctuation">,</span>test_iter<span class="token punctuation">,</span>loss<span class="token punctuation">,</span>num_epochs<span class="token punctuation">,</span>batch_size<span class="token punctuation">,</span>params <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>lr<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>optimizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    train_loss<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>
    test_loss<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment">#外循环控制循环轮次---跑完一轮，也就把数据走了一遍</span>
        train_l_sum<span class="token operator">=</span><span class="token number">0.0</span><span class="token comment">#记录训练集上的损失</span>
        train_acc_num<span class="token operator">=</span><span class="token number">0.0</span><span class="token comment">#记录训练集上的准确数</span>
        n <span class="token operator">=</span><span class="token number">0.0</span>
        <span class="token comment">#step1在训练集上，进行小批量梯度下降更新参数</span>
        <span class="token keyword">for</span> X<span class="token punctuation">,</span>y <span class="token keyword">in</span> train_iter<span class="token punctuation">:</span><span class="token comment">#内循环控制训练批次</span>
            y_hat <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
            <span class="token comment">#保证y与y_hat维度一致，否则将会发生广播</span>
            l <span class="token operator">=</span> loss<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span>y<span class="token punctuation">)</span><span class="token comment">#这里计算出的loss是已经求和过的，l.size = torch.Size([]),即说明loss为表示*标量*的tensor`</span>
            <span class="token comment">#梯度清零</span>
            <span class="token keyword">if</span> optimizer <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">elif</span> params <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">and</span> params<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>grad <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                <span class="token keyword">for</span> param <span class="token keyword">in</span> params<span class="token punctuation">:</span>
                    param<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>
            l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> optimizer <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                SGD<span class="token punctuation">(</span>params<span class="token punctuation">,</span>lr<span class="token punctuation">,</span>batch_size<span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token comment">#每一个迭代周期中得到的训练集上的loss累积进来</span>
            train_l_sum <span class="token operator">+=</span> l<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">*</span>y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
            <span class="token comment">#计算训练样本的准确率---将每个迭代周期中预测正确的样本数累积进来</span>
            train_acc_num <span class="token operator">+=</span> <span class="token punctuation">(</span>y_hat<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">==</span>y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">#转为int类型</span>
            n <span class="token operator">+=</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        <span class="token comment">#step2 每经过一个轮次的训练， 记录训练集和测试集上的loss</span>
        <span class="token comment">#注意要取平均值，loss默认求了sum</span>
        train_loss<span class="token punctuation">.</span>append<span class="token punctuation">(</span>train_l_sum<span class="token operator">/</span>n<span class="token punctuation">)</span><span class="token comment">#训练集loss</span>
        test_acc<span class="token punctuation">,</span>test_l <span class="token operator">=</span> evaluate_accuracy_loss<span class="token punctuation">(</span>net<span class="token punctuation">,</span>test_iter<span class="token punctuation">)</span>
        test_loss<span class="token punctuation">.</span>append<span class="token punctuation">(</span>test_l<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"epoch %d,train_loss %.6f,test_loss %.6f,train_acc %.6f,test_acc %.6f"</span><span class="token operator">%</span><span class="token punctuation">(</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span>train_loss<span class="token punctuation">[</span>epoch<span class="token punctuation">]</span><span class="token punctuation">,</span>test_loss<span class="token punctuation">[</span>epoch<span class="token punctuation">]</span><span class="token punctuation">,</span>train_acc_num<span class="token operator">/</span>n<span class="token punctuation">,</span>test_acc<span class="token punctuation">)</span><span class="token punctuation">)</span> 
    <span class="token keyword">return</span> train_loss<span class="token punctuation">,</span> test_loss
</code></pre> 
<h4><a id="_632"></a>开始训练模型</h4> 
<pre><code class="prism language-python">lr <span class="token operator">=</span> <span class="token number">0.01</span>
num_epochs<span class="token operator">=</span><span class="token number">30</span>
train_loss<span class="token punctuation">,</span>test_loss<span class="token operator">=</span>train3<span class="token punctuation">(</span>net<span class="token punctuation">,</span>train_iter<span class="token punctuation">,</span>test_iter<span class="token punctuation">,</span>loss<span class="token punctuation">,</span>num_epochs<span class="token punctuation">,</span>batch_size<span class="token punctuation">,</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr<span class="token punctuation">,</span>optimizer<span class="token punctuation">)</span>
</code></pre> 
<pre><code>epoch 1,train_loss 1.176481,test_loss 0.477081,train_acc 0.734050,test_acc 0.876300
epoch 2,train_loss 0.414445,test_loss 0.348338,train_acc 0.887100,test_acc 0.903500
epoch 3,train_loss 0.341379,test_loss 0.305867,train_acc 0.903383,test_acc 0.914100
epoch 4,train_loss 0.307838,test_loss 0.280942,train_acc 0.912317,test_acc 0.920700
epoch 5,train_loss 0.283337,test_loss 0.261160,train_acc 0.920200,test_acc 0.926500
epoch 6,train_loss 0.262854,test_loss 0.245003,train_acc 0.925567,test_acc 0.931400
epoch 7,train_loss 0.244713,test_loss 0.229990,train_acc 0.931100,test_acc 0.935600
epoch 8,train_loss 0.228311,test_loss 0.214327,train_acc 0.935733,test_acc 0.939500
...
epoch 25,train_loss 0.102477,test_loss 0.111054,train_acc 0.971933,test_acc 0.966100
epoch 26,train_loss 0.099015,test_loss 0.108542,train_acc 0.972850,test_acc 0.967700
epoch 27,train_loss 0.095864,test_loss 0.106288,train_acc 0.973533,test_acc 0.968900
epoch 28,train_loss 0.092862,test_loss 0.103809,train_acc 0.974700,test_acc 0.969000
epoch 29,train_loss 0.089862,test_loss 0.101595,train_acc 0.975550,test_acc 0.970400
epoch 30,train_loss 0.087256,test_loss 0.100992,train_acc 0.976000,test_acc 0.970800
</code></pre> 
<h4><a id="lossacc_658"></a>绘制loss、acc曲线</h4> 
<pre><code class="prism language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
x<span class="token operator">=</span>np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token builtin">len</span><span class="token punctuation">(</span>train_loss<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token builtin">len</span><span class="token punctuation">(</span>train_loss<span class="token punctuation">)</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span>train_loss<span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">"train_loss"</span><span class="token punctuation">,</span>linewidth<span class="token operator">=</span><span class="token number">1.5</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span>test_loss<span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">"test_loss"</span><span class="token punctuation">,</span>linewidth<span class="token operator">=</span><span class="token number">1.5</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">"epoch"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"loss"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>​<br> <img src="https://images2.imgbox.com/0d/6d/fXzMAgxx_o.png" alt="在这里插入图片描述"></p> 
<p>​</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/bdbe4cbc10ef5460e04ef507f4aaa6ca/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">文本情感分类模型之BERT</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e98fda3c3e6e5b579c888f628a2050a5/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">小表驱动大表vlog</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>