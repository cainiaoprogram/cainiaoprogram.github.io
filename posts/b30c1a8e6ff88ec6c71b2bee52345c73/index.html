<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【基础知识】深度学习中各种归一化方式详解 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【基础知识】深度学习中各种归一化方式详解" />
<meta property="og:description" content="本文转载自 https://blog.csdn.net/qq_23981335/article/details/106572171
仅作记录学习~
总结 BN，LN，IN，GN，WS 从学术上解释差异：
BatchNorm：batch方向做归一化，算NHW的均值，对小batchsize效果不好；BN主要缺点是对batchsize的大小比较敏感，由于每次计算均值和方差是在一个batch上，所以如果batchsize太小，则计算的均值、方差不足以代表整个数据分布；LayerNorm：channel方向做归一化，算CHW的均值，主要对RNN作用明显；InstanceNorm：一个channel内做归一化，算H*W的均值，用在风格化迁移；因为在图像风格化中，生成结果主要依赖于某个图像实例，所以对整个batch归一化不适合图像风格化中，因而对HW做归一化。可以加速模型收敛，并且保持每个图像实例之间的独立。GroupNorm：将channel方向分group，然后每个group内做归一化，算(C//G)HW的均值；这样与batchsize无关，不受其约束。SwitchableNorm：将BN、LN、IN结合，赋予权重，让网络自己去学习归一化层应该使用什么方法。Weight Standardization：权重标准化，2019年约翰霍普金斯大学研究人员提出。 详细阐述 1. BatchNorm torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) torch.nn.BatchNorm3d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) 参数：
num_features： 来自期望输入的特征数，该期望输入的大小为’batch_size × num_features [× width]’eps： 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。momentum： 动态均值和动态方差所使用的动量。默认为0.1。 affine： 布尔值，当设为true，给该层添加可学习的仿射变换参数。track_running_stats：布尔值，当设为true，记录训练过程中的均值和方差； 公式：
大部分深度网络通常都会使用 BN 层去加速训练和帮助模型更好收敛。虽然 BN 层非常实用，但从研究者的角度看，依然有一些非常显眼的缺点。比如
（1）我们非常缺乏对于 BN 层成功原因的理解；
（2）BN 层仅在 batch size 足够大时才有明显的效果，因此不能用在微批次的训练中。虽然现在已经有专门针对微批次训练设计的归一化方法（GN），但图 1 所示，它很难在大批次训练时媲美 BN 的效果。
2. GroupNorm FAIR 团队的吴育昕和何恺明提出了组归一化（Group Normalization，简称 GN）的方法，GN 将信号通道分成一个个组别，并在每个组别内计算归一化的均值和方差，以进行归一化处理。GN 的计算与批量大小无关，而且在批次大小大幅变化时，精度依然稳定。通常来说，在使用 Batch Normalization（以下将简称 BN）时，采用小批次很难训练一个网络，而对于不使用批次的优化方法来说，效果很难媲美采用大批次BN时的训练结果。当使用 Group Normalization（以下将简称 GN），且 batch size 大小为 1 时，仅需要多写两行代码加入权重标准化方法，就能比肩甚至超越大批次BN时的训练效果。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/b30c1a8e6ff88ec6c71b2bee52345c73/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-05-14T11:17:30+08:00" />
<meta property="article:modified_time" content="2021-05-14T11:17:30+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【基础知识】深度学习中各种归一化方式详解</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>本文转载自 https://blog.csdn.net/qq_23981335/article/details/106572171<br> 仅作记录学习~</p> 
<h2><a id="_3"></a>总结</h2> 
<p><strong>BN，LN，IN，GN，WS 从学术上解释差异</strong>：</p> 
<blockquote> 
 <ul><li><strong>BatchNorm</strong>：batch方向做归一化，算NHW的均值，对小batchsize效果不好；BN主要缺点是对batchsize的大小比较敏感，由于每次计算均值和方差是在一个batch上，所以如果batchsize太小，则计算的均值、方差不足以代表整个数据分布；</li><li><strong>LayerNorm</strong>：channel方向做归一化，算CHW的均值，主要对RNN作用明显；</li><li><strong>InstanceNorm</strong>：一个channel内做归一化，算H*W的均值，用在风格化迁移；因为在图像风格化中，生成结果主要依赖于某个图像实例，所以对整个batch归一化不适合图像风格化中，因而对HW做归一化。可以加速模型收敛，并且保持每个图像实例之间的独立。</li><li><strong>GroupNorm</strong>：将channel方向分group，然后每个group内做归一化，算(C//G)HW的均值；这样与batchsize无关，不受其约束。</li><li><strong>SwitchableNorm</strong>：将BN、LN、IN结合，赋予权重，让网络自己去学习归一化层应该使用什么方法。</li><li><strong>Weight Standardization</strong>：权重标准化，2019年约翰霍普金斯大学研究人员提出。</li></ul> 
</blockquote> 
<p><img src="https://images2.imgbox.com/98/02/kUNEtjBV_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/e5/15/p62LosWW_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_15"></a>详细阐述</h2> 
<h3><a id="1_BatchNorm_16"></a>1. BatchNorm</h3> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>BatchNorm1d<span class="token punctuation">(</span>num_features<span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">05</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> track_running_stats<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>num_features<span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">05</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> track_running_stats<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>BatchNorm3d<span class="token punctuation">(</span>num_features<span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">05</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> track_running_stats<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/00/fc/Nz80XawD_o.png" alt="在这里插入图片描述"><br> <strong>参数：</strong></p> 
<blockquote> 
 <ul><li><strong>num_features</strong>： 来自期望输入的特征数，该期望输入的大小为’batch_size × num_features [× width]’</li><li><strong>eps</strong>： 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。</li><li><strong>momentum</strong>： 动态均值和动态方差所使用的动量。默认为0.1。 affine： 布尔值，当设为true，给该层添加可学习的仿射变换参数。</li><li><strong>track_running_stats</strong>：布尔值，当设为true，记录训练过程中的均值和方差；</li></ul> 
</blockquote> 
<p><strong>公式：</strong><br> <img src="https://images2.imgbox.com/53/e4/csxAIiER_o.png" alt="在这里插入图片描述"><br> 大部分深度网络通常都会使用 BN 层去加速训练和帮助模型更好收敛。虽然 BN 层非常实用，但从研究者的角度看，依然有一些非常显眼的缺点。比如<br> （1）我们非常缺乏对于 BN 层成功原因的理解；<br> （2）BN 层仅在 batch size 足够大时才有明显的效果，因此不能用在微批次的训练中。虽然现在已经有专门针对微批次训练设计的归一化方法（GN），但图 1 所示，它很难在大批次训练时媲美 BN 的效果。</p> 
<h3><a id="2_GroupNorm_36"></a>2. GroupNorm</h3> 
<p>FAIR 团队的吴育昕和何恺明提出了<strong>组归一化</strong>（Group Normalization，简称 GN）的方法，GN 将信号通道分成一个个组别，并在每个组别内计算归一化的均值和方差，以进行归一化处理。GN 的计算与批量大小无关，而且在批次大小大幅变化时，精度依然稳定。通常来说，在使用 Batch Normalization（以下将简称 BN）时，采用小批次很难训练一个网络，而对于不使用批次的优化方法来说，效果很难媲美采用大批次BN时的训练结果。当使用 Group Normalization（以下将简称 GN），且 batch size 大小为 1 时，仅需要多写两行代码加入权重标准化方法，就能比肩甚至超越大批次BN时的训练效果。</p> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>GroupNorm<span class="token punctuation">(</span>num_groups<span class="token punctuation">,</span> num_channels<span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">05</span><span class="token punctuation">,</span> affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>参数：</strong></p> 
<blockquote> 
 <ul><li><strong>num_groups</strong>：需要划分为的groups</li><li><strong>num_features</strong>： 来自期望输入的特征数，该期望输入的大小为’batch_size x num_features [x width]’</li><li><strong>eps</strong>： 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。</li><li><strong>momentum</strong>： 动态均值和动态方差所使用的动量。默认为0.1。</li><li><strong>affine</strong>： 布尔值，当设为true，给该层添加可学习的仿射变换参数。</li></ul> 
</blockquote> 
<p><strong>实现公式</strong>：</p> 
<p><img src="https://images2.imgbox.com/26/9c/qIWA1D36_o.png" alt="在这里插入图片描述"><br> <strong>tf代码如下：</strong><br> <img src="https://images2.imgbox.com/50/bb/vRmWXYXf_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="3_InstanceNorm_56"></a>3. InstanceNorm</h3> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>InstanceNorm1d<span class="token punctuation">(</span>num_features<span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">05</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> affine<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> track_running_stats<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>InstanceNorm2d<span class="token punctuation">(</span>num_features<span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">05</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> affine<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> track_running_stats<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>InstanceNorm3d<span class="token punctuation">(</span>num_features<span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">05</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> affine<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> track_running_stats<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>参数：</strong></p> 
<blockquote> 
 <ul><li><strong>num_features</strong>： 来自期望输入的特征数，该期望输入的大小为’batch_size x num_features [x width]’</li><li><strong>eps</strong>： 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。</li><li><strong>momentum</strong>： 动态均值和动态方差所使用的动量。默认为0.1。</li><li><strong>affine</strong>： 布尔值，当设为true，给该层添加可学习的仿射变换参数。</li><li><strong>track_running_stats</strong>：布尔值，当设为true，记录训练过程中的均值和方差；</li></ul> 
</blockquote> 
<p><strong>实现公式：</strong><br> <img src="https://images2.imgbox.com/7f/c0/axKnPUfX_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="4_LayerNorm_73"></a>4. LayerNorm</h3> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>normalized_shape<span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">05</span><span class="token punctuation">,</span> elementwise_affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/cd/eb/HIsYaiHg_o.png" alt="在这里插入图片描述"><br> <strong>参数：</strong></p> 
<blockquote> 
 <ul><li><strong>normalized_shape</strong>： 输入尺寸 [∗×normalized_shape[0]×normalized_shape[1]×…×normalized_shape[−1]]</li><li><strong>eps</strong>： 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。</li><li><strong>elementwise_affine</strong>： 布尔值，当设为true，给该层添加可学习的仿射变换参数。</li></ul> 
</blockquote> 
<p><strong>实现公式：</strong><br> <img src="https://images2.imgbox.com/7f/f6/bWMzPcHT_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="5_LocalResponseNorm_87"></a>5. LocalResponseNorm</h3> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>LocalResponseNorm<span class="token punctuation">(</span>size<span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.0001</span><span class="token punctuation">,</span> beta<span class="token operator">=</span><span class="token number">0.75</span><span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>参数：</strong></p> 
<blockquote> 
 <ul><li><strong>size</strong>：用于归一化的邻居通道数</li><li><strong>alpha</strong>：乘积因子，Default: 0.0001</li><li><strong>beta</strong> ：指数，Default: 0.75</li><li><strong>k</strong>：附加因子，Default: 1</li></ul> 
</blockquote> 
<p><strong>实现公式：</strong><br> <img src="https://images2.imgbox.com/e7/c8/Xq2zMAsY_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="6_Weight_Standardization_102"></a>6. Weight Standardization</h3> 
<blockquote> 
 <ul><li>论文<a href="https://arxiv.org/pdf/1903.10520v2.pdf" rel="nofollow">《Micro-Batch Training with Batch-ChannelNormalization and Weight Standardization》</a> JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST<br> 20151</li><li>代码 <a href="https://github.com/joe-siyuan-qiao/WeightStandardization">Github/joe-siyuan-qiao / WeightStandardization</a></li></ul> 
</blockquote> 
<p><img src="https://images2.imgbox.com/1b/91/toETfQGd_o.png" alt="在这里插入图片描述"><br> 下图是在**网络前馈（青色）<strong>和</strong>反向传播（红色）**时，进行权重梯度标准化的计算表达式：<br> <img src="https://images2.imgbox.com/5c/98/cgaAKDKb_o.png" alt="在这里插入图片描述"><br> 以卷积神经网络中的卷积核为例，Pytorch中传统的卷积模块为：<br> <img src="https://images2.imgbox.com/0e/3b/s9Npa899_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/00/07/Cvr9c8qv_o.png" alt="在这里插入图片描述"><br> <strong>引入WS后的实现为：</strong></p> 
<pre><code class="prism language-python"><span class="token comment"># Pytorch</span>
<span class="token keyword">class</span> <span class="token class-name">Conv2d</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">'''
    shape:
    input: (Batch_size, in_channels, H_in, W_in)
    output: ((Batch_size, out_channels, H_out, W_out))
    '''</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
                 padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> groups<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Conv2d<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> stride<span class="token punctuation">,</span>
                 padding<span class="token punctuation">,</span> dilation<span class="token punctuation">,</span> groups<span class="token punctuation">,</span> bias<span class="token punctuation">)</span>
 
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        weight <span class="token operator">=</span> self<span class="token punctuation">.</span>weight   <span class="token comment">#self.weight 的shape为(out_channels, in_channels, kernel_size_w, kernel_size_h)</span>
        weight_mean <span class="token operator">=</span> weight<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
                                  keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        weight <span class="token operator">=</span> weight <span class="token operator">-</span> weight_mean
        std <span class="token operator">=</span> weight<span class="token punctuation">.</span>view<span class="token punctuation">(</span>weight<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>std<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">5</span>
        weight <span class="token operator">=</span> weight <span class="token operator">/</span> std<span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>weight<span class="token punctuation">)</span>
        <span class="token keyword">return</span> F<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>x<span class="token punctuation">,</span> weight<span class="token punctuation">,</span> self<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> self<span class="token punctuation">.</span>stride<span class="token punctuation">,</span>
                        self<span class="token punctuation">.</span>padding<span class="token punctuation">,</span> self<span class="token punctuation">.</span>dilation<span class="token punctuation">,</span> self<span class="token punctuation">.</span>groups<span class="token punctuation">)</span>
</code></pre> 
<h2><a id="_140"></a>附录</h2> 
<h3><a id="_141"></a>个人理解和代码测试</h3> 
<p>下图是对<strong>BatchNorm, LayerNorm, InstanceNorm和GroupNorm</strong>四种Normalization方式的一个汇总(我个人感觉这个图看起来方便一些).</p> 
<ul><li>图中每一个正方体块表示一个数据(比如说这里一个正方体就是一个图像)</li><li>每一个正方体中的C, H, W分别表示channel(通道个数), height(图像的高), weight(图像的宽)</li><li>下图介绍了4中Norm的方式, 如Layer Norm中NHWC-----&gt;N111表示是将后面的三个进行标准化, 不与batchsize有关.</li><li>我们可以看到, 后面的LayerNorm, InstanceNorm和GroupNorm这三种方式都是和Batchsize是没有关系的。</li></ul> 
<p><img src="https://images2.imgbox.com/fe/c6/tW16Rkbh_o.png" alt="在这里插入图片描述"><br> 下面我们使用一个(2, 2, 4)的数据来举一个例子, 我们可以将其看成有2个图像组成的单通道的图像，</p> 
<h4><a id="1_152"></a>（1）生成测试使用数据</h4> 
<p>我们首先生成测试使用的数据, 数据的大小为(2, 2, 4)；</p> 
<pre><code class="prism language-python">x_test <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                   <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
x_test <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
x_test
<span class="token triple-quoted-string string">"""
tensor([[[ 1.,  2., -1.,  1.],
         [ 3.,  4., -2.,  2.]],
        [[ 1.,  2., -1.,  1.],
         [ 3.,  4., -2.,  2.]]])
"""</span>
</code></pre> 
<h4><a id="2LayerNormGroupNorm_168"></a>（2）测试LayerNorm与GroupNorm</h4> 
<p>关于这里的计算的细节, 会在后面的计算细节描述部分进行叙述. 这里就看一下如何使用Pytorch来进行计算, 和最终计算得到的结果。</p> 
<p>LayerNorm就是对(2, 2, 4), 后面这一部分进行整个的标准化。可以理解为对整个图像进行标准化。</p> 
<pre><code class="prism language-python">m <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>normalized_shape <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
output <span class="token operator">=</span> m<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span>
output
<span class="token triple-quoted-string string">"""
tensor([[[-0.1348,  0.4045, -1.2136, -0.1348],
         [ 0.9439,  1.4832, -1.7529,  0.4045]],
        [[-0.1348,  0.4045, -1.2136, -0.1348],
         [ 0.9439,  1.4832, -1.7529,  0.4045]]], grad_fn=&lt;AddcmulBackward&gt;)
"""</span>
</code></pre> 
<p>当GroupNorm中group的数量是1的时候, 是与上面的LayerNorm是等价的.</p> 
<pre><code class="prism language-python"><span class="token comment"># Separate 2 channels into 1 groups (equivalent with LayerNorm)</span>
m <span class="token operator">=</span> nn<span class="token punctuation">.</span>GroupNorm<span class="token punctuation">(</span>num_groups<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> num_channels<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> affine<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
output <span class="token operator">=</span> m<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span>
output
<span class="token triple-quoted-string string">"""
tensor([[[-0.1348,  0.4045, -1.2136, -0.1348],
         [ 0.9439,  1.4832, -1.7529,  0.4045]],
        [[-0.1348,  0.4045, -1.2136, -0.1348],
         [ 0.9439,  1.4832, -1.7529,  0.4045]]])
"""</span>
</code></pre> 
<h4><a id="3InstanceNormGroupNorm_199"></a>（3）测试InstanceNorm和GroupNorm</h4> 
<p>InstanceNorm就是对(2, 2, 4), 标红的这一部分进行Norm。</p> 
<pre><code class="prism language-python">m <span class="token operator">=</span> nn<span class="token punctuation">.</span>InstanceNorm1d<span class="token punctuation">(</span>num_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
output <span class="token operator">=</span> m<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span>
output
<span class="token triple-quoted-string string">"""
tensor([[[ 0.2294,  1.1471, -1.6059,  0.2294],
         [ 0.5488,  0.9879, -1.6465,  0.1098]],
        [[ 0.2294,  1.1471, -1.6059,  0.2294],
         [ 0.5488,  0.9879, -1.6465,  0.1098]]])
"""</span>
</code></pre> 
<p>上面这种InstanceNorm等价于当GroupNorm时num_groups的数量等于num_channel的数量。</p> 
<pre><code class="prism language-python"><span class="token comment"># Separate 2 channels into 2 groups (equivalent with InstanceNorm)</span>
m <span class="token operator">=</span> nn<span class="token punctuation">.</span>GroupNorm<span class="token punctuation">(</span>num_groups<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> num_channels<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> affine<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
output <span class="token operator">=</span> m<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span>
output
<span class="token triple-quoted-string string">"""
tensor([[[ 0.2294,  1.1471, -1.6059,  0.2294],
         [ 0.5488,  0.9879, -1.6465,  0.1098]],
        [[ 0.2294,  1.1471, -1.6059,  0.2294],
         [ 0.5488,  0.9879, -1.6465,  0.1098]]])
"""</span>
</code></pre> 
<h4><a id="4_228"></a>（4）计算细节描述</h4> 
<p>我们看一下在上面的LayerNorm和InstanceNorm中的结果是如何计算出来的. 我们只看第一行第一列的数据1进行标准化的过程. 下面是详细的计算的过程(这里的计算结果与上面直接计算的结果是相同的)。<br> <img src="https://images2.imgbox.com/cd/7d/pAjAKHlw_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="5_231"></a>（5）每一种方式适合的场景</h4> 
<blockquote> 
 <ul><li>batchNorm是在batch上，对小batchsize效果不好；</li><li>layerNorm在通道方向上，主要对RNN作用明显；</li><li>instanceNorm在图像像素上，用在风格化迁移；</li><li>GroupNorm将channel分组，然后再做归一化, 在batchsize&lt;16的时候, 可以使用这种归一化。</li></ul> 
</blockquote>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/82127e5c19e3fb7e48be9c789b15720e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">sql语句之Data Manipulation Language(DML 数据操纵语言)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/af5894f7773ad17c8198ddc18c619a7a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">ValueError: invalid literal for int() with base 10: “ ”</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>