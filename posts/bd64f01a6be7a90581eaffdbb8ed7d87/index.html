<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>彻底搞定 SpringBoot 整合 Kafka（spring-kafka深入探秘） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="彻底搞定 SpringBoot 整合 Kafka（spring-kafka深入探秘）" />
<meta property="og:description" content="前言 kafka是一个消息队列产品，基于Topic partitions的设计，能达到非常高的消息发送处理性能。Spring创建了一个项目Spring-kafka，封装了Apache 的Kafka-client，用于在Spring项目里快速集成kafka。
除了简单的收发消息外，Spring-kafka还提供了很多高级功能，下面我们就来一一探秘这些用法。
简单集成 引入依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt; &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt; &lt;version&gt;2.2.6.RELEASE&lt;/version&gt; &lt;/dependency&gt; 添加配置 spring.kafka.producer.bootstrap-servers=127.0.0.1:9092 测试发送和接收 /** * @author: SpringRoot * @date: 2019/5/30 */ @SpringBootApplication @RestController public class Application { private final Logger logger = LoggerFactory.getLogger(Application.class); public static void main(String[] args) { SpringApplication.run(Application.class, args); } @Autowired private KafkaTemplate&lt;Object, Object&gt; template; @GetMapping(&#34;/send/{input}&#34;) public void sendFoo(@PathVariable String input) { this.template.send(&#34;topic_input&#34;, input); } @KafkaListener(id = &#34;webGroup&#34;, topics = &#34;topic_input&#34;) public void listen(String input) { logger." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/bd64f01a6be7a90581eaffdbb8ed7d87/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-11-21T09:53:42+08:00" />
<meta property="article:modified_time" content="2019-11-21T09:53:42+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">彻底搞定 SpringBoot 整合 Kafka（spring-kafka深入探秘）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3>前言</h3> 
<p>kafka是一个消息队列产品，基于Topic partitions的设计，能达到非常高的消息发送处理性能。Spring创建了一个项目Spring-kafka，封装了Apache 的Kafka-client，用于在Spring项目里快速集成kafka。</p> 
<p>除了简单的收发消息外，Spring-kafka还提供了很多高级功能，下面我们就来一一探秘这些用法。</p> 
<p> </p> 
<hr> 
<h3>简单集成</h3> 
<h4>引入依赖</h4> 
<pre class="has"><code>&lt;dependency&gt;
  &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt;
  &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt;
  &lt;version&gt;2.2.6.RELEASE&lt;/version&gt;
&lt;/dependency&gt;
</code></pre> 
<h4>添加配置</h4> 
<pre class="has"><code>spring.kafka.producer.bootstrap-servers=127.0.0.1:9092
</code></pre> 
<h4>测试发送和接收</h4> 
<pre class="has"><code>/**
 * @author: SpringRoot
 * @date: 2019/5/30
 */
@SpringBootApplication
@RestController
public class Application {

    private final Logger logger = LoggerFactory.getLogger(Application.class);

    public static void main(String[] args) {
        SpringApplication.run(Application.class, args);
    }

    @Autowired
    private KafkaTemplate&lt;Object, Object&gt; template;

    @GetMapping("/send/{input}")
    public void sendFoo(@PathVariable String input) {
        this.template.send("topic_input", input);
    }
    @KafkaListener(id = "webGroup", topics = "topic_input")
    public void listen(String input) {
        logger.info("input value: {}" , input);
    }
}
</code></pre> 
<p>启动应用后，在浏览器中输入：http://localhost:8080/send/kl。就可以在控制台看到有日志输出了：input value: "kl"。基础的使用就这么简单。发送消息时注入一个KafkaTemplate，接收消息时添加一个<code>@KafkaListener</code>注解即可。</p> 
<hr> 
<h3>Spring-kafka-test嵌入式Kafka Server</h3> 
<p>不过上面的代码能够启动成功，前提是你已经有了Kafka Server的服务环境，我们知道Kafka是由Scala + Zookeeper构建的，可以从官网下载部署包在本地部署。</p> 
<p>但是，我想告诉你，为了简化开发环节验证Kafka相关功能，Spring-Kafka-Test已经封装了Kafka-test提供了注解式的一键开启Kafka Server的功能，使用起来也是超级简单。本文后面的所有测试用例的Kafka都是使用这种嵌入式服务提供的。</p> 
<h4>引入依赖</h4> 
<pre class="has"><code>&lt;dependency&gt;
   &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt;
   &lt;artifactId&gt;spring-kafka-test&lt;/artifactId&gt;
   &lt;version&gt;2.2.6.RELEASE&lt;/version&gt;
   &lt;scope&gt;test&lt;/scope&gt;
&lt;/dependency&gt;
</code></pre> 
<h4>启动服务</h4> 
<p>下面使用Junit测试用例，直接启动一个Kafka Server服务，包含四个Broker节点。</p> 
<pre class="has"><code>@RunWith(SpringRunner.class)
@SpringBootTest(classes = ApplicationTests.class)
@EmbeddedKafka(count = 4,ports = {9092,9093,9094,9095})
public class ApplicationTests {
    @Test
    public void contextLoads()throws IOException {
        System.in.read();
    }
}
</code></pre> 
<p>如上：只需要一个注解<code>@EmbeddedKafka</code>即可，就可以启动一个功能完整的Kafka服务，是不是很酷。默认只写注解不加参数的情况下，是创建一个随机端口的Broker，在启动的日志中会输出具体的端口以及默认的一些配置项。</p> 
<p>不过这些我们在Kafka安装包配置文件中的配置项，在注解参数中都可以配置，下面详解下<code>@EmbeddedKafka</code>注解中的可设置参数 ：</p> 
<blockquote> 
 <ul><li><strong>value：</strong>broker节点数量</li><li><strong>count：</strong>同value作用一样，也是配置的broker的节点数量</li><li><strong>controlledShutdown：</strong>控制关闭开关，主要用来在Broker意外关闭时减少此Broker上Partition的不可用时间</li></ul> 
</blockquote> 
<p>Kafka是多Broker架构的高可用服务，一个Topic对应多个partition，一个Partition可以有多个副本Replication，这些Replication副本保存在多个Broker，用于高可用。</p> 
<p>但是，虽然存在多个分区副本集，当前工作副本集却只有一个，默认就是首次分配的副本集【首选副本】为Leader，负责写入和读取数据。当我们升级Broker或者更新Broker配置时需要重启服务，这个时候需要将partition转移到可用的Broker。</p> 
<p>下面涉及到三种情况</p> 
<p><strong>1、直接关闭Broker：</strong>当Broker关闭时，Broker集群会重新进行选主操作，选出一个新的Broker来作为Partition Leader，选举时此Broker上的Partition会短时不可用</p> 
<p><strong>2、开启controlledShutdown：</strong>当Broker关闭时，Broker本身会先尝试将Leader角色转移到其他可用的Broker上</p> 
<p><strong>3、使用命令行工具：</strong>使用bin/kafka-preferred-replica-election.sh，手动触发PartitionLeader角色转移</p> 
<p><strong>ports：</strong>端口列表，是一个数组。对应了count参数，有几个Broker，就要对应几个端口号</p> 
<p><strong>brokerProperties：</strong>Broker参数设置，是一个数组结构，支持如下方式进行Broker参数设置：</p> 
<pre class="has"><code>@EmbeddedKafka(brokerProperties = {"log.index.interval.bytes = 4096","num.io.threads = 8"})
</code></pre> 
<p><strong>okerPropertiesLocation：</strong>Broker参数文件设置</p> 
<p>功能同上面的brokerProperties，只是Kafka Broker的可设置参数达182个之多，都像上面这样配置肯定不是最优方案，所以提供了加载本地配置文件的功能，如：</p> 
<pre class="has"><code>@EmbeddedKafka(brokerPropertiesLocation = "classpath:application.properties")
</code></pre> 
<hr> 
<h3>创建新的Topic</h3> 
<p>默认情况下，如果在使用KafkaTemplate发送消息时，Topic不存在，会创建一个新的Topic，默认的分区数和副本数为如下Broker参数来设定</p> 
<pre class="has"><code>num.partitions = 1 #默认Topic分区数
num.replica.fetchers = 1 #默认副本数
</code></pre> 
<h4>程序启动时创建Topic</h4> 
<pre class="has"><code>/**
 * @author: SpringRoot
 * @date: 2019/10/31
 */
@Configuration
public class KafkaConfig {
    @Bean
    public KafkaAdmin admin(KafkaProperties properties){
        KafkaAdmin admin = new KafkaAdmin(properties.buildAdminProperties());
        admin.setFatalIfBrokerNotAvailable(true);
        return admin;
    }
    @Bean
    public NewTopic topic2() {
        return new NewTopic("topic-kl", 1, (short) 1);
    }
}
</code></pre> 
<p>如果Kafka Broker支持（1.0.0或更高版本），则如果发现现有Topic的Partition 数少于设置的Partition 数，则会新增新的Partition分区。</p> 
<p><strong>关于KafkaAdmin有几个常用的用法如下：</strong></p> 
<p><strong>setFatalIfBrokerNotAvailable(true)：</strong>默认这个值是False的，在Broker不可用时，不影响Spring 上下文的初始化。如果你觉得Broker不可用影响正常业务需要显示的将这个值设置为True</p> 
<p><strong>setAutoCreate(false) : </strong>默认值为True，也就是Kafka实例化后会自动创建已经实例化的NewTopic对象</p> 
<p><strong>initialize()：</strong>当setAutoCreate为false时，需要我们程序显示的调用admin的initialize()方法来初始化NewTopic对象</p> 
<h4>代码逻辑中创建</h4> 
<p>有时候我们在程序启动时并不知道某个Topic需要多少Partition数合适，但是又不能一股脑的直接使用Broker的默认设置，这个时候就需要使用Kafka-Client自带的AdminClient来进行处理。</p> 
<p>上面的Spring封装的KafkaAdmin也是使用的AdminClient来处理的。如：</p> 
<pre class="has"><code>    @Autowired
    private KafkaProperties properties;
    @Test
    public void testCreateToipc(){
        AdminClient client = AdminClient.create(properties.buildAdminProperties());
        if(client !=null){
            try {
                Collection&lt;NewTopic&gt; newTopics = new ArrayList&lt;&gt;(1);
                newTopics.add(new NewTopic("topic-kl",1,(short) 1));
                client.createTopics(newTopics);
            }catch (Throwable e){
                e.printStackTrace();
            }finally {
                client.close();
            }
        }
    }
</code></pre> 
<h4>ps:其他的方式创建Topic</h4> 
<p>上面的这些创建Topic方式前提是你的spring boot版本到2.x以上了，因为spring-kafka2.x版本只支持spring boot2.x的版本。在1.x的版本中还没有这些api。下面补充一种在程序中通过Kafka_2.10创建Topic的方式</p> 
<h4>引入依赖</h4> 
<pre class="has"><code>       &lt;dependency&gt;
            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
            &lt;artifactId&gt;kafka_2.10&lt;/artifactId&gt;
            &lt;version&gt;0.8.2.2&lt;/version&gt;
        &lt;/dependency&gt;
</code></pre> 
<h4>api方式创建</h4> 
<pre class="has"><code>    @Test
    public void testCreateTopic()throws Exception{
        ZkClient zkClient =new ZkClient("127.0.0.1:2181", 3000, 3000, ZKStringSerializer$.MODULE$)
        String topicName = "topic-kl";
        int partitions = 1;
        int replication = 1;
        AdminUtils.createTopic(zkClient,topicName,partitions,replication,new Properties());
    }
</code></pre> 
<p>注意下ZkClient最后一个构造入参，是一个序列化反序列化的接口实现，博主测试如果不填的话，创建的Topic在ZK上的数据是有问题的，默认的Kafka实现也很简单，就是做了字符串UTF-8编码处理。</p> 
<p><code>ZKStringSerializer$</code>是Kafka中已经实现好的一个接口实例，是一个Scala的伴生对象，在Java中直接调用点MODULE$就可以得到一个实例</p> 
<h4>命令方式创建</h4> 
<pre class="has"><code>    @Test
    public void testCreateTopic(){
        String [] options= new String[]{
                "--create",
                "--zookeeper","127.0.0.1:2181",
                "--replication-factor", "3",
                "--partitions", "3",
                "--topic", "topic-kl"
        };
        TopicCommand.main(options);
    }
</code></pre> 
<hr> 
<h3>消息发送之KafkaTemplate探秘</h3> 
<h4>获取发送结果</h4> 
<p><strong>异步获取</strong></p> 
<pre class="has"><code>        template.send("","").addCallback(new ListenableFutureCallback&lt;SendResult&lt;Object, Object&gt;&gt;() {
            @Override
            public void onFailure(Throwable throwable) {
                ......
            }

            @Override
            public void onSuccess(SendResult&lt;Object, Object&gt; objectObjectSendResult) {
                ....
            }
        });
</code></pre> 
<p><strong>同步获取</strong></p> 
<pre class="has"><code>        ListenableFuture&lt;SendResult&lt;Object,Object&gt;&gt; future = template.send("topic-kl","kl");
        try {
            SendResult&lt;Object,Object&gt; result = future.get();
        }catch (Throwable e){
            e.printStackTrace();
        }
</code></pre> 
<h4>kafka事务消息</h4> 
<p>默认情况下，Spring-kafka自动生成的KafkaTemplate实例，是不具有事务消息发送能力的。需要使用如下配置激活事务特性。事务激活后，所有的消息发送只能在发生事务的方法内执行了，不然就会抛一个没有事务交易的异常</p> 
<pre class="has"><code>spring.kafka.producer.transaction-id-prefix=kafka_tx.
</code></pre> 
<p>当发送消息有事务要求时，比如，当所有消息发送成功才算成功，如下面的例子：假设第一条消费发送后，在发第二条消息前出现了异常，那么第一条已经发送的消息也会回滚。</p> 
<p>而且正常情况下，假设在消息一发送后休眠一段时间，在发送第二条消息，消费端也只有在事务方法执行完成后才会接收到消息</p> 
<pre class="has"><code>    @GetMapping("/send/{input}")
    public void sendFoo(@PathVariable String input) {
        template.executeInTransaction(t -&gt;{
            t.send("topic_input","kl");
            if("error".equals(input)){
                throw new RuntimeException("failed");
            }
            t.send("topic_input","ckl");
            return true;
        });
    }
</code></pre> 
<p>当事务特性激活时，同样，在方法上面加<code>@Transactional</code>注解也会生效</p> 
<pre class="has"><code>    @GetMapping("/send/{input}")
    @Transactional(rollbackFor = RuntimeException.class)
    public void sendFoo(@PathVariable String input) {
        template.send("topic_input", "kl");
        if ("error".equals(input)) {
            throw new RuntimeException("failed");
        }
        template.send("topic_input", "ckl");
    }
</code></pre> 
<p>Spring-Kafka的事务消息是基于Kafka提供的事务消息功能的。而Kafka Broker默认的配置针对的三个或以上Broker高可用服务而设置的。这边在测试的时候为了简单方便，使用了嵌入式服务新建了一个单Broker的Kafka服务，出现了一些问题：如</p> 
<p><strong>1、事务日志副本集大于Broker数量，会抛如下异常：</strong></p> 
<blockquote> 
 <p>Number of alive brokers '1' does not meet the required replication factor '3'<br> for the transactions state topic (configured via 'transaction.state.log.replication.factor').<br> This error can be ignored if the cluster is starting up and not all brokers are up yet.</p> 
</blockquote> 
<p>默认Broker的配置<code>transaction.state.log.replication.factor=3</code>，单节点只能调整为1</p> 
<p><strong>2、副本数小于副本同步队列数目，会抛如下异常</strong></p> 
<blockquote> 
 <p>Number of insync replicas for partition __transaction_state-13 is [1], below required minimum [2]</p> 
</blockquote> 
<p>默认Broker的配置transaction.state.log.min.isr=2，单节点只能调整为1</p> 
<h4>ReplyingKafkaTemplate获得消息回复</h4> 
<p>ReplyingKafkaTemplate是KafkaTemplate的一个子类，除了继承父类的方法，新增了一个方法sendAndReceive，实现了消息发送\回复语义</p> 
<pre class="has"><code>RequestReplyFuture&lt;K, V, R&gt; sendAndReceive(ProducerRecord&lt;K, V&gt; record);
</code></pre> 
<p>也就是我发送一条消息，能够拿到消费者给我返回的结果。就像传统的RPC交互那样。当消息的发送者需要知道消息消费者的具体的消费情况，非常适合这个api。</p> 
<p>如，一条消息中发送一批数据，需要知道消费者成功处理了哪些数据。下面代码演示了怎么集成以及使用ReplyingKafkaTemplate</p> 
<pre class="has"><code>/**
 * @author: SpringRoot
 * @date: 2019/10/30
 */
@SpringBootApplication
@RestController
public class Application {
    private final Logger logger = LoggerFactory.getLogger(Application.class);
    public static void main(String[] args) {
        SpringApplication.run(Application.class, args);
    }
    @Bean
    public ConcurrentMessageListenerContainer&lt;String, String&gt; repliesContainer(ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; containerFactory) {
        ConcurrentMessageListenerContainer&lt;String, String&gt; repliesContainer = containerFactory.createContainer("replies");
        repliesContainer.getContainerProperties().setGroupId("repliesGroup");
        repliesContainer.setAutoStartup(false);
        return repliesContainer;
    }

    @Bean
    public ReplyingKafkaTemplate&lt;String, String, String&gt; replyingTemplate(ProducerFactory&lt;String, String&gt; pf, ConcurrentMessageListenerContainer&lt;String, String&gt; repliesContainer) {
        return new ReplyingKafkaTemplate(pf, repliesContainer);
    }

    @Bean
    public KafkaTemplate kafkaTemplate(ProducerFactory&lt;String, String&gt; pf) {
        return new KafkaTemplate(pf);
    }

    @Autowired
    private ReplyingKafkaTemplate template;

    @GetMapping("/send/{input}")
    @Transactional(rollbackFor = RuntimeException.class)
    public void sendFoo(@PathVariable String input) throws Exception {
        ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;("topic-kl", input);
        RequestReplyFuture&lt;String, String, String&gt; replyFuture = template.sendAndReceive(record);
        ConsumerRecord&lt;String, String&gt; consumerRecord = replyFuture.get();
        System.err.println("Return value: " + consumerRecord.value());
    }

    @KafkaListener(id = "webGroup", topics = "topic-kl")
    @SendTo
    public String listen(String input) {
        logger.info("input value: {}", input);
        return "successful";
    }
}
</code></pre> 
<hr> 
<h3>Spring-kafka消息消费用法探秘</h3> 
<h4>@KafkaListener的使用</h4> 
<p>前面在简单集成中已经演示过了<code>@KafkaListener</code>接收消息的能力，但是<code>@KafkaListener</code>的功能不止如此，其他的比较常见的，使用场景比较多的功能点如下：</p> 
<blockquote> 
 <ul><li>显示的指定消费哪些Topic和分区的消息，</li><li>设置每个Topic以及分区初始化的偏移量，</li><li>设置消费线程并发度</li><li>设置消息异常处理器</li></ul> 
</blockquote> 
<pre class="has"><code>    @KafkaListener(id = "webGroup", topicPartitions = {
            @TopicPartition(topic = "topic1", partitions = {"0", "1"}),
                    @TopicPartition(topic = "topic2", partitions = "0",
                            partitionOffsets = @PartitionOffset(partition = "1", initialOffset = "100"))
            },concurrency = "6",errorHandler = "myErrorHandler")
    public String listen(String input) {
        logger.info("input value: {}", input);
        return "successful";
    }
</code></pre> 
<p>其他的注解参数都很好理解，errorHandler需要说明下，设置这个参数需要实现一个接口<code>KafkaListenerErrorHandler</code>。而且注解里的配置，是你自定义实现实例在spring上下文中的Name。比如，上面配置为<code>errorHandler = "myErrorHandler"</code>。则在spring上线中应该存在这样一个实例：</p> 
<pre class="has"><code>/**
 * @author: SpringRoot
 * @date: 2019/10/31
 */
@Service("myErrorHandler")
public class MyKafkaListenerErrorHandler implements KafkaListenerErrorHandler {
    Logger logger =LoggerFactory.getLogger(getClass());
    @Override
    public Object handleError(Message&lt;?&gt; message, ListenerExecutionFailedException exception) {
        logger.info(message.getPayload().toString());
        return null;
    }
    @Override
    public Object handleError(Message&lt;?&gt; message, ListenerExecutionFailedException exception, Consumer&lt;?, ?&gt; consumer) {
        logger.info(message.getPayload().toString());
        return null;
    }
}
</code></pre> 
<h4>手动Ack模式</h4> 
<p>手动ACK模式，由业务逻辑控制提交偏移量。比如程序在消费时，有这种语义，特别异常情况下不确认ack，也就是不提交偏移量，那么你只能使用手动Ack模式来做了。开启手动首先需要关闭自动提交，然后设置下consumer的消费模式</p> 
<pre class="has"><code>spring.kafka.consumer.enable-auto-commit=false
spring.kafka.listener.ack-mode=manual
</code></pre> 
<p>上面的设置好后，在消费时，只需要在<code>@KafkaListener</code>监听方法的入参加入Acknowledgment 即可，执行到<code>ack.acknowledge()</code>代表提交了偏移量</p> 
<pre class="has"><code>    @KafkaListener(id = "webGroup", topics = "topic-kl")
    public String listen(String input, Acknowledgment ack) {
        logger.info("input value: {}", input);
        if ("kl".equals(input)) {
            ack.acknowledge();
        }
        return "successful";
    }
</code></pre> 
<h4>@KafkaListener注解监听器生命周期</h4> 
<p><code>@KafkaListener</code>注解的监听器的生命周期是可以控制的，默认情况下，<code>@KafkaListener</code>的参数<code>autoStartup = "true"</code>。也就是自动启动消费，但是也可以同过<code>KafkaListenerEndpointRegistry</code>来干预他的生命周期。</p> 
<p><code>KafkaListenerEndpointRegistry</code>有三个动作方法分别如：<code>start()</code>,<code>pause()</code>,<code>resume()</code>/启动，停止，继续。如下代码详细演示了这种功能。</p> 
<pre class="has"><code>/**
 * @author: SpringRoot
 * @date: 2019/10/30
 */
@SpringBootApplication
@RestController
public class Application {
    private final Logger logger = LoggerFactory.getLogger(Application.class);

    public static void main(String[] args) {
        SpringApplication.run(Application.class, args);
    }

    @Autowired
    private KafkaTemplate template;

    @GetMapping("/send/{input}")
    @Transactional(rollbackFor = RuntimeException.class)
    public void sendFoo(@PathVariable String input) throws Exception {
        ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;("topic-kl", input);
        template.send(record);
    }

    @Autowired
    private KafkaListenerEndpointRegistry registry;

    @GetMapping("/stop/{listenerID}")
    public void stop(@PathVariable String listenerID){
        registry.getListenerContainer(listenerID).pause();
    }
    @GetMapping("/resume/{listenerID}")
    public void resume(@PathVariable String listenerID){
        registry.getListenerContainer(listenerID).resume();
    }
    @GetMapping("/start/{listenerID}")
    public void start(@PathVariable String listenerID){
        registry.getListenerContainer(listenerID).start();
    }
    @KafkaListener(id = "webGroup", topics = "topic-kl",autoStartup = "false")
    public String listen(String input) {
        logger.info("input value: {}", input);
        return "successful";
    }
}
</code></pre> 
<p>在上面的代码中，listenerID就是@KafkaListener中的id值“webGroup”。项目启动好后，分别执行如下url，就可以看到效果了。</p> 
<p>先发送一条消息：http://localhost:8081/send/ckl。因为autoStartup = "false"，所以并不会看到有消息进入监听器。</p> 
<p>接着启动监听器：http://localhost:8081/start/webGroup。可以看到有一条消息进来了。</p> 
<p>暂停和继续消费的效果使用类似方法就可以测试出来了。</p> 
<h4>SendTo消息转发</h4> 
<p>前面的消息发送响应应用里面已经见过@SendTo,其实除了做发送响应语义外，@SendTo注解还可以带一个参数，指定转发的Topic队列。</p> 
<p>常见的场景如，一个消息需要做多重加工，不同的加工耗费的cup等资源不一致，那么就可以通过跨不同Topic和部署在不同主机上的consumer来解决了。如：</p> 
<pre class="has"><code>    @KafkaListener(id = "webGroup", topics = "topic-kl")
    @SendTo("topic-ckl")
    public String listen(String input) {
        logger.info("input value: {}", input);
        return input + "hello!";
    }

    @KafkaListener(id = "webGroup1", topics = "topic-ckl")
    public void listen2(String input) {
        logger.info("input value: {}", input);
    }
</code></pre> 
<h4>消息重试和死信队列的应用</h4> 
<p>除了上面谈到的通过手动Ack模式来控制消息偏移量外，其实Spring-kafka内部还封装了可重试消费消息的语义，也就是可以设置为当消费数据出现异常时，重试这个消息。而且可以设置重试达到多少次后，让消息进入预定好的Topic。也就是死信队列里。</p> 
<p>下面代码演示了这种效果：</p> 
<pre class="has"><code>    @Autowired
    private KafkaTemplate template;

    @Bean
    public ConcurrentKafkaListenerContainerFactory&lt;?, ?&gt; kafkaListenerContainerFactory(
            ConcurrentKafkaListenerContainerFactoryConfigurer configurer,
            ConsumerFactory&lt;Object, Object&gt; kafkaConsumerFactory,
            KafkaTemplate&lt;Object, Object&gt; template) {
        ConcurrentKafkaListenerContainerFactory&lt;Object, Object&gt; factory = new ConcurrentKafkaListenerContainerFactory&lt;&gt;();
        configurer.configure(factory, kafkaConsumerFactory);
        //最大重试三次
        factory.setErrorHandler(new SeekToCurrentErrorHandler(new DeadLetterPublishingRecoverer(template), 3));
        return factory;
    }

    @GetMapping("/send/{input}")
    public void sendFoo(@PathVariable String input) {
        template.send("topic-kl", input);
    }

    @KafkaListener(id = "webGroup", topics = "topic-kl")
    public String listen(String input) {
        logger.info("input value: {}", input);
        throw new RuntimeException("dlt");
    }

    @KafkaListener(id = "dltGroup", topics = "topic-kl.DLT")
    public void dltListen(String input) {
        logger.info("Received from DLT: " + input);
    }
</code></pre> 
<p>上面应用，在topic-kl监听到消息会，会触发运行时异常，然后监听器会尝试三次调用，当到达最大的重试次数后。消息就会被丢掉重试死信队列里面去。死信队列的Topic的规则是，业务Topic名字+“.DLT”。</p> 
<p>如上面业务Topic的name为“topic-kl”，那么对应的死信队列的Topic就是“topic-kl.DLT”</p> 
<hr> 
<h3>文末结语</h3> 
<p>之前业务上使用了kafka用到了Spring-kafka，所以系统性的探索了下Spring-kafka的各种用法，发现了很多好玩很酷的特性，比如，一个注解开启嵌入式的Kafka服务、像RPC调用一样的发送\响应语义调用、事务消息等功能。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/77cefc942388dda71bb0803f47e04a92/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Navicat Premium 使用数据泵向 ORALE 数据库中导入DMP文件</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/45834db7fea4cf03fed1f59de1411c14/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">linux中 &gt; 、&gt;&gt; 的用法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>