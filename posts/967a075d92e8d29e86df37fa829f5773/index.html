<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>深度学习总结（一）各种优化算法 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="深度学习总结（一）各种优化算法" />
<meta property="og:description" content="参考博文： 码农王小呆：https://blog.csdn.net/manong_wxd/article/details/78735439 深度学习最全优化方法总结： https://blog.csdn.net/u012759136/article/details/52302426 超级详细每个算法的讲解，可参考： https://blog.csdn.net/tsyccnh/article/details/76673073
一.优化算法介绍 1.批量梯度下降（Batch gradient descent，BGD） θ=θ−η⋅∇θJ(θ) 每迭代一步，都要用到训练集的所有数据，每次计算出来的梯度求平均 η代表学习率LR
2.随机梯度下降（Stochastic Gradient Descent，SGD） θ=θ−η⋅∇θJ(θ;x(i);y(i)) 通过每个样本来迭代更新一次，以损失很小的一部分精确度和增加一定数量的迭代次数为代价，换取了总体的优化效率的提升。增加的迭代次数远远小于样本的数量。
缺点：
对于参数比较敏感，需要注意参数的初始化 容易陷入局部极小值 当数据较多时，训练时间长 每迭代一步，都要用到训练集所有的数据。
3. 小批量梯度下降（Mini Batch Gradient Descent，MBGD） θ=θ−η⋅∇θJ(θ;x(i:i&#43;n);y(i:i&#43;n)) 为了避免SGD和标准梯度下降中存在的问题，对每个批次中的n个训练样本，这种方法只执行一次更新。【每次更新全部梯度的平均值】
4.指数加权平均的概念 从这里我们就可已看出指数加权平均的名称由来，第100个数据其实是前99个数据加权和，而前面每一个数的权重呈现指数衰减，即越靠前的数据对当前结果的影响较小 缺点：存在开始数据的过低问题，可以通过偏差修正，但是在深度学习的优化算法中一般会忽略这个问题 当t不断增大时，分母逐渐接近1，影响就会逐渐减小了
优点：【相较于滑动窗口平均】 1.占用内存小，每次覆盖即可 2.运算简单
5.Momentum（动量梯度下降法） momentum是模拟物理里动量的概念，积累之前的动量来替代真正的梯度。公式如下： 然而网上更多的是另外一种版本，即去掉（1-β） 相当于上一版本上本次梯度的影响权值*1/(1-β) 两者效果相当，只不过会影响一些最优学习率的选取 优点
下降初期时，使用上一次参数更新，下降方向一致，乘上较大的μ能够进行很好的加速下降中后期时，在局部最小值来回震荡的时候，gradient→0，β得更新幅度增大，跳出陷阱在梯度改变方向的时候，μ能够减少更新 即在正确梯度方向上加速，并且抑制波动方向张的波动大小，在后期本次计算出来的梯度会很小，以至于无法跳出局部极值，Momentum方法也可以帮助跳出局部极值 参数设置 β的常用值为0.9，即可以一定意义上理解为平均了前10/9次的梯度。 至于LR学习率的设置，后面所有方法一起总结吧
6.Nesterov accelerated gradient (NAG) 优点： 这种基于预测的更新方法，使我们避免过快地前进，并提高了算法地响应能力，大大改进了 RNN 在一些任务上的表现【为什么对RNN好呢，不懂啊】 没有对比就没有伤害，NAG方法收敛速度明显加快。波动也小了很多。实际上NAG方法用到了二阶信息，所以才会有这么好的结果。先按照原来的梯度走一步的时候已经求了一次梯度，后面再修正的时候又求了一次梯度，所以是二阶信息。 参数设置： 同Momentum
其实，momentum项和nesterov项都是为了使梯度更新更加灵活，对不同情况有针对性。但是，人工设置一些学习率总还是有些生硬，接下来介绍几种自适应学习率的方法
7.Adagrad 前面的一系列优化算法有一个共同的特点，就是对于每一个参数都用相同的学习率进行更新。但是在实际应用中各个参数的重要性肯定是不一样的，所以我们对于不同的参数要动态的采取不同的学习率，让目标函数更快的收敛。 adagrad方法是将每一个参数的每一次迭代的梯度取平方累加再开方，用基础学习率除以这个数，来做学习率的动态更新。【这样每一个参数的学习率就与他们的梯度有关系了，那么每一个参数的学习率就不一样了！也就是所谓的自适应学习率】 优点：
前期Gt较小的时候， regularizer较大，能够放大梯度后期Gt较大的时候，regularizer较小，能够约束梯度适合处理稀疏梯度:相当于为每一维参数设定了不同的学习率：压制常常变化的参数，突出稀缺的更新。能够更有效地利用少量有意义样本 参数设置： 只需要设置初始学习率，后面学习率会自我调整，越来越小
缺点： Adagrad的一大优势时可以避免手动调节学习率，比如设置初始的缺省学习率为0." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/967a075d92e8d29e86df37fa829f5773/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-07-03T20:04:15+08:00" />
<meta property="article:modified_time" content="2018-07-03T20:04:15+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深度学习总结（一）各种优化算法</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>参考博文： <br> 码农王小呆：<a href="https://blog.csdn.net/manong_wxd/article/details/78735439">https://blog.csdn.net/manong_wxd/article/details/78735439</a> <br> 深度学习最全优化方法总结： <br> <a href="https://blog.csdn.net/u012759136/article/details/52302426">https://blog.csdn.net/u012759136/article/details/52302426</a> <br> 超级详细每个算法的讲解，可参考： <br> <a href="https://blog.csdn.net/tsyccnh/article/details/76673073">https://blog.csdn.net/tsyccnh/article/details/76673073</a></p> 
<h3 id="一优化算法介绍">一.优化算法介绍</h3> 
<h4 id="1批量梯度下降batch-gradient-descentbgd">1.批量梯度下降（Batch gradient descent，BGD）</h4> 
<p>θ=θ−η⋅∇θJ(θ) <br> 每迭代一步，都要用到训练集的所有数据，每次计算出来的梯度求平均 <br> η代表学习率LR</p> 
<h4 id="2随机梯度下降stochastic-gradient-descentsgd">2.随机梯度下降（Stochastic Gradient Descent，SGD）</h4> 
<p>θ=θ−η⋅∇θJ(θ;x(i);y(i)) <br> 通过每个样本来迭代更新一次，以损失很小的一部分精确度和增加一定数量的迭代次数为代价，换取了总体的优化效率的提升。增加的迭代次数远远小于样本的数量。</p> 
<p><strong>缺点：</strong></p> 
<p>对于参数比较敏感，需要注意参数的初始化 <br> 容易陷入局部极小值 <br> 当数据较多时，训练时间长 <br> 每迭代一步，都要用到训练集所有的数据。</p> 
<h4 id="3-小批量梯度下降mini-batch-gradient-descentmbgd">3. 小批量梯度下降（Mini Batch Gradient Descent，MBGD）</h4> 
<p>θ=θ−η⋅∇θJ(θ;x(i:i+n);y(i:i+n)) <br> 为了避免SGD和标准梯度下降中存在的问题，对每个批次中的n个训练样本，这种方法只执行一次更新。【每次更新全部梯度的平均值】</p> 
<h4 id="4指数加权平均的概念">4.指数加权平均的概念</h4> 
<p><img src="https://images2.imgbox.com/63/05/3v2AFkiz_o.png" alt="这里写图片描述" title=""> <br> <img src="https://images2.imgbox.com/22/81/HUo7HWX1_o.png" alt="这里写图片描述" title=""> <br> 从这里我们就可已看出指数加权平均的名称由来，第100个数据其实是前99个数据加权和，而前面每一个数的权重呈现指数衰减，即越靠前的数据对当前结果的影响较小 <br> <img src="https://images2.imgbox.com/08/d1/AX0YS50b_o.png" alt="这里写图片描述" title=""></p> 
<p><strong>缺点：</strong>存在开始数据的过低问题，可以通过偏差修正，但是在深度学习的优化算法中一般会忽略这个问题 <br> <img src="https://images2.imgbox.com/24/93/JHVG2BUv_o.png" alt="这里写图片描述" title=""> <br> 当t不断增大时，分母逐渐接近1，影响就会逐渐减小了</p> 
<p><strong>优点：</strong>【相较于滑动窗口平均】 <br> 1.占用内存小，每次覆盖即可 <br> 2.运算简单</p> 
<h4 id="5momentum动量梯度下降法">5.Momentum（动量梯度下降法）</h4> 
<p>momentum是模拟物理里动量的概念，积累之前的动量来替代真正的梯度。公式如下： <br> <img src="https://images2.imgbox.com/87/6e/sf2ivEGi_o.png" alt="这里写图片描述" title=""> <br> 然而网上更多的是另外一种版本，即去掉（1-β） <br> <img src="https://images2.imgbox.com/bd/31/esDY2ORU_o.gif" alt="这里写图片描述" title=""> <br> 相当于上一版本上本次梯度的影响权值*1/(1-β) <br> 两者效果相当，只不过会影响一些最优学习率的选取 <br> <strong>优点</strong></p> 
<ul><li>下降初期时，使用上一次参数更新，下降方向一致，乘上较大的μ能够进行很好的加速</li><li>下降中后期时，在局部最小值来回震荡的时候，gradient→0，β得更新幅度增大，跳出陷阱</li><li>在梯度改变方向的时候，μ能够减少更新</li></ul> 
<p>即在正确梯度方向上加速，并且抑制波动方向张的波动大小，在后期本次计算出来的梯度会很小，以至于无法跳出局部极值，Momentum方法也可以帮助跳出局部极值 <br> <strong>参数设置</strong> <br> β的常用值为0.9，即可以一定意义上理解为平均了前10/9次的梯度。 <br> 至于LR学习率的设置，后面所有方法一起总结吧</p> 
<h4 id="6nesterov-accelerated-gradient-nag">6.Nesterov accelerated gradient (NAG)</h4> 
<p><img src="https://images2.imgbox.com/22/3c/jkhKRmwj_o.jpg" alt="Momentum图解" title=""> <br> <img src="https://images2.imgbox.com/0c/67/mUBnBT7S_o.jpg" alt="NAG图解" title=""> <br> <img src="https://images2.imgbox.com/2e/1c/rQcubArn_o.png" alt="这里写图片描述" title=""> <br> <strong>优点：</strong> <br> 这种基于预测的更新方法，使我们<font color="red">避免过快地前进</font>，并提高了算法地响应能力，大大改进了 RNN 在一些任务上的表现【为什么对RNN好呢，不懂啊】 <br> 没有对比就没有伤害，NAG方法收敛速度明显加快。波动也小了很多。实际上NAG方法用到了<font color="red">二阶信息</font>，所以才会有这么好的结果。<em>先按照原来的梯度走一步的时候已经求了一次梯度，后面再修正的时候又求了一次梯度，所以是二阶信息。</em> <br> <strong>参数设置：</strong> <br> 同Momentum</p> 
<p>其实，momentum项和nesterov项都是为了使梯度更新更加灵活，对不同情况有针对性。但是，人工设置一些学习率总还是有些生硬，接下来介绍几种<strong>自适应学习率</strong>的方法</p> 
<h4 id="7adagrad">7.Adagrad</h4> 
<p>前面的一系列优化算法有一个共同的特点，就是对于每一个参数都用相同的学习率进行更新。但是在实际应用中各个参数的重要性肯定是不一样的，所以我们对于不同的参数要动态的采取不同的学习率，让目标函数更快的收敛。 <br> adagrad方法是将每一个参数的每一次迭代的梯度取平方累加再开方，用基础学习率除以这个数，来做学习率的动态更新。【这样每一个参数的学习率就与他们的梯度有关系了，那么每一个参数的学习率就不一样了！也就是所谓的<strong>自适应学习率</strong>】 <br> <img src="https://images2.imgbox.com/95/f8/iv05HVl5_o.png" alt="这里写图片描述" title=""></p> 
<p><strong>优点：</strong></p> 
<ul><li>前期Gt较小的时候， regularizer较大，能够放大梯度</li><li>后期Gt较大的时候，regularizer较小，能够约束梯度</li><li>适合处理稀疏梯度:相当于为每一维参数设定了不同的学习率：压制常常变化的参数，突出稀缺的更新。能够更有效地利用少量有意义样本</li></ul> 
<p><strong>参数设置：</strong> <br> 只需要设置初始学习率，后面学习率会自我调整，越来越小</p> 
<p><strong>缺点：</strong> <br> Adagrad的一大优势时可以避免手动调节学习率，比如设置初始的缺省学习率为0.01，然后就不管它，另其在学习的过程中自己变化。当然它也有缺点，就是它计算时要在分母上计算梯度平方的和，由于所有的<font color="red">参数平方【上述公式推导中并没有写出来是梯度的平方，感觉应该是上文的公式推导忘了写】</font>必为正数，这样就造成在训练的过程中，分母累积的和会越来越大。这样学习到后来的阶段，网络的更新能力会越来越弱，能学到的更多知识的能力也越来越弱，因为学习率会变得极其小【就会提前停止学习】，为了解决这样的问题又提出了Adadelta算法。</p> 
<h4 id="8adadelta">8.Adadelta</h4> 
<p>Adagrad会累加之前所有的梯度平方，而Adadelta只累加固定大小的项【其实就是相当于指数滑动平均，只用了前多少步的梯度平方平均值】，并且也不直接存储这些项，仅仅是近似计算对应的平均值【这也就是指数滑动平均的优点】 <br> <img src="https://images2.imgbox.com/1e/64/X5xaPcvE_o.png" alt="这里写图片描述" title=""> <br> <strong>优点：</strong> <br> 不用依赖于全局学习率了 <br> 训练初中期，加速效果不错，很快 <br> 避免参数更新时两边单位不统一的问题 <br> <strong>缺点：</strong> <br> 训练后期，反复在局部最小值附近抖动</p> 
<h4 id="9rmsprop">9.RMSprop</h4> 
<p><img src="https://images2.imgbox.com/33/e2/xeUeenBQ_o.png" alt="这里写图片描述" title=""> <br> <strong>特点：</strong></p> 
<ul><li>其实RMSprop依然依赖于全局学习率</li><li>RMSprop算是Adagrad的一种发展，和Adadelta的变体，效果趋于二者之间</li><li>适合处理非平稳目标（也就是与时间有关的）</li><li>对于RNN效果很好，因为RMSprop的更新只依赖于上一时刻的更新，所以适合。？？？</li></ul> 
<h4 id="10adam">10.Adam</h4> 
<p>Adam = Adaptive + Momentum，顾名思义Adam集成了SGD的一阶动量和RMSProp的二阶动量。 <br> <img src="https://images2.imgbox.com/8e/ec/nv5nMMtL_o.png" alt="这里写图片描述" title=""> <br> <strong>特点：</strong></p> 
<ul><li>结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点</li><li>对内存需求较小</li><li>为不同的参数计算不同的自适应学习率</li><li>也适用于大多非凸优化</li><li>适用于大数据集和高维空间</li></ul> 
<h4 id="11adamax">11.Adamax</h4> 
<p><img src="https://images2.imgbox.com/c3/33/UEXe9swb_o.png" alt="这里写图片描述" title=""></p> 
<h4 id="12nadam">12.Nadam</h4> 
<p><img src="https://images2.imgbox.com/fb/7f/0e86QETn_o.png" alt="这里写图片描述" title=""></p> 
<h4 id="13总结">13.总结</h4> 
<p>提速可以归纳为以下几个方面： <br> - 使用momentum来保持前进方向(velocity)； <br> - 为每一维参数设定不同的学习率：在梯度连续性强的方向上加速前进； <br> - 用历史迭代的平均值归一化学习率：突出稀有的梯度；</p> 
<h4 id="keras中的默认参数">Keras中的默认参数</h4> 
<pre class="prettyprint"><code class=" hljs avrasm">optimizers<span class="hljs-preprocessor">.SGD</span>(lr=<span class="hljs-number">0.001</span>,momentum=<span class="hljs-number">0.9</span>)

optimizers<span class="hljs-preprocessor">.Adagrad</span>(lr=<span class="hljs-number">0.01</span>,epsilon=<span class="hljs-number">1e-8</span>)

optimizers<span class="hljs-preprocessor">.Adadelta</span>(lr=<span class="hljs-number">0.01</span>,rho=<span class="hljs-number">0.95</span>,epsilon=<span class="hljs-number">1e-8</span>)

optimizers<span class="hljs-preprocessor">.RMSprop</span>(lr=<span class="hljs-number">0.001</span>,rho=<span class="hljs-number">0.9</span>,epsilon=<span class="hljs-number">1e-8</span>)

optimizers<span class="hljs-preprocessor">.Adam</span>(lr=<span class="hljs-number">0.001</span>,beta_1=<span class="hljs-number">0.9</span>,beta_2=<span class="hljs-number">0.999</span>,epsilon=<span class="hljs-number">1e-8</span>)</code></pre> 
<h4 id="14牛顿法二阶优化方法待补充">14.牛顿法——二阶优化方法【待补充】</h4> 
<h3 id="二相关注意问题">二.相关注意问题</h3> 
<h4 id="1关于批量梯度下降的batchsize选择问题">1.关于批量梯度下降的batch_size选择问题</h4> 
<p><strong>训练集较小【&lt;2000】</strong>：直接使用batch梯度下降，每次用全部的样本进行梯度更新 <br> <strong>训练集较大</strong>：batch_size一般设定为[64,512]之间，设置为2的n次方更符合电脑内存设置，代码会运行快一些 <br> 此外还要考虑GPU和CPU的存储空间和训练过程的波动问题 <br> <img src="https://images2.imgbox.com/e3/5e/LfQvQbR5_o.png" alt="这里写图片描述" title=""> <br> batch_size越小，梯度的波动越大，正则化的效果也越强，自然训练速度也会变慢，实验时应该多选择几个batch_size进行实验，以挑选出最优的模型。</p> 
<h4 id="2关于批量梯度下降的weightdecay似乎与l2正则有关系待补充">2.关于批量梯度下降的weight_decay【似乎与L2正则有关系，待补充】</h4> 
<h4 id="3关于优化算法选择的经验之谈">3.关于优化算法选择的经验之谈</h4> 
<ol><li>Adam在实际应用中效果良好，超过了其他的自适应技术。</li><li>如果输入数据集比较稀疏，SGD、NAG和动量项等方法可能效果不好。因此对于稀疏数据集，应该使用某种自适应学习率的方法，且另一好处为不需要人为调整学习率，使用默认参数就可能获得最优值。</li><li>如果想使训练深层网络模型快速收敛或所构建的神经网络较为复杂，则应该使用Adam或其他自适应学习速率的方法，因为这些方法的实际效果更优。</li><li>SGD通常训练时间更长，但是在好的初始化和学习率调度方案的情况下，结果更可靠。</li><li>Adadelta，RMSprop，Adam是比较相近的算法，在相似的情况下表现差不多。在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果</li></ol> 
<h4 id="4训练优化器的目的">4.训练优化器的目的</h4> 
<p>加速收敛 2. 防止过拟合 3. 防止局部最优</p> 
<h4 id="5选用优化器的目的">5.选用优化器的目的</h4> 
<p>在构建神经网络模型时，选择出最佳的优化器，以便快速收敛并正确学习，同时调整内部参数，最大程度地最小化损失函数。</p> 
<h4 id="6为什么神经网络的训练不采用二阶优化方法-如newton-quasi-newton">6.为什么神经网络的训练不采用二阶优化方法 (如Newton, Quasi Newton)？</h4> 
<p><img src="https://images2.imgbox.com/8f/e9/ROWcJbJf_o.png" alt="这里写图片描述" title=""></p> 
<h4 id="7-优化sgd的其他手段">7. 优化SGD的其他手段</h4> 
<p><img src="https://images2.imgbox.com/98/c5/hj57ZRW5_o.png" alt="这里写图片描述" title=""> <br> <img src="https://images2.imgbox.com/eb/78/XjOUsj4Q_o.gif" alt="这里写图片描述" title=""> <br> <img src="https://images2.imgbox.com/c3/b4/VtcuB28h_o.gif" alt="这里写图片描述" title=""></p> 
<h3 id="附录">附录</h3> 
<p><img src="https://images2.imgbox.com/23/c8/zIZtQ8AQ_o.png" alt="这里写图片描述" title=""></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d7b34d16ab2d6ab597df1bf4352540a9/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">如何判断一个变量是有符号数还是无符号数？</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1d757f2751ab0a3eb8f056c006a686cc/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">冒泡排序、选择排序和插入排序的区别</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>