<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>深度学习模型压缩方法综述（一） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="深度学习模型压缩方法综述（一）" />
<meta property="og:description" content="版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。 本文链接： https://blog.csdn.net/wspba/article/details/75671573 深度学习模型压缩方法综述（一） 深度学习模型压缩方法综述（二） 深度学习模型压缩方法综述（三）
前言 目前在深度学习领域分类两个派别，一派为学院派，研究强大、复杂的模型网络和实验方法，为了追求更高的性能；另一派为工程派，旨在将算法更稳定、高效的落地在硬件平台上，效率是其追求的目标。复杂的模型固然具有更好的性能，但是高额的存储空间、计算资源消耗是使其难以有效的应用在各硬件平台上的重要原因。
最近正好在关注有关深度学习模型压缩的方法，发现目前已有越来越多关于模型压缩方法的研究，从理论研究到平台实现，取得了非常大的进展。
2015年，Han发表的Deep Compression是一篇对于模型压缩方法的综述型文章，将裁剪、权值共享和量化、编码等方式运用在模型压缩上，取得了非常好的效果，作为ICLR2016的best paper，也引起了模型压缩方法研究的热潮。其实模型压缩最早可以追溯到1989年，Lecun老爷子的那篇Optimal Brain Damage（OBD）就提出来，可以将网络中不重要的参数剔除，达到压缩尺寸的作用，想想就可怕，那时候连个深度网络都训练不出来，更没有现在这么发达的技术，Lecun就已经想好怎么做裁剪了，真是有先见之明，目前很多裁剪方案，都是基于老爷子的OBD方法。
目前深度学习模型压缩方法的研究主要可以分为以下几个方向： 更精细模型的设计，目前的很多网络都具有模块化的设计，在深度和宽度上都很大，这也造成了参数的冗余很多，因此有很多关于模型设计的研究，如SqueezeNet、MobileNet等，使用更加细致、高效的模型设计，能够很大程度的减少模型尺寸，并且也具有不错的性能。 模型裁剪，结构复杂的网络具有非常好的性能，其参数也存在冗余，因此对于已训练好的模型网络，可以寻找一种有效的评判手段，将不重要的connection或者filter进行裁剪来减少模型的冗余。 核的稀疏化，在训练过程中，对权重的更新进行诱导，使其更加稀疏，对于稀疏矩阵，可以使用更加紧致的存储方式，如CSC，但是使用稀疏矩阵操作在硬件平台上运算效率不高，容易受到带宽的影响，因此加速并不明显。 除此之外，量化、Low-rank分解、迁移学习等方法也有很多研究，并在模型压缩中起到了非常好的效果。
基于核的稀疏化方法 核的稀疏化，是在训练过程中，对权重的更新加以正则项进行诱导，使其更加稀疏，使大部分的权值都为0。核的稀疏化方法分为regular和irregular，regular的稀疏化后，裁剪起来更加容易，尤其是对im2col的矩阵操作，效率更高；而irregular的稀疏化后，参数需要特定的存储方式，或者需要平台上稀疏矩阵操作库的支持，可以参考的论文有：
Learning Structured Sparsity in Deep Neural Networks 论文地址 本文作者提出了一种Structured Sparsity Learning的学习方式，能够学习一个稀疏的结构来降低计算消耗，所学到的结构性稀疏化能够有效的在硬件上进行加速。 传统非结构化的随机稀疏化会带来不规则的内存访问，因此在GPU等硬件平台上无法有效的进行加速。 作者在网络的目标函数上增加了group lasso的限制项，可以实现filter级与channel级以及shape级稀疏化。所有稀疏化的操作都是基于下面的loss func进行的，其中Rg为group lasso： 则filter-channel wise： 而shape wise： 由于在GEMM中将weight tensor拉成matrix的结构，因此可以通过将filter级与shape级的稀疏化进行结合来将2D矩阵的行和列稀疏化，再分别在矩阵的行和列上裁剪掉剔除全为0的值可以来降低矩阵的维度从而提升模型的运算效率。该方法是regular的方法，压缩粒度较粗，可以适用于各种现成的算法库，但是训练的收敛性和优化难度不确定。作者的源码为：https://github.com/wenwei202/caffe/tree/scnn
Dynamic Network Surgery for Efficient DNNs 论文地址 作者提出了一种动态的模型裁剪方法，包括以下两个过程：pruning和splicing，其中pruning就是将认为不中要的weight裁掉，但是往往无法直观的判断哪些weight是否重要，因此在这里增加了一个splicing的过程，将哪些重要的被裁掉的weight再恢复回来，类似于一种外科手术的过程，将重要的结构修补回来，它的算法如下： 作者通过在W上增加一个T来实现，T为一个2值矩阵，起到的相当于一个mask的功能，当某个位置为1时，将该位置的weight保留，为0时，裁剪。在训练过程中通过一个可学习mask将weight中真正不重要的值剔除，从而使得weight变稀疏。由于在删除一些网络的连接，会导致网络其他连接的重要性发生改变，所以通过优化最小损失函数来训练删除后的网络比较合适。 优化问题表达如下： 参数迭代如下： 其中用于表示网络连接的重要性 h 函数定义如下： 该算法采取了剪枝与嫁接相结合、训练与压缩相同步的策略完成网络压缩任务。通过网络嫁接操作的引入，避免了错误剪枝所造成的性能损失，从而在实际操作中更好地逼近网络压缩的理论极限。属于irregular的方式，但是ak和bk的值在不同的模型以及不同的层中无法确定，并且容易受到稀疏矩阵算法库以及带宽的限制。论文源码：https://github.com/yiwenguo/Dynamic-Network-Surgery
Training Skinny Deep Neural Networks with Iterative Hard Thresholding Methods 论文地址 作者想通过训练一个稀疏度高的网络来降低模型的运算量，通过在网络的损失函数中增加一个关于W的L0范式可以降低W的稀疏度，但是L0范式就导致这是一个N-P难题，是一个难优化求解问题，因此作者从另一个思路来训练这个稀疏化的网络。算法的流程如下： 先正常训练网络s1轮，然后Ok(W)表示选出W中数值最大的k个数，而将剩下的值置为0，supp(W,k)表示W中最大的k个值的序号，继续训练s2轮，仅更新非0的W，然后再将之前置为0的W放开进行更新，继续训练s1轮，这样反复直至训练完毕。 同样也是对参数进行诱导的方式，边训练边裁剪，先将认为不重要的值裁掉，再通过一个restore的过程将重要却被误裁的参数恢复回来。也是属于irregular的方式，边训边裁，性能不错，压缩的力度难以保证。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/35c8d6af6e90dc3d1cea6f19572a55e9/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-09-24T10:08:19+08:00" />
<meta property="article:modified_time" content="2019-09-24T10:08:19+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深度学习模型压缩方法综述（一）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-kimbie-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <div id="article_content" class="article_content clearfix"> 
 <div class="article-copyright"> 
  <span class="creativecommons"> <a rel="nofollow" href="http://creativecommons.org/licenses/by-sa/4.0/"> </a>  版权声明：本文为博主原创文章，遵循<a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="noopener noopener noreferrer" target="_blank"> CC 4.0 BY-SA </a>版权协议，转载请附上原文出处链接和本声明。  </span> 
  <div class="article-source-link2222">
    本文链接： 
   <a href="https://blog.csdn.net/wspba/article/details/75671573">https://blog.csdn.net/wspba/article/details/75671573</a> 
  </div> 
 </div> 
</div> 
<p><a href="http://blog.csdn.net/wspba/article/details/75671573" rel="nofollow">深度学习模型压缩方法综述（一）</a> <br> <a href="http://blog.csdn.net/wspba/article/details/75675554" rel="nofollow">深度学习模型压缩方法综述（二）</a> <br> <a href="http://blog.csdn.net/wspba/article/details/76039135" rel="nofollow">深度学习模型压缩方法综述（三）</a></p> 
<h3 id="前言">前言</h3> 
<p>目前在深度学习领域分类两个派别，一派为学院派，研究强大、复杂的模型网络和实验方法，为了追求更高的性能；另一派为工程派，旨在将算法更稳定、高效的落地在硬件平台上，效率是其追求的目标。复杂的模型固然具有更好的性能，但是高额的存储空间、计算资源消耗是使其难以有效的应用在各硬件平台上的重要原因。</p> 
<p>最近正好在关注有关深度学习模型压缩的方法，发现目前已有越来越多关于模型压缩方法的研究，从理论研究到平台实现，取得了非常大的进展。</p> 
<p>2015年，Han发表的<a href="https://arxiv.org/abs/1510.00149" rel="nofollow noopener noreferrer" target="_blank">Deep Compression</a>是一篇对于模型压缩方法的综述型文章，将裁剪、权值共享和量化、编码等方式运用在模型压缩上，取得了非常好的效果，作为ICLR2016的best paper，也引起了模型压缩方法研究的热潮。其实模型压缩最早可以追溯到1989年，Lecun老爷子的那篇<a href="http://papers.nips.cc/paper/250-optimal-brain-damage.pdf" rel="nofollow noopener noreferrer" target="_blank">Optimal Brain Damage（OBD）</a>就提出来，可以将网络中不重要的参数剔除，达到压缩尺寸的作用，想想就可怕，那时候连个深度网络都训练不出来，更没有现在这么发达的技术，Lecun就已经想好怎么做裁剪了，真是有先见之明，目前很多裁剪方案，都是基于老爷子的OBD方法。</p> 
<p>目前深度学习模型压缩方法的研究主要可以分为以下几个方向： <br> <strong>更精细模型的设计</strong>，目前的很多网络都具有模块化的设计，在深度和宽度上都很大，这也造成了参数的冗余很多，因此有很多关于模型设计的研究，如SqueezeNet、MobileNet等，使用更加细致、高效的模型设计，能够很大程度的减少模型尺寸，并且也具有不错的性能。 <br> <strong>模型裁剪</strong>，结构复杂的网络具有非常好的性能，其参数也存在冗余，因此对于已训练好的模型网络，可以寻找一种有效的评判手段，将不重要的connection或者filter进行裁剪来减少模型的冗余。 <br> <strong>核的稀疏化</strong>，在训练过程中，对权重的更新进行诱导，使其更加稀疏，对于稀疏矩阵，可以使用更加紧致的存储方式，如CSC，但是使用稀疏矩阵操作在硬件平台上运算效率不高，容易受到带宽的影响，因此加速并不明显。 <br> 除此之外，<strong>量化</strong>、<strong>Low-rank分解</strong>、<strong>迁移学习</strong>等方法也有很多研究，并在模型压缩中起到了非常好的效果。</p> 
<h3 id="基于核的稀疏化方法">基于核的稀疏化方法</h3> 
<p>核的稀疏化，是在训练过程中，对权重的更新加以正则项进行诱导，使其更加稀疏，使大部分的权值都为0。核的稀疏化方法分为regular和irregular，regular的稀疏化后，裁剪起来更加容易，尤其是对im2col的矩阵操作，效率更高；而irregular的稀疏化后，参数需要特定的存储方式，或者需要平台上稀疏矩阵操作库的支持，可以参考的论文有：</p> 
<ul><li><p>Learning Structured Sparsity in Deep Neural Networks <a href="http://papers.nips.cc/paper/6504-learning-structured-sparsity-in-deep-neural-networks.pdf" rel="nofollow noopener noreferrer" target="_blank">论文地址</a> <br> 本文作者提出了一种Structured Sparsity Learning的学习方式，能够学习一个稀疏的结构来降低计算消耗，所学到的结构性稀疏化能够有效的在硬件上进行加速。 传统非结构化的随机稀疏化会带来不规则的内存访问，因此在GPU等硬件平台上无法有效的进行加速。 作者在网络的目标函数上增加了group lasso的限制项，可以实现filter级与channel级以及shape级稀疏化。所有稀疏化的操作都是基于下面的loss func进行的，其中Rg为group lasso： <br> <img src="https://images2.imgbox.com/e5/c4/t6o1Hvt2_o.png" alt="这里写图片描述" title=""> <br> 则filter-channel wise： <br> <img src="https://images2.imgbox.com/82/dd/8hwD8xBw_o.png" alt="这里写图片描述" title=""> <br> 而shape wise： <br> <img src="https://images2.imgbox.com/81/71/AFzsZDTx_o.png" alt="这里写图片描述" title=""> <br> 由于在GEMM中将weight tensor拉成matrix的结构，因此可以通过将filter级与shape级的稀疏化进行结合来将2D矩阵的行和列稀疏化，再分别在矩阵的行和列上裁剪掉剔除全为0的值可以来降低矩阵的维度从而提升模型的运算效率。该方法是regular的方法，压缩粒度较粗，可以适用于各种现成的算法库，但是训练的收敛性和优化难度不确定。作者的源码为：<a href="https://github.com/wenwei202/caffe/tree/scnn" rel="nofollow noopener noreferrer" target="_blank">https://github.com/wenwei202/caffe/tree/scnn</a></p></li><li><p>Dynamic Network Surgery for Efficient DNNs <a href="http://arxiv.org/abs/1608.04493" rel="nofollow noopener noreferrer" target="_blank">论文地址</a> <br> 作者提出了一种动态的模型裁剪方法，包括以下两个过程：pruning和splicing，其中pruning就是将认为不中要的weight裁掉，但是往往无法直观的判断哪些weight是否重要，因此在这里增加了一个splicing的过程，将哪些重要的被裁掉的weight再恢复回来，类似于一种外科手术的过程，将重要的结构修补回来，它的算法如下： <br> <img src="https://images2.imgbox.com/c7/04/fj1n5Ufr_o.png" alt="这里写图片描述" title=""> <br> 作者通过在W上增加一个T来实现，T为一个2值矩阵，起到的相当于一个mask的功能，当某个位置为1时，将该位置的weight保留，为0时，裁剪。在训练过程中通过一个可学习mask将weight中真正不重要的值剔除，从而使得weight变稀疏。由于在删除一些网络的连接，会导致网络其他连接的重要性发生改变，所以通过优化最小损失函数来训练删除后的网络比较合适。 <br> 优化问题表达如下： <br> <img src="https://images2.imgbox.com/e4/06/7ff1SVwZ_o.png" alt="这里写图片描述" title=""> <br> 参数迭代如下： <br> <img src="https://images2.imgbox.com/54/b4/Eh6EWd75_o.png" alt="这里写图片描述" title=""> <br> 其中用于表示网络连接的重要性 h 函数定义如下： <br> <img src="https://images2.imgbox.com/9f/e4/uwX7es67_o.png" alt="这里写图片描述" title=""> <br> 该算法采取了剪枝与嫁接相结合、训练与压缩相同步的策略完成网络压缩任务。通过网络嫁接操作的引入，避免了错误剪枝所造成的性能损失，从而在实际操作中更好地逼近网络压缩的理论极限。属于irregular的方式，但是ak和bk的值在不同的模型以及不同的层中无法确定，并且容易受到稀疏矩阵算法库以及带宽的限制。论文源码：<a href="https://github.com/yiwenguo/Dynamic-Network-Surgery" rel="nofollow noopener noreferrer" target="_blank">https://github.com/yiwenguo/Dynamic-Network-Surgery</a></p></li><li><p>Training Skinny Deep Neural Networks with Iterative Hard Thresholding Methods <a href="https://arxiv.org/abs/1607.05423" rel="nofollow noopener noreferrer" target="_blank">论文地址</a> <br> 作者想通过训练一个稀疏度高的网络来降低模型的运算量，通过在网络的损失函数中增加一个关于W的L0范式可以降低W的稀疏度，但是L0范式就导致这是一个N-P难题，是一个难优化求解问题，因此作者从另一个思路来训练这个稀疏化的网络。算法的流程如下： <br> <img src="https://images2.imgbox.com/5e/fa/1CIhwC6D_o.png" alt="这里写图片描述" title=""> <br> 先正常训练网络s1轮，然后Ok(W)表示选出W中数值最大的k个数，而将剩下的值置为0，supp(W,k)表示W中最大的k个值的序号，继续训练s2轮，仅更新非0的W，然后再将之前置为0的W放开进行更新，继续训练s1轮，这样反复直至训练完毕。 同样也是对参数进行诱导的方式，边训练边裁剪，先将认为不重要的值裁掉，再通过一个restore的过程将重要却被误裁的参数恢复回来。也是属于irregular的方式，边训边裁，性能不错，压缩的力度难以保证。</p></li></ul> 
<h3 id="总结">总结</h3> 
<p>以上三篇文章都是基于核稀疏化的方法，都是在训练过程中，对参数的更新进行限制，使其趋向于稀疏，或者在训练的过程中将不重要的连接截断掉，其中第一篇文章提供了结构化的稀疏化，可以利用GEMM的矩阵操作来实现加速。第二篇文章同样是在权重更新的时候增加限制，虽然通过对权重的更新进行限制可以很好的达到稀疏化的目的，但是给训练的优化增加了难度，降低了模型的收敛性。此外第二篇和第三篇文章都是非结构化的稀疏化，容易受到稀疏矩阵算法库以及带宽的限制，这两篇文章在截断连接后还使用了一个surgery的过程，能够降低重要参数被裁剪的风险。之后还会对其他的模型压缩方法进行介绍。（未完待续）</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/781afc7e95df2868691c7a241844110b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">使用 ajax 多次请求，并将结果集合并（ajax 非异步）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/5bbb7f6a1340123c6870f06c75e9dfe7/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">深度学习模型压缩方法综述（二）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>