<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Deep Learning for Generic Object Detection: A Survey -- 目标检测综述总结 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Deep Learning for Generic Object Detection: A Survey -- 目标检测综述总结" />
<meta property="og:description" content="最近，中国国防科技大学、芬兰奥卢大学、澳大利亚悉尼大学、中国香港中文大学和加拿大滑铁卢大学等人推出一篇最新目标检测综述，详细阐述了当前目标检测最新成就和关键技术。文章最后总结了未来8个比较有前景的方向，对学习目标检测的人员提供了很大的帮助，在此翻译这篇文章，方便阅读与理解。
此外，来自首尔国立大学的 Lee hoseong 在近期开源了「deep learning object detection」GitHub 项目，正是参考该论文开发的。该项目集合了从 2013 年 11 月提出的 R-CNN 至在近期举办的 ECCV2018 上发表的 RFBNet 等四十多篇关于目标检测的论文，相当全面。这些论文很多都曾发表在机器学习或人工智能顶会上，如 ICLR、NIPS、CVPR、ICCV、ECCV 等。正如图中红色字体标示的那样，其中也包含了很多代表性的成果，如从 R-CNN 到 Mask R-CNN 的 R-CNN 系列、YOLO 系列、RPN、SSD、FPN 以及 RetinaNet 等。无论对刚入门的机器学习新手，还是想深入研究目标检测的研究者，都是难得的学习、参考资源。不仅如此，项目作者还给出了相应的完整资源列表，包括论文、官方/非官方实现。下边就是GitHub的链接。
GitHub - hoya012/deep_learning_object_detection: A paper list of object detection using deep learning.
文章目录 摘要1.介绍1.1与之前目标检测方法相对比1.2 分类方法 2.背景2.1目标检测的问题2.2主要挑战2.2.1 有关精准度的挑战2.2.2 有关效率的挑战2.3 过去二十年取得的进展 3. 目标检测的结构3.1 Two Stage FrameworkRCNNSPPNetFast RCNNFaster RCNNRFCNMask RCNNLight Head RCNN 3.2 Unified Pipeline (One Stage Pipeline)DetectorNetOverFeatYOLOYOLOv2 and YOLO9000SSD 4." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/37b47dc871b9071fd133523798bd4fa4/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-09-25T22:06:47+08:00" />
<meta property="article:modified_time" content="2018-09-25T22:06:47+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Deep Learning for Generic Object Detection: A Survey -- 目标检测综述总结</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-dracula">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p> 　最近，中国国防科技大学、芬兰奥卢大学、澳大利亚悉尼大学、中国香港中文大学和加拿大滑铁卢大学等人推出一篇最新目标检测综述，详细阐述了当前目标检测最新成就和关键技术。文章最后总结了未来8个比较有前景的方向，对学习目标检测的人员提供了很大的帮助，在此翻译这篇文章，方便阅读与理解。</p> 
<p><img src="https://images2.imgbox.com/d5/38/lLVruA0v_o.png" alt="在这里插入图片描述"></p> 
<p> 　此外，来自首尔国立大学的 Lee hoseong 在近期开源了「deep learning object detection」GitHub 项目，正是参考该论文开发的。该项目集合了从 2013 年 11 月提出的 R-CNN 至在近期举办的 ECCV2018 上发表的 RFBNet 等四十多篇关于目标检测的论文，相当全面。这些论文很多都曾发表在机器学习或人工智能顶会上，如 ICLR、NIPS、CVPR、ICCV、ECCV 等。正如图中红色字体标示的那样，其中也包含了很多代表性的成果，如从 R-CNN 到 Mask R-CNN 的 R-CNN 系列、YOLO 系列、RPN、SSD、FPN 以及 RetinaNet 等。无论对刚入门的机器学习新手，还是想深入研究目标检测的研究者，都是难得的学习、参考资源。不仅如此，项目作者还给出了相应的完整资源列表，包括论文、官方/非官方实现。下边就是GitHub的链接。<br> <a href="https://github.com/hoya012/deep_learning_object_detection#2014">GitHub - hoya012/deep_learning_object_detection: A paper list of object detection using deep learning.</a></p> 
<p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#_10" rel="nofollow">摘要</a></li><li><a href="#1_17" rel="nofollow">1.介绍</a></li><li><ul><li><a href="#11_32" rel="nofollow">1.1与之前目标检测方法相对比</a></li><li><a href="#12__45" rel="nofollow">1.2 分类方法</a></li></ul> 
  </li><li><a href="#2_51" rel="nofollow">2.背景</a></li><li><ul><li><a href="#21_52" rel="nofollow">2.1目标检测的问题</a></li><li><a href="#22_62" rel="nofollow">2.2主要挑战</a></li><li><ul><li><a href="#221__84" rel="nofollow">2.2.1 有关精准度的挑战</a></li><li><a href="#222__94" rel="nofollow">2.2.2 有关效率的挑战</a></li><li><a href="#23__100" rel="nofollow">2.3 过去二十年取得的进展</a></li></ul> 
  </li></ul> 
  </li><li><a href="#3__117" rel="nofollow">3. 目标检测的结构</a></li><li><ul><li><a href="#31_Two_Stage_Framework_130" rel="nofollow">3.1 Two Stage Framework</a></li><li><ul><li><a href="#RCNN_136" rel="nofollow">RCNN</a></li><li><a href="#SPPNet_153" rel="nofollow">SPPNet</a></li><li><a href="#Fast_RCNN_157" rel="nofollow">Fast RCNN</a></li><li><a href="#Faster_RCNN_161" rel="nofollow">Faster RCNN</a></li><li><a href="#RFCN_166" rel="nofollow">RFCN</a></li><li><a href="#Mask_RCNN_169" rel="nofollow">Mask RCNN</a></li><li><a href="#Light_Head_RCNN_173" rel="nofollow">Light Head RCNN</a></li></ul> 
   </li><li><a href="#32_Unified_Pipeline_One_Stage_Pipeline_176" rel="nofollow">3.2 Unified Pipeline (One Stage Pipeline)</a></li><li><ul><li><a href="#DetectorNet_180" rel="nofollow">DetectorNet</a></li><li><a href="#OverFeat_183" rel="nofollow">OverFeat</a></li><li><a href="#YOLO_186" rel="nofollow">YOLO</a></li><li><a href="#YOLOv2_and_YOLO9000_189" rel="nofollow">YOLOv2 and YOLO9000</a></li><li><a href="#SSD_192" rel="nofollow">SSD</a></li></ul> 
  </li></ul> 
  </li><li><a href="#4_Fundamental_SubProblems_195" rel="nofollow">4. Fundamental SubProblems</a></li><li><ul><li><a href="#41_DCNN_198" rel="nofollow">4.1 基于DCNN的目标表示</a></li><li><ul><li><a href="#411_CNN_202" rel="nofollow">4.1.1 受欢迎的CNN架构</a></li><li><a href="#412__216" rel="nofollow">4.1.2 改进目标表示的方法</a></li><li><ul><li><a href="#CNN_225" rel="nofollow">结合CNN多层特征进行检测</a></li><li><a href="#CNN_228" rel="nofollow">多个CNN层进行探测</a></li><li><a href="#_231" rel="nofollow">以上两种方法的组合</a></li><li><a href="#_241" rel="nofollow">模型几何转换</a></li><li><a href="#_244" rel="nofollow">建模目标变形</a></li></ul> 
   </li></ul> 
   </li><li><a href="#42__247" rel="nofollow">4.2 情境建模</a></li><li><a href="#43_264" rel="nofollow">4.3检测的建议方法</a></li><li><a href="#44__298" rel="nofollow">4.4 其他特殊问题</a></li></ul> 
  </li><li><a href="#5_Datasets_and_Performance_Evaluation_302" rel="nofollow">5. Datasets and Performance Evaluation</a></li><li><ul><li><a href="#51__304" rel="nofollow">5.1 数据集</a></li><li><a href="#52__329" rel="nofollow">5.2 评价指标</a></li></ul> 
  </li><li><a href="#6__342" rel="nofollow">6. 总结</a></li><li><ul><li><a href="#1_Open_World_Learning_347" rel="nofollow">(1) Open World Learning</a></li><li><a href="#2_Better_and_More_Efficient_Detection_Frameworks_351" rel="nofollow">(2) Better and More Efficient Detection Frameworks</a></li><li><a href="#3___Compact_and_Efficient_Deep_CNN_Features_354" rel="nofollow">(3) Compact and Efficient Deep CNN Features</a></li><li><a href="#4___Robust_Object_Representations_357" rel="nofollow">(4) Robust Object Representations</a></li><li><a href="#5__Context_Reasoning_360" rel="nofollow">(5) Context Reasoning</a></li><li><a href="#6__Object_Instance_Segmentation_364" rel="nofollow">(6) Object Instance Segmentation</a></li><li><a href="#7__Weakly_Supervised_or_Unsupervised_Learning_368" rel="nofollow">(7) Weakly Supervised or Unsupervised Learning</a></li><li><a href="#8__3D_Object_Detection_371" rel="nofollow">(8) 3D Object Detection</a></li><li><a href="#_381" rel="nofollow">参考文献</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="_10"></a>摘要</h2> 
<p> 　通用目标检测旨在从自然图像中定位大量预定义类别的物体实例,是计算机视觉中最基本和最具挑战性的问题之一。近年来，深度学习技术已成为数据中学习特征表示的有力方法，并在通用目标检测领域取得了显著的突破。在深度学习快速发展的时期，本文提供对深度学习技术带来的这一领域最近成就的全面调研。本次调研包括250多项关键技术，涵盖了通用目标检测研究的许多方面：<strong>前沿的检测框架和基本子问题，包括目标特征表示，候选区域生成，上下文信息建模和训练策略等；评价问题，特别是benchmark数据集，评价指标和最先进的方法。最后，我们讨论了未来研究的方向。</strong></p> 
<p><strong>关键词：目标检测；深度学习；卷积神经网络；目标识别。</strong></p> 
<h2><a id="1_17"></a>1.介绍</h2> 
<p> 　图像目标检测作为计算机视觉的一个长期、基本和具有挑战性的问题，已经成为几十年来研究的一个活跃领域。目标检测的目的是确定是否有任何目标的实例从给定的类别(如人类,汽车,自行车,狗和猫)在某些给定的图像,如果存在,返回每个目标实例的空间位置和程度(例如,通过一个边界框[179])。作为图像理解和计算机视觉的基础，目标检测构成了解决更复杂或更高层次的视觉任务的基础，如分割、场景理解、目标跟踪、图像定位、事件检测和活动识别。目标检测在人工智能和信息技术的许多领域都有广泛的应用，包括机器人视觉、消费电子、安全、自动驾驶、人机交互、基于内容的图像检索、智能视频监控和增强现实。</p> 
<p> 　最近，深度学习技术[81, 116]已经成为从数据中自动学习特征表示的强大方法。特别是，这些技术为目标检测提供了显著的改进，这一问题在过去五年中引起了极大的关注，尽管它已经被心理物理学家、神经学家和工程师们研究了几十年。</p> 
<p> 　目标检测可以分为两种类型 [69,240]：特定实例的检测和特定类别的检测。第一种类型的目标是检测特定目标的实例，而第二种类型的目标是检测不同的预定义目标类别的实例（例如人类、汽车、自行车和狗）。从历史上看，在目标检测领域的大部分努力都集中在检测单个类别（如面孔和行人）或少数特定类别。与此相反，在过去的几年中，研究界已经开始朝着建立通用目标探测系统的挑战目标迈进，这些系统的目标探测能力的广度与人类不相上下。</p> 
<p> 　然而，在2012年，Krizhevsky等人提出了一个叫做AlexNet的深度卷积神经网络（DCNN），它在大规模的视觉识别挑战（ILSRVC）[179]中实现了破纪录的图像分类精度。从那时起，许多计算机视觉应用领域的研究都集中在深度学习方法上。基于深度学习的许多方法已经在通用目标检测中涌现出来，[65、77、64,183、176]取得了巨大的进步，但我们不知道在过去的5年里对这个主题进行了全面的调查。在这个快速进化的时代，这篇论文的重点是通过深度学习来获得一般的目标检测，以便在通用目标检测中获得更清晰的全景。</p> 
<p><img src="https://images2.imgbox.com/83/84/R5nUSsCT_o.png" alt="在这里插入图片描述"></p> 
<p> 　通用目标检测问题本身的定义如下：给定一个任意的图像，确定是否有来自预定义类别的语义目标的实例，如果存在的话，返回空间位置和范围。物体指的是一种可以被看到和触摸的物质。尽管在很大程度上是目标类检测的同义词，但通用目标检测更侧重于探测广泛的自然类别的方法，而不是目标实例或专门类别（例如，人脸、行人或汽车）。一般的目标检测已经得到了极大的关注，最近在目标检测方面的进展就证明了这一点，例如从2006年到2012年的PASCAL VOC检测挑战，从2013年到179年，ILSVRC的大规模检测挑战，以及自2015年以来的MS COCO大规模检测挑战。图1显示了近年来的显著改善。</p> 
<h3><a id="11_32"></a>1.1与之前目标检测方法相对比</h3> 
<p>表1. 2000年以来的一些相关调查摘要<br> <img src="https://images2.imgbox.com/55/3b/iMqLq7Ze_o.png" alt="在这里插入图片描述"></p> 
<p> 　如表1所总结的，已经发表了许多值得注意的目标检测调查。其中包括许多关于特定目标检测问题的优秀调查，例如行人检测[51、60、48]人脸检测[226、232],车辆检测{196}和文本检测{227}。Ponce et al. [169], Dickinson [46], Galleguillos 和 Belongie [59], Grauman 和 Leibe [69], 以及 Andreopoulos 和 Tsotsos [5]也做出了重要贡献。</p> 
<p> 　最近很少有直接针对通用目标检测问题的调查，除了Zhang等人的研究，他对目标类检测的课题进行了调查。然而，在[69、5、240]进行的研究主要是在2012年之前，因此在深度学习和相关方法最近取得的显著成功之前。</p> 
<p> 　深度学习允许由多个层次层组成的计算模型来学习非常复杂、微妙和抽象的表示。在过去的几年中，深度学习在诸如视觉识别、物体检测、语音识别、自然语言处理、医学图像分析、药物发现和基因组学等广泛的问题上取得了重大进展。在不同类型的深层神经网络中，深度卷积神经网络（DCNN）[115、109、116]在处理图像、视频、语音和音频方面取得了突破。在这个快速进化的时代，研究人员最近发表了关于深度学习的不同方面的调查，包括Bengio等人[12]，LeCun等人[116]，Litjens等人[133]，Gu等人[71]，以及最近在ICCV和CVPR的教程。</p> 
<p> 　尽管已经提出了许多基于深度学习的方法来进行反对检测，但我们并不知道在过去的五年中对这一课题的全面调查，这是这项调查的重点。对现有工作进行彻底的审查和总结，对于进一步推进目标检测至关重要，对于希望进入该领域的研究人员来说尤其如此。在我们的讨论中不包括对特定目标检测的广泛研究，如人脸检测[120、237、92]，行人检测[238、85]，车辆检测[247]和交通标志检测[253]。</p> 
<h3><a id="12__45"></a>1.2 分类方法</h3> 
<p> 　自深度学习进入以来，关于通用目标检测的论文数量惊人。事实上，很多人，对艺术状态的全面回顾已经超过了像这样的论文的可能性。有必要建立一些选择标准，例如，一篇论文的完整性和对该领域的重要性。我们更倾向于包括顶级期刊和会议论文。由于对空间的限制和我们的知识，我们真诚地向那些作品不包括在这篇文章中的作者道歉。对于相关主题的调查，读者可以参考表1中的文章。本调查主要集中在过去五年中取得的重大进展;但是为了完整性和更好的可读性，一些早期相关的工作也包括在内。我们把自己限制在静态图片上，把视频目标检测作为一个单独的主题。</p> 
<p> 　本文的其余部分按如下方式组织。第2节总结了过去20年的相关背景，包括问题、关键挑战和取得的进展。第3节中我们描述了目标检测中的里程碑目标检测器。第4节介绍了设计目标探测器的基本子问题和相关问题。第5节中对流行数据库的总结和评价指标进行解释。在第6节中讨论了几个有前景比较广阔的目标检测方向。</p> 
<h2><a id="2_51"></a>2.背景</h2> 
<h3><a id="21_52"></a>2.1目标检测的问题</h3> 
<p> 　通用目标检测（即通用目标类别检测），也称为目标类检测[240]或目标类别检测，定义如下。给定一个图像，通用目标检测的目标是确定是否有来自许多预定义类别的目标的实例，如果存在，则返回每个实例的空间位置和范围。它更注重于检测广泛的自然类别，而不是特定的目标类别检测，只有较窄的预定义类别（如面孔、行人或汽车）可能存在。尽管成千上万的物体占据了我们生活的视觉世界，但目前研究界主要对高度结构化的物体（如汽车、面孔、自行车和飞机）的定位感兴趣，并对（如人类、牛和马）的定位（如天空、草和云）进行了阐述。</p> 
<p> 　通常，一个物体的空间位置和范围可以用一个边界框来粗略地定义。一个与轴对齐的矩形紧紧地将物体[53、179]，一个精确的像素化的分割掩码，或一个封闭的边界[180、129]，如图3所示。据我们所知，在目前的文献中，边界框被更广泛地用于评估通用目标检测算法[53，179]，并且将是我们在这个调查中采用的方法。然而，社区正在走向深入的场景理解（从图像级目标分类到单一目标的定位，到一般的目标检测，以及像素化的目标分割），因此预期未来的挑战将会在像素级别[129]。</p> 
<p><img src="https://images2.imgbox.com/94/c5/D8ztSE5r_o.png" alt="在这里插入图片描述"><br> 图3 与通用目标检测相关的识别问题。（a）图像级目标分类，（b）边界框级通用目标检测，（c）像素级语义分割，（d）实例级语义分割。</p> 
<p> 　通用目标检测与语义图像分割密切相关（图3（c）），它的目标是将图像中的每个像素分配给语义类标签。目标实例分割（图3（d））旨在区分同一目标类的不同实例，而语义划分则不能区分不同的实例。通用目标检测也区分了同一目标的不同实例。与分割不同，目标检测包括边界框中的背景区域，这可能对分析有用。</p> 
<h3><a id="22_62"></a>2.2主要挑战</h3> 
<p><img src="https://images2.imgbox.com/53/f4/XPnn79Dk_o.png" alt="在这里插入图片描述"><br> 图4 通用目标检测中的挑战摘要</p> 
<ul><li>理想分类器 
  <ul><li>高准确度 
    <ul><li>高鲁棒性 
      <ul><li>每个类别内的差异（不同的颜色、纹理、材质、形状等）</li><li>目标实例差异（姿势、形变）</li><li>成像条件和无约束环境（照明，视图点，刻度，遮挡，阴影，杂波，模糊，运动，天气状况）</li><li>图像噪声（成像噪声，滤波失真，压缩噪声）</li></ul> </li><li>高辨别性 
      <ul><li>组内歧义</li><li>过多的真实的目标分类（有结构与无结构的）</li></ul> </li></ul> </li><li>高效 
    <ul><li>现实世界中成千上万的目标类别</li><li>要求定位和识别目标</li><li>大量可能的目标位置</li><li>大规模图像/视频数据</li></ul> </li></ul> </li></ul> 
<p> 　通用目标检测的目标是定位和识别广泛的自然目标类别。如图4所示，通用目标检测的理想目标是开发通用目标检测算法，实现两个相互竞争的目标：高质量、高准确性和高效率。如图5所示，高质量检测必须准确地定位和识别图像或视频帧中的物体，这样才能区分真实世界中各种各样的目标类别（例如高度的区别性），以及来自同一类别的目标实例，受限于类内外观的变化，可以被本地化和识别（例如、高鲁棒性)。高效率要求整个检测任务以足够高的帧速率运行，并使用可接受的内存和存储使用。尽管经过了几十年的研究和取得了重大进展，但准确和效率的综合目标还没有得到满足。</p> 
<h4><a id="221__84"></a>2.2.1 有关精准度的挑战</h4> 
<p>准确的说，精准度的挑战来自于大量的类内变化和大量的目标类别的挑战。<br> <img src="https://images2.imgbox.com/9c/a4/djBdoUEx_o.png" alt="在这里插入图片描述"><br> 图5 同一类的成像条件变化的变化与成像条件的变化</p> 
<p> 　我们从细胞内的变化开始，可以分为两种类型：内在因素和成像条件。对于前者,每个目标类别可以有许多不同的目标实例,可能在一个或多个不同的颜色,质地,材料,形状,大小,如椅子类别图5所示(h)。即使在一个更加狭义的类,如人或马,目标实例可以出现在不同的姿势,非刚性变形和不同的衣服。</p> 
<p> 　对于后者，这些变化是由成像条件的变化和不受约束的环境造成的，这可能会对物体的外观产生巨大的影响。特别地，不同的实例，甚至是相同的实例，都可以被捕获到不同的地方：不同的时间、地点、天气条件、摄像机、背景、光照、视点和观看距离。所有这些条件都会产生显著的物体外观变化，如光照、姿势、尺度、遮挡、背景杂波、阴影、模糊和运动，如图5（a-g）所示。数字化的工件、噪音的腐败、糟糕的解决方案和过滤的扭曲，可能会增加更多的挑战。</p> 
<h4><a id="222__94"></a>2.2.2 有关效率的挑战</h4> 
<p> 　除了类内的变化外，大量的目标类别，按照10000- 100000的顺序，要求探测器有很大的辨别能力，以区分不同种类的不同种类的差异，如图5（i）所示）。在实践中，当前的检测器主要关注结构化的目标类别，例如PASCAL VOC [53]、ILSVRC [179]和COCO [129]的目标类。显然，现有的基准数据集所考虑的目标类别的数量比人类所能识别的要小得多。</p> 
<p> 　效率的挑战源于需要本地化和识别所有目标实例非常大量的目标类别,和非常大的可能位置和尺度在单一的图像,如图所示,图5中的例子©,另一个挑战是可伸缩性:探测器应该能够处理看不见的目标,未知的情况下,迅速增加图像数据。例如，ILSVRC [179]的规模已经对可以获得的手工注释施加了限制。由于图像的数量和类别的数量变得更大，可能不可能手工注释它们，迫使算法更多地依赖于弱监控的训练数据。</p> 
<h4><a id="23__100"></a>2.3 过去二十年取得的进展</h4> 
<p> 　对目标识别的早期研究基于模板匹配技术和简单的基于部分的模型，重点关注那些空间布局大致是刚性的特定目标，比如人脸。在1990年之前，目标识别的主要模式是基于几何表示，之后的焦点从几何和先前的模型转向使用统计分类器（如神经网络，SVM和Adaboost ），基于外观特征。这个成功的目标探测器家族为大多数人发挥的平台。<br> <img src="https://images2.imgbox.com/5c/4b/xeEFdXjq_o.png" alt="在这里插入图片描述"></p> 
<p> 　在20世纪90年代末和21世纪初，目标探测研究取得了显著的进步。近年来，目标检测的里程碑在图2中显示，其中两个主要的时代（SIFT vs. DCNN）被高亮显示。外观特征从全局表示从转移到局部表示，这些表示在翻译、缩放、旋转、光照、视点和遮挡等方面都是不变的。手工的局部不变特征得到了极大的流行，从比例不变式开始特征转换（筛选）特性，并且在各种视觉识别任务上的进展主要是基于对本地描述符的使用，比如Haar、SIFT、形状联系、梯度直方图（HOG）、局部二值法（LBP）、协方差。这些局部特性通常是由简单的连接或功能池编码器聚合而成的，比如Sivic和Zisserman 、Csurka等、弓形模型的空间金字塔匹配（SPM）以及Fisher矢量。</p> 
<p> 　多年来，手工设计的目标定位和鉴别分类器的多级优化通道在计算机视觉领域占据了许多领域，包括目标检测，直到2012年的重大转折点，深度卷积神经网络（DCNN）在图像分类中达到了创纪录的破纪录。成功地将DCNNs应用于图像分类，进而转移到目标检测，从而导致了基于里程碑的CNN（RCNN）的Girshick等人的探测器。从那时起，目标检测领域已经发生了巨大的变化，许多基于深度学习的方法已经开发出来，这在一定程度上要归功于可用的GPU计算资源，以及大规模数据集的可用性，以及诸如ImageNet和COCO 的挑战比赛。有了这些新的数据集，研究人员可以在检测到数百个类别的物体时，从具有巨大的内部变化和类间相似性的图像中，找出更现实、更复杂的问题。</p> 
<p> 　研究界已经开始朝着建立通用目标探测系统的挑战性目标迈进，该系统能够探测到许多目标类别与人类的匹配。这是一个主要的挑战：根据认知科学家的说法，人类可以识别大约3000个入门级的类别，总共有3万个视觉类别，以及与领域专业知识不同的类别的数量可能是十万类。：尽管在过去的几年里取得了显著的进步，但是设计一个精确、强健、高效的检测和识别系统，在10000到100000个类别中接近人类水平的表现无疑是一个开放的问题。</p> 
<h2><a id="3__117"></a>3. 目标检测的结构</h2> 
<p> 　在目标特征表示和识别分类器方面已经有了稳定的进展，从手工制作特征到学习DCNN特征的巨大变化就是很好地证明。</p> 
<p> 　相比之下，本地化的基本滑动窗口策略仍然是主流，尽管做了一些努力。然而，窗口的数量是很大的，并且随着像素的数量呈二次增长，在多个尺度和纵横比上搜索的需求进一步增加了搜索空间。搜索空间巨大，计算复杂度高。因此，高效的检测框架设计起着关键作用。通常采用的策略包括级联、共享特性计算和减少每个窗口的计算。</p> 
<p> 　如图6所示，我们将回顾自深度学习进入通用目标检测领域以来出现的里程碑检测框架，总结如表10所示。在过去几年中，几乎所有提议的探测器都基于这些里程碑式的探测器之一，试图在一个或多个方面进行改进。这些探测器大体上可分为两大类：</p> 
<ol><li>两级检测框架，其中包括对区域提案的预处理步骤，使整个检测过程分为两个阶段；</li><li>单级检测框架，或区域建议自由框架，是一种不单独检测候选框的单一方法，使整个流水线处于单一阶段。</li></ol> 
<h3><a id="31_Two_Stage_Framework_130"></a>3.1 Two Stage Framework</h3> 
<p><img src="https://images2.imgbox.com/cf/7f/C7OdF7K2_o.png" alt="在这里插入图片描述"></p> 
<p> 　在基于区域的框架中，从图像中生成独立于类别的区域建议框，从这些区域中提取CNN的特征，然后使用特定于类别的分类器来确定建议框的类别标签。从上图可以看出，DetectorNet、OverFeat、MultiBox和RCNN独立且几乎同时提出使用CNNs进行通用目标检测。</p> 
<h4><a id="RCNN_136"></a>RCNN</h4> 
<p><img src="https://images2.imgbox.com/a0/70/8kOjfiPs_o.png" alt="在这里插入图片描述"></p> 
<p> 　RCNN灵感来自于突破图像分类结果由CNN和选择性搜索区域的手工特征提取,Girshick等人是最早探索CNN通用目标检测和发达RCNN,集AlexNet与区域建议方法选择性搜索。如上图所示，RCNN框架中的训练由多个阶段组成。</p> 
<ol><li>使用选择性搜素对未知区域进行提取可能包含目标的区域建议;</li><li>从图像中裁剪并扭曲成相同大小的区域提案，作为输入，使用ImageNet等大型数据集对CNN模型进行预训练;</li><li>利用CNN提取的固定长度特征，对一类特定的线性支持向量机分类器进行训练，取代了用finetuning学习的softmax分类器；</li><li>通过CNN特征的每个目标类学习建议框回归。</li></ol> 
<p>尽管RCNN实现了高目标检测质量，但仍存在明显的缺陷：</p> 
<ol><li>训练是一个多阶段的复杂过程，由于每个阶段都必须单独训练，所以训练不优雅、缓慢且难以优化。</li><li>许多只提供粗略本地化的区域建议需要外部检测。</li><li>SVM分类器和候选框回归训练在磁盘空间和时间上都是占用十分大，因为CNN的特征是独立于每幅图像的每个区域提案提取出来的，这给大规模检测带来了很大的挑战，尤其是非常深入的CNN网络，如AlexNet[109]和VGG[191]。</li><li>CNN特征从每个检测图像中提取每个目标的候选框，这个测试是十分缓慢的。</li></ol> 
<h4><a id="SPPNet_153"></a>SPPNet</h4> 
<p> 　在测试过程中，CNN特征提取是RCNN检测的主要瓶颈，RCNN检测管道需要从数千个扭曲区域中提取CNN特征来获得图像。注意到这些明显的缺点，He 等人将传统的空间金字塔池(SPP)引入到CNN架构中。自卷积层接受输入的任意尺寸,固定大小的图像在CNN的要求仅仅是由于完全连接(FC)层，He 等人发现这个事实并添加一个SPP层之上的最后卷积(CONV)层获得功能的固定长度的FC层。有了SPPnet, RCNN在不牺牲任何检测质量的情况下获得了显著的加速，因为它只需要在整个测试图像上运行卷积层一次，就可以为任意大小的区域提案生成固定长度的特性。虽然SPPnet以数量级加速了RCNN的评估，但它并没有导致检测器训练的类似加速。此外，SPPnet中的finetuning无法在SPP层之前更新卷积层，这限制了深度网络的准确性。</p> 
<h4><a id="Fast_RCNN_157"></a>Fast RCNN</h4> 
<p> 　Girshick[64]提出了Fast RCNN，解决了RCNN和SPnet的一些缺点，同时提高了它们的检测速度和质量。如图8所示，Fast RCNN通过开发一个流线化的训练过程来实现端到端检测器训练（当忽略区域建议生成过程时），该过程同时学习软最大分类器和使用多任务损失的特定于类的边界盒回归，而不是比在RCNN/SPPNET中的三个独立的阶段训练SULTMax分类器、SVM和BBR。Fast RCNN采用跨区域建议共享卷积计算的思想，在最后一CONV层和第一FC层之间添加感兴趣区域(RoI)汇聚层来提取每个区域建议(即RoI)的固定长度特征。本质上，ROI池使用特征级的翘曲来近似图像水平上的翘曲。RoI汇聚层之后的特征被馈送到FC层序列中，FC层最终分支到两个兄弟输出层：用于目标类别预测的软最大概率和用于建议细化的特定于类的边界框回归偏移。与RCNN/SPPnet相比，Fast RCNN显著提高了效率——通常训练速度快3倍，测试速度快10倍。综上所述，Fast RCNN具有检测质量高、单阶段训练过程更新所有网络层以及无需存储特征缓存等优点。</p> 
<h4><a id="Faster_RCNN_161"></a>Faster RCNN</h4> 
<p> 　虽然Fast RCNN显著加快了检测过程，但仍然依赖于外部区域的建议。区域建议书计算成为Fast RCNN的新瓶颈。最近的研究表明，CNNs具有在CONV层中定位目标的非凡能力[243,244,36,158,75]，这种能力在FC层中被削弱了。因此，选择性搜索可以被CNN在产地提案中所替代。Ren等人[175,176]提出的更快的RCNN框架提出了一个高效准确的区域提案网络(RPN)来生成区域提案。它们利用单一网络完成区域提案的RPN和区域分类的Fast RCNN任务。在Fast RCNN中，RPN和Fast RCNN共享大量的卷积层。上一个共享卷积层的特性用于区域提议和来自不同分支的区域分类。RPN首先在每个CONV 特征图位置初始化不同比例和纵横比的k n<em>n参考框(即所谓的锚)。每个n</em>n锚被映射到一个较低的维向量(例如ZF的256和VGG的512)，该向量被注入两个兄弟的FC层，一个目标分类层和一个box回归层。与Fast RCNN不同，RPN中用于回归的特征大小相同。RPN与Fast RCNN共享CONV特性，从而支持高效的区域提案计算。RPN实际上是一种完全卷积网络(FCN) [138,185];因此，更快的RCNN是一个纯粹的基于CNN的框架，不使用手工制作的特性。对于非常深入的VGG16型号[191]，更快的RCNN可以在GPU上测试5fps(包括所有步骤)，同时在PASCAL VOC 2007上使用每幅图像300个建议书实现了最先进的目标检测精度。[175]中最初的更快的RCNN包含几个交替的训练步骤。然后在[176]中通过一步联合训练简化了这一过程。<br>  　在Faster RCNN中,Lenc Vedaldi[117]挑战的作用区域建议代选择性搜索等方法,研究区域的作用建议在基于CNN的探测器,一代,发现CNN包含足够的几何信息准确的目标检测CONV而不是FC层。他们证明了构建集成的、简单的、快速的目标探测器的可能性，这些探测器完全依赖于CNNs，消除了区域提议生成方法，比如选择性搜索。</p> 
<h4><a id="RFCN_166"></a>RFCN</h4> 
<p> 　RFCN (Region based Fully Convolutional Network)，虽然Faster RCNN快一个数量级的速度比Fast RCNN,哪些地区的子网仍然需要每RoI应用(每张图象几百RoI)领导戴等。[40]提出RFCN探测器完全卷积(没有隐藏的FC层),几乎所有的计算共享整个图像。如图8所示，RFCN仅在RoI子网络中不同于更快的RCNN。在更快的RCNN中，RoI池层之后的计算无法共享。一个自然的想法是最小化无法共享的计算量，因此Dai等人[40]提议使用所有CONV层来构建一个共享的RoI子网络，并且在预测之前从CONV特性的最后一层提取RoI作物。然而，Dai等人[40]发现这种天真的设计结果具有相当低的检测精度，推测较深的CONV层对类别语义更敏感，对翻译的敏感性较低，而目标检测需要尊重翻译方差的本地化表示。基于这一观察，Dai等人[40]以一组专门的CONV层作为FCN输出构建了一组位置敏感评分映射，在此基础上增加了一个与更标准的RoI池不同的位置敏感RoI池层[64,175]。他们表明使用ResNet101[79]的RFCN可以达到与更快的RCNN相当的精度，通常在更快的运行时间。</p> 
<h4><a id="Mask_RCNN_169"></a>Mask RCNN</h4> 
<p> 　He 等人[80]遵循概念简单、效率和灵活性的精神，提出了Mask RCNN，通过扩展更快的RCNN来处理像素级的目标实例分割。Mask RCNN采用相同的两级管道，相同的一级(RPN)。在第二阶段，在预测类和框偏移量的同时，Mask RCNN添加了一个分支，为每个RoI输出一个二进制掩码。新的分支是一个完全卷积网络(FCN)[138,185]，位于CNN专题地图之上。为了避免原始RoI池层(RoIPool)造成的错位，提出了一个RoIAlign层来保持像素级空间对应。Mask RCNN采用骨干网络ResNeXt101-FPN[223,130]，实现了COCO目标实例分割和边界盒目标检测的顶级结果。它很容易训练，概括得很好，并且只给更快的RCNN增加了很小的开销，运行速度为5 FPS[80]。</p> 
<h4><a id="Light_Head_RCNN_173"></a>Light Head RCNN</h4> 
<p> 　为了进一步加快RFCN[40]的检测速度，Li等人[128]提出了轻头RCNN，使检测网络的头部尽可能轻，减少RoI区域计算。Li等人[128]利用大核可分离卷积，得到了具有小信道数和廉价RCNN子网络的薄特征映射，在速度和精度上取得了很好的平衡</p> 
<h3><a id="32_Unified_Pipeline_One_Stage_Pipeline_176"></a>3.2 Unified Pipeline (One Stage Pipeline)</h3> 
<p> 　RCNN[65]以来，3.1节基于区域的管道策略在检测基准上占主导地位。3.1节中介绍的重要工作导致了更快、更精确的检测器，目前流行的基准数据集的领先结果都基于更快的RCNN[175]。尽管取得了这些进展，但基于区域的方法对移动/可穿戴设备来说计算成本可能很高，因为它们的存储和计算能力有限。因此，研究人员不再试图优化基于区域的复杂管道的单个组件，而是开始制定统一的检测策略。<br> 统一管道(Unified pipeline)广泛指的是直接通过单一的前向CNN网络(不涉及区域提议生成或后分类)从完整图像中预测类概率和边框偏移的体系结构。这种方法简单而优雅，因为它完全消除了区域提案生成和后续像素或特征重采样阶段，将所有计算封装在一个网络中。由于整个检测管道是一个单一的网络，可以直接从检测性能上进行端到端优化。</p> 
<h4><a id="DetectorNet_180"></a>DetectorNet</h4> 
<p> 　Szegedy等人[198]率先探索CNNs用于目标检测。DetectorNet设计的目标检测是一个目标包围盒掩码的回归问题。他们使用AlexNet[109]，用回归层代替最终的softmax分类器层。给定一个图像窗口，他们使用一个网络来预测粗网格上的前景像素，以及四个额外的网络来预测目标的上、下、左、右半边。然后分组进程将预测掩码转换为检测到的边框。每个目标类型和掩码类型都需要训练一个网络。它不会扩展到多个类。DetectorNet必须获取图像的许多作物，并为每个作物的每个部分运行多个网络。</p> 
<h4><a id="OverFeat_183"></a>OverFeat</h4> 
<p> 　Sermanet等人[183]提出的一种基于完全卷积深度网络的现代单级目标检测器。它是最成功的目标检测框架之一，赢得了ILSVRC2013本地化竞赛。OverFeat通过一个通过CNN网络的前进通道，以多尺度滑动窗口的方式执行目标检测，这个网络(最终分类/回归层除外)只包含卷积层。通过这种方式，它们自然地在重叠区域之间共享计算。OverFeat生成一个特征向量网格，每个特征向量代表输入图像中稍微不同的上下文视图位置，可以预测目标的存在。一旦确定了一个目标，同样的特征就会被用来预测单个包围盒回归子。此外，OverFeat利用多尺度特性来提高整体性能，它通过网络将多达6个放大的原始图像缩放到一起，并迭代地将它们聚合在一起，从而导致评估上下文视图(最终特征向量)的数量显著增加。OverFeat比RCNN有显著的速度优势[65]，RCNN是在同一时期提出的，但由于在那个阶段很难训练完全卷积网络，因此其准确率明显较低。速度优势来自于使用完全卷积网络共享重叠窗口之间的卷积计算</p> 
<h4><a id="YOLO_186"></a>YOLO</h4> 
<p> 　YOLO (You Only Look Once)，Redmon等人[174]提出了YOLO，一种统一的检测器浇注目标检测方法，作为从图像像素到空间分隔的边界框和相关类概率的回归问题。YOLO的设计如图8所示。由于区域提议生成阶段完全取消，YOLO直接使用一小部分候选区域预测探测。与基于区域的方法(如更快的RCNN)不同，YOLO使用全局图像的特征来预测检测结果。特别是，YOLO将图像划分为S - S网格。每个网格预测C类概率、B包围框位置和这些框的信任得分。这些预测被编码为一个S (5B +C)张量。通过完全抛弃区域提案生成步骤，YOLO的设计速度很快，实时运行45 FPS，快速版本，即快速YOLO[174]，运行155 FPS。由于YOLO在进行预测时看到了整个图像，因此它隐式地编码了目标类的上下文信息，不太可能在后台预测误报。YOLO对边框位置、比例尺、长宽比进行粗划分，造成定位误差较大。正如[174]中所讨论的，YOLO可能无法本地化一些目标，尤其是小目标，可能是因为网格划分非常粗糙，并且通过构建每个网格单元只能包含一个目标。目前还不清楚YOLO在数据集上能在多大程度上转化为具有大量目标的良好性能，比如ILSVRC检测挑战。</p> 
<h4><a id="YOLOv2_and_YOLO9000_189"></a>YOLOv2 and YOLO9000</h4> 
<p> 　Redmon和Farhadi[173]提出YOLOv2,YOLO,意思的一个改良版本的定制GoogLeNet[200]网络被替换为一个更简单的DarkNet19,加上利用一些策略是从现有的工作,如批量标准化[78],移除完全连接层,并使用好锚箱kmeans和多尺度的培训学习。YOLOv2在标准检测任务上达到了最先进的水平，比如PASCAL VOC和MS COCO。此外，Redmon和Farhadi[173]引入了YOLO9000，它可以实时检测超过9000个目标类别，提出了一种联合优化方法，利用WordTree在ImageNet和COCO上同步训练，将来自多个数据源的数据组合起来。</p> 
<h4><a id="SSD_192"></a>SSD</h4> 
<p> 　SSD (Single Shot Detector)，为了在不牺牲太多检测精度的前提下保持实时速度，Liu等人[136]提出了SSD，其速度快于YOLO[174]，其精度与最先进的基于区域的探测器(包括更快的RCNN)相比具有竞争力[175]。SSD有效地结合了RPN在更快的RCNN[175]、YOLO[174]和多尺度CONV特性[75]中的思想，在保持高检测质量的同时，实现了快速的检测速度。与YOLO一样，SSD预测了固定数量的边界框和这些框中存在目标类实例的得分，然后是NMS步骤，以生成最终检测。SSD中的CNN网络是完全卷积的，其早期层基于标准架构，如VGG<a href="%E5%9C%A8%E4%BB%BB%E4%BD%95%E5%88%86%E7%B1%BB%E5%B1%82%E4%B9%8B%E5%89%8D%E8%A2%AB%E6%88%AA%E6%96%AD" rel="nofollow">191</a>，这被称为基网络。然后，在基础网络的末端添加几个尺寸逐渐减小的辅助CONV层。最后一层低分辨率的信息在空间上可能过于粗糙，无法进行精确定位。SSD使用具有更高分辨率的较浅的层来检测小目标。对于不同大小的目标，SSD通过在多个CONV feature map上操作来在多个尺度上执行检测，每个CONV feature map都预测相应大小的边框的类别得分和框偏移量。对于300300的输入，SSD在VOC2007测试中以59 FPS的速度在Nvidia Titan X上实现了74:3%的mAP。</p> 
<h2><a id="4_Fundamental_SubProblems_195"></a>4. Fundamental SubProblems</h2> 
<p> 　在这一节中描述了重要的子问题，包括特征表示、区域建议、上下文信息挖掘和训练策略。对每种方法的主要贡献进行了审查。</p> 
<h3><a id="41_DCNN_198"></a>4.1 基于DCNN的目标表示</h3> 
<p> 　作为任何检测器的主要组成部分之一，良好的特征表示在目标检测中是最重要的[46,65,62,249]。过去,大量的工作是致力于设计局部描述符(例如,筛选[139]和猪[42])和探索方法(例如,袋字[194]和费舍尔向量[166])组和抽象描述符到更高层次表示为了让歧视目标部分开始出现,但是这些特性表征方法需要仔细的工程和相当大的专业领域。<br>  　相比之下，由多个处理层组成的深度学习方法(特别是深度CNNs，或DCNNs)可以直接从原始图像中学习到具有多层次抽象的强大特征表示[12,116]。随着学习过程降低了传统特征工程中特定领域知识的依赖性和复杂过程的需要[12,116]，特征表示的负担已经转移到更好的网络架构的设计上。<br>  　第3节(RCNN [65]， Fast RCNN [64]， Faster RCNN [175]， YOLO [174]， SSD[136])中综述的主要框架一直在提高检测的准确性和速度。人们普遍认为CNN的表现起着至关重要的作用，而CNN架构是探测器的引擎。因此，最近在检测精度方面的大多数改进都是通过研究新网络的发展来实现的。因此我们首先回顾流行CNN架构中使用通用目标检测,其次是审查表示努力致力于改善目标的特性,如发展不变的特性来适应几何目标规模的变化,姿势,观点,一部分变形和执行多尺度分析,提高目标检测在一个广泛的鳞片。</p> 
<h4><a id="411_CNN_202"></a>4.1.1 受欢迎的CNN架构</h4> 
<p><img src="https://images2.imgbox.com/dd/a5/oD14RZUc_o.png" alt="在这里插入图片描述"><br>  　ZFNet [234] VGGNet[191]、GoogLeNet[200]、Inception系列[99、201、202]、ResNet[79]、DenseNet[94]和SENet[91]等，其在目标识别方面的网络改进如表2所示。对CNN最近进展的进一步回顾可以在[71]中找到。<br> 简单地说，CNN有一个分层结构，由卷积、非线性、池化等层组成。从较细到较粗的层，图像反复经历滤波卷积，每一层都增加了这些滤波器的接受域(支持域)。例如,开拓AlexNet[110]有五个卷积层和两个完全连接(FC)层,和第一层包含96过滤器的大小11×11×3。一般来说，第一个CNN层提取的是低层特征(如边缘)，中间层提取的是复杂度不断增加的特征，如低层特征的组合，后期卷积层检测目标为早期部分的组合[234,12,116,157]。</p> 
<p> 　从表2中可以观察到,架构演变的趋势是,网络越来越深入:AlexNet由8层,VGGNet 16层,以及最近ResNet和DenseNet都超过了100层,这是VGGNet[191]和GoogLeNet[200],特别是,这表明,增加深度可以提高深层网络的表征能力。有趣的是，从表2中可以观察到，AlexNet、OverFeat、ZFNet和VGGNet等网络的参数数量非常庞大，尽管只有几层，因为大部分的参数来自FC层。因此，像Inception、ResNet和DenseNet这样的新网络虽然具有非常大的网络深度，但是由于避免使用FC层，它们的参数要少得多。<br> <img src="https://images2.imgbox.com/17/8f/XRiHcTTc_o.png" alt="在这里插入图片描述"></p> 
<p> 　随着在精心设计的拓扑中使用Inception模块，GoogLeNet的参数大大减少了。同样地，ResNet也证明了跳过连接来学习具有数百层的极深网络的有效性，赢得了ILSVRC 2015分类任务。InceptionResNets[202]受ResNet[79]的启发，将Inception网络与快捷连接相结合，声称快捷连接可以显著加速Inception网络的训练。Huang等人[94]提出了从密集块构建的DenseNets，密集块以前馈方式将每一层连接到每一层，从而带来了诸如参数效率、隐式深度监视和特性重用等引人注目的优势。最近,胡锦涛等。[79]提出了一个建筑单元称为紧缩和激励(SE)块,可以结合现有的深架构来提高他们的性能代价最小的额外计算,通过自适应地调整channelwise特性响应通过显式地建模卷积特性之间的相互依赖关系渠道,导致2017年赢得ILSVRC分类任务。CNN体系结构的研究依然活跃，大量的骨干网络仍在涌现，如扩张的剩余网络[230]，Xception [35]， DetNet[127]，双路径网络(DPN)[31]。</p> 
<p> 　CNN的训练需要一个具有足够标签和内部多样性的大标记数据集。与图像分类不同，检测需要从图像中定位(可能有很多)目标。研究表明[161]，使用具有目标级注释(如ImageNet分类和定位数据集)的大型数据集对深度模型进行预处理，而不仅仅是图像级注释，可以提高检测性能。然而，收集边框标签是很昂贵的，尤其是对于成千上万个类别来说。一个常见的场景是CNN在一个具有图像级别标签的大型数据集(通常具有大量视觉类别)上进行预训练;然后，经过预先训练的CNN可以作为通用特征提取器(172,8,49,228)直接应用到一个小数据集上，它可以支持更广泛的视觉识别任务。对于检测，预训练网络通常在给定的检测数据集上是finetuned2[49, 65, 67]。CNN预处理中使用了几个大规模的图像分类数据集;其中，ImageNet1000数据集[44,179]有120万张图像的1000个目标类别，或者比ImageNet1000大得多但类更少的Places数据集[245]，或者是最近的一个将Places和ImageNet数据集结合起来的混合数据集[245]。</p> 
<p> 　在[49,67,1]中，我们探索了未精细化的预训练CNNs用于目标分类和检测，其中特征性能是提取层的函数;例如，对于在ImageNet上进行预训练的AlexNet, FC6 / FC7 / Pool5的检测准确率呈降序排列[49,67];finetuning预先训练的网络可以显著提高检测性能[65,67]，尽管在AlexNet的例子中，FC6和FC7的finetuning性能提升要比Pool5大得多，这表明Pool5的特性更加普遍。此外，源数据与目标数据集之间的关系或相似性起着关键作用，例如基于ImageNet的CNN特征在与目标相关的图像数据集上表现出更好的性能[243]</p> 
<h4><a id="412__216"></a>4.1.2 改进目标表示的方法</h4> 
<p> 　常见深度学习目标检测网络RCNN [65]， Fast RCNN [64]，Faster RCNN [175] 和YOLO [174]，通常使用2中列出的深CNN架构为骨干网络和使用特性从CNN顶层目标表示,然而检测目标在一个大范围的尺度是一个基本的挑战。解决这个问题的一个经典策略是在大量缩放的输入图像(例如图像金字塔)上运行检测器[56,65,77]，这通常产生更精确的检测，但推理时间和内存有明显的限制。相比之下，CNN逐层计算其特征层次结构，特征层次结构中的子采样层导致固有的多尺度金字塔。<br>  　这个层次结构固有特性产生特征图不同的空间分辨率,但在结构固有的问题[75、138、190]:后来的(或更高版本)层有一个很大的接受域和强大的语义,并最健壮的变化例如目标构成,照明和部分变形,但分辨率低和几何细节丢失。相反，较早的(或较低的)层具有较小的接受域和丰富的几何细节，但分辨率高，对语义的敏感性小得多。直观上，根据目标的大小，目标的语义概念可以出现在不同的层中。因此，如果目标目标很小，那么它需要较早层中的详细信息，并且很可能在较晚层中消失，这在原则上使小目标检测变得非常具有挑战性，因此有人提出了一些技巧，如扩张卷积[229]或无粘性卷积[40,27]。另一方面，如果目标目标很大，那么语义概念将出现在更晚的层中。显然，仅从一层进行特征特征的不同尺度目标的预测并不是最理想的，因此提出了多种方法[190、241、130、104]，通过利用多CNN层来提高检测精度，大致可分为三种多尺度目标检测：</p> 
<ol><li>结合CNN多层特征进行检测[75,103,10]</li><li>多个CNN层进行探测</li><li>以上两种方法的组合[58,130,190,104,246,239]。</li></ol> 
<h5><a id="CNN_225"></a>结合CNN多层特征进行检测</h5> 
<p> 　利用CNN多个层的联合特征进行检测，是在进行预测之前，从多个层结合特征。具有代表性的方法包括Hypercolumns[75]、HyperNet[103]和ION[10]。这种特性的组合通常是通过跳跃连接来完成的，这是一种经典的神经网络思想，它跳过网络中的某些层，将较早层的输出作为后一层的输入，这种体系结构最近在语义分割方面变得流行[138,185,75]。如图10 (a)所示，ION[10]使用跳跃池从多层中提取RoI特征，然后利用组合特征对选择性搜索和边框生成的目标提案进行分类。HyperNet[103],如图10 (b)所示，采用类似的思路，融合深、中、浅层特征，通过端到端联合训练策略生成目标方案并预测目标。这种方法只提取每幅图像中的100个候选区域。组合特性更具有描述性，更有利于定位和分类，但增加了计算复杂度。<img src="https://images2.imgbox.com/85/57/eTi6Nphm_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="CNN_228"></a>多个CNN层进行探测</h5> 
<p> 　在多个CNN层进行检测[138,185]，通过平均分割概率，将多个层的粗预测和细预测结合起来。SSD[136]和MSCNN[20]、RBFNet[135]和DSOD[186]结合了多特征映射的预测来处理不同大小的目标。SSD将不同规模的默认框分散到CNN中的多个层，强制每个层集中于预测某个规模的目标。Liu等[135]提出了RFBNet，简单地将SSD后期的卷积层替换为接受域块(receiver Field Block, RFB)，以增强特征的识别性和鲁棒性。RFB是一个多分支卷积块，类似于初始块[200]，但它将多个具有不同内核的分支和卷积层[27]组合在一起。MSCNN[20]对CNN的多层应用反褶积来增加feature map的分辨率，然后使用这些层来学习区域提议和池特性。</p> 
<h5><a id="_231"></a>以上两种方法的组合</h5> 
<p> 　结合上述两种方法，一方面，通过简单地将skip特征合并到UNet[154]、hypercolumn[75]、HyperNet[103]和ION[10]等检测中，超特征表示的实用性并没有因为高维性而产生显著的改善。另一方面，从较晚的接受域较大的层中检测较大的目标，以及使用较早的接受域较小的层来检测较小的目标是很自然的;然而，简单地从较早的层检测目标可能会导致较低的性能，因为较早的层具有较少的语义信息。因此，为了结合两者的优点，最近的一些作品提出了多层检测目标的方法，通过对不同层的特征进行组合，得到每个检测层的特征。代表方法包括SharpMask[168],Deconvolutional单发射击检测器(DSSD)[58],金字塔网络特性(红外系统)[130],自顶向下调制(TDM)[190],反向连接与客体性网络(罗恩)[104]之前,ZIP<a href="%E5%9B%BE12%E6%89%80%E7%A4%BA" rel="nofollow">122</a>,传输检测网络规模(标准化)[246],RefineDet StairNet和[239][217],如表3所示,对比图11所示。</p> 
<p><img src="https://images2.imgbox.com/31/d1/1FeIPp9I_o.png" alt="在这里插入图片描述"></p> 
<p> 　从图11 (a1)到(e1)可以看出，这些方法具有高度相似的检测体系结构，它们包含了一个自顶向下的网络和横向连接，以补充标准的自底向上前馈网络。具体来说，自底向上传递后，最终的高级语义特征通过自顶向下网络传输回来，并在经过横向处理后与中间层的自底向上特征相结合。结合后的特征被进一步处理，然后用于检测，也通过自顶向下网络传输.可以从图11 (a2)到(e2)，一个主要的区别是反向融合块(RFB)的设计，它处理下层过滤器的选择和多层特征的组合。自顶向下和横向特征通过小卷积处理，并与元素和或元素积或连接相结合。FPN作为一种通用的特征提取器在一些应用中得到了显著的改进，包括目标检测[130,131]和实例分割[80]，例如在基本的更快的RCNN检测器中使用FPN。这些方法必须增加额外的层来获得多尺度的特征，引入了不可忽视的成本。STDN[246]利用DenseNet[94]将不同层的特征进行组合，设计了一个scale transfer模块，获得不同分辨率的特征映射。规模转移模块模块可以直接嵌入DenseNet中，几乎不需要额外的成本。<br> <img src="https://images2.imgbox.com/b2/2a/h9jMxVB8_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/35/b9/rFBTUq7J_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="_241"></a>模型几何转换</h5> 
<p> 　DCNNs天生局限于模型重要的几何变换。一个 DCNN表示与图像变换的不变性和等价性的实证研究可以在[118]中找到。为了提高CNN表示的鲁棒性，我们提出了一些方法，目的是学习不同类型转换的CNN表示，比如scale [101, 18]， rotate[18, 32, 218, 248]，或者两者都要[100]。</p> 
<h5><a id="_244"></a>建模目标变形</h5> 
<p> 　在深度学习之前，基于可变形部件的模型(DPMs)[56]在通用目标检测中非常成功，它通过可变形配置中的组件部件来表示目标。这种DPM模型对物体姿态、视点和非刚性变形的转换不太敏感，因为部件的位置是相应的，并且它们的局部外观是稳定的，这促使研究人员[41,66,147,160,214]明确地建模目标组合，以改进基于CNN的检测。第一次尝试[66,214]使用AlexNet在基于DPM的检测中学习到的深层特性，将DPMs和CNNs结合起来，但是没有区域建议。为了使CNN能够享受到物体零件形变建模的内置功能，我们提出了许多方法，包括DeepIDNet[160]、DCN[41]和DPFCN<a href="%E5%A6%82%E8%A1%A83%E6%89%80%E7%A4%BA" rel="nofollow">147</a>。DeepIDNet[161]设计了一个变形约束池层取代常规max池层共享视觉学习模式及其deformationproperties跨不同的目标类、傣族等。[41]设计了一个可变形的卷积层和一层变形RoI池,这两个是基于的想法增加正则网格采样地点的特征图谱与额外的位置偏移和学习通过旋转偏移,在DPFCN[147]中，Mordan等人提出了基于可变形部分的RoI池层，该层通过同时优化各部分的潜在位移，在目标方案周围选择有区别的部分。</p> 
<h3><a id="42__247"></a>4.2 情境建模</h3> 
<p> 　在物理世界中，视觉目标出现在特定的环境中，通常与其他相关目标共存，有很强的心理学证据[13,9]表明语境在人类目标识别中起着重要的作用。人们认识到，适当的背景建模有助于目标检测和识别[203,155,27,26,47,59]，特别是当目标的外观特征不足，因为小的目标大小，遮挡，或图像质量差。已经讨论了许多不同类型的上下文，特别是参见调查[47,59]。上下文可以大致分为三类[13,59]</p> 
<ol><li>语义上下文:在某些场景中发现目标而在其他场景中不存在的可能性;</li><li>语义上下文:在某些场景中发现目标而在其他场景中不存在的可能性</li><li>缩放上下文:目标相对于场景中的其他目标有一个有限的大小集合。</li></ol> 
<p> 　在深度学习普及之前，有大量的工作[28,47,59,143,152,171,162]，然而大部分工作还没有在基于DCNN的目标探测器中进行探索[29,90]。</p> 
<p> 　目标检测的当前技术状态[175,136,80]检测目标没有明确利用任何上下文信息。人们普遍认为，DCNNs隐式地利用上下文信息[234,242]，因为它们学习了具有多层抽象的层次表示。尽管如此，在基于DCNN的检测器中明确地探索上下文信息仍然有价值[90,29,236]，因此，下面将回顾最近在基于DCNN的目标检测器中挖掘上下文线索的工作，这些工作被组织为全局上下文和局部上下文，这是由[240,59]中的早期工作所激发的。表4总结了具有代表性的方法。<br> <strong>全局上下文</strong>[240,59]是指图像或场景级上下文，可以作为目标检测的线索(例如，卧室可以预测床的存在)。DeepIDNet[160]将图像分类分数作为上下文特征，与目标检测分数相连接，提高检测结果。在ION[10]中，Bell等人提出使用空间递归神经网络(RNNs)来探索整个图像的上下文信息。在SegDeepM[250]中，Zhu等人提出了一种MRF模型，对每次检测的外观和上下文进行打分，并允许每个候选框选择一个片段，对它们之间的协议进行打分。在[188]中，语义分割被用作语境启动的一种形式。</p> 
<p><strong>本地上下文</strong>[240,59,171]考虑目标关系中的本地环境，目标与其周围区域之间的交互。一般来说,建模目标的关系是具有挑战性的,需要推理不同的类的边界框,位置、尺度等。在深度学习时代,研究显式模型目标的关系是相当有限的,代表的是空间记忆网络(SMN)[29],[90]目标关系网络,和结构推断网络(罪)[137]。在SMN中，空间内存本质上是将目标实例组装回一个伪图像表示，很容易将其输入到另一个CNN中进行目标关系推理，这导致了一种新的顺序推理体系结构，其中图像和内存并行处理，以获得检测结果，从而进一步更新内存。Hu等人[90]受自然语言处理领域注意力模块近期成功的启发[211]，提出了一种轻量级ORN，它通过外观特征和几何形状之间的交互，同时处理一组目标。它不需要额外的监督，而且很容易嵌入到现有的网络中。实践证明，该方法能够有效地改进现代目标检测管道中的目标识别和重复删除步骤，从而产生了第一个完全端到端的目标检测器。SIN[137]考虑了两种上下文，包括场景上下文信息和单一图像中的目标关系。将目标检测问题归结为图结构推理问题，在给定图像的情况下，将目标视为图中的节点，将目标之间的关系建模为图中的边<br> 更广泛的方法更简单地解决了这个问题，通常是通过增大检测窗口大小来提取某种形式的局部上下文。代表性的方法有MRCNN[62]、门控双向CNN (GBDNet)[235, 236]、对语境CNN (ACCNN)[123]、CoupleNet[251]、Sermanet等[182]。</p> 
<p> 　在MRCNN<a href="a" rel="nofollow">62</a>(图13),除了功能从原始目标中提取建议在最后CONV骨干层,Gidaris和Komodakis提出提取目标的特性从不同的地区建议(一半地区、边境地区、中部地区、上下文区域和语义分割区域),以获得更丰富、更健壮的目标表示。所有这些特性都通过连接简单地组合在一起。<br>  　从那以后，人们提出了许多与MRCNN密切相关的方法。[233]中的方法只使用了四个上下文区域，组织在一个中央凹结构中，在那里分类器被共同训练到末端。Zeng等人提出了GBDNet<a href="%E5%9B%BE13" rel="nofollow" title="b">235,236</a>，从目标方案周围的多尺度上下文化区域提取特征，以提高检测性能。与单纯的对每个区域分别学习CNN特征并将其连接起来的方法不同，GBDNet可以通过卷积的方式在不同上下文区域的特征之间传递消息。Zeng等人注意到消息传递并不总是有帮助的，而是依赖于单个样本，他们使用门控函数来控制消息传输，就像在长短时记忆(Long - Short - Term Memory, LSTM)网络中一样[83]。与GBDNet同时，Li等[123]提出了ACCNN(图13 ©)，利用全局和局部上下文信息方便目标检测。为了捕获全局上下文，我们提出了一个多尺度的局部上下文化(MLC)子网络，该子网络通过多重叠加的LSTM层为输入图像生成一个注意力映射，以突出有用的全局上下文位置。Li等[123]为了对局部环境上下文进行编码，采用了与MRCNN相似的方法[62]。如图13 (d)所示，CoupleNet[251]在概念上与ACCNN[123]相似，但构建于RFCN[40]之上。除了RFCN[40]中的原始分支使用位置敏感的RoI池来捕获目标信息外，CoupleNet[251]还添加了一个分支来使用RoI池对全局上下文信息进行编码。</p> 
<h3><a id="43_264"></a>4.3检测的建议方法</h3> 
<p> 　一个物体可以定位在图像的任何位置和比例。在手工制作特征描述符(例如SIFT[140]、HOG[42]和LBP[153])的鼎盛时期，词包(BoW)[194,37]和DPM[55]使用滑动窗口技术[213,42,55,76,212]。然而，窗口的数量是很大的，并且随着图像中像素的增加而增加，在多个尺度和纵横比下的搜索需求进一步显著增加了搜索空间。因此，应用更复杂的分类器在计算上过于昂贵。<br> 在2011年左右，研究人员提出了利用检测方案s3来缓解计算可操作性和高检测质量之间的紧张关系[210,209]。object proposal源于[2]提出的object - eness这个概念，object proposal是图像中可能包含目标的一组候选区域。检测建议通常作为预处理步骤，以通过限制需要由检测器评估的区域数量来降低计算复杂度。因此，一个好的检测方案应该具有以下特征:</p> 
<ol><li>高召回，只需提出几个建议即可实现;</li><li>建议尽可能准确地匹配目标;</li><li>高效率</li><li></ol> 
<p> 　基于选择性搜索给出的检测建议[210,209]的目标检测的成功吸引了广泛的兴趣[21,7,3,33,254,50,105,144]。</p> 
<p> 　对目标建议书算法的全面综述超出了本文的范围，因为目标建议书具有超出目标检测的应用[6,72,252]。我们请感兴趣的读者参考最近的调查[86,23]，该调查对许多经典的目标建议算法及其对检测性能的影响进行了深入分析。我们的兴趣在于回顾基于DCNNs、输出类无关建议以及与通用目标检测相关的目标建议方法。</p> 
<p> 　2014年，目标提案[210,209]与DCNN特性[109]的融合，使得RCNN在通用目标检测领域具有里程碑意义[65]。从那时起，检测方案算法迅速成为标准的预处理步骤，2014年以来PASCAL VOC[53]、ILSVRC[179]和MS COCO[129]目标检测挑战的所有获奖作品都使用了检测方案[65,160,64,175,236,80]。<br> <img src="https://images2.imgbox.com/a1/1a/Z2imzCjW_o.png" alt="在这里插入图片描述"></p> 
<p> 　在基于传统低级线索(如颜色、纹理、边缘和渐变)的目标提议方法中，选择性搜索[209]、MCG[7]和edgebox[254]是最流行的。随着领域的快速发展，传统的目标提议方法<a href="%E5%A6%82%E9%80%89%E6%8B%A9%E6%80%A7%E6%90%9C%E7%B4%A2%5B209%5D%E5%92%8C%5B254%5D" rel="nofollow">86</a>作为独立于检测器的外部模块，成为检测管道的瓶颈[175]。使用DCNNs的一类新出现的目标建议算法[52,175,111,61,167,224]引起了广泛的关注。</p> 
<p>最近基于DCNN的目标提议方法一般分为两类:基于边框的和基于目标段的，有代表性的方法如表5所示。<br> <img src="https://images2.imgbox.com/03/3e/KY5ozXb3_o.png" alt="在这里插入图片描述"></p> 
<p><strong>边框建议方法</strong>,Ren等人的RPC方法[175]最能说明包围盒提案方法，如图14所示。RPN通过在最后一个共享CONV层的feature map上滑动一个小网络来预测目标提案(如图14所示)。在每个滑动窗口位置，它通过使用k个锚箱同时预测k个提案，其中每个锚箱4在图像的某个位置居中，并与特定的比例和长宽比相关联。Ren等人[175]提出通过共享卷积层将RPN和Fast RCNN集成到一个网络中。这样的设计导致了大量的加速和首个端到端检测管道，更快的RCNN[175]。从表3和表4可以看出，RPN已被许多最先进的目标探测器广泛选择为提案方法</p> 
<p><img src="https://images2.imgbox.com/a2/9c/uTWRR0Sf_o.png" alt="在这里插入图片描述"></p> 
<p> 　Lu等人[141]没有将一组锚固定为MultiBox[52,199]和RPN[175]，而是提出使用递归搜索策略生成锚点位置，该策略可以自适应地引导计算资源关注可能包含目标的子区域。从整个图像开始，搜索过程中访问的所有区域都充当锚。对于搜索过程中遇到的任何锚点区域，使用标量变焦指示器来决定是否进一步划分该区域，并使用一个称为邻接和变焦网络(AZNet)的深度网络计算一组具有目标得分的边界框。AZNet通过添加一个分支来扩展RPN，以与现有分支并行计算标量缩放指示器。</p> 
<p> 　还有进一步的工作试图通过利用多层卷积特性来生成目标提案[103,61,224,122]。Ghodrati等人[61]与RPN[175]同时提出了DeepProposal，该方案通过使用多个卷积特性的级联来生成目标建议，构建一个逆级联来选择最有前途的目标位置，并以一种粗到细的方式来完善它们的框。HyperNet[103]是RPN的一种改进变体，它设计了一种超级特征，这种特征将多层卷积特征聚合在一起，并通过端到端联合训练策略在生成提案和检测目标时共享。Yang等人提出的CRAFT[224]也采用了cascade策略，首先训练RPN网络生成目标提案，然后用它们训练另一个二进制快速RCNN网络，进一步区分目标和背景。Li等人[122]提出了一种用于改进RPN的ZIP方法，该方法利用了一种常用的方法，即在网络的不同深度使用多个卷积特征映射来预测目标提议，从而集成了低层细节和高层语义。ZIP中使用的骨干网是一个缩小的网络，受conv和deconv结构的启发[138]。</p> 
<p> 　最后，最近值得一提的工作包括Deepbox[111]，它提出了一个轻量级的CNN来学习如何重新排列EdgeBox生成的提案，DeNet[208]引入了一个包围盒角估计来有效预测目标提案，从而在一个更快的RCNN风格的两级检测器中取代RPN。</p> 
<p><strong>目标段建议书方法</strong>[167,168]旨在生成可能与目标相对应的段建议书。段建议比包围盒建议更具有信息性，并且在目标实例分割方面更进一步[74,39,126]。Pinheiro等人[167]提出的DeepMask是一项开创性的工作，通过深度网络直接从原始图像数据中学习片段建议。与RPN共享相似点之后，在多个共享卷积层之后，DeepMask将网络分割为两个分支，以预测一个类无关的遮罩和一个相关的目标得分。类似于OverFeat中的高效滑动窗口预测策略[183]，经过训练的DeepMask网络在推理过程中以滑动窗口的方式应用于图像(及其重新调整后的版本)。最近Pinheiro等人[168]提出SharpMask，通过使用细化模块对DeepMask体系结构进行增强，类似于图11 (b1)和(b2)所示的体系结构，通过自顶向下的细化过程对前馈网络进行增强。SharpMask可以有效地将早期特征中丰富的空间信息与后期层中编码的强语义信息结合起来，生成高保真目标掩码。</p> 
<p> 　由于完全卷积网络(FCN)用于语义分割[138]和DeepMask [167]， Dai等人提出InstanceFCN[38]用于生成实例段建议。与DeepMask类似，InstanceFCN网络被拆分为两个分支，但是这两个分支完全是卷积的，其中一个分支生成一小组实例敏感评分映射，然后是输出实例的组装模块，另一个分支用于预测目标得分。Hu等人提出的FastMask[89]以类似于SSD[136]的一次性方式高效生成实例段提案，以便在深度网络中利用多尺度卷积特性。从多尺度卷积特征图中提取的滑动窗口被输入到一个可缩放的注意头模块，以预测分割掩码和目标得分。FastMask声称在13 FPS 800×600分辨率图像与一个轻微的贸易平均召回。Qiao等人[170]提出ScaleNet，通过显式地添加一个规模预测阶段，对SharpMask等[168]等以前的目标建议方法进行了扩展。也就是说，ScaleNet估计了输入图像的目标尺度分布，SharpMask在此基础上根据ScaleNet预测的尺度搜索输入图像，并输出实例段建议。乔等人[170]的研究表明，他们的方法在超市数据集上的表现远远优于之前的先进水平。</p> 
<h3><a id="44__298"></a>4.4 其他特殊问题</h3> 
<p> 　为了获得更好、更健壮的DCNN特征表示，通常使用数据增强技巧[22,64,65]。它可以在训练时使用，在测试时使用，或者两者都使用。增强是指通过使基础类别保持不变的转换(如裁剪、翻转、旋转、缩放和翻译)来扰乱图像，以生成类的其他样本。数据增强会影响深度特征表示的识别性能。然而，它有明显的局限性。训练和推理的计算复杂度都显著增加，限制了其在实际应用中的应用。在大范围尺度变化下的目标检测，特别是对非常小的目标的检测是一个关键的挑战。已经证明[96,136]，图像分辨率对检测精度有相当大的影响。因此，在这些数据增强技巧中，缩放(尤其是更高分辨率的输入)是最常用的，因为高分辨率的输入增加了小目标被检测的可能性[96]。最近Singh等人提出了先进高效的数据论证方法SNIP[192]和SNIPER[193]来说明尺度不变性问题，如表6所示。Singh等人基于对小物体和大物体在较小尺度和较大尺度下难以检测的直观理解，提出了一种新的训练方案SNIP可以在训练过程中减小尺度变化，但不减少训练样本。狙击手[193]是一种有效的多尺度训练方法。它只在适当的比例下处理ground truth目标周围的上下文区域，而不是处理整个图像金字塔。Shrivastava等人[189]和Lin等人探索了处理极端前背景类失衡问题的方法[131]。Wang等[216]提出通过训练一个对抗性网络来生成目标检测器难以识别的具有遮挡和变形的例子。有一些工作集中于开发更好的非最大抑制方法[16,87,207]</p> 
<h2><a id="5_Datasets_and_Performance_Evaluation_302"></a>5. Datasets and Performance Evaluation</h2> 
<h3><a id="51__304"></a>5.1 数据集</h3> 
<p> 　在物体识别研究的历史上，数据集一直扮演着重要的角色。它们已成为该领域取得长足进展的最重要因素之一，不仅是衡量和比较竞争算法性能的共同基础，而且还将该领域推向日益复杂和具有挑战性的问题。目前可以在互联网上访问大量的图像，这使得建立越来越多的图像和类别的综合数据集成为可能，以获取越来越丰富和多样化的目标。拥有数百万张图像的大规模数据集的兴起为重大突破铺平了道路，并在目标识别方面实现了前所未有的性能。认识到空间的局限性，我们请感兴趣的读者参考几篇论文[53,54,129,179,107]以详细描述相关数据集。<br> <img src="https://images2.imgbox.com/cd/b9/5AI5VM05_o.png" alt="在这里插入图片描述"><br>  　从Caltech101[119]开始，代表性的数据集包括Caltech256[70]、Scenes15[114]、PASCAL VOC(2007)[54]、Tiny Images[204]、CIFAR10[108]、SUN[221]、ImageNet[44]、Places[245]、MS COCO[129]、Open Images[106]。表7总结了这些数据集的特征，选取的样本图像如图15所示。</p> 
<p> 　较早的数据集，如Caltech101或Caltech256，由于缺乏所显示的内部变化而受到批评。因此，通过寻找描述各种场景类别的图像来收集SUN[221]，其中很多图像都有场景和目标标注，可以支持场景识别和目标检测。Tiny Images[204]以前所未有的规模创建了一个数据集，全面覆盖了所有的目标类别和场景，但是它的注释没有人工验证，包含了大量的错误，因此两个具有可靠标签的基准(CIFAR10和CIFAR100[108])都来自于Tiny Images。</p> 
<p> 　PASCAL VOC[53,54]是一项持续多年的工作，致力于创建和维护一系列用于分类和目标检测的基准数据集，开创了以年度竞赛形式对识别算法进行标准化评估的先例。从2005年的四类开始，增加到日常生活中常见的20类，如图15所示。ImageNet[44]包含超过1400万幅图像和超过20000个类别，这是ILSVRC[44,179]挑战赛的支柱，将目标识别研究推向了一个新的高度。</p> 
<p> 　ImageNet一直受到批评，认为数据集中的目标往往很大，并且集中得很好，这使得数据集在真实世界场景中是非典型的数据集。为了解决这个问题，并将研究推向更丰富的图像理解，研究人员创建了MS COCO数据库[129]。COCO图像是复杂的日常场景，包含了自然环境下的普通物体，更接近于现实生活，目标通过充分分割实例进行标记，以提供更精确的检测器评估。Places数据库[245]包含了1000万幅场景图像，这些图像被标记为场景语义类别，为数据饥渴的深度学习算法提供了机会，以达到人类对视觉模式的层次识别。最近，Open Images[106]是一个包含约900万张图像的数据集，这些图像已经用图像级别标签和目标边界框进行了注释。</p> 
<p><img src="https://images2.imgbox.com/93/17/AR3eNSwe_o.png" alt="在这里插入图片描述"><br>  　通用目标检测有三个著名的挑战:PASCAL VOC[53,54]、ILSVRC[179]和MS COCO[129]。每个挑战由两个部分组成:(1)公开可获得的图像数据集，连同地面真相注释和标准化评估软件;(2)年度比赛及相应的工作坊。表8给出了用于检测挑战的训练、验证和测试数据集5中图像和目标实例数量的统计数据。</p> 
<p> 　对于PASCAL VOC的挑战，从2009年开始，这些数据包括了前几年的图像与新图像的叠加，使得图像的数量每年都在增加，更重要的是，这意味着测试结果可以与前几年的图像进行比较。</p> 
<p> 　ILSVRC[179]将PASCAL VOC检测算法的标准化训练和评估目标在目标类和图像数量上提升了一个数量级。ILSVRC目标检测挑战从2013年到现在每年都在进行</p> 
<p> 　COCO目标检测挑战旨在推动通用目标检测技术的发展，从2015年开始每年进行一次。它具有两个目标检测任务:使用边界框输出或目标实例分割输出。它的目标类别比ILSVRC少(COCO中是80个，而ILSVRC目标检测中是200个)，但是每个类别的实例多(平均11000个，而ILSVRC目标检测中是2600个)。此外，它还包含目标分割注释，这些注释目前在ILSVRC中是不可用的。COCO引入了一些新的挑战:(1)它包含了范围广泛的目标，包括高百分比的小目标(例如小于图像区域的1%[192])。(2)目标不那么具有标志性，并且处于杂乱或严重遮挡之中，(3)评价指标(见表9)鼓励更精确的目标定位。</p> 
<p> 　COCO已成为应用最广泛的通用目标检测数据集，表8总结了用于培训、验证和测试的数据集统计数据。从2017年开始，测试集只有Dev和Challenge部分，其中test -Dev部分是默认的测试数据，并且论文中的结果通常报告在test -Dev上，以便进行公平的比较。</p> 
<p> 　2018年，继PASCAL VOC、ImageNet和COCO的传统之后，开放图像目标检测挑战开始引入，但规模空前。与以前的挑战相比，它提供了更广泛的目标类，并且有两个任务:500个不同类的包围盒目标检测和检测特定关系中的目标对的视觉关系检测。</p> 
<h3><a id="52__329"></a>5.2 评价指标</h3> 
<p> 　评价检测算法性能的标准有三个:检测速度(帧每秒，FPS)、精度和召回率。最常用的度量标准是平均精度(AP)，它是从精度和召回得出的。AP通常以特定于类别的方式进行评估，即，分别计算每个目标类别。在一般的目标检测中，检测器通常是通过检测一些目标类别来进行测试的。为了比较所有目标类别的性能，采用所有目标类别的平均AP (mAP)作为性能的最终度量。关于这些指标的更多细节可以在[53,54,179,84]中找到。</p> 
<p> 　用于测试图像I的检测器的标准输出是否有预测的探测<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
         
         
           { 
          
         
           ( 
          
          
          
            b 
           
          
            j 
           
          
         
           , 
          
          
          
            c 
           
          
            j 
           
          
         
           , 
          
          
          
            p 
           
          
            j 
           
          
         
           ) 
          
         
           } 
          
         
        
          j 
         
        
       
      
        \left\{ (b_j,c_j,p_j) \right\}_j 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.22192em; vertical-align: -0.471916em;"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top: 0em;">{<!-- --></span><span class="mopen">(</span><span class="mord"><span class="mord mathit">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right: 0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathit">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right: 0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathit">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right: 0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose delimcenter" style="top: 0em;">}</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.125856em;"><span class="" style="top: -2.36419em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right: 0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.471916em;"><span class=""></span></span></span></span></span></span></span></span></span></span>,索引为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         j 
        
       
      
        j 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.85396em; vertical-align: -0.19444em;"></span><span class="mord mathit" style="margin-right: 0.05724em;">j</span></span></span></span></span>。给定检测<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ( 
        
       
         b 
        
       
         , 
        
       
         c 
        
       
         , 
        
       
         p 
        
       
         ) 
        
       
      
        (b,c,p) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord mathit">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathit">c</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathit">p</span><span class="mclose">)</span></span></span></span></span>表示具有预测类别标签<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         c 
        
       
      
        c 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathit">c</span></span></span></span></span>和置信水平<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         p 
        
       
      
        p 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span class="mord mathit">p</span></span></span></span></span>的预测位置(边界框，BB)<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         b 
        
       
      
        b 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathit">b</span></span></span></span></span>。预测检测<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ( 
        
       
         b 
        
       
         , 
        
       
         c 
        
       
         , 
        
       
         p 
        
       
         ) 
        
       
      
        (b,c,p) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord mathit">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathit">c</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathit">p</span><span class="mclose">)</span></span></span></span></span>被认为是一个检测正确的结果(TP)。如果</p> 
<ul><li>预测标签与真实标签相同</li><li>重叠比IOU(相交于并)<br> <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          I 
         
        
          O 
         
        
          U 
         
        
          ( 
         
        
          b 
         
        
          , 
         
         
         
           b 
          
         
           g 
          
         
        
          ) 
         
        
          = 
         
         
          
          
            a 
           
          
            r 
           
          
            e 
           
          
            a 
           
          
            ( 
           
          
            b 
           
          
            ⋂ 
           
           
           
             b 
            
           
             g 
            
           
          
            ) 
           
          
          
          
            a 
           
          
            r 
           
          
            e 
           
          
            a 
           
          
            ( 
           
          
            b 
           
          
            ⋃ 
           
           
           
             b 
            
           
             g 
            
           
          
            ) 
           
          
         
        
       
         IOU(b,b^g)=\frac{area(b\bigcap b^g)}{area(b\bigcup b^g)} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathit" style="margin-right: 0.07847em;">I</span><span class="mord mathit" style="margin-right: 0.02778em;">O</span><span class="mord mathit" style="margin-right: 0.10903em;">U</span><span class="mopen">(</span><span class="mord mathit">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathit">b</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.664392em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right: 0.03588em;">g</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.53001em; vertical-align: -0.520007em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.01001em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">a</span><span class="mord mathit mtight" style="margin-right: 0.02778em;">r</span><span class="mord mathit mtight">e</span><span class="mord mathit mtight">a</span><span class="mopen mtight">(</span><span class="mord mathit mtight">b</span><span class="mspace mtight" style="margin-right: 0.195167em;"></span><span class="mop op-symbol small-op mtight" style="position: relative; top: -5e-06em;">⋃</span><span class="mspace mtight" style="margin-right: 0.195167em;"></span><span class="mord mtight"><span class="mord mathit mtight">b</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.593543em;"><span class="" style="top: -2.786em; margin-right: 0.0714286em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathit mtight" style="margin-right: 0.03588em;">g</span></span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.48501em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">a</span><span class="mord mathit mtight" style="margin-right: 0.02778em;">r</span><span class="mord mathit mtight">e</span><span class="mord mathit mtight">a</span><span class="mopen mtight">(</span><span class="mord mathit mtight">b</span><span class="mspace mtight" style="margin-right: 0.195167em;"></span><span class="mop op-symbol small-op mtight" style="position: relative; top: -5e-06em;">⋂</span><span class="mspace mtight" style="margin-right: 0.195167em;"></span><span class="mord mtight"><span class="mord mathit mtight">b</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.738543em;"><span class="" style="top: -2.931em; margin-right: 0.0714286em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathit mtight" style="margin-right: 0.03588em;">g</span></span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.520007em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></li></ul> 
<p> 　在预测的BB b和真实标记之间，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          b 
         
        
          g 
         
        
       
      
        b^g 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathit">b</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.664392em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right: 0.03588em;">g</span></span></span></span></span></span></span></span></span></span></span></span>不小于预定义的阈值<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ε 
        
       
      
        \varepsilon 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathit">ε</span></span></span></span></span>。此处面积<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         a 
        
       
         r 
        
       
         e 
        
       
         a 
        
       
         ( 
        
       
         b 
        
       
         ⋂ 
        
        
        
          b 
         
        
          g 
         
        
       
         ) 
        
       
      
        area(b\bigcap b^g) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.00001em; vertical-align: -0.25001em;"></span><span class="mord mathit">a</span><span class="mord mathit" style="margin-right: 0.02778em;">r</span><span class="mord mathit">e</span><span class="mord mathit">a</span><span class="mopen">(</span><span class="mord mathit">b</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mop op-symbol small-op" style="position: relative; top: -5e-06em;">⋂</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathit">b</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.664392em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right: 0.03588em;">g</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>表示预测与真实标记<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         B 
        
       
         B 
        
       
         s 
        
       
      
        BBs 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathit" style="margin-right: 0.05017em;">B</span><span class="mord mathit" style="margin-right: 0.05017em;">B</span><span class="mord mathit">s</span></span></span></span></span>的交集，面积<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         a 
        
       
         r 
        
       
         e 
        
       
         a 
        
       
         ( 
        
       
         b 
        
       
         ⋃ 
        
        
        
          b 
         
        
          g 
         
        
       
         ) 
        
       
      
        area(b\bigcup b^g) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.00001em; vertical-align: -0.25001em;"></span><span class="mord mathit">a</span><span class="mord mathit" style="margin-right: 0.02778em;">r</span><span class="mord mathit">e</span><span class="mord mathit">a</span><span class="mopen">(</span><span class="mord mathit">b</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mop op-symbol small-op" style="position: relative; top: -5e-06em;">⋃</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathit">b</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.664392em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right: 0.03588em;">g</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>表示两者的结合。<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ε 
        
       
      
        \varepsilon 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathit">ε</span></span></span></span></span>的典型值是0.5。<br> 否则，它被认为是假阳性(FP)。置信水平p通常与一些阈值β确定预测类标签c是接受。</p> 
<p><img src="https://images2.imgbox.com/dc/95/NEN8OVPq_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="6__342"></a>6. 总结</h2> 
<p> 　通用目标检测是计算机视觉中一个重要而又具有挑战性的问题，受到了广泛的关注。由于深度学习技术的显著发展，目标检测领域发生了巨大的变化。本文是对一般目标检测的深度学习的综合考察，重点介绍了近年来的研究成果，根据方法在检测中的作用进行了结构分类，总结了现有的常用数据集和评价标准，讨论了最具代表性的方法的性能。</p> 
<p> 　尽管在过去几年中取得了巨大的成功(例如，检测准确率从ILSVRC2013的23%显著提高到ILSVRC2017的73%)，但在技术水平和人类水平之间仍然存在巨大的差距，尤其是在开放世界学习方面。还有很多工作要做，我们将重点放在以下八个领域。</p> 
<h3><a id="1_Open_World_Learning_347"></a>(1) Open World Learning</h3> 
<p> 　目标检测的最终目的是能够准确有效地识别和定位所有开放世界场景中所有目标类别(数千个或更多目标类)的实例，与人类视觉系统相竞争。最近的目标检测算法是通过有限的数据集PASCAL VOC、ILSVRC与 MS COCO 来学习的，识别和定位数据集中所包含的目标类别，但原则上对数据集中之外的其他目标类别是盲目的，尽管理想的强大检测系统应该能够识别新的目标类别。目前的检测数据集PASCAL VOC、ILSVRC与 MS COCO只包含几十到几百个类别，这远远小于人类能够识别的类别。为了实现这一目标，需要开发新的大规模标记数据集，这些数据集具有明显更多的类别，用于通用目标检测，因为CNNs的技术水平需要大量数据才能很好地训练。然而，收集如此大量的数据，特别是用于目标检测的包围盒标签，是非常昂贵的，尤其是对于数十万个类别而言。</p> 
<h3><a id="2_Better_and_More_Efficient_Detection_Frameworks_351"></a>(2) Better and More Efficient Detection Frameworks</h3> 
<p> 　通用目标检测取得巨大成功的因素之一是开发了更好的基于区域的检测框架。基于区域的探测器具有最高的精度，但对于嵌入式或实时系统来说，计算量太大。单级探测器有可能更快、更简单，但尚未达到基于区域的探测器的精度。一个可能的限制是艺术目标探测器的状态依赖于底层的骨干网络,已初步优化了图像分类,导致学习偏差由于分类和检测之间的差异,这样一个潜在的战略是从头开始学习目标探测器,例如 DSOD检测器。</p> 
<h3><a id="3___Compact_and_Efficient_Deep_CNN_Features_354"></a>(3) Compact and Efficient Deep CNN Features</h3> 
<p> 　在通用目标检测方面取得重大进展的另一个重要因素是强大的深度CNNs的开发，其深度显著增加，从几个层(如AlexNet)到数百层(如ResNet、DenseNet)。这些网络有数百万至数亿个参数，需要大量数据和耗电的gpu进行培训，这再次限制了它们的应用程序只能用于实时/嵌入式应用程序。因此，设计紧凑和轻量级网络、网络压缩和加速以及网络解释和理解的研究兴趣越来越大。</p> 
<h3><a id="4___Robust_Object_Representations_357"></a>(4) Robust Object Representations</h3> 
<p> 　使物体识别问题如此具有挑战性的一个重要因素是现实世界图像的巨大变异性，包括视点和光照变化、物体比例尺、物体姿态、物体零件变形、背景杂波、遮挡、外观变化、图像模糊、图像分辨率、噪声、相机限制和失真。尽管在深度网络方面取得了进展，但由于缺乏对这些变体的健壮性，它们仍然受到限制，这极大地限制了实际应用程序的可用性。</p> 
<h3><a id="5__Context_Reasoning_360"></a>(5) Context Reasoning</h3> 
<p> 　现实世界的目标通常与其他目标和环境共存。人们已经认识到，上下文信息(目标关系、全局场景统计)有助于目标的检测和识别，特别是在小的或遮挡的目标或图像质量较差的情况下。在深度学习之前有大量的工作要做，但是自从深度学习时代以来，在挖掘语境信息方面的进展非常有限。如何有效地结合上下文信息仍有待探索，理想的指导是人类如何快速地将注意力引导到自然场景中感兴趣的目标。</p> 
<h3><a id="6__Object_Instance_Segmentation_364"></a>(6) Object Instance Segmentation</h3> 
<p> 　持续的趋势朝着更丰富和更详细的理解图像内容(例如,从图像分类单一目标定位目标检测),下一个挑战将是解决像素级目标实例分割,目标实例分割可以发挥重要的作用在许多潜在的应用,需要个人的精确边界实例。</p> 
<h3><a id="7__Weakly_Supervised_or_Unsupervised_Learning_368"></a>(7) Weakly Supervised or Unsupervised Learning</h3> 
<p> 　当前状态的艺术探测器采用全程指导模型从标签数据与目标边界框或分割面具，然而这种完全监督学习存在严重的局限性,在边界框注释的假设可能会有问题,尤其是当目标类别的数量很大。完全监督学习在没有完全标记训练数据的情况下是不可扩展的，因此研究CNNs在弱监督或非监督检测中如何利用其威力是有价值的。</p> 
<h3><a id="8__3D_Object_Detection_371"></a>(8) 3D Object Detection</h3> 
<p> 　深度相机的发展使得深度信息能够以RGB-D图像或3D点云的形式获取。深度模态可以用来帮助目标检测和识别，但在这个方向上的工作有限，但这可能得益于大量高质量CAD模型的集合。</p> 
<p><strong>一般目标检测的研究领域还很不完善;鉴于过去5年在算法方面取得的巨大突破，我们仍对未来5年的机遇持乐观态度。</strong></p> 
<p><img src="https://images2.imgbox.com/56/12/La1bj2KC_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/4d/6b/9uAsdKB9_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_381"></a>参考文献</h3> 
<p><a href="https://arxiv.org/abs/1809.02165" rel="nofollow">Deep Learning for Generic Object Detection: A Survey</a><br> <a href="https://mp.weixin.qq.com/s/BFL2PhAojZ5zHstgTrhlrw" rel="nofollow">悉尼大学欧阳万里等人30页最新目标检测综述-专知</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4732154ef81bd002c5573674994bd560/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">numpy数组与list之间的转换</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d1a85018ef3a2089dd27839d635c11c4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Mission Planner 开发环境搭建 二次开发编译方法与问题解决   新手篇 vs2017</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>