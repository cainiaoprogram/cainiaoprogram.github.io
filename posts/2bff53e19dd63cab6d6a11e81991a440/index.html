<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>（2）Hadoop笔记：hadoop-eclipse-plugin插件的安装和mapReduce小栗子 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="（2）Hadoop笔记：hadoop-eclipse-plugin插件的安装和mapReduce小栗子" />
<meta property="og:description" content="注： 1.eclipse所在环境为windows 2.hadoop版本2.8.3 3.hadoop-eclipse-plugin版本2.8.3 4.eclipse版本Luna Service Release 1 (4.4.1) 5.JDK 1.7
插件安装 hadoop-eclipse-plugin编译 因为我本地使用的JDK为1.7，而现在网上能找到的hadoop-eclipse-plugin-2.8.3都是基于JDK1.8编译的，所以都不能使用，因此需要下载hadoop-eclipse-plugin的源码然后在本地编译，过程中需要Hadoop-2.8.3的支持。当然如果你能找到符合自己环境的插件包，这步就可以跳过了。 Hadoop-2.8.3.tar.gz hadoop2x-eclipse-plugin （github托管的源码） apache-ant-1.9.11-bin.zip 下载完后解压apache-ant-1.9.11-bin.zip，并配置环境变量。 新建ANT_HOME=E:apache-ant-1.9.4 在PATH后面加;%ANT_HOME%\bin 测试下
ant -version 解压Hadoop-2.8.3.tar.gz，hadoop2x-eclipse-plugin 编辑E:\hadoop2x-eclipse-plugin-master\src\contrib\eclipse-plugin\build.xml 添加下图红框中的部分，3项代表的意思应该都懂，就不多说了。 然后往下翻能发现有一堆这个东西，之后编译时大概会出错的地方。先打开放在这，不用修改。① 然后编辑E:\hadoop2x-eclipse-plugin-master\ivy\libraries.properties.xml 一般来讲只要修改下红框中的版本即可，然后这里有一大堆版本，而上面①图中那些都是在引用这里的版本。这里的版本又对应着Hadoop-2.8.3中的资源。在编译时可能会出现这里写的版本和Hadoop实际用的版本不一样导致找不到jar包的报错，这时就可以根据报错信息来修改下方的版本号，使得符合实际。基本都在${hadoop.home}/share/hadoop/common/lib文件夹中 进入hadoop2x-eclipse-plugin-master\src\contrib\eclipse-plugin目录，运行cmd命令行
ant jar 编译完成，遇到错误就根据提示修改，最大可能遇到的就是我上面提到的问题，还有当编译时长时间卡在ivy-resolve-common: 处时，大概率已经编译失败。 编译完成后的文件在 hadoop2x-eclipse-plugin-master\build\contrib\eclipse-plugin文件夹中 将jar包放入eclipse/plugins文件夹中，重启eclipse 可以看到如下图标 添加 window-&gt;show view-&gt;other 配置连接，如下图所示，随便取个名字就行 然后如果连接成功的话就如下图所示，这里刚新搭建的Hadoop环境的话应该一开始就是空的，所以可能会抛空指针异常，没什么影响的，直接新建东西就行。 新建后需要刷新一下，同时可以访问http://xx.xx.xx.xx:50070/explorer.html#/网址查看是否真的新建了。 通过这个我们可以轻松快捷的将文件上传给Hadoop文件系统，方便本地调试。
window-&gt;preferences，选择Hadoop文件，这里主要目的是可以直接引用相关jar包 使用例子，单词计数 首先需要下载hadoop.dll，这个网上找找对应版本的吧，我用的是2.X的，放在C/windows/System32下。 File-&gt;New-&gt;other next，填写名字，finish 新建4个文件 WordCountMapper.java
package com.hadoop.demo; import java.io.IOException; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Mapper; /* * KEYIN：输入kv数据对中key的数据类型 * VALUEIN：输入kv数据对中value的数据类型 * KEYOUT：输出kv数据对中key的数据类型 * VALUEOUT：输出kv数据对中value的数据类型 */ public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;{ /* * map方法是提供给map task进程来调用的，map task进程是每读取一行文本来调用一次我们自定义的map方法 * map task在调用map方法时，传递的参数： * 一行的起始偏移量LongWritable作为key * 一行的文本内容Text作为value */ @Override protected void map(LongWritable key, Text value,Context context) throws IOException, InterruptedException { //拿到一行文本内容，转换成String 类型 String line = value." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/2bff53e19dd63cab6d6a11e81991a440/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-04-03T16:25:34+08:00" />
<meta property="article:modified_time" content="2018-04-03T16:25:34+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">（2）Hadoop笔记：hadoop-eclipse-plugin插件的安装和mapReduce小栗子</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>注： <br> 1.eclipse所在环境为windows <br> 2.hadoop版本2.8.3 <br> 3.hadoop-eclipse-plugin版本2.8.3 <br> 4.eclipse版本Luna Service Release 1 (4.4.1) <br> 5.JDK 1.7</p> 
<ul><li>插件安装</li></ul> 
<p>hadoop-eclipse-plugin编译 <br> 因为我本地使用的JDK为1.7，而现在网上能找到的hadoop-eclipse-plugin-2.8.3都是基于JDK1.8编译的，所以都不能使用，因此需要下载hadoop-eclipse-plugin的源码然后在本地编译，过程中需要Hadoop-2.8.3的支持。当然如果你能找到符合自己环境的插件包，这步就可以跳过了。 <br> <a href="http://mirrors.shu.edu.cn/apache/hadoop/common/hadoop-2.8.3/hadoop-2.8.3.tar.gz" rel="nofollow">Hadoop-2.8.3.tar.gz</a> <br> <a href="https://github.com/winghc/hadoop2x-eclipse-plugin/">hadoop2x-eclipse-plugin</a> （github托管的源码） <br> <a href="http://mirror.bit.edu.cn/apache//ant/binaries/apache-ant-1.9.11-bin.zip" rel="nofollow">apache-ant-1.9.11-bin.zip</a> <br> 下载完后解压apache-ant-1.9.11-bin.zip，并配置环境变量。 <br> 新建ANT_HOME=E:apache-ant-1.9.4 <br> 在PATH后面加;%ANT_HOME%\bin <br> 测试下</p> 
<pre class="prettyprint"><code class=" hljs livecodeserver">ant -<span class="hljs-built_in">version</span></code></pre> 
<p><img src="https://images2.imgbox.com/ab/f7/SQ2zUZYO_o.png" alt="这里写图片描述" title=""></p> 
<p>解压Hadoop-2.8.3.tar.gz，hadoop2x-eclipse-plugin <br> 编辑E:\hadoop2x-eclipse-plugin-master\src\contrib\eclipse-plugin\build.xml <br> 添加下图红框中的部分，3项代表的意思应该都懂，就不多说了。 <br> <img src="https://images2.imgbox.com/3c/7c/55Cb7u1A_o.png" alt="这里写图片描述" title=""></p> 
<p>然后往下翻能发现有一堆这个东西，之后编译时大概会出错的地方。先打开放在这，不用修改。① <br> <img src="https://images2.imgbox.com/06/02/AS64Cd8E_o.png" alt="这里写图片描述" title=""></p> 
<p>然后编辑E:\hadoop2x-eclipse-plugin-master\ivy\libraries.properties.xml <br> 一般来讲只要修改下红框中的版本即可，然后这里有一大堆版本，而上面①图中那些都是在引用这里的版本。这里的版本又对应着Hadoop-2.8.3中的资源。在编译时可能会出现这里写的版本和Hadoop实际用的版本不一样导致找不到jar包的报错，这时就可以根据报错信息来修改下方的版本号，使得符合实际。基本都在${hadoop.home}/share/hadoop/common/lib文件夹中 <br> <img src="https://images2.imgbox.com/b9/4d/modmqzXj_o.png" alt="这里写图片描述" title=""></p> 
<p>进入hadoop2x-eclipse-plugin-master\src\contrib\eclipse-plugin目录，运行cmd命令行</p> 
<pre class="prettyprint"><code class=" hljs ">ant jar</code></pre> 
<p>编译完成，遇到错误就根据提示修改，最大可能遇到的就是我上面提到的问题，还有当编译时长时间卡在ivy-resolve-common: 处时，大概率已经编译失败。 <br> <img src="https://images2.imgbox.com/25/fb/khpxpPgI_o.png" alt="这里写图片描述" title=""></p> 
<p>编译完成后的文件在 <br> hadoop2x-eclipse-plugin-master\build\contrib\eclipse-plugin文件夹中 <br> <img src="https://images2.imgbox.com/90/46/RhVz503s_o.png" alt="这里写图片描述" title=""></p> 
<p>将jar包放入eclipse/plugins文件夹中，重启eclipse <br> 可以看到如下图标 <br> <img src="https://images2.imgbox.com/0a/5a/CDPvrs8F_o.png" alt="这里写图片描述" title=""> <br> 添加 <br> window-&gt;show view-&gt;other <br> <img src="https://images2.imgbox.com/d9/ea/CRcbdaG3_o.png" alt="这里写图片描述" title=""></p> 
<p>配置连接，如下图所示，随便取个名字就行 <br> <img src="https://images2.imgbox.com/0a/03/0NwhWGIp_o.png" alt="这里写图片描述" title=""></p> 
<p>然后如果连接成功的话就如下图所示，这里刚新搭建的Hadoop环境的话应该一开始就是空的，所以可能会抛空指针异常，没什么影响的，直接新建东西就行。 <br> <img src="https://images2.imgbox.com/8b/43/dLroe9qd_o.png" alt="这里写图片描述" title=""></p> 
<p><img src="https://images2.imgbox.com/44/70/n8FNyvhU_o.png" alt="这里写图片描述" title=""></p> 
<p>新建后需要刷新一下，同时可以访问<a href="http://xx.xx.xx.xx:50070/explorer.html#/" rel="nofollow">http://xx.xx.xx.xx:50070/explorer.html#/</a>网址查看是否真的新建了。 <br> <img src="https://images2.imgbox.com/d8/6f/i9nd24z6_o.png" alt="这里写图片描述" title=""></p> 
<p>通过这个我们可以轻松快捷的将文件上传给Hadoop文件系统，方便本地调试。</p> 
<p>window-&gt;preferences，选择Hadoop文件，这里主要目的是可以直接引用相关jar包 <br> <img src="https://images2.imgbox.com/64/8a/dZ6HG69P_o.png" alt="这里写图片描述" title=""></p> 
<ul><li>使用例子，单词计数 <br> 首先需要下载hadoop.dll，这个网上找找对应版本的吧，我用的是2.X的，放在C/windows/System32下。</li></ul> 
<p>File-&gt;New-&gt;other <br> next，填写名字，finish <br> <img src="https://images2.imgbox.com/12/c1/97kG9T3K_o.png" alt="这里写图片描述" title=""></p> 
<p><img src="https://images2.imgbox.com/fc/5a/pmQJnjRf_o.png" alt="这里写图片描述" title=""></p> 
<p>新建4个文件 <br> <img src="https://images2.imgbox.com/6e/f2/oEQ9ohGa_o.png" alt="这里写图片描述" title=""> <br> WordCountMapper.java</p> 
<pre class="prettyprint"><code class=" hljs scala"><span class="hljs-keyword">package</span> com.hadoop.demo;

<span class="hljs-keyword">import</span> java.io.IOException;

<span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;
<span class="hljs-keyword">import</span> org.apache.hadoop.io.LongWritable;
<span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;
<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Mapper;

<span class="hljs-comment">/*
 * KEYIN：输入kv数据对中key的数据类型
 * VALUEIN：输入kv数据对中value的数据类型
 * KEYOUT：输出kv数据对中key的数据类型
 * VALUEOUT：输出kv数据对中value的数据类型
 */</span>
public <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WordCountMapper</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Mapper</span>&lt;<span class="hljs-title">LongWritable</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">IntWritable</span>&gt;{<!-- --></span>

    <span class="hljs-comment">/*
     * map方法是提供给map task进程来调用的，map task进程是每读取一行文本来调用一次我们自定义的map方法
     * map task在调用map方法时，传递的参数：
     *      一行的起始偏移量LongWritable作为key
     *      一行的文本内容Text作为value
     */</span>
    <span class="hljs-annotation">@Override</span>
    <span class="hljs-keyword">protected</span> void map(LongWritable key, Text value,Context context) <span class="hljs-keyword">throws</span> IOException, InterruptedException {
        <span class="hljs-comment">//拿到一行文本内容，转换成String 类型</span>
        String line = value.toString();
        <span class="hljs-comment">//将这行文本切分成单词</span>
        String[] words=line.split(<span class="hljs-string">" "</span>);

        <span class="hljs-comment">//输出&lt;单词，1&gt;</span>
        <span class="hljs-keyword">for</span>(String word:words){
            context.write(<span class="hljs-keyword">new</span> Text(word), <span class="hljs-keyword">new</span> IntWritable(<span class="hljs-number">1</span>));
        }
    }
}
</code></pre> 
<p>WordCountReducer.java</p> 
<pre class="prettyprint"><code class=" hljs scala"><span class="hljs-keyword">package</span> com.hadoop.demo;

<span class="hljs-keyword">import</span> java.io.IOException;

<span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;
<span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;
<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Reducer;

<span class="hljs-comment">/*
 * KEYIN：对应mapper阶段输出的key类型
 * VALUEIN：对应mapper阶段输出的value类型
 * KEYOUT：reduce处理完之后输出的结果kv对中key的类型
 * VALUEOUT：reduce处理完之后输出的结果kv对中value的类型
 */</span>
public <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WordCountReducer</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Reducer</span>&lt;<span class="hljs-title">Text</span>, <span class="hljs-title">IntWritable</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">IntWritable</span>&gt;{<!-- --></span>
    <span class="hljs-annotation">@Override</span>
    <span class="hljs-comment">/*
     * reduce方法提供给reduce task进程来调用
     * 
     * reduce task会将shuffle阶段分发过来的大量kv数据对进行聚合，聚合的机制是相同key的kv对聚合为一组
     * 然后reduce task对每一组聚合kv调用一次我们自定义的reduce方法
     * 比如：&lt;hello,1&gt;&lt;hello,1&gt;&lt;hello,1&gt;&lt;tom,1&gt;&lt;tom,1&gt;&lt;tom,1&gt;
     *  hello组会调用一次reduce方法进行处理，tom组也会调用一次reduce方法进行处理
     *  调用时传递的参数：
     *          key：一组kv中的key
     *          values：一组kv中所有value的迭代器
     */</span>
    <span class="hljs-keyword">protected</span> void reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context) <span class="hljs-keyword">throws</span> IOException, InterruptedException {
        <span class="hljs-comment">//定义一个计数器</span>
        int count = <span class="hljs-number">0</span>;
        <span class="hljs-comment">//通过value这个迭代器，遍历这一组kv中所有的value，进行累加</span>
        <span class="hljs-keyword">for</span>(IntWritable value:values){
            count+=value.get();
        }

        <span class="hljs-comment">//输出这个单词的统计结果</span>
        context.write(key, <span class="hljs-keyword">new</span> IntWritable(count));
    }
}
</code></pre> 
<p>WordCountJobSubmitter.java</p> 
<pre class="prettyprint"><code class=" hljs avrasm">package <span class="hljs-keyword">com</span><span class="hljs-preprocessor">.hadoop</span><span class="hljs-preprocessor">.demo</span><span class="hljs-comment">;</span>

import java<span class="hljs-preprocessor">.io</span><span class="hljs-preprocessor">.IOException</span><span class="hljs-comment">;</span>
import java<span class="hljs-preprocessor">.util</span><span class="hljs-preprocessor">.Date</span><span class="hljs-comment">;</span>

import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.hadoop</span><span class="hljs-preprocessor">.conf</span><span class="hljs-preprocessor">.Configuration</span><span class="hljs-comment">;</span>
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.hadoop</span><span class="hljs-preprocessor">.fs</span><span class="hljs-preprocessor">.Path</span><span class="hljs-comment">;</span>
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.hadoop</span><span class="hljs-preprocessor">.io</span><span class="hljs-preprocessor">.IntWritable</span><span class="hljs-comment">;</span>
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.hadoop</span><span class="hljs-preprocessor">.io</span><span class="hljs-preprocessor">.Text</span><span class="hljs-comment">;</span>
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.hadoop</span><span class="hljs-preprocessor">.mapreduce</span><span class="hljs-preprocessor">.Job</span><span class="hljs-comment">;</span>
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.hadoop</span><span class="hljs-preprocessor">.mapreduce</span><span class="hljs-preprocessor">.lib</span><span class="hljs-preprocessor">.input</span><span class="hljs-preprocessor">.FileInputFormat</span><span class="hljs-comment">;</span>
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.hadoop</span><span class="hljs-preprocessor">.mapreduce</span><span class="hljs-preprocessor">.lib</span><span class="hljs-preprocessor">.output</span><span class="hljs-preprocessor">.FileOutputFormat</span><span class="hljs-comment">;</span>

public class WordCountJobSubmitter {

    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
        Configuration conf = new Configuration()<span class="hljs-comment">;</span>
        Job wordCountJob = Job<span class="hljs-preprocessor">.getInstance</span>(conf)<span class="hljs-comment">;</span>

        //重要：指定本job所在的jar包
        wordCountJob<span class="hljs-preprocessor">.setJarByClass</span>(WordCountJobSubmitter<span class="hljs-preprocessor">.class</span>)<span class="hljs-comment">;</span>

        //设置wordCountJob所用的mapper逻辑类为哪个类
        wordCountJob<span class="hljs-preprocessor">.setMapperClass</span>(WordCountMapper<span class="hljs-preprocessor">.class</span>)<span class="hljs-comment">;</span>
        //设置wordCountJob所用的reducer逻辑类为哪个类
        wordCountJob<span class="hljs-preprocessor">.setReducerClass</span>(WordCountReducer<span class="hljs-preprocessor">.class</span>)<span class="hljs-comment">;</span>

        //设置map阶段输出的kv数据类型
        wordCountJob<span class="hljs-preprocessor">.setMapOutputKeyClass</span>(Text<span class="hljs-preprocessor">.class</span>)<span class="hljs-comment">;</span>
        wordCountJob<span class="hljs-preprocessor">.setMapOutputValueClass</span>(IntWritable<span class="hljs-preprocessor">.class</span>)<span class="hljs-comment">;</span>

        //设置最终输出的kv数据类型
        wordCountJob<span class="hljs-preprocessor">.setOutputKeyClass</span>(Text<span class="hljs-preprocessor">.class</span>)<span class="hljs-comment">;</span>
        wordCountJob<span class="hljs-preprocessor">.setOutputValueClass</span>(IntWritable<span class="hljs-preprocessor">.class</span>)<span class="hljs-comment">;</span>

        Long fileName = (new Date())<span class="hljs-preprocessor">.getTime</span>()<span class="hljs-comment">;</span>

        //设置要处理的文本数据所存放的路径
        FileInputFormat<span class="hljs-preprocessor">.setInputPaths</span>(wordCountJob, <span class="hljs-string">"hdfs://xx.xx.xx.xx:9002/test/1.data"</span>)<span class="hljs-comment">;</span>
        FileOutputFormat<span class="hljs-preprocessor">.setOutputPath</span>(wordCountJob, new Path(<span class="hljs-string">"hdfs://xx.xx.xx.xx:9002/test/"</span>+fileName))<span class="hljs-comment">;</span>

        //提交job给hadoop集群
        wordCountJob<span class="hljs-preprocessor">.waitForCompletion</span>(true)<span class="hljs-comment">;</span>
    }
}
</code></pre> 
<p>log4j.properties</p> 
<pre class="prettyprint"><code class=" hljs avrasm">log4j<span class="hljs-preprocessor">.rootLogger</span>=INFO, stdout
log4j<span class="hljs-preprocessor">.appender</span><span class="hljs-preprocessor">.stdout</span>=org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.log</span>4j<span class="hljs-preprocessor">.ConsoleAppender</span>
log4j<span class="hljs-preprocessor">.appender</span><span class="hljs-preprocessor">.stdout</span><span class="hljs-preprocessor">.layout</span>=org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.log</span>4j<span class="hljs-preprocessor">.PatternLayout</span>
log4j<span class="hljs-preprocessor">.appender</span><span class="hljs-preprocessor">.stdout</span><span class="hljs-preprocessor">.layout</span><span class="hljs-preprocessor">.ConversionPattern</span>=%d %p [%c] - %m%n
log4j<span class="hljs-preprocessor">.appender</span><span class="hljs-preprocessor">.logfile</span>=org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.log</span>4j<span class="hljs-preprocessor">.FileAppender</span>
log4j<span class="hljs-preprocessor">.appender</span><span class="hljs-preprocessor">.logfile</span><span class="hljs-preprocessor">.File</span>=target/spring<span class="hljs-preprocessor">.log</span>
log4j<span class="hljs-preprocessor">.appender</span><span class="hljs-preprocessor">.logfile</span><span class="hljs-preprocessor">.layout</span>=org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.log</span>4j<span class="hljs-preprocessor">.PatternLayout</span>
log4j<span class="hljs-preprocessor">.appender</span><span class="hljs-preprocessor">.logfile</span><span class="hljs-preprocessor">.layout</span><span class="hljs-preprocessor">.ConversionPattern</span>=%d %p [%c] - %m%n</code></pre> 
<p>然后准备下1.data的数据（就是个txt编辑后修改后缀为data的文件），将该文件通过DFS上传</p> 
<pre class="prettyprint"><code class=" hljs matlab">hello tom
hello jim
how are you
<span class="hljs-built_in">i</span> love you
<span class="hljs-built_in">i</span> miss you
<span class="hljs-built_in">i</span> love you</code></pre> 
<p><img src="https://images2.imgbox.com/05/7a/0rozzg91_o.png" alt="这里写图片描述" title=""></p> 
<p>因为我这里放的路径是test下，实际使用时请在WordCountJobSubmitter .java中替换为自己的路径和文件名。</p> 
<p>运行，选中WordCountJobSubmitter.java（有main函数的），右键run as -&gt;run configurations。hdfs第一行为输入，第二行为输出 <br> <img src="https://images2.imgbox.com/4e/c7/fot7yzWm_o.png" alt="这里写图片描述" title=""></p> 
<p>运行，可以看见生成了新的文件，点击查看，可以看到单词计数的结果了。 <br> <img src="https://images2.imgbox.com/c2/32/qUlz6qxK_o.png" alt="这里写图片描述" title=""></p> 
<p>因为大部分都是根据记忆写的，所以过程中如果出现错误，望指正 _ (:з」∠*)_。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/effec1d4b3bda8e48c164b0349ae2a72/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">OpenStreetMap下载数据</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/8dbf94038e8ffa119b58b4d6b106e92f/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">[Android] Preloader 预加载框架</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>