<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>移动端深度学习部署：TFlite - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="移动端深度学习部署：TFlite" />
<meta property="og:description" content="1.TFlite介绍 （1）TFlite概念 tflite是谷歌自己的一个轻量级推理库。主要用于移动端。 tflite使用的思路主要是从预训练的模型转换为tflite模型文件，拿到移动端部署。 tflite的源模型可以来自tensorflow的saved model或者frozen model,也可以来自keras。 （2）TFlite优点 用Flatbuffer序列化模型文件，这种格式磁盘占用少，加载快
可以对模型进行量化，把float参数量化为uint8类型，模型文件更小、计算更快。
可以对模型进行剪枝、结构合并和蒸馏。
对NNAPI的支持。可调用安卓底层的接口，把异构的计算能力利用起来。
（3）TFlite量化 a.量化的好处 较小的存储大小：小模型在用户设备上占用的存储空间更少。例如，一个使用小模型的 Android 应用在用户的移动设备上会占用更少的存储空间。 较小的下载大小：小模型下载到用户设备所需的时间和带宽较少。 更少的内存用量：小模型在运行时使用的内存更少，从而释放内存供应用的其他部分使用，并可以转化为更好的性能和稳定性。 b.量化的过程
tflite的量化并不是全程使用uint8计算。而是存储每层的最大和最小值，然后把这个区间线性分成 256 个离散值，于是此范围内的每个浮点数可以用八位 (二进制) 整数来表示，近似为离得最近的那个离散值。比如，最小值是 -3 而最大值是 6 的情形，0 字节表示 -3，255 表示 6，而 128 是 1.5。每个操作都先用整形计算，输出时重新转换为浮点型。下图是量化Relu的示意图。
Tensorflow官方量化文档
c.量化的实现
训练后动态量化
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
#converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
tflite_model1 = converter.convert()
open(&#34;xxx.tflite&#34;, &#34;wb&#34;).write(tflite_model1)
训练后float16量化
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/fb0e38525a7b7a6133b56ca4c261ccb7/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-21T15:59:54+08:00" />
<meta property="article:modified_time" content="2023-12-21T15:59:54+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">移动端深度学习部署：TFlite</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2 style="margin-left:0px;text-align:left;"><strong>1.TFlite</strong><strong>介绍</strong></h2> 
<p></p> 
<h3 style="margin-left:0px;text-align:left;"><strong>（</strong><strong>1</strong><strong>）</strong><strong>TFlite</strong><strong>概念</strong></h3> 
<ul><li style="text-align:left;"><strong>tflite</strong><strong>是谷歌自己的一个轻量级推理库。主要用于移动端。</strong></li></ul> 
<ul><li style="text-align:left;"><strong>tflite</strong><strong>使用的思路主要是从预训练的模型转换为</strong><strong>tflite</strong><strong>模型文件，拿到移动端部署。</strong></li></ul> 
<ul><li style="text-align:left;"><strong>tflite</strong><strong>的源模型可以来自</strong><strong>tensorflow</strong><strong>的</strong><strong>saved model</strong><strong>或者</strong><strong>frozen model,</strong><strong>也可以来自</strong><strong>keras</strong><strong>。</strong></li></ul> 
<p style="text-align:left;"><img alt="" height="519" src="https://images2.imgbox.com/67/8c/sWVDzJX7_o.png" width="886"></p> 
<h3 style="margin-left:0px;text-align:left;"><strong>（</strong><strong>2</strong><strong>）</strong><strong>TFlite</strong><strong>优点</strong></h3> 
<p style="margin-left:0;text-align:left;">用Flatbuffer序列化模型文件，这种格式磁盘占用少，加载快</p> 
<p style="margin-left:0;text-align:left;">可以对模型进行量化，把float参数量化为uint8类型，模型文件更小、计算更快。</p> 
<p style="margin-left:0;text-align:left;">可以对模型进行剪枝、结构合并和蒸馏。</p> 
<p style="margin-left:0;text-align:left;">对NNAPI的支持。可调用安卓底层的接口，把异构的计算能力利用起来。</p> 
<h3 style="margin-left:0px;text-align:left;"><strong>（</strong><strong>3</strong><strong>）</strong><strong>TFlite</strong><strong>量化</strong></h3> 
<p style="margin-left:0;text-align:left;"><strong>a.量化的好处        </strong></p> 
<ul><li style="text-align:left;"><strong>较小的存储大小</strong>：小模型在用户设备上占用的存储空间更少。例如，一个使用小模型的 Android 应用在用户的移动设备上会占用更少的存储空间。</li></ul> 
<ul><li style="text-align:left;"><strong>较小的下载大小</strong>：小模型下载到用户设备所需的时间和带宽较少。</li></ul> 
<ul><li style="text-align:left;"><strong>更少的内存用量</strong>：小模型在运行时使用的内存更少，从而释放内存供应用的其他部分使用，并可以转化为更好的性能和稳定性。</li></ul> 
<p style="margin-left:0;text-align:left;"><strong>b.</strong><strong>量化的过程</strong></p> 
<p style="margin-left:0;text-align:left;">        tflite的量化并不是全程使用uint8计算。而是存储每层的最大和最小值，然后把这个区间线性分成 256 个离散值，于是此范围内的每个浮点数可以用八位 (二进制) 整数来表示，近似为离得最近的那个离散值。比如，最小值是 -3 而最大值是 6 的情形，0 字节表示 -3，255 表示 6，而 128 是 1.5。每个操作都先用整形计算，输出时重新转换为浮点型。下图是量化Relu的示意图。</p> 
<table cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:171.85pt;"> <p style="margin-left:0;text-align:center;"><img alt="" height="462" src="https://images2.imgbox.com/ab/e7/9Dh6SG8v_o.png" width="302"></p> <p style="margin-left:0;text-align:center;"></p> </td><td style="border-color:#000000;vertical-align:top;width:253.35pt;"> <p style="margin-left:0;text-align:center;"><img alt="" height="543" src="https://images2.imgbox.com/14/2d/0pSBtr8D_o.png" width="523"></p> </td></tr></tbody></table> 
<p><a href="https://www.tensorflow.org/lite/performance/post_training_quantization" rel="nofollow" title="Tensorflow官方量化文档">Tensorflow官方量化文档</a></p> 
<p><img alt="" height="333" src="https://images2.imgbox.com/3c/e9/6A32lnMm_o.png" width="886"></p> 
<p><img alt="" height="538" src="https://images2.imgbox.com/bc/b9/7tGKBw3F_o.png" width="886"></p> 
<p></p> 
<p style="margin-left:0;text-align:left;"><strong>c.</strong><strong>量化的实现</strong></p> 
<p style="text-align:left;">训练后动态量化</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="background-color:#f5f6f7;border-color:#dee0e3;width:15cm;"> <p style="margin-left:0;text-align:left;">import tensorflow as tf<br> converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)<br> #converter.optimizations = [tf.lite.Optimize.DEFAULT]<br> converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]<br> tflite_model1 = converter.convert()<br> open("xxx.tflite", "wb").write(tflite_model1)</p> </td></tr></tbody></table> 
<p style="text-align:left;">训练后float16量化</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="background-color:#f5f6f7;border-color:#dee0e3;width:15cm;"> <p style="margin-left:0;text-align:left;">import tensorflow as tf<br> converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)<br> converter.optimizations = [tf.lite.Optimize.DEFAULT]<br> converter.target_spec.supported_types = [tf.float16]<br> tflite_quant_model = converter.convert()<br> tflite_model1 = converter.convert()<br> open("xxx.tflite", "wb").write(tflite_model1)</p> </td></tr></tbody></table> 
<p style="text-align:left;">训练后int8量化</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="background-color:#f5f6f7;border-color:#dee0e3;width:15cm;"> <p style="margin-left:0;text-align:left;">import tensorflow as tf<br> converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)<br> converter.optimizations = [tf.lite.Optimize.DEFAULT]<br> def representative_dataset_gen():<br>   for _ in range(num_calibration_steps):<br>     # Get sample input data as a numpy array in a method of your choosing.<br>     yield [input]<br> converter.representative_dataset = representative_dataset_gen<br> converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]<br> converter.inference_input_type = tf.int8  # or tf.uint8<br> converter.inference_output_type = tf.int8  # or tf.uint8<br> tflite_model1 = converter.convert()<br> open("xxx.tflite", "wb").write(tflite_model1)</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:left;">注：float32和float16量化可以运行再GPU上，int8量化只能运行再CPU上</p> 
<h2 style="margin-left:0px;text-align:left;"><strong>2.TFlite</strong><strong>模型转换</strong></h2> 
<h3 style="margin-left:0px;text-align:left;"><strong>（</strong><strong>1</strong><strong>）在训练的时候就保存</strong><strong>tflite</strong><strong>模型</strong></h3> 
<table border="1" cellspacing="0"><tbody><tr><td style="background-color:#f5f6f7;border-color:#dee0e3;width:15cm;"> <p style="margin-left:0;text-align:left;">import tensorflow as tf<br> img = tf.placeholder(name="img", dtype=tf.float32, shape=(1, 64, 64, 3))<br> val = img + tf.constant([1., 2., 3.]) + tf.constant([1., 4., 4.])<br> out = tf.identity(val, name="out")<br> with tf.Session() as sess:<br>   tflite_model = tf.lite.toco_convert(sess.graph_def, [img], [out])<br>   open("converteds_model.tflite", "wb").write(tflite_model)</p> </td></tr></tbody></table> 
<h3 style="margin-left:0px;text-align:left;"><strong>（</strong><strong>2</strong><strong>）使用其他格式的</strong><strong>TensorFlow</strong><strong>模型转换成</strong><strong>tflite</strong><strong>模型</strong></h3> 
<p style="margin-left:0;text-align:left;">        首先要安装Bazel，参考：https://docs.bazel.build/versions/master/install-ubuntu.html ，只需要完成Installing using binary installer这一部分即可。然后克隆TensorFlow的源码：</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="background-color:#f5f6f7;border-color:#dee0e3;width:15cm;"> <p style="margin-left:0;text-align:left;">git clone https://github.com/tensorflow/tensorflow.git</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:left;">        接着编译转换工具，这个编译时间可能比较长：</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="background-color:#f5f6f7;border-color:#dee0e3;width:15cm;"> <p style="margin-left:0;text-align:left;">cd tensorflow/<br> bazel build tensorflow/python/tools:freeze_graph<br> bazel build tensorflow/lite/toco:toco</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:left;">        获得到转换工具之后，开始转换模型，以下操作是冻结图：</p> 
<ul><li style="text-align:left;">input_graph对应的是.pb文件；</li></ul> 
<ul><li style="text-align:left;">input_checkpoint对应mobilenet_v1_1.0_224.ckpt.data-00000-of-00001，使用时去掉后缀名。</li></ul> 
<ul><li style="text-align:left;">output_node_names这个可以在mobilenet_v1_1.0_224_info.txt中获取。</li></ul> 
<table border="1" cellspacing="0"><tbody><tr><td style="background-color:#f5f6f7;border-color:#dee0e3;width:15cm;"> <p style="margin-left:0;text-align:left;">./freeze_graph --input_graph=/mobilenet_v1_1.0_224/mobilenet_v1_1.0_224_frozen.pb \<br>   --input_checkpoint=/mobilenet_v1_1.0_224/mobilenet_v1_1.0_224.ckpt \<br>   --input_binary=true \<br>   --output_graph=/tmp/frozen_mobilenet_v1_224.pb \<br>   --output_node_names=MobilenetV1/Predictions/Reshape_1</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:left;">将冻结的图转换成tflite模型：</p> 
<ul><li style="text-align:left;"><span style="background-color:#eff0f1;">input_file</span>是已经冻结的图；</li></ul> 
<ul><li style="text-align:left;"><span style="background-color:#eff0f1;">output_file</span>是转换后输出的路径；</li></ul> 
<ul><li style="text-align:left;"><span style="background-color:#eff0f1;">output_arrays</span>这个可以在<span style="background-color:#eff0f1;">mobilenet_v1_1.0_224_info.txt</span>中获取；</li></ul> 
<ul><li style="text-align:left;"><span style="background-color:#eff0f1;">input_shapes</span>这个是预测数据的shape</li></ul> 
<table border="1" cellspacing="0"><tbody><tr><td style="background-color:#f5f6f7;border-color:#dee0e3;width:15cm;"> <p style="margin-left:0;text-align:left;">./toco --input_file=/tmp/mobilenet_v1_1.0_224_frozen.pb \<br>   --input_format=TENSORFLOW_GRAPHDEF \<br>   --output_format=TFLITE \<br>   --output_file=/tmp/mobilenet_v1_1.0_224.tflite \<br>   --inference_type=FLOAT \<br>   --input_type=FLOAT \<br>   --input_arrays=input \<br>   --output_arrays=MobilenetV1/Predictions/Reshape_1 \<br>   --input_shapes=1,224,224,3</p> </td></tr></tbody></table> 
<h3 style="margin-left:0px;text-align:left;"><strong>（</strong><strong>3</strong><strong>）使用检查点进行模型转换</strong></h3> 
<ul><li style="text-align:left;">将tensorflow模型保存成.pb文件</li></ul> 
<table border="1" cellspacing="0"><tbody><tr><td style="background-color:#f5f6f7;border-color:#dee0e3;width:15cm;"> <p style="margin-left:0;text-align:left;">import tensorflow as tf<br> from tensorflow.python.framework import graph_util<br> from tensorflow.python.platform import gfile<br>  <br> if __name__ == "__main__":<br>     a = tf.Variable(tf.constant(5.,shape=[1]),name="a")<br>     b = tf.Variable(tf.constant(6.,shape=[1]),name="b")<br>     c = a + b<br>     init = tf.initialize_all_variables()<br>     sess = tf.Session()<br>     sess.run(init)<br>     #导出当前计算图的GraphDef部分<br>     graph_def = tf.get_default_graph().as_graph_def()<br>     #保存指定的节点，并将节点值保存为常数<br>     output_graph_def = graph_util.convert_variables_to_constants(sess,graph_def,['add'])<br>     #将计算图写入到模型文件中<br>     model_f = tf.gfile.GFile("model.pb","wb")<br>     model_f.write(output_graph_def.SerializeToString())</p> </td></tr></tbody></table> 
<ul><li style="text-align:left;">模型文件的读取</li></ul> 
<table border="1" cellspacing="0"><tbody><tr><td style="background-color:#f5f6f7;border-color:#dee0e3;width:15cm;"> <p style="margin-left:0;text-align:left;">Bash<br>  sess = tf.Session()<br>     #将保存的模型文件解析为GraphDef<br>     model_f = gfile.FastGFile("model.pb",'rb')<br>     graph_def = tf.GraphDef()<br>     graph_def.ParseFromString(model_f.read())<br>     c = tf.import_graph_def(graph_def,return_elements=["add:0"])<br>     print(sess.run(c))<br>     #[array([ 11.], dtype=float32)]</p> </td></tr></tbody></table> 
<ul><li style="text-align:left;">pb文件转tflite</li></ul> 
<table border="1" cellspacing="0"><tbody><tr><td style="background-color:#f5f6f7;border-color:#dee0e3;width:15cm;"> <p style="margin-left:0;text-align:left;">Python<br> import tensorflow as tf<br>  <br> in_path=r'D:\tmp_mobilenet_v1_100_224_classification_3\output_graph.pb'<br> out_path=r'D:\tmp_mobilenet_v1_100_224_classification_3\output_graph.tflite'<br>  <br> input_tensor_name=['Placeholder']<br> input_tensor_shape={'Placeholder':[1,224,224,3]}<br>  <br> class_tensor_name=['final_result']<br>  <br> convertr=tf.lite.TFLiteConverter.from_frozen_graph(in_path,input_arrays=input_tensor_name<br>                                                    ,output_arrays=class_tensor_name<br>                                                    ,input_shapes=input_tensor_shape)<br>  <br> # convertr=tf.lite.TFLiteConverter.from_saved_model(saved_model_dir=in_path,input_arrays=[input_tensor_name],output_arrays=[class_tensor_name])<br> tflite_model=convertr.convert()<br>  <br> with open(out_path,'wb') as f:<br>     f.write(tflite_model)</p> </td></tr></tbody></table> 
<p></p> 
<h2 style="margin-left:0px;text-align:left;"><strong>3.</strong><strong>在</strong><strong>Android</strong><strong>端调用</strong><strong>TFlite</strong><strong>模型文件</strong></h2> 
<h3 style="margin-left:0px;text-align:left;"><strong>（</strong><strong>1</strong><strong>）</strong><strong>Android studio</strong><strong>中调用</strong><strong>TFlite</strong><strong>模型实现推理的流程</strong></h3> 
<ul><li style="text-align:left;">定义一个interpreter</li></ul> 
<ul><li style="text-align:left;">初始化interpreter(加载tflite模型)</li></ul> 
<ul><li style="text-align:left;">在Android中加载图片到buffer中</li></ul> 
<ul><li style="text-align:left;">用解释器执行图形（推理）</li></ul> 
<ul><li style="text-align:left;">将推理的结果在app中进行显示</li></ul> 
<p style="margin-left:0;text-align:left;"></p> 
<h3 style="margin-left:0px;text-align:left;"><strong>（</strong><strong>2</strong><strong>）在</strong><strong>Android Studio</strong><strong>中导入</strong><strong>TFLite</strong><strong>模型步骤</strong></h3> 
<ul><li style="text-align:left;">新建或打开现有Android项目工程。</li></ul> 
<ul><li style="text-align:left;">通过菜单项 <strong>File &gt; New &gt; Other &gt; TensorFlow Lite Model</strong> 打开TFLite模型导入对话框。</li></ul> 
<ul><li style="text-align:left;">选择后缀名为.tflite的模型文件。模型文件可以从网上下载或自行训练。</li></ul> 
<ul><li style="text-align:left;">导入的.tflite模型文件位于工程的 <strong>ml/</strong> 文件夹下面。</li></ul> 
<p style="margin-left:0;text-align:left;"><img alt="" height="680" src="https://images2.imgbox.com/33/6e/lg2e0KxC_o.png" width="934"></p> 
<p style="margin-left:0;text-align:left;">模型主要包括如下三种信息：</p> 
<ul><li style="text-align:left;">模型：包括模型名称、描述、版本、作者等等。</li></ul> 
<ul><li style="text-align:left;">张量：输入和输出张量。比如图片需要预先处理成合适的尺寸，才能进行推理。</li></ul> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c03efde37cc77decac7dab70e65a76fd/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">PyTorch模型部署流程(ONNX Runtime)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/2c54571761227edf7faa1624b7651b5e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">轻量级网络模型MobileNet发展脉络（V1-V2-V3）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>