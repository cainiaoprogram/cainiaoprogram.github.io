<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Focal and Global Knowledge Distillation for Detectors（CVPR 2022）原理与代码解析 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Focal and Global Knowledge Distillation for Detectors（CVPR 2022）原理与代码解析" />
<meta property="og:description" content="paper：Focal and Global Knowledge Distillation for Detectors
official implementation：https://github.com/yzd-v/FGD
存在的问题 如图1所示，前景区域教师和学生注意力之间的差异非常大，背景区域则相对较小。此外通道注意力的差异也非常明显。
作者还设计了实验解耦了蒸馏过程中的前景和背景，结果如表1所示，令人惊讶的是，前景背景一起进行蒸馏的效果是最差的，比单独蒸馏前景或背景还差。
上述结果表明，特征图中的不均匀差异会对蒸馏产生负面效果。这种不均匀差异不仅存在于前背景之间，也存在于不同像素位置和通道之间。
本文的创新点 针对前背景、空间位置、通道之间的差异，本文提出了focal distillation，在分离前背景的同时，还计算了教师特征不同空间位置和通道的注意力，使得学生专注于学习教师的关键像素和通道。
但是只关注关键信息还不够，在检测任务中全局语义信息也很重要。为了弥补focal蒸馏中缺失的全局信息，作者还提出了global distillation，其中利用GcBlock来提取不同像素之间的关系，然后传递给学生。
方法介绍 Focal Distillation 首先用一个binary mask \(M\) 来分离前背景
其中 \(r\) 是ground truth box，\(i,j\) 表示像素位置的坐标。
为了消除不同大小的gt box的尺度的影响和不同图片中前背景比例的差异，作者又设置了一个scale mask \(S\)
其中 \(H_{r},W_{r}\) 表示gt box \(r\) 的高和宽，如果一个像素属于不同的target，选择最小的box来计算 \(S\)。
接着作者借鉴SENet和CBAM的方法提取通道注意力和空间注意力
\(G^{S},G^{C}\) 分别表示空间和通道attention map，然后attention mask按下式计算
其中 \(T\) 是温度系数。
利用binary mask \(M\)、scale mask \(S\)、attention mask \(A^{S},A^{C}\)，特征损失 \(L_{fea}\) 如下
其中 \(A^{S},A^{C}\) 表示教师的空间和通道attention mask，\(F^{T},F^{S}\) 分别表示教师和学生的feature map，\(\alpha, \beta\) 是balance超参。
此外作者还提出了注意力损失 \(L_{at}\) 让学生模仿教师的attention mask" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/51d3860a9c7867afd4e6b1655c5c7aa8/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-08-08T00:38:54+08:00" />
<meta property="article:modified_time" content="2023-08-08T00:38:54+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Focal and Global Knowledge Distillation for Detectors（CVPR 2022）原理与代码解析</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><strong>paper：<a href="https://arxiv.org/abs/2111.11837v2" rel="nofollow" title="Focal and Global Knowledge Distillation for Detectors">Focal and Global Knowledge Distillation for Detectors</a></strong></p> 
<p id="u72413480"><strong>official implementation：<a href="https://github.com/yzd-v/FGD" title="https://github.com/yzd-v/FGD">https://github.com/yzd-v/FGD</a></strong></p> 
<h2>存在的问题 </h2> 
<p id="u5470f3b4">如图1所示，前景区域教师和学生注意力之间的差异非常大，背景区域则相对较小。此外通道注意力的差异也非常明显。</p> 
<p style="text-align:center;"><img alt="" height="429" src="https://images2.imgbox.com/c5/1a/KxcNvTGS_o.png" width="525"></p> 
<p>作者还设计了实验解耦了蒸馏过程中的前景和背景，结果如表1所示，令人惊讶的是，前景背景一起进行蒸馏的效果是最差的，比单独蒸馏前景或背景还差。</p> 
<p style="text-align:center;"><img alt="" height="311" src="https://images2.imgbox.com/98/df/q9tppz5B_o.png" width="512"> </p> 
<p>上述结果表明，特征图中的不均匀差异会对蒸馏产生负面效果。这种不均匀差异不仅存在于前背景之间，也存在于不同像素位置和通道之间。</p> 
<h2>本文的创新点</h2> 
<p>针对前背景、空间位置、通道之间的差异，本文提出了focal distillation，在分离前背景的同时，还计算了教师特征不同空间位置和通道的注意力，使得学生专注于学习教师的关键像素和通道。</p> 
<p id="ufa7e8906">但是只关注关键信息还不够，在检测任务中全局语义信息也很重要。为了弥补focal蒸馏中缺失的全局信息，作者还提出了global distillation，其中利用GcBlock来提取不同像素之间的关系，然后传递给学生。</p> 
<h2>方法介绍</h2> 
<h3>Focal Distillation</h3> 
<p>首先用一个binary mask \(M\) 来分离前背景</p> 
<p style="text-align:center;"><img alt="" height="69" src="https://images2.imgbox.com/d1/aa/L5JgC6Tn_o.png" width="493"> </p> 
<p>其中 \(r\) 是ground truth box，\(i,j\) 表示像素位置的坐标。</p> 
<p>为了消除不同大小的gt box的尺度的影响和不同图片中前背景比例的差异，作者又设置了一个scale mask \(S\)</p> 
<p style="text-align:center;"><img alt="" height="172" src="https://images2.imgbox.com/a3/90/xM8fNOT8_o.png" width="494"></p> 
<p>其中 \(H_{r},W_{r}\) 表示gt box \(r\) 的高和宽，如果一个像素属于不同的target，选择最小的box来计算 \(S\)。</p> 
<p id="u114bbed5">接着作者借鉴SENet和CBAM的方法提取通道注意力和空间注意力</p> 
<p style="text-align:center;"><img alt="" height="167" src="https://images2.imgbox.com/fe/84/jBucCEzw_o.png" width="494"></p> 
<p>\(G^{S},G^{C}\) 分别表示空间和通道attention map，然后attention mask按下式计算</p> 
<p style="text-align:center;"><img alt="" height="88" src="https://images2.imgbox.com/5c/81/46B044Lc_o.png" width="495"></p> 
<p>其中 \(T\) 是温度系数。</p> 
<p>利用binary mask \(M\)、scale mask \(S\)、attention mask \(A^{S},A^{C}\)，特征损失 \(L_{fea}\) 如下</p> 
<p style="text-align:center;"><img alt="" height="177" src="https://images2.imgbox.com/f6/9e/P1QSGdF5_o.png" width="515"></p> 
<p>其中 \(A^{S},A^{C}\)  表示教师的空间和通道attention mask，\(F^{T},F^{S}\) 分别表示教师和学生的feature map，\(\alpha, \beta\) 是balance超参。</p> 
<p>此外作者还提出了注意力损失 \(L_{at}\) 让学生模仿教师的attention mask</p> 
<p style="text-align:center;"><img alt="" height="35" src="https://images2.imgbox.com/ff/01/b5oLQVjW_o.png" width="522"></p> 
<p>\(l\) 表示L1损失。</p> 
<p id="u8b145f34">完整的focal损失就是特征损失和注意力损失的和</p> 
<p style="text-align:center;"><img alt="" height="33" src="https://images2.imgbox.com/b7/a9/FAFWNhFw_o.png" width="526"></p> 
<h3>Global Distillation</h3> 
<p id="u17043f62">如图4所示，作者用GcBlock来提取全局关系信息，关于GcBlock的详细介绍可以参考<a href="https://blog.csdn.net/ooooocj/article/details/129655090" title="GCNet: Global Context Network（ICCV 2019）原理与代码解析">GCNet: Global Context Network（ICCV 2019）原理与代码解析</a></p> 
<p style="text-align:center;"><img alt="" height="433" src="https://images2.imgbox.com/bb/4f/xdRNu3ia_o.png" width="498"></p> 
<p>全局损失 \(L_{global}\) 如下</p> 
<p style="text-align:center;"><img alt="" height="166" src="https://images2.imgbox.com/ce/f7/ckMMPyBu_o.png" width="518"> </p> 
<p>\(W_{k},W_{v1},W_{v2}\) 是卷积层，\(LN\) 表示layer normalization，\(N_{p}\) 是特征中所有像素个数，\(\lambda\) 是balance超参。</p> 
<h3>Overall loss</h3> 
<p id="uc9149c14">完整的损失函数如下，包括原本的训练损失和蒸馏损失，蒸馏损失又包括focal损失和global损失</p> 
<p style="text-align:center;"><img alt="" height="35" src="https://images2.imgbox.com/2f/57/jn1GTWug_o.png" width="534"></p> 
<h2>实验结果</h2> 
<p style="text-align:center;"><img alt="" height="685" src="https://images2.imgbox.com/57/2e/Q4SRLpNn_o.png" width="523"></p> 
<p style="text-align:center;"><img alt="" height="371" src="https://images2.imgbox.com/7b/6e/YIBB23OR_o.png" width="673">  </p> 
<p id="u94680dc3">其中inheriting strategry是《Instance-conditional knowledge distillation for object detection》这篇文章中提出的用教师的neck和head参数初始化学生网络，可以得到更好的效果。</p> 
<h2>代码解析</h2> 
<p>主要实现在mmdet/distillation/losses/fgd.py中，函数<strong>forward</strong>中，首先教师和学生的attention mask，即文中的式(5)~(8)</p> 
<pre><code class="language-python">S_attention_t, C_attention_t = self.get_attention(preds_T, self.temp)  # (N,H,W),(N,C)
S_attention_s, C_attention_s = self.get_attention(preds_S, self.temp)</code></pre> 
<pre><code class="language-python">def get_attention(self, preds, temp):
    """ preds: Bs*C*W*H """
    N, C, H, W = preds.shape

    value = torch.abs(preds)
    # Bs*W*H
    fea_map = value.mean(axis=1, keepdim=True)
    S_attention = (H * W * F.softmax((fea_map / temp).view(N, -1), dim=1)).view(N, H, W)

    # Bs*C
    channel_map = value.mean(axis=2, keepdim=False).mean(axis=2, keepdim=False)
    C_attention = C * F.softmax(channel_map / temp, dim=1)

    return S_attention, C_attention</code></pre> 
<p id="u4f1e1a97">接下来为了减小不同target尺度和前背景比例的影响，计算scale mask，即文中的式(2)~式(4)。其中内层的for循环是当一个像素属于不同的target时，选择最小的box来计算。</p> 
<pre><code class="language-python">Mask_fg = torch.zeros_like(S_attention_t)
Mask_bg = torch.ones_like(S_attention_t)
wmin, wmax, hmin, hmax = [], [], [], []
for i in range(N):
    new_boxxes = torch.ones_like(gt_bboxes[i])
    new_boxxes[:, 0] = gt_bboxes[i][:, 0] / img_metas[i]['img_shape'][1] * W
    new_boxxes[:, 2] = gt_bboxes[i][:, 2] / img_metas[i]['img_shape'][1] * W
    new_boxxes[:, 1] = gt_bboxes[i][:, 1] / img_metas[i]['img_shape'][0] * H
    new_boxxes[:, 3] = gt_bboxes[i][:, 3] / img_metas[i]['img_shape'][0] * H

    wmin.append(torch.floor(new_boxxes[:, 0]).int())
    wmax.append(torch.ceil(new_boxxes[:, 2]).int())
    hmin.append(torch.floor(new_boxxes[:, 1]).int())
    hmax.append(torch.ceil(new_boxxes[:, 3]).int())

    area = 1.0 / (hmax[i].view(1, -1) + 1 - hmin[i].view(1, -1)) / (wmax[i].view(1, -1) + 1 - wmin[i].view(1, -1))

    for j in range(len(gt_bboxes[i])):
        Mask_fg[i][hmin[i][j]:hmax[i][j] + 1, wmin[i][j]:wmax[i][j] + 1] = \
            torch.maximum(Mask_fg[i][hmin[i][j]:hmax[i][j] + 1, wmin[i][j]:wmax[i][j] + 1], area[0][j])

    Mask_bg[i] = torch.where(Mask_fg[i] &gt; 0, 0, 1)
    if torch.sum(Mask_bg[i]):
        Mask_bg[i] /= torch.sum(Mask_bg[i])</code></pre> 
<p id="uce4e6d68">接着就是完整的feature损失，即文中的式(9)</p> 
<pre><code class="language-python">fg_loss, bg_loss = self.get_fea_loss(preds_S, preds_T, Mask_fg, Mask_bg,
                                     C_attention_s, C_attention_t, S_attention_s, S_attention_t)</code></pre> 
<pre><code class="language-python">def get_fea_loss(self, preds_S, preds_T, Mask_fg, Mask_bg, C_s, C_t, S_s, S_t):
    loss_mse = nn.MSELoss(reduction='sum')

    Mask_fg = Mask_fg.unsqueeze(dim=1)
    Mask_bg = Mask_bg.unsqueeze(dim=1)

    C_t = C_t.unsqueeze(dim=-1)
    C_t = C_t.unsqueeze(dim=-1)

    S_t = S_t.unsqueeze(dim=1)

    fea_t = torch.mul(preds_T, torch.sqrt(S_t))
    fea_t = torch.mul(fea_t, torch.sqrt(C_t))
    fg_fea_t = torch.mul(fea_t, torch.sqrt(Mask_fg))
    bg_fea_t = torch.mul(fea_t, torch.sqrt(Mask_bg))

    fea_s = torch.mul(preds_S, torch.sqrt(S_t))
    fea_s = torch.mul(fea_s, torch.sqrt(C_t))
    fg_fea_s = torch.mul(fea_s, torch.sqrt(Mask_fg))
    bg_fea_s = torch.mul(fea_s, torch.sqrt(Mask_bg))

    fg_loss = loss_mse(fg_fea_s, fg_fea_t) / len(Mask_fg)
    bg_loss = loss_mse(bg_fea_s, bg_fea_t) / len(Mask_bg)

    return fg_loss, bg_loss</code></pre> 
<p id="ua100c11a">文中作者还提出了用L1 loss的attention损失，即式(10)</p> 
<pre><code class="language-python">mask_loss = self.get_mask_loss(C_attention_s, C_attention_t, S_attention_s, S_attention_t)</code></pre> 
<pre><code class="language-python">def get_mask_loss(self, C_s, C_t, S_s, S_t):
    mask_loss = torch.sum(torch.abs((C_s - C_t))) / len(C_s) + torch.sum(torch.abs((S_s - S_t))) / len(S_s)

    return mask_loss</code></pre> 
<p id="uf9e3bd4f">feature loss和attention loss一起组成的focal loss，为了弥补全局语义信息的缺失，作者又引入了全局蒸馏损失，其中用到了GcBlock，即式(12)</p> 
<pre><code class="language-python">rela_loss = self.get_rela_loss(preds_S, preds_T)</code></pre> 
<pre><code class="language-python">def get_rela_loss(self, preds_S, preds_T):
    loss_mse = nn.MSELoss(reduction='sum')

    context_s = self.spatial_pool(preds_S, 0)
    context_t = self.spatial_pool(preds_T, 1)

    out_s = preds_S
    out_t = preds_T

    channel_add_s = self.channel_add_conv_s(context_s)
    out_s = out_s + channel_add_s

    channel_add_t = self.channel_add_conv_t(context_t)
    out_t = out_t + channel_add_t

    rela_loss = loss_mse(out_s, out_t) / len(out_s)

    return rela_loss</code></pre> 
<pre><code class="language-python">def spatial_pool(self, x, in_type):
    batch, channel, width, height = x.size()
    input_x = x
    # [N, C, H * W]
    input_x = input_x.view(batch, channel, height * width)
    # [N, 1, C, H * W]
    input_x = input_x.unsqueeze(1)
    # [N, 1, H, W]
    if in_type == 0:
        context_mask = self.conv_mask_s(x)
    else:
        context_mask = self.conv_mask_t(x)
    # [N, 1, H * W]
    context_mask = context_mask.view(batch, 1, height * width)
    # [N, 1, H * W]
    context_mask = F.softmax(context_mask, dim=2)
    # [N, 1, H * W, 1]
    context_mask = context_mask.unsqueeze(-1)
    # [N, 1, C, 1]
    context = torch.matmul(input_x, context_mask)
    # [N, C, 1, 1]
    context = context.view(batch, channel, 1, 1)

    return context


self.channel_add_conv_s = nn.Sequential(
    nn.Conv2d(teacher_channels, teacher_channels//2, kernel_size=1),
    nn.LayerNorm([teacher_channels//2, 1, 1]),
    nn.ReLU(inplace=True),  # yapf: disable
    nn.Conv2d(teacher_channels//2, teacher_channels, kernel_size=1))
self.channel_add_conv_t = nn.Sequential(
    nn.Conv2d(teacher_channels, teacher_channels//2, kernel_size=1),
    nn.LayerNorm([teacher_channels//2, 1, 1]),
    nn.ReLU(inplace=True),  # yapf: disable
    nn.Conv2d(teacher_channels//2, teacher_channels, kernel_size=1))</code></pre> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/1dba616247ec115a1d709a04e9f741c0/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【嵌入式环境下linux内核及驱动学习笔记-（18）LCD驱动框架1-LCD控制原理】</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/2169ba93891418e9631cece9520da34e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Simulink----Switch模块</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>