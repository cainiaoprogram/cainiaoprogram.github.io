<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【话题】ChatGPT等大语言模型为什么没有智能2 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【话题】ChatGPT等大语言模型为什么没有智能2" />
<meta property="og:description" content="我们接着上一次的讨论，继续探索大模型的存在的问题。正巧CSDN最近在搞文章活动，我们来看看大模型“幻觉”。当然，本文可能有很多我自己的“幻觉”，欢迎批评指正。如果这么说的话，其实很容易得出一个小结论——大模型如果想朝着真的智能发展，“幻觉”问题不可能被完全解决，只可能缓解。
向量碎片化：大模型也会连接错误 大模型“幻觉”的问题其实就是if-clauses走入了一个死胡同，概率连接到错误地方或者说预测下一个token进入了一个奇怪的局部最优，并且很难走出来。
最简单的一个例子就是 1&#43;1 = 2这个公式，如果在训练中，一直喂 1&#43;1 = 3，你觉得当回答1&#43;1 = ？这个问题的时候，神经网络会回答几？
这里我们看人类如何处理。在回答 1&#43;1 = ？的时候，2 作为最快cache会最先被获取到；但是，如果在特殊context下，它的答案多种多样。比如算式里的1是二进制，那么1&#43;1 = 10；比如遇到社会学科的时候，1&#43;1 = 3 可以给生一个娃的家庭建模，人为定义=3；比如高中或者初中化学老师经常用一些公式进行比喻1个化学物质和另1个化学物质进行反应，结果多种多样。
所以这也是我们在上一话提过词向量碎片化的问题。当训练数据开始因为context产生歧义的时候，词向量开始碎片化，向量长度的固定意味着原本我可以用更多维度表示一种概念，之后会不停压缩，让这些维度分别去对应不同context下不同的意义，于是诸如transformer里feedforward层那2个线性矩阵真的能模拟诸多离散的情况么？当然是可能的，就是你参数的规模不停变大，而且随着context增多，这个规模是爆炸的；即使不管这个爆炸，碎片化越严重，超过了向量维度承载能力，整个模型也会崩塌。
宕开一笔，现在deep learning铺天盖地，把以前很多其他学派的文献冲淡的一塌糊涂（如果大家感兴趣，可以参考《终极算法》一书，bayes，logic，nn，svm/knn，ea五大学派）；真的得抱怨下，钱途无限的世界很现实也很残酷，哪里钱多点哪里so easy。
个人感觉上，在embedding层上需要作一些调整，一个词向量可能可以是多个，然后引入evolutionary algorithm，在诸如用蒙特卡洛法选择一个向量的时候，最常用概念的向量大概率被最先取到，我们用个top k就能应付一些其他context了。当然，这个和MoE有那么一点类似，但是最终架构应该是往另一个方向了，因为MoE用到最后你会发现我要处理gating的时候需要recursive，就是gating的gating然后堆上去。
说着说着，我们就又得聊到目前整个神经网络的问题，它是一个外轮廓模型，就相当于你有一个吹得很大的气球，你要训练这个模型就是你把一个事物装进这个气球，然后气球开始放气，最后气球完全贴合了整个事物得外轮廓，loss就是0了；如果这个气球太小，你没有办法把一个事物装进去，就是你的参数太少。现在deep learning基本已经探索出了人类视觉和文字的外轮廓，视觉嘛大概19层cnn&#43;poolmax&#43;residual基本“击穿”了；文字上transformer在当前的规模也基本“击穿”了；这里的“击穿”指的是气球够大了，效果出来了。其实还有一个内轮廓模型，这个太难了，目前RL、EA这些都是在探索这个模型，我最喜欢的例子就是code parser；你看本来一个模型只能把整篇code所有字符标记成unknown，当你告诉它 &#34; 可以作为字符串的开始和结束，它开始能把code中的字符串标记出来，再接着你告诉它 &#34; 表示转义，你能得到更复杂的状态机，它需要逐步学习，慢慢补全事物的内轮廓。相当于你把事物挖空，把瘪的气球放进去充气，最后得到事物的内轮廓。人类的学习感觉上是走在外轮廓和内轮廓的交界处的，这个是真正的智能学习，当然，这个可能只是我个人的“幻觉”。所以如果神经网络只知道外轮廓，在它推理的边界，很容易产生“幻觉”；如果神经网络太小，模型看不清外轮廓，也会有“幻觉”。
另一方面，如果我们人为定义“幻觉”，其实大模型也不能很好去处理。比如改变数学规则定义新“幻觉”，神经网络只能根据之前的学习作概率预测，很难处理其中逻辑。曾经作过一个试验，定义0&#43;1=1&#43;0=1, 1&#43;1=3, 1&#43;3=3&#43;1=2, 1&#43;2=2&#43;1=5, 1&#43;5=5&#43;1=4, 1&#43;4=4&#43;1=6, 1&#43;6=6&#43;1=8, 1&#43;8=8&#43;1=7, 1&#43;7=7&#43;1=9, 1&#43;9=9&#43;1=10，问5367&#43;3456=?：目前还没有一个大模型能给出正确答案。
这个问题应该还是出在词向量上。妄图用global的一个向量表示所有local概念，很傻。我们刚才讨论过用EA作多向量，其实embedding这里应该就是少了一个组件，这个组件需要将global vector transform成local vector，貌似transformer里K Q V有这样模糊的操作，但是linear去猜测离散，效率可想而知。关于这个话题，我想等到我思索得更清楚点的时候再展开，现在就简单提两句，作抛砖引玉。
“争议”是另一种“幻觉” 想想，如果你是1&#43;1=2的信徒，你进入的世界人家定义就是1&#43;1=3，那么你所相信的1&#43;1=2算是“幻觉”么？“幻觉”的另一个问题是“争议”。比如“中医”，各大论坛上吵得不可开交，到底它有没有用？像这样的话题，如果给到训练数据，大部分都是有偏见的；即使我们均衡了训练数据，50%的人说有用，50%的人说无用，那这个概率不偏不倚，机器只能因为误差偏向了另一个，对，就是那个50.0000000002%和49.9999999998%。这个问题其实是我们还没有系统去处理，我们需要有一个数据库去构建，赞成和反对的声音有多少，如果存在争议，我们最好的办法是给出reference让读者自己判断，而不是硬下结论。
最近stackoverflow的流量下降的厉害，因为ChatGPT确实解决了太多问题；与其我等别人回复，不如ChatGPT直接给出解；当然，这个是恶性的；当GPT扼杀community的时候，好的训练数据会越来越少。但是我们从“幻觉”来看，感觉社区的另一个方向就是有争议的问题列出来投票，盖棺定论的都放在wiki上就好了搜索引擎会解决一切。
我们从“争议”看到了另一种“幻觉”，也从“争议”聊到了那个什么50.0000000002%。不要忘记神经网络参数更新的原理 W = W &#43; a.dW；这个是一个迭代过程，迭代会积累误差，所以如果我们遇上了混沌(Chaos)，那么你可能会发现新大陆——一只蝴蝶引发了飓风。混沌会导致训练的时候就是不收敛，所以学习到的“幻觉”一直消除不了。
所以这里再放送一个可以免费体验各种大模型（gpt4, gpt3.5, gemini-pro, yi-34b, mistral 8x7b, qwen, …）的学术机构网址，可能需要科学上网：https://chat.lmsys.org。从我个人体验来说，目前GPT领先的不仅仅是在模型上，而是在训练数据上。这也是为什么GPT能缓解一定的“幻觉”，在解答一些问题上比其他模型准确。从这方面来说，也看出现在的大预言模型“没有智能”，当然这个有点“幻觉”硬下结论了，至少在我看来，有智能意味着它可以生成代码，按照代码的形式执行各种复杂分析，更偏向于内轮廓模型；不过欣喜的是，GPT已经有一些雏形了。所以我后面的兴趣就是研究内轮廓模型，干掉GPU，一台CPU机器作为一个智能agent，白日梦中……" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/e0be8b1ac3acc8cdea2b8e55fe2aabdd/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-02T09:14:50+08:00" />
<meta property="article:modified_time" content="2024-01-02T09:14:50+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【话题】ChatGPT等大语言模型为什么没有智能2</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>我们接着上一次的讨论，继续探索大模型的存在的问题。正巧CSDN最近在搞文章活动，我们来看看大模型“幻觉”。当然，本文可能有很多我自己的“幻觉”，欢迎批评指正。如果这么说的话，其实很容易得出一个小结论——大模型如果想朝着真的智能发展，“幻觉”问题不可能被完全解决，只可能缓解。</p> 
<h2><a id="_2"></a>向量碎片化：大模型也会连接错误</h2> 
<p>大模型“幻觉”的问题其实就是if-clauses走入了一个死胡同，概率连接到错误地方或者说预测下一个token进入了一个奇怪的局部最优，并且很难走出来。</p> 
<p>最简单的一个例子就是 1+1 = 2这个公式，如果在训练中，一直喂 1+1 = 3，你觉得当回答1+1 = ？这个问题的时候，神经网络会回答几？</p> 
<p>这里我们看人类如何处理。在回答 1+1 = ？的时候，2 作为最快cache会最先被获取到；但是，如果在特殊context下，它的答案多种多样。比如算式里的1是二进制，那么1+1 = 10；比如遇到社会学科的时候，1+1 = 3 可以给生一个娃的家庭建模，人为定义=3；比如高中或者初中化学老师经常用一些公式进行比喻1个化学物质和另1个化学物质进行反应，结果多种多样。</p> 
<p>所以这也是我们在上一话提过词向量碎片化的问题。当训练数据开始因为context产生歧义的时候，词向量开始碎片化，向量长度的固定意味着原本我可以用更多维度表示一种概念，之后会不停压缩，让这些维度分别去对应不同context下不同的意义，于是诸如transformer里feedforward层那2个线性矩阵真的能模拟诸多离散的情况么？当然是可能的，就是你参数的规模不停变大，而且随着context增多，这个规模是爆炸的；即使不管这个爆炸，碎片化越严重，超过了向量维度承载能力，整个模型也会崩塌。</p> 
<p>宕开一笔，现在deep learning铺天盖地，把以前很多其他学派的文献冲淡的一塌糊涂（如果大家感兴趣，可以参考《终极算法》一书，bayes，logic，nn，svm/knn，ea五大学派）；真的得抱怨下，钱途无限的世界很现实也很残酷，哪里钱多点哪里so easy。</p> 
<p>个人感觉上，在embedding层上需要作一些调整，一个词向量可能可以是多个，然后引入evolutionary algorithm，在诸如用蒙特卡洛法选择一个向量的时候，最常用概念的向量大概率被最先取到，我们用个top k就能应付一些其他context了。当然，这个和MoE有那么一点类似，但是最终架构应该是往另一个方向了，因为MoE用到最后你会发现我要处理gating的时候需要recursive，就是gating的gating然后堆上去。</p> 
<p>说着说着，我们就又得聊到目前整个神经网络的问题，它是一个外轮廓模型，就相当于你有一个吹得很大的气球，你要训练这个模型就是你把一个事物装进这个气球，然后气球开始放气，最后气球完全贴合了整个事物得外轮廓，loss就是0了；如果这个气球太小，你没有办法把一个事物装进去，就是你的参数太少。现在deep learning基本已经探索出了人类视觉和文字的外轮廓，视觉嘛大概19层cnn+poolmax+residual基本“击穿”了；文字上transformer在当前的规模也基本“击穿”了；这里的“击穿”指的是气球够大了，效果出来了。其实还有一个内轮廓模型，这个太难了，目前RL、EA这些都是在探索这个模型，我最喜欢的例子就是code parser；你看本来一个模型只能把整篇code所有字符标记成unknown，当你告诉它 " 可以作为字符串的开始和结束，它开始能把code中的字符串标记出来，再接着你告诉它 " 表示转义，你能得到更复杂的状态机，它需要逐步学习，慢慢补全事物的内轮廓。相当于你把事物挖空，把瘪的气球放进去充气，最后得到事物的内轮廓。人类的学习感觉上是走在外轮廓和内轮廓的交界处的，这个是真正的智能学习，当然，这个可能只是我个人的“幻觉”。所以如果神经网络只知道外轮廓，在它推理的边界，很容易产生“幻觉”；如果神经网络太小，模型看不清外轮廓，也会有“幻觉”。<br> <img src="https://images2.imgbox.com/b8/60/HvqoryeU_o.png" alt="在这里插入图片描述" width="200"><br> 另一方面，如果我们人为定义“幻觉”，其实大模型也不能很好去处理。比如改变数学规则定义新“幻觉”，神经网络只能根据之前的学习作概率预测，很难处理其中逻辑。曾经作过一个试验，定义<code>0+1=1+0=1, 1+1=3, 1+3=3+1=2, 1+2=2+1=5, 1+5=5+1=4, 1+4=4+1=6, 1+6=6+1=8, 1+8=8+1=7, 1+7=7+1=9, 1+9=9+1=10</code>，问<code>5367+3456=?</code>：目前还没有一个大模型能给出正确答案。</p> 
<p>这个问题应该还是出在词向量上。妄图用global的一个向量表示所有local概念，很傻。我们刚才讨论过用EA作多向量，其实embedding这里应该就是少了一个组件，这个组件需要将global vector transform成local vector，貌似transformer里K Q V有这样模糊的操作，但是linear去猜测离散，效率可想而知。关于这个话题，我想等到我思索得更清楚点的时候再展开，现在就简单提两句，作抛砖引玉。</p> 
<h2><a id="_21"></a>“争议”是另一种“幻觉”</h2> 
<p>想想，如果你是1+1=2的信徒，你进入的世界人家定义就是1+1=3，那么你所相信的1+1=2算是“幻觉”么？“幻觉”的另一个问题是“争议”。比如“中医”，各大论坛上吵得不可开交，到底它有没有用？像这样的话题，如果给到训练数据，大部分都是有偏见的；即使我们均衡了训练数据，50%的人说有用，50%的人说无用，那这个概率不偏不倚，机器只能因为误差偏向了另一个，对，就是那个50.0000000002%和49.9999999998%。这个问题其实是我们还没有系统去处理，我们需要有一个数据库去构建，赞成和反对的声音有多少，如果存在争议，我们最好的办法是给出reference让读者自己判断，而不是硬下结论。</p> 
<p>最近stackoverflow的流量下降的厉害，因为ChatGPT确实解决了太多问题；与其我等别人回复，不如ChatGPT直接给出解；当然，这个是恶性的；当GPT扼杀community的时候，好的训练数据会越来越少。但是我们从“幻觉”来看，感觉社区的另一个方向就是有争议的问题列出来投票，盖棺定论的都放在wiki上就好了搜索引擎会解决一切。</p> 
<p>我们从“争议”看到了另一种“幻觉”，也从“争议”聊到了那个什么50.0000000002%。不要忘记神经网络参数更新的原理 W = W + a.dW；这个是一个迭代过程，迭代会积累误差，所以如果我们遇上了混沌(Chaos)，那么你可能会发现新大陆——一只蝴蝶引发了飓风。混沌会导致训练的时候就是不收敛，所以学习到的“幻觉”一直消除不了。</p> 
<p>所以这里再放送一个可以免费体验各种大模型（gpt4, gpt3.5, gemini-pro, yi-34b, mistral 8x7b, qwen, …）的学术机构网址，可能需要科学上网：<a href="https://chat.lmsys.org" rel="nofollow">https://chat.lmsys.org</a>。从我个人体验来说，目前GPT领先的不仅仅是在模型上，而是在训练数据上。这也是为什么GPT能缓解一定的“幻觉”，在解答一些问题上比其他模型准确。从这方面来说，也看出现在的大预言模型“没有智能”，当然这个有点“幻觉”硬下结论了，至少在我看来，有智能意味着它可以生成代码，按照代码的形式执行各种复杂分析，更偏向于内轮廓模型；不过欣喜的是，GPT已经有一些雏形了。所以我后面的兴趣就是研究内轮廓模型，干掉GPU，一台CPU机器作为一个智能agent，白日梦中……<br> 今天我们就先到这里吧。后面我们再继续讨论更多问题。<br> 如果想更多交流，可以扫描下面的二维码注明话题和我一起研究讨论。<br> <img src="https://images2.imgbox.com/e8/3f/DxYVauHp_o.jpg" alt="" width="100"></p> 
<p>J.Y.Liu<br> 2024.01.01</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/88f8f38839ca36bf2df20f5e9e05a874/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">架构设计模式详解：夯实架构设计的基础</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/2cd71f6112579330279092d186680449/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">html益智游戏拼图游戏源代码</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>