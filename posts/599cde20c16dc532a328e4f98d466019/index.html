<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>CUDA和FFMPEG硬件解码视频流 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="CUDA和FFMPEG硬件解码视频流" />
<meta property="og:description" content="本文主要讲述了通过FFMPEG获取H264格式的RTSP流数据（也可以获取本地视频文件），并通过CUDA进行硬件解码的过程。其他博客给出的教程要么只是给出了伪代码，非常的模糊，要么是基于D3D进行显示，使得给出的源码非常复杂，而无法看出CUDA解码的核心框架，而本文将其他非核心部分剥离出去，视频播放部分通过opencv调用cv::mat显示。
当然本博客的工作也参考了其他博客的内容，CSDN上原创的东西比较难找，大部分都是转载的，所以大家还是积极的贡献力量吧。
本文将分为以下两个部分：
1.CUDA硬件解码核心原理和框架解释；
2.解码核心功能代码的实现
CUDA硬件解码核心原理和框架
做过FFMPEG解码开发的同学肯定都对以下函数比较熟悉avcodec_decode_video2()，该函数实现可以解码从视频流中获取的数据包AVPACKET转化为AV_FRAME,AV_FRAME中包含了解码后的数据。通过CUDA硬件进行解码，最核心的思想就通过回调函数形式来调用CUDA硬件解码接口，对该函数替换，将CPU解码功能转移到GPU中去。
博客给出了一个很好的基础性框架，本文也是借鉴了该博客，该博客中修改了原始的VideoSource，将视频流的获取改为了ffmpeg，而CUDA解码部分框架如下图
1.1 VideoSource
VideoSourceData中包含了CUvideoparser和FrameQueue，通过上图可以看出，CUvideoparser是在VideoDecoder基础上实现了接口的封装，而VideoSource则是通过CUvideoparser进行解码。FrameQueue是存储硬件解码后图像的队列，注意硬件解码完的图像是存放在GPU显存里面了，而VideoDecoder中函数mapFrame，可完成从显存到内存的映射。
1.2 VideoParser
VideoParser中最重要的是三个回调函数，static int CUDAAPI HandleVideoSequence(void *pUserData, CUVIDEOFORMAT *pFormat)， HandlePictureDecode(void *pUserData, CUVIDPICPARAMS *pPicParams)，HandlePictureDisplay(void *pUserData, CUVIDPARSERDISPINFO *pPicParams)，实现对视频格式变换、视频解码、解码后显示等处理功能。HandleVideoSequence主要负责视频格式进行校验，没有实现其他功能，解码函数HandlePictureDecode调用的就是VideoDecoder的解码函数（CUDA的接口），显示函数HandlePictureDisplay完成了解码后GPU图像进入FrameQueue。
1.3 VideoDecoder
该类是最核心的硬件解码功能类，CUVIDDECODECREATEINFO oVideoDecodeCreateInfo_是创建解码信息结构体，CUvideodecoder oDecoder_是最内核的CUDA硬件解码器，VideoParser的解码功能实际上是在CUvideodecoder解码内核上封装实现的（层层封装导致源码有点复杂，所以想看懂实现机制需要有点耐心）。
2 核心解码模块的实现
示例中NvDecodeD3D9.cpp实现了D3D环境的创建，CUDA模块的初始化，其中取视频帧图像显示的函数如下，该函数实现了从解码图像队列取出图像（实际上是显存指针），完成格式转换（NV12到ARGB），最后映射到D3D的Texture进行显示等功能，代码中我给出了关键部位的解释。
bool copyDecodedFrameToTexture(unsigned int &amp;nRepeats, int bUseInterop, int *pbIsProgressive) { CUVIDPARSERDISPINFO oDisplayInfo; if (g_pFrameQueue-&gt;dequeue(&amp;oDisplayInfo)) { CCtxAutoLock lck(g_CtxLock); // Push the current CUDA context (only if we are using CUDA decoding path) CUresult result = cuCtxPushCurrent(g_oContext); //创建解码图像的显存指针，注意存储的是NV12格式的 CUdeviceptr pDecodedFrame[3] = { 0, 0, 0 }; //用于解码图像后进行格式转换 CUdeviceptr pInteropFrame[3] = { 0, 0, 0 }; *pbIsProgressive = oDisplayInfo." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/599cde20c16dc532a328e4f98d466019/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-11-30T16:07:32+08:00" />
<meta property="article:modified_time" content="2023-11-30T16:07:32+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">CUDA和FFMPEG硬件解码视频流</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>本文主要讲述了通过FFMPEG获取H264格式的RTSP流数据（也可以获取本地视频文件），并通过CUDA进行硬件解码的过程。其他博客给出的教程要么只是给出了伪代码，非常的模糊，要么是基于D3D进行显示，使得给出的源码非常复杂，而无法看出CUDA解码的核心框架，而本文将其他非核心部分剥离出去，视频播放部分通过opencv调用cv::mat显示。</p> 
<p>当然本博客的工作也参考了其他博客的内容，CSDN上原创的东西比较难找，大部分都是转载的，所以大家还是积极的贡献力量吧。</p> 
<p>本文将分为以下两个部分：<br> 1.CUDA硬件解码核心原理和框架解释；<br> 2.解码核心功能代码的实现</p> 
<p>CUDA硬件解码核心原理和框架</p> 
<p>做过FFMPEG解码开发的同学肯定都对以下函数比较熟悉avcodec_decode_video2()，该函数实现可以解码从视频流中获取的数据包AVPACKET转化为AV_FRAME,AV_FRAME中包含了解码后的数据。通过CUDA硬件进行解码，最核心的思想就通过回调函数形式来调用CUDA硬件解码接口，对该函数替换，将CPU解码功能转移到GPU中去。</p> 
<p>博客给出了一个很好的基础性框架，本文也是借鉴了该博客，该博客中修改了原始的VideoSource，将视频流的获取改为了ffmpeg，而CUDA解码部分框架如下图</p> 
<p></p> 
<p class="img-center"><img alt="" height="314" src="https://images2.imgbox.com/2f/a1/grbCHgM8_o.png" width="720"></p> 
<p>1.1 VideoSource</p> 
<p>VideoSourceData中包含了CUvideoparser和FrameQueue，通过上图可以看出，CUvideoparser是在VideoDecoder基础上实现了接口的封装，而VideoSource则是通过CUvideoparser进行解码。FrameQueue是存储硬件解码后图像的队列，注意硬件解码完的图像是存放在GPU显存里面了，而VideoDecoder中函数mapFrame，可完成从显存到内存的映射。</p> 
<p>1.2 VideoParser</p> 
<p>VideoParser中最重要的是三个回调函数，static int CUDAAPI HandleVideoSequence(void *pUserData, CUVIDEOFORMAT *pFormat)， HandlePictureDecode(void *pUserData, CUVIDPICPARAMS *pPicParams)，HandlePictureDisplay(void *pUserData, CUVIDPARSERDISPINFO *pPicParams)，实现对视频格式变换、视频解码、解码后显示等处理功能。HandleVideoSequence主要负责视频格式进行校验，没有实现其他功能，解码函数HandlePictureDecode调用的就是VideoDecoder的解码函数（CUDA的接口），显示函数HandlePictureDisplay完成了解码后GPU图像进入FrameQueue。</p> 
<p>1.3 VideoDecoder</p> 
<p>该类是最核心的硬件解码功能类，CUVIDDECODECREATEINFO oVideoDecodeCreateInfo_是创建解码信息结构体，CUvideodecoder oDecoder_是最内核的CUDA硬件解码器，VideoParser的解码功能实际上是在CUvideodecoder解码内核上封装实现的（层层封装导致源码有点复杂，所以想看懂实现机制需要有点耐心）。</p> 
<p></p> 
<p>2 核心解码模块的实现</p> 
<p>示例中NvDecodeD3D9.cpp实现了D3D环境的创建，CUDA模块的初始化，其中取视频帧图像显示的函数如下，该函数实现了从解码图像队列取出图像（实际上是显存指针），完成格式转换（NV12到ARGB），最后映射到D3D的Texture进行显示等功能，代码中我给出了关键部位的解释。</p> 
<p></p> 
<pre><code>bool copyDecodedFrameToTexture(unsigned int &amp;nRepeats, int bUseInterop, int *pbIsProgressive)
{
    CUVIDPARSERDISPINFO oDisplayInfo;

    if (g_pFrameQueue-&gt;dequeue(&amp;oDisplayInfo))
    {
    CCtxAutoLock lck(g_CtxLock);
    // Push the current CUDA context (only if we are using CUDA decoding path)
    CUresult result = cuCtxPushCurrent(g_oContext);
    //创建解码图像的显存指针，注意存储的是NV12格式的
    CUdeviceptr  pDecodedFrame[3] = { 0, 0, 0 }; 
    //用于解码图像后进行格式转换
    CUdeviceptr  pInteropFrame[3] = { 0, 0, 0 };

    *pbIsProgressive = oDisplayInfo.progressive_frame;
    g_bIsProgressive = oDisplayInfo.progressive_frame ? true : false;

    int num_fields = 1;
    if (g_bUseVsync) {            
        num_fields = std::min(2 + oDisplayInfo.repeat_first_field, 3);            
    }
    nRepeats = num_fields;

    CUVIDPROCPARAMS oVideoProcessingParameters;
    memset(&amp;oVideoProcessingParameters, 0, sizeof(CUVIDPROCPARAMS));

    oVideoProcessingParameters.progressive_frame = oDisplayInfo.progressive_frame;
    oVideoProcessingParameters.top_field_first = oDisplayInfo.top_field_first;
    oVideoProcessingParameters.unpaired_field = (oDisplayInfo.repeat_first_field &lt; 0);

    for (int active_field = 0; active_field &lt; num_fields; active_field++)
    {
        unsigned int nDecodedPitch = 0;
        unsigned int nWidth = 0;
        unsigned int nHeight = 0;

        oVideoProcessingParameters.second_field = active_field;

        // map decoded video frame to CUDA surfae
        // 调用Videodecoder中映射功能，找到解码后图像的显存地址，并得到Pitch关键参数
        if (g_pVideoDecoder-&gt;mapFrame(oDisplayInfo.picture_index, &amp;pDecodedFrame[active_field], &amp;nDecodedPitch, &amp;oVideoProcessingParameters) != CUDA_SUCCESS)
        {
            // release the frame, so it can be re-used in decoder
            g_pFrameQueue-&gt;releaseFrame(&amp;oDisplayInfo);

            // Detach from the Current thread
            checkCudaErrors(cuCtxPopCurrent(NULL));

            return false;
        }
        nWidth  = g_pVideoDecoder-&gt;targetWidth();
        nHeight = g_pVideoDecoder-&gt;targetHeight();
        // map DirectX texture to CUDA surface
        size_t nTexturePitch = 0;

        // If we are Encoding and this is the 1st Frame, we make sure we allocate system memory for readbacks
        if (g_bReadback &amp;&amp; g_bFirstFrame &amp;&amp; g_ReadbackSID)
        {
            CUresult result;
            checkCudaErrors(result = cuMemAllocHost((void **)&amp;g_pFrameYUV[0], (nDecodedPitch * nHeight + nDecodedPitch*nHeight/2)));
            checkCudaErrors(result = cuMemAllocHost((void **)&amp;g_pFrameYUV[1], (nDecodedPitch * nHeight + nDecodedPitch*nHeight/2)));
            checkCudaErrors(result = cuMemAllocHost((void **)&amp;g_pFrameYUV[2], (nDecodedPitch * nHeight + nDecodedPitch*nHeight/2)));
            checkCudaErrors(result = cuMemAllocHost((void **)&amp;g_pFrameYUV[3], (nDecodedPitch * nHeight + nDecodedPitch*nHeight/2)));
            checkCudaErrors(result = cuMemAllocHost((void **)&amp;g_pFrameYUV[4], (nDecodedPitch * nHeight + nDecodedPitch*nHeight / 2)));
            checkCudaErrors(result = cuMemAllocHost((void **)&amp;g_pFrameYUV[5], (nDecodedPitch * nHeight + nDecodedPitch*nHeight / 2)));

            g_bFirstFrame = false;

            if (result != CUDA_SUCCESS)
            {
                printf("cuMemAllocHost returned %d\n", (int)result);
                checkCudaErrors(result);
            }
        }

        // If streams are enabled, we can perform the readback to the host while the kernel is executing
        if (g_bReadback &amp;&amp; g_ReadbackSID)
        {
            CUresult result = cuMemcpyDtoHAsync(g_pFrameYUV[active_field], pDecodedFrame[active_field], (nDecodedPitch * nHeight * 3 / 2), g_ReadbackSID);

            if (result != CUDA_SUCCESS)
            {
                printf("cuMemAllocHost returned %d\n", (int)result);
                checkCudaErrors(result);
            }
        }

#if ENABLE_DEBUG_OUT
        printf("%s = %02d, PicIndex = %02d, OutputPTS = %08d\n",
               (oDisplayInfo.progressive_frame ? "Frame" : "Field"),
               g_DecodeFrameCount, oDisplayInfo.picture_index, oDisplayInfo.timestamp);
#endif

        if (g_pImageDX)
        {
            // map the texture surface
            g_pImageDX-&gt;map(&amp;pInteropFrame[active_field], &amp;nTexturePitch, active_field);
        }
        else
        {
            pInteropFrame[active_field] = g_pInteropFrame[active_field];
            nTexturePitch = g_pVideoDecoder-&gt;targetWidth() * 2;
        }

        // perform post processing on the CUDA surface (performs colors space conversion and post processing)
        // comment this out if we inclue the line of code seen above
        //调用CUDA功能模块，完成从NV12格式到ARGB格式的转换，该功能模块比较复杂，后面我将给出一个简单的实现方式
        cudaPostProcessFrame(&amp;pDecodedFrame[active_field], nDecodedPitch, &amp;pInteropFrame[active_field], 
                             nTexturePitch, g_pCudaModule-&gt;getModule(), g_kernelNV12toARGB, g_KernelSID);

        if (g_pImageDX)
        {
            // unmap the texture surface
            g_pImageDX-&gt;unmap(active_field);
        }

        // unmap video frame
        // unmapFrame() synchronizes with the VideoDecode API (ensures the frame has finished decoding)
        g_pVideoDecoder-&gt;unmapFrame(pDecodedFrame[active_field]);                  
        g_DecodeFrameCount++;

        if (g_bWriteFile)
        {
            checkCudaErrors(cuStreamSynchronize(g_ReadbackSID));
            SaveFrameAsYUV(g_pFrameYUV[active_field + 3],
                g_pFrameYUV[active_field],
                nWidth, nHeight, nDecodedPitch);
        }
    }

    // Detach from the Current thread
    checkCudaErrors(cuCtxPopCurrent(NULL));
    // release the frame, so it can be re-used in decoder
    g_pFrameQueue-&gt;releaseFrame(&amp;oDisplayInfo);
}
else
{
    // Frame Queue has no frames, we don't compute FPS until we start
    return false;
}

// check if decoding has come to an end.
// if yes, signal the app to shut down.
if (!g_pVideoSource-&gt;isStarted() &amp;&amp; g_pFrameQueue-&gt;isEndOfDecode() &amp;&amp; g_pFrameQueue-&gt;isEmpty())
{
    // Let's free the Frame Data
    if (g_ReadbackSID &amp;&amp; g_pFrameYUV)
    {
        cuMemFreeHost((void *)g_pFrameYUV[0]);
        cuMemFreeHost((void *)g_pFrameYUV[1]);
        cuMemFreeHost((void *)g_pFrameYUV[2]);
        cuMemFreeHost((void *)g_pFrameYUV[3]);
        cuMemFreeHost((void *)g_pFrameYUV[4]);
        cuMemFreeHost((void *)g_pFrameYUV[5]);
        g_pFrameYUV[0] = NULL;
        g_pFrameYUV[1] = NULL;
        g_pFrameYUV[2] = NULL;
        g_pFrameYUV[3] = NULL;
        g_pFrameYUV[4] = NULL;
        g_pFrameYUV[5] = NULL;
    }

    // Let's just stop, and allow the user to quit, so they can at least see the results
    g_pVideoSource-&gt;stop();

    // If we want to loop reload the video file and restart
    if (g_bLoop &amp;&amp; !g_bAutoQuit)
    {
        HRESULT hr = reinitCudaResources();
        if (SUCCEEDED(hr))
        {
            g_FrameCount = 0;
            g_DecodeFrameCount = 0;
            g_pVideoSource-&gt;start();
        }
    }

    if (g_bAutoQuit)
    {
        g_bDone = true;
    }
}
return true;
}
</code></pre> 
<p>以上功能模块与D3D掺和在一起，难以找到解码后取图像数据功能的核心模块，下面我给出基于opencv Mat的取图像数据方法，代码如下：</p> 
<p>bool GetGpuDecodeFrame(shared_ptr ptr_video_stream, Mat &amp;frame)</p> 
<p>{<!-- --></p> 
<p>CUVIDPARSERDISPINFO oDisplayInfo;</p> 
<p></p> 
<pre><code>if (ptr_video_stream-&gt;p_cuda_frame_queue)
{
    if (ptr_video_stream-&gt;p_cuda_frame_queue-&gt;FrameNumInQueue()&gt;0 &amp;&amp; ptr_video_stream-&gt;p_cuda_frame_queue-&gt;dequeue(&amp;oDisplayInfo))
    {
        CCtxAutoLock lck(cuvideo_ctx_lock_);
        CUresult result = cuCtxPushCurrent(cuda_context_);
        CUdeviceptr  pDecodedFrame = 0;
        int num_fields = 1;

        CUVIDPROCPARAMS oVideoProcessingParameters;
        memset(&amp;oVideoProcessingParameters, 0, sizeof(CUVIDPROCPARAMS));

        oVideoProcessingParameters.progressive_frame = oDisplayInfo.progressive_frame;
        oVideoProcessingParameters.top_field_first = oDisplayInfo.top_field_first;
        oVideoProcessingParameters.unpaired_field = (oDisplayInfo.repeat_first_field &lt; 0);
        oVideoProcessingParameters.second_field = 0;

        unsigned int nDecodedPitch = 0;
        unsigned int nWidth = 0;
        unsigned int nHeight = 0;
        //找到图像数据GPU显存地址
        if (ptr_video_stream-&gt;p_cuda_video_decoder-&gt;mapFrame(oDisplayInfo.picture_index, &amp;pDecodedFrame, &amp;nDecodedPitch, &amp;oVideoProcessingParameters) != CUDA_SUCCESS)
        {
            // release the frame, so it can be re-used in decoder
            ptr_video_stream-&gt;p_cuda_frame_queue-&gt;releaseFrame(&amp;oDisplayInfo);
            // Detach from the Current thread
            checkCudaErrors(cuCtxPopCurrent(NULL));
            return false;
        }
        nWidth = ptr_video_stream-&gt;p_cuda_video_decoder-&gt;targetWidth();
        nHeight = ptr_video_stream-&gt;p_cuda_video_decoder-&gt;targetHeight();
        Mat raw_frame = Mat::zeros(cvSize(nWidth, nHeight), CV_8UC3);
        //直接对显存图像数据进行NV12到RGB格式的转换，并将转换后的数据拷贝到内存
        VasGpuBoost::ColorConvert::Get()-&gt;ConvertD2HYUV422pToRGB24((uchar*)pDecodedFrame, raw_frame.data, nWidth, nHeight, nDecodedPitch);
        resize(raw_frame, frame, cvSize(ptr_video_stream-&gt;decode_param_.dst_width(), ptr_video_stream-&gt;decode_param_.dst_height()));
        raw_frame.release();
        ptr_video_stream-&gt;p_cuda_video_decoder-&gt;unmapFrame(pDecodedFrame);
        checkCudaErrors(cuCtxPopCurrent(NULL));
        ptr_video_stream-&gt;p_cuda_frame_queue-&gt;releaseFrame(&amp;oDisplayInfo);

        pts = 0;
        No = 0;
        return true;
    }
    else return false;
}
else return false;
}
</code></pre> 
<p>解码用到的cuda核心函数如下，需要强调的Ptich的大小并不是图像宽度，而是解码后图像存放数据行的宽度，通常情况下要比图像宽度要大，实际上格式转换过程参考了博客。<a href="https://link.zhihu.com/?target=http%3A//www.cnblogs.com/azraelly/archive/2013/01/01/2841269.html" rel="nofollow" title="常见图像格式转换">常见图像格式转换</a></p> 
<pre><code>__global__ void DevYuv420iToRgb(const uchar* yuv_data, uchar *rgb_data, const int width, const int height, const int pitch, const uchar *table_r, const uchar *table_g, const uchar *table_b)
{
    int i = threadIdx.x + blockIdx.x * blockDim.x;

    int64 size = pitch*height;
    int64 compute_size = width*height;

    int x, y;
    x = i % width;
    y = i / width;

    CUDA_KERNEL_LOOP(i, compute_size)
    {
        int y_offset = pitch * y + x;
        int nv_index = y / 2 * pitch + x - x % 2;


        int v_offset = size + nv_index;
        int u_offset = v_offset + 1;

        int Y = *(yuv_data + y_offset);
        int U = *(yuv_data + u_offset);
        int V = *(yuv_data + v_offset);

        *(rgb_data + 3 * i) = table_r[(Y &lt;&lt; 8) + V];
        *(rgb_data + 3 * i + 1) = table_g[(Y &lt;&lt; 16) + (U &lt;&lt; 8) + V];
        *(rgb_data + 3 * i + 2) = table_b[(Y &lt;&lt; 8) + U];
    }
}
</code></pre> 
<p>小结</p> 
<p>本文旨在揭示CUDA的硬件框架，通过对比实验发现硬件解码还是强大的，GTX970能够做到720p视频大约800fps的速度。我也是基于此框架，实现了一套基于cuda的多路视频硬件解码C++接口，输出opencv mat格式图像，做后续视频分析。</p> 
<p>由于时间关系，行文较为仓促，错误或者讲的不清楚的地方，大家可以给我留言。</p> 
<p></p> 
<p>原文链接：<a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/poweroranger/article/details/77845238" rel="nofollow" title="CUDA和FFMPEG硬件解码视频流_c# cuda avframe转surface-CSDN博客">CUDA和FFMPEG硬件解码视频流_c# cuda avframe转surface-CSDN博客</a></p> 
<p><span style="color:#fe2c24;"><strong>★文末名片可以免费领取音视频开发学习资料，内容包括（FFmpeg ，webRTC ，rtmp ，hls ，rtsp ，ffplay ，srs）以及音视频学习路线图等等。</strong></span></p> 
<p><span style="color:#fe2c24;"><strong>见下方!↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓</strong></span></p> 
<p><span style="color:#fe2c24;"> </span></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/586733070f2594aa75b7aa2ff12e5b27/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">FileInputStream(文件字节输入流)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d543a65caf5ac811cc85efbbce50061d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">用 LangChain 搭建基于 Notion 文档的 RAG 应用</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>