<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>用LSTM进行时间序列预测（单步，多步，单变量，多变量） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="用LSTM进行时间序列预测（单步，多步，单变量，多变量）" />
<meta property="og:description" content="0. LSTM用于时间序列预测 LSTM因其具有记忆的功能，可以利用很长的序列信息来建立学习模型，所以用它来进行时间序列的预测会很有优势。
在实际工程中用LSTM进行时间序列的预测主要有两个难点：一是前期对数据的处理，二是初始模型的搭建。对数据的处理无论是单步、多步、单变量还是多变量都会用到滑动窗口来处理数据，具体处理的方法后面会进行阐述；而对模型的搭建则要根据要解决的问题，原始数据的情况等，对模型的参数做相应的设定，如果单纯的LSTM无法满足要求，一般会加入全连接层来解决。
1. 单变量单步 单变量单步的预测是最简单的，比如说有一组数据为：
[1,2,3,4,5,6,7,8,9,10],然后要求利用这组数据训练一个模型，使其能对后面提供的数据做一个单步的预测。
我们先来对数据进行处理，处理的时候要根据不同框架所要求的LSTM的输入形式来处理，比如我们用Pytorch框架的时候输入要求是(seq_len, batch, input_size)。
因为是单变量的数据，我们令input_size=3,然后设seq_len=7，然后batch为1,
处理后的数据变为:
[[1,2,3], =====&gt; [4]
[2,3,4], =====&gt; [5]
[3,4,5], =====&gt; [6]
[4,5,6], =====&gt; [7]
[5,6,7], =====&gt; [8]
[6,7,8], =====&gt; [9]
[7,8,9]] =====&gt; [10]
前半部分是输入，箭头指向输出。也就是说将数据分批输入LSTM，没三个数据对应一个单步的预测目标。
数据处理好之后就是模型的搭建了，我们根据输入输出的形式来反推模型的结构。我们知道LSTM的结构是lstm = nn.LSTM(input_size, hidden_size, num_layers)，由上面给出的输入的形式可以确定input_size=3， hidden_size, num_layers这两个参数要自己设置，也就是隐藏层的大小和层数。我们的预计输出的形式为(7,1)，但是LSTM的标准输出形式为：
output=(seq_len,batch,num_directionshidden_size)=(7,1,num_directionshidden_size)，
当num_directionshidden_size不为1时我们需要在LSTM的后面加一个全连接层，并令全连接层的输入形式为num_directionshidden_size，输出为1 。这样我们的输出就变为了output=(7,1,1) ，然后用reshape方法(当out为numpy array时)或者view方法(当out为tensor时)将output整形为(7,1)。这样就可以用在后面的训练中了，即计算损失函数和反向传播。
2. 多变量多步 比如一组数据为：
[[1,11],[2,12],[3,13],[4,14],[5,15],[6,16],[7,17],[8,18],[9,19],[10,20]]
要求对其进行两步预测，则我们对其进行处理：
[[[1,11],[2,12],[3,13]], ======&gt;[[4,14],[5,15]]
[[2,12],[3,13],[4,14]], ======&gt;[[5,15],[6,16]]
[[3,13],[4,14],[5,15]], ======&gt;[[6,16],[7,17]]
[[4,14],[5,15],[6,16]], ======&gt;[[7,17],[8,18]]
[[5,15],[6,16],[7,17]], ======&gt;[[8,18],[9,19]]
[[6,16],[7,17],[8,18]], ======&gt;[[9,19],[10,20]]
前半部分是输入，箭头指向输出。seq_len=6,batch=3,input_size=2
搭建模型的部分跟上面的分析是一样的，output的形状跟target一样为(6,2,2)，而输入的形状为(6,3,2)，所以还是需要一个全连接层使其输入为(6,3,2)输出为(6,2,2)，这样才能用在后面的训练中。
最后如果要画出输出的图像的话可以取out[:3, 0, :]和out[:,1,:]在axis=0上拼接出来。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/67ded6c3bd8716bfbe44c4f4c469d3cb/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-05-20T22:17:50+08:00" />
<meta property="article:modified_time" content="2020-05-20T22:17:50+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">用LSTM进行时间序列预测（单步，多步，单变量，多变量）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="0_LSTM_0"></a>0. LSTM用于时间序列预测</h2> 
<p>LSTM因其具有记忆的功能，可以利用很长的序列信息来建立学习模型，所以用它来进行时间序列的预测会很有优势。</p> 
<p>在实际工程中用LSTM进行时间序列的预测主要有两个难点：一是前期对数据的处理，二是初始模型的搭建。对数据的处理无论是单步、多步、单变量还是多变量都会用到滑动窗口来处理数据，具体处理的方法后面会进行阐述；而对模型的搭建则要根据要解决的问题，原始数据的情况等，对模型的参数做相应的设定，如果单纯的LSTM无法满足要求，一般会加入全连接层来解决。</p> 
<h2><a id="1__4"></a>1. 单变量单步</h2> 
<p>单变量单步的预测是最简单的，比如说有一组数据为：<br> [1,2,3,4,5,6,7,8,9,10],然后要求利用这组数据训练一个模型，使其能对后面提供的数据做一个单步的预测。</p> 
<p>我们先来对数据进行处理，处理的时候要根据不同框架所要求的LSTM的输入形式来处理，比如我们用Pytorch框架的时候输入要求是(seq_len, batch, input_size)。</p> 
<p>因为是单变量的数据，我们令input_size=3,然后设seq_len=7，然后batch为1,<br> 处理后的数据变为:<br> [[1,2,3], =====&gt; [4]<br> [2,3,4], =====&gt; [5]<br> [3,4,5], =====&gt; [6]<br> [4,5,6], =====&gt; [7]<br> [5,6,7], =====&gt; [8]<br> [6,7,8], =====&gt; [9]<br> [7,8,9]] =====&gt; [10]<br> 前半部分是输入，箭头指向输出。也就是说将数据分批输入LSTM，没三个数据对应一个单步的预测目标。</p> 
<p>数据处理好之后就是模型的搭建了，我们根据输入输出的形式来反推模型的结构。我们知道LSTM的结构是lstm = nn.LSTM(input_size, hidden_size, num_layers)，由上面给出的输入的形式可以确定input_size=3， hidden_size, num_layers这两个参数要自己设置，也就是隐藏层的大小和层数。我们的预计输出的形式为(7,1)，但是LSTM的标准输出形式为：<br> output=(seq_len,batch,num_directions<em>hidden_size)=(7,1,num_directions</em>hidden_size)，<br> 当num_directions<em>hidden_size不为1时我们需要在LSTM的后面加一个全连接层，并令全连接层的输入形式为num_directions</em>hidden_size，输出为1 。这样我们的输出就变为了output=(7,1,1) ，然后用reshape方法(当out为numpy array时)或者view方法(当out为tensor时)将output整形为(7,1)。这样就可以用在后面的训练中了，即计算损失函数和反向传播。</p> 
<h2><a id="2__29"></a>2. 多变量多步</h2> 
<p>比如一组数据为：<br> [[1,11],[2,12],[3,13],[4,14],[5,15],[6,16],[7,17],[8,18],[9,19],[10,20]]<br> 要求对其进行两步预测，则我们对其进行处理：<br> [[[1,11],[2,12],[3,13]], ======&gt;[[4,14],[5,15]]<br> [[2,12],[3,13],[4,14]], ======&gt;[[5,15],[6,16]]<br> [[3,13],[4,14],[5,15]], ======&gt;[[6,16],[7,17]]<br> [[4,14],[5,15],[6,16]], ======&gt;[[7,17],[8,18]]<br> [[5,15],[6,16],[7,17]], ======&gt;[[8,18],[9,19]]<br> [[6,16],[7,17],[8,18]], ======&gt;[[9,19],[10,20]]<br> 前半部分是输入，箭头指向输出。seq_len=6,batch=3,input_size=2</p> 
<p>搭建模型的部分跟上面的分析是一样的，output的形状跟target一样为(6,2,2)，而输入的形状为(6,3,2)，所以还是需要一个全连接层使其输入为(6,3,2)输出为(6,2,2)，这样才能用在后面的训练中。</p> 
<p>最后如果要画出输出的图像的话可以取out[:3, 0, :]和out[:,1,:]在axis=0上拼接出来。</p> 
<p>下面是模型的建立：</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">LSTM</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> num_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>LSTM<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size
        self<span class="token punctuation">.</span>num_layers <span class="token operator">=</span> num_layers
        self<span class="token punctuation">.</span>lstm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span> batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># 2 for bidirection</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Set initial states</span>
        h0 <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">,</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>  <span class="token comment"># 2 for bidirection</span>
        c0 <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">,</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>

        <span class="token comment"># Forward propagate LSTM</span>
        out<span class="token punctuation">,</span> _ <span class="token operator">=</span> self<span class="token punctuation">.</span>lstm<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">(</span>h0<span class="token punctuation">,</span> c0<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># out: tensor of shape (batch_size, seq_length, hidden_size*2)</span>
        <span class="token comment"># print("output_in=", out.shape)</span>
        <span class="token comment"># print("fc_in_shape=", out[:, -1, :].shape)</span>
        <span class="token comment"># Decode the hidden state of the last time step</span>
        <span class="token comment"># out = torch.cat((out[:, 0, :], out[-1, :, :]), axis=0)</span>
        <span class="token comment"># out = self.fc(out[:, -1, :])  # 取最后一列为out</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        <span class="token keyword">return</span> out
</code></pre> 
<h2><a id="3__71"></a>3. 其余情况</h2> 
<p>单变量多步和多变量单步的情况的处理方法跟上面两个是类似的。</p> 
<p><a href="https://www.jianshu.com/p/3b60cefa3109" rel="nofollow">基于LSTM的时间序列数据(多步)预测</a><br> <a href="https://www.zhihu.com/question/360438648/answer/974245599" rel="nofollow">如何进行多变量LSTM时间序列预测未来一周的数据？</a><br> <a href="https://www.jianshu.com/p/38df71cad1f6" rel="nofollow">用LSTM预测未来一个月航班数</a><br> <a href="https://blog.csdn.net/qq_35649669/article/details/89575949">多维多步预测</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/339da67fee4d0a613d916ee4ca6dc100/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【Python】Tkinter教程</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1e6d6f17558849a7b970374b76774d8a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">xml的cdata</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>