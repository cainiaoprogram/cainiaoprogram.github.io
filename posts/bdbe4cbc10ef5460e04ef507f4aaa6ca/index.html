<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>文本情感分类模型之BERT - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="文本情感分类模型之BERT" />
<meta property="og:description" content="BERT是google开源的一种自然语言处理领域的经典模型，全称是Bidirectional Encoder Representations from Transformers 。它使用多头注意力和位置嵌入，来替换不易并行的循环神经网络。它的出现一举打破自然语言处理领域11个不同问题的记录，直接将自然语言处理推动到了一个新的时代。
本实验使用pytorch的相关神经网络库， 编写BERT 的语言模型， 并基于训练好的词向量，利用少量的训练数据，微调BERT 模型用于文本分类任务。实验使用的数据集是IMDB数据集。实验的主要内容包括：
实验准备：搭建基于GPU的pytorch实验环境。下载IMDB数据集。数据预处理：使用pytorch所提供的标准数据接口，将原始数据处理为方便模型训练脚本所使用的数据结构。语言模型：参考《动手学深度学习》，搭建BERT 模型并加载大语料库上预训练的模型参数。推荐的预训练参数来源为huggingface。情感分类：基于训练好的语言模型，编写一个情感分类模型，包含一个BERT 模型和一个分类器（MLP）。首先，将一个句子中的每个单词对应的词向量输入BERT，得到句子的向量表征。然后将句向量作为分类器的输入，输出二元分类预测，同样进行loss 计算和反向梯度传播训练，这里的loss是交叉熵loss。此次实验中不固定BERT 模型的参数，而须随分类器一同训练以达到微调的目的。超参数调试：通过调整学习率等超参数提高情感分类模型的准确率。测试性能：选择在上一步中最合适的一组超参数训练得到最终的模型，然后在测试集上测试性能，并于第二次实验中采用RNN模型的实验结果进行对比分析。 相关的代码下载链接：从零开始在Pytorch实现Bert模型
Pytorch下用Bert&#43;MLP实现文本情感分类网络 bert情感分类中用tokenizer实现文本预处理
pytorch实现具备预训练参数加载功能的bert模型
1.搭建实验环境 搭建GPU版Pytorch实验环境如下：
名称
版本
备注
Python
3.8
Pytorch
1.12.1
GPU
RTX2060
安装对应版本的cuda
2.下载IMDB数据集 下载IMDB数据集（地址：
https://ai.stanford.edu/~amaas/data/sentiment/
），解压后得到包含正负样本的电影评论数据集，具体如下：
3.数据预处理 对下载的数据集中的test和train分别进行预处理从而方便后续模型训练。预处理主要包括：txt文件读取、文本标签处理、特殊字符过滤、分词，使用了bert预训练模型的tokenizer将文本处理为bert所需输入形式，包括input_ids、token_type_ids和attention_mask以及每个文本的类别标签label。最后将处理后的数据，使用pickle 模块的 dump()方法进行序列化并存储，以方便后续调试。
读取IMDB数据
加载预训练的tokenizer并利用其对文本进行处理得到训练需要的输入数据集，其功能主要由ClsDataset()实现，核心代码如下。
数据集包括训练集、验证集和测试集，其中训练集和验证集由IMDB中的train文件夹中数据组成，总共25000个样本，随机分为训练集和验证集，大小分别为训练集20000样本，验证集5000样本，两个数据集不交叉。测试集由IMDB中的test文件夹中数据组成，用于最终的模型性能测试，大小为25000个样本。另外最终模型的训练在train文件夹的25000个样本上进行。
保存数据集
4.语言模型搭建和训练 4.1.搭建Bert模型 关于如何搭建Bert模型可以参考网上代码，或者参考本博客的相关文章和资源（
手动搭建Bert模型并实现与训练参数加载和微调）。
4.2.搭建情感分类模型 基于训练好的语言模型，编写一个情感分类模型，包含一个BERT 模型和一个分类器（MLP）。首先，将一个句子中的每个单词对应的词向量输入BERT，得到句子的向量表征。然后将句向量作为分类器的输入，输出二元分类预测，同样进行loss 计算和反向梯度传播训练，这里的loss是交叉熵loss。此次实验中不固定BERT 模型的参数，而须随分类器一同训练以达到微调的目的。
4.2.1.情感分类模型定义 情感分类模型，包含一个BERT 模型和一个分类器（MLP），两者间有一个dropout层，BERT模型实现了预训练参数加载功能。首先，将一个句子中的每个单词对应的词向量输入BERT，得到句子的向量表征。然后将句向量经过dropout层再输入分类器，最后输出二元分类预测。主要步骤及代码段如下：
4.2.2.训练情感分类模型 1）预训练模型和预处理数据集加载
加载预训练模型和预处理数据集，数据集为IMDB，预处理方法如3.3节所述，基于训练、验证、测试数据集构建了各自的迭代器train_loader、eval_loader、test_loader。
2）模型参数和训练参数定义
分别定义模型结构参数和训练的参数，其中由于GPU内存限制batch_size设置为4，但实际在训练时参数每8步更新一次，可以认为等效于每个batch_size为32.具体如下：
3）训练模型
实列化分类模型，并定义损失函数为CrossEntropyLoss()，优化器为Adam()，训练设备为GPU（cuda0），学习率等参数已在上一步说明，这些参数在训练中有调整。Bert模型的参数在实列化时加载了HuggingFace的bert_base_uncased预训练模型参数。
将模型设置为训练模式，加载输入样本和标签至GPU进行训练。
每个Epoch结束时，在验证集上测试模型的性能，包括分类准确度等指标，并保存当前表现最好的模型。
​​​​​​​4.3.超参数分析和调试 在前述情感分类模型基础上对相关超参数进行了不同的调整、比较并分析了情感分类模型在各个情况下的性能。由于GPU内存限制和时间周期限制，并未对不同batchsize大小、不同优化器、MLP结构对模型性能的影响进行分析，仅分析比较了不同dropout和学习率对模型性能的影响。batchsize大小、不同优化器、MLP结构等超参数参考了《动手学深度学习》和网络上的资料设定，batchsize大小为32，优化器选择为Adam，MLP使用了1层，hidden_size大小与Bert输出维度一致为768。
5.实验结果 修改数据集，将训练集和验证集合并为训练集，在该数据集使用上一节分析得到的最优参数，Bert模型采用HuggingFace的bert_base_uncased预训练模型的结构参数，总共包含了12层Transformer。模型的其他参数也参考了HuggingFace的bert_base_uncased预训练模型的结构参数。vocab_size为bert_base_uncased预训练模型的字典大小，hidden_size为768，attention_head_num为12，intermediate_size为3072，hidden_act与论文中保持一致使用gelu，Dropout为0.1，学习率为5e-6，epoch为5。重新训练模型，并在测试集上进行模型性能测试。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/bdbe4cbc10ef5460e04ef507f4aaa6ca/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-06T11:10:57+08:00" />
<meta property="article:modified_time" content="2023-04-06T11:10:57+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">文本情感分类模型之BERT</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>BERT是google开源的一种自然语言处理领域的经典模型，全称是<strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers 。它使用多头注意力和位置嵌入，来替换不易并行的循环神经网络。它的出现一举打破自然语言处理领域11个不同问题的记录，直接将自然语言处理推动到了一个新的时代。</p> 
<p>本实验使用pytorch的相关神经网络库， 编写BERT 的语言模型， 并基于训练好的词向量，利用少量的训练数据，微调BERT 模型用于文本分类任务。实验使用的数据集是IMDB数据集。实验的主要内容包括：</p> 
<ol><li style="text-align:justify;">实验准备：搭建基于GPU的pytorch实验环境。下载IMDB数据集。</li><li style="text-align:justify;">数据预处理：使用pytorch所提供的标准数据接口，将原始数据处理为方便模型训练脚本所使用的数据结构。</li><li style="text-align:justify;">语言模型：参考《动手学深度学习》，搭建BERT 模型并加载大语料库上预训练的模型参数。推荐的预训练参数来源为huggingface。</li><li style="text-align:justify;">情感分类：基于训练好的语言模型，编写一个情感分类模型，包含一个BERT 模型和一个分类器（MLP）。首先，将一个句子中的每个单词对应的词向量输入BERT，得到句子的向量表征。然后将句向量作为分类器的输入，输出二元分类预测，同样进行loss 计算和反向梯度传播训练，这里的loss是交叉熵loss。此次实验中不固定BERT 模型的参数，而须随分类器一同训练以达到微调的目的。</li><li style="text-align:justify;">超参数调试：通过调整学习率等超参数提高情感分类模型的准确率。</li><li style="text-align:justify;">测试性能：选择在上一步中最合适的一组超参数训练得到最终的模型，然后在测试集上测试性能，并于第二次实验中采用RNN模型的实验结果进行对比分析。</li></ol> 
<p> 相关的代码下载链接：<a class="link-info" href="https://download.csdn.net/download/m0_61142248/87364714" title="从零开始在Pytorch实现Bert模型">从零开始在Pytorch实现Bert模型</a></p> 
<p>                                     <a class="link-info" href="https://download.csdn.net/download/m0_61142248/87360585" title="Pytorch下用Bert+MLP实现文本情感分类网络">Pytorch下用Bert+MLP实现文本情感分类网络</a> </p> 
<p>                                     <a class="link-info" href="https://download.csdn.net/download/m0_61142248/87360575" title="bert情感分类中用tokenizer实现文本预处理">bert情感分类中用tokenizer实现文本预处理</a></p> 
<p>                                     <a class="link-info" href="https://download.csdn.net/download/m0_61142248/87360565" title="pytorch实现具备预训练参数加载功能的bert模型">pytorch实现具备预训练参数加载功能的bert模型</a></p> 
<h2>1.<strong>搭建实验环境</strong></h2> 
<p>搭建GPU版Pytorch实验环境如下：</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:138.25pt;"> <p style="margin-left:0;text-align:justify;"><span style="color:#000000;">名称</span></p> </td><td style="border-color:#000000;vertical-align:top;width:138.25pt;"> <p style="margin-left:0;text-align:justify;"><span style="color:#000000;">版本</span></p> </td><td style="border-color:#000000;vertical-align:top;width:138.3pt;"> <p style="margin-left:0;text-align:justify;"><span style="color:#000000;">备注</span></p> </td></tr><tr><td style="border-color:#000000;vertical-align:top;width:138.25pt;"> <p style="margin-left:0;text-align:justify;"><span style="color:#000000;">Python</span></p> </td><td style="vertical-align:top;width:138.25pt;"> <p style="margin-left:0;text-align:justify;"><span style="color:#000000;">3.8</span></p> </td><td style="vertical-align:top;width:138.3pt;"> <p style="margin-left:0;text-align:justify;"></p> </td></tr><tr><td style="border-color:#000000;vertical-align:top;width:138.25pt;"> <p style="margin-left:0;text-align:justify;"><span style="color:#000000;">Pytorch</span></p> </td><td style="vertical-align:top;width:138.25pt;"> <p style="margin-left:0;text-align:justify;"><span style="color:#000000;">1.12.1</span></p> </td><td style="vertical-align:top;width:138.3pt;"> <p style="margin-left:0;text-align:justify;"></p> </td></tr><tr><td style="border-color:#000000;vertical-align:top;width:138.25pt;"> <p style="margin-left:0;text-align:justify;"><span style="color:#000000;">GPU</span></p> </td><td style="vertical-align:top;width:138.25pt;"> <p style="margin-left:0;text-align:justify;"><span style="color:#000000;">RTX2060</span></p> </td><td style="vertical-align:top;width:138.3pt;"> <p style="margin-left:0;text-align:justify;"><span style="color:#000000;">安装对应版本的cuda</span></p> </td></tr></tbody></table> 
<h2>2.<strong>下载IMDB</strong><strong>数据集</strong></h2> 
<p style="margin-left:0;text-align:justify;">下载IMDB数据集（地址：<br> https://ai.stanford.edu/~amaas/data/sentiment/<br> ），解压后得到包含正负样本的电影评论数据集，具体如下：</p> 
<p style="margin-left:0;text-align:justify;"><img alt="" height="188" src="https://images2.imgbox.com/57/3b/7cc1EFcT_o.png" width="801"></p> 
<h2> 3.<strong>数据预处理</strong></h2> 
<p></p> 
<p style="margin-left:0;text-align:justify;">对下载的数据集中的test和train分别进行预处理从而方便后续模型训练。预处理主要包括：txt文件读取、文本标签处理、特殊字符<span style="background-color:#ffffff;"><span style="color:#000000;">过滤、</span></span>分词，使用了bert预训练模型的tokenizer将文本处理为bert所需输入形式，包括input_ids、token_type_ids和attention_mask以及每个文本的类别标签label。最后将处理后的数据，使用pickle 模块的 dump()方法进行序列化并存储，以方便后续调试。</p> 
<p style="margin-left:0;text-align:justify;">读取IMDB数据</p> 
<p style="margin-left:0;text-align:justify;"><img alt="" height="509" src="https://images2.imgbox.com/da/ac/I54gnKWG_o.png" width="1200"></p> 
<p style="margin-left:0;text-align:justify;">加载预训练的tokenizer并利用其对文本进行处理得到训练需要的输入数据集，其功能主要由ClsDataset()实现，核心代码如下。</p> 
<p style="margin-left:0;text-align:justify;"><img alt="" height="601" src="https://images2.imgbox.com/a7/92/z8HViogQ_o.png" width="1200"></p> 
<p style="margin-left:0;text-align:justify;">数据集包括训练集、验证集和测试集，其中训练集和验证集由IMDB中的train文件夹中数据组成，总共25000个样本，随机分为训练集和验证集，大小分别为训练集20000样本，验证集5000样本，两个数据集不交叉。测试集由IMDB中的test文件夹中数据组成，用于最终的模型性能测试，大小为25000个样本。另外最终模型的训练在train文件夹的25000个样本上进行。</p> 
<p style="margin-left:0;text-align:justify;"><img alt="" height="185" src="https://images2.imgbox.com/6c/76/DkD3qNaZ_o.png" width="1200"></p> 
<p style="margin-left:0;text-align:justify;">保存数据集</p> 
<p style="margin-left:0;text-align:justify;"><img alt="" height="210" src="https://images2.imgbox.com/22/fe/9BxZvmVW_o.png" width="1200"></p> 
<h2 style="margin-left:0px;text-align:justify;">4.语言模型搭建和训练</h2> 
<h3>4.1.搭建Bert模型</h3> 
<p>关于如何搭建Bert模型可以参考网上代码，或者参考本博客的相关文章和资源（</p> 
<p><a href="https://mp.csdn.net/mp_blog/creation/editor/129984751" title="手动搭建Bert模型并实现与训练参数加载和微调">手动搭建Bert模型并实现与训练参数加载和微调</a>）。</p> 
<h3>4.2.搭建情感分类模型</h3> 
<p><span style="background-color:#ffffff;"><span style="color:#000000;">基于训练好的语言模型，编写一个情感分类模型，包含一个</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">BERT </span></span><span style="background-color:#ffffff;"><span style="color:#000000;">模型和一个分类器（</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">MLP</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">）。首先，将一个句子中的每个单词对应的词向量输入</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">BERT</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">，得到句子的向量表征。然后将句向量作为分类器的输入，输出二元分类预测，同样进行</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">loss </span></span><span style="background-color:#ffffff;"><span style="color:#000000;">计算和反向梯度传播训练，这里的</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">loss</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">是交叉熵</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">loss</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">。此次实验中不固定</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">BERT </span></span><span style="background-color:#ffffff;"><span style="color:#000000;">模型的参数，而须随分类器一同训练以达到微调的目的。</span></span></p> 
<h4><span style="background-color:#ffffff;"><span style="color:#000000;">4.2.1.情感分类模型定义</span></span></h4> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#ffffff;"><span style="color:#000000;">情感分类模型，包含一个</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">BERT </span></span><span style="background-color:#ffffff;"><span style="color:#000000;">模型和一个分类器（</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">MLP</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">），两者间有一个</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">dropout</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">层，</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">BERT</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">模型实现了预训练参数加载功能。首先，将一个句子中的每个单词对应的词向量输入</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">BERT</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">，得到句子的向量表征。然后将句向量经过</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">dropout</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">层再输入分类器，最后输出二元分类预测。主要步骤及代码段如下：</span></span></p> 
<p style="margin-left:0;text-align:justify;"><img alt="" height="601" src="https://images2.imgbox.com/fc/a4/3fzjjVsX_o.png" width="1200"></p> 
<h4>4.2.2.<strong>训练情感分类模型</strong></h4> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#ffffff;"><span style="color:#000000;">1</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">）预训练模型和预处理数据集加载</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#ffffff;"><span style="color:#000000;">加载预训练模型和预处理数据集，数据集为</span></span>IMDB，预处理方法如3.3节所述，基于训练、验证、测试数据集构建了各自的迭代器train_loader、eval_loader、test_loader<span style="background-color:#ffffff;"><span style="color:#000000;">。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><img alt="" height="568" src="https://images2.imgbox.com/46/24/83OF17Mz_o.png" width="1170"></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#ffffff;"><span style="color:#000000;">2</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">）模型参数和训练参数定义</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#ffffff;"><span style="color:#000000;">分别定义模型结构参数和训练的参数，其中由于</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">GPU</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">内存限制</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">batch_size</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">设置为</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">4</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">，但实际在训练时参数每</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">8</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">步更新一次，可以认为等效于每个</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">batch_size</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">为</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">32.</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">具体如下：</span></span></p> 
<p style="margin-left:0;text-align:justify;"><img alt="" height="769" src="https://images2.imgbox.com/41/70/GsiHBXGQ_o.png" width="1200"></p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#ffffff;"><span style="color:#000000;">3</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">）训练模型</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#ffffff;"><span style="color:#000000;">实列化分类模型，并定义损失函数为</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">CrossEntropyLoss()</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">，优化器为</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">Adam()</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">，训练设备为</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">GPU</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">（</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">cuda0</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">），学习率等参数已在上一步说明，这些参数在训练中有调整。</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">Bert</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">模型的参数在实列化时加载了</span></span>HuggingFace的bert_base_uncased预训练模型参数。</p> 
<p style="margin-left:0;text-align:justify;"><img alt="" height="347" src="https://images2.imgbox.com/e0/c4/kGgedeib_o.png" width="1200"></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#ffffff;"><span style="color:#000000;">将模型设置为训练模式，加载输入样本和标签至</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">GPU</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">进行训练。</span></span></p> 
<p style="margin-left:0;text-align:justify;"><img alt="" height="448" src="https://images2.imgbox.com/36/3d/aPitwfoq_o.png" width="1148"></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#ffffff;"><span style="color:#000000;">每个</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">Epoch</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">结束时，在验证集上测试模型的性能，包括分类准确度等指标，并保存当前表现最好的模型。</span></span></p> 
<h2 style="margin-left:0px;text-align:justify;"><img alt="" height="485" src="https://images2.imgbox.com/11/fc/Rq8LBaBS_o.png" width="1091"></h2> 
<h3>​​​​​​​4.3.<strong>超参数分析和调试</strong></h3> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#ffffff;"><span style="color:#000000;">在前述情感分类模型基础上对相关超参数进行了不同的调整、比较并分析了情感分类模型在各个情况下的性能。由于</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">GPU</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">内存限制和时间周期限制，并未对不同</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">batchsize</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">大小、不同优化器、</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">MLP</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">结构对模型性能的影响进行分析，仅分析比较了不同</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">dropout</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">和学习率对模型性能的影响。</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">batchsize</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">大小、不同优化器、</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">MLP</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">结构等超参数参考了</span></span>《动手学深度学习》和网络上的资料设定，<span style="background-color:#ffffff;"><span style="color:#000000;">batchsize</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">大小为</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">32</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">，优化器选择为</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">Adam</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">，</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">MLP</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">使用了</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">1</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">层，</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">hidden_size</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">大小与</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">Bert</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">输出维度一致为</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">768</span></span><span style="background-color:#ffffff;"><span style="color:#000000;">。</span></span></p> 
<h2 style="margin-left:0px;text-align:justify;">5.<strong>实验结果</strong></h2> 
<p style="margin-left:0;text-align:justify;">修改数据集，将训练集和验证集合并为训练集，在该数据集使用上一节分析得到的最优参数，Bert模型采用HuggingFace的bert_base_uncased预训练模型的结构参数，总共包含了12层Transformer。模型的其他参数也参考了HuggingFace的bert_base_uncased预训练模型的结构参数。vocab_size为bert_base_uncased预训练模型的字典大小，hidden_size为768，attention_head_num为12，intermediate_size为3072，hidden_act与论文中保持一致使用gelu，Dropout为0.1，学习率为5e-6，epoch为5。重新训练模型，并在测试集上进行模型性能测试。</p> 
<p style="margin-left:0;text-align:justify;">在测试集上测试模型性能，在IMDB的测试数据集上（test，包含25000个样本）测得的结果如下：</p> 
<p style="margin-left:0;text-align:justify;"><img alt="" height="297" src="https://images2.imgbox.com/9c/b3/nUaRA1Aw_o.png" width="1200"></p> 
<p style="margin-left:0;text-align:justify;">测试结果表明最后的模型在测试集上的准确度为94%。</p> 
<h2 style="margin-left:0px;text-align:justify;">6.结语</h2> 
<p style="margin-left:0;text-align:justify;">通过本次实验，熟悉了基于Pytorch和nvidiaGPU的深度学习开发环境搭建过程。掌握了自然语言处理中Bert模型的基本原理和实现方法，基于预训练模型的参数加载方法和基于Bert模型实现情感分析的模型架构、搭建和训练方法。通过对文本情感分类网络的调参分析和模型训练掌握了超参数调节方法、各种超参数对训练过程、模型性能的影响。</p> 
<p></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/db638ccf4b0f19de5389df3a3722f3cc/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">YOLOV5实战教程（超级详细图文教程）！！！</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/9c63efae1c352bd1878c8c73ab8620d3/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">PyTorch实现前馈神经网络（torch.nn）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>