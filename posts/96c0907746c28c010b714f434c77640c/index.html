<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>图像分割系列概述FCN、UNet、DeepLab - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="图像分割系列概述FCN、UNet、DeepLab" />
<meta property="og:description" content="摘要
图像分割的本质是像素级别的分割。广泛应用于医疗成像、自动驾驶领域等。
图像分割的类型：
普通分割：将不同类别不同物体的像素区域分开，比如分开前景和后景。
语义分割：在普通分割基础上，分类出每块区域的语义（什么物体）。将画面中所有物体都指出各自的类别。
实例分割：在语义分割的基础上， 识别并给出每个相同的物体的编号。
FCN
普通CNN网络用于输出图像级别的分类和回归任务。它的尾部是全连接输出固定的值。而FCN将CNN最后一层全连接层替换为卷积层，输出一张Label好的图片。这种结构就是一种编解码的结构。但是FCN得到的结果还是不够精细，上采样的结果还是比较模糊和平滑，细节敏感性低。concat不够，应用了池化，像素位置有问题。
基于FCN框架的UNet
采用FCN的思想，四个下采样提取目标特征，再通过四个上采样，最后逐个对其像素点进行分类，那么这实际上是一个基于编码器（encode）-解码器（decode）思想。
网络模型结构：
下采样方法（压缩图像的特征）：使用大步长的卷积；池化；使用Padding的卷积。
上采样方法：像素插值（双线性插值、邻近插值——信息丢失少，速度适中），反卷积（转置卷积T.conv——参数少，速度快），反池化（Unpooling），像素融合（通道信息平铺，不丢失信息）
在Unet中使用了转置卷积实现上采样，在Unet中每一个上采样Block里，运用了一个跳连接把前面一部分特征Concat到了上采样后的特征图上。（思想类似ResNet），目的是使上采样后的特征图有更多的浅层语义信息，增强分割精度，在最后一层直接使用sigmoid二分类把mask分为前景和背景。
损失函数：常见的图像分割损失函数有，Binary crossentropy（BCE），Dice coefficient，Focal loss（解决类别不平衡）。Unet使用如下的损失函数。
存在的问题：网络是固定层数，对不同的分割需求是冗余的。可以改进模型结构，适应不同的分割需求。
UNet &#43;&#43;
论文:https://arxiv.org/abs/1807.10165
UNet&#43;&#43;根据UNet模型的结构进行优化。可以把UNet&#43;&#43;看成多个不同深度，不同池化的Unet 的集合模型。提出了Deep Supervision 深监督，作用类似辅助loss，可以加快模型收敛速度。在训练时是集合训练，在测试使用时，可以通过不同剪枝来选择更简单合适的模型。
两个问题：
为什么UNet&#43;&#43;可以剪枝？
模型是长连接加短连接，所以每一层都可以单独剪下来去使用，结构决定。
如何剪枝？
应用了一个深监督模型1*1的卷积，监督他每一层的输出，都要去训练单独使用。
好处：适应各种简单或复杂的任务。
为什么测试时剪枝？而不是剪枝后再去训练？
剪完枝后去训练没有共同促进的作用，而且剪完训练后使用也更复杂，无法判断哪个模型适应哪个任务，只能主观判断。而测试的时候剪枝可以通过模型的dice系数在哪一层的效果最好去决定。
DeepLab v1-v3
论文：
V1 https://arxiv.org/pdf/1412.7062v3.pdf
V2 https://arxiv.org/pdf/1606.00915.pdf
V3 https://arxiv.org/pdf/1706.05587.pdf
V3&#43; https://arxiv.org/pdf/1802.02611.pdf
DeepLab系列使用了空洞卷积，ASPP结构，Encoder-Decoder（编解码）结构，CNN与PGM（概率图模型）结合，。
空洞卷积：
这里的Input stride理解为空洞的洞的大小，这里洞为1，在3x3的卷积核下，使用了Input stride使原本3x3的卷积核有了5x5大小的感受野，但是原始卷积核还是没变。
ASPP结构：
目的是通过不同的比例rate构建不同感受野大小的卷积核，来获取多尺度的物体信息。
CRF：
条件随机场
在V3中，作者把解决多尺度的问题的方法分为4类：
图像金字塔、编解码、网络层最后加入其它模块如DenseCRF或串联不同感受野大小的卷积、在网络层最后并行空间金字塔池化模块获取不同尺度物体信息。
V3在实验中主要做了两个改进：
使用残差网络结构时通过串联结构，做到了更深的空洞卷积；
优化了ASPP。
V3&#43;：
把原DeepLabv3当作encoder，添加decoder得到新的模型（DeepLabv3&#43;），也就是把ASPP模块与E-Dcoder融合为一体。使用了深度可分离卷积和膨胀卷积。
网络结构：
在Encoder部分使用空洞卷积，得到多尺度的特征图，再将这部分拼接在一起后使用1x1的卷积，将通道变小后上采样（使用双线性插值）输入Decoder。
在Decoder部分先将前面的低维度特征通过1x1卷积变换通道后，将Encoder的输出拼接。使用3x3的卷积、上采样输出。
由于普通卷积学习的是像素之间的关系，具有平移不变性，而在这里用在图像语义分割上不好。而平移同变性指位置发生变换，相应的侦测输出也会变化。我们希望图像分割任务能够及时响应物体位置的变化。
同一个像素在同一图像上，在某一位置是前景，而在另外一位置可能变成了后景。在不同的位置具有不同的语义信息。
高级语义：抽象程度高，用于检测分类回归任务。
低级语义：像素点的定位。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/96c0907746c28c010b714f434c77640c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-06-22T11:18:33+08:00" />
<meta property="article:modified_time" content="2020-06-22T11:18:33+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">图像分割系列概述FCN、UNet、DeepLab</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-github-gist">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><strong>摘要</strong></p> 
<p>图像分割的本质是像素级别的分割。广泛应用于医疗成像、自动驾驶领域等。</p> 
<p><strong>图像分割的类型：</strong></p> 
<p>普通分割：将不同类别不同物体的像素区域分开，比如分开前景和后景。<br> 语义分割：在普通分割基础上，分类出每块区域的语义（什么物体）。将画面中所有物体都指出各自的类别。<br> 实例分割：在语义分割的基础上， 识别并给出每个相同的物体的编号。</p> 
<p><strong>FCN</strong></p> 
<p>普通CNN网络用于输出图像级别的分类和回归任务。它的尾部是全连接输出固定的值。而FCN将CNN最后一层全连接层替换为卷积层，输出一张Label好的图片。这种结构就是一种编解码的结构。但是FCN得到的结果还是不够精细，上采样的结果还是比较模糊和平滑，细节敏感性低。concat不够，应用了池化，像素位置有问题。<br> <img src="https://images2.imgbox.com/ac/fe/dGxP3nFx_o.png" alt="1"></p> 
<p><strong>基于FCN框架的UNet</strong></p> 
<p>采用FCN的思想，四个下采样提取目标特征，再通过四个上采样，最后逐个对其像素点进行分类，那么这实际上是一个基于编码器（encode）-解码器（decode）思想。</p> 
<p>网络模型结构：<br> <img src="https://images2.imgbox.com/fa/d7/HHmWAfKh_o.png" alt="1"></p> 
<p>下采样方法（压缩图像的特征）：使用大步长的卷积；池化；使用Padding的卷积。<br> 上采样方法：像素插值（双线性插值、邻近插值——信息丢失少，速度适中），反卷积（转置卷积T.conv——参数少，速度快），反池化（Unpooling），像素融合（通道信息平铺，不丢失信息）</p> 
<p>在Unet中使用了转置卷积实现上采样，在Unet中每一个上采样Block里，运用了一个跳连接把前面一部分特征Concat到了上采样后的特征图上。（思想类似ResNet），目的是使上采样后的特征图有更多的浅层语义信息，增强分割精度，在最后一层直接使用sigmoid二分类把mask分为前景和背景。</p> 
<p>损失函数：常见的<a href="http://aiuai.cn/aifarm1330.html" rel="nofollow">图像分割损失函数</a>有，Binary crossentropy（BCE），Dice coefficient，Focal loss（解决类别不平衡）。Unet使用如下的损失函数。<br> <img src="https://images2.imgbox.com/46/e4/gKcwcWtb_o.png" alt="1"></p> 
<p>存在的问题：网络是固定层数，对不同的分割需求是冗余的。可以改进模型结构，适应不同的分割需求。</p> 
<p><strong>UNet ++</strong></p> 
<p>论文:<a href="https://arxiv.org/abs/1807.10165" rel="nofollow">https://arxiv.org/abs/1807.10165</a></p> 
<p>UNet++根据UNet模型的结构进行优化。可以把UNet++看成多个不同深度，不同池化的Unet 的集合模型。提出了Deep Supervision 深监督，作用类似辅助loss，可以加快模型收敛速度。在训练时是集合训练，在测试使用时，可以通过不同剪枝来选择更简单合适的模型。<br> <img src="https://images2.imgbox.com/00/89/TLxQyAS9_o.png" alt="2"><br> 两个问题：<br> 为什么UNet++可以剪枝？<br> 模型是长连接加短连接，所以每一层都可以单独剪下来去使用，结构决定。<br> 如何剪枝？<br> 应用了一个深监督模型1*1的卷积，监督他每一层的输出，都要去训练单独使用。<br> 好处：适应各种简单或复杂的任务。<br> 为什么测试时剪枝？而不是剪枝后再去训练？<br> 剪完枝后去训练没有共同促进的作用，而且剪完训练后使用也更复杂，无法判断哪个模型适应哪个任务，只能主观判断。而测试的时候剪枝可以通过模型的dice系数在哪一层的效果最好去决定。</p> 
<p><strong>DeepLab v1-v3</strong></p> 
<p>论文：<br> V1 <a href="https://arxiv.org/pdf/1412.7062v3.pdf" rel="nofollow">https://arxiv.org/pdf/1412.7062v3.pdf</a><br> V2 <a href="https://arxiv.org/pdf/1606.00915.pdf" rel="nofollow">https://arxiv.org/pdf/1606.00915.pdf</a><br> V3 <a href="https://arxiv.org/pdf/1706.05587.pdf" rel="nofollow">https://arxiv.org/pdf/1706.05587.pdf</a><br> V3+ <a href="https://arxiv.org/pdf/1802.02611.pdf" rel="nofollow">https://arxiv.org/pdf/1802.02611.pdf</a></p> 
<p>DeepLab系列使用了空洞卷积，ASPP结构，Encoder-Decoder（编解码）结构，CNN与PGM（概率图模型）结合，。</p> 
<p><strong>空洞卷积：</strong><br> <img src="https://images2.imgbox.com/02/63/ueH153x7_o.png" alt="1"><br> 这里的Input stride理解为空洞的洞的大小，这里洞为1，在3x3的卷积核下，使用了Input stride使原本3x3的卷积核有了5x5大小的感受野，但是原始卷积核还是没变。<br> <img src="https://images2.imgbox.com/13/b0/EaonDVmJ_o.gif" alt="1"><br> <strong>ASPP结构：</strong><br> <img src="https://images2.imgbox.com/01/42/sGuuXb3a_o.png" alt="1"><br> 目的是通过不同的比例rate构建不同感受野大小的卷积核，来获取多尺度的物体信息。</p> 
<p><strong>CRF：</strong></p> 
<p><a href="https://www.jianshu.com/p/5d6a03c1f83f" rel="nofollow">条件随机场</a></p> 
<p>在V3中，作者把解决多尺度的问题的方法分为4类：</p> 
<p>图像金字塔、编解码、网络层最后加入其它模块如DenseCRF或串联不同感受野大小的卷积、在网络层最后并行空间金字塔池化模块获取不同尺度物体信息。</p> 
<p><img src="https://images2.imgbox.com/64/57/snieraMd_o.png" alt="1"><br> V3在实验中主要做了两个改进：</p> 
<p>使用残差网络结构时通过串联结构，做到了更深的空洞卷积；<br> 优化了ASPP。</p> 
<p>V3+：</p> 
<p>把原DeepLabv3当作encoder，添加decoder得到新的模型（DeepLabv3+），也就是把ASPP模块与E-Dcoder融合为一体。使用了深度可分离卷积和膨胀卷积。</p> 
<p>网络结构：<br> <img src="https://images2.imgbox.com/9a/09/EBj8dmbN_o.png" alt="1"><br> 在Encoder部分使用空洞卷积，得到多尺度的特征图，再将这部分拼接在一起后使用1x1的卷积，将通道变小后上采样（使用双线性插值）输入Decoder。<br> 在Decoder部分先将前面的低维度特征通过1x1卷积变换通道后，将Encoder的输出拼接。使用3x3的卷积、上采样输出。</p> 
<p>由于普通卷积学习的是像素之间的关系，具有平移不变性，而在这里用在图像语义分割上不好。而平移同变性指位置发生变换，相应的侦测输出也会变化。我们希望图像分割任务能够及时响应物体位置的变化。<br> 同一个像素在同一图像上，在某一位置是前景，而在另外一位置可能变成了后景。在不同的位置具有不同的语义信息。<br> 高级语义：抽象程度高，用于检测分类回归任务。<br> 低级语义：像素点的定位。</p> 
<p><strong>Mask-RCNN</strong></p> 
<p>以Faster RCNN原型，增加了一个分支用于分割任务。所以这个模型同时包含了检测分类回归分割任务，当然单对于分割任务有些冗余。</p> 
<p>使用<a href="https://www.cnblogs.com/codehome/p/10910180.html" rel="nofollow">ROI Align</a>替代ROI Pooling。</p> 
<p><strong>图像分割评估指标</strong></p> 
<p><strong>DICE 系数：</strong></p> 
<p>取值：[0，1]<br> <img src="https://images2.imgbox.com/4a/57/q2j7ztbe_o.png" alt="1"><br> <strong>Jaccard 系数</strong></p> 
<p>取值：[0，1]<br> <img src="https://images2.imgbox.com/c2/e2/7UC6mfEZ_o.png" alt="1"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/047aaa2c078e162fab34f01754082f1e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Graylog3 布署</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e45cd3a39842a95c4086d0a33eb1d204/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Javase基础（二）——数据类型</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>