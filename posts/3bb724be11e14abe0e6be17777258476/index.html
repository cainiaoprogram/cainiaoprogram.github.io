<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>模型评估：A/B测试的陷阱 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="模型评估：A/B测试的陷阱" />
<meta property="og:description" content=" 互联网公司中，A/B测试是验证新模块、新功能、新产品是否有效；新算法、新模型的效果是否有提升；新设计是否受到用户欢迎；新更改是否影响用户体验的主要测试方法。在机器学习领域中，A/B测试是验证模型最终效果的主要手段。
1. 在对模型进行过充分的离线评估之后，为什么还要进行在线A/B测试？
离线评估无法完全消除模型过拟合的影响，因此，得出的离线评估结果无法完全替代线上评估结果。离线评估结果无法完全还原线上的工程环境。一般来讲，离线评估往往不会考虑线上环境的延迟、数据丢失、标签数据缺失等情况。因此，离线评估的结果是理想工程环境下的结果。线上系统的某些商业指标在离线评估中无法计算。离线评估一般是针对模型本身进行评估，而与模型相关的其他指标，特别是商业指标，往往无法直接获得。比如，上线了新的推荐算法，离线评估往往关注的是ROC曲线，P-R曲线等的改进，而线上评估可以全面了解该推荐算法带来的用户点击率、留存时长、PV访问量等的变化。这些都要由A/B测试来进行全面的评估。 2. 如何进行线上A/B测试？
进行A/B测试的主要手段是进行用户分桶，即将用户分成实验组和对照组，对实验组的用户施以新模型，对对照组的用户施以旧模型。在分桶的过程中，要注意样本的独立性和采样方式的无偏性，确保同一个用户每次只能分到同一个桶中。在分桶过程中所选取的user_id需要是一个随机数，这样才能保证桶中的样本是无偏的。 3. 如何划分实验组和对照组？
H公司的算法工程师们最近针对系统中的“美国用户”研发了一套全新的视频推荐模型A，而目前正在使用的针对全体用户的推荐模型是B。在正式上线之前，工程师们希望通过A/B测试来验证新推荐模型的效果。下面有三种实验组和对照组的划分方法，请指出哪种划分方法是正确的？
根据user_id（user_id完全随机生成）个位数的奇偶性将用户划分为实验组和对照组，对实验组施以推荐模型A，对照组施以推荐模型B；将user_id个位数为奇数且为美国用户的作为实验组，其余用户为对照组；将user_id个位数为奇数且为美国用户的作为实验组，user_id个位数为偶数的用户作为对照组。 上述三种A/B测试的划分方法都不正确。我们用包含关系图来说明三种划分方法，如图2.4所示。
方法1（见图2.4(a））没有区分是否为美国用户，实验组和对照组的实验结果均有稀释；方法2（见图2.4(b））的实验组选取无误，并将其余所有用户划分为对照组，导致对照组的结果被稀释；方法3（见图2.4(c））的对照组存在偏差。正确做法（见图2.4(d））是将所有美国用户根据user_id个位数划分为实验组和对照组，分别施以模型A和B，才能够验证模型A的效果。 " />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/3bb724be11e14abe0e6be17777258476/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-11T20:26:40+08:00" />
<meta property="article:modified_time" content="2024-01-11T20:26:40+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">模型评估：A/B测试的陷阱</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>互联网公司中，<span style="color:#511b78;"><strong>A/B测试是验证新模块、新功能、新产品是否有效；新算法、新模型的效果是否有提升；新设计是否受到用户欢迎；新更改是否影响用户体验的主要测试方法</strong></span>。在机器学习领域中，A/B测试是验证模型最终效果的主要手段。</p> 
<p><strong>1. 在对模型进行过充分的离线评估之后，为什么还要进行在线A/B测试？</strong></p> 
<ol><li>离线评估无法完全消除模型过拟合的影响，因此，得出的离线评估结果无法完全替代线上评估结果。</li><li>离线评估结果无法完全还原线上的工程环境。一般来讲，离线评估往往不会考虑线上环境的延迟、数据丢失、标签数据缺失等情况。因此，离线评估的结果是理想工程环境下的结果。</li><li>线上系统的某些商业指标在离线评估中无法计算。离线评估一般是针对模型本身进行评估，而与模型相关的其他指标，特别是商业指标，往往无法直接获得。比如，上线了新的推荐算法，<span style="color:#be191c;"><strong>离线评估往往关注的是ROC曲线，P-R曲线等的改进，而线上评估可以全面了解该推荐算法带来的用户点击率、留存时长、PV访问量等的变化。</strong></span>这些都要由A/B测试来进行全面的评估。</li></ol> 
<p><strong>2. 如何进行线上A/B测试？</strong></p> 
<ul><li>进行A/B测试的主要手段是进行用户分桶，即将用户分成实验组和对照组，对实验组的用户施以新模型，对对照组的用户施以旧模型。</li><li>在分桶的过程中，<span style="color:#1c7331;"><strong>要注意样本的独立性和采样方式的无偏性，确保同一个用户每次只能分到同一个桶中</strong></span>。</li><li>在分桶过程中所选取的user_id需要是一个随机数，这样才能保证桶中的样本是无偏的。</li></ul> 
<p><strong>3. 如何划分实验组和对照组？</strong></p> 
<p>H公司的算法工程师们最近针对系统中的“美国用户”研发了一套全新的视频推荐模型A，而目前正在使用的针对全体用户的推荐模型是B。在正式上线之前，工程师们希望通过A/B测试来验证新推荐模型的效果。<span style="color:#4da8ee;"><strong>下面有三种实验组和对照组的划分方法，请指出哪种划分方法是正确的？</strong></span></p> 
<ol><li>根据user_id（user_id完全随机生成）个位数的奇偶性将用户划分为实验组和对照组，对实验组施以推荐模型A，对照组施以推荐模型B；</li><li>将user_id个位数为奇数且为美国用户的作为实验组，其余用户为对照组；</li><li>将user_id个位数为奇数且为美国用户的作为实验组，user_id个位数为偶数的用户作为对照组。</li></ol> 
<p>上述三种A/B测试的划分方法都不正确。我们用包含关系图来说明三种划分方法，如图2.4所示。</p> 
<p><img alt="" height="455" src="https://images2.imgbox.com/01/44/GnjI9IcJ_o.png" width="577"></p> 
<ul><li>方法1（见图2.4(a））没有区分是否为美国用户，<span style="color:#b95514;"><strong>实验组和对照组的实验结果均有稀释；</strong></span></li><li>方法2（见图2.4(b））的实验组选取无误，并将其余所有用户划分为对照组，导致对照组的结果被稀释；</li><li>方法3（见图2.4(c））的对照组存在偏差。</li><li>正确做法（见图2.4(d））是将所有美国用户根据user_id个位数划分为实验组和对照组，分别施以模型A和B，才能够验证模型A的效果。</li></ul> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/2d3aeb784dc8e1689351ba12184eea6a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Uncaught ReferenceError: videojs is not defined</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e2bfad6cc2f8efce27654051b55761c1/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">MongoDB之Change Stream实战</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>