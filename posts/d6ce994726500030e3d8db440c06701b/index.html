<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>vc sneå’Œt sneæ˜¯æˆ‘çš„é‚»å±… - èœé¸Ÿç¨‹åºå‘˜åšå®¢</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="vc sneå’Œt sneæ˜¯æˆ‘çš„é‚»å±…" />
<meta property="og:description" content="Many dimensionality reduction techniques attempt to preserve distances of the original data. However, it can be beneficial to focus on preserving the nearest neighbours for visualization. t-SNE[van der Maaten/Hinton 2008] abstracts away density and distance information. Since it preserves the neighbours, it often reveals the cluster structure more clearly than any other dimensionality reduction technique. t-SNE is really popular in many applications including life science.
è®¸å¤šé™ç»´æŠ€æœ¯è¯•å›¾ä¿ç•™åŸå§‹æ•°æ®çš„è·ç¦»ã€‚ ä½†æ˜¯ï¼Œä¸“æ³¨äºä¿ç•™æœ€è¿‘çš„é‚»å±…ä»¥è¿›è¡Œå¯è§†åŒ–å¯èƒ½æ˜¯æœ‰ç›Šçš„ã€‚ t-SNE [van der Maaten / Hinton 2008]æå–äº†å¯†åº¦å’Œè·ç¦»ä¿¡æ¯ã€‚ ç”±äºå®ƒä¿ç•™äº†é‚»å±…ï¼Œå› æ­¤ä¸å…¶ä»–é™ç»´æŠ€æœ¯ç›¸æ¯”ï¼Œå®ƒé€šå¸¸æ›´æ¸…æ¥šåœ°æ­ç¤ºå‡ºç°‡ç»“æ„ã€‚ t-SNEåœ¨åŒ…æ‹¬ç”Ÿå‘½ç§‘å­¦åœ¨å†…çš„è®¸å¤šåº”ç”¨ä¸­ç¡®å®å¾ˆå—æ¬¢è¿ã€‚" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/d6ce994726500030e3d8db440c06701b/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-10-13T23:47:28+08:00" />
<meta property="article:modified_time" content="2020-10-13T23:47:28+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="èœé¸Ÿç¨‹åºå‘˜åšå®¢" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">èœé¸Ÿç¨‹åºå‘˜åšå®¢</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">vc sneå’Œt sneæ˜¯æˆ‘çš„é‚»å±…</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <article style="font-size: 16px;"> 
 <div> 
  <section> 
   <div> 
    <div> 
     <p>Many dimensionality reduction techniques attempt to preserve distances of the original data. However, it can be beneficial to focus on preserving the nearest neighbours for visualization. t-SNE[van der Maaten/Hinton 2008] abstracts away density and distance information. Since it preserves the neighbours, it often reveals the cluster structure more clearly than any other dimensionality reduction technique. t-SNE is really popular in many applications including life science.</p> 
     <p>è®¸å¤šé™ç»´æŠ€æœ¯è¯•å›¾ä¿ç•™åŸå§‹æ•°æ®çš„è·ç¦»ã€‚ ä½†æ˜¯ï¼Œä¸“æ³¨äºä¿ç•™æœ€è¿‘çš„é‚»å±…ä»¥è¿›è¡Œå¯è§†åŒ–å¯èƒ½æ˜¯æœ‰ç›Šçš„ã€‚ t-SNE [van der Maaten / Hinton 2008]æå–äº†å¯†åº¦å’Œè·ç¦»ä¿¡æ¯ã€‚ ç”±äºå®ƒä¿ç•™äº†é‚»å±…ï¼Œå› æ­¤ä¸å…¶ä»–é™ç»´æŠ€æœ¯ç›¸æ¯”ï¼Œå®ƒé€šå¸¸æ›´æ¸…æ¥šåœ°æ­ç¤ºå‡ºç°‡ç»“æ„ã€‚ t-SNEåœ¨åŒ…æ‹¬ç”Ÿå‘½ç§‘å­¦åœ¨å†…çš„è®¸å¤šåº”ç”¨ä¸­ç¡®å®å¾ˆå—æ¬¢è¿ã€‚</p> 
     <h2> æ¯”è¾ƒæ–¹å¼ <span style="font-weight: bold;">(</span>Comparison<span style="font-weight: bold;">)</span></h2> 
     <p>We are going to do many dimensionality reduction techniques for the same purpose to cluster the MNIST dataset.</p> 
     <p>ä¸ºäº†ç›¸åŒçš„ç›®çš„ï¼Œæˆ‘ä»¬å°†åšè®¸å¤šé™ç»´æŠ€æœ¯æ¥èšç±»MNISTæ•°æ®é›†ã€‚</p> 
    </div> 
   </div> 
   <div> 
    <div> 
     <div> 
      <div> 
       <figure style="display:block;text-align:center;"> 
        <div> 
         <div> 
          <div> 
           <div style="text-align: center;"> 
            <img alt="Image for post" src="https://images2.imgbox.com/99/ff/dZS0qO8o_o.png" width="640" height="480" style="outline: none;"> 
           </div> 
          </div> 
         </div> 
        </div> 
       </figure> 
       <figure style="display:block;text-align:center;"> 
        <div> 
         <div> 
          <div> 
           <div style="text-align: center;"> 
            <img alt="Image for post" src="https://images2.imgbox.com/55/ee/eyBjCGXu_o.png" width="640" height="480" style="outline: none;"> 
           </div> 
          </div> 
         </div> 
        </div> 
        <figcaption>
          Left: PCA, Right: ISOMAP [scikit-learn, Manifold learning on handwritten digits] 
        </figcaption> 
        <figcaption>
          å·¦ï¼šPCAï¼Œå³ï¼šISOMAP [scikit-learnï¼Œåœ¨æ‰‹å†™æ•°å­—ä¸Šçš„æµå½¢å­¦ä¹ ] 
        </figcaption> 
       </figure> 
      </div> 
      <div> 
       <figure style="display:block;text-align:center;"> 
        <div> 
         <div> 
          <div> 
           <div style="text-align: center;"> 
            <img alt="Image for post" src="https://images2.imgbox.com/fd/66/90w3eQsW_o.png" width="640" height="480" style="outline: none;"> 
           </div> 
          </div> 
         </div> 
        </div> 
       </figure> 
       <figure style="display:block;text-align:center;"> 
        <div> 
         <div> 
          <div> 
           <div style="text-align: center;"> 
            <img alt="Image for post" src="https://images2.imgbox.com/35/38/Wmr62Ekt_o.png" width="640" height="480" style="outline: none;"> 
           </div> 
          </div> 
         </div> 
        </div> 
        <figcaption>
          Left: MDS, Right: t-SNE [scikit-learn, Manifold learning on handwritten digits] 
        </figcaption> 
        <figcaption>
          å·¦ï¼šMDSï¼Œå³ï¼št-SNE [scikit-learnï¼Œæ‰‹å†™æ•°å­—ä¸Šçš„æµå½¢å­¦ä¹ ] 
        </figcaption> 
       </figure> 
      </div> 
     </div> 
    </div> 
   </div> 
   <div> 
    <div> 
     <p>This images from <a href="https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html" rel="noopener nofollow">the sci-kit-learn official guideline</a> you can take a look more result. As you can see, t-SNE overwhelmingly perform well. It also shows the semantics of distances. The small cluster of 1, it has an underbar at the bottom, is closer to 2 than 1 because 2 has the same underbar. Now, you know why you should learn t-SNE.</p> 
     <p> è¿™å¼ æ¥è‡ª<a href="https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html" rel="noopener nofollow">sci-kit-learnå®˜æ–¹æŒ‡å—çš„å›¾ç‰‡</a>å¯ä»¥ä½¿æ‚¨çœ‹åˆ°æ›´å¤šç»“æœã€‚ å¦‚æ‚¨æ‰€è§ï¼Œt-SNEè¡¨ç°å‡ºè‰²ã€‚ å®ƒè¿˜æ˜¾ç¤ºäº†è·ç¦»çš„è¯­ä¹‰ã€‚ 1çš„å°é›†ç¾¤åœ¨åº•éƒ¨æœ‰ä¸€ä¸ªä¸‹åˆ’çº¿ï¼Œæ¯”1æ›´æ¥è¿‘2ï¼Œå› ä¸º2å…·æœ‰ç›¸åŒçš„ä¸‹åˆ’çº¿ã€‚ ç°åœ¨ï¼Œæ‚¨çŸ¥é“äº†ä¸ºä»€ä¹ˆåº”è¯¥å­¦ä¹ t-SNEã€‚</p> 
     <h2> SNE <span style="font-weight: bold;">(</span>SNE<span style="font-weight: bold;">)</span></h2> 
     <p>Stochastic Neighborhood Embedding(SNE) is the basic idea of t-SNE. We need to know SNE before we learn t-SNE. SNE defines distance-based conditional probabilities that xi (data point) would pick xj (the other data point) as its neighbor, P(xj is neighbor hood| xi). We use exponential distribution, you can think normal distribution, to calculate the probabilities.</p> 
     <p> éšæœºé‚»åŸŸåµŒå…¥(SNE)æ˜¯t-SNEçš„åŸºæœ¬æ€æƒ³ã€‚ åœ¨å­¦ä¹ t-SNEä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦äº†è§£SNEã€‚ SNEå®šä¹‰äº†åŸºäºè·ç¦»çš„æ¡ä»¶æ¦‚ç‡ï¼Œå³xi(æ•°æ®ç‚¹)å°†xj(å¦ä¸€ä¸ªæ•°æ®ç‚¹)ä½œä¸ºå…¶é‚»å±…ï¼ŒP(xjæ˜¯é‚»å±…hood | xi)ã€‚ æˆ‘ä»¬ä½¿ç”¨æŒ‡æ•°åˆ†å¸ƒï¼Œå¯ä»¥è®¤ä¸ºæ˜¯æ­£æ€åˆ†å¸ƒï¼Œå¯ä»¥è®¡ç®—å‡ºæ¦‚ç‡ã€‚</p> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/20/c7/DSdTyqfr_o.png" width="852" height="165" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        Probability calculation of SNE 
      </figcaption> 
      <figcaption>
        SNEçš„æ¦‚ç‡è®¡ç®— 
      </figcaption> 
     </figure> 
     <p>It compares the distance of the selected data points and the sum of distances of other points. It will give a high probability if the selected data points close to each other. The variance of the distribution is based on the specific data point, sparse region will give you a high variance, I will elaborate this later on this post. Now, we need to turn this information into low dimensions. We will use the KL divergence. Thus, we need to calculate the corresponding probability in low dimensions.</p> 
     <p>å®ƒæ¯”è¾ƒæ‰€é€‰æ•°æ®ç‚¹çš„è·ç¦»å’Œå…¶ä»–ç‚¹çš„è·ç¦»ä¹‹å’Œã€‚ å¦‚æœæ‰€é€‰æ•°æ®ç‚¹å½¼æ­¤é è¿‘ï¼Œåˆ™å°†å…·æœ‰å¾ˆé«˜çš„æ¦‚ç‡ã€‚ åˆ†å¸ƒçš„æ–¹å·®åŸºäºç‰¹å®šçš„æ•°æ®ç‚¹ï¼Œç¨€ç–åŒºåŸŸä¼šç»™æ‚¨å¸¦æ¥å¾ˆå¤§çš„æ–¹å·®ï¼Œæˆ‘å°†åœ¨æœ¬æ–‡çš„åé¢è¯¦ç»†è¯´æ˜ã€‚ ç°åœ¨ï¼Œæˆ‘ä»¬éœ€è¦å°†è¿™äº›ä¿¡æ¯è½¬æ¢ä¸ºä½ç»´åº¦ã€‚ æˆ‘ä»¬å°†ä½¿ç”¨KLæ•£åº¦ã€‚ å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—ä½ç»´çš„ç›¸åº”æ¦‚ç‡ã€‚</p> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div style="text-align: center;"> 
          <img alt="Image for post" src="https://images2.imgbox.com/74/e7/tNJ0Ue1E_o.png" width="700" height="189" style="outline: none;"> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        Probability in low dimensions. 
      </figcaption> 
      <figcaption>
        ä½å°ºå¯¸çš„æ¦‚ç‡ã€‚ 
      </figcaption> 
     </figure> 
     <p>The probability function in low dimensions is the same as the high dimension one. The variance is determined by yourself, it chooses how much space you use for visualization.</p> 
     <p> ä½ç»´çš„æ¦‚ç‡å‡½æ•°ä¸é«˜ç»´çš„æ¦‚ç‡å‡½æ•°ç›¸åŒã€‚ æ–¹å·®ç”±æ‚¨è‡ªå·±ç¡®å®šï¼Œå®ƒé€‰æ‹©æ‚¨ç”¨äºå¯è§†åŒ–çš„ç©ºé—´ã€‚</p> 
     <p>KL Divergence is methods to approximate two probability distribution, it makes two distribution the same. You should know KL Divergence is asymmetric. KL(P|Q) != KL(Q|P).</p> 
     <p> KLæ•£åº¦æ˜¯è¿‘ä¼¼ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„æ–¹æ³•ï¼Œå®ƒä½¿ä¸¤ä¸ªåˆ†å¸ƒç›¸åŒã€‚ æ‚¨åº”è¯¥çŸ¥é“KL Divergenceæ˜¯ä¸å¯¹ç§°çš„ã€‚ KL(P | Q)ï¼= KL(Q | P)ã€‚</p> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/fa/6c/feBRZH0t_o.png" width="996" height="175" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        Cost function 
      </figcaption> 
      <figcaption>
        æˆæœ¬å‡½æ•° 
      </figcaption> 
     </figure> 
     <p>We defined the cost function using KL divergence. Letâ€™s look at the meaning of the cost function. If the points are close to each other in high dimensional space, p will be high. Therefore, q should be smaller and smaller. If the points are far from each other in high dimensional space, p will be small. Therefore, q should be somehow higher. However, you can immediately observe the asymmetric relationship. The former case costs are expensive but the later case costs are relatively small. This result affects the behavior of SNE, it keeps neighbors close together but it doesnâ€™t care so much about widely separated points.</p> 
     <p>æˆ‘ä»¬ä½¿ç”¨KLæ•£åº¦å®šä¹‰äº†æˆæœ¬å‡½æ•°ã€‚ è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹æˆæœ¬å‡½æ•°çš„å«ä¹‰ã€‚ å¦‚æœè¿™äº›ç‚¹åœ¨é«˜ç»´ç©ºé—´ä¸­å½¼æ­¤é è¿‘ï¼Œåˆ™på°†å¾ˆé«˜ã€‚ å› æ­¤ï¼Œqåº”è¯¥è¶Šæ¥è¶Šå°ã€‚ å¦‚æœåœ¨é«˜ç»´ç©ºé—´ä¸­è¿™äº›ç‚¹å½¼æ­¤è¿œç¦»ï¼Œåˆ™på°†å¾ˆå°ã€‚ å› æ­¤ï¼Œqåº”è¯¥æ›´é«˜ã€‚ ä½†æ˜¯ï¼Œæ‚¨å¯ä»¥ç«‹å³è§‚å¯Ÿåˆ°ä¸å¯¹ç§°å…³ç³»ã€‚ å‰ä¸€ç§æƒ…å†µçš„æˆæœ¬æ˜‚è´µï¼Œè€Œåä¸€ç§æƒ…å†µçš„æˆæœ¬ç›¸å¯¹è¾ƒå°ã€‚ æ­¤ç»“æœå½±å“SNEçš„è¡Œä¸ºï¼Œä½¿é‚»å±…ä¿æŒé è¿‘ï¼Œä½†ä¸å¤ªåœ¨ä¹åˆ†æ•£çš„ç‚¹ã€‚</p> 
     <h2> é€‰æ‹©å…·æœ‰å›°æƒ‘åº¦çš„æ–¹å·®ã€‚ <span style="font-weight: bold;">(</span>Select the variance with Perplexity.<span style="font-weight: bold;">)</span></h2> 
     <p>Using a fixed ğœ does not work well when densities in high-dimensional space vary. SNE uses the desired perplexity that the user chooses instead of the fixed ğœ.</p> 
     <p> å½“é«˜ç»´ç©ºé—´ä¸­çš„å¯†åº¦å˜åŒ–æ—¶ï¼Œä½¿ç”¨å›ºå®šçš„ğœæ•ˆæœä¸ä½³ã€‚ SNEä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„æ‰€éœ€å›°æƒ‘åº¦ä»£æ›¿å›ºå®šğœã€‚</p> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/a6/fa/vByB2Bvd_o.png" width="837" height="166" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        Perplexity function with entropy. 
      </figcaption> 
      <figcaption>
        å¸¦ç†µçš„å›°æƒ‘å‡½æ•°ã€‚ 
      </figcaption> 
     </figure> 
     <p>It intuitionally reflects the effective number of neighbors. You can think this method is similar to kNN. You can choose perplexity but y<strong>ou should not select the number that is greater than the number of data points</strong>. If you think when the entropy will be the maximum value, it is when every probability has the same value. In this case, the perplexity is n!</p> 
     <p> å®ƒç›´è§‚åœ°åæ˜ äº†é‚»å±…çš„æœ‰æ•ˆæ•°é‡ã€‚ æ‚¨å¯ä»¥è®¤ä¸ºæ­¤æ–¹æ³•ç±»ä¼¼äºkNNã€‚ æ‚¨å¯ä»¥é€‰æ‹©å›°æƒ‘åº¦ï¼Œä½†æ˜¯<strong>æ‚¨ä¸åº”é€‰æ‹©å¤§äºæ•°æ®ç‚¹æ•°é‡çš„æ•°å­—</strong>ã€‚ å¦‚æœæ‚¨è®¤ä¸ºä½•æ—¶ç†µå°†æ˜¯æœ€å¤§å€¼ï¼Œé‚£å°±æ˜¯æ¯ä¸ªæ¦‚ç‡éƒ½å…·æœ‰ç›¸åŒçš„å€¼ã€‚ åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå›°æƒ‘åº¦ä¸ºnï¼</p> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/56/87/8bOKbMfD_o.png" width="989" height="640" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        Sigma changes by perplexity user choose. 
      </figcaption> 
      <figcaption>
        Sigmaä¼šå› ç”¨æˆ·é€‰æ‹©çš„å›°æƒ‘è€Œå˜åŒ–ã€‚ 
      </figcaption> 
     </figure> 
     <p>You can see how the probability of becoming neighbors is changing depending on the selected perplexity. These distributions are representing for one data point, it is zero in distribution. To sum up, the selected perplexity controls how many data points will be considered neighbors and it is calculated for each data point with respect to its probability distribution, entropy is calculated based on it.</p> 
     <p> æ‚¨å¯ä»¥çœ‹åˆ°æˆä¸ºé‚»å±…çš„å¯èƒ½æ€§å¦‚ä½•æ ¹æ®æ‰€é€‰çš„å›°æƒ‘è€Œå˜åŒ–ã€‚ è¿™äº›åˆ†å¸ƒä»£è¡¨ä¸€ä¸ªæ•°æ®ç‚¹ï¼Œåˆ†å¸ƒä¸ºé›¶ã€‚ ç»¼ä¸Šæ‰€è¿°ï¼Œæ‰€é€‰çš„å›°æƒ‘åº¦æ§åˆ¶ç€å¤šå°‘ä¸ªæ•°æ®ç‚¹å°†è¢«è§†ä¸ºé‚»å±…ï¼Œå¹¶ä¸”é’ˆå¯¹æ¯ä¸ªæ•°æ®ç‚¹é’ˆå¯¹å…¶æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œè®¡ç®—ï¼Œå¹¶æ ¹æ®å…¶è®¡ç®—ç†µã€‚</p> 
     <h2> ä¼˜åŒ– <span style="font-weight: bold;">(</span>Optimization<span style="font-weight: bold;">)</span></h2> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/83/e2/9VSS8lfi_o.png" width="898" height="151" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        Optimization Function 
      </figcaption> 
      <figcaption>
        ä¼˜åŒ–åŠŸèƒ½ 
      </figcaption> 
     </figure> 
     <ol><li>Initialize y(the data points in low dimensions) randomly<p class="nodelete"></p>éšæœºåˆå§‹åŒ–y(ä½ç»´æ•°æ®ç‚¹)</li><li>Iteratively shift y in Optimization function with learning rate<p class="nodelete"></p> å¸¦å­¦ä¹ ç‡çš„ä¼˜åŒ–å‡½æ•°ä¸­çš„yè¿­ä»£ç§»ä½ </li></ol> 
     <p>The learning rate controls how fast the optimization will be done by stressing the cost and we multiply the distance between y1 and y2 to emphasize the difference of the probabilities of each distribution.</p> 
     <p>å­¦ä¹ é€Ÿç‡é€šè¿‡å¼ºè°ƒæˆæœ¬æ¥æ§åˆ¶ä¼˜åŒ–çš„å®Œæˆé€Ÿåº¦ï¼Œæˆ‘ä»¬ä¹˜ä»¥y1å’Œy2ä¹‹é—´çš„è·ç¦»ä»¥å¼ºè°ƒæ¯ç§åˆ†å¸ƒçš„æ¦‚ç‡ä¹‹å·®ã€‚</p> 
     <p>In practice, we use other tricks to boost the speed or correctness of algorithms.</p> 
     <p> åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å…¶ä»–æŠ€å·§æ¥æé«˜ç®—æ³•çš„é€Ÿåº¦æˆ–æ­£ç¡®æ€§ã€‚</p> 
     <ul><li>Decreasing the learning rate with time.<p class="nodelete"></p> éšç€æ—¶é—´çš„æµé€é™ä½å­¦ä¹ é€Ÿåº¦ã€‚ </li><li>Randomly perturbing points to avoid local suboptima.<p class="nodelete"></p> éšæœºæ‰°åŠ¨ç‚¹ä»¥é¿å…å±€éƒ¨æ¬¡ä¼˜ã€‚ </li><li>Including a momentum term that continues to drive points in a similar direction as was taken previously.<p class="nodelete"></p> åŒ…æ‹¬ä¸€ä¸ªåŠ¨é‡é¡¹ï¼Œè¯¥åŠ¨é‡é¡¹ç»§ç»­é©±åŠ¨ç‚¹æœç€ä¸ä»¥å‰ç›¸ä¼¼çš„æ–¹å‘ç§»åŠ¨ã€‚ </li><li>Early compression and early exaggeration<p class="nodelete"></p> æ—©æœŸå‹ç¼©å’Œæ—©æœŸå¤¸å¼  </li></ul> 
     <p>These methods are also widely used in deep learning. I believe you familiar with them.</p> 
     <p>è¿™äº›æ–¹æ³•ä¹Ÿå¹¿æ³›ç”¨äºæ·±åº¦å­¦ä¹ ã€‚ æˆ‘ç›¸ä¿¡æ‚¨ç†Ÿæ‚‰å®ƒä»¬ã€‚</p> 
     <h2> æ‹¥æŒ¤çš„é—®é¢˜ <span style="font-weight: bold;">(</span>The Crowding Problem<span style="font-weight: bold;">)</span></h2> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div style="text-align: center;"> 
          <img alt="Image for post" src="https://images2.imgbox.com/54/35/H0B0I758_o.png" width="500" height="272" style="outline: none;"> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        Swiss roll, do you like the roll cake? 
      </figcaption> 
      <figcaption>
        ç‘å£«å·ï¼Œä½ å–œæ¬¢å·è›‹ç³•å—ï¼Ÿ 
      </figcaption> 
     </figure> 
     <p><a rel="nofollow" href="https://medium.com/@jeheonpark93/vc-isomap-manifolds-learning-965e758316eb">ISOMAP</a> solved the swiss roll problem perfectly because it is intrinsically two-dimensional. However, what if the intrinsic dimensionality is not 2D and it is greater than 2D. Think of a sphere in 3D and try to project down all points into 2D. Many points will be collapsed together. This problem occurs in SNE too. t-SNE is developed to solve this problem.</p> 
     <p> <a rel="nofollow" href="https://medium.com/@jeheonpark93/vc-isomap-manifolds-learning-965e758316eb">ISOMAP</a>å®Œç¾åœ°è§£å†³äº†ç‘å£«å·é—®é¢˜ï¼Œå› ä¸ºå®ƒæœ¬è´¨ä¸Šæ˜¯äºŒç»´çš„ã€‚ ä½†æ˜¯ï¼Œå¦‚æœå›ºæœ‰å°ºå¯¸ä¸æ˜¯2Då¹¶ä¸”å¤§äº2Dï¼Œè¯¥æ€ä¹ˆåŠã€‚ è€ƒè™‘ä¸€ä¸‹3Dçƒä½“ï¼Œç„¶åå°è¯•å°†æ‰€æœ‰ç‚¹æŠ•å½±åˆ°2Dä¸­ã€‚ è®¸å¤šç‚¹å°†ä¸€èµ·å´©æºƒã€‚ SNEä¸­ä¹Ÿä¼šå‘ç”Ÿæ­¤é—®é¢˜ã€‚ å¼€å‘t-SNEå°±æ˜¯ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</p> 
     <h2> å¨ä½ <span style="font-weight: bold;">(</span>t-SNE<span style="font-weight: bold;">)</span></h2> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/8d/9e/V2FyuK4I_o.png" width="1200" height="960" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        t-distribution used for distribution of t-SNE 
      </figcaption> 
      <figcaption>
        tåˆ†å¸ƒç”¨äºt-SNEçš„åˆ†å¸ƒ 
      </figcaption> 
     </figure> 
     <p>The problem occurs because SNE makes the area indicating neighbors too small. t-SNE changes the distribution in high dimensions and low dimensions to t-distribution. Heavy tails allow points to move further apart in low dimensions compared to high dimensions. t-distribution is a well-known distribution for heavy tails.</p> 
     <p>å‘ç”Ÿé—®é¢˜æ˜¯å› ä¸ºSNEä½¿æŒ‡ç¤ºé‚»å±…çš„åŒºåŸŸå¤ªå°ã€‚ t-SNEå°†é«˜å°ºå¯¸å’Œä½å°ºå¯¸çš„åˆ†å¸ƒæ›´æ”¹ä¸ºtåˆ†å¸ƒã€‚ ä¸é«˜å°ºå¯¸çš„å°¾å·´ç›¸æ¯”ï¼Œæ²‰é‡çš„å°¾å·´ä½¿ç‚¹åœ¨ä½å°ºå¯¸çš„åœ°æ–¹å¯ä»¥è¿›ä¸€æ­¥åˆ†å¼€ã€‚ tåˆ†å¸ƒæ˜¯ä¼—æ‰€å‘¨çŸ¥çš„é‡å°¾å·´åˆ†å¸ƒã€‚</p> 
     <h2> å¯¹ç§°æˆæœ¬å‡½æ•° <span style="font-weight: bold;">(</span>Symmetric Cost Function<span style="font-weight: bold;">)</span></h2> 
     <p>SNE has an asymmetric cost function but t-SNE changes it to a symmetric cost function.</p> 
     <p>SNEå…·æœ‰ä¸å¯¹ç§°æˆæœ¬å‡½æ•°ï¼Œä½†t-SNEå°†å…¶æ›´æ”¹ä¸ºå¯¹ç§°æˆæœ¬å‡½æ•°ã€‚</p> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/99/50/9kQBdzVm_o.png" width="999" height="337" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        Symmetric Cost Function 
      </figcaption> 
      <figcaption>
        å¯¹ç§°æˆæœ¬å‡½æ•° 
      </figcaption> 
     </figure> 
     <p>It changes p and q to unconditional probabilities, this is why it is symmetric. p is normalized by n. q is also changed. q does not use exponential terms. Thus, its computation is more convenient.</p> 
     <p>å®ƒå°†på’Œqæ›´æ”¹ä¸ºæ— æ¡ä»¶æ¦‚ç‡ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆå®ƒæ˜¯å¯¹ç§°çš„ã€‚ pç”±nå½’ä¸€åŒ–ã€‚ qä¹Ÿæ”¹å˜äº†ã€‚ qä¸ä½¿ç”¨æŒ‡æ•°é¡¹ã€‚ å› æ­¤ï¼Œå…¶è®¡ç®—æ›´åŠ æ–¹ä¾¿ã€‚</p> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/44/19/0vDP06HT_o.png" width="1030" height="152" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        New optimization function 
      </figcaption> 
      <figcaption>
        æ–°çš„ä¼˜åŒ–åŠŸèƒ½ 
      </figcaption> 
     </figure> 
     <p>The optimization function is also changed because the cost function was changed.</p> 
     <p>ç”±äºæˆæœ¬å‡½æ•°å·²æ›´æ”¹ï¼Œå› æ­¤ä¼˜åŒ–åŠŸèƒ½ä¹Ÿå·²æ›´æ”¹ã€‚</p> 
     <h2> <strong>è®¡ç®—æˆæœ¬</strong> <span style="font-weight: bold;">(</span><strong>Computational Cost</strong><span style="font-weight: bold;">)</span></h2> 
     <p>If you apply this naive application, it requires Î˜(n^2) effort. Can we improve this? <a href="https://www.jmlr.org/papers/volume15/vandermaaten14a/vandermaaten14a.pdf" rel="noopener nofollow">[van der Maaten 2014]</a>, this paper will give you an answer.</p> 
     <p>å¦‚æœæ‚¨åº”ç”¨æ­¤ç®€å•åº”ç”¨ç¨‹åºï¼Œåˆ™éœ€è¦Î˜(n ^ 2)çš„åŠªåŠ›ã€‚ æˆ‘ä»¬å¯ä»¥æ”¹å–„å—ï¼Ÿ <a href="https://www.jmlr.org/papers/volume15/vandermaaten14a/vandermaaten14a.pdf" rel="noopener nofollow">[van der Maaten 2014]</a> ï¼Œæœ¬æ–‡å°†ä¸ºæ‚¨æä¾›ç­”æ¡ˆã€‚</p> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div style="text-align: center;"> 
          <img alt="Image for post" src="https://images2.imgbox.com/4b/9e/FvMof1yo_o.png" width="372" height="375" style="outline: none;"> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        Make grids and cluster the points for computation. 
      </figcaption> 
      <figcaption>
        åˆ¶ä½œç½‘æ ¼å¹¶èšé›†ç‚¹ä»¥è¿›è¡Œè®¡ç®—ã€‚ 
      </figcaption> 
     </figure> 
     <p>The strategy is:</p> 
     <p> è¯¥ç­–ç•¥æ˜¯ï¼š</p> 
     <ul><li>Only compute p, high dimensions, for a fixed number of nearest neighbors, we choose the perplexity and we multiply 3. The data point in the range of 3*perplexity will be calculated.<p class="nodelete"></p> å¯¹äºå›ºå®šæ•°é‡çš„æœ€è¿‘é‚»å±…ï¼Œä»…è®¡ç®—pç»´ï¼Œå³é«˜ç»´ï¼Œæˆ‘ä»¬é€‰æ‹©å›°æƒ‘åº¦å¹¶ä¹˜ä»¥3ã€‚å°†è®¡ç®—3 *å›°æƒ‘åº¦èŒƒå›´å†…çš„æ•°æ®ç‚¹ã€‚ </li><li>Other remote clusters of points are approximated with a single value.<p class="nodelete"></p> å…¶ä»–ç‚¹çš„è¿œç¨‹ç¾¤é›†ç”¨å•ä¸ªå€¼è¿‘ä¼¼ã€‚ </li><li>For computing q, they put embedding points into the spatial acceleration data structure. It helps to identify the groups. It only calculates the distance between the virtual points and the target point. It gives them the weight of the number of data points.<p class="nodelete"></p> ä¸ºäº†è®¡ç®—qï¼Œä»–ä»¬å°†åµŒå…¥ç‚¹æ”¾å…¥ç©ºé—´åŠ é€Ÿåº¦æ•°æ®ç»“æ„ä¸­ã€‚ å®ƒæœ‰åŠ©äºè¯†åˆ«ç»„ã€‚ å®ƒä»…è®¡ç®—è™šæ‹Ÿç‚¹å’Œç›®æ ‡ç‚¹ä¹‹é—´çš„è·ç¦»ã€‚ å®ƒèµ‹äºˆå®ƒä»¬æ•°æ®ç‚¹æ•°é‡çš„æƒé‡ã€‚ </li></ul> 
     <h2> ç”Ÿå‘½ç§‘å­¦çš„ä¾‹å­ <span style="font-weight: bold;">(</span>Example in Life Science<span style="font-weight: bold;">)</span></h2> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div style="text-align: center;"> 
          <img alt="Image for post" src="https://images2.imgbox.com/fa/f7/cHWKlWxj_o.png" width="651" height="436" style="outline: none;"> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        [Amir et al., Nature Biotechnology 2013] 
      </figcaption> 
      <figcaption>
        [Amirç­‰ï¼Œè‡ªç„¶ç”Ÿç‰©æŠ€æœ¯ï¼Œ2013å¹´] 
      </figcaption> 
     </figure> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/6a/11/3HwgMpHy_o.png" width="1050" height="452" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </div> 
      </div> 
     </figure> 
     <p>They use to distinguish the cell types by high dimensional protein expression profiles. They find that some proteins, it was considered important, are not that important to distinguish cell types by subtracting it from t-SNE. The shape of the t-SNE still remains similar.</p> 
     <p> ä»–ä»¬é€šè¿‡é«˜ç»´è›‹ç™½è´¨è¡¨è¾¾è°±æ¥åŒºåˆ†ç»†èƒç±»å‹ã€‚ ä»–ä»¬å‘ç°ä¸€äº›è›‹ç™½è´¨ï¼Œè¢«è®¤ä¸ºå¾ˆé‡è¦ï¼Œé€šè¿‡ä»t-SNEä¸­å‡å»æ¥åŒºåˆ†ç»†èƒç±»å‹å¹¶ä¸é‚£ä¹ˆé‡è¦ã€‚ t-SNEçš„å½¢çŠ¶ä»ç„¶ä¿æŒç›¸ä¼¼ã€‚</p> 
     <h2> è­¦å‘Šï¼ <span style="font-weight: bold;">(</span>Warning!<span style="font-weight: bold;">)</span></h2> 
     <p><a href="https://distill.pub/2016/misread-tsne/" rel="noopener nofollow">This site</a> helps you to understand t-SNE with practical experiments.</p> 
     <p> <a href="https://distill.pub/2016/misread-tsne/" rel="noopener nofollow">è¯¥ç«™ç‚¹</a>å¯é€šè¿‡å®é™…å®éªŒå¸®åŠ©æ‚¨äº†è§£t-SNEã€‚</p> 
    </div> 
   </div> 
   <div> 
    <div> 
     <div> 
      <div> 
       <figure style="display:block;text-align:center;"> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/a5/b0/edXOmMtB_o.png" width="267" height="308" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </figure> 
       <figure style="display:block;text-align:center;"> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/f9/d9/gCb2hZLQ_o.png" width="540" height="305" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
        <figcaption>
          It does not preserve densities and cluster spreads. 
        </figcaption> 
        <figcaption>
          å®ƒä¸ä¿ç•™å¯†åº¦å’Œç°‡åˆ†å¸ƒã€‚ 
        </figcaption> 
       </figure> 
      </div> 
     </div> 
    </div> 
   </div> 
   <div> 
    <div> 
     <p>This is an obvious picture because t-SNE only care about neighbors. Therefore, the densities and spreads of clusters of original data are not preserved.</p> 
     <p> è¿™æ˜¯æ˜¾è€Œæ˜“è§çš„æƒ…å†µï¼Œå› ä¸ºt-SNEåªå…³å¿ƒé‚»å±…ã€‚ å› æ­¤ï¼ŒåŸå§‹æ•°æ®çš„ç¾¤é›†çš„å¯†åº¦å’Œæ•£å¸ƒä¸ä¿ç•™ã€‚</p> 
    </div> 
   </div> 
   <div> 
    <div> 
     <div> 
      <div> 
       <figure style="display:block;text-align:center;"> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/08/59/TirFyrmF_o.png" width="270" height="309" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </figure> 
       <figure style="display:block;text-align:center;"> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/e5/76/cSpHv0A3_o.png" width="540" height="303" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
        <figcaption>
          the distance can be different 
        </figcaption> 
        <figcaption>
          è·ç¦»å¯ä»¥ä¸åŒ 
        </figcaption> 
       </figure> 
      </div> 
     </div> 
    </div> 
   </div> 
   <div> 
    <div> 
     <p>Distance is affected by perplexity. Low perplexity means low sigma. Thus, a smaller region will be considered as neighborhood region. If perplexity is big enough to see other clusters, then it can consider the distances.</p> 
     <p>è·ç¦»å—å›°æƒ‘çš„å½±å“ã€‚ ä½å›°æƒ‘åº¦æ„å‘³ç€ä½sigmaã€‚ å› æ­¤ï¼Œè¾ƒå°çš„åŒºåŸŸå°†è¢«è§†ä¸ºé‚»è¿‘åŒºåŸŸã€‚ å¦‚æœå›°æƒ‘åº¦è¶³å¤Ÿå¤§ï¼Œå¯ä»¥çœ‹åˆ°å…¶ä»–ç¾¤é›†ï¼Œåˆ™å¯ä»¥è€ƒè™‘è·ç¦»ã€‚</p> 
    </div> 
   </div> 
   <div> 
    <div> 
     <div> 
      <div> 
       <figure style="display:block;text-align:center;"> 
        <div> 
         <div> 
          <div> 
           <div style="text-align: center;"> 
            <img alt="Image for post" src="https://images2.imgbox.com/4d/f4/QrOEJlum_o.png" width="570" height="348" style="outline: none;"> 
           </div> 
          </div> 
         </div> 
        </div> 
       </figure> 
       <figure style="display:block;text-align:center;"> 
        <div> 
         <div> 
          <div> 
           <div style="text-align: center;"> 
            <img alt="Image for post" src="https://images2.imgbox.com/13/69/JGbSkj7j_o.png" width="544" height="348" style="outline: none;"> 
           </div> 
          </div> 
         </div> 
        </div> 
        <figcaption>
          You should know cluster in t-SNE can be a hallucination. 
        </figcaption> 
        <figcaption>
          æ‚¨åº”è¯¥çŸ¥é“ï¼Œt-SNEä¸­çš„ç°‡å¯èƒ½æ˜¯å¹»è§‰ã€‚ 
        </figcaption> 
       </figure> 
      </div> 
     </div> 
    </div> 
   </div> 
   <div> 
    <div> 
     <p>Perplexity 2 case shows many clusters but it is not actual clusters in the original data. It exaggerates the randomness and it makes a hallucination.</p> 
     <p> å›°æƒ‘2çš„æƒ…å†µæ˜¾ç¤ºäº†è®¸å¤šèšç±»ï¼Œä½†åœ¨åŸå§‹æ•°æ®ä¸­ä¸æ˜¯å®é™…çš„èšç±»ã€‚ å®ƒå¤¸å¤§äº†éšæœºæ€§ï¼Œå¹¶äº§ç”Ÿäº†å¹»è§‰ã€‚</p> 
     <p><strong>Please keep them in mind these tips when you use t-SNE.</strong></p> 
     <p> <strong>ä½¿ç”¨t-SNEæ—¶ï¼Œè¯·è®°ä½è¿™äº›æç¤ºã€‚</strong></p> 
     <p>This post is published on 9/10/2020</p> 
     <p> æ­¤å¸–å‘å¸ƒäº9/10/2020</p> 
    </div> 
   </div> 
  </section> 
 </div> 
 <blockquote> 
  <p>ç¿»è¯‘è‡ª: <a href="https://medium.com/@jeheonpark93/vc-sne-and-t-sne-who-is-my-neighbor-34e738bf9e71" rel="nofollow">https://medium.com/@jeheonpark93/vc-sne-and-t-sne-who-is-my-neighbor-34e738bf9e71</a></p> 
 </blockquote> 
</article>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/04e4c2d220c79c9455100cdd686b015d/" rel="prev">
			<span class="pager__subtitle">Â«&thinsp;Previous</span>
			<p class="pager__title">Webå‰ç«¯é¢è¯•é¢˜-åº”ç”¨CSSæ ·å¼</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/76a7b4d3da9fc5058d566dedeb82f22a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;Â»</span>
			<p class="pager__title">githubè¯·æ±‚è¶…æ—¶_ä½¿ç”¨å±é™©jså’Œgithubæ“ä½œè‡ªåŠ¨æ‰§è¡Œå¸¸è§çš„è¯·æ±‚è¯·æ±‚åé¦ˆ</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 èœé¸Ÿç¨‹åºå‘˜åšå®¢.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>