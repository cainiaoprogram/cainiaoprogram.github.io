<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>vc sne和t sne是我的邻居 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="vc sne和t sne是我的邻居" />
<meta property="og:description" content="Many dimensionality reduction techniques attempt to preserve distances of the original data. However, it can be beneficial to focus on preserving the nearest neighbours for visualization. t-SNE[van der Maaten/Hinton 2008] abstracts away density and distance information. Since it preserves the neighbours, it often reveals the cluster structure more clearly than any other dimensionality reduction technique. t-SNE is really popular in many applications including life science.
许多降维技术试图保留原始数据的距离。 但是，专注于保留最近的邻居以进行可视化可能是有益的。 t-SNE [van der Maaten / Hinton 2008]提取了密度和距离信息。 由于它保留了邻居，因此与其他降维技术相比，它通常更清楚地揭示出簇结构。 t-SNE在包括生命科学在内的许多应用中确实很受欢迎。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/d6ce994726500030e3d8db440c06701b/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-10-13T23:47:28+08:00" />
<meta property="article:modified_time" content="2020-10-13T23:47:28+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">vc sne和t sne是我的邻居</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <article style="font-size: 16px;"> 
 <div> 
  <section> 
   <div> 
    <div> 
     <p>Many dimensionality reduction techniques attempt to preserve distances of the original data. However, it can be beneficial to focus on preserving the nearest neighbours for visualization. t-SNE[van der Maaten/Hinton 2008] abstracts away density and distance information. Since it preserves the neighbours, it often reveals the cluster structure more clearly than any other dimensionality reduction technique. t-SNE is really popular in many applications including life science.</p> 
     <p>许多降维技术试图保留原始数据的距离。 但是，专注于保留最近的邻居以进行可视化可能是有益的。 t-SNE [van der Maaten / Hinton 2008]提取了密度和距离信息。 由于它保留了邻居，因此与其他降维技术相比，它通常更清楚地揭示出簇结构。 t-SNE在包括生命科学在内的许多应用中确实很受欢迎。</p> 
     <h2> 比较方式 <span style="font-weight: bold;">(</span>Comparison<span style="font-weight: bold;">)</span></h2> 
     <p>We are going to do many dimensionality reduction techniques for the same purpose to cluster the MNIST dataset.</p> 
     <p>为了相同的目的，我们将做许多降维技术来聚类MNIST数据集。</p> 
    </div> 
   </div> 
   <div> 
    <div> 
     <div> 
      <div> 
       <figure style="display:block;text-align:center;"> 
        <div> 
         <div> 
          <div> 
           <div style="text-align: center;"> 
            <img alt="Image for post" src="https://images2.imgbox.com/99/ff/dZS0qO8o_o.png" width="640" height="480" style="outline: none;"> 
           </div> 
          </div> 
         </div> 
        </div> 
       </figure> 
       <figure style="display:block;text-align:center;"> 
        <div> 
         <div> 
          <div> 
           <div style="text-align: center;"> 
            <img alt="Image for post" src="https://images2.imgbox.com/55/ee/eyBjCGXu_o.png" width="640" height="480" style="outline: none;"> 
           </div> 
          </div> 
         </div> 
        </div> 
        <figcaption>
          Left: PCA, Right: ISOMAP [scikit-learn, Manifold learning on handwritten digits] 
        </figcaption> 
        <figcaption>
          左：PCA，右：ISOMAP [scikit-learn，在手写数字上的流形学习] 
        </figcaption> 
       </figure> 
      </div> 
      <div> 
       <figure style="display:block;text-align:center;"> 
        <div> 
         <div> 
          <div> 
           <div style="text-align: center;"> 
            <img alt="Image for post" src="https://images2.imgbox.com/fd/66/90w3eQsW_o.png" width="640" height="480" style="outline: none;"> 
           </div> 
          </div> 
         </div> 
        </div> 
       </figure> 
       <figure style="display:block;text-align:center;"> 
        <div> 
         <div> 
          <div> 
           <div style="text-align: center;"> 
            <img alt="Image for post" src="https://images2.imgbox.com/35/38/Wmr62Ekt_o.png" width="640" height="480" style="outline: none;"> 
           </div> 
          </div> 
         </div> 
        </div> 
        <figcaption>
          Left: MDS, Right: t-SNE [scikit-learn, Manifold learning on handwritten digits] 
        </figcaption> 
        <figcaption>
          左：MDS，右：t-SNE [scikit-learn，手写数字上的流形学习] 
        </figcaption> 
       </figure> 
      </div> 
     </div> 
    </div> 
   </div> 
   <div> 
    <div> 
     <p>This images from <a href="https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html" rel="noopener nofollow">the sci-kit-learn official guideline</a> you can take a look more result. As you can see, t-SNE overwhelmingly perform well. It also shows the semantics of distances. The small cluster of 1, it has an underbar at the bottom, is closer to 2 than 1 because 2 has the same underbar. Now, you know why you should learn t-SNE.</p> 
     <p> 这张来自<a href="https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html" rel="noopener nofollow">sci-kit-learn官方指南的图片</a>可以使您看到更多结果。 如您所见，t-SNE表现出色。 它还显示了距离的语义。 1的小集群在底部有一个下划线，比1更接近2，因为2具有相同的下划线。 现在，您知道了为什么应该学习t-SNE。</p> 
     <h2> SNE <span style="font-weight: bold;">(</span>SNE<span style="font-weight: bold;">)</span></h2> 
     <p>Stochastic Neighborhood Embedding(SNE) is the basic idea of t-SNE. We need to know SNE before we learn t-SNE. SNE defines distance-based conditional probabilities that xi (data point) would pick xj (the other data point) as its neighbor, P(xj is neighbor hood| xi). We use exponential distribution, you can think normal distribution, to calculate the probabilities.</p> 
     <p> 随机邻域嵌入(SNE)是t-SNE的基本思想。 在学习t-SNE之前，我们需要了解SNE。 SNE定义了基于距离的条件概率，即xi(数据点)将xj(另一个数据点)作为其邻居，P(xj是邻居hood | xi)。 我们使用指数分布，可以认为是正态分布，可以计算出概率。</p> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/20/c7/DSdTyqfr_o.png" width="852" height="165" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        Probability calculation of SNE 
      </figcaption> 
      <figcaption>
        SNE的概率计算 
      </figcaption> 
     </figure> 
     <p>It compares the distance of the selected data points and the sum of distances of other points. It will give a high probability if the selected data points close to each other. The variance of the distribution is based on the specific data point, sparse region will give you a high variance, I will elaborate this later on this post. Now, we need to turn this information into low dimensions. We will use the KL divergence. Thus, we need to calculate the corresponding probability in low dimensions.</p> 
     <p>它比较所选数据点的距离和其他点的距离之和。 如果所选数据点彼此靠近，则将具有很高的概率。 分布的方差基于特定的数据点，稀疏区域会给您带来很大的方差，我将在本文的后面详细说明。 现在，我们需要将这些信息转换为低维度。 我们将使用KL散度。 因此，我们需要计算低维的相应概率。</p> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div style="text-align: center;"> 
          <img alt="Image for post" src="https://images2.imgbox.com/74/e7/tNJ0Ue1E_o.png" width="700" height="189" style="outline: none;"> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        Probability in low dimensions. 
      </figcaption> 
      <figcaption>
        低尺寸的概率。 
      </figcaption> 
     </figure> 
     <p>The probability function in low dimensions is the same as the high dimension one. The variance is determined by yourself, it chooses how much space you use for visualization.</p> 
     <p> 低维的概率函数与高维的概率函数相同。 方差由您自己确定，它选择您用于可视化的空间。</p> 
     <p>KL Divergence is methods to approximate two probability distribution, it makes two distribution the same. You should know KL Divergence is asymmetric. KL(P|Q) != KL(Q|P).</p> 
     <p> KL散度是近似两个概率分布的方法，它使两个分布相同。 您应该知道KL Divergence是不对称的。 KL(P | Q)！= KL(Q | P)。</p> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/fa/6c/feBRZH0t_o.png" width="996" height="175" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        Cost function 
      </figcaption> 
      <figcaption>
        成本函数 
      </figcaption> 
     </figure> 
     <p>We defined the cost function using KL divergence. Let’s look at the meaning of the cost function. If the points are close to each other in high dimensional space, p will be high. Therefore, q should be smaller and smaller. If the points are far from each other in high dimensional space, p will be small. Therefore, q should be somehow higher. However, you can immediately observe the asymmetric relationship. The former case costs are expensive but the later case costs are relatively small. This result affects the behavior of SNE, it keeps neighbors close together but it doesn’t care so much about widely separated points.</p> 
     <p>我们使用KL散度定义了成本函数。 让我们看一下成本函数的含义。 如果这些点在高维空间中彼此靠近，则p将很高。 因此，q应该越来越小。 如果在高维空间中这些点彼此远离，则p将很小。 因此，q应该更高。 但是，您可以立即观察到不对称关系。 前一种情况的成本昂贵，而后一种情况的成本相对较小。 此结果影响SNE的行为，使邻居保持靠近，但不太在乎分散的点。</p> 
     <h2> 选择具有困惑度的方差。 <span style="font-weight: bold;">(</span>Select the variance with Perplexity.<span style="font-weight: bold;">)</span></h2> 
     <p>Using a fixed 𝜎 does not work well when densities in high-dimensional space vary. SNE uses the desired perplexity that the user chooses instead of the fixed 𝜎.</p> 
     <p> 当高维空间中的密度变化时，使用固定的𝜎效果不佳。 SNE使用用户选择的所需困惑度代替固定𝜎。</p> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/a6/fa/vByB2Bvd_o.png" width="837" height="166" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        Perplexity function with entropy. 
      </figcaption> 
      <figcaption>
        带熵的困惑函数。 
      </figcaption> 
     </figure> 
     <p>It intuitionally reflects the effective number of neighbors. You can think this method is similar to kNN. You can choose perplexity but y<strong>ou should not select the number that is greater than the number of data points</strong>. If you think when the entropy will be the maximum value, it is when every probability has the same value. In this case, the perplexity is n!</p> 
     <p> 它直观地反映了邻居的有效数量。 您可以认为此方法类似于kNN。 您可以选择困惑度，但是<strong>您不应选择大于数据点数量的数字</strong>。 如果您认为何时熵将是最大值，那就是每个概率都具有相同的值。 在这种情况下，困惑度为n！</p> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/56/87/8bOKbMfD_o.png" width="989" height="640" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        Sigma changes by perplexity user choose. 
      </figcaption> 
      <figcaption>
        Sigma会因用户选择的困惑而变化。 
      </figcaption> 
     </figure> 
     <p>You can see how the probability of becoming neighbors is changing depending on the selected perplexity. These distributions are representing for one data point, it is zero in distribution. To sum up, the selected perplexity controls how many data points will be considered neighbors and it is calculated for each data point with respect to its probability distribution, entropy is calculated based on it.</p> 
     <p> 您可以看到成为邻居的可能性如何根据所选的困惑而变化。 这些分布代表一个数据点，分布为零。 综上所述，所选的困惑度控制着多少个数据点将被视为邻居，并且针对每个数据点针对其概率分布进行计算，并根据其计算熵。</p> 
     <h2> 优化 <span style="font-weight: bold;">(</span>Optimization<span style="font-weight: bold;">)</span></h2> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/83/e2/9VSS8lfi_o.png" width="898" height="151" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        Optimization Function 
      </figcaption> 
      <figcaption>
        优化功能 
      </figcaption> 
     </figure> 
     <ol><li>Initialize y(the data points in low dimensions) randomly<p class="nodelete"></p>随机初始化y(低维数据点)</li><li>Iteratively shift y in Optimization function with learning rate<p class="nodelete"></p> 带学习率的优化函数中的y迭代移位 </li></ol> 
     <p>The learning rate controls how fast the optimization will be done by stressing the cost and we multiply the distance between y1 and y2 to emphasize the difference of the probabilities of each distribution.</p> 
     <p>学习速率通过强调成本来控制优化的完成速度，我们乘以y1和y2之间的距离以强调每种分布的概率之差。</p> 
     <p>In practice, we use other tricks to boost the speed or correctness of algorithms.</p> 
     <p> 在实践中，我们使用其他技巧来提高算法的速度或正确性。</p> 
     <ul><li>Decreasing the learning rate with time.<p class="nodelete"></p> 随着时间的流逝降低学习速度。 </li><li>Randomly perturbing points to avoid local suboptima.<p class="nodelete"></p> 随机扰动点以避免局部次优。 </li><li>Including a momentum term that continues to drive points in a similar direction as was taken previously.<p class="nodelete"></p> 包括一个动量项，该动量项继续驱动点朝着与以前相似的方向移动。 </li><li>Early compression and early exaggeration<p class="nodelete"></p> 早期压缩和早期夸张 </li></ul> 
     <p>These methods are also widely used in deep learning. I believe you familiar with them.</p> 
     <p>这些方法也广泛用于深度学习。 我相信您熟悉它们。</p> 
     <h2> 拥挤的问题 <span style="font-weight: bold;">(</span>The Crowding Problem<span style="font-weight: bold;">)</span></h2> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div style="text-align: center;"> 
          <img alt="Image for post" src="https://images2.imgbox.com/54/35/H0B0I758_o.png" width="500" height="272" style="outline: none;"> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        Swiss roll, do you like the roll cake? 
      </figcaption> 
      <figcaption>
        瑞士卷，你喜欢卷蛋糕吗？ 
      </figcaption> 
     </figure> 
     <p><a rel="nofollow" href="https://medium.com/@jeheonpark93/vc-isomap-manifolds-learning-965e758316eb">ISOMAP</a> solved the swiss roll problem perfectly because it is intrinsically two-dimensional. However, what if the intrinsic dimensionality is not 2D and it is greater than 2D. Think of a sphere in 3D and try to project down all points into 2D. Many points will be collapsed together. This problem occurs in SNE too. t-SNE is developed to solve this problem.</p> 
     <p> <a rel="nofollow" href="https://medium.com/@jeheonpark93/vc-isomap-manifolds-learning-965e758316eb">ISOMAP</a>完美地解决了瑞士卷问题，因为它本质上是二维的。 但是，如果固有尺寸不是2D并且大于2D，该怎么办。 考虑一下3D球体，然后尝试将所有点投影到2D中。 许多点将一起崩溃。 SNE中也会发生此问题。 开发t-SNE就是为了解决这个问题。</p> 
     <h2> 吨位 <span style="font-weight: bold;">(</span>t-SNE<span style="font-weight: bold;">)</span></h2> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/8d/9e/V2FyuK4I_o.png" width="1200" height="960" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        t-distribution used for distribution of t-SNE 
      </figcaption> 
      <figcaption>
        t分布用于t-SNE的分布 
      </figcaption> 
     </figure> 
     <p>The problem occurs because SNE makes the area indicating neighbors too small. t-SNE changes the distribution in high dimensions and low dimensions to t-distribution. Heavy tails allow points to move further apart in low dimensions compared to high dimensions. t-distribution is a well-known distribution for heavy tails.</p> 
     <p>发生问题是因为SNE使指示邻居的区域太小。 t-SNE将高尺寸和低尺寸的分布更改为t分布。 与高尺寸的尾巴相比，沉重的尾巴使点在低尺寸的地方可以进一步分开。 t分布是众所周知的重尾巴分布。</p> 
     <h2> 对称成本函数 <span style="font-weight: bold;">(</span>Symmetric Cost Function<span style="font-weight: bold;">)</span></h2> 
     <p>SNE has an asymmetric cost function but t-SNE changes it to a symmetric cost function.</p> 
     <p>SNE具有不对称成本函数，但t-SNE将其更改为对称成本函数。</p> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/99/50/9kQBdzVm_o.png" width="999" height="337" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        Symmetric Cost Function 
      </figcaption> 
      <figcaption>
        对称成本函数 
      </figcaption> 
     </figure> 
     <p>It changes p and q to unconditional probabilities, this is why it is symmetric. p is normalized by n. q is also changed. q does not use exponential terms. Thus, its computation is more convenient.</p> 
     <p>它将p和q更改为无条件概率，这就是为什么它是对称的。 p由n归一化。 q也改变了。 q不使用指数项。 因此，其计算更加方便。</p> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/44/19/0vDP06HT_o.png" width="1030" height="152" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        New optimization function 
      </figcaption> 
      <figcaption>
        新的优化功能 
      </figcaption> 
     </figure> 
     <p>The optimization function is also changed because the cost function was changed.</p> 
     <p>由于成本函数已更改，因此优化功能也已更改。</p> 
     <h2> <strong>计算成本</strong> <span style="font-weight: bold;">(</span><strong>Computational Cost</strong><span style="font-weight: bold;">)</span></h2> 
     <p>If you apply this naive application, it requires Θ(n^2) effort. Can we improve this? <a href="https://www.jmlr.org/papers/volume15/vandermaaten14a/vandermaaten14a.pdf" rel="noopener nofollow">[van der Maaten 2014]</a>, this paper will give you an answer.</p> 
     <p>如果您应用此简单应用程序，则需要Θ(n ^ 2)的努力。 我们可以改善吗？ <a href="https://www.jmlr.org/papers/volume15/vandermaaten14a/vandermaaten14a.pdf" rel="noopener nofollow">[van der Maaten 2014]</a> ，本文将为您提供答案。</p> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div style="text-align: center;"> 
          <img alt="Image for post" src="https://images2.imgbox.com/4b/9e/FvMof1yo_o.png" width="372" height="375" style="outline: none;"> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        Make grids and cluster the points for computation. 
      </figcaption> 
      <figcaption>
        制作网格并聚集点以进行计算。 
      </figcaption> 
     </figure> 
     <p>The strategy is:</p> 
     <p> 该策略是：</p> 
     <ul><li>Only compute p, high dimensions, for a fixed number of nearest neighbors, we choose the perplexity and we multiply 3. The data point in the range of 3*perplexity will be calculated.<p class="nodelete"></p> 对于固定数量的最近邻居，仅计算p维，即高维，我们选择困惑度并乘以3。将计算3 *困惑度范围内的数据点。 </li><li>Other remote clusters of points are approximated with a single value.<p class="nodelete"></p> 其他点的远程群集用单个值近似。 </li><li>For computing q, they put embedding points into the spatial acceleration data structure. It helps to identify the groups. It only calculates the distance between the virtual points and the target point. It gives them the weight of the number of data points.<p class="nodelete"></p> 为了计算q，他们将嵌入点放入空间加速度数据结构中。 它有助于识别组。 它仅计算虚拟点和目标点之间的距离。 它赋予它们数据点数量的权重。 </li></ul> 
     <h2> 生命科学的例子 <span style="font-weight: bold;">(</span>Example in Life Science<span style="font-weight: bold;">)</span></h2> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div style="text-align: center;"> 
          <img alt="Image for post" src="https://images2.imgbox.com/fa/f7/cHWKlWxj_o.png" width="651" height="436" style="outline: none;"> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        [Amir et al., Nature Biotechnology 2013] 
      </figcaption> 
      <figcaption>
        [Amir等，自然生物技术，2013年] 
      </figcaption> 
     </figure> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/6a/11/3HwgMpHy_o.png" width="1050" height="452" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </div> 
      </div> 
     </figure> 
     <p>They use to distinguish the cell types by high dimensional protein expression profiles. They find that some proteins, it was considered important, are not that important to distinguish cell types by subtracting it from t-SNE. The shape of the t-SNE still remains similar.</p> 
     <p> 他们通过高维蛋白质表达谱来区分细胞类型。 他们发现一些蛋白质，被认为很重要，通过从t-SNE中减去来区分细胞类型并不那么重要。 t-SNE的形状仍然保持相似。</p> 
     <h2> 警告！ <span style="font-weight: bold;">(</span>Warning!<span style="font-weight: bold;">)</span></h2> 
     <p><a href="https://distill.pub/2016/misread-tsne/" rel="noopener nofollow">This site</a> helps you to understand t-SNE with practical experiments.</p> 
     <p> <a href="https://distill.pub/2016/misread-tsne/" rel="noopener nofollow">该站点</a>可通过实际实验帮助您了解t-SNE。</p> 
    </div> 
   </div> 
   <div> 
    <div> 
     <div> 
      <div> 
       <figure style="display:block;text-align:center;"> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/a5/b0/edXOmMtB_o.png" width="267" height="308" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </figure> 
       <figure style="display:block;text-align:center;"> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/f9/d9/gCb2hZLQ_o.png" width="540" height="305" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
        <figcaption>
          It does not preserve densities and cluster spreads. 
        </figcaption> 
        <figcaption>
          它不保留密度和簇分布。 
        </figcaption> 
       </figure> 
      </div> 
     </div> 
    </div> 
   </div> 
   <div> 
    <div> 
     <p>This is an obvious picture because t-SNE only care about neighbors. Therefore, the densities and spreads of clusters of original data are not preserved.</p> 
     <p> 这是显而易见的情况，因为t-SNE只关心邻居。 因此，原始数据的群集的密度和散布不保留。</p> 
    </div> 
   </div> 
   <div> 
    <div> 
     <div> 
      <div> 
       <figure style="display:block;text-align:center;"> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/08/59/TirFyrmF_o.png" width="270" height="309" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </figure> 
       <figure style="display:block;text-align:center;"> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/e5/76/cSpHv0A3_o.png" width="540" height="303" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
        <figcaption>
          the distance can be different 
        </figcaption> 
        <figcaption>
          距离可以不同 
        </figcaption> 
       </figure> 
      </div> 
     </div> 
    </div> 
   </div> 
   <div> 
    <div> 
     <p>Distance is affected by perplexity. Low perplexity means low sigma. Thus, a smaller region will be considered as neighborhood region. If perplexity is big enough to see other clusters, then it can consider the distances.</p> 
     <p>距离受困惑的影响。 低困惑度意味着低sigma。 因此，较小的区域将被视为邻近区域。 如果困惑度足够大，可以看到其他群集，则可以考虑距离。</p> 
    </div> 
   </div> 
   <div> 
    <div> 
     <div> 
      <div> 
       <figure style="display:block;text-align:center;"> 
        <div> 
         <div> 
          <div> 
           <div style="text-align: center;"> 
            <img alt="Image for post" src="https://images2.imgbox.com/4d/f4/QrOEJlum_o.png" width="570" height="348" style="outline: none;"> 
           </div> 
          </div> 
         </div> 
        </div> 
       </figure> 
       <figure style="display:block;text-align:center;"> 
        <div> 
         <div> 
          <div> 
           <div style="text-align: center;"> 
            <img alt="Image for post" src="https://images2.imgbox.com/13/69/JGbSkj7j_o.png" width="544" height="348" style="outline: none;"> 
           </div> 
          </div> 
         </div> 
        </div> 
        <figcaption>
          You should know cluster in t-SNE can be a hallucination. 
        </figcaption> 
        <figcaption>
          您应该知道，t-SNE中的簇可能是幻觉。 
        </figcaption> 
       </figure> 
      </div> 
     </div> 
    </div> 
   </div> 
   <div> 
    <div> 
     <p>Perplexity 2 case shows many clusters but it is not actual clusters in the original data. It exaggerates the randomness and it makes a hallucination.</p> 
     <p> 困惑2的情况显示了许多聚类，但在原始数据中不是实际的聚类。 它夸大了随机性，并产生了幻觉。</p> 
     <p><strong>Please keep them in mind these tips when you use t-SNE.</strong></p> 
     <p> <strong>使用t-SNE时，请记住这些提示。</strong></p> 
     <p>This post is published on 9/10/2020</p> 
     <p> 此帖发布于9/10/2020</p> 
    </div> 
   </div> 
  </section> 
 </div> 
 <blockquote> 
  <p>翻译自: <a href="https://medium.com/@jeheonpark93/vc-sne-and-t-sne-who-is-my-neighbor-34e738bf9e71" rel="nofollow">https://medium.com/@jeheonpark93/vc-sne-and-t-sne-who-is-my-neighbor-34e738bf9e71</a></p> 
 </blockquote> 
</article>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/04e4c2d220c79c9455100cdd686b015d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Web前端面试题-应用CSS样式</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/76a7b4d3da9fc5058d566dedeb82f22a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">github请求超时_使用危险js和github操作自动执行常见的请求请求反馈</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>