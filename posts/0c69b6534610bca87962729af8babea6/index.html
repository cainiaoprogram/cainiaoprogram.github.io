<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>线性回归算法中损失函数（误差函数/目标函数）的来历及推导 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="线性回归算法中损失函数（误差函数/目标函数）的来历及推导" />
<meta property="og:description" content="本文以线形回归为例，利用高斯分布概率密度函数和极大似然估计公式推导出线性回归误差函数通用表达式。
以一元线性回归算法为例，其模型如下所示：
(1)
假如给定 i=14 个样本数据 ：
[ [29, 77], [28, 62], [34, 93], [31, 84], [25, 59], [29, 64], [32, 80], [31, 75], [24, 58], [33, 91], [25, 51], [31, 73], [26, 65], [30, 84]]
线性回归的原理即为通过确定模型参数，确定所有样本值代入模型后得到的结果值与真实值累计误差最小。以一元线性回归为例，直观的看如图所示，即为找到一条直线，穿过所有样本点，并使各个点到直线的累计距离 最小，从而求出直线方程的参数 和 参数 的值。
如果是多元线性回归，即样本的特征向量不止一个维度，同样遵循以上模型特点和原理，更通用的线形回归表达方式如下：
(2)
以上表达式可通过向量的方式进行表达，如下所示：
(3)
因为误差具有随机性，符合独立同分布特点，高斯概率分布函数如下：
(4)
因误差项满足高斯概率分布：
(5)
将误差代入高斯分布：
(6)
根据极大似然估计的原理，根据样本发生的概率估计整个事件的概率，其主要思想是所有样本发生的总概率最大的概率即为事件的概率，因为样本独立同分布，故所有样本概率积求最大值：
(7)
由于概率小于1，连乘会导致最后的值非常小，故两边取对数，让连乘变连加，且不会改变函数的单调性。
(8)
(9)
(10)
要是上述对数函数最大，即可让与连加部分最小即可，而这部分就是我们要推导的目标函数：
(11)
以上目标函数同时也可以用向量表达：
(12)
通过上诉步骤推导出的目标函数，与我们最朴素的想法通过求误差平方和不谋而合：
(13)
：是指给定的样本数据中的y值，即真实值
：是指通过线性回归模型（即直线方程）求得的 y 值，即： 上述目标函数求极小值可以通过最小二乘法求解，又名正规方程解：
即对目标函数求导等于0：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/0c69b6534610bca87962729af8babea6/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-08-03T15:37:46+08:00" />
<meta property="article:modified_time" content="2023-08-03T15:37:46+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">线性回归算法中损失函数（误差函数/目标函数）的来历及推导</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>本文以线形回归为例，利用高斯分布概率密度函数和极大似然估计公式推导出线性回归误差函数通用表达式。</p> 
<p>以一元线性回归算法为例，其模型如下所示：</p> 
<p style="text-align:right;"><img alt="f(x) = \beta _{0} + \beta _{1}x + \varepsilon" class="mathcode" src="https://images2.imgbox.com/d0/61/bZIK8EYU_o.png">                                                                        (1)</p> 
<p>假如给定 i=14 个样本数据 <img alt="[xi, yi]" class="mathcode" src="https://images2.imgbox.com/c3/6a/1bMMYgTa_o.png">：</p> 
<p>[ [29, 77], [28, 62], [34, 93], [31, 84], [25, 59], [29, 64], [32, 80], [31, 75], [24, 58], [33, 91], [25, 51], [31, 73], [26, 65], [30, 84]]</p> 
<p>线性回归的原理即为通过确定模型参数，确定所有样本值代入模型后得到的结果值与真实值累计误差最小。以一元线性回归为例，直观的看如图所示，即为找到一条直线，穿过所有样本点，并使各个点到直线的累计距离 <img alt="\varepsilon" class="mathcode" src="https://images2.imgbox.com/6d/53/IT3fBZVH_o.png"> 最小，从而求出直线方程的参数 <img alt="\beta _{0}" class="mathcode" src="https://images2.imgbox.com/46/29/7iDX4PU0_o.png"> 和 参数 <img alt="\beta _{1}" class="mathcode" src="https://images2.imgbox.com/fc/a8/sNzaWlqz_o.png"> 的值。</p> 
<p><img alt="" height="480" src="https://images2.imgbox.com/63/8a/pFFy76Dv_o.png" width="640"></p> 
<p>如果是多元线性回归，即样本的特征向量不止一个维度，同样遵循以上模型特点和原理，更通用的线形回归表达方式如下：</p> 
<p style="text-align:right;"><img alt="f(x) = \beta _{0} + \beta _{1}x_{1} + \beta _{2}x_{2} +... + \beta _{i}x_{i}+ \varepsilon" class="mathcode" src="https://images2.imgbox.com/3b/b7/8hOKjXfD_o.png">                                 (2)</p> 
<p>以上表达式可通过向量的方式进行表达，如下所示：</p> 
<p style="text-align:right;"><img alt="y^{(i)} = \beta ^{T}X^{(i)} + \varepsilon ^{(i)}" class="mathcode" src="https://images2.imgbox.com/fa/7f/Knk4vdO7_o.png">                                                                 (3)</p> 
<p>因为误差具有随机性，符合独立同分布特点，高斯概率分布函数如下：</p> 
<p style="text-align:right;"><img alt="f(x) = \frac{1}{\sigma\sqrt{2\pi }}exp(-\frac{(x-\mu )^{2}}{2\sigma ^{2}})" class="mathcode" src="https://images2.imgbox.com/2b/9e/hsHKTve4_o.png">                                                          (4)</p> 
<p>因误差项满足高斯概率分布：</p> 
<p style="text-align:right;"><img alt="\rho (\varepsilon ^{i}) = \frac{1}{\sigma\sqrt{2\pi }}exp(-\frac{(\varepsilon ^{(i)})^{2}}{2\sigma ^{2}})" class="mathcode" src="https://images2.imgbox.com/00/6a/GTj6iOyH_o.png">                                                       (5)</p> 
<p>将误差代入高斯分布：</p> 
<p style="text-align:right;"><img alt="\rho (y^{(i)}|X^{(i)};\beta ) = \frac{1}{\sigma\sqrt{2\pi }}exp(-\frac{(y^{(i)}-\beta ^{T}X^{(i)})^{2}}{2\sigma ^{2}})" class="mathcode" src="https://images2.imgbox.com/53/0d/eHAUz5a5_o.png">                        (6)</p> 
<p>根据极大似然估计的原理，根据样本发生的概率估计整个事件的概率，其主要思想是所有样本发生的总概率最大的概率即为事件的概率，因为样本独立同分布，故所有样本概率积求最大值：</p> 
<p style="text-align:right;"><img alt="L(\beta ) = \prod_{i=1}^{m}P(y^{(i)}|X^{(i)};\beta ) = \prod_{i=1}^{m}\frac{1}{ \sigma\sqrt{2\pi }}exp(-\frac{(y^{(i)}-\beta ^{T}X^{(i)})^{2}}{2\sigma ^{2}})" class="mathcode" src="https://images2.imgbox.com/3d/ef/KNRkjOgx_o.png">                (7)</p> 
<p>由于概率小于1，连乘会导致最后的值非常小，故两边取对数，让连乘变连加，且不会改变函数的单调性。</p> 
<p style="text-align:right;"><img alt="logL(\beta ) = log\prod_{i=1}^{m}\frac{1}{\sigma \sqrt{2\pi }}exp(-\frac{(y^{(i)}-\beta ^{T}X^{(i)})^{2}}{2\sigma ^{2}})" class="mathcode" src="https://images2.imgbox.com/5d/c6/ISdn1Lfn_o.png">                        (8)</p> 
<p style="text-align:right;"><img alt="=\sum_{i=1}^{m}log\frac{1}{\sigma \sqrt{2\pi }}exp(-\frac{(y^{(i)}-\beta ^{T}X^{(i)})^{2}}{2\sigma ^{2}})" class="mathcode" src="https://images2.imgbox.com/83/e0/9WAAOg8l_o.png">                                        (9)</p> 
<p style="text-align:right;"><img alt="=m*log\frac{1}{\sigma \sqrt{2\pi }}-\frac{1}{2\sigma ^{2}}\sum_{i=1}^{m}(y^{(i)}-\beta ^{T}X^{(i)})^{2}" class="mathcode" src="https://images2.imgbox.com/db/1c/jVkRSF7S_o.png">                        (10)</p> 
<p>要是上述对数函数最大，即可让与连加部分最小即可，而这部分就是我们要推导的目标函数：</p> 
<p style="text-align:right;"><img alt="J(\beta ) = \frac{1}{2}\sum_{i=1}^{m}(y^{(i)}-\beta ^{T}X^{(i)})^{2}" class="mathcode" src="https://images2.imgbox.com/e4/aa/u3JATBBe_o.png">                                        (11)</p> 
<p>以上目标函数同时也可以用向量表达：</p> 
<p style="text-align:right;"><img alt="J(\beta ) = \frac{1}{2}(y-X\beta )^{T}(y-X\beta )" class="mathcode" src="https://images2.imgbox.com/ae/77/GDKaEtox_o.png">                                (12)</p> 
<p>通过上诉步骤推导出的目标函数，与我们最朴素的想法通过求误差平方和不谋而合：</p> 
<p style="text-align:right;"></p> 
<p style="text-align:right;"><img alt="\sum_{i=1}^{m}({\hat{y}}^{(i)}-y^{(i)})^{2}" class="mathcode" src="https://images2.imgbox.com/f1/28/4neR9m3u_o.png">                                                                (13)</p> 
<p><img alt="y^{(i)}" class="mathcode" src="https://images2.imgbox.com/a0/99/3Arx60S5_o.png">：是指给定的样本数据中的y值，即真实值</p> 
<p><img alt="\hat{y}^{(i)}" class="mathcode" src="https://images2.imgbox.com/79/d8/tjkObjRY_o.png">：是指通过线性回归模型（即直线方程）求得的 y 值，即：<img alt="\hat{y}^{(i)} = ax^{(i)} + b" class="mathcode" src="https://images2.imgbox.com/e0/25/WV2oMlFY_o.png">   </p> 
<p>上述目标函数求极小值可以通过最小二乘法求解，又名正规方程解：</p> 
<p>即对目标函数求导等于0：</p> 
<p style="text-align:right;"><img alt="\bigtriangledown _{\beta }J(\beta ) = X^{T}X\beta -X^{T}y=0" class="mathcode" src="https://images2.imgbox.com/3e/96/2LHA2bwM_o.png">                                        (14)</p> 
<p style="text-align:right;"><img alt="\beta =(X^{T}X)^{-1}X^{T}y" class="mathcode" src="https://images2.imgbox.com/1a/6b/oERTiq3M_o.png">                                                        (15)</p> 
<p>通过最小二乘法求极小值不仅计算量大，而且不是每个特征矩阵有逆，故通常使用梯度下降法求解，梯度下降本质是一种迭代搜索目标函数最优解的方法，顾名思义，其搜索方向是其梯度方向，因梯度的反方向是函数值减小最快的方向，因此故又称之为最速下降法。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/7b00cc09a9b55b68d965774f36b1657b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">关于CPU对Cache的访存操作 浅解</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/62197cf59e46e32549f34b54722e4266/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">C&#43;&#43;工程编译链接错误汇总VisualStudio</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>