<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>浅谈图像分割算法 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="浅谈图像分割算法" />
<meta property="og:description" content="文章目录 FCNUnetSegNetPSPNetDeepLab 系列（V1-V4）RefineNetLarge_kernel_Matters 目前比较有名的图像分割算法当属，Unet，SegNet，FCN，DeepLab 系列，RefineNet，PSPNet，Large kernel Matter 等。本文旨在对这写分割算法进行一个简单的总结比较，不会对文章的细节细抠，因为不同的文章肯定有很多不一样的实现细节，所以如果对相关网络及思想感兴趣的读者可以自行阅读原论文。 后续对一些高效的分割网络，例如 ENet, DFANet 等 FCN FCN 是一篇很经典的工作，是第一次实现使用全卷积网络可以端到端训练来进行语义分割任务，并且模型并不需要任何的前置处理和后置处理。
文章主要是将当前分类网络的全连接层（也是限制网络输入图像分辨率的主要原因）看作是卷积核覆盖了整张图的卷积层。经过等效转换以后就可以使用全卷积网络，并且可以使用分类任务中训练好的预训练权重了，最后加入解卷积层的操作来进行像素级别的类别预测。
文中使用的提前特征的网络结构是 VGGNet, 然后加入了多级合并的结构来得到更精细的结果，上采样过程中，如果遇到前后尺寸上采样后不一样的情况，需要使用 Crop 操作，从大尺寸的特征图中心裁剪层一样尺寸然后融合，具体的结构图下图（图片参考自本小节的链接博客）：
下面这篇文章介绍的挺详细的，有兴趣的可以看一下：
https://blog.csdn.net/qq_36269513/article/details/80420363
Unet 其首次提出是主要应用在医学图像分割领域，因为医学图像相对来说结构固定单一，数据量少，并且需要分割的很精细，文中提出了 Unet网络，以及对应的医学图像的增广方式来进行医学图像的检测任务。其网络结构以型似 U 型而得名，其中有跨层连接（Concat）的操作，为的就是弥补编码器部分下采样信息的损失。网络结构方面编码器部分是使用了 VGGNet 作为主干网络进行特征提取，应为输入的尺寸（572x572）并不是 2 的次幂，所以在解码器部分还涉及到编、解码器部分尺寸的对齐（Crop）操作，这里的上采样使用的是解卷积层（deconvolution layer）。
针对医学图像问题，为了边缘检测更准备，文中对原图片进行了边缘扩张，如下图所示，将红框边界部分都进行了镜像，以此来有更多的上下文信息预测后续黄色框中的区域，多余的区域在解码器过程中应该就通过 crop 操作去除掉了。
文中为了加强细胞的边缘学习，还用了形态学的方法，得到一张细胞边缘权重图，是边缘的地方给予 Loss 更大的权重去训练网络，取得最后分割输出。
SegNet 相较于 Unet， SegNet 也是使用了 VGGNet 作为主干网络来提取特征，很大的区别就是在解码器部分，在文中 SegNet 的提出重点是场景分割，需要场景分割的图片特性是，需要有能力去分辨不同类别物体的外貌特性（道路、建筑），形状特性（车、人），并且一些空间关系（主道路和路边），以道路分割为例，可能一张图中大部分是道路和建筑，车辆或者行人所占的像素很少，那么车辆和行人更多的就需要根据它们的边缘形状信息来判断，所以本文的主要特点就是上采样方式的不同。在解码器端 SegNet 主要是利用解码器端使用 MaxPooling 时的索引信息上采样（Caffe 框架下，使用了一个索引矩阵来存储 MaxPooling 选择的最大值的位置信息），以此来尽可能保住物体的边缘及形状信息。
文中基于 FCN 包括不同的上采样方式：双线性插值，MaxPooling索引，解卷积学习。做了几个不同模型的对比实验，具体感兴趣的读者可以前往阅读原论文。
PSPNet 本片论文主要讨论的点是场景分割的过程种有很多物体是很难被分割准确的，我们需要更多的上下文信息（例如，一个 船 很可能被错分割成 车 ，但是如果加上上下文信息，既 船 周围的像素被分割成了 水， 这样被分割成 车 的概率就会小很多），主要网络如上图所示，是从图像金字塔中获得的灵感，巧妙的点是，作者把这个思路很好的应用在了分割这种像素级别的分类任务中。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/dca56523524f32e7215ed494caa13448/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-07-31T22:16:21+08:00" />
<meta property="article:modified_time" content="2019-07-31T22:16:21+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">浅谈图像分割算法</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-github-gist">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><ul><li><ul><li><ul><li><a href="#FCNhttpsarxivorgpdf14114038pdf_2" rel="nofollow">FCN</a></li><li><a href="#Unethttpsarxivorgabs150504597_12" rel="nofollow">Unet</a></li><li><a href="#SegNethttpsarxivorgpdf151100561pdf_22" rel="nofollow">SegNet</a></li><li><a href="#PSPNethttpsarxivorgabs161201105_27" rel="nofollow">PSPNet</a></li><li><a href="#DeepLab_V1V4_34" rel="nofollow">DeepLab 系列（V1-V4）</a></li><li><a href="#RefineNethttpsarxivorgabs161106612_36" rel="nofollow">RefineNet</a></li><li><a href="#Large_kernel_Mattershttpsarxivorgpdf170302719pdf_46" rel="nofollow">Large_kernel_Matters</a></li></ul> 
   </li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<br> 
<strong>目前比较有名的图像分割算法当属，Unet，SegNet，FCN，DeepLab 系列，RefineNet，PSPNet，Large kernel Matter 等。本文旨在对这写分割算法进行一个简单的总结比较，不会对文章的细节细抠，因为不同的文章肯定有很多不一样的实现细节，所以如果对相关网络及思想感兴趣的读者可以自行阅读原论文。 后续对一些高效的分割网络，例如 ENet, DFANet 等</strong> 
<p></p> 
<h5><a id="FCNhttpsarxivorgpdf14114038pdf_2"></a><a href="https://arxiv.org/pdf/1411.4038.pdf" rel="nofollow">FCN</a></h5> 
<p>FCN 是一篇很经典的工作，是第一次实现使用全卷积网络可以端到端训练来进行语义分割任务，并且模型并不需要任何的前置处理和后置处理。<br> 文章主要是将当前分类网络的全连接层（也是限制网络输入图像分辨率的主要原因）看作是卷积核覆盖了整张图的卷积层。经过等效转换以后就可以使用全卷积网络，并且可以使用分类任务中训练好的预训练权重了，最后加入解卷积层的操作来进行像素级别的类别预测。<br> 文中使用的提前特征的网络结构是 VGGNet, 然后加入了多级合并的结构来得到更精细的结果，上采样过程中，如果遇到前后尺寸上采样后不一样的情况，需要使用 Crop 操作，从大尺寸的特征图中心裁剪层一样尺寸然后融合，具体的结构图下图（图片参考自本小节的链接博客）：<br> <img src="https://images2.imgbox.com/07/c3/Vzpsfa6E_o.png" alt="在这里插入图片描述"></p> 
<p>下面这篇文章介绍的挺详细的，有兴趣的可以看一下：<br> https://blog.csdn.net/qq_36269513/article/details/80420363</p> 
<h5><a id="Unethttpsarxivorgabs150504597_12"></a><a href="https://arxiv.org/abs/1505.04597" rel="nofollow">Unet</a></h5> 
<img src="https://images2.imgbox.com/b3/d4/dnVH469c_o.png" width="70%"> 
<p>其首次提出是主要应用在医学图像分割领域，因为医学图像相对来说结构固定单一，数据量少，并且需要分割的很精细，文中提出了 Unet网络，以及对应的医学图像的增广方式来进行医学图像的检测任务。其网络结构以型似 U 型而得名，其中有跨层连接（Concat）的操作，为的就是弥补编码器部分下采样信息的损失。网络结构方面编码器部分是使用了 VGGNet 作为主干网络进行特征提取，应为输入的尺寸（572x572）并不是 2 的次幂，所以在解码器部分还涉及到编、解码器部分尺寸的对齐（Crop）操作，这里的上采样使用的是解卷积层（deconvolution layer）。</p> 
<p>针对医学图像问题，为了边缘检测更准备，文中对原图片进行了边缘扩张，如下图所示，将红框边界部分都进行了镜像，以此来有更多的上下文信息预测后续黄色框中的区域，多余的区域在解码器过程中应该就通过 crop 操作去除掉了。<br> <img src="https://images2.imgbox.com/c7/d4/v7Lno2Wy_o.png" width="70%"></p> 
<p>文中为了加强细胞的边缘学习，还用了形态学的方法，得到一张细胞边缘权重图，是边缘的地方给予 Loss 更大的权重去训练网络，取得最后分割输出。</p> 
<h5><a id="SegNethttpsarxivorgpdf151100561pdf_22"></a><a href="https://arxiv.org/pdf/1511.00561.pdf" rel="nofollow">SegNet</a></h5> 
<p><img src="https://images2.imgbox.com/d7/52/y4w9B8ks_o.png" alt="在这里插入图片描述"><br> 相较于 Unet， SegNet 也是使用了 VGGNet 作为主干网络来提取特征，很大的区别就是在解码器部分，在文中 SegNet 的提出重点是场景分割，需要场景分割的图片特性是，需要有能力去分辨不同类别物体的外貌特性（道路、建筑），形状特性（车、人），并且一些空间关系（主道路和路边），以道路分割为例，可能一张图中大部分是道路和建筑，车辆或者行人所占的像素很少，那么车辆和行人更多的就需要根据它们的边缘形状信息来判断，所以本文的主要特点就是上采样方式的不同。在解码器端 SegNet 主要是利用解码器端使用 MaxPooling 时的索引信息上采样（Caffe 框架下，使用了一个索引矩阵来存储 MaxPooling 选择的最大值的位置信息），以此来尽可能保住物体的边缘及形状信息。<br> 文中基于 FCN 包括不同的上采样方式：双线性插值，MaxPooling索引，解卷积学习。做了几个不同模型的对比实验，具体感兴趣的读者可以前往阅读原论文。</p> 
<h5><a id="PSPNethttpsarxivorgabs161201105_27"></a><a href="https://arxiv.org/abs/1612.01105" rel="nofollow">PSPNet</a></h5> 
<p><img src="https://images2.imgbox.com/86/87/POCNJTIo_o.png" alt="在这里插入图片描述"><br> 本片论文主要讨论的点是场景分割的过程种有很多物体是很难被分割准确的，我们需要更多的上下文信息（例如，一个 船 很可能被错分割成 车 ，但是如果加上上下文信息，既 船 周围的像素被分割成了 水， 这样被分割成 车 的概率就会小很多），主要网络如上图所示，是从图像金字塔中获得的灵感，巧妙的点是，作者把这个思路很好的应用在了分割这种像素级别的分类任务中。<br> 如上图所以，对于特征提取网络输出的特征图，例如它的尺寸为 16x16，那么会分别将它们经过不同 stride 的 AVE-pooling（stride=2，stride=4，stride=8，stride=16等），这样我们就可以通过不同的分支得到了不同感受野的均值特征图，把他们经过卷积层卷积后再经过上采样层，全部上采样回原先的分辨率 16x16，最后将拼接后的特征图经过卷积层融合特征以后再讲过上采样回到原图分辨率与真值图做 Loss。<br> <img src="https://images2.imgbox.com/04/fc/0OreVdA8_o.png" alt="在这里插入图片描述"><br> 值得一提的是在训练过程中，作者还对网络增加了一个辅助 Loss 来对网络进行一个深监督，让它们一起方向传播来训练使用预训练模型的网络，这里会使用不同的权重来平衡两个 Loss 之间的关系。最后在测试网络的时候就可以放弃这个辅助 loss 了。<br> （Ps,如果是使用 Caffe 框架的话，BN层的操作也是作者自定义的，需要提前训练完后就 froze 住，实验证明这样是有好处的，具体可以参考原论文）。</p> 
<h5><a id="DeepLab_V1V4_34"></a>DeepLab 系列（V1-V4）</h5> 
<p>XXXXXXXXXXXX</p> 
<h5><a id="RefineNethttpsarxivorgabs161106612_36"></a><a href="https://arxiv.org/abs/1611.06612" rel="nofollow">RefineNet</a></h5> 
<p>DeepLab 系列无疑是具有很强影响力的，其中最具特性的就是空洞卷积的使用，但是空洞卷积无疑是有自身缺点的，主要就是需要维持一个大的分辨率（一般都是原图像的 1/8 ）,这还是需要很大的内存开销的，尤其是随着网络加深，通道数越来越多。<br> RefineNet 是在正常的分类结构上进行分割任务，其实也是在探讨如何更有效的结合不同层次的特征的问题，这个网络我没有具体复现训练过，但是从论文介绍，网络的结构就很复杂：<br> <img src="https://images2.imgbox.com/27/24/P45o1F2M_o.png" alt="在这里插入图片描述"><br> 上面是一个 RefineNet 的结构，可以看到，不同分辨率的特征图输入网络时，RefineNet 不是直接进行卷积完 Cooncat 或者 SUM，而是经过了两个 ResNet block,然后再上采样，最后 SUM，而且 SUM 完又是经过了链式的 Pooling 操作来得到不同尺寸的特征信息，SUM 完再经过 ResNet block。 这个只是一级的操作，整体网络构成如下图所示：<br> <img src="https://images2.imgbox.com/bc/33/72KaHSzG_o.png" alt="在这里插入图片描述"><br> 当然文中也给了自定义使用 RefineNet 的个数：<br> <img src="https://images2.imgbox.com/ac/74/J3N9Snss_o.png" alt="在这里插入图片描述"><br> 结果当然是越多越好了。</p> 
<h5><a id="Large_kernel_Mattershttpsarxivorgpdf170302719pdf_46"></a><a href="https://arxiv.org/pdf/1703.02719.pdf" rel="nofollow">Large_kernel_Matters</a></h5> 
<p>本文是深入探讨了分割问题的两个子问题，像素级别的分类与目标位置定位问题，本身这两个问题就是有矛盾点的，分类需要对位置信息不敏感，而定位需要对位置信息敏感。<strong>首先从定位角度来说</strong>，网络结构应该是全卷积网络，且网络中不应该有全局的池化层，或者是全连接层，这样会完全丢失位置信息。<strong>其次从分类的角度说</strong>，网络应该尽可能的包含密集连接，这样可以增加分类性能。从上面两点思路出发，就引出了<strong>全局卷积网络</strong>（Global Convolutional Network）,以此来构建自己的解码器，解码器部分还是很有自己的特点的。<br> <img src="https://images2.imgbox.com/97/b0/4O9iNM8m_o.png" alt="在这里插入图片描述"><br> 上面可以看到，这里的 k 是一个可以自定义的值，文中实验了 3~15 不等的尺寸，实验证明随着 k 的增大性能是在逐渐增加的。为了进一步优化分割的边缘，文中在全局卷积网络结构的后面接上了边缘优化（Boundary Refinement）模块。且每一个卷积层输出的都是目标类别数量的特征图，这也是缓解使用大的 Kernel size 带来的内存开销吧，且学习了 GoogLeNet V3 中 kxk 分解为 kx1 和 1xk 的思想，进一步缓解计算量的问题。<br> <img src="https://images2.imgbox.com/fd/49/lxJF6HCX_o.png" alt="在这里插入图片描述"><br> 值得一题的是，为了优化边界，有看过其他两篇相关论文，一篇是 Adobe 的 《Deep Image Mating》 关于抠图的，最后也是会为了优化边缘而加入额外的卷积层来进行训练，实验结果证明这种思路确实有效。这里 LKM 也是使用额外的卷积层来达到相同目的，实验结果证明，加入 BR 结构对整体分割指标并没有显著提升，但是对边缘有很好的优化作用。<br> 最后基于上面的两个结构构建了 LKM 的整体网络结构如下：<br> <img src="https://images2.imgbox.com/b8/74/cDuWyy2f_o.png" alt="在这里插入图片描述"></p> 
<p>未完待续…</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c1207ed375b49a9c593ddfbafe0cebf7/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">CBAM: Convolutional Block Attention Module—— channel attention &#43; spatial attention</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/bc630a64cdccec9028e01a6230cdbfd2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【AI不惑境】计算机视觉中注意力机制原理及其模型发展和应用</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>