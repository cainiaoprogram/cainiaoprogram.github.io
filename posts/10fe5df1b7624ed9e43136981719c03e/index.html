<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>第十周：机器学习周报 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="第十周：机器学习周报" />
<meta property="og:description" content="目录
摘要
Abstract
1 Slot Filling（槽位填充）
2 Recurrent Neural Network（循环神经网络）
2.1 RNN的结构
2.2 RNN的分类
2.3 RNN的原理
2.4 RNN can be deep
2.5 Bidirectional RNN（ 双向RNN）
2.6 RNN的训练
3 Long Short-term Memory(长短期记忆)
3.1 LSTM的结构
3.2 Multiple-layer LSTM
3.3 LSTM Example
3.4 GRU
4 RNN的应用
总结
摘要 在做语义辨识任务中，传统的前馈神经网络无法结合上下文分辨出同一词汇的不同语义，而循环神经网络（RNN）是一类用于处理序列数据的神经网络，它能挖掘数据中的时序信息以及语义信息，是一种有记忆力的神经元。通过手动演算RNN的运作过程，认识到网络的输入顺序将会影响输出，这是因为它与CNN等结构不同，它不仅考虑前一时刻的输入,而且赋予了网络对前面的内容的一种“记忆”功能。RNN进行训练时会存在梯度无效和梯度爆炸等问题，为了解决这些问题有一种特殊的RNN叫做Long short-term memory，相比普通的RNN，LSTM能够在更长的序列中有更好的表现，解决了RNN训练时的梯度无效问题。同时学习了LSTM三个门的原理以及LSTM的一个计算实例，还有RNN和LSTM的实际应用领域。
Abstract In the task of semantic identification, the traditional Feedforward neural network can not distinguish different semantics of the same word in combination with the context, while Recurrent neural network (RNN) is a kind of neural network used to process sequence data." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/10fe5df1b7624ed9e43136981719c03e/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-02T20:43:18+08:00" />
<meta property="article:modified_time" content="2023-07-02T20:43:18+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">第十周：机器学习周报</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E6%91%98%E8%A6%81-toc" style="margin-left:0px;"><a href="#%E6%91%98%E8%A6%81" rel="nofollow">摘要</a></p> 
<p id="Abstract-toc" style="margin-left:0px;"><a href="#Abstract" rel="nofollow">Abstract</a></p> 
<p id="1%20Slot%20Filling%EF%BC%88%E6%A7%BD%E4%BD%8D%E5%A1%AB%E5%85%85%EF%BC%89-toc" style="margin-left:0px;"><a href="#1%20Slot%20Filling%EF%BC%88%E6%A7%BD%E4%BD%8D%E5%A1%AB%E5%85%85%EF%BC%89" rel="nofollow">1 Slot Filling（槽位填充）</a></p> 
<p id="2%20Recurrent%20Neural%20Network%EF%BC%88%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%89-toc" style="margin-left:0px;"><a href="#2%20Recurrent%20Neural%20Network%EF%BC%88%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%89" rel="nofollow">2 Recurrent Neural Network（循环神经网络）</a></p> 
<p id="2.1%20RNN%E7%9A%84%E7%BB%93%E6%9E%84-toc" style="margin-left:40px;"><a href="#2.1%20RNN%E7%9A%84%E7%BB%93%E6%9E%84" rel="nofollow">2.1 RNN的结构</a></p> 
<p id="2.2%20RNN%E7%9A%84%E5%88%86%E7%B1%BB-toc" style="margin-left:40px;"><a href="#2.2%20RNN%E7%9A%84%E5%88%86%E7%B1%BB" rel="nofollow">2.2 RNN的分类</a></p> 
<p id="2.3%20RNN%E7%9A%84%E5%8E%9F%E7%90%86-toc" style="margin-left:40px;"><a href="#2.3%20RNN%E7%9A%84%E5%8E%9F%E7%90%86" rel="nofollow">2.3 RNN的原理</a></p> 
<p id="2.4%20RNN%20can%20be%20deep-toc" style="margin-left:40px;"><a href="#2.4%20RNN%20can%20be%20deep" rel="nofollow">2.4 RNN can be deep</a></p> 
<p id="2.5%20Bidirectional%20RNN%EF%BC%88%C2%A0%E5%8F%8C%E5%90%91RNN%EF%BC%89-toc" style="margin-left:40px;"><a href="#2.5%20Bidirectional%20RNN%EF%BC%88%C2%A0%E5%8F%8C%E5%90%91RNN%EF%BC%89" rel="nofollow">2.5 Bidirectional RNN（ 双向RNN）</a></p> 
<p id="2.6%20RNN%E7%9A%84%E8%AE%AD%E7%BB%83-toc" style="margin-left:40px;"><a href="#2.6%20RNN%E7%9A%84%E8%AE%AD%E7%BB%83" rel="nofollow">2.6 RNN的训练</a></p> 
<p id="3%20Long%20Short-term%20Memory(%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86)-toc" style="margin-left:0px;"><a href="#3%20Long%20Short-term%20Memory%28%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%29" rel="nofollow">3 Long Short-term Memory(长短期记忆)</a></p> 
<p id="3.1%20LSTM%E7%9A%84%E7%BB%93%E6%9E%84-toc" style="margin-left:40px;"><a href="#3.1%20LSTM%E7%9A%84%E7%BB%93%E6%9E%84" rel="nofollow">3.1 LSTM的结构</a></p> 
<p id="3.2%20Multiple-layer%20LSTM-toc" style="margin-left:40px;"><a href="#3.2%20Multiple-layer%20LSTM" rel="nofollow">3.2 Multiple-layer LSTM</a></p> 
<p id="3.3%20LSTM%20Example-toc" style="margin-left:40px;"><a href="#3.3%20LSTM%20Example" rel="nofollow">3.3 LSTM Example</a></p> 
<p id="3.4%20GRU-toc" style="margin-left:40px;"><a href="#3.4%20GRU" rel="nofollow">3.4 GRU</a></p> 
<p id="4%20RNN%E7%9A%84%E5%BA%94%E7%94%A8-toc" style="margin-left:0px;"><a href="#4%20RNN%E7%9A%84%E5%BA%94%E7%94%A8" rel="nofollow">4 RNN的应用</a></p> 
<p id="%E6%80%BB%E7%BB%93-toc" style="margin-left:0px;"><a href="#%E6%80%BB%E7%BB%93" rel="nofollow">总结</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="%E6%91%98%E8%A6%81">摘要</h2> 
<p>在做语义辨识任务中，传统的前馈神经网络无法结合上下文分辨出同一词汇的不同语义，而循环神经网络（RNN）是一类用于处理序列数据的神经网络，它能挖掘数据中的时序信息以及语义信息，是一种有记忆力的神经元。通过手动演算RNN的运作过程，认识到网络的输入顺序将会影响输出，这是因为它与CNN等结构不同，它不仅考虑前一时刻的输入,而且赋予了网络对前面的内容的一种“记忆”功能。RNN进行训练时会存在梯度无效和梯度爆炸等问题，为了解决这些问题有一种特殊的RNN叫做Long short-term memory，相比普通的RNN，LSTM能够在更长的序列中有更好的表现，解决了RNN训练时的梯度无效问题。同时学习了LSTM三个门的原理以及LSTM的一个计算实例，还有RNN和LSTM的实际应用领域。</p> 
<h2 id="Abstract">Abstract</h2> 
<p>In the task of semantic identification, the traditional Feedforward neural network can not distinguish different semantics of the same word in combination with the context, while Recurrent neural network (RNN) is a kind of neural network used to process sequence data. It can mine the sequence information and semantic information in data, and is a kind of neuron with memory. By manually calculating the operation process of RNN, it is realized that the input order of the network will affect the output. This is because it is different from CNN and other structures. It not only considers the input of the previous moment, but also gives the network a "memory" function for the previous content. When training RNN, there are problems such as gradient invalidity and gradient explosion. To solve these problems, there is a special RNN called Long short term memory. Compared with ordinary RNN, LSTM can perform better in longer sequences, solving the problem of gradient invalidity during RNN training. I also learned the principles of the three gates of LSTM and a computational example of LSTM, as well as the practical application fields of RNN and LSTM.</p> 
<h2 id="1%20Slot%20Filling%EF%BC%88%E6%A7%BD%E4%BD%8D%E5%A1%AB%E5%85%85%EF%BC%89">1 Slot Filling（槽位填充）</h2> 
<p>在下图的这句话中，假设有两个slot分别是目的地和到达时间，那么判断这句话中的每个单词分别属于哪个slot的任务就叫Slot Filling。比如判断出“Taipei”属于Destination，November 2nd属于time of arrival，其他词汇不属于任何一个slot。</p> 
<p></p> 
<p><img alt="0796c33601544c39afc9a74656d612f2.png" src="https://images2.imgbox.com/2d/b3/yLtx0LTJ_o.png"></p> 
<p> 通过前向反馈神经网络解决Slot Filling，将词汇转化为向量输入全连接网络，网络的输出为该词汇属于每个slot的概率。</p> 
<p><img alt="84f011a1b61743c18a54ae082092681b.png" src="https://images2.imgbox.com/72/c1/zaIyRehO_o.png"></p> 
<p>将词汇转换成向量的方法在上一周已经学到过有one-hot encoding，除此之外还有1-of-N encoding，1-of-N encoding中有一个存有很多词的词典，然后我们可以用类似one-hot编码的方式根据词典将词汇转化为向量。</p> 
<p><img alt="d5542b4c415d44d4b0289cb4215d9145.png" src="https://images2.imgbox.com/65/7c/oH5tDmnM_o.png"></p> 
<p>世界上的词汇量太大了，如果全部要进行编码，那向量的维度就会过大，因此将一些不常用的词汇都归到other这个种类。还可以采用单词的前后缀来构造向量的Word hashing方法。</p> 
<p><img alt="20b054dbf6194c61b3b499c4d19ac723.png" src="https://images2.imgbox.com/26/74/F6wDUYV4_o.png"></p> 
<p> 现在的问题是下图中给出的这句话中有两个“Taipei”，假设有两个slot分别代表目的地名和出发地名，希望神经网络可以识别一句话中相同单词的不同语义，在以往的前向神经网络中，相同的输入只能得到相同的输出。但对于人来说是可以通过前面的单词“arrive”和“leave”分辨一个单词的语义，因此我们也希望神经网络能够记住上一个输入的单词，通过前面的输入判断一个单词的不同语义。</p> 
<p><img alt="5a2c786e697b460b9d865989d4ce01e3.png" src="https://images2.imgbox.com/b1/29/fovEWZF8_o.png"></p> 
<p>因此神经网络需要有记忆力，那就用到了<strong>RNN，它的神经元是具有记忆力的。</strong></p> 
<h2 id="2%20Recurrent%20Neural%20Network%EF%BC%88%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%89">2 Recurrent Neural Network（循环神经网络）</h2> 
<h3 id="2.1%20RNN%E7%9A%84%E7%BB%93%E6%9E%84">2.1 RNN的结构</h3> 
<p>下图所示的一个全连接的神经网络，假设权值都是1，没有bias，所有激活函数均为线性函数。对于每一层的神经元都能保存当前输入通过计算得到的值，也就是输出给下一层神经元的值，当下一次输入的时候，每层神经元已经记住了上一层神经元的输出，而上一个词汇（如果不是第一个词汇的话）又具有上上个词汇的信息，以此类推，当前词汇就具有了上文所有词汇的信息，因此这时的神经元要同时考虑输入和上一层的输出。</p> 
<p><img alt="6fe5390b6d5d44eab11bfb68ddc9ff07.png" src="https://images2.imgbox.com/23/0b/KQsEfFTt_o.png"></p> 
<h3 id="2.2%20RNN%E7%9A%84%E5%88%86%E7%B1%BB">2.2 RNN的分类</h3> 
<p>Elman Network是将hide layer 的输出存入memory，Jordan Network是将整个network的输出存入memory。</p> 
<p><img alt="fb4f32067dc240ea8cd9a0c812c39b2e.png" src="https://images2.imgbox.com/60/7e/9xkiHWoD_o.png"></p> 
<p></p> 
<blockquote> 
 <p>Q：在现实应用中Jordan Network表现得比Elman Network更好，为什么Jordan Network可以得到更好的效果？</p> 
 <p>A：因为hide layer的W是没有target的，所以很难控制它学到的东西，因此放入memory的信息是不可控的，但是整个y是有target的，可以比较清楚放在memory里面的信息。最后的输出比中间层的输出具有更具体完备的特征，即信息更有用。</p> 
</blockquote> 
<h3 id="2.3%20RNN%E7%9A%84%E5%8E%9F%E7%90%86">2.3 RNN的原理</h3> 
<p> 假设现在有三个输入序列：<img alt="eq?%5Cbegin%7Bbmatrix%7D%201%5C%5C%201%20%5Cend%7Bbmatrix%7D" class="mathcode" src="https://images2.imgbox.com/4d/98/khLFQh32_o.png">、<img alt="eq?%5Cbegin%7Bbmatrix%7D%201%5C%5C%201%20%5Cend%7Bbmatrix%7D" class="mathcode" src="https://images2.imgbox.com/18/87/xQtnawiI_o.png">、<img alt="eq?%5Cbegin%7Bbmatrix%7D%202%5C%5C%202%20%5Cend%7Bbmatrix%7D" class="mathcode" src="https://images2.imgbox.com/ee/ca/qt64uTdl_o.png">，对于第一次和第二次的序列是相同的但是它们的的output却是不一样的，因为第一次输入的时候，神经元保存的是初始值0，但是第二次输入神经元的值是第一次计算的值，也就是<img alt="eq?%5Cbegin%7Bbmatrix%7D%202%5C%5C%202%20%5Cend%7Bbmatrix%7D" class="mathcode" src="https://images2.imgbox.com/4c/1e/zBbUG5d2_o.png">，所以即使是相同的输入，也会得到不同的结果。</p> 
<p><img alt="42896758ef08425fb7da858bf583483f.png" src="https://images2.imgbox.com/fb/30/Vhf8UKCr_o.png"></p> 
<blockquote> 
 <p><strong> RNN是会考虑输入的顺序的，可以发现如果调换输入的顺序，那么output序列也会改变</strong></p> 
</blockquote> 
<p>图中的网络中<strong>并不是3个RNN,而是同一个RNN在不同的时间点,使用3次。</strong></p> 
<p><img alt="c0d68bf632834ef597e51015ad7f571d.png" src="https://images2.imgbox.com/a9/e0/t1mTOOlq_o.png"></p> 
<p>两个“Taipei”前面的输入向量不一样，因此x1通过neuron的输出是不一样的，所以存到memory的内容不一样，所以两个“Taipei”的输出是不一样的。</p> 
<p> <img alt="edce28e3724f4ddcb8f968ff74eda7d6.png" src="https://images2.imgbox.com/8d/6e/TDxUprgy_o.png"></p> 
<p></p> 
<h3 id="2.4%20RNN%20can%20be%20deep">2.4 RNN can be deep</h3> 
<p>RNN可以有有多层，输入一个向量可以经过很多个hide layer 得到输出，每一个hide layer的output都会被存在memory里面，在下一个时间点的hide layer会把上一个时间点存的值读入，一直持续下去。</p> 
<p><img alt="1cb47e236fc64322bbe6e56aa76e4e7b.png" src="https://images2.imgbox.com/09/da/cTVduS5X_o.png"></p> 
<h3 id="2.5%20Bidirectional%20RNN%EF%BC%88%C2%A0%E5%8F%8C%E5%90%91RNN%EF%BC%89">2.5 Bidirectional RNN（ 双向RNN）</h3> 
<p>单向的RNN可以考虑前面的输入，但无法考虑后面的输入。双向的RNN弥补了这个缺陷，如果只有正向network，那么在产生y的时候，只能考虑x之前输入的信息，而双向的RNN不仅可以考虑前面的input，同时也可以考虑后面的input，相当于考虑了全部的input。</p> 
<p><img alt="7e2f4220474b40409d340bdaf4add1bf.png" src="https://images2.imgbox.com/19/69/VDsve8xS_o.png"></p> 
<h3 id="2.6%20RNN%E7%9A%84%E8%AE%AD%E7%BB%83">2.6 RNN的训练</h3> 
<p>RNN的训练采用gradient descend方法，然而基于RNN的网络的训练是很困难的，在很幸运的情况可能训练成功降低了loss值，可是在有的时候会出现“起起伏伏”无法训练下去的情况。</p> 
<p><img alt="895474b70a6b49c68b9c3bfed2115eac.png" src="https://images2.imgbox.com/45/b3/eY0TtSpG_o.png"></p> 
<blockquote> 
 <p>Q：为什么RNN的training有问题？</p> 
 <p>A：因为RNN在进行gradient descend的时候error surface不是太平坦就是太陡峭，在更新参数的时候，可能恰好跳过悬崖发生loss暴增。而在很平坦的区域learning rate会慢慢的调大，如果在更新过程中不下心走到了悬崖处，即橙色点处，那么很大的gradient再乘上很大的learning rate就是大幅度更新，参数就飞出了范围。</p> 
</blockquote> 
<p><img alt="729eccbabb59437c938d53edbce67b3e.png" src="https://images2.imgbox.com/48/a4/T0R95hTb_o.png"></p> 
<p></p> 
<p>一个很直观判断参数gradient大小的方法就是将<strong>该参数做微小的变化，看对它输出影响有多大</strong>。以一个简单的RNN为例，输入门和输出门的权重都是1，从forget gate到输入的权重是w，第一个时间点输入1，其他的时间点输入都是0。将1输进去时候一直乘以w，到第1000个时间的输出值为<img alt="eq?w%5E%7B999%7D" class="mathcode" src="https://images2.imgbox.com/0d/7d/Avvk5wdI_o.png">，在w=1附近可以看出来当w的值发生微小变化时，对输出值的影响却很大，因此w在等于1处有很大的的gradient，learning rate的值就需要设置得小一些，而在w在0.99处的gradient又很小，需要大一点的learning rate。</p> 
<p><img alt="0588343f4850499abab1167b69da29cc.png" src="https://images2.imgbox.com/7d/c8/9eEwJyUC_o.png"></p> 
<p> 因此可以发现在很小的区域内，gradient就会有很大的变化，这就是为什么RNN在训练的时候存在问题，这是因为<strong>RNN把同样的东西在transform的时候重复用在了每一个时间节点上</strong>，因此参数w只要发生变化就是发生两种情况，一个是没有造成任何影响，一个是造成非常大的影响，产生蝴蝶效应。</p> 
<p>如今解决RNN训练难的问题方法就是LSTM，所以我们将RNN换成了LSTM，在普通的RNN架构中，每一个时间点的memory里面的值都会被覆盖，就不会再产生影响，但LSTM是将原本memory里面的值乘上一个权重再加到input，如果权重不为0，那么一旦有一个值被存入memory造成了影响，那么这个影响将会一直存在，除非forget gate把memory清空了，因此不会存在gradient vanishing问题，让error surface不那么崎岖，可以做到将比较平坦的地方拿掉，解决梯度消失的问题，但是不能解决梯度爆炸的问题，所以在训练的时候需要使用较小的learning rate。</p> 
<h2 id="3%20Long%20Short-term%20Memory(%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86)">3 Long Short-term Memory(长短期记忆)</h2> 
<h3 id="3.1%20LSTM%E7%9A%84%E7%BB%93%E6%9E%84">3.1 LSTM的结构</h3> 
<p>LSTM是一种特殊的RNN，比普通的RNN多了三个“门”，可以让信息有选择性的通过。当一条序列很长的时候，RNN很难将从一开始的信息传送到后面的时间点，而LSTM能够学习长期依赖的信息，记住早时间点的信息，更好的联系上下文。</p> 
<p><img alt="099fcba55e8e4db2b21655eed84ce7db.png" src="https://images2.imgbox.com/8f/4b/P36Ptelr_o.png"></p> 
<blockquote> 
 <p>Input Gate：控制input数据输入</p> 
 <p>Forget Gate：控制是否保存中间结果</p> 
 <p>Output Gate：控制output数据输出</p> 
</blockquote> 
<p><img alt="eq?x%5E%7Bt%7D" class="mathcode" src="https://images2.imgbox.com/81/5c/LJHsyEQo_o.png">乘上一个matrix变成向量<img alt="eq?z" class="mathcode" src="https://images2.imgbox.com/07/4e/2tzyB4aj_o.png">，<img alt="eq?x%5E%7Bt%7D" class="mathcode" src="https://images2.imgbox.com/01/f2/j8ieM4nV_o.png">再分别乘以一个transform得到<img alt="eq?z_%7Bi%7D" class="mathcode" src="https://images2.imgbox.com/bc/05/VKF60eTQ_o.png">、<img alt="eq?z_%7Bo%7D" class="mathcode" src="https://images2.imgbox.com/a1/6d/78bgnwh8_o.png">、<img alt="eq?z_%7Bf%7D" class="mathcode" src="https://images2.imgbox.com/09/06/C7wDWmxH_o.png">，它们的维数跟LSTM的数量一样，<img alt="eq?z" class="mathcode" src="https://images2.imgbox.com/94/1c/ACD1rULo_o.png">的每一维的数值分别代表每个LSTM的input，<img alt="eq?z_%7Bi%7D" class="mathcode" src="https://images2.imgbox.com/8b/cb/6jZqCW5s_o.png">、<img alt="eq?z_%7Bo%7D" class="mathcode" src="https://images2.imgbox.com/de/01/GuzG8Fmq_o.png">、<img alt="eq?z_%7Bf%7D" class="mathcode" src="https://images2.imgbox.com/f8/9d/UHYWyifa_o.png">的每一维操纵一个LSTM的Input Gate、Output Gate、Forget Gate。</p> 
<p><img alt="26243fa67ef74991bb52be6b418be65f.png" src="https://images2.imgbox.com/41/e2/IRRVjuOg_o.png"></p> 
<blockquote> 
 <p>输入：<img alt="eq?z" class="mathcode" src="https://images2.imgbox.com/85/33/43JUy5BK_o.png"></p> 
 <p>输出门控制信号:<img alt="eq?z_%7Bo%7D" class="mathcode" src="https://images2.imgbox.com/1b/00/Miq5oq7p_o.png"></p> 
 <p>输入门控制信号:<img alt="eq?z_%7Bi%7D" class="mathcode" src="https://images2.imgbox.com/bd/54/d33TFdwJ_o.png"></p> 
 <p>遗忘门控制信号:<img alt="eq?z_%7Bf%7D" class="mathcode" src="https://images2.imgbox.com/0d/46/E7PVIGUB_o.png"></p> 
 <p>激活函数基本都是sigmoid函数，值域在0和1之间，函数值决定所控制门的开关程度,值越大，门打开程度越大。如果<img alt="eq?z_%7Bf%7D" class="mathcode" src="https://images2.imgbox.com/98/bd/wi9Tx5rC_o.png">为0，这说明存在memory中的数值不能被读取出来。Input Gate与Output Gate的开与关是由network进行学习后自行决定的，对memory里的哪些值进行forget也是由network自行学习的。</p> 
</blockquote> 
<p><img alt="51adc116b817415eb4e770f4e523c39b.png" src="https://images2.imgbox.com/e9/6a/JTlASxNQ_o.png"></p> 
<p> 将LSTM当做一个特殊的神经元，但是从图中可以看出LSTM比普通的神经元多了三个门，每个门都需要相应的参数，因此LSTM需要的参数量相当于普通神经元的4倍。</p> 
<p><img alt="05d5ab371484405da3a419708bf126ef.png" src="https://images2.imgbox.com/ff/8d/MC759r8U_o.png"></p> 
<p>其实就是原来简单的neuron换成LSTM。</p> 
<p><img alt="47362115f8454d098ebdb61a468ac4cb.png" src="https://images2.imgbox.com/8c/3d/RZSYsGbk_o.png"></p> 
<h3 id="3.2%20Multiple-layer%20LSTM">3.2 Multiple-layer LSTM</h3> 
<p>真正的 LSTM在输入的时候还会把 hidden layer 的输出<img alt="eq?h%5E%7Bt-1%7D" class="mathcode" src="https://images2.imgbox.com/34/13/J7SwzmUH_o.png"> 接上，还会加上上一个时间点 memory cell 的值 <img alt="eq?c%5E%7Bt-1%7D" class="mathcode" src="https://images2.imgbox.com/75/af/IcqKSkLq_o.png"> ，再在经过不同的 transform 得到四个不同的 z vector 去控制 LSTM，LSTM 也不止一层，可以叠多层。<br><img alt="622aed486e674649a9bd21b4bc587d96.png" src="https://images2.imgbox.com/13/f4/Zfr9lFv4_o.png"></p> 
<p></p> 
<h3 id="3.3%20LSTM%20Example">3.3 LSTM Example</h3> 
<p>当​<img alt="eq?x_%7B2%7D%3D1" class="mathcode" src="https://images2.imgbox.com/a0/e4/2u7U0EmN_o.png">​的时候，将<img alt="eq?x_%7B1%7D" class="mathcode" src="https://images2.imgbox.com/86/d0/oAPOUFLo_o.png">的的值添加到memory；</p> 
<p>当<img alt="eq?x_%7B2%7D%3D-1" class="mathcode" src="https://images2.imgbox.com/5d/fc/jqeVy0HM_o.png">的时候，将memory中的值移除；</p> 
<p>当<img alt="eq?x_%7B3%7D%3D1" class="mathcode" src="https://images2.imgbox.com/e3/05/YwvSvnAX_o.png">的时候，输出memory里的值。</p> 
<p><img alt="58bf495787b244d7b4769e3bab648b69.png" src="https://images2.imgbox.com/11/ee/zi5s0iii_o.png"></p> 
<p> 以第一个序列为例，设置输入门<img alt="eq?x_%7B2%7D" class="mathcode" src="https://images2.imgbox.com/05/38/8j7PXzBx_o.png">的权重为100，偏差值为1权重为-10，也就是当<img alt="eq?x_%7B2%7D" class="mathcode" src="https://images2.imgbox.com/8b/e5/BhtrqzWx_o.png">的值小于0.1即小于0，那么通过sigmoid函数之后输入门的值就接近0，代表门被关闭，否则可以将<img alt="eq?x_%7B1%7D" class="mathcode" src="https://images2.imgbox.com/b9/25/zDTWBCQU_o.png">的值存入memory，同理可知输出门和遗忘门的权重设置，但遗忘门的偏差为10，通常是被打开的，只有当<img alt="eq?x_%7B2%7D" class="mathcode" src="https://images2.imgbox.com/0d/5e/AAGRRha4_o.png">的值为一个很大的负数的时候才会被关闭。序列1输入，memory存的值不变，并且输出门值为0不输出，序列2输入将<img alt="eq?x_%7B1%7D" class="mathcode" src="https://images2.imgbox.com/3a/49/ddKe9fLY_o.png">的值3保存到memory，但输出门为0，memory的值不输出，序列3输入，将将<img alt="eq?x_%7B1%7D" class="mathcode" src="https://images2.imgbox.com/60/e8/ZClmZVkX_o.png">的值4保存到memory，此时memory的值为3+4=7，输出门不为0，输出memory中的值7。</p> 
<p><img alt="94c4943783a740ca8d681ef8bc4513d6.png" src="https://images2.imgbox.com/9e/9f/bej0QB3R_o.png"></p> 
<h3 id="3.4%20GRU">3.4 GRU</h3> 
<p>LSTM是RNN的升级版，加了门控装置，解决了长时记忆依赖的问题。但由于参数量复杂，带来了计算量增加，所以引进了简化版的LSTM，即GRU。</p> 
<p>GRU在LSTM的基础上减少了一个门，即用更新门代替了遗忘门和输出门，其参数更少，训练相对简单一点。而从效果上说，二者并没有优劣之分，取决于具体的任务和数据集而定。实际上来讲，二者的表现差距往往不大，远远没有调参效果明显。</p> 
<h2 id="4%20RNN%E7%9A%84%E5%BA%94%E7%94%A8">4 RNN的应用</h2> 
<ul><li>slot filling</li><li>句子情绪分析（输入一个 character sequence，output 对于整句的理解分类，例如正面或者负面）</li><li>关键词提取（给 machine 一篇文章，然后自动找出这篇文章中有哪些关键词）</li><li>语音辨识（输入输出都是序列，但是输出更短，常用的方法有CTC。定义一个空符号，在输出的时候将空符号去掉得到真正的输出）</li><li>document to sequence（sequence-to-sequence auto-encoder 方法）</li><li>阅读理解</li><li>可视化答题</li><li>语音问题回答（托福测试）</li></ul> 
<p></p> 
<h2 id="%E6%80%BB%E7%BB%93">总结</h2> 
<p>这周学习了RNN以及LSTM，不同于以往CNN等结构，RNN在全连接网络上加上了记忆功能，基础的神经网络只是在层与层之间建立了权值连接，而RNN在层之间的神经元之间也建立了权值连接，它能够处理序列变化的数据。而LSTM又在RNN的基础上加上了记忆定义的概念，在RNN中只会针对比较短的序列进行操作，而比较长的序列一般使用LSTM，LSTM适合处理序列中间间隔和延迟相对较长的问题，解决在长序列训练过程中梯度消失的问题，例如聊天机器人、语音识别等。</p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/217d3a744e325c901ed7ced426102301/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Nuxt3入门</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1a04affdd2e1e370bb843b9d43b9c2a0/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">浅谈JavaAgent</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>