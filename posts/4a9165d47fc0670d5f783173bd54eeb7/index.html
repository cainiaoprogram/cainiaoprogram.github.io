<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>面向小白的深度学习代码库，一行代码实现30&#43;中attention机制。 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="面向小白的深度学习代码库，一行代码实现30&#43;中attention机制。" />
<meta property="og:description" content="Hello，大家好，我是小马🚀🚀🚀，最近创建了一个深度学习代码库，欢迎大家来玩呀！代码库地址是https://github.com/xmu-xiaoma666/External-Attention-pytorch，目前实现了将近40个深度学习的常见算法！
For 小白（Like Me）：最近在读论文的时候会发现一个问题，有时候论文核心思想非常简单，核心代码可能也就十几行。但是打开作者release的源码时，却发现提出的模块嵌入到分类、检测、分割等任务框架中，导致代码比较冗余，对于特定任务框架不熟悉的我，很难找到核心代码，导致在论文和网络思想的理解上会有一定困难。
For 进阶者（Like You）：如果把Conv、FC、RNN这些基本单元看做小的Lego积木，把Transformer、ResNet这些结构看成已经搭好的Lego城堡。那么本项目提供的模块就是一个个具有完整语义信息的Lego组件。让科研工作者们避免反复造轮子，只需思考如何利用这些“Lego组件”，搭建出更多绚烂多彩的作品。
For 大神（May Be Like You）：能力有限，不喜轻喷！！！
For All：本项目就是要实现一个既能让深度学习小白也能搞懂，又能服务科研和工业社区的代码库。本项目的宗旨是从代码角度，实现🚀让世界上没有难读的论文🚀。
（同时也非常欢迎各位科研工作者将自己的工作的核心代码整理到本项目中，推动科研社区的发展，会在readme中注明代码的作者~）
Attention Series 1. External Attention Usage 1.1. Paper &#34;Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks&#34;
1.2. Overview 1.3. Usage Code from&amp;nbsp;model.attention.ExternalAttention&amp;nbsp; import&amp;nbsp;ExternalAttention import&amp;nbsp;torch input=torch.randn( 50, 49, 512) ea&amp;nbsp;=&amp;nbsp;ExternalAttention(d_model= 512,S= 8) output=ea(input) print(output.shape) 2. Self Attention Usage 2.1. Paper &#34;Attention Is All You Need&#34;
1.2. Overview 1.3. Usage Code from&amp;nbsp;model.attention.SelfAttention&amp;nbsp; import&amp;nbsp;ScaledDotProductAttention import&amp;nbsp;torch input=torch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/4a9165d47fc0670d5f783173bd54eeb7/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-26T11:14:25+08:00" />
<meta property="article:modified_time" content="2022-07-26T11:14:25+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">面向小白的深度学习代码库，一行代码实现30&#43;中attention机制。</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <img src="https://images2.imgbox.com/e2/28/ZqQvGJEM_o.jpg" alt="5f77ba0b3c7f6be6e3b9488cbc27e778.jpeg"> 
<p>Hello，大家好，我是小马🚀🚀🚀，最近创建了一个深度学习代码库，欢迎大家来玩呀！代码库地址是<em><strong>https://github.com/xmu-xiaoma666/External-Attention-pytorch</strong></em>，目前实现了将近40个深度学习的常见算法！</p> 
<p><em><strong>For 小白（Like Me）：</strong></em>最近在读论文的时候会发现一个问题，有时候论文核心思想非常简单，核心代码可能也就十几行。但是打开作者release的源码时，却发现提出的模块嵌入到分类、检测、分割等任务框架中，导致代码比较冗余，对于特定任务框架不熟悉的我，<strong>很难找到核心代码</strong>，导致在论文和网络思想的理解上会有一定困难。</p> 
<p><em><strong>For 进阶者（Like You）：</strong></em>如果把Conv、FC、RNN这些基本单元看做小的Lego积木，把Transformer、ResNet这些结构看成已经搭好的Lego城堡。那么本项目提供的模块就是一个个具有完整语义信息的Lego组件。<strong>让科研工作者们避免反复造轮子</strong>，只需思考如何利用这些“Lego组件”，搭建出更多绚烂多彩的作品。</p> 
<p><em><strong>For 大神（May Be Like You）：</strong></em>能力有限，<strong>不喜轻喷</strong>！！！</p> 
<p><em><strong>For All：</strong></em>本项目就是要实现一个既能<strong>让深度学习小白也能搞懂</strong>，又能<strong>服务科研和工业社区</strong>的代码库。本项目的宗旨是从代码角度，实现🚀<strong>让世界上没有难读的论文</strong>🚀。</p> 
<p>（同时也非常欢迎各位科研工作者将自己的工作的核心代码整理到本项目中，推动科研社区的发展，会在readme中注明代码的作者~）</p> 
<h2>Attention Series</h2> 
<h4>1. External Attention Usage</h4> 
<h5>1.1. Paper</h5> 
<p>"Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks"</p> 
<h5>1.2. Overview</h5> 
<img src="https://images2.imgbox.com/20/93/zaAaI3Ef_o.jpg" alt="fcf75af793e04139f085997715ed9930.jpeg"> 
<h5>1.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.ExternalAttention&amp;nbsp; 
import&amp;nbsp;ExternalAttention 
<br> 
import&amp;nbsp;torch 
<br> 
<br>input=torch.randn( 
50, 
49, 
512) 
<br>ea&amp;nbsp;=&amp;nbsp;ExternalAttention(d_model= 
512,S= 
8) 
<br>output=ea(input) 
<br>print(output.shape) 
<br> 
<h4>2. Self Attention Usage</h4> 
<h5>2.1. Paper</h5> 
<p>"Attention Is All You Need"</p> 
<h5>1.2. Overview</h5> 
<img src="https://images2.imgbox.com/cd/61/dM8MN5iV_o.jpg" alt="295fc20c148d35138f00f1b1800b7c77.jpeg"> 
<h5>1.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.SelfAttention&amp;nbsp; 
import&amp;nbsp;ScaledDotProductAttention 
<br> 
import&amp;nbsp;torch 
<br> 
<br>input=torch.randn( 
50, 
49, 
512) 
<br>sa&amp;nbsp;=&amp;nbsp;ScaledDotProductAttention(d_model= 
512,&amp;nbsp;d_k= 
512,&amp;nbsp;d_v= 
512,&amp;nbsp;h= 
8) 
<br>output=sa(input,input,input) 
<br>print(output.shape) 
<br> 
<h4>3. Simplified Self Attention Usage</h4> 
<h5>3.1. Paper</h5> 
<p>None</p> 
<h5>3.2. Overview</h5> 
<img src="https://images2.imgbox.com/76/be/lZHAxPZz_o.jpg" alt="2d794f26302a161c91e22b0c511d8d4c.jpeg"> 
<h5>3.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.SimplifiedSelfAttention&amp;nbsp; 
import&amp;nbsp;SimplifiedScaledDotProductAttention 
<br> 
import&amp;nbsp;torch 
<br> 
<br>input=torch.randn( 
50, 
49, 
512) 
<br>ssa&amp;nbsp;=&amp;nbsp;SimplifiedScaledDotProductAttention(d_model= 
512,&amp;nbsp;h= 
8) 
<br>output=ssa(input,input,input) 
<br>print(output.shape) 
<br> 
<br> 
<h4>4. Squeeze-and-Excitation Attention Usage</h4> 
<h5>4.1. Paper</h5> 
<p>"Squeeze-and-Excitation Networks"</p> 
<h5>4.2. Overview</h5> 
<img src="https://images2.imgbox.com/b3/35/jEMGcgNq_o.jpg" alt="d809f3ee689de8c5ff127d42e0233fd0.jpeg"> 
<h5>4.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.SEAttention&amp;nbsp; 
import&amp;nbsp;SEAttention 
<br> 
import&amp;nbsp;torch 
<br> 
<br>input=torch.randn( 
50, 
512, 
7, 
7) 
<br>se&amp;nbsp;=&amp;nbsp;SEAttention(channel= 
512,reduction= 
8) 
<br>output=se(input) 
<br>print(output.shape) 
<br> 
<br> 
<h4>5. SK Attention Usage</h4> 
<h5>5.1. Paper</h5> 
<p>"Selective Kernel Networks"</p> 
<h5>5.2. Overview</h5> 
<img src="https://images2.imgbox.com/9c/4a/O0JH6ssK_o.jpg" alt="fd03b5cd21f510b3fef0d5cbbf278c40.jpeg"> 
<h5>5.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.SKAttention&amp;nbsp; 
import&amp;nbsp;SKAttention 
<br> 
import&amp;nbsp;torch 
<br> 
<br>input=torch.randn( 
50, 
512, 
7, 
7) 
<br>se&amp;nbsp;=&amp;nbsp;SKAttention(channel= 
512,reduction= 
8) 
<br>output=se(input) 
<br>print(output.shape) 
<br> 
<br> 
<h4>6. CBAM Attention Usage</h4> 
<h5>6.1. Paper</h5> 
<p>"CBAM: Convolutional Block Attention Module"</p> 
<h5>6.2. Overview</h5> 
<img src="https://images2.imgbox.com/67/94/fsq9aHrR_o.jpg" alt="fb4a2eaaeb5568f75b5246e2846f5179.jpeg"> 
<img src="https://images2.imgbox.com/3e/80/VsSjS2zs_o.jpg" alt="16a05b8db84d33f045df138c04be9d0f.jpeg"> 
<h5>6.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.CBAM&amp;nbsp; 
import&amp;nbsp;CBAMBlock 
<br> 
import&amp;nbsp;torch 
<br> 
<br>input=torch.randn( 
50, 
512, 
7, 
7) 
<br>kernel_size=input.shape[ 
2] 
<br>cbam&amp;nbsp;=&amp;nbsp;CBAMBlock(channel= 
512,reduction= 
16,kernel_size=kernel_size) 
<br>output=cbam(input) 
<br>print(output.shape) 
<br> 
<br> 
<h4>7. BAM Attention Usage</h4> 
<h5>7.1. Paper</h5> 
<p>"BAM: Bottleneck Attention Module"</p> 
<h5>7.2. Overview</h5> 
<img src="https://images2.imgbox.com/d7/ab/VbcLZU0X_o.jpg" alt="a683dba114112e6401b50c5c37673e96.jpeg"> 
<h5>7.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.BAM&amp;nbsp; 
import&amp;nbsp;BAMBlock 
<br> 
import&amp;nbsp;torch 
<br> 
<br>input=torch.randn( 
50, 
512, 
7, 
7) 
<br>bam&amp;nbsp;=&amp;nbsp;BAMBlock(channel= 
512,reduction= 
16,dia_val= 
2) 
<br>output=bam(input) 
<br>print(output.shape) 
<br> 
<br> 
<h4>8. ECA Attention Usage</h4> 
<h5>8.1. Paper</h5> 
<p>"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks"</p> 
<h5>8.2. Overview</h5> 
<img src="https://images2.imgbox.com/6e/70/8gyPMTNq_o.jpg" alt="8f97606dc4d441e301da4491bba46884.jpeg"> 
<h5>8.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.ECAAttention&amp;nbsp; 
import&amp;nbsp;ECAAttention 
<br> 
import&amp;nbsp;torch 
<br> 
<br>input=torch.randn( 
50, 
512, 
7, 
7) 
<br>eca&amp;nbsp;=&amp;nbsp;ECAAttention(kernel_size= 
3) 
<br>output=eca(input) 
<br>print(output.shape) 
<br> 
<br> 
<h4>9. DANet Attention Usage</h4> 
<h5>9.1. Paper</h5> 
<p>"Dual Attention Network for Scene Segmentation"</p> 
<h5>9.2. Overview</h5> 
<img src="https://images2.imgbox.com/8a/0e/RHd2XD4n_o.jpg" alt="05ecffebd3d4f4bf74f8ca5e1bf33959.jpeg"> 
<h5>9.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.DANet&amp;nbsp; 
import&amp;nbsp;DAModule 
<br> 
import&amp;nbsp;torch 
<br> 
<br>input=torch.randn( 
50, 
512, 
7, 
7) 
<br>danet=DAModule(d_model= 
512,kernel_size= 
3,H= 
7,W= 
7) 
<br>print(danet(input).shape) 
<br> 
<br> 
<h4>10. Pyramid Split Attention Usage</h4> 
<h5>10.1. Paper</h5> 
<p>"EPSANet: An Efficient Pyramid Split Attention Block on Convolutional Neural Network"</p> 
<h5>10.2. Overview</h5> 
<img src="https://images2.imgbox.com/a1/db/2hFDH8gY_o.jpg" alt="63ebe69e1e5985648f7b2a33ec469432.jpeg"> 
<h5>10.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.PSA&amp;nbsp; 
import&amp;nbsp;PSA 
<br> 
import&amp;nbsp;torch 
<br> 
<br>input=torch.randn( 
50, 
512, 
7, 
7) 
<br>psa&amp;nbsp;=&amp;nbsp;PSA(channel= 
512,reduction= 
8) 
<br>output=psa(input) 
<br>print(output.shape) 
<br> 
<br> 
<h4>11. Efficient Multi-Head Self-Attention Usage</h4> 
<h5>11.1. Paper</h5> 
<p>"ResT: An Efficient Transformer for Visual Recognition"</p> 
<h5>11.2. Overview</h5> 
<img src="https://images2.imgbox.com/64/8f/42WUUZQQ_o.jpg" alt="62bd119bdea8bfbb8bc76941dcfb3b6d.jpeg"> 
<h5>11.3. Usage Code</h5> 
 
<br> 
from&amp;nbsp;model.attention.EMSA&amp;nbsp; 
import&amp;nbsp;EMSA 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
50, 
64, 
512) 
<br>emsa&amp;nbsp;=&amp;nbsp;EMSA(d_model= 
512,&amp;nbsp;d_k= 
512,&amp;nbsp;d_v= 
512,&amp;nbsp;h= 
8,H= 
8,W= 
8,ratio= 
2,apply_transform= 
True) 
<br>output=emsa(input,input,input) 
<br>print(output.shape) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
<br> 
<h4>12. Shuffle Attention Usage</h4> 
<h5>12.1. Paper</h5> 
<p>"SA-NET: SHUFFLE ATTENTION FOR DEEP CONVOLUTIONAL NEURAL NETWORKS"</p> 
<h5>12.2. Overview</h5> 
<img src="https://images2.imgbox.com/0f/ca/tROZT9iQ_o.jpg" alt="9312736b0068714a2567e04b7c830e58.jpeg"> 
<h5>12.3. Usage Code</h5> 
 
<br> 
from&amp;nbsp;model.attention.ShuffleAttention&amp;nbsp; 
import&amp;nbsp;ShuffleAttention 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br> 
<br>input=torch.randn( 
50, 
512, 
7, 
7) 
<br>se&amp;nbsp;=&amp;nbsp;ShuffleAttention(channel= 
512,G= 
8) 
<br>output=se(input) 
<br>print(output.shape) 
<br> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
<br> 
<h4>13. MUSE Attention Usage</h4> 
<h5>13.1. Paper</h5> 
<p>"MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning"</p> 
<h5>13.2. Overview</h5> 
<img src="https://images2.imgbox.com/7e/99/NnZOD8ii_o.jpg" alt="f853d767b471dbac9329d29ea137b596.jpeg"> 
<h5>13.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.MUSEAttention&amp;nbsp; 
import&amp;nbsp;MUSEAttention 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br> 
<br>input=torch.randn( 
50, 
49, 
512) 
<br>sa&amp;nbsp;=&amp;nbsp;MUSEAttention(d_model= 
512,&amp;nbsp;d_k= 
512,&amp;nbsp;d_v= 
512,&amp;nbsp;h= 
8) 
<br>output=sa(input,input,input) 
<br>print(output.shape) 
<br> 
<br> 
<h4>14. SGE Attention Usage</h4> 
<h5>14.1. Paper</h5> 
<p>Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks</p> 
<h5>14.2. Overview</h5> 
<img src="https://images2.imgbox.com/b5/86/rp7nZnFJ_o.jpg" alt="e3d663771820a847d6c2a07ef466f49e.jpeg"> 
<h5>14.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.SGE&amp;nbsp; 
import&amp;nbsp;SpatialGroupEnhance 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
50, 
512, 
7, 
7) 
<br>sge&amp;nbsp;=&amp;nbsp;SpatialGroupEnhance(groups= 
8) 
<br>output=sge(input) 
<br>print(output.shape) 
<br> 
<br> 
<h4>15. A2 Attention Usage</h4> 
<h5>15.1. Paper</h5> 
<p>A2-Nets: Double Attention Networks</p> 
<h5>15.2. Overview</h5> 
<img src="https://images2.imgbox.com/07/34/j9JtQuqD_o.jpg" alt="790a1a7070d786233574336b7eb4e1f6.jpeg"> 
<h5>15.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.A2Atttention&amp;nbsp; 
import&amp;nbsp;DoubleAttention 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
50, 
512, 
7, 
7) 
<br>a2&amp;nbsp;=&amp;nbsp;DoubleAttention( 
512, 
128, 
128, 
True) 
<br>output=a2(input) 
<br>print(output.shape) 
<br> 
<br> 
<h4>16. AFT Attention Usage</h4> 
<h5>16.1. Paper</h5> 
<p>An Attention Free Transformer</p> 
<h5>16.2. Overview</h5> 
<img src="https://images2.imgbox.com/e7/48/OPXreprJ_o.jpg" alt="cfb1dd18353531a2e41a3a7859cb7bdf.jpeg"> 
<h5>16.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.AFT&amp;nbsp; 
import&amp;nbsp;AFT_FULL 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
50, 
49, 
512) 
<br>aft_full&amp;nbsp;=&amp;nbsp;AFT_FULL(d_model= 
512,&amp;nbsp;n= 
49) 
<br>output=aft_full(input) 
<br>print(output.shape) 
<br> 
<br> 
<h4>17. Outlook Attention Usage</h4> 
<h5>17.1. Paper</h5> 
<p>VOLO: Vision Outlooker for Visual Recognition"</p> 
<h5>17.2. Overview</h5> 
<img src="https://images2.imgbox.com/5b/a9/RVtkPrcV_o.jpg" alt="1b9177689fe510da8d96259febdf345a.jpeg"> 
<h5>17.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.OutlookAttention&amp;nbsp; 
import&amp;nbsp;OutlookAttention 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
50, 
28, 
28, 
512) 
<br>outlook&amp;nbsp;=&amp;nbsp;OutlookAttention(dim= 
512) 
<br>output=outlook(input) 
<br>print(output.shape) 
<br> 
<br> 
<h4>18. ViP Attention Usage</h4> 
<h5>18.1. Paper</h5> 
<p>Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition"</p> 
<h5>18.2. Overview</h5> 
<img src="https://images2.imgbox.com/91/34/46MAJE8S_o.jpg" alt="18bf83a125070c1d5e898354481be168.jpeg"> 
<h5>18.3. Usage Code</h5> 
 
<br> 
from&amp;nbsp;model.attention.ViP&amp;nbsp; 
import&amp;nbsp;WeightedPermuteMLP 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
64, 
8, 
8, 
512) 
<br>seg_dim= 
8 
<br>vip=WeightedPermuteMLP( 
512,seg_dim) 
<br>out=vip(input) 
<br>print(out.shape) 
<br> 
<br> 
<h4>19. CoAtNet Attention Usage</h4> 
<h5>19.1. Paper</h5> 
<p>CoAtNet: Marrying Convolution and Attention for All Data Sizes"</p> 
<h5>19.2. Overview</h5> 
<p>None</p> 
<h5>19.3. Usage Code</h5> 
 
<br> 
from&amp;nbsp;model.attention.CoAtNet&amp;nbsp; 
import&amp;nbsp;CoAtNet 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
1, 
3, 
224, 
224) 
<br>mbconv=CoAtNet(in_ch= 
3,image_size= 
224) 
<br>out=mbconv(input) 
<br>print(out.shape) 
<br> 
<br> 
<h4>20. HaloNet Attention Usage</h4> 
<h5>20.1. Paper</h5> 
<p>Scaling Local Self-Attention for Parameter Efficient Visual Backbones"</p> 
<h5>20.2. Overview</h5> 
<img src="https://images2.imgbox.com/e3/83/RT3G9sD1_o.jpg" alt="e45e84e76330662bf67cd53e9f4fda0a.jpeg"> 
<h5>20.3. Usage Code</h5> 
 
<br> 
from&amp;nbsp;model.attention.HaloAttention&amp;nbsp; 
import&amp;nbsp;HaloAttention 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
1, 
512, 
8, 
8) 
<br>halo&amp;nbsp;=&amp;nbsp;HaloAttention(dim= 
512, 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;block_size= 
2, 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;halo_size= 
1,) 
<br>output=halo(input) 
<br>print(output.shape) 
<br> 
<br> 
<h4>21. Polarized Self-Attention Usage</h4> 
<h5>21.1. Paper</h5> 
<p>Polarized Self-Attention: Towards High-quality Pixel-wise Regression"</p> 
<h5>21.2. Overview</h5> 
<img src="https://images2.imgbox.com/f2/3f/3dmJdk4g_o.jpg" alt="753a3272f63a82f5e181940ca27c3d04.jpeg"> 
<h5>21.3. Usage Code</h5> 
 
<br> 
from&amp;nbsp;model.attention.PolarizedSelfAttention&amp;nbsp; 
import&amp;nbsp;ParallelPolarizedSelfAttention,SequentialPolarizedSelfAttention 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
1, 
512, 
7, 
7) 
<br>psa&amp;nbsp;=&amp;nbsp;SequentialPolarizedSelfAttention(channel= 
512) 
<br>output=psa(input) 
<br>print(output.shape) 
<br> 
<br> 
<br> 
<h4>22. CoTAttention Usage</h4> 
<h5>22.1. Paper</h5> 
<p>Contextual Transformer Networks for Visual Recognition---arXiv 2021.07.26</p> 
<h5>22.2. Overview</h5> 
<img src="https://images2.imgbox.com/db/a2/ZZTkwVRN_o.jpg" alt="6a4bb7d7cb71e1c079025aeb401c79f6.jpeg"> 
<h5>22.3. Usage Code</h5> 
 
<br> 
from&amp;nbsp;model.attention.CoTAttention&amp;nbsp; 
import&amp;nbsp;CoTAttention 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
50, 
512, 
7, 
7) 
<br>cot&amp;nbsp;=&amp;nbsp;CoTAttention(dim= 
512,kernel_size= 
3) 
<br>output=cot(input) 
<br>print(output.shape) 
<br> 
<br> 
<br> 
<br> 
<h4>23. Residual Attention Usage</h4> 
<h5>23.1. Paper</h5> 
<p>Residual Attention: A Simple but Effective Method for Multi-Label Recognition---ICCV2021</p> 
<h5>23.2. Overview</h5> 
<img src="https://images2.imgbox.com/47/74/SeukcPlo_o.jpg" alt="70653e28b6d3a8fb84011334ca1cd1cc.jpeg"> 
<h5>23.3. Usage Code</h5> 
 
<br> 
from&amp;nbsp;model.attention.ResidualAttention&amp;nbsp; 
import&amp;nbsp;ResidualAttention 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
50, 
512, 
7, 
7) 
<br>resatt&amp;nbsp;=&amp;nbsp;ResidualAttention(channel= 
512,num_class= 
1000,la= 
0.2) 
<br>output=resatt(input) 
<br>print(output.shape) 
<br> 
<br> 
<br> 
<br> 
<h4>24. S2 Attention Usage</h4> 
<h5>24.1. Paper</h5> 
<p>S²-MLPv2: Improved Spatial-Shift MLP Architecture for Vision---arXiv 2021.08.02</p> 
<h5>24.2. Overview</h5> 
<img src="https://images2.imgbox.com/f2/1e/SrDLB5bs_o.jpg" alt="6b3873e79f496c6360e33924f78e256c.jpeg"> 
<h5>24.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.S2Attention&amp;nbsp; 
import&amp;nbsp;S2Attention 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
50, 
512, 
7, 
7) 
<br>s2att&amp;nbsp;=&amp;nbsp;S2Attention(channels= 
512) 
<br>output=s2att(input) 
<br>print(output.shape) 
<br> 
<br> 
<h4>25. GFNet Attention Usage</h4> 
<h5>25.1. Paper</h5> 
<p>Global Filter Networks for Image Classification---arXiv 2021.07.01</p> 
<h5>25.2. Overview</h5> 
<img src="https://images2.imgbox.com/7b/4e/xcV22ZRr_o.jpg" alt="7cffe876bb5a80a3ea8d7ac9003bc181.jpeg"> 
<h5>25.3. Usage Code - Implemented by&amp;nbsp;Wenliang Zhao (Author)</h5> 
from&amp;nbsp;model.attention.gfnet&amp;nbsp; 
import&amp;nbsp;GFNet 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>x&amp;nbsp;=&amp;nbsp;torch.randn( 
1,&amp;nbsp; 
3,&amp;nbsp; 
224,&amp;nbsp; 
224) 
<br>gfnet&amp;nbsp;=&amp;nbsp;GFNet(embed_dim= 
384,&amp;nbsp;img_size= 
224,&amp;nbsp;patch_size= 
16,&amp;nbsp;num_classes= 
1000) 
<br>out&amp;nbsp;=&amp;nbsp;gfnet(x) 
<br>print(out.shape) 
<br> 
<br> 
<h4>26. TripletAttention Usage</h4> 
<h5>26.1. Paper</h5> 
<p>Rotate to Attend: Convolutional Triplet Attention Module---CVPR 2021</p> 
<h5>26.2. Overview</h5> 
<img src="https://images2.imgbox.com/d0/cf/flf3Yyp0_o.jpg" alt="103ae7c2abdaafa7ab194989c74f23c1.jpeg"> 
<h5>26.3. Usage Code - Implemented by&amp;nbsp;digantamisra98</h5> 
 
from&amp;nbsp;model.attention.TripletAttention&amp;nbsp; 
import&amp;nbsp;TripletAttention 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br>input=torch.randn( 
50, 
512, 
7, 
7) 
<br>triplet&amp;nbsp;=&amp;nbsp;TripletAttention() 
<br>output=triplet(input) 
<br>print(output.shape) 
<br> 
<h4>27. Coordinate Attention Usage</h4> 
<h5>27.1. Paper</h5> 
<p>Coordinate Attention for Efficient Mobile Network Design---CVPR 2021</p> 
<h5>27.2. Overview</h5> 
<img src="https://images2.imgbox.com/45/29/nGi19oyI_o.jpg" alt="a5929b6a14d2ea55658ce07b10112b86.jpeg"> 
<h5>27.3. Usage Code - Implemented by&amp;nbsp;Andrew-Qibin</h5> 
 
from&amp;nbsp;model.attention.CoordAttention&amp;nbsp; 
import&amp;nbsp;CoordAtt 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>inp=torch.rand([ 
2,&amp;nbsp; 
96,&amp;nbsp; 
56,&amp;nbsp; 
56]) 
<br>inp_dim,&amp;nbsp;oup_dim&amp;nbsp;=&amp;nbsp; 
96,&amp;nbsp; 
96 
<br>reduction= 
32 
<br> 
<br>coord_attention&amp;nbsp;=&amp;nbsp;CoordAtt(inp_dim,&amp;nbsp;oup_dim,&amp;nbsp;reduction=reduction) 
<br>output=coord_attention(inp) 
<br>print(output.shape) 
<br> 
<h4>28. MobileViT Attention Usage</h4> 
<h5>28.1. Paper</h5> 
<p>MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2021.10.05</p> 
<h5>28.2. Overview</h5> 
<img src="https://images2.imgbox.com/37/aa/jFQEpwLY_o.jpg" alt="dee603b5567bff8999a3fba41db77d02.jpeg"> 
<h5>28.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.MobileViTAttention&amp;nbsp; 
import&amp;nbsp;MobileViTAttention 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br> 
if&amp;nbsp;__name__&amp;nbsp;==&amp;nbsp; 
'__main__': 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;m=MobileViTAttention() 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;input=torch.randn( 
1, 
3, 
49, 
49) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;output=m(input) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;print(output.shape)&amp;nbsp;&amp;nbsp; 
<span style="font-style:italic;">#output:(1,3,49,49)</span> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
<br> 
<h4>29. ParNet Attention Usage</h4> 
<h5>29.1. Paper</h5> 
<p>Non-deep Networks---ArXiv 2021.10.20</p> 
<h5>29.2. Overview</h5> 
<img src="https://images2.imgbox.com/fe/9d/4893Ikzb_o.jpg" alt="d2021299c1a03950ff52639d0483da3e.jpeg"> 
<h5>29.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.ParNetAttention&amp;nbsp; 
import&amp;nbsp;* 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br> 
if&amp;nbsp;__name__&amp;nbsp;==&amp;nbsp; 
'__main__': 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;input=torch.randn( 
50, 
512, 
7, 
7) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;pna&amp;nbsp;=&amp;nbsp;ParNetAttention(channel= 
512) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;output=pna(input) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;print(output.shape)&amp;nbsp; 
<span style="font-style:italic;">#50,512,7,7</span> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
<br> 
<h4>30. UFO Attention Usage</h4> 
<h5>30.1. Paper</h5> 
<p>UFO-ViT: High Performance Linear Vision Transformer without Softmax---ArXiv 2021.09.29</p> 
<h5>30.2. Overview</h5> 
<img src="https://images2.imgbox.com/0b/8b/uVo0dJ0Z_o.jpg" alt="ec1ad4123ad0784c126f2d4887ef6472.jpeg"> 
<h5>30.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.UFOAttention&amp;nbsp; 
import&amp;nbsp;* 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br> 
if&amp;nbsp;__name__&amp;nbsp;==&amp;nbsp; 
'__main__': 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;input=torch.randn( 
50, 
49, 
512) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;ufo&amp;nbsp;=&amp;nbsp;UFOAttention(d_model= 
512,&amp;nbsp;d_k= 
512,&amp;nbsp;d_v= 
512,&amp;nbsp;h= 
8) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;output=ufo(input,input,input) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;print(output.shape)&amp;nbsp; 
<span style="font-style:italic;">#[50,&amp;nbsp;49,&amp;nbsp;512]</span> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
<br> 
<ul><li><br></li></ul> 
<h4>31. MobileViTv2 Attention Usage</h4> 
<h5>31.1. Paper</h5> 
<p>Separable Self-attention for Mobile Vision Transformers---ArXiv 2022.06.06</p> 
<h5>31.2. Overview</h5> 
<img src="https://images2.imgbox.com/07/35/3KfKF4ap_o.jpg" alt="2b81013e2fb3321e4c5447f40bb18623.jpeg"> 
<h5>31.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.UFOAttention&amp;nbsp; 
import&amp;nbsp;* 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br> 
if&amp;nbsp;__name__&amp;nbsp;==&amp;nbsp; 
'__main__': 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;input=torch.randn( 
50, 
49, 
512) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;ufo&amp;nbsp;=&amp;nbsp;UFOAttention(d_model= 
512,&amp;nbsp;d_k= 
512,&amp;nbsp;d_v= 
512,&amp;nbsp;h= 
8) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;output=ufo(input,input,input) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;print(output.shape)&amp;nbsp; 
<span style="font-style:italic;">#[50,&amp;nbsp;49,&amp;nbsp;512]</span> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
<br> 
<h2>Backbone Series</h2> 
<h4>1. ResNet Usage</h4> 
<h5>1.1. Paper</h5> 
<p>"Deep Residual Learning for Image Recognition---CVPR2016 Best Paper"</p> 
<h5>1.2. Overview</h5> 
<img src="https://images2.imgbox.com/d2/d2/Itm0EvjO_o.jpg" alt="8deb859a8e1aa117ddea72fc9ab2b73c.jpeg"> 
<img src="https://images2.imgbox.com/14/75/hcz8neYf_o.jpg" alt="9296caf356c172a9482a29db067a3327.jpeg"> 
<h5>1.3. Usage Code</h5> 
 
<br> 
from&amp;nbsp;model.backbone.resnet&amp;nbsp; 
import&amp;nbsp;ResNet50,ResNet101,ResNet152 
<br> 
import&amp;nbsp;torch 
<br> 
if&amp;nbsp;__name__&amp;nbsp;==&amp;nbsp; 
'__main__': 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;input=torch.randn( 
50, 
3, 
224, 
224) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;resnet50=ResNet50( 
1000) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
<span style="font-style:italic;">#&amp;nbsp;resnet101=ResNet101(1000)</span> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
<span style="font-style:italic;">#&amp;nbsp;resnet152=ResNet152(1000)</span> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;out=resnet50(input) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;print(out.shape) 
<br> 
<br> 
<h4>2. ResNeXt Usage</h4> 
<h5>2.1. Paper</h5> 
<p>"Aggregated Residual Transformations for Deep Neural Networks---CVPR2017"</p> 
<h5>2.2. Overview</h5> 
<img src="https://images2.imgbox.com/c9/f9/qUX8zMnZ_o.jpg" alt="3635637da4b46736933f2bc992913e26.jpeg"> 
<h5>2.3. Usage Code</h5> 
 
<br> 
from&amp;nbsp;model.backbone.resnext&amp;nbsp; 
import&amp;nbsp;ResNeXt50,ResNeXt101,ResNeXt152 
<br> 
import&amp;nbsp;torch 
<br> 
<br> 
if&amp;nbsp;__name__&amp;nbsp;==&amp;nbsp; 
'__main__': 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;input=torch.randn( 
50, 
3, 
224, 
224) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;resnext50=ResNeXt50( 
1000) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
<span style="font-style:italic;">#&amp;nbsp;resnext101=ResNeXt101(1000)</span> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
<span style="font-style:italic;">#&amp;nbsp;resnext152=ResNeXt152(1000)</span> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;out=resnext50(input) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;print(out.shape) 
<br> 
<br> 
<br> 
<h4>3. MobileViT Usage</h4> 
<h5>3.1. Paper</h5> 
<p>MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2020.10.05</p> 
<h5>3.2. Overview</h5> 
<img src="https://images2.imgbox.com/a1/63/vrCCO79R_o.jpg" alt="50197e84a437af9257e4df6c3051ae2c.jpeg"> 
<h5>3.3. Usage Code</h5> 
 
<br> 
from&amp;nbsp;model.backbone.MobileViT&amp;nbsp; 
import&amp;nbsp;* 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br> 
if&amp;nbsp;__name__&amp;nbsp;==&amp;nbsp; 
'__main__': 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;input=torch.randn( 
1, 
3, 
224, 
224) 
<br> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
<span style="font-style:italic;">###&amp;nbsp;mobilevit_xxs</span> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;mvit_xxs=mobilevit_xxs() 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;out=mvit_xxs(input) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;print(out.shape) 
<br> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
<span style="font-style:italic;">###&amp;nbsp;mobilevit_xs</span> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;mvit_xs=mobilevit_xs() 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;out=mvit_xs(input) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;print(out.shape) 
<br> 
<br> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
<span style="font-style:italic;">###&amp;nbsp;mobilevit_s</span> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;mvit_s=mobilevit_s() 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;out=mvit_s(input) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;print(out.shape) 
<br> 
<br> 
<h4>4. ConvMixer Usage</h4> 
<h5>4.1. Paper</h5> 
<p>Patches Are All You Need?---ICLR2022 (Under Review)</p> 
<h5>4.2. Overview</h5> 
<img src="https://images2.imgbox.com/44/80/y5gQuNyc_o.jpg" alt="5585424bc56df98d514d317debe05eaf.jpeg"> 
<h5>4.3. Usage Code</h5> 
 
<br> 
from&amp;nbsp;model.backbone.ConvMixer&amp;nbsp; 
import&amp;nbsp;* 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br> 
if&amp;nbsp;__name__&amp;nbsp;==&amp;nbsp; 
'__main__': 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;x=torch.randn( 
1, 
3, 
224, 
224) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;convmixer=ConvMixer(dim= 
512,depth= 
12) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;out=convmixer(x) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;print(out.shape)&amp;nbsp;&amp;nbsp; 
<span style="font-style:italic;">#[1,&amp;nbsp;1000]</span> 
<br> 
<br> 
<br> 
<h2>MLP Series</h2> 
<h4>1. RepMLP Usage</h4> 
<h5>1.1. Paper</h5> 
<p>"RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition"</p> 
<h5>1.2. Overview</h5> 
<img src="https://images2.imgbox.com/4f/27/3Fwyvqyy_o.jpg" alt="c59ccabd5e6060df8773e8b79338cfdc.jpeg"> 
<h5>1.3. Usage Code</h5> 
 
from&amp;nbsp;model.mlp.repmlp&amp;nbsp; 
import&amp;nbsp;RepMLP 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
<br>N= 
4&amp;nbsp; 
<span style="font-style:italic;">#batch&amp;nbsp;size</span> 
<br>C= 
512&amp;nbsp; 
<span style="font-style:italic;">#input&amp;nbsp;dim</span> 
<br>O= 
1024&amp;nbsp; 
<span style="font-style:italic;">#output&amp;nbsp;dim</span> 
<br>H= 
14&amp;nbsp; 
<span style="font-style:italic;">#image&amp;nbsp;height</span> 
<br>W= 
14&amp;nbsp; 
<span style="font-style:italic;">#image&amp;nbsp;width</span> 
<br>h= 
7&amp;nbsp; 
<span style="font-style:italic;">#patch&amp;nbsp;height</span> 
<br>w= 
7&amp;nbsp; 
<span style="font-style:italic;">#patch&amp;nbsp;width</span> 
<br>fc1_fc2_reduction= 
1&amp;nbsp; 
<span style="font-style:italic;">#reduction&amp;nbsp;ratio</span> 
<br>fc3_groups= 
8&amp;nbsp; 
<span style="font-style:italic;">#&amp;nbsp;groups</span> 
<br>repconv_kernels=[ 
1, 
3, 
5, 
7]&amp;nbsp; 
<span style="font-style:italic;">#kernel&amp;nbsp;list</span> 
<br>repmlp=RepMLP(C,O,H,W,h,w,fc1_fc2_reduction,fc3_groups,repconv_kernels=repconv_kernels) 
<br>x=torch.randn(N,C,H,W) 
<br>repmlp.eval() 
<br> 
for&amp;nbsp;module&amp;nbsp; 
in&amp;nbsp;repmlp.modules(): 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
if&amp;nbsp;isinstance(module,&amp;nbsp;nn.BatchNorm2d)&amp;nbsp; 
or&amp;nbsp;isinstance(module,&amp;nbsp;nn.BatchNorm1d): 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;nn.init.uniform_(module.running_mean,&amp;nbsp; 
0,&amp;nbsp; 
0.1) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;nn.init.uniform_(module.running_var,&amp;nbsp; 
0,&amp;nbsp; 
0.1) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;nn.init.uniform_(module.weight,&amp;nbsp; 
0,&amp;nbsp; 
0.1) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;nn.init.uniform_(module.bias,&amp;nbsp; 
0,&amp;nbsp; 
0.1) 
<br> 
<br> 
<span style="font-style:italic;">#training&amp;nbsp;result</span> 
<br>out=repmlp(x) 
<br> 
<span style="font-style:italic;">#inference&amp;nbsp;result</span> 
<br>repmlp.switch_to_deploy() 
<br>deployout&amp;nbsp;=&amp;nbsp;repmlp(x) 
<br> 
<br>print(((deployout-out)** 
2).sum()) 
<br> 
<h4>2. MLP-Mixer Usage</h4> 
<h5>2.1. Paper</h5> 
<p>"MLP-Mixer: An all-MLP Architecture for Vision"</p> 
<h5>2.2. Overview</h5> 
<img src="https://images2.imgbox.com/ad/55/s13k8Sh5_o.jpg" alt="92a385825da4fc31ccd64656ca02c814.jpeg"> 
<h5>2.3. Usage Code</h5> 
 
from&amp;nbsp;model.mlp.mlp_mixer&amp;nbsp; 
import&amp;nbsp;MlpMixer 
<br> 
import&amp;nbsp;torch 
<br>mlp_mixer=MlpMixer(num_classes= 
1000,num_blocks= 
10,patch_size= 
10,tokens_hidden_dim= 
32,channels_hidden_dim= 
1024,tokens_mlp_dim= 
16,channels_mlp_dim= 
1024) 
<br>input=torch.randn( 
50, 
3, 
40, 
40) 
<br>output=mlp_mixer(input) 
<br>print(output.shape) 
<br> 
<h4>3. ResMLP Usage</h4> 
<h5>3.1. Paper</h5> 
<p>"ResMLP: Feedforward networks for image classification with data-efficient training"</p> 
<h5>3.2. Overview</h5> 
<img src="https://images2.imgbox.com/76/ca/brz1ox74_o.jpg" alt="32f6996663646c4c154014a4891ed4d9.jpeg"> 
<h5>3.3. Usage Code</h5> 
 
from&amp;nbsp;model.mlp.resmlp&amp;nbsp; 
import&amp;nbsp;ResMLP 
<br> 
import&amp;nbsp;torch 
<br> 
<br>input=torch.randn( 
50, 
3, 
14, 
14) 
<br>resmlp=ResMLP(dim= 
128,image_size= 
14,patch_size= 
7,class_num= 
1000) 
<br>out=resmlp(input) 
<br>print(out.shape)&amp;nbsp; 
<span style="font-style:italic;">#the&amp;nbsp;last&amp;nbsp;dimention&amp;nbsp;is&amp;nbsp;class_num</span> 
<br> 
<h4>4. gMLP Usage</h4> 
<h5>4.1. Paper</h5> 
<p>"Pay Attention to MLPs"</p> 
<h5>4.2. Overview</h5> 
<img src="https://images2.imgbox.com/a3/1d/nAv9VXpc_o.jpg" alt="e5d3387d845820423325dc55ce282ebb.jpeg"> 
<h5>4.3. Usage Code</h5> 
 
from&amp;nbsp;model.mlp.g_mlp&amp;nbsp; 
import&amp;nbsp;gMLP 
<br> 
import&amp;nbsp;torch 
<br> 
<br>num_tokens= 
10000 
<br>bs= 
50 
<br>len_sen= 
49 
<br>num_layers= 
6 
<br>input=torch.randint(num_tokens,(bs,len_sen))&amp;nbsp; 
<span style="font-style:italic;">#bs,len_sen</span> 
<br>gmlp&amp;nbsp;=&amp;nbsp;gMLP(num_tokens=num_tokens,len_sen=len_sen,dim= 
512,d_ff= 
1024) 
<br>output=gmlp(input) 
<br>print(output.shape) 
<br> 
<h4>5. sMLP Usage</h4> 
<h5>5.1. Paper</h5> 
<p>"Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?"</p> 
<h5>5.2. Overview</h5> 
<img src="https://images2.imgbox.com/e9/96/AxGRGjw4_o.jpg" alt="c171cc2f378be86e49a2dbd678707772.jpeg"> 
<h5>5.3. Usage Code</h5> 
 
from&amp;nbsp;model.mlp.sMLP_block&amp;nbsp; 
import&amp;nbsp;sMLPBlock 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br> 
if&amp;nbsp;__name__&amp;nbsp;==&amp;nbsp; 
'__main__': 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;input=torch.randn( 
50, 
3, 
224, 
224) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;smlp=sMLPBlock(h= 
224,w= 
224) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;out=smlp(input) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;print(out.shape) 
<br> 
<h2>Re-Parameter Series</h2> 
<h4>1. RepVGG Usage</h4> 
<h5>1.1. Paper</h5> 
<p>"RepVGG: Making VGG-style ConvNets Great Again"</p> 
<h5>1.2. Overview</h5> 
<img src="https://images2.imgbox.com/99/ab/HiMusVAK_o.jpg" alt="87f8ea54d81fdb6207bc19ebb939a1c3.jpeg"> 
<h5>1.3. Usage Code</h5> 
 
<br> 
from&amp;nbsp;model.rep.repvgg&amp;nbsp; 
import&amp;nbsp;RepBlock 
<br> 
import&amp;nbsp;torch 
<br> 
<br> 
<br>input=torch.randn( 
50, 
512, 
49, 
49) 
<br>repblock=RepBlock( 
512, 
512) 
<br>repblock.eval() 
<br>out=repblock(input) 
<br>repblock._switch_to_deploy() 
<br>out2=repblock(input) 
<br>print( 
'difference&amp;nbsp;between&amp;nbsp;vgg&amp;nbsp;and&amp;nbsp;repvgg') 
<br>print(((out2-out)** 
2).sum()) 
<br> 
<h4>2. ACNet Usage</h4> 
<h5>2.1. Paper</h5> 
<p>"ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks"</p> 
<h5>2.2. Overview</h5> 
<img src="https://images2.imgbox.com/70/86/jdFhHHnq_o.jpg" alt="f44742ab99f8c0122903ae293ef813cf.jpeg"> 
<h5>2.3. Usage Code</h5> 
 
from&amp;nbsp;model.rep.acnet&amp;nbsp; 
import&amp;nbsp;ACNet 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
<br>input=torch.randn( 
50, 
512, 
49, 
49) 
<br>acnet=ACNet( 
512, 
512) 
<br>acnet.eval() 
<br>out=acnet(input) 
<br>acnet._switch_to_deploy() 
<br>out2=acnet(input) 
<br>print( 
'difference:') 
<br>print(((out2-out)** 
2).sum()) 
<br> 
<br> 
<h4>2. Diverse Branch Block Usage</h4> 
<h5>2.1. Paper</h5> 
<p>"Diverse Branch Block: Building a Convolution as an Inception-like Unit"</p> 
<h5>2.2. Overview</h5> 
<img src="https://images2.imgbox.com/a2/51/XvcHvek1_o.jpg" alt="5408daffd5e00cf8fcffc7f30f8f2993.jpeg"> 
<h5>2.3. Usage Code</h5> 
<h6>2.3.1 Transform I</h6> 
 
from&amp;nbsp;model.rep.ddb&amp;nbsp; 
import&amp;nbsp;transI_conv_bn 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
1, 
64, 
7, 
7) 
<br> 
<span style="font-style:italic;">#conv+bn</span> 
<br>conv1=nn.Conv2d( 
64, 
64, 
3,padding= 
1) 
<br>bn1=nn.BatchNorm2d( 
64) 
<br>bn1.eval() 
<br>out1=bn1(conv1(input)) 
<br> 
<br> 
<span style="font-style:italic;">#conv_fuse</span> 
<br>conv_fuse=nn.Conv2d( 
64, 
64, 
3,padding= 
1) 
<br>conv_fuse.weight.data,conv_fuse.bias.data=transI_conv_bn(conv1,bn1) 
<br>out2=conv_fuse(input) 
<br> 
<br>print( 
"difference:",((out2-out1)** 
2).sum().item()) 
<br> 
<h6>2.3.2 Transform II</h6> 
 
from&amp;nbsp;model.rep.ddb&amp;nbsp; 
import&amp;nbsp;transII_conv_branch 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
1, 
64, 
7, 
7) 
<br> 
<br> 
<span style="font-style:italic;">#conv+conv</span> 
<br>conv1=nn.Conv2d( 
64, 
64, 
3,padding= 
1) 
<br>conv2=nn.Conv2d( 
64, 
64, 
3,padding= 
1) 
<br>out1=conv1(input)+conv2(input) 
<br> 
<br> 
<span style="font-style:italic;">#conv_fuse</span> 
<br>conv_fuse=nn.Conv2d( 
64, 
64, 
3,padding= 
1) 
<br>conv_fuse.weight.data,conv_fuse.bias.data=transII_conv_branch(conv1,conv2) 
<br>out2=conv_fuse(input) 
<br> 
<br>print( 
"difference:",((out2-out1)** 
2).sum().item()) 
<br> 
<h6>2.3.3 Transform III</h6> 
 
from&amp;nbsp;model.rep.ddb&amp;nbsp; 
import&amp;nbsp;transIII_conv_sequential 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
1, 
64, 
7, 
7) 
<br> 
<br> 
<span style="font-style:italic;">#conv+conv</span> 
<br>conv1=nn.Conv2d( 
64, 
64, 
1,padding= 
0,bias= 
False) 
<br>conv2=nn.Conv2d( 
64, 
64, 
3,padding= 
1,bias= 
False) 
<br>out1=conv2(conv1(input)) 
<br> 
<br> 
<br> 
<span style="font-style:italic;">#conv_fuse</span> 
<br>conv_fuse=nn.Conv2d( 
64, 
64, 
3,padding= 
1,bias= 
False) 
<br>conv_fuse.weight.data=transIII_conv_sequential(conv1,conv2) 
<br>out2=conv_fuse(input) 
<br> 
<br>print( 
"difference:",((out2-out1)** 
2).sum().item()) 
<br> 
<h6>2.3.4 Transform IV</h6> 
 
from&amp;nbsp;model.rep.ddb&amp;nbsp; 
import&amp;nbsp;transIV_conv_concat 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
1, 
64, 
7, 
7) 
<br> 
<br> 
<span style="font-style:italic;">#conv+conv</span> 
<br>conv1=nn.Conv2d( 
64, 
32, 
3,padding= 
1) 
<br>conv2=nn.Conv2d( 
64, 
32, 
3,padding= 
1) 
<br>out1=torch.cat([conv1(input),conv2(input)],dim= 
1) 
<br> 
<br> 
<span style="font-style:italic;">#conv_fuse</span> 
<br>conv_fuse=nn.Conv2d( 
64, 
64, 
3,padding= 
1) 
<br>conv_fuse.weight.data,conv_fuse.bias.data=transIV_conv_concat(conv1,conv2) 
<br>out2=conv_fuse(input) 
<br> 
<br>print( 
"difference:",((out2-out1)** 
2).sum().item()) 
<br> 
<h6>2.3.5 Transform V</h6> 
 
from&amp;nbsp;model.rep.ddb&amp;nbsp; 
import&amp;nbsp;transV_avg 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
1, 
64, 
7, 
7) 
<br> 
<br>avg=nn.AvgPool2d(kernel_size= 
3,stride= 
1) 
<br>out1=avg(input) 
<br> 
<br>conv=transV_avg( 
64, 
3) 
<br>out2=conv(input) 
<br> 
<br>print( 
"difference:",((out2-out1)** 
2).sum().item()) 
<br> 
<h6>2.3.6 Transform VI</h6> 
 
from&amp;nbsp;model.rep.ddb&amp;nbsp; 
import&amp;nbsp;transVI_conv_scale 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
1, 
64, 
7, 
7) 
<br> 
<br> 
<span style="font-style:italic;">#conv+conv</span> 
<br>conv1x1=nn.Conv2d( 
64, 
64, 
1) 
<br>conv1x3=nn.Conv2d( 
64, 
64,( 
1, 
3),padding=( 
0, 
1)) 
<br>conv3x1=nn.Conv2d( 
64, 
64,( 
3, 
1),padding=( 
1, 
0)) 
<br>out1=conv1x1(input)+conv1x3(input)+conv3x1(input) 
<br> 
<br> 
<span style="font-style:italic;">#conv_fuse</span> 
<br>conv_fuse=nn.Conv2d( 
64, 
64, 
3,padding= 
1) 
<br>conv_fuse.weight.data,conv_fuse.bias.data=transVI_conv_scale(conv1x1,conv1x3,conv3x1) 
<br>out2=conv_fuse(input) 
<br> 
<br>print( 
"difference:",((out2-out1)** 
2).sum().item()) 
<br> 
<h2>Convolution Series</h2> 
<p><span style="font-size:20px;font-weight:bold;">1. Depthwise Separable Convolution Usage</span><br></p> 
<h5>1.1. Paper</h5> 
<p>"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"</p> 
<h5>1.2. Overview</h5> 
<img src="https://images2.imgbox.com/2b/63/3Pvko9Dw_o.jpg" alt="b47d48e0c0dc9c6c4af28d7661605bb0.jpeg"> 
<h5>1.3. Usage Code</h5> 
 
from&amp;nbsp;model.conv.DepthwiseSeparableConvolution&amp;nbsp; 
import&amp;nbsp;DepthwiseSeparableConvolution 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
1, 
3, 
224, 
224) 
<br>dsconv=DepthwiseSeparableConvolution( 
3, 
64) 
<br>out=dsconv(input) 
<br>print(out.shape) 
<br> 
<h4>2. MBConv Usage</h4> 
<h5>2.1. Paper</h5> 
<p>"Efficientnet: Rethinking model scaling for convolutional neural networks"</p> 
<h5>2.2. Overview</h5> 
<img src="https://images2.imgbox.com/89/30/kS10DQsx_o.jpg" alt="17ec969b7a17a9d884d3abd6ab748db0.jpeg"> 
<h5>2.3. Usage Code</h5> 
 
from&amp;nbsp;model.conv.MBConv&amp;nbsp; 
import&amp;nbsp;MBConvBlock 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
1, 
3, 
224, 
224) 
<br>mbconv=MBConvBlock(ksize= 
3,input_filters= 
3,output_filters= 
512,image_size= 
224) 
<br>out=mbconv(input) 
<br>print(out.shape) 
<br> 
<br> 
<br> 
<h4>3. Involution Usage</h4> 
<h5>3.1. Paper</h5> 
<p>"Involution: Inverting the Inherence of Convolution for Visual Recognition"</p> 
<h5>3.2. Overview</h5> 
<img src="https://images2.imgbox.com/f2/df/VxcMSNUn_o.jpg" alt="c44c9c9f336835de973adb3fc53e28e7.jpeg"> 
<h5>3.3. Usage Code</h5> 
 
from&amp;nbsp;model.conv.Involution&amp;nbsp; 
import&amp;nbsp;Involution 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
1, 
4, 
64, 
64) 
<br>involution=Involution(kernel_size= 
3,in_channel= 
4,stride= 
2) 
<br>out=involution(input) 
<br>print(out.shape) 
<br> 
<h4>4. DynamicConv Usage</h4> 
<h5>4.1. Paper</h5> 
<p>"Dynamic Convolution: Attention over Convolution Kernels"</p> 
<h5>4.2. Overview</h5> 
<img src="https://images2.imgbox.com/8b/92/LL2w96D3_o.jpg" alt="54a23b6088f71d4bc61071efc891352e.jpeg"> 
<h5>4.3. Usage Code</h5> 
 
from&amp;nbsp;model.conv.DynamicConv&amp;nbsp; 
import&amp;nbsp;* 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br> 
if&amp;nbsp;__name__&amp;nbsp;==&amp;nbsp; 
'__main__': 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;input=torch.randn( 
2, 
32, 
64, 
64) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;m=DynamicConv(in_planes= 
32,out_planes= 
64,kernel_size= 
3,stride= 
1,padding= 
1,bias= 
False) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;out=m(input) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;print(out.shape)&amp;nbsp; 
<span style="font-style:italic;">#&amp;nbsp;2,32,64,64</span> 
<br> 
<br> 
<h4>5. CondConv Usage</h4> 
<h5>5.1. Paper</h5> 
<p>"CondConv: Conditionally Parameterized Convolut ions for Efficient Inference"</p> 
<h5>5.2. Overview</h5> 
<img src="https://images2.imgbox.com/93/81/LTjegfMX_o.jpg" alt="30bb6fe0c5ded0ce3e035280d8b92455.jpeg"> 
<h5>5.3. Usage Code</h5> 
 
from&amp;nbsp;model.conv.CondConv&amp;nbsp; 
import&amp;nbsp;* 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br> 
if&amp;nbsp;__name__&amp;nbsp;==&amp;nbsp; 
'__main__': 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;input=torch.randn( 
2, 
32, 
64, 
64) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;m=CondConv(in_planes= 
32,out_planes= 
64,kernel_size= 
3,stride= 
1,padding= 
1,bias= 
False) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;out=m(input) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;print(out.shape) 
<br> 
<br> 
<p>已建立深度学习公众号——<strong>FightingCV</strong>，欢迎大家关注！！！</p> 
<p><strong>ICCV、CVPR、NeurIPS、ICML论文解析汇总：https://github.com/xmu-xiaoma666/FightingCV-Paper-Reading</strong></p> 
<p><strong>面向小白的Attention、重参数、MLP、卷积核心代码学习：https://github.com/xmu-xiaoma666/External-Attention-pytorch</strong></p> 
<p>加入交流群，请添加小助手wx：<strong>FightngCV666</strong></p> 
<p><br></p> 
<p><br></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8667216149efdbdc3974976c1236f87a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">IDEA一些常用设置及插件</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1182ef1826d2a1851406aff417aa09f3/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">webapi接口文件下载时跨域问题</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>