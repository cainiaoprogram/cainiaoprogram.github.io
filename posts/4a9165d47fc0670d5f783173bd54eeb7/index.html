<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>é¢å‘å°ç™½çš„æ·±åº¦å­¦ä¹ ä»£ç åº“ï¼Œä¸€è¡Œä»£ç å®ç°30&#43;ä¸­attentionæœºåˆ¶ã€‚ - èœé¸Ÿç¨‹åºå‘˜åšå®¢</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="é¢å‘å°ç™½çš„æ·±åº¦å­¦ä¹ ä»£ç åº“ï¼Œä¸€è¡Œä»£ç å®ç°30&#43;ä¸­attentionæœºåˆ¶ã€‚" />
<meta property="og:description" content="Helloï¼Œå¤§å®¶å¥½ï¼Œæˆ‘æ˜¯å°é©¬ğŸš€ğŸš€ğŸš€ï¼Œæœ€è¿‘åˆ›å»ºäº†ä¸€ä¸ªæ·±åº¦å­¦ä¹ ä»£ç åº“ï¼Œæ¬¢è¿å¤§å®¶æ¥ç©å‘€ï¼ä»£ç åº“åœ°å€æ˜¯https://github.com/xmu-xiaoma666/External-Attention-pytorchï¼Œç›®å‰å®ç°äº†å°†è¿‘40ä¸ªæ·±åº¦å­¦ä¹ çš„å¸¸è§ç®—æ³•ï¼
For å°ç™½ï¼ˆLike Meï¼‰ï¼šæœ€è¿‘åœ¨è¯»è®ºæ–‡çš„æ—¶å€™ä¼šå‘ç°ä¸€ä¸ªé—®é¢˜ï¼Œæœ‰æ—¶å€™è®ºæ–‡æ ¸å¿ƒæ€æƒ³éå¸¸ç®€å•ï¼Œæ ¸å¿ƒä»£ç å¯èƒ½ä¹Ÿå°±åå‡ è¡Œã€‚ä½†æ˜¯æ‰“å¼€ä½œè€…releaseçš„æºç æ—¶ï¼Œå´å‘ç°æå‡ºçš„æ¨¡å—åµŒå…¥åˆ°åˆ†ç±»ã€æ£€æµ‹ã€åˆ†å‰²ç­‰ä»»åŠ¡æ¡†æ¶ä¸­ï¼Œå¯¼è‡´ä»£ç æ¯”è¾ƒå†—ä½™ï¼Œå¯¹äºç‰¹å®šä»»åŠ¡æ¡†æ¶ä¸ç†Ÿæ‚‰çš„æˆ‘ï¼Œå¾ˆéš¾æ‰¾åˆ°æ ¸å¿ƒä»£ç ï¼Œå¯¼è‡´åœ¨è®ºæ–‡å’Œç½‘ç»œæ€æƒ³çš„ç†è§£ä¸Šä¼šæœ‰ä¸€å®šå›°éš¾ã€‚
For è¿›é˜¶è€…ï¼ˆLike Youï¼‰ï¼šå¦‚æœæŠŠConvã€FCã€RNNè¿™äº›åŸºæœ¬å•å…ƒçœ‹åšå°çš„Legoç§¯æœ¨ï¼ŒæŠŠTransformerã€ResNetè¿™äº›ç»“æ„çœ‹æˆå·²ç»æ­å¥½çš„LegoåŸå ¡ã€‚é‚£ä¹ˆæœ¬é¡¹ç›®æä¾›çš„æ¨¡å—å°±æ˜¯ä¸€ä¸ªä¸ªå…·æœ‰å®Œæ•´è¯­ä¹‰ä¿¡æ¯çš„Legoç»„ä»¶ã€‚è®©ç§‘ç ”å·¥ä½œè€…ä»¬é¿å…åå¤é€ è½®å­ï¼Œåªéœ€æ€è€ƒå¦‚ä½•åˆ©ç”¨è¿™äº›â€œLegoç»„ä»¶â€ï¼Œæ­å»ºå‡ºæ›´å¤šç»šçƒ‚å¤šå½©çš„ä½œå“ã€‚
For å¤§ç¥ï¼ˆMay Be Like Youï¼‰ï¼šèƒ½åŠ›æœ‰é™ï¼Œä¸å–œè½»å–·ï¼ï¼ï¼
For Allï¼šæœ¬é¡¹ç›®å°±æ˜¯è¦å®ç°ä¸€ä¸ªæ—¢èƒ½è®©æ·±åº¦å­¦ä¹ å°ç™½ä¹Ÿèƒ½ææ‡‚ï¼Œåˆèƒ½æœåŠ¡ç§‘ç ”å’Œå·¥ä¸šç¤¾åŒºçš„ä»£ç åº“ã€‚æœ¬é¡¹ç›®çš„å®—æ—¨æ˜¯ä»ä»£ç è§’åº¦ï¼Œå®ç°ğŸš€è®©ä¸–ç•Œä¸Šæ²¡æœ‰éš¾è¯»çš„è®ºæ–‡ğŸš€ã€‚
ï¼ˆåŒæ—¶ä¹Ÿéå¸¸æ¬¢è¿å„ä½ç§‘ç ”å·¥ä½œè€…å°†è‡ªå·±çš„å·¥ä½œçš„æ ¸å¿ƒä»£ç æ•´ç†åˆ°æœ¬é¡¹ç›®ä¸­ï¼Œæ¨åŠ¨ç§‘ç ”ç¤¾åŒºçš„å‘å±•ï¼Œä¼šåœ¨readmeä¸­æ³¨æ˜ä»£ç çš„ä½œè€…~ï¼‰
Attention Series 1. External Attention Usage 1.1. Paper &#34;Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks&#34;
1.2. Overview 1.3. Usage Code from&amp;nbsp;model.attention.ExternalAttention&amp;nbsp; import&amp;nbsp;ExternalAttention import&amp;nbsp;torch input=torch.randn( 50, 49, 512) ea&amp;nbsp;=&amp;nbsp;ExternalAttention(d_model= 512,S= 8) output=ea(input) print(output.shape) 2. Self Attention Usage 2.1. Paper &#34;Attention Is All You Need&#34;
1.2. Overview 1.3. Usage Code from&amp;nbsp;model.attention.SelfAttention&amp;nbsp; import&amp;nbsp;ScaledDotProductAttention import&amp;nbsp;torch input=torch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/4a9165d47fc0670d5f783173bd54eeb7/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-26T11:14:25+08:00" />
<meta property="article:modified_time" content="2022-07-26T11:14:25+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="èœé¸Ÿç¨‹åºå‘˜åšå®¢" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">èœé¸Ÿç¨‹åºå‘˜åšå®¢</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">é¢å‘å°ç™½çš„æ·±åº¦å­¦ä¹ ä»£ç åº“ï¼Œä¸€è¡Œä»£ç å®ç°30&#43;ä¸­attentionæœºåˆ¶ã€‚</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <img src="https://images2.imgbox.com/e2/28/ZqQvGJEM_o.jpg" alt="5f77ba0b3c7f6be6e3b9488cbc27e778.jpeg"> 
<p>Helloï¼Œå¤§å®¶å¥½ï¼Œæˆ‘æ˜¯å°é©¬ğŸš€ğŸš€ğŸš€ï¼Œæœ€è¿‘åˆ›å»ºäº†ä¸€ä¸ªæ·±åº¦å­¦ä¹ ä»£ç åº“ï¼Œæ¬¢è¿å¤§å®¶æ¥ç©å‘€ï¼ä»£ç åº“åœ°å€æ˜¯<em><strong>https://github.com/xmu-xiaoma666/External-Attention-pytorch</strong></em>ï¼Œç›®å‰å®ç°äº†å°†è¿‘40ä¸ªæ·±åº¦å­¦ä¹ çš„å¸¸è§ç®—æ³•ï¼</p> 
<p><em><strong>For å°ç™½ï¼ˆLike Meï¼‰ï¼š</strong></em>æœ€è¿‘åœ¨è¯»è®ºæ–‡çš„æ—¶å€™ä¼šå‘ç°ä¸€ä¸ªé—®é¢˜ï¼Œæœ‰æ—¶å€™è®ºæ–‡æ ¸å¿ƒæ€æƒ³éå¸¸ç®€å•ï¼Œæ ¸å¿ƒä»£ç å¯èƒ½ä¹Ÿå°±åå‡ è¡Œã€‚ä½†æ˜¯æ‰“å¼€ä½œè€…releaseçš„æºç æ—¶ï¼Œå´å‘ç°æå‡ºçš„æ¨¡å—åµŒå…¥åˆ°åˆ†ç±»ã€æ£€æµ‹ã€åˆ†å‰²ç­‰ä»»åŠ¡æ¡†æ¶ä¸­ï¼Œå¯¼è‡´ä»£ç æ¯”è¾ƒå†—ä½™ï¼Œå¯¹äºç‰¹å®šä»»åŠ¡æ¡†æ¶ä¸ç†Ÿæ‚‰çš„æˆ‘ï¼Œ<strong>å¾ˆéš¾æ‰¾åˆ°æ ¸å¿ƒä»£ç </strong>ï¼Œå¯¼è‡´åœ¨è®ºæ–‡å’Œç½‘ç»œæ€æƒ³çš„ç†è§£ä¸Šä¼šæœ‰ä¸€å®šå›°éš¾ã€‚</p> 
<p><em><strong>For è¿›é˜¶è€…ï¼ˆLike Youï¼‰ï¼š</strong></em>å¦‚æœæŠŠConvã€FCã€RNNè¿™äº›åŸºæœ¬å•å…ƒçœ‹åšå°çš„Legoç§¯æœ¨ï¼ŒæŠŠTransformerã€ResNetè¿™äº›ç»“æ„çœ‹æˆå·²ç»æ­å¥½çš„LegoåŸå ¡ã€‚é‚£ä¹ˆæœ¬é¡¹ç›®æä¾›çš„æ¨¡å—å°±æ˜¯ä¸€ä¸ªä¸ªå…·æœ‰å®Œæ•´è¯­ä¹‰ä¿¡æ¯çš„Legoç»„ä»¶ã€‚<strong>è®©ç§‘ç ”å·¥ä½œè€…ä»¬é¿å…åå¤é€ è½®å­</strong>ï¼Œåªéœ€æ€è€ƒå¦‚ä½•åˆ©ç”¨è¿™äº›â€œLegoç»„ä»¶â€ï¼Œæ­å»ºå‡ºæ›´å¤šç»šçƒ‚å¤šå½©çš„ä½œå“ã€‚</p> 
<p><em><strong>For å¤§ç¥ï¼ˆMay Be Like Youï¼‰ï¼š</strong></em>èƒ½åŠ›æœ‰é™ï¼Œ<strong>ä¸å–œè½»å–·</strong>ï¼ï¼ï¼</p> 
<p><em><strong>For Allï¼š</strong></em>æœ¬é¡¹ç›®å°±æ˜¯è¦å®ç°ä¸€ä¸ªæ—¢èƒ½<strong>è®©æ·±åº¦å­¦ä¹ å°ç™½ä¹Ÿèƒ½ææ‡‚</strong>ï¼Œåˆèƒ½<strong>æœåŠ¡ç§‘ç ”å’Œå·¥ä¸šç¤¾åŒº</strong>çš„ä»£ç åº“ã€‚æœ¬é¡¹ç›®çš„å®—æ—¨æ˜¯ä»ä»£ç è§’åº¦ï¼Œå®ç°ğŸš€<strong>è®©ä¸–ç•Œä¸Šæ²¡æœ‰éš¾è¯»çš„è®ºæ–‡</strong>ğŸš€ã€‚</p> 
<p>ï¼ˆåŒæ—¶ä¹Ÿéå¸¸æ¬¢è¿å„ä½ç§‘ç ”å·¥ä½œè€…å°†è‡ªå·±çš„å·¥ä½œçš„æ ¸å¿ƒä»£ç æ•´ç†åˆ°æœ¬é¡¹ç›®ä¸­ï¼Œæ¨åŠ¨ç§‘ç ”ç¤¾åŒºçš„å‘å±•ï¼Œä¼šåœ¨readmeä¸­æ³¨æ˜ä»£ç çš„ä½œè€…~ï¼‰</p> 
<h2>Attention Series</h2> 
<h4>1. External Attention Usage</h4> 
<h5>1.1. Paper</h5> 
<p>"Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks"</p> 
<h5>1.2. Overview</h5> 
<img src="https://images2.imgbox.com/20/93/zaAaI3Ef_o.jpg" alt="fcf75af793e04139f085997715ed9930.jpeg"> 
<h5>1.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.ExternalAttention&amp;nbsp; 
import&amp;nbsp;ExternalAttention 
<br> 
import&amp;nbsp;torch 
<br> 
<br>input=torch.randn( 
50, 
49, 
512) 
<br>ea&amp;nbsp;=&amp;nbsp;ExternalAttention(d_model= 
512,S= 
8) 
<br>output=ea(input) 
<br>print(output.shape) 
<br> 
<h4>2. Self Attention Usage</h4> 
<h5>2.1. Paper</h5> 
<p>"Attention Is All You Need"</p> 
<h5>1.2. Overview</h5> 
<img src="https://images2.imgbox.com/cd/61/dM8MN5iV_o.jpg" alt="295fc20c148d35138f00f1b1800b7c77.jpeg"> 
<h5>1.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.SelfAttention&amp;nbsp; 
import&amp;nbsp;ScaledDotProductAttention 
<br> 
import&amp;nbsp;torch 
<br> 
<br>input=torch.randn( 
50, 
49, 
512) 
<br>sa&amp;nbsp;=&amp;nbsp;ScaledDotProductAttention(d_model= 
512,&amp;nbsp;d_k= 
512,&amp;nbsp;d_v= 
512,&amp;nbsp;h= 
8) 
<br>output=sa(input,input,input) 
<br>print(output.shape) 
<br> 
<h4>3. Simplified Self Attention Usage</h4> 
<h5>3.1. Paper</h5> 
<p>None</p> 
<h5>3.2. Overview</h5> 
<img src="https://images2.imgbox.com/76/be/lZHAxPZz_o.jpg" alt="2d794f26302a161c91e22b0c511d8d4c.jpeg"> 
<h5>3.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.SimplifiedSelfAttention&amp;nbsp; 
import&amp;nbsp;SimplifiedScaledDotProductAttention 
<br> 
import&amp;nbsp;torch 
<br> 
<br>input=torch.randn( 
50, 
49, 
512) 
<br>ssa&amp;nbsp;=&amp;nbsp;SimplifiedScaledDotProductAttention(d_model= 
512,&amp;nbsp;h= 
8) 
<br>output=ssa(input,input,input) 
<br>print(output.shape) 
<br> 
<br> 
<h4>4. Squeeze-and-Excitation Attention Usage</h4> 
<h5>4.1. Paper</h5> 
<p>"Squeeze-and-Excitation Networks"</p> 
<h5>4.2. Overview</h5> 
<img src="https://images2.imgbox.com/b3/35/jEMGcgNq_o.jpg" alt="d809f3ee689de8c5ff127d42e0233fd0.jpeg"> 
<h5>4.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.SEAttention&amp;nbsp; 
import&amp;nbsp;SEAttention 
<br> 
import&amp;nbsp;torch 
<br> 
<br>input=torch.randn( 
50, 
512, 
7, 
7) 
<br>se&amp;nbsp;=&amp;nbsp;SEAttention(channel= 
512,reduction= 
8) 
<br>output=se(input) 
<br>print(output.shape) 
<br> 
<br> 
<h4>5. SK Attention Usage</h4> 
<h5>5.1. Paper</h5> 
<p>"Selective Kernel Networks"</p> 
<h5>5.2. Overview</h5> 
<img src="https://images2.imgbox.com/9c/4a/O0JH6ssK_o.jpg" alt="fd03b5cd21f510b3fef0d5cbbf278c40.jpeg"> 
<h5>5.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.SKAttention&amp;nbsp; 
import&amp;nbsp;SKAttention 
<br> 
import&amp;nbsp;torch 
<br> 
<br>input=torch.randn( 
50, 
512, 
7, 
7) 
<br>se&amp;nbsp;=&amp;nbsp;SKAttention(channel= 
512,reduction= 
8) 
<br>output=se(input) 
<br>print(output.shape) 
<br> 
<br> 
<h4>6. CBAM Attention Usage</h4> 
<h5>6.1. Paper</h5> 
<p>"CBAM: Convolutional Block Attention Module"</p> 
<h5>6.2. Overview</h5> 
<img src="https://images2.imgbox.com/67/94/fsq9aHrR_o.jpg" alt="fb4a2eaaeb5568f75b5246e2846f5179.jpeg"> 
<img src="https://images2.imgbox.com/3e/80/VsSjS2zs_o.jpg" alt="16a05b8db84d33f045df138c04be9d0f.jpeg"> 
<h5>6.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.CBAM&amp;nbsp; 
import&amp;nbsp;CBAMBlock 
<br> 
import&amp;nbsp;torch 
<br> 
<br>input=torch.randn( 
50, 
512, 
7, 
7) 
<br>kernel_size=input.shape[ 
2] 
<br>cbam&amp;nbsp;=&amp;nbsp;CBAMBlock(channel= 
512,reduction= 
16,kernel_size=kernel_size) 
<br>output=cbam(input) 
<br>print(output.shape) 
<br> 
<br> 
<h4>7. BAM Attention Usage</h4> 
<h5>7.1. Paper</h5> 
<p>"BAM: Bottleneck Attention Module"</p> 
<h5>7.2. Overview</h5> 
<img src="https://images2.imgbox.com/d7/ab/VbcLZU0X_o.jpg" alt="a683dba114112e6401b50c5c37673e96.jpeg"> 
<h5>7.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.BAM&amp;nbsp; 
import&amp;nbsp;BAMBlock 
<br> 
import&amp;nbsp;torch 
<br> 
<br>input=torch.randn( 
50, 
512, 
7, 
7) 
<br>bam&amp;nbsp;=&amp;nbsp;BAMBlock(channel= 
512,reduction= 
16,dia_val= 
2) 
<br>output=bam(input) 
<br>print(output.shape) 
<br> 
<br> 
<h4>8. ECA Attention Usage</h4> 
<h5>8.1. Paper</h5> 
<p>"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks"</p> 
<h5>8.2. Overview</h5> 
<img src="https://images2.imgbox.com/6e/70/8gyPMTNq_o.jpg" alt="8f97606dc4d441e301da4491bba46884.jpeg"> 
<h5>8.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.ECAAttention&amp;nbsp; 
import&amp;nbsp;ECAAttention 
<br> 
import&amp;nbsp;torch 
<br> 
<br>input=torch.randn( 
50, 
512, 
7, 
7) 
<br>eca&amp;nbsp;=&amp;nbsp;ECAAttention(kernel_size= 
3) 
<br>output=eca(input) 
<br>print(output.shape) 
<br> 
<br> 
<h4>9. DANet Attention Usage</h4> 
<h5>9.1. Paper</h5> 
<p>"Dual Attention Network for Scene Segmentation"</p> 
<h5>9.2. Overview</h5> 
<img src="https://images2.imgbox.com/8a/0e/RHd2XD4n_o.jpg" alt="05ecffebd3d4f4bf74f8ca5e1bf33959.jpeg"> 
<h5>9.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.DANet&amp;nbsp; 
import&amp;nbsp;DAModule 
<br> 
import&amp;nbsp;torch 
<br> 
<br>input=torch.randn( 
50, 
512, 
7, 
7) 
<br>danet=DAModule(d_model= 
512,kernel_size= 
3,H= 
7,W= 
7) 
<br>print(danet(input).shape) 
<br> 
<br> 
<h4>10. Pyramid Split Attention Usage</h4> 
<h5>10.1. Paper</h5> 
<p>"EPSANet: An Efficient Pyramid Split Attention Block on Convolutional Neural Network"</p> 
<h5>10.2. Overview</h5> 
<img src="https://images2.imgbox.com/a1/db/2hFDH8gY_o.jpg" alt="63ebe69e1e5985648f7b2a33ec469432.jpeg"> 
<h5>10.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.PSA&amp;nbsp; 
import&amp;nbsp;PSA 
<br> 
import&amp;nbsp;torch 
<br> 
<br>input=torch.randn( 
50, 
512, 
7, 
7) 
<br>psa&amp;nbsp;=&amp;nbsp;PSA(channel= 
512,reduction= 
8) 
<br>output=psa(input) 
<br>print(output.shape) 
<br> 
<br> 
<h4>11. Efficient Multi-Head Self-Attention Usage</h4> 
<h5>11.1. Paper</h5> 
<p>"ResT: An Efficient Transformer for Visual Recognition"</p> 
<h5>11.2. Overview</h5> 
<img src="https://images2.imgbox.com/64/8f/42WUUZQQ_o.jpg" alt="62bd119bdea8bfbb8bc76941dcfb3b6d.jpeg"> 
<h5>11.3. Usage Code</h5> 
 
<br> 
from&amp;nbsp;model.attention.EMSA&amp;nbsp; 
import&amp;nbsp;EMSA 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
50, 
64, 
512) 
<br>emsa&amp;nbsp;=&amp;nbsp;EMSA(d_model= 
512,&amp;nbsp;d_k= 
512,&amp;nbsp;d_v= 
512,&amp;nbsp;h= 
8,H= 
8,W= 
8,ratio= 
2,apply_transform= 
True) 
<br>output=emsa(input,input,input) 
<br>print(output.shape) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
<br> 
<h4>12. Shuffle Attention Usage</h4> 
<h5>12.1. Paper</h5> 
<p>"SA-NET: SHUFFLE ATTENTION FOR DEEP CONVOLUTIONAL NEURAL NETWORKS"</p> 
<h5>12.2. Overview</h5> 
<img src="https://images2.imgbox.com/0f/ca/tROZT9iQ_o.jpg" alt="9312736b0068714a2567e04b7c830e58.jpeg"> 
<h5>12.3. Usage Code</h5> 
 
<br> 
from&amp;nbsp;model.attention.ShuffleAttention&amp;nbsp; 
import&amp;nbsp;ShuffleAttention 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br> 
<br>input=torch.randn( 
50, 
512, 
7, 
7) 
<br>se&amp;nbsp;=&amp;nbsp;ShuffleAttention(channel= 
512,G= 
8) 
<br>output=se(input) 
<br>print(output.shape) 
<br> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
<br> 
<h4>13. MUSE Attention Usage</h4> 
<h5>13.1. Paper</h5> 
<p>"MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning"</p> 
<h5>13.2. Overview</h5> 
<img src="https://images2.imgbox.com/7e/99/NnZOD8ii_o.jpg" alt="f853d767b471dbac9329d29ea137b596.jpeg"> 
<h5>13.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.MUSEAttention&amp;nbsp; 
import&amp;nbsp;MUSEAttention 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br> 
<br>input=torch.randn( 
50, 
49, 
512) 
<br>sa&amp;nbsp;=&amp;nbsp;MUSEAttention(d_model= 
512,&amp;nbsp;d_k= 
512,&amp;nbsp;d_v= 
512,&amp;nbsp;h= 
8) 
<br>output=sa(input,input,input) 
<br>print(output.shape) 
<br> 
<br> 
<h4>14. SGE Attention Usage</h4> 
<h5>14.1. Paper</h5> 
<p>Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks</p> 
<h5>14.2. Overview</h5> 
<img src="https://images2.imgbox.com/b5/86/rp7nZnFJ_o.jpg" alt="e3d663771820a847d6c2a07ef466f49e.jpeg"> 
<h5>14.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.SGE&amp;nbsp; 
import&amp;nbsp;SpatialGroupEnhance 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
50, 
512, 
7, 
7) 
<br>sge&amp;nbsp;=&amp;nbsp;SpatialGroupEnhance(groups= 
8) 
<br>output=sge(input) 
<br>print(output.shape) 
<br> 
<br> 
<h4>15. A2 Attention Usage</h4> 
<h5>15.1. Paper</h5> 
<p>A2-Nets: Double Attention Networks</p> 
<h5>15.2. Overview</h5> 
<img src="https://images2.imgbox.com/07/34/j9JtQuqD_o.jpg" alt="790a1a7070d786233574336b7eb4e1f6.jpeg"> 
<h5>15.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.A2Atttention&amp;nbsp; 
import&amp;nbsp;DoubleAttention 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
50, 
512, 
7, 
7) 
<br>a2&amp;nbsp;=&amp;nbsp;DoubleAttention( 
512, 
128, 
128, 
True) 
<br>output=a2(input) 
<br>print(output.shape) 
<br> 
<br> 
<h4>16. AFT Attention Usage</h4> 
<h5>16.1. Paper</h5> 
<p>An Attention Free Transformer</p> 
<h5>16.2. Overview</h5> 
<img src="https://images2.imgbox.com/e7/48/OPXreprJ_o.jpg" alt="cfb1dd18353531a2e41a3a7859cb7bdf.jpeg"> 
<h5>16.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.AFT&amp;nbsp; 
import&amp;nbsp;AFT_FULL 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
50, 
49, 
512) 
<br>aft_full&amp;nbsp;=&amp;nbsp;AFT_FULL(d_model= 
512,&amp;nbsp;n= 
49) 
<br>output=aft_full(input) 
<br>print(output.shape) 
<br> 
<br> 
<h4>17. Outlook Attention Usage</h4> 
<h5>17.1. Paper</h5> 
<p>VOLO: Vision Outlooker for Visual Recognition"</p> 
<h5>17.2. Overview</h5> 
<img src="https://images2.imgbox.com/5b/a9/RVtkPrcV_o.jpg" alt="1b9177689fe510da8d96259febdf345a.jpeg"> 
<h5>17.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.OutlookAttention&amp;nbsp; 
import&amp;nbsp;OutlookAttention 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
50, 
28, 
28, 
512) 
<br>outlook&amp;nbsp;=&amp;nbsp;OutlookAttention(dim= 
512) 
<br>output=outlook(input) 
<br>print(output.shape) 
<br> 
<br> 
<h4>18. ViP Attention Usage</h4> 
<h5>18.1. Paper</h5> 
<p>Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition"</p> 
<h5>18.2. Overview</h5> 
<img src="https://images2.imgbox.com/91/34/46MAJE8S_o.jpg" alt="18bf83a125070c1d5e898354481be168.jpeg"> 
<h5>18.3. Usage Code</h5> 
 
<br> 
from&amp;nbsp;model.attention.ViP&amp;nbsp; 
import&amp;nbsp;WeightedPermuteMLP 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
64, 
8, 
8, 
512) 
<br>seg_dim= 
8 
<br>vip=WeightedPermuteMLP( 
512,seg_dim) 
<br>out=vip(input) 
<br>print(out.shape) 
<br> 
<br> 
<h4>19. CoAtNet Attention Usage</h4> 
<h5>19.1. Paper</h5> 
<p>CoAtNet: Marrying Convolution and Attention for All Data Sizes"</p> 
<h5>19.2. Overview</h5> 
<p>None</p> 
<h5>19.3. Usage Code</h5> 
 
<br> 
from&amp;nbsp;model.attention.CoAtNet&amp;nbsp; 
import&amp;nbsp;CoAtNet 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
1, 
3, 
224, 
224) 
<br>mbconv=CoAtNet(in_ch= 
3,image_size= 
224) 
<br>out=mbconv(input) 
<br>print(out.shape) 
<br> 
<br> 
<h4>20. HaloNet Attention Usage</h4> 
<h5>20.1. Paper</h5> 
<p>Scaling Local Self-Attention for Parameter Efficient Visual Backbones"</p> 
<h5>20.2. Overview</h5> 
<img src="https://images2.imgbox.com/e3/83/RT3G9sD1_o.jpg" alt="e45e84e76330662bf67cd53e9f4fda0a.jpeg"> 
<h5>20.3. Usage Code</h5> 
 
<br> 
from&amp;nbsp;model.attention.HaloAttention&amp;nbsp; 
import&amp;nbsp;HaloAttention 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
1, 
512, 
8, 
8) 
<br>halo&amp;nbsp;=&amp;nbsp;HaloAttention(dim= 
512, 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;block_size= 
2, 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;halo_size= 
1,) 
<br>output=halo(input) 
<br>print(output.shape) 
<br> 
<br> 
<h4>21. Polarized Self-Attention Usage</h4> 
<h5>21.1. Paper</h5> 
<p>Polarized Self-Attention: Towards High-quality Pixel-wise Regression"</p> 
<h5>21.2. Overview</h5> 
<img src="https://images2.imgbox.com/f2/3f/3dmJdk4g_o.jpg" alt="753a3272f63a82f5e181940ca27c3d04.jpeg"> 
<h5>21.3. Usage Code</h5> 
 
<br> 
from&amp;nbsp;model.attention.PolarizedSelfAttention&amp;nbsp; 
import&amp;nbsp;ParallelPolarizedSelfAttention,SequentialPolarizedSelfAttention 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
1, 
512, 
7, 
7) 
<br>psa&amp;nbsp;=&amp;nbsp;SequentialPolarizedSelfAttention(channel= 
512) 
<br>output=psa(input) 
<br>print(output.shape) 
<br> 
<br> 
<br> 
<h4>22. CoTAttention Usage</h4> 
<h5>22.1. Paper</h5> 
<p>Contextual Transformer Networks for Visual Recognition---arXiv 2021.07.26</p> 
<h5>22.2. Overview</h5> 
<img src="https://images2.imgbox.com/db/a2/ZZTkwVRN_o.jpg" alt="6a4bb7d7cb71e1c079025aeb401c79f6.jpeg"> 
<h5>22.3. Usage Code</h5> 
 
<br> 
from&amp;nbsp;model.attention.CoTAttention&amp;nbsp; 
import&amp;nbsp;CoTAttention 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
50, 
512, 
7, 
7) 
<br>cot&amp;nbsp;=&amp;nbsp;CoTAttention(dim= 
512,kernel_size= 
3) 
<br>output=cot(input) 
<br>print(output.shape) 
<br> 
<br> 
<br> 
<br> 
<h4>23. Residual Attention Usage</h4> 
<h5>23.1. Paper</h5> 
<p>Residual Attention: A Simple but Effective Method for Multi-Label Recognition---ICCV2021</p> 
<h5>23.2. Overview</h5> 
<img src="https://images2.imgbox.com/47/74/SeukcPlo_o.jpg" alt="70653e28b6d3a8fb84011334ca1cd1cc.jpeg"> 
<h5>23.3. Usage Code</h5> 
 
<br> 
from&amp;nbsp;model.attention.ResidualAttention&amp;nbsp; 
import&amp;nbsp;ResidualAttention 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
50, 
512, 
7, 
7) 
<br>resatt&amp;nbsp;=&amp;nbsp;ResidualAttention(channel= 
512,num_class= 
1000,la= 
0.2) 
<br>output=resatt(input) 
<br>print(output.shape) 
<br> 
<br> 
<br> 
<br> 
<h4>24. S2 Attention Usage</h4> 
<h5>24.1. Paper</h5> 
<p>SÂ²-MLPv2: Improved Spatial-Shift MLP Architecture for Vision---arXiv 2021.08.02</p> 
<h5>24.2. Overview</h5> 
<img src="https://images2.imgbox.com/f2/1e/SrDLB5bs_o.jpg" alt="6b3873e79f496c6360e33924f78e256c.jpeg"> 
<h5>24.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.S2Attention&amp;nbsp; 
import&amp;nbsp;S2Attention 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
50, 
512, 
7, 
7) 
<br>s2att&amp;nbsp;=&amp;nbsp;S2Attention(channels= 
512) 
<br>output=s2att(input) 
<br>print(output.shape) 
<br> 
<br> 
<h4>25. GFNet Attention Usage</h4> 
<h5>25.1. Paper</h5> 
<p>Global Filter Networks for Image Classification---arXiv 2021.07.01</p> 
<h5>25.2. Overview</h5> 
<img src="https://images2.imgbox.com/7b/4e/xcV22ZRr_o.jpg" alt="7cffe876bb5a80a3ea8d7ac9003bc181.jpeg"> 
<h5>25.3. Usage Code - Implemented by&amp;nbsp;Wenliang Zhao (Author)</h5> 
from&amp;nbsp;model.attention.gfnet&amp;nbsp; 
import&amp;nbsp;GFNet 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>x&amp;nbsp;=&amp;nbsp;torch.randn( 
1,&amp;nbsp; 
3,&amp;nbsp; 
224,&amp;nbsp; 
224) 
<br>gfnet&amp;nbsp;=&amp;nbsp;GFNet(embed_dim= 
384,&amp;nbsp;img_size= 
224,&amp;nbsp;patch_size= 
16,&amp;nbsp;num_classes= 
1000) 
<br>out&amp;nbsp;=&amp;nbsp;gfnet(x) 
<br>print(out.shape) 
<br> 
<br> 
<h4>26. TripletAttention Usage</h4> 
<h5>26.1. Paper</h5> 
<p>Rotate to Attend: Convolutional Triplet Attention Module---CVPR 2021</p> 
<h5>26.2. Overview</h5> 
<img src="https://images2.imgbox.com/d0/cf/flf3Yyp0_o.jpg" alt="103ae7c2abdaafa7ab194989c74f23c1.jpeg"> 
<h5>26.3. Usage Code - Implemented by&amp;nbsp;digantamisra98</h5> 
 
from&amp;nbsp;model.attention.TripletAttention&amp;nbsp; 
import&amp;nbsp;TripletAttention 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br>input=torch.randn( 
50, 
512, 
7, 
7) 
<br>triplet&amp;nbsp;=&amp;nbsp;TripletAttention() 
<br>output=triplet(input) 
<br>print(output.shape) 
<br> 
<h4>27. Coordinate Attention Usage</h4> 
<h5>27.1. Paper</h5> 
<p>Coordinate Attention for Efficient Mobile Network Design---CVPR 2021</p> 
<h5>27.2. Overview</h5> 
<img src="https://images2.imgbox.com/45/29/nGi19oyI_o.jpg" alt="a5929b6a14d2ea55658ce07b10112b86.jpeg"> 
<h5>27.3. Usage Code - Implemented by&amp;nbsp;Andrew-Qibin</h5> 
 
from&amp;nbsp;model.attention.CoordAttention&amp;nbsp; 
import&amp;nbsp;CoordAtt 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>inp=torch.rand([ 
2,&amp;nbsp; 
96,&amp;nbsp; 
56,&amp;nbsp; 
56]) 
<br>inp_dim,&amp;nbsp;oup_dim&amp;nbsp;=&amp;nbsp; 
96,&amp;nbsp; 
96 
<br>reduction= 
32 
<br> 
<br>coord_attention&amp;nbsp;=&amp;nbsp;CoordAtt(inp_dim,&amp;nbsp;oup_dim,&amp;nbsp;reduction=reduction) 
<br>output=coord_attention(inp) 
<br>print(output.shape) 
<br> 
<h4>28. MobileViT Attention Usage</h4> 
<h5>28.1. Paper</h5> 
<p>MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2021.10.05</p> 
<h5>28.2. Overview</h5> 
<img src="https://images2.imgbox.com/37/aa/jFQEpwLY_o.jpg" alt="dee603b5567bff8999a3fba41db77d02.jpeg"> 
<h5>28.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.MobileViTAttention&amp;nbsp; 
import&amp;nbsp;MobileViTAttention 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br> 
if&amp;nbsp;__name__&amp;nbsp;==&amp;nbsp; 
'__main__': 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;m=MobileViTAttention() 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;input=torch.randn( 
1, 
3, 
49, 
49) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;output=m(input) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;print(output.shape)&amp;nbsp;&amp;nbsp; 
<span style="font-style:italic;">#output:(1,3,49,49)</span> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
<br> 
<h4>29. ParNet Attention Usage</h4> 
<h5>29.1. Paper</h5> 
<p>Non-deep Networks---ArXiv 2021.10.20</p> 
<h5>29.2. Overview</h5> 
<img src="https://images2.imgbox.com/fe/9d/4893Ikzb_o.jpg" alt="d2021299c1a03950ff52639d0483da3e.jpeg"> 
<h5>29.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.ParNetAttention&amp;nbsp; 
import&amp;nbsp;* 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br> 
if&amp;nbsp;__name__&amp;nbsp;==&amp;nbsp; 
'__main__': 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;input=torch.randn( 
50, 
512, 
7, 
7) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;pna&amp;nbsp;=&amp;nbsp;ParNetAttention(channel= 
512) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;output=pna(input) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;print(output.shape)&amp;nbsp; 
<span style="font-style:italic;">#50,512,7,7</span> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
<br> 
<h4>30. UFO Attention Usage</h4> 
<h5>30.1. Paper</h5> 
<p>UFO-ViT: High Performance Linear Vision Transformer without Softmax---ArXiv 2021.09.29</p> 
<h5>30.2. Overview</h5> 
<img src="https://images2.imgbox.com/0b/8b/uVo0dJ0Z_o.jpg" alt="ec1ad4123ad0784c126f2d4887ef6472.jpeg"> 
<h5>30.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.UFOAttention&amp;nbsp; 
import&amp;nbsp;* 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br> 
if&amp;nbsp;__name__&amp;nbsp;==&amp;nbsp; 
'__main__': 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;input=torch.randn( 
50, 
49, 
512) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;ufo&amp;nbsp;=&amp;nbsp;UFOAttention(d_model= 
512,&amp;nbsp;d_k= 
512,&amp;nbsp;d_v= 
512,&amp;nbsp;h= 
8) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;output=ufo(input,input,input) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;print(output.shape)&amp;nbsp; 
<span style="font-style:italic;">#[50,&amp;nbsp;49,&amp;nbsp;512]</span> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
<br> 
<ul><li><br></li></ul> 
<h4>31. MobileViTv2 Attention Usage</h4> 
<h5>31.1. Paper</h5> 
<p>Separable Self-attention for Mobile Vision Transformers---ArXiv 2022.06.06</p> 
<h5>31.2. Overview</h5> 
<img src="https://images2.imgbox.com/07/35/3KfKF4ap_o.jpg" alt="2b81013e2fb3321e4c5447f40bb18623.jpeg"> 
<h5>31.3. Usage Code</h5> 
 
from&amp;nbsp;model.attention.UFOAttention&amp;nbsp; 
import&amp;nbsp;* 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br> 
if&amp;nbsp;__name__&amp;nbsp;==&amp;nbsp; 
'__main__': 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;input=torch.randn( 
50, 
49, 
512) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;ufo&amp;nbsp;=&amp;nbsp;UFOAttention(d_model= 
512,&amp;nbsp;d_k= 
512,&amp;nbsp;d_v= 
512,&amp;nbsp;h= 
8) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;output=ufo(input,input,input) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;print(output.shape)&amp;nbsp; 
<span style="font-style:italic;">#[50,&amp;nbsp;49,&amp;nbsp;512]</span> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
<br> 
<h2>Backbone Series</h2> 
<h4>1. ResNet Usage</h4> 
<h5>1.1. Paper</h5> 
<p>"Deep Residual Learning for Image Recognition---CVPR2016 Best Paper"</p> 
<h5>1.2. Overview</h5> 
<img src="https://images2.imgbox.com/d2/d2/Itm0EvjO_o.jpg" alt="8deb859a8e1aa117ddea72fc9ab2b73c.jpeg"> 
<img src="https://images2.imgbox.com/14/75/hcz8neYf_o.jpg" alt="9296caf356c172a9482a29db067a3327.jpeg"> 
<h5>1.3. Usage Code</h5> 
 
<br> 
from&amp;nbsp;model.backbone.resnet&amp;nbsp; 
import&amp;nbsp;ResNet50,ResNet101,ResNet152 
<br> 
import&amp;nbsp;torch 
<br> 
if&amp;nbsp;__name__&amp;nbsp;==&amp;nbsp; 
'__main__': 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;input=torch.randn( 
50, 
3, 
224, 
224) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;resnet50=ResNet50( 
1000) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
<span style="font-style:italic;">#&amp;nbsp;resnet101=ResNet101(1000)</span> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
<span style="font-style:italic;">#&amp;nbsp;resnet152=ResNet152(1000)</span> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;out=resnet50(input) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;print(out.shape) 
<br> 
<br> 
<h4>2. ResNeXt Usage</h4> 
<h5>2.1. Paper</h5> 
<p>"Aggregated Residual Transformations for Deep Neural Networks---CVPR2017"</p> 
<h5>2.2. Overview</h5> 
<img src="https://images2.imgbox.com/c9/f9/qUX8zMnZ_o.jpg" alt="3635637da4b46736933f2bc992913e26.jpeg"> 
<h5>2.3. Usage Code</h5> 
 
<br> 
from&amp;nbsp;model.backbone.resnext&amp;nbsp; 
import&amp;nbsp;ResNeXt50,ResNeXt101,ResNeXt152 
<br> 
import&amp;nbsp;torch 
<br> 
<br> 
if&amp;nbsp;__name__&amp;nbsp;==&amp;nbsp; 
'__main__': 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;input=torch.randn( 
50, 
3, 
224, 
224) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;resnext50=ResNeXt50( 
1000) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
<span style="font-style:italic;">#&amp;nbsp;resnext101=ResNeXt101(1000)</span> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
<span style="font-style:italic;">#&amp;nbsp;resnext152=ResNeXt152(1000)</span> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;out=resnext50(input) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;print(out.shape) 
<br> 
<br> 
<br> 
<h4>3. MobileViT Usage</h4> 
<h5>3.1. Paper</h5> 
<p>MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2020.10.05</p> 
<h5>3.2. Overview</h5> 
<img src="https://images2.imgbox.com/a1/63/vrCCO79R_o.jpg" alt="50197e84a437af9257e4df6c3051ae2c.jpeg"> 
<h5>3.3. Usage Code</h5> 
 
<br> 
from&amp;nbsp;model.backbone.MobileViT&amp;nbsp; 
import&amp;nbsp;* 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br> 
if&amp;nbsp;__name__&amp;nbsp;==&amp;nbsp; 
'__main__': 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;input=torch.randn( 
1, 
3, 
224, 
224) 
<br> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
<span style="font-style:italic;">###&amp;nbsp;mobilevit_xxs</span> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;mvit_xxs=mobilevit_xxs() 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;out=mvit_xxs(input) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;print(out.shape) 
<br> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
<span style="font-style:italic;">###&amp;nbsp;mobilevit_xs</span> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;mvit_xs=mobilevit_xs() 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;out=mvit_xs(input) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;print(out.shape) 
<br> 
<br> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
<span style="font-style:italic;">###&amp;nbsp;mobilevit_s</span> 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;mvit_s=mobilevit_s() 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;out=mvit_s(input) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;print(out.shape) 
<br> 
<br> 
<h4>4. ConvMixer Usage</h4> 
<h5>4.1. Paper</h5> 
<p>Patches Are All You Need?---ICLR2022 (Under Review)</p> 
<h5>4.2. Overview</h5> 
<img src="https://images2.imgbox.com/44/80/y5gQuNyc_o.jpg" alt="5585424bc56df98d514d317debe05eaf.jpeg"> 
<h5>4.3. Usage Code</h5> 
 
<br> 
from&amp;nbsp;model.backbone.ConvMixer&amp;nbsp; 
import&amp;nbsp;* 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br> 
if&amp;nbsp;__name__&amp;nbsp;==&amp;nbsp; 
'__main__': 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;x=torch.randn( 
1, 
3, 
224, 
224) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;convmixer=ConvMixer(dim= 
512,depth= 
12) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;out=convmixer(x) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;print(out.shape)&amp;nbsp;&amp;nbsp; 
<span style="font-style:italic;">#[1,&amp;nbsp;1000]</span> 
<br> 
<br> 
<br> 
<h2>MLP Series</h2> 
<h4>1. RepMLP Usage</h4> 
<h5>1.1. Paper</h5> 
<p>"RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition"</p> 
<h5>1.2. Overview</h5> 
<img src="https://images2.imgbox.com/4f/27/3Fwyvqyy_o.jpg" alt="c59ccabd5e6060df8773e8b79338cfdc.jpeg"> 
<h5>1.3. Usage Code</h5> 
 
from&amp;nbsp;model.mlp.repmlp&amp;nbsp; 
import&amp;nbsp;RepMLP 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
<br>N= 
4&amp;nbsp; 
<span style="font-style:italic;">#batch&amp;nbsp;size</span> 
<br>C= 
512&amp;nbsp; 
<span style="font-style:italic;">#input&amp;nbsp;dim</span> 
<br>O= 
1024&amp;nbsp; 
<span style="font-style:italic;">#output&amp;nbsp;dim</span> 
<br>H= 
14&amp;nbsp; 
<span style="font-style:italic;">#image&amp;nbsp;height</span> 
<br>W= 
14&amp;nbsp; 
<span style="font-style:italic;">#image&amp;nbsp;width</span> 
<br>h= 
7&amp;nbsp; 
<span style="font-style:italic;">#patch&amp;nbsp;height</span> 
<br>w= 
7&amp;nbsp; 
<span style="font-style:italic;">#patch&amp;nbsp;width</span> 
<br>fc1_fc2_reduction= 
1&amp;nbsp; 
<span style="font-style:italic;">#reduction&amp;nbsp;ratio</span> 
<br>fc3_groups= 
8&amp;nbsp; 
<span style="font-style:italic;">#&amp;nbsp;groups</span> 
<br>repconv_kernels=[ 
1, 
3, 
5, 
7]&amp;nbsp; 
<span style="font-style:italic;">#kernel&amp;nbsp;list</span> 
<br>repmlp=RepMLP(C,O,H,W,h,w,fc1_fc2_reduction,fc3_groups,repconv_kernels=repconv_kernels) 
<br>x=torch.randn(N,C,H,W) 
<br>repmlp.eval() 
<br> 
for&amp;nbsp;module&amp;nbsp; 
in&amp;nbsp;repmlp.modules(): 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
if&amp;nbsp;isinstance(module,&amp;nbsp;nn.BatchNorm2d)&amp;nbsp; 
or&amp;nbsp;isinstance(module,&amp;nbsp;nn.BatchNorm1d): 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;nn.init.uniform_(module.running_mean,&amp;nbsp; 
0,&amp;nbsp; 
0.1) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;nn.init.uniform_(module.running_var,&amp;nbsp; 
0,&amp;nbsp; 
0.1) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;nn.init.uniform_(module.weight,&amp;nbsp; 
0,&amp;nbsp; 
0.1) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;nn.init.uniform_(module.bias,&amp;nbsp; 
0,&amp;nbsp; 
0.1) 
<br> 
<br> 
<span style="font-style:italic;">#training&amp;nbsp;result</span> 
<br>out=repmlp(x) 
<br> 
<span style="font-style:italic;">#inference&amp;nbsp;result</span> 
<br>repmlp.switch_to_deploy() 
<br>deployout&amp;nbsp;=&amp;nbsp;repmlp(x) 
<br> 
<br>print(((deployout-out)** 
2).sum()) 
<br> 
<h4>2. MLP-Mixer Usage</h4> 
<h5>2.1. Paper</h5> 
<p>"MLP-Mixer: An all-MLP Architecture for Vision"</p> 
<h5>2.2. Overview</h5> 
<img src="https://images2.imgbox.com/ad/55/s13k8Sh5_o.jpg" alt="92a385825da4fc31ccd64656ca02c814.jpeg"> 
<h5>2.3. Usage Code</h5> 
 
from&amp;nbsp;model.mlp.mlp_mixer&amp;nbsp; 
import&amp;nbsp;MlpMixer 
<br> 
import&amp;nbsp;torch 
<br>mlp_mixer=MlpMixer(num_classes= 
1000,num_blocks= 
10,patch_size= 
10,tokens_hidden_dim= 
32,channels_hidden_dim= 
1024,tokens_mlp_dim= 
16,channels_mlp_dim= 
1024) 
<br>input=torch.randn( 
50, 
3, 
40, 
40) 
<br>output=mlp_mixer(input) 
<br>print(output.shape) 
<br> 
<h4>3. ResMLP Usage</h4> 
<h5>3.1. Paper</h5> 
<p>"ResMLP: Feedforward networks for image classification with data-efficient training"</p> 
<h5>3.2. Overview</h5> 
<img src="https://images2.imgbox.com/76/ca/brz1ox74_o.jpg" alt="32f6996663646c4c154014a4891ed4d9.jpeg"> 
<h5>3.3. Usage Code</h5> 
 
from&amp;nbsp;model.mlp.resmlp&amp;nbsp; 
import&amp;nbsp;ResMLP 
<br> 
import&amp;nbsp;torch 
<br> 
<br>input=torch.randn( 
50, 
3, 
14, 
14) 
<br>resmlp=ResMLP(dim= 
128,image_size= 
14,patch_size= 
7,class_num= 
1000) 
<br>out=resmlp(input) 
<br>print(out.shape)&amp;nbsp; 
<span style="font-style:italic;">#the&amp;nbsp;last&amp;nbsp;dimention&amp;nbsp;is&amp;nbsp;class_num</span> 
<br> 
<h4>4. gMLP Usage</h4> 
<h5>4.1. Paper</h5> 
<p>"Pay Attention to MLPs"</p> 
<h5>4.2. Overview</h5> 
<img src="https://images2.imgbox.com/a3/1d/nAv9VXpc_o.jpg" alt="e5d3387d845820423325dc55ce282ebb.jpeg"> 
<h5>4.3. Usage Code</h5> 
 
from&amp;nbsp;model.mlp.g_mlp&amp;nbsp; 
import&amp;nbsp;gMLP 
<br> 
import&amp;nbsp;torch 
<br> 
<br>num_tokens= 
10000 
<br>bs= 
50 
<br>len_sen= 
49 
<br>num_layers= 
6 
<br>input=torch.randint(num_tokens,(bs,len_sen))&amp;nbsp; 
<span style="font-style:italic;">#bs,len_sen</span> 
<br>gmlp&amp;nbsp;=&amp;nbsp;gMLP(num_tokens=num_tokens,len_sen=len_sen,dim= 
512,d_ff= 
1024) 
<br>output=gmlp(input) 
<br>print(output.shape) 
<br> 
<h4>5. sMLP Usage</h4> 
<h5>5.1. Paper</h5> 
<p>"Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?"</p> 
<h5>5.2. Overview</h5> 
<img src="https://images2.imgbox.com/e9/96/AxGRGjw4_o.jpg" alt="c171cc2f378be86e49a2dbd678707772.jpeg"> 
<h5>5.3. Usage Code</h5> 
 
from&amp;nbsp;model.mlp.sMLP_block&amp;nbsp; 
import&amp;nbsp;sMLPBlock 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br> 
if&amp;nbsp;__name__&amp;nbsp;==&amp;nbsp; 
'__main__': 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;input=torch.randn( 
50, 
3, 
224, 
224) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;smlp=sMLPBlock(h= 
224,w= 
224) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;out=smlp(input) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;print(out.shape) 
<br> 
<h2>Re-Parameter Series</h2> 
<h4>1. RepVGG Usage</h4> 
<h5>1.1. Paper</h5> 
<p>"RepVGG: Making VGG-style ConvNets Great Again"</p> 
<h5>1.2. Overview</h5> 
<img src="https://images2.imgbox.com/99/ab/HiMusVAK_o.jpg" alt="87f8ea54d81fdb6207bc19ebb939a1c3.jpeg"> 
<h5>1.3. Usage Code</h5> 
 
<br> 
from&amp;nbsp;model.rep.repvgg&amp;nbsp; 
import&amp;nbsp;RepBlock 
<br> 
import&amp;nbsp;torch 
<br> 
<br> 
<br>input=torch.randn( 
50, 
512, 
49, 
49) 
<br>repblock=RepBlock( 
512, 
512) 
<br>repblock.eval() 
<br>out=repblock(input) 
<br>repblock._switch_to_deploy() 
<br>out2=repblock(input) 
<br>print( 
'difference&amp;nbsp;between&amp;nbsp;vgg&amp;nbsp;and&amp;nbsp;repvgg') 
<br>print(((out2-out)** 
2).sum()) 
<br> 
<h4>2. ACNet Usage</h4> 
<h5>2.1. Paper</h5> 
<p>"ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks"</p> 
<h5>2.2. Overview</h5> 
<img src="https://images2.imgbox.com/70/86/jdFhHHnq_o.jpg" alt="f44742ab99f8c0122903ae293ef813cf.jpeg"> 
<h5>2.3. Usage Code</h5> 
 
from&amp;nbsp;model.rep.acnet&amp;nbsp; 
import&amp;nbsp;ACNet 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
<br>input=torch.randn( 
50, 
512, 
49, 
49) 
<br>acnet=ACNet( 
512, 
512) 
<br>acnet.eval() 
<br>out=acnet(input) 
<br>acnet._switch_to_deploy() 
<br>out2=acnet(input) 
<br>print( 
'difference:') 
<br>print(((out2-out)** 
2).sum()) 
<br> 
<br> 
<h4>2. Diverse Branch Block Usage</h4> 
<h5>2.1. Paper</h5> 
<p>"Diverse Branch Block: Building a Convolution as an Inception-like Unit"</p> 
<h5>2.2. Overview</h5> 
<img src="https://images2.imgbox.com/a2/51/XvcHvek1_o.jpg" alt="5408daffd5e00cf8fcffc7f30f8f2993.jpeg"> 
<h5>2.3. Usage Code</h5> 
<h6>2.3.1 Transform I</h6> 
 
from&amp;nbsp;model.rep.ddb&amp;nbsp; 
import&amp;nbsp;transI_conv_bn 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
1, 
64, 
7, 
7) 
<br> 
<span style="font-style:italic;">#conv+bn</span> 
<br>conv1=nn.Conv2d( 
64, 
64, 
3,padding= 
1) 
<br>bn1=nn.BatchNorm2d( 
64) 
<br>bn1.eval() 
<br>out1=bn1(conv1(input)) 
<br> 
<br> 
<span style="font-style:italic;">#conv_fuse</span> 
<br>conv_fuse=nn.Conv2d( 
64, 
64, 
3,padding= 
1) 
<br>conv_fuse.weight.data,conv_fuse.bias.data=transI_conv_bn(conv1,bn1) 
<br>out2=conv_fuse(input) 
<br> 
<br>print( 
"difference:",((out2-out1)** 
2).sum().item()) 
<br> 
<h6>2.3.2 Transform II</h6> 
 
from&amp;nbsp;model.rep.ddb&amp;nbsp; 
import&amp;nbsp;transII_conv_branch 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
1, 
64, 
7, 
7) 
<br> 
<br> 
<span style="font-style:italic;">#conv+conv</span> 
<br>conv1=nn.Conv2d( 
64, 
64, 
3,padding= 
1) 
<br>conv2=nn.Conv2d( 
64, 
64, 
3,padding= 
1) 
<br>out1=conv1(input)+conv2(input) 
<br> 
<br> 
<span style="font-style:italic;">#conv_fuse</span> 
<br>conv_fuse=nn.Conv2d( 
64, 
64, 
3,padding= 
1) 
<br>conv_fuse.weight.data,conv_fuse.bias.data=transII_conv_branch(conv1,conv2) 
<br>out2=conv_fuse(input) 
<br> 
<br>print( 
"difference:",((out2-out1)** 
2).sum().item()) 
<br> 
<h6>2.3.3 Transform III</h6> 
 
from&amp;nbsp;model.rep.ddb&amp;nbsp; 
import&amp;nbsp;transIII_conv_sequential 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
1, 
64, 
7, 
7) 
<br> 
<br> 
<span style="font-style:italic;">#conv+conv</span> 
<br>conv1=nn.Conv2d( 
64, 
64, 
1,padding= 
0,bias= 
False) 
<br>conv2=nn.Conv2d( 
64, 
64, 
3,padding= 
1,bias= 
False) 
<br>out1=conv2(conv1(input)) 
<br> 
<br> 
<br> 
<span style="font-style:italic;">#conv_fuse</span> 
<br>conv_fuse=nn.Conv2d( 
64, 
64, 
3,padding= 
1,bias= 
False) 
<br>conv_fuse.weight.data=transIII_conv_sequential(conv1,conv2) 
<br>out2=conv_fuse(input) 
<br> 
<br>print( 
"difference:",((out2-out1)** 
2).sum().item()) 
<br> 
<h6>2.3.4 Transform IV</h6> 
 
from&amp;nbsp;model.rep.ddb&amp;nbsp; 
import&amp;nbsp;transIV_conv_concat 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
1, 
64, 
7, 
7) 
<br> 
<br> 
<span style="font-style:italic;">#conv+conv</span> 
<br>conv1=nn.Conv2d( 
64, 
32, 
3,padding= 
1) 
<br>conv2=nn.Conv2d( 
64, 
32, 
3,padding= 
1) 
<br>out1=torch.cat([conv1(input),conv2(input)],dim= 
1) 
<br> 
<br> 
<span style="font-style:italic;">#conv_fuse</span> 
<br>conv_fuse=nn.Conv2d( 
64, 
64, 
3,padding= 
1) 
<br>conv_fuse.weight.data,conv_fuse.bias.data=transIV_conv_concat(conv1,conv2) 
<br>out2=conv_fuse(input) 
<br> 
<br>print( 
"difference:",((out2-out1)** 
2).sum().item()) 
<br> 
<h6>2.3.5 Transform V</h6> 
 
from&amp;nbsp;model.rep.ddb&amp;nbsp; 
import&amp;nbsp;transV_avg 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
1, 
64, 
7, 
7) 
<br> 
<br>avg=nn.AvgPool2d(kernel_size= 
3,stride= 
1) 
<br>out1=avg(input) 
<br> 
<br>conv=transV_avg( 
64, 
3) 
<br>out2=conv(input) 
<br> 
<br>print( 
"difference:",((out2-out1)** 
2).sum().item()) 
<br> 
<h6>2.3.6 Transform VI</h6> 
 
from&amp;nbsp;model.rep.ddb&amp;nbsp; 
import&amp;nbsp;transVI_conv_scale 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
1, 
64, 
7, 
7) 
<br> 
<br> 
<span style="font-style:italic;">#conv+conv</span> 
<br>conv1x1=nn.Conv2d( 
64, 
64, 
1) 
<br>conv1x3=nn.Conv2d( 
64, 
64,( 
1, 
3),padding=( 
0, 
1)) 
<br>conv3x1=nn.Conv2d( 
64, 
64,( 
3, 
1),padding=( 
1, 
0)) 
<br>out1=conv1x1(input)+conv1x3(input)+conv3x1(input) 
<br> 
<br> 
<span style="font-style:italic;">#conv_fuse</span> 
<br>conv_fuse=nn.Conv2d( 
64, 
64, 
3,padding= 
1) 
<br>conv_fuse.weight.data,conv_fuse.bias.data=transVI_conv_scale(conv1x1,conv1x3,conv3x1) 
<br>out2=conv_fuse(input) 
<br> 
<br>print( 
"difference:",((out2-out1)** 
2).sum().item()) 
<br> 
<h2>Convolution Series</h2> 
<p><span style="font-size:20px;font-weight:bold;">1. Depthwise Separable Convolution Usage</span><br></p> 
<h5>1.1. Paper</h5> 
<p>"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"</p> 
<h5>1.2. Overview</h5> 
<img src="https://images2.imgbox.com/2b/63/3Pvko9Dw_o.jpg" alt="b47d48e0c0dc9c6c4af28d7661605bb0.jpeg"> 
<h5>1.3. Usage Code</h5> 
 
from&amp;nbsp;model.conv.DepthwiseSeparableConvolution&amp;nbsp; 
import&amp;nbsp;DepthwiseSeparableConvolution 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
1, 
3, 
224, 
224) 
<br>dsconv=DepthwiseSeparableConvolution( 
3, 
64) 
<br>out=dsconv(input) 
<br>print(out.shape) 
<br> 
<h4>2. MBConv Usage</h4> 
<h5>2.1. Paper</h5> 
<p>"Efficientnet: Rethinking model scaling for convolutional neural networks"</p> 
<h5>2.2. Overview</h5> 
<img src="https://images2.imgbox.com/89/30/kS10DQsx_o.jpg" alt="17ec969b7a17a9d884d3abd6ab748db0.jpeg"> 
<h5>2.3. Usage Code</h5> 
 
from&amp;nbsp;model.conv.MBConv&amp;nbsp; 
import&amp;nbsp;MBConvBlock 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
1, 
3, 
224, 
224) 
<br>mbconv=MBConvBlock(ksize= 
3,input_filters= 
3,output_filters= 
512,image_size= 
224) 
<br>out=mbconv(input) 
<br>print(out.shape) 
<br> 
<br> 
<br> 
<h4>3. Involution Usage</h4> 
<h5>3.1. Paper</h5> 
<p>"Involution: Inverting the Inherence of Convolution for Visual Recognition"</p> 
<h5>3.2. Overview</h5> 
<img src="https://images2.imgbox.com/f2/df/VxcMSNUn_o.jpg" alt="c44c9c9f336835de973adb3fc53e28e7.jpeg"> 
<h5>3.3. Usage Code</h5> 
 
from&amp;nbsp;model.conv.Involution&amp;nbsp; 
import&amp;nbsp;Involution 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br>input=torch.randn( 
1, 
4, 
64, 
64) 
<br>involution=Involution(kernel_size= 
3,in_channel= 
4,stride= 
2) 
<br>out=involution(input) 
<br>print(out.shape) 
<br> 
<h4>4. DynamicConv Usage</h4> 
<h5>4.1. Paper</h5> 
<p>"Dynamic Convolution: Attention over Convolution Kernels"</p> 
<h5>4.2. Overview</h5> 
<img src="https://images2.imgbox.com/8b/92/LL2w96D3_o.jpg" alt="54a23b6088f71d4bc61071efc891352e.jpeg"> 
<h5>4.3. Usage Code</h5> 
 
from&amp;nbsp;model.conv.DynamicConv&amp;nbsp; 
import&amp;nbsp;* 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br> 
if&amp;nbsp;__name__&amp;nbsp;==&amp;nbsp; 
'__main__': 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;input=torch.randn( 
2, 
32, 
64, 
64) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;m=DynamicConv(in_planes= 
32,out_planes= 
64,kernel_size= 
3,stride= 
1,padding= 
1,bias= 
False) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;out=m(input) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;print(out.shape)&amp;nbsp; 
<span style="font-style:italic;">#&amp;nbsp;2,32,64,64</span> 
<br> 
<br> 
<h4>5. CondConv Usage</h4> 
<h5>5.1. Paper</h5> 
<p>"CondConv: Conditionally Parameterized Convolut ions for Efficient Inference"</p> 
<h5>5.2. Overview</h5> 
<img src="https://images2.imgbox.com/93/81/LTjegfMX_o.jpg" alt="30bb6fe0c5ded0ce3e035280d8b92455.jpeg"> 
<h5>5.3. Usage Code</h5> 
 
from&amp;nbsp;model.conv.CondConv&amp;nbsp; 
import&amp;nbsp;* 
<br> 
import&amp;nbsp;torch 
<br> 
from&amp;nbsp;torch&amp;nbsp; 
import&amp;nbsp;nn 
<br> 
from&amp;nbsp;torch.nn&amp;nbsp; 
import&amp;nbsp;functional&amp;nbsp; 
as&amp;nbsp;F 
<br> 
<br> 
if&amp;nbsp;__name__&amp;nbsp;==&amp;nbsp; 
'__main__': 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;input=torch.randn( 
2, 
32, 
64, 
64) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;m=CondConv(in_planes= 
32,out_planes= 
64,kernel_size= 
3,stride= 
1,padding= 
1,bias= 
False) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;out=m(input) 
<br>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;print(out.shape) 
<br> 
<br> 
<p>å·²å»ºç«‹æ·±åº¦å­¦ä¹ å…¬ä¼—å·â€”â€”<strong>FightingCV</strong>ï¼Œæ¬¢è¿å¤§å®¶å…³æ³¨ï¼ï¼ï¼</p> 
<p><strong>ICCVã€CVPRã€NeurIPSã€ICMLè®ºæ–‡è§£ææ±‡æ€»ï¼šhttps://github.com/xmu-xiaoma666/FightingCV-Paper-Reading</strong></p> 
<p><strong>é¢å‘å°ç™½çš„Attentionã€é‡å‚æ•°ã€MLPã€å·ç§¯æ ¸å¿ƒä»£ç å­¦ä¹ ï¼šhttps://github.com/xmu-xiaoma666/External-Attention-pytorch</strong></p> 
<p>åŠ å…¥äº¤æµç¾¤ï¼Œè¯·æ·»åŠ å°åŠ©æ‰‹wxï¼š<strong>FightngCV666</strong></p> 
<p><br></p> 
<p><br></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8667216149efdbdc3974976c1236f87a/" rel="prev">
			<span class="pager__subtitle">Â«&thinsp;Previous</span>
			<p class="pager__title">IDEAä¸€äº›å¸¸ç”¨è®¾ç½®åŠæ’ä»¶</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1182ef1826d2a1851406aff417aa09f3/" rel="next">
			<span class="pager__subtitle">Next&thinsp;Â»</span>
			<p class="pager__title">webapiæ¥å£æ–‡ä»¶ä¸‹è½½æ—¶è·¨åŸŸé—®é¢˜</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 èœé¸Ÿç¨‹åºå‘˜åšå®¢.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>