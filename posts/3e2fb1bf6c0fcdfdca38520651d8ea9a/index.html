<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>PyTorch|Tensor基础操作：拼接、切分、索引和变换 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="PyTorch|Tensor基础操作：拼接、切分、索引和变换" />
<meta property="og:description" content="01 | 写在前面 在学习了Tensor的创建方法之后，接下来你可能会问：那么我们可以对Tensor进行哪些操作呢？
不急，今天我们就来聊聊Tensor的操作方法。这部分主要包含两类：
Tensor的基础操作：如拼接、切分、索引和变换
Tensor的数学运算
02 | Tensor基础操作 021 | Tensor的拼接 当我们想拼接两个张量（Tensor）时，可以选用两种方法，一类是“torch.cat()”，一类则是“torch.stack()”。
torch.cat()的功能是将张量按制定的维度参数（dim）进行拼接，并返回一个新张量，其关键参数有二：
tensors：要拼接的张量序列
dim：要进行拼接的维度
这里的维度指的是张量所张开的维度，如（3,2）从第0维度看（投影）有3个“元素”，而第1维度则有2个“元素”。
torch.stack()与torch.cat()的不同在于，拼接张量时会在指定的维度（dim）上插入一个新维度，得到一个维度增加后的张量。
为了进一步演示其作用，我们在PyCharm上运行测试代码：
# 测试torch.cat()的拼接作用 flag = True if flag: # 创建一个（2,3）张量 t = torch.ones((2,3)) print(t) t_0 = torch.cat([t, t], dim=0) t_1 = torch.cat([t, t], dim=1) print(&#34;t_0:{} shape:{}\nt_1:{} shape:{}&#34;.format(t_0, t_0.shape, t_1, t_1.shape)) 上例展示了torch.cat()在不同维度上拼接的作用，原有的(2,3)张量可以看作是在两个维度上分别张开的张量，当dim=0时指定第一个维度，该维度上每个张量均有2个元素，因此拼接后可以得到(4,3)的新张量（多维数组）；当指定dim=1时指定第二个维度，该维度上张量有3个元素，因此拼接后得到(2,6)的新张量。
当我们采用**torch.stack()**拼接时，会在原有的维度上添加新维度，此时需要将原始张量看作是多维矩阵，重新分配元素并计算张量中各部分大小。
# 测试torch_stack()拼接张量 # 在新维度2上拼接，原有t是2×3，两个t拼接后变成12个元素，那么拼接后形状为（2,3,2），意思为“2个3×2矩阵” t_3 = torch.stack([t, t], dim=2) print(&#34;t_3 is{}\nt_3 shape is{}&#34;.format(t_3, t_3.shape)) # 在第1个维度前插入个新维度，原有维度降级 t_4 = torch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/3e2fb1bf6c0fcdfdca38520651d8ea9a/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-01-11T16:00:34+08:00" />
<meta property="article:modified_time" content="2023-01-11T16:00:34+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">PyTorch|Tensor基础操作：拼接、切分、索引和变换</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="01___0"></a>01 | 写在前面</h3> 
<p>在学习了Tensor的创建方法之后，接下来你可能会问：那么我们可以对Tensor进行哪些操作呢？</p> 
<p>不急，今天我们就来聊聊Tensor的操作方法。这部分主要包含两类：</p> 
<ul><li> <p>Tensor的基础操作：如拼接、切分、索引和变换</p> </li><li> <p>Tensor的数学运算</p> </li></ul> 
<h3><a id="02__Tensor_10"></a>02 | Tensor基础操作</h3> 
<h4><a id="021__Tensor_12"></a>021 | Tensor的拼接</h4> 
<p>当我们想拼接两个张量（Tensor）时，可以选用两种方法，一类是“<strong>torch.cat()</strong>”，一类则是“<strong>torch.stack()</strong>”。</p> 
<p>torch.cat()的功能是将张量按制定的维度参数（dim）进行拼接，并返回一个新张量，其关键参数有二：</p> 
<ul><li> <p>tensors：要拼接的张量序列</p> </li><li> <p>dim：要进行拼接的维度</p> </li></ul> 
<p>这里的维度指的是张量所张开的维度，如（3,2）从第0维度看（投影）有3个“元素”，而第1维度则有2个“元素”。</p> 
<p>torch.stack()与torch.cat()的不同在于，拼接张量时会在指定的维度（dim）上插入一个新维度，得到一个维度增加后的张量。</p> 
<p><img src="https://images2.imgbox.com/9b/22/uFyi5Yef_o.jpg" alt=""></p> 
<p>为了进一步演示其作用，我们在PyCharm上运行测试代码：</p> 
<pre><code class="prism language-python"><span class="token comment"># 测试torch.cat()的拼接作用 </span>
flag <span class="token operator">=</span> <span class="token boolean">True</span> 
<span class="token keyword">if</span> flag<span class="token punctuation">:</span>    
		<span class="token comment"># 创建一个（2,3）张量        </span>
		t <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        
		<span class="token keyword">print</span><span class="token punctuation">(</span>t<span class="token punctuation">)</span>            
		t_0 <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>t<span class="token punctuation">,</span> t<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>        
		t_1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>t<span class="token punctuation">,</span> t<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>            
		<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"t_0:{} shape:{}\nt_1:{} shape:{}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>t_0<span class="token punctuation">,</span> t_0<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> t_1<span class="token punctuation">,</span> t_1<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/0b/48/kcZApsAc_o.jpg" alt=""><br> 上例展示了torch.cat()在不同维度上拼接的作用，原有的(2,3)张量可以看作是在两个维度上分别张开的张量，当dim=0时指定第一个维度，该维度上每个张量均有2个元素，因此拼接后可以得到(4,3)的新张量（多维数组）；当指定dim=1时指定第二个维度，该维度上张量有3个元素，因此拼接后得到(2,6)的新张量。</p> 
<p>当我们采用**torch.stack()**拼接时，会在原有的维度上添加新维度，此时需要将原始张量看作是多维矩阵，重新分配元素并计算张量中各部分大小。</p> 
<pre><code class="prism language-python"><span class="token comment"># 测试torch_stack()拼接张量 </span>
<span class="token comment"># 在新维度2上拼接，原有t是2×3，两个t拼接后变成12个元素，那么拼接后形状为（2,3,2），意思为“2个3×2矩阵” </span>
t_3 <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>t<span class="token punctuation">,</span> t<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span> 
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"t_3 is{}\nt_3 shape is{}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>t_3<span class="token punctuation">,</span> t_3<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span> 
<span class="token comment"># 在第1个维度前插入个新维度，原有维度降级 </span>
t_4 <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>t<span class="token punctuation">,</span> t<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span> 
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"t_4 is{}\nt_4 shape is{}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>t_4<span class="token punctuation">,</span> t_4<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span> 
<span class="token comment"># 在第1个和第2个维度间插入了新维度 </span>
t_5 <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>t<span class="token punctuation">,</span> t<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> 
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"t_5 is{}\nt_5 shape is{}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>t_5<span class="token punctuation">,</span> t_5<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/02/17/6v68oGU1_o.jpg" alt=""></p> 
<h4><a id="022__Tensor_60"></a>022 | Tensor的切分</h4> 
<p>既然可以将两个张量拼接起来，那么自然也可以将一个张量进行切分。一种常见的方法是基于“<strong>torch.chunk()</strong>”的平均切分方法，该方法返回切分后的张量列表，需要注意的是，当切分数量不能整除时，最后一份张量小于其他张量。</p> 
<p>其中重要的参数有三项：</p> 
<ul><li> <p>input：表示是要切分的张量</p> </li><li> <p>chunks：要切分的份数</p> </li><li> <p>dim：要切分的维度</p> </li></ul> 
<p>此时可以在PyCharm中运行测试代码，可以发现当指定dim=1时，实际是从原始张量的列张开方向予以均分的，但是由于原始张量有5列，均分2.5，四舍五入为3，则最后一个张量只有（5-3）=2列，即2×2的方阵。</p> 
<pre><code class="prism language-python"><span class="token comment"># **************************** example **************************** # 通过torch.chunk()平均切分张量 </span>
flag <span class="token operator">=</span> <span class="token boolean">True</span> 
<span class="token keyword">if</span> flag<span class="token punctuation">:</span>   
		a <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>       
		<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'原始张量{}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span>        
		list_of_tensors <span class="token operator">=</span> torch<span class="token punctuation">.</span>chunk<span class="token punctuation">(</span>a<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> chunks<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>        
		<span class="token keyword">for</span> idx<span class="token punctuation">,</span> t <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>list_of_tensors<span class="token punctuation">)</span><span class="token punctuation">:</span>              
		<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'第{}个张量：{}，shape is {}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>idx<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> t<span class="token punctuation">,</span> t<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/db/60/kt4BAMqO_o.jpg" alt=""><br> 除去统一均分的切分方式外，torch还提供了“torch.split()”方法提供更加灵活强大的张量切分方式，此时可以将张量按维度dim和指定长度进行切分（未必均分）。其主要参数为：</p> 
<ul><li> <p>tensor：表示要切分的张量</p> </li><li> <p>split_size_or_sections：当为int类型，表示每一份的长度；若为list类型，则按list元素切分</p> </li><li> <p>dim：表示要切分的维度</p> </li></ul> 
<p>同样在PyCharm中运行测试代码，可见当指定分片长度=2时，从5列中依次选取2列构成一个张量，而最后1列单独构成一个张量。</p> 
<pre><code class="prism language-python"><span class="token keyword">if</span> flag<span class="token punctuation">:</span>    
		a <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        
		<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'原始张量{}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span>     
		<span class="token comment"># 指定切分后每部分长度为2，最后一部分因为不够而小于2     	</span>
		list_of_tensors <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>a<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        
		<span class="token keyword">for</span> idx<span class="token punctuation">,</span> t <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>list_of_tensors<span class="token punctuation">)</span><span class="token punctuation">:</span>           
	  		<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'第{}个张量：{}，shape is {}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>idx<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> t<span class="token punctuation">,</span> t<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/27/22/uklXbqKi_o.jpg" alt="在这里插入图片描述">感兴趣的同学还可以试试修改为列表[2,1,2]时切分的效果。</p> 
<blockquote> 
 <p>list_of_tensors = torch.split(a, [2,1,2], dim=1)</p> 
</blockquote> 
<h4><a id="023__Tensor_108"></a>023 | Tensor的索引</h4> 
<p>PyTorch也为我们提供了查找特定元素数值的方法，即通过“索引”实现。<br> 第一种索引的方法是“<strong>torch.index_select()</strong>”，其功能为在指定的维度dim上，基于索引index查找返回数据，最终返回基于index索引数据拼接的张量。其主要参数为：</p> 
<ul><li> <p>input：要索引的张量</p> </li><li> <p>dim：要索引的维度</p> </li><li> <p>index：要索引数据的序号</p> </li></ul> 
<p>照旧我们在PyCharm上运行测试代码，如此更加直观形象。从实验中可以看出，原始张量为从[0,9)中随机选取数值构建的3×3方阵（矩阵或二维数组），指定dim=0即意味着从“行向量”角度索引，基于索引张量Tensor(0,2)可以确定索引第0行和第2行输出返回。</p> 
<p>需要注意的是，这里的索引张量必须是torch.long类型，不能是float。</p> 
<pre><code class="prism language-python"><span class="token comment"># 采用torch.index_select()索引张量中数据 </span>
flag <span class="token operator">=</span> <span class="token boolean">True</span> 
<span class="token keyword">if</span> flag<span class="token punctuation">:</span>   
		t <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>          
		idx <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">)</span>        
		t_select <span class="token operator">=</span> torch<span class="token punctuation">.</span>index_select<span class="token punctuation">(</span>t<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> index<span class="token operator">=</span>idx<span class="token punctuation">)</span>        
		<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"t:\n{}t_select:\n{}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>t<span class="token punctuation">,</span> t_select<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/c0/96/CgdT9ZLS_o.jpg" alt=""><br> 那么我们是否可以类似于正则表达式一样的规则进行索引呢？当然可以！<br> 此时我们需要借助“torch.masked_select()”方法，按mask中的True值项进行索引，但是由于并不知晓符合True值的数据个数，因而只返回一个一维张量（数组）。其主要参数为：</p> 
<ul><li> <p>input：要索引的张量</p> </li><li> <p>mask：与input同形状的布尔类型张量</p> </li></ul> 
<p>由实验中可以看出，当采用布尔型张量t.ge(4)时（表示所有大于等于4的数值），可以借助torch.masked_select正确筛选出所有大于等于4的数据，并返回一维张量。</p> 
<pre><code class="prism language-python"><span class="token comment"># 采用torch.masked_select()正则性索引张量 </span>
flag <span class="token operator">=</span> <span class="token boolean">True</span> 
<span class="token keyword">if</span> flag<span class="token punctuation">:</span>   
		t <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        
		mask <span class="token operator">=</span> t<span class="token punctuation">.</span>ge<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span> <span class="token comment"># gt is mean greater than or equal, gt/le/lt        </span>
		t_select <span class="token operator">=</span> torch<span class="token punctuation">.</span>masked_select<span class="token punctuation">(</span>t<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>       
		<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"t:\n{}t_select:\n{}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>t<span class="token punctuation">,</span> t_select<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/b1/e5/H8wGOr1V_o.jpg" alt=""></p> 
<h4><a id="024__Tensor_152"></a>024 | Tensor的变换</h4> 
<p>我们再介绍一种Tensor的变形操作，如“<strong>torch.reshape()</strong>”功能是变换张量形状，注意当张量在内存中是连续时，新张量与input共享数据内存，其主要参数为：</p> 
<ul><li> <p>input：要变换的张量</p> </li><li> <p>shape：表示新张量的形状</p> </li></ul> 
<p>由于变形仅仅是从人类观察视角发生了变化，因此并不改变原始数据的属性。这里有几点需要注意：</p> 
<p>1）torch.reshape()的shape参数中，若出现“-1”则表示对该维度不关系，则基于给定的“4”确定该维度数值即可；</p> 
<p>2）默认内存连续情况下，变形前后的张量共享内存，故而修改后发生连锁反应。</p> 
<pre><code class="prism language-python"><span class="token comment"># 借助torch_reshape()改变张量形状 </span>
flag <span class="token operator">=</span> <span class="token boolean">True</span> 
<span class="token keyword">if</span> flag<span class="token punctuation">:</span>    
		t <span class="token operator">=</span> torch<span class="token punctuation">.</span>randperm<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">)</span>        
		t_reshaple <span class="token operator">=</span> torch<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>t<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        
		<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"t:{}\nt_reshape:{}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>t<span class="token punctuation">,</span> t_reshaple<span class="token punctuation">)</span><span class="token punctuation">)</span>      
		<span class="token comment"># 测试修改张量数据会影响原始张量     </span>
		t<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1024</span>        
		<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"t:{}\nt_reshape:{}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>t<span class="token punctuation">,</span> t_reshaple<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/c9/dd/AfJ6jaeK_o.jpg" alt="">除去常用的torch.reshape()之外，训练模型过程中还常用到矩阵转置，可以借助**torch.transpose()**交换张量的两个维度，其主要参数为：</p> 
<ul><li> <p>input：要变换的张量</p> </li><li> <p>dim0：要交换的维度</p> </li><li> <p>dim1：要交换的维度</p> </li></ul> 
<p>我们还是通过PyCharm代码来说明。可以看到借助torch.transpose()方法，将原先的2个3×4张量，转变为了2个4×3张量，而且原有张量元素的坐标发生了转置交换，如0.1620数据初始索引为index(1,0)，而交换后变为了index(0,1)，作用上实现了矩阵转置。</p> 
<p>为了简洁需要，也可以使用“torch.t()”实现二维张量的转置，其相当于“torch.transpose(input, 0, 1)”。</p> 
<pre><code class="prism language-python"><span class="token comment"># 测试torch.transpose()转置交换维度 </span>
flag <span class="token operator">=</span> <span class="token boolean">True</span> 
<span class="token keyword">if</span> flag<span class="token punctuation">:</span>   
		t <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        
		t_transpose <span class="token operator">=</span> torch<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>t<span class="token punctuation">,</span> dim0<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> dim1<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>        
		<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"t:{}\nt_transpose:{}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>t<span class="token punctuation">,</span> t_transpose<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/ce/93/SY3vpR0J_o.jpg" alt=""></p> 
<h4><a id="025___201"></a>025 | 张量的压缩</h4> 
<p>最后简要介绍一种张量压缩的操作，即借助方法“<strong>torch.squeeze()</strong>”实现压缩长度为1的维度（轴），其重要参数为“dim”，若设置为None时，则移除所有长度为1的轴；若指定维度，当且仅当该轴长度为1时，可以被移除。</p> 
<p>同样地，我们有对应的扩展维度方法“torch.unsqueeze()”，其中参数“dim”指定扩展的维度。</p> 
<p>同理在PyCharm中运行测试代码。</p> 
<p>初始张量维度为（1,2,3,1），当使用torch.squeeze(）时，默认所有长度为1的轴，即移除了第0和第3个维度，则压缩后t_sq应为(2,3)维度。<br> 若指定的维度长度不为1，则不移除；反之也移除该维度。t_0的第0维度移除，而t_1的第1维度未进行移除。</p> 
<pre><code class="prism language-python"><span class="token comment"># 测试torch.squeeze()压缩张量 </span>
flag <span class="token operator">=</span> <span class="token boolean">True</span> 
<span class="token keyword">if</span> flag<span class="token punctuation">:</span>    
		t <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        
		t_sq <span class="token operator">=</span> torch<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>t<span class="token punctuation">)</span>        
		t_0 <span class="token operator">=</span> torch<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>t<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>        
		t_1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>t<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        
		<span class="token keyword">print</span><span class="token punctuation">(</span>t<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>        
		<span class="token keyword">print</span><span class="token punctuation">(</span>t_sq<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>        
		<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"t:{}\nt_sq:{}\nt_0:{}\nt_1:{}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>t<span class="token punctuation">,</span> t_sq<span class="token punctuation">,</span> t_0<span class="token punctuation">,</span> t_1<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/e9/3e/lpEONTYL_o.jpg" alt=""></p> 
<h3><a id="03__Tensor_227"></a>03 | Tensor的数学运算</h3> 
<p>Tensor常用的数学运算主要涉及三类：加减乘除基本运算、对数+指数+幂函数运算以及三角函数等。</p> 
<p>数学运算方法相对简单，日后用到时随时查阅资料即可。</p> 
<p>常见的四则运算方法有：**torch.add() | torch.addcdiv() | torch.addcmul() | torch.sub() | torch.div() | torch.mul()**等</p> 
<p>常见的对数指数运算有：**torch.log() | torch.log10() | torch.log2() | torch.exp() | torch.pow()**等</p> 
<p>常见的三角函数运算有：**torch.abs() | torch.acos() | torch.cos() | torch.cosh() | torch.asin() | torch.atan() | torch.atan2()**等。</p> 
<p>其中torch.add()有些特殊，主要是为了便于神经网络运算，预定义了乘项因子，如torch.add(X, α, Z)的作用是计算下述公式，这里的X和Y均为张量。</p> 
<p><img src="https://images2.imgbox.com/70/68/PAR29Aem_o.png" alt=""></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/2c3699ccaa0419be73b02af0de6ca080/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">如何使用wireshark抓取HTTPS数据包？</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/478c7de28d5a28b3b0e3473b3450356a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Java关键字synchronized</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>