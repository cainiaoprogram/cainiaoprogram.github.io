<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>é»‘è‹¹æœæ£€æµ‹_è‹¹æœæŠ€æœ¯è¿›è¡Œæƒ…ç»ªæ£€æµ‹ - èœé¸Ÿç¨‹åºå‘˜åšå®¢</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="é»‘è‹¹æœæ£€æµ‹_è‹¹æœæŠ€æœ¯è¿›è¡Œæƒ…ç»ªæ£€æµ‹" />
<meta property="og:description" content="é»‘è‹¹æœæ£€æµ‹
ä»‹ç» (Introduction) Before we get our hands dirty, letâ€™s prepare ourselves for whatâ€™s coming next.
åœ¨å¼„è„æ‰‹ä¹‹å‰ï¼Œè®©æˆ‘ä»¬ä¸ºæ¥ä¸‹æ¥å‘ç”Ÿçš„äº‹æƒ…åšå¥½å‡†å¤‡ã€‚ ç¬¬ä¸€ä»¶äº‹ (First things first) Artificial Intelligence can be defined as an area of computer science that has an emphasis on the creation of intelligent machines that can work and react like humans.
äººå·¥æ™ºèƒ½å¯ä»¥å®šä¹‰ä¸ºè®¡ç®—æœºç§‘å­¦é¢†åŸŸï¼Œå…¶é‡ç‚¹æ˜¯åˆ›å»ºå¯ä»¥åƒäººç±»ä¸€æ ·å·¥ä½œå’Œåšå‡ºReactçš„æ™ºèƒ½æœºå™¨ ã€‚ Machine Learning can be defined as a subset of AI, in which machines can learn on their own without being explicitly programmed: they can think and perform actions based on their past experiences." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/0753d54cebffd94f2fa808d9040e4bc6/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-09-19T22:20:51+08:00" />
<meta property="article:modified_time" content="2020-09-19T22:20:51+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="èœé¸Ÿç¨‹åºå‘˜åšå®¢" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">èœé¸Ÿç¨‹åºå‘˜åšå®¢</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">é»‘è‹¹æœæ£€æµ‹_è‹¹æœæŠ€æœ¯è¿›è¡Œæƒ…ç»ªæ£€æµ‹</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <article style="font-size: 16px;"> 
 <p>é»‘è‹¹æœæ£€æµ‹</p> 
 <div> 
  <section> 
   <div> 
    <div> 
     <h2> ä»‹ç» <span style="font-weight: bold;">(</span>Introduction<span style="font-weight: bold;">)</span></h2> 
     <p>Before we get our hands dirty, letâ€™s prepare ourselves for whatâ€™s coming next.</p> 
     <p> åœ¨å¼„è„æ‰‹ä¹‹å‰ï¼Œè®©æˆ‘ä»¬ä¸ºæ¥ä¸‹æ¥å‘ç”Ÿçš„äº‹æƒ…åšå¥½å‡†å¤‡ã€‚ </p> 
     <h3> ç¬¬ä¸€ä»¶äº‹ <span style="font-weight: bold;">(</span>First things first<span style="font-weight: bold;">)</span></h3> 
     <p>Artificial Intelligence can be defined as an area of computer science that has an emphasis on the creation of <strong>intelligent machines</strong> that can work and react like humans.</p> 
     <p> äººå·¥æ™ºèƒ½å¯ä»¥å®šä¹‰ä¸ºè®¡ç®—æœºç§‘å­¦é¢†åŸŸï¼Œå…¶é‡ç‚¹æ˜¯åˆ›å»ºå¯ä»¥åƒäººç±»ä¸€æ ·å·¥ä½œå’Œåšå‡ºReactçš„<strong>æ™ºèƒ½æœºå™¨</strong> ã€‚ </p> 
     <p>Machine Learning can be defined as a <strong>subset</strong> of AI, in which machines can <strong>learn on their own</strong> without being explicitly programmed: they can think and perform actions based on their past experiences.In this way, they can <strong>change their algorithm</strong> based on the data sets on which they are operating.</p> 
     <p> æœºå™¨å­¦ä¹ å¯ä»¥å®šä¹‰ä¸ºAIçš„ä¸€ä¸ª<strong>å­é›†</strong> ï¼Œå…¶ä¸­æœºå™¨å¯ä»¥<strong>åœ¨</strong>ä¸ç»è¿‡æ˜ç¡®ç¼–ç¨‹<strong>çš„</strong>æƒ…å†µä¸‹<strong>è‡ªè¡Œå­¦ä¹ </strong> ï¼šä»–ä»¬å¯ä»¥æ ¹æ®è¿‡å»çš„ç»éªŒæ¥æ€è€ƒå’Œæ‰§è¡ŒåŠ¨ä½œï¼Œä»è€Œå¯ä»¥æ ¹æ®æ•°æ®é›†<strong>æ›´æ”¹ç®—æ³•</strong>ä»–ä»¬åœ¨å…¶ä¸Šè¿è¡Œã€‚ </p> 
     <p>Machine Learningâ€™s popularity is growing day after day and so are the possible use cases, also thanks to the huge amount of data produced by applications.</p> 
     <p> æœºå™¨å­¦ä¹ çš„å—æ¬¢è¿ç¨‹åº¦æ¯å¤©éƒ½åœ¨å¢é•¿ï¼Œå¯èƒ½çš„ç”¨ä¾‹ä¹Ÿåœ¨ä¸æ–­å¢é•¿ï¼Œè¿™ä¹Ÿè¦å½’åŠŸäºåº”ç”¨ç¨‹åºäº§ç”Ÿçš„å¤§é‡æ•°æ®ã€‚ </p> 
     <p>Machine Learning is used anywhere, from automating daily tasks to offering intelligent insights for basically every industry.</p> 
     <p> æœºå™¨å­¦ä¹ æ— å¤„ä¸åœ¨ï¼Œä»è‡ªåŠ¨åŒ–æ—¥å¸¸ä»»åŠ¡åˆ°ä¸ºå‡ ä¹æ¯ä¸ªè¡Œä¸šæä¾›æ™ºèƒ½è§è§£ã€‚ </p> 
     <p>ML is used for prediction, image recognition, or speech recognition. It is trained to <strong>recognize</strong> cancerous tissues, frauds, or to <strong>optimize</strong> businesses.</p> 
     <p> MLç”¨äºé¢„æµ‹ï¼Œå›¾åƒè¯†åˆ«æˆ–è¯­éŸ³è¯†åˆ«ã€‚ ç»è¿‡åŸ¹è®­å¯ä»¥<strong>è¯†åˆ«</strong>ç™Œç»„ç»‡ï¼Œæ¬ºè¯ˆæˆ–<strong>ä¼˜åŒ–</strong>ä¸šåŠ¡ã€‚ </p> 
     <p>Machine learning can be classified into <strong>3 types</strong> of algorithms.</p> 
     <p> æœºå™¨å­¦ä¹ å¯åˆ†ä¸º<strong>3ç§</strong>ç®—æ³•ã€‚ </p> 
     <ul><li><p><strong>Supervised Learning</strong>: we give <strong>labeled</strong> data to the AI system. This means that each data is tagged with the correct label.</p><p> <strong>ç›‘ç£å­¦ä¹ </strong> ï¼šæˆ‘ä»¬å°†<strong>æ ‡è®°çš„</strong>æ•°æ®æä¾›ç»™AIç³»ç»Ÿã€‚ è¿™æ„å‘³ç€æ¯ä¸ªæ•°æ®éƒ½ä½¿ç”¨æ­£ç¡®çš„æ ‡ç­¾è¿›è¡Œäº†æ ‡è®°ã€‚ </p></li><li><p><strong>Unsupervised Learning</strong>: we give <strong>unlabeled</strong>, uncategorized data to the AI system and it acts on the data without any prior training, so the output is dependent upon the coded algorithms.</p><p> <strong>æ— ç›‘ç£å­¦ä¹ </strong> ï¼šæˆ‘ä»¬å°†æœªç»<strong>æ ‡è®°</strong> ï¼Œæœªç»åˆ†ç±»çš„æ•°æ®æä¾›ç»™AIç³»ç»Ÿï¼Œå¹¶ä¸”æ— éœ€äº‹å…ˆåŸ¹è®­å³å¯å¯¹æ•°æ®è¿›è¡Œæ“ä½œï¼Œå› æ­¤è¾“å‡ºå–å†³äºç¼–ç ç®—æ³•ã€‚ </p></li><li><p><strong>Reinforcement Learning</strong>: the system learns with no human intervention: given an environment, it will receive rewards for performing correct actions and penalties for the incorrect ones.</p><p> <strong>å¼ºåŒ–å­¦ä¹ </strong> ï¼šç³»ç»Ÿæ— éœ€äººå·¥å¹²é¢„å³å¯<strong>å­¦ä¹ </strong> ï¼šåœ¨ç‰¹å®šç¯å¢ƒä¸‹ï¼Œæ‰§è¡Œæ­£ç¡®çš„åŠ¨ä½œå°†è·å¾—å¥–åŠ±ï¼Œå¯¹ä¸æ­£ç¡®çš„è¡Œä¸ºå°†å—åˆ°æƒ©ç½šã€‚ </p></li></ul> 
    </div> 
   </div> 
  </section> 
  <section> 
   <div> 
    <div> 
     <blockquote> 
      <p>A machine learning model can be a mathematical representation of a real-world process.</p> 
      <p> æœºå™¨å­¦ä¹ æ¨¡å‹å¯ä»¥æ˜¯ç°å®è¿‡ç¨‹çš„æ•°å­¦è¡¨ç¤ºã€‚ </p> 
     </blockquote> 
     <p>To understand this, we must first know how we come to this point, for the scope of this article, we will talk more specifically about training a classification model.</p> 
     <p> è¦ç†è§£è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é¦–å…ˆå¿…é¡»çŸ¥é“å¦‚ä½•è¾¾åˆ°è¿™ä¸€ç‚¹ï¼Œåœ¨æœ¬æ–‡çš„èŒƒå›´å†…ï¼Œæˆ‘ä»¬å°†æ›´å…·ä½“åœ°è®¨è®ºè®­ç»ƒåˆ†ç±»æ¨¡å‹ã€‚ </p> 
     <h3> è®­ç»ƒ <span style="font-weight: bold;">(</span>Training<span style="font-weight: bold;">)</span></h3> 
     <blockquote> 
      <p>Training a model simply means learning good values</p> 
      <p> è®­ç»ƒæ¨¡å‹ä»…æ„å‘³ç€å­¦ä¹ è‰¯å¥½çš„ä»·å€¼è§‚ </p> 
     </blockquote> 
     <p>A neural network, at first, will try to <strong>guess </strong>the output value <strong>randomly</strong>, then, it will gradually learn from its errors and adjust its <strong>values </strong>(weights)<strong> </strong>based on these.</p> 
     <p> é¦–å…ˆï¼Œç¥ç»ç½‘ç»œå°†å°è¯•<strong>éšæœº</strong> <strong>çŒœæµ‹</strong>è¾“å‡ºå€¼ï¼Œç„¶åé€æ¸ä»é”™è¯¯ä¸­å­¦ä¹ å¹¶è°ƒæ•´å…¶<strong>å€¼</strong> (æƒé‡) <strong> </strong> åŸºäºè¿™äº›ã€‚ </p> 
     <p>There are many types of classification problems:</p> 
     <p> åˆ†ç±»é—®é¢˜æœ‰å¾ˆå¤šç±»å‹ï¼š </p> 
     <ul><li><p><strong>Binary Classification</strong>: predict a binary possibility (one of two possible classes).</p><p> <strong>äºŒè¿›åˆ¶åˆ†ç±»</strong> ï¼šé¢„æµ‹äºŒè¿›åˆ¶å¯èƒ½æ€§(ä¸¤ä¸ªå¯èƒ½çš„ç±»åˆ«ä¹‹ä¸€)ã€‚ </p></li><li><p><strong>Multiclass Classification: </strong>allow you to generate predictions for multiple classes (predict one of more than two outcomes).</p><p> <strong>å¤š</strong>ç±»åˆ«<strong>åˆ†ç±»ï¼š</strong>å…è®¸æ‚¨ç”Ÿæˆå¤šä¸ªç±»åˆ«çš„é¢„æµ‹(é¢„æµ‹ä¸¤ä¸ªä»¥ä¸Šç»“æœä¹‹ä¸€)ã€‚ </p></li></ul> 
     <p>For iOS developers, Apple provides machine learning tools like Core ML, Vision, and NLP. iOS developers have different choices for accessing trained models to <strong>provide inference</strong>:</p> 
     <p> Appleä¸ºiOSå¼€å‘äººå‘˜æä¾›äº†æœºå™¨å­¦ä¹ å·¥å…·ï¼Œä¾‹å¦‚Core MLï¼ŒVisionå’ŒNLPã€‚ iOSå¼€å‘äººå‘˜åœ¨è®¿é—®ç»è¿‡è®­ç»ƒçš„æ¨¡å‹ä»¥<strong>æä¾›æ¨è®ºæ—¶</strong>æœ‰ä¸åŒçš„é€‰æ‹©ï¼š </p> 
     <ul><li>Use Core ML to access a local on-device pre-trained model.<p class="nodelete"></p> ä½¿ç”¨Core MLè®¿é—®æœ¬åœ°çš„è®¾å¤‡ä¸Šé¢„å…ˆè®­ç»ƒçš„æ¨¡å‹ã€‚ </li><li>Host a Machine Learning Model in the cloud and send data from the device to the hosted endpoint to provide predictions.<p class="nodelete"></p> åœ¨äº‘ä¸­æ‰˜ç®¡æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¹¶å°†æ•°æ®ä»è®¾å¤‡å‘é€åˆ°æ‰˜ç®¡ç«¯ç‚¹ä»¥æä¾›é¢„æµ‹ã€‚ </li><li>Call third-party API-Driven Machine Learning cloud managed services where the service hosts and manages a pre-defined trained model. User data is passed through an API call from the device and the service returns the predicted values.<p class="nodelete"></p> è°ƒç”¨ç¬¬ä¸‰æ–¹APIé©±åŠ¨çš„æœºå™¨å­¦ä¹ äº‘æ‰˜ç®¡æœåŠ¡ï¼Œå…¶ä¸­è¯¥æœåŠ¡æ‰˜ç®¡å’Œç®¡ç†é¢„å®šä¹‰çš„ç»è¿‡è®­ç»ƒçš„æ¨¡å‹ã€‚ ç”¨æˆ·æ•°æ®é€šè¿‡è®¾å¤‡çš„APIè°ƒç”¨ä¼ é€’ï¼ŒæœåŠ¡è¿”å›é¢„æµ‹å€¼ã€‚ </li></ul> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/ff/f6/l6Cml98O_o.png" width="3000" height="2000" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption> 
       <a href="https://unsplash.com/@heyerlein?utm_source=medium&amp;utm_medium=referral" rel="noopener nofollow noopener noreferrer" target="_blank">h heyerlein</a> on 
       <a href="https://unsplash.com/@heyerlein?utm_source=medium&amp;utm_medium=referral" rel="noopener nofollow noopener noreferrer" target="_blank">h heyerleinæ‘„</a>äº 
       <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener nofollow noopener noreferrer" target="_blank">Unsplash</a> 
       <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener nofollow noopener noreferrer" target="_blank">Unsplash</a> 
      </figcaption> 
     </figure> 
     <h3> ä»€ä¹ˆæ˜¯åˆ›å»ºMLï¼Ÿ <span style="font-weight: bold;">(</span>What is Create ML?<span style="font-weight: bold;">)</span></h3> 
     <p>Focused at present on vision and natural language data, developers can use Create ML with Swift to create machine learning models, models which are then trained to handle tasks such as understanding text, recognizing photos, or finding relationships between numbers.</p> 
     <p> ç›®å‰ï¼Œå¼€å‘äººå‘˜å¯ä»¥ä¸“æ³¨äºè§†è§‰å’Œè‡ªç„¶è¯­è¨€æ•°æ®ï¼Œå¯ä»¥ä½¿ç”¨å¸¦æœ‰Swiftçš„Create MLåˆ›å»ºæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œç„¶åå¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒä»¥å¤„ç†è¯¸å¦‚ç†è§£æ–‡æœ¬ï¼Œè¯†åˆ«ç…§ç‰‡æˆ–æŸ¥æ‰¾æ•°å­—ä¹‹é—´çš„å…³ç³»ç­‰ä»»åŠ¡ã€‚ </p> 
     <p>It lets developers build machine learning models on their Macs that they can then deploy across Appleâ€™s platforms using <strong>Swift</strong>.</p> 
     <p> å®ƒä½¿å¼€å‘äººå‘˜å¯ä»¥åœ¨Macä¸Šæ„å»ºæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œç„¶åå¯ä»¥ä½¿ç”¨<strong>Swift</strong>åœ¨è‹¹æœçš„å¹³å°ä¸Šè¿›è¡Œéƒ¨ç½²ã€‚ </p> 
     <p>Appleâ€™s decision to commoditize its machine learning tech means developers can build natural language and image classification models much <strong>faster</strong> than the task takes if built from scratch.</p> 
     <p> è‹¹æœå…¬å¸å†³å®šå°†å…¶æœºå™¨å­¦ä¹ æŠ€æœ¯å•†å“åŒ–ï¼Œè¿™æ„å‘³ç€å¼€å‘äººå‘˜å¯ä»¥ä»¥æ¯”ä»å¤´å¼€å§‹æ„å»ºä»»åŠ¡<strong>å¿«å¾—å¤šçš„é€Ÿåº¦</strong>æ„å»ºè‡ªç„¶è¯­è¨€å’Œå›¾åƒåˆ†ç±»æ¨¡å‹ã€‚ </p> 
     <p>It also makes it possible to create these models without the use of third-party AI training systems, such as IBM Watson or TensorFlow (though Create ML supports only very specific models).</p> 
     <p> è¿™ä¹Ÿä½¿åˆ›å»ºè¿™äº›æ¨¡å‹æˆä¸ºå¯èƒ½ï¼Œè€Œæ— éœ€ä½¿ç”¨ç¬¬ä¸‰æ–¹AIåŸ¹è®­ç³»ç»Ÿï¼Œä¾‹å¦‚IBM Watsonæˆ–TensorFlow(å°½ç®¡Create MLä»…æ”¯æŒéå¸¸ç‰¹å®šçš„æ¨¡å‹)ã€‚ </p> 
     <h3> ä»€ä¹ˆæ˜¯æ ¸å¿ƒMLï¼Ÿ <span style="font-weight: bold;">(</span>What is Core ML?<span style="font-weight: bold;">)</span></h3> 
     <p>Core ML is the machine learning framework used across Apple products (macOS, iOS, watchOS, and tvOS) for performing<strong> fast prediction</strong> or <strong>inference</strong> with easy integration of pre-trained machine learning models on the edge, which allows you to perform <strong>real-time predictions</strong> of live images or video on the device.</p> 
     <p> Core MLæ˜¯è·¨Appleäº§å“(macOSï¼ŒiOSï¼ŒwatchOSå’ŒtvOS)ä½¿ç”¨çš„æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºé€šè¿‡è¾¹ç¼˜ä¸Šé¢„å…ˆè®­ç»ƒå¥½çš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„è½»æ¾é›†æˆæ¥æ‰§è¡Œ<strong>å¿«é€Ÿé¢„æµ‹</strong>æˆ–<strong>æ¨æ–­</strong> ï¼Œä»è€Œä½¿æ‚¨å¯ä»¥æ‰§è¡Œ<strong>å®æ—¶é¢„æµ‹</strong>è®¾å¤‡ä¸Šçš„å®æ—¶å›¾åƒæˆ–è§†é¢‘ã€‚ </p> 
     <h3> æœºå™¨å­¦ä¹ çš„ä¼˜åŠ¿ <span style="font-weight: bold;">(</span>Advantages of ML on the edge<span style="font-weight: bold;">)</span></h3> 
     <p><strong>Low Latency</strong> and Near Real-Time Results: You donâ€™t need to make a network API call by sending the data and then waiting for a response. This can be critical for applications such as video processing of successive frames from the on-device camera.</p> 
     <p> <strong>ä½å»¶è¿Ÿ</strong>å’Œè¿‘ä¹å®æ—¶çš„ç»“æœï¼šæ‚¨ä¸éœ€è¦é€šè¿‡å‘é€æ•°æ®ç„¶åç­‰å¾…å“åº”æ¥è¿›è¡Œç½‘ç»œAPIè°ƒç”¨ã€‚ è¿™å¯¹äºåº”ç”¨ç¨‹åº(ä¾‹å¦‚æ¥è‡ªè®¾å¤‡ä¸Šæ‘„åƒå¤´çš„è¿ç»­å¸§çš„è§†é¢‘å¤„ç†)è‡³å…³é‡è¦ã€‚ </p> 
     <p>Availability (Offline), <strong>Privacy</strong>, and Compelling Cost as the application runs without network connection, <strong>no API calls</strong>, and the data<strong> never</strong> leaves the device. Imagine using your mobile device to identify historic tiles while in the subway, catalog private vacation photos while in airplane mode, or detect poisonous plants while in the wilderness.</p> 
     <p> åº”ç”¨ç¨‹åºåœ¨æ²¡æœ‰ç½‘ç»œè¿æ¥çš„æƒ…å†µä¸‹è¿è¡Œï¼Œ <strong>æ²¡æœ‰APIè°ƒç”¨</strong>ä¸”æ•°æ®<strong>æ°¸è¿œä¸ä¼š</strong>ç¦»å¼€è®¾å¤‡ï¼Œå› æ­¤å¯ç”¨æ€§(è„±æœº)ï¼Œ <strong>éšç§</strong>å’Œè¯±äººçš„æˆæœ¬ã€‚ æƒ³è±¡ä¸€ä¸‹ï¼Œä½¿ç”¨ç§»åŠ¨è®¾å¤‡åœ¨åœ°é“ä¸­è¯†åˆ«å†å²ç“·ç –ï¼Œåœ¨é£æœºæ¨¡å¼ä¸­åˆ†ç±»ç§äººåº¦å‡ç…§ç‰‡æˆ–åœ¨æ—·é‡ä¸­æ£€æµ‹æœ‰æ¯’æ¤ç‰©ã€‚ </p> 
     <h3> MLçš„ç¼ºç‚¹ <span style="font-weight: bold;">(</span>Disadvantages of ML on the edge<span style="font-weight: bold;">)</span></h3> 
     <ul><li><p><strong>Application Size</strong>: By adding the model to the device, youâ€™re increasing the size of the app and some accurate models can be quite large.</p><p> <strong>åº”ç”¨ç¨‹åºå¤§å°</strong> ï¼šé€šè¿‡å°†æ¨¡å‹æ·»åŠ åˆ°è®¾å¤‡ä¸­ï¼Œæ‚¨æ­£åœ¨å¢åŠ åº”ç”¨ç¨‹åºçš„å¤§å°ï¼ŒæŸäº›å‡†ç¡®çš„æ¨¡å‹å¯èƒ½ä¼šå¾ˆå¤§ã€‚ </p></li><li><p><strong>System Utilization</strong>: Prediction and inference on the mobile device involves lots of computation, which increases battery drain. Older devices may struggle to provide real-time predictions.</p><p> <strong>ç³»ç»Ÿåˆ©ç”¨ç‡</strong> ï¼šå¯¹ç§»åŠ¨è®¾å¤‡çš„é¢„æµ‹å’Œæ¨æ–­æ¶‰åŠå¤§é‡è®¡ç®—ï¼Œè¿™ä¼šå¢åŠ ç”µæ± æ¶ˆè€—ã€‚ è¾ƒæ—§çš„è®¾å¤‡å¯èƒ½éš¾ä»¥æä¾›å®æ—¶é¢„æµ‹ã€‚ </p></li><li><p><strong>Model Training</strong>: In most cases, the model on the device must be continually trained outside of the device with new user data. Once the model is retrained, the app will need to be updated with the new model, and depending on the size of the model, this could strain network transfer for the user. Refer back to the application size challenge listed above, and now we have a <strong>potential user experience problem</strong>.</p><p> <strong>æ¨¡å‹è®­ç»ƒ</strong> ï¼šåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå¿…é¡»ä½¿ç”¨æ–°çš„ç”¨æˆ·æ•°æ®åœ¨è®¾å¤‡å¤–éƒ¨æŒç»­è®­ç»ƒè®¾å¤‡ä¸Šçš„æ¨¡å‹ã€‚ é‡æ–°è®­ç»ƒæ¨¡å‹åï¼Œå°†éœ€è¦ä½¿ç”¨æ–°æ¨¡å‹æ›´æ–°åº”ç”¨ç¨‹åºï¼Œå¹¶ä¸”æ ¹æ®æ¨¡å‹çš„å¤§å°ï¼Œè¿™å¯èƒ½ä¼šç»™ç”¨æˆ·å¸¦æ¥ç½‘ç»œä¼ è¾“å‹åŠ›ã€‚ è¿”å›ä¸Šé¢åˆ—å‡ºçš„åº”ç”¨ç¨‹åºå¤§å°æŒ‘æˆ˜ï¼Œç°åœ¨æˆ‘ä»¬æœ‰ä¸€ä¸ª<strong>æ½œåœ¨çš„ç”¨æˆ·ä½“éªŒé—®é¢˜</strong> ã€‚ </p></li></ul> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/73/fe/YlCdgN4R_o.png" width="3882" height="2584" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        Photo by 
       <a href="https://unsplash.com/@cgower?utm_source=medium&amp;utm_medium=referral" rel="noopener nofollow noopener noreferrer" target="_blank">Christopher Gower</a> on 
       <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener nofollow noopener noreferrer" target="_blank">Unsplash</a> 
      </figcaption> 
      <figcaption> 
       <a href="https://unsplash.com/@cgower?utm_source=medium&amp;utm_medium=referral" rel="noopener nofollow noopener noreferrer" target="_blank">Christopher Gower</a>åœ¨ 
       <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener nofollow noopener noreferrer" target="_blank">Unsplash</a>ä¸Šçš„ 
       <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener nofollow noopener noreferrer" target="_blank">ç…§ç‰‡</a> 
      </figcaption> 
     </figure> 
     <h2> å¼„è„ä½ çš„æ‰‹ <span style="font-weight: bold;">(</span>Getting your hands dirty<span style="font-weight: bold;">)</span></h2> 
     <p>As we dive deeper into the core of this article, we are assuming that you are quite familiar with an iOS Development environment and you have some basic knowledge about python.</p> 
     <p> å½“æˆ‘ä»¬æ·±å…¥ç ”ç©¶æœ¬æ–‡çš„æ ¸å¿ƒæ—¶ï¼Œæˆ‘ä»¬å‡è®¾æ‚¨å¯¹iOSå¼€å‘ç¯å¢ƒéå¸¸ç†Ÿæ‚‰ï¼Œå¹¶ä¸”æ‚¨å…·æœ‰æœ‰å…³pythonçš„ä¸€äº›åŸºæœ¬çŸ¥è¯†ã€‚ </p> 
     <h3> ç”¨pythonè½¬æ¢æ¨¡å‹ <span style="font-weight: bold;">(</span>Converting a model with python<span style="font-weight: bold;">)</span></h3> 
     <p>Now letâ€™s say that we found an interesting model on the web, unfortunately, we notice itâ€™s <strong>not in CoreML format</strong>, but we absolutely want to use it in our iOS app and thereâ€™s no other way to obtain it, we can just try to convert it.</p> 
     <p> ç°åœ¨ï¼Œæˆ‘ä»¬è¯´æˆ‘ä»¬åœ¨ç½‘ç»œä¸Šæ‰¾åˆ°äº†ä¸€ä¸ªæœ‰è¶£çš„æ¨¡å‹ï¼Œä¸å¹¸çš„æ˜¯ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°å®ƒ<strong>ä¸æ˜¯CoreMLæ ¼å¼çš„</strong> ï¼Œä½†æ˜¯æˆ‘ä»¬ç»å¯¹å¸Œæœ›åœ¨æˆ‘ä»¬çš„iOSåº”ç”¨ä¸­ä½¿ç”¨å®ƒï¼Œå¹¶ä¸”æ²¡æœ‰å…¶ä»–æ–¹æ³•å¯ä»¥è·å–å®ƒï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•å°†å…¶è½¬æ¢ã€‚ </p> 
     <p>Apple has a specific tool to accomplish this task, a python module called <em>coremltools</em> that can be found at <a href="https://github.com/apple/coremltools" target="_blank" rel="noopener nofollow noopener noreferrer">this link</a>.</p> 
     <p> Appleæœ‰ä¸€ä¸ªå®Œæˆæ­¤ä»»åŠ¡çš„ç‰¹å®šå·¥å…·ï¼Œä¸€ä¸ªåä¸º<em>coremltools</em>çš„pythonæ¨¡å—ï¼Œå¯ä»¥åœ¨<a href="https://github.com/apple/coremltools" target="_blank" rel="noopener nofollow noopener noreferrer">æ­¤é“¾æ¥ä¸­</a>æ‰¾åˆ°ã€‚ </p> 
     <p>The <a href="https://github.com/priya-dwivedi/face_and_emotion_detection" target="_blank" rel="noopener nofollow noopener noreferrer"><strong>interesting model</strong></a> is built with keras (tensorflow as backend) and itâ€™s about emotion detection, you can download it from <a href="https://github.com/priya-dwivedi/face_and_emotion_detection/blob/master/emotion_detector_models/model_v6_23.hdf5" target="_blank" rel="noopener nofollow noopener noreferrer">here</a>. Letâ€™s now convert it, first of all weâ€™ll install the required packages. For <strong>compatibility reasons</strong>, please use <em>python 2.7 </em>and packagesâ€™ specified versions, as <em>coremltools</em> relies on these.</p> 
     <p> è¿™ä¸ª<a href="https://github.com/priya-dwivedi/face_and_emotion_detection" target="_blank" rel="noopener nofollow noopener noreferrer"><strong>æœ‰è¶£çš„æ¨¡å‹</strong></a>æ˜¯ä½¿ç”¨keras(tensorflowä½œä¸ºåç«¯)æ„å»ºçš„ï¼Œå®ƒä¸æƒ…æ„Ÿæ£€æµ‹æœ‰å…³ï¼Œæ‚¨å¯ä»¥ä»<a href="https://github.com/priya-dwivedi/face_and_emotion_detection/blob/master/emotion_detector_models/model_v6_23.hdf5" target="_blank" rel="noopener nofollow noopener noreferrer">æ­¤å¤„</a>ä¸‹è½½ã€‚ ç°åœ¨è®©æˆ‘ä»¬å¯¹å…¶è¿›è¡Œè½¬æ¢ï¼Œé¦–å…ˆæˆ‘ä»¬å°†å®‰è£…æ‰€éœ€çš„è½¯ä»¶åŒ…ã€‚ å‡ºäº<strong>å…¼å®¹æ€§åŸå› </strong> ï¼Œè¯·ä½¿ç”¨<em>python 2.7</em>å’Œè½¯ä»¶åŒ…çš„æŒ‡å®šç‰ˆæœ¬ï¼Œå› ä¸º<em>coremltools</em>ä¾èµ–äºè¿™äº›ç‰ˆæœ¬ã€‚ </p> 
     <p>One final note, since we are using a deprecated version of python, we create a virtual environment to run our code.</p> 
     <p> æœ€åä¸€ç‚¹ï¼Œç”±äºæˆ‘ä»¬ä½¿ç”¨çš„Pythonç‰ˆæœ¬å·²å¼ƒç”¨ï¼Œå› æ­¤æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªè™šæ‹Ÿç¯å¢ƒæ¥è¿è¡Œæˆ‘ä»¬çš„ä»£ç ã€‚ </p> 
     <p>Final final note, <strong>your path</strong> to Python 2.7 <strong>might be different</strong>, if youâ€™re using Mac OS or Linux, check your /usr/bin/ directory. If youâ€™re using Windows, check the path in which you decided to install python.</p> 
     <p> æœ€åçš„æœ€åä¸€ç‚¹ï¼Œ <strong>æ‚¨</strong>ä½¿ç”¨Python 2.7 <strong>çš„è·¯å¾„</strong> <strong>å¯èƒ½æœ‰æ‰€ä¸åŒ</strong> ï¼Œå¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯Mac OSæˆ–Linuxï¼Œè¯·æ£€æŸ¥/ usr / bin /ç›®å½•ã€‚ å¦‚æœæ‚¨ä½¿ç”¨Windowsï¼Œè¯·æ£€æŸ¥å†³å®šå®‰è£…pythonçš„è·¯å¾„ã€‚ </p> 
     <pre><code class="has">pip3 install virtualenvvirtualenv -p /usr/bin/python2.7 venv</code></pre> 
     <p>Now we activate the virtual environment we just created.</p> 
     <p> ç°åœ¨ï¼Œæˆ‘ä»¬æ¿€æ´»åˆšåˆšåˆ›å»ºçš„è™šæ‹Ÿç¯å¢ƒã€‚ </p> 
     <pre><code class="has">source venv/bin/activate</code></pre> 
     <p>And finally, we install our dependencies.</p> 
     <p> æœ€åï¼Œæˆ‘ä»¬å®‰è£…ä¾èµ–é¡¹ã€‚ </p> 
     <pre><code class="has">pip install coremltools keras==2.2.4 tensorflow==1.14.0</code></pre> 
     <p>After this, we can start writing our script. ğŸš€</p> 
     <p> ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹ç¼–å†™è„šæœ¬äº†ã€‚ ğŸš€ </p> 
     <p>Create a file named <strong>converter.py,</strong> the first step will be to import coremltools.</p> 
     <p> åˆ›å»ºä¸€ä¸ªåä¸º<strong>converter.py</strong>çš„æ–‡ä»¶<strong>ï¼Œ</strong>ç¬¬ä¸€æ­¥å°†æ˜¯å¯¼å…¥coremltoolsã€‚ </p> 
     <pre><code class="has">import coremltools</code></pre> 
     <p>Last but not least, we convert our model into a <strong>.mlmodel</strong> one.</p> 
     <p> æœ€åä½†å¹¶éæœ€ä¸é‡è¦çš„ä¸€ç‚¹æ˜¯ï¼Œæˆ‘ä»¬å°†æ¨¡å‹è½¬æ¢ä¸º<strong>.mlmodel</strong> ã€‚ </p> 
     <pre><code class="has">output_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']ml_model = coremltools.converters.keras.convert(<br>    './model_v6.h5', <br>    input_names=['image'], <br>    output_names=['output'], <br>    class_labels=output_labels, <br>    image_input_names='image'<br>)ml_model.save('./model_v6.mlmodel')</code></pre> 
     <p>As you can see, the first line is about <strong>output labels</strong>, this is the most important thing we need to know before converting a model, otherwise, the results will be useless for us since we will not be able to know what the output is about.</p> 
     <p> å¦‚æ‚¨æ‰€è§ï¼Œç¬¬ä¸€è¡Œæ˜¯å…³äº<strong>è¾“å‡ºæ ‡ç­¾çš„</strong> ï¼Œè¿™æ˜¯æˆ‘ä»¬åœ¨è½¬æ¢æ¨¡å‹ä¹‹å‰éœ€è¦äº†è§£çš„æœ€é‡è¦çš„äº‹æƒ…ï¼Œå¦åˆ™ï¼Œç»“æœå¯¹äºæˆ‘ä»¬æ¥è¯´å°†æ˜¯æ— ç”¨çš„ï¼Œå› ä¸ºæˆ‘ä»¬å°†æ— æ³•çŸ¥é“è¾“å‡ºæ˜¯å…³äºä»€ä¹ˆçš„ã€‚ </p> 
     <p>The second line is the main instruction of the script, it calls the Keras<strong> converter</strong> from the coremltools converters and converts our model based on our specifications about input and output (in this case we are specifying that we need an image as input and <strong>output_labels</strong> as output).</p> 
     <p> ç¬¬äºŒè¡Œæ˜¯è„šæœ¬çš„ä¸»è¦æŒ‡ä»¤ï¼Œå®ƒä»coremltoolsè½¬æ¢å™¨è°ƒç”¨Keras <strong>è½¬æ¢å™¨</strong>å’Œæˆ‘ä»¬çš„æ¨¡å‹åŸºäºæˆ‘ä»¬å¯¹è¾“å…¥å’Œè¾“å‡ºè§„æ ¼è½¬æ¢(åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æŒ‡å®šï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå›¾åƒä½œä¸ºè¾“å…¥å’Œ<strong>output_labels</strong>ä½œä¸ºè¾“å‡º)ã€‚ </p> 
     <p>Finally, we <strong>save</strong> the converted model that is ready to use in our app.</p> 
     <p> æœ€åï¼Œæˆ‘ä»¬<strong>ä¿å­˜</strong>å·²è½¬æ¢çš„æ¨¡å‹ï¼Œå‡†å¤‡åœ¨æˆ‘ä»¬çš„åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨ã€‚ </p> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/f2/b4/pr30UHl3_o.png" width="3648" height="2048" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        Photo by 
       <a href="https://unsplash.com/@hiteshchoudhary?utm_source=medium&amp;utm_medium=referral" rel="noopener nofollow noopener noreferrer" target="_blank">Hitesh Choudhary</a> on 
       <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener nofollow noopener noreferrer" target="_blank">Unsplash</a> 
      </figcaption> 
      <figcaption> 
       <a href="https://unsplash.com/@hiteshchoudhary?utm_source=medium&amp;utm_medium=referral" rel="noopener nofollow noopener noreferrer" target="_blank">Hitesh Choudhary</a> 
       <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener nofollow noopener noreferrer" target="_blank">æ‘„</a>äº 
       <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener nofollow noopener noreferrer" target="_blank">Unsplash</a> 
      </figcaption> 
     </figure> 
     <h2> ä½¿ç”¨AppleæŠ€æœ¯è¿›è¡Œæœºå™¨å­¦ä¹  <span style="font-weight: bold;">(</span>Machine Learning with Apple technologies<span style="font-weight: bold;">)</span></h2> 
     <p>This is what we expect our final result to be.</p> 
     <p> è¿™å°±æ˜¯æˆ‘ä»¬æœŸæœ›çš„æœ€ç»ˆç»“æœã€‚ </p> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div style="text-align: center;"> 
          <img alt="A quick tour of the application" src="https://images2.imgbox.com/c4/69/gv3w9AeU_o.gif" width="384" height="848" style="outline: none;"> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        A quick tour of the finished app 
      </figcaption> 
      <figcaption>
        å¿«é€Ÿæµè§ˆå®Œæˆçš„åº”ç”¨ç¨‹åº 
      </figcaption> 
     </figure> 
     <p>The first thing to do is to get a good model for our scope.</p> 
     <p> é¦–å…ˆè¦åšçš„æ˜¯ä¸ºæˆ‘ä»¬çš„èŒƒå›´å»ºç«‹ä¸€ä¸ªè‰¯å¥½çš„æ¨¡å‹ã€‚ </p> 
     <h3> é€šè¿‡CreateMLåº”ç”¨ç¨‹åºåˆ›å»ºæ¨¡å‹ <span style="font-weight: bold;">(</span>Creation of a Model via CreateML app<span style="font-weight: bold;">)</span></h3> 
     <p>CreateML app is an application presented in the WWDC 2019 for Xcode 11.0 and Swift 5. This application allows everyone to create an ML Model without having a very big knowledge about training an ML model. Itâ€™s only necessary to find the information we want to use to train the model, label them (because the base of CreateML is the <strong>Supervised Training</strong>) and import everything in the application.</p> 
     <p> CreateMLåº”ç”¨ç¨‹åºæ˜¯WWDC 2019ä¸­é’ˆå¯¹Xcode 11.0å’ŒSwift 5æ¨å‡ºçš„åº”ç”¨ç¨‹åºã€‚æ­¤åº”ç”¨ç¨‹åºä½¿æ¯ä¸ªäººéƒ½å¯ä»¥åˆ›å»ºMLæ¨¡å‹ï¼Œè€Œæ— éœ€æŒæ¡æœ‰å…³è®­ç»ƒMLæ¨¡å‹çš„å¤§é‡çŸ¥è¯†ã€‚ åªéœ€æ‰¾åˆ°æˆ‘ä»¬æƒ³è¦ç”¨æ¥è®­ç»ƒæ¨¡å‹çš„ä¿¡æ¯ï¼Œå¯¹å…¶è¿›è¡Œæ ‡è®°(å› ä¸ºCreateMLçš„åŸºç¡€æ˜¯<strong>Supervised Training</strong> )ï¼Œç„¶åå°†æ‰€æœ‰å†…å®¹å¯¼å…¥åº”ç”¨ç¨‹åºã€‚ </p> 
     <p>Now we will use the CreateML app to train our model which will <strong>recognize</strong> our emotions.</p> 
     <p> ç°åœ¨ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨CreateMLåº”ç”¨ç¨‹åºæ¥è®­ç»ƒå°†<strong>è¯†åˆ«</strong>æˆ‘ä»¬æƒ…ç»ªçš„æ¨¡å‹ã€‚ </p> 
     <p>First of all, you have to find the images. I suggest to find a very rich dataset of images because the precision is very important, but the images mustnâ€™t have a very high resolution. After that, you have to <strong>divide your images</strong> into several categories you decide, in the base of emotions you want to recognize and create a folder for each emotion. Then you have to create two super folders: train and test. The first folder is richer of images and itâ€™s the folder where you put the images for training the model, the second folder is used to test the model just trained.</p> 
     <p> é¦–å…ˆï¼Œæ‚¨å¿…é¡»æ‰¾åˆ°å›¾åƒã€‚ æˆ‘å»ºè®®æ‰¾åˆ°ä¸€ä¸ªéå¸¸ä¸°å¯Œçš„å›¾åƒæ•°æ®é›†ï¼Œå› ä¸ºç²¾åº¦éå¸¸é‡è¦ï¼Œä½†æ˜¯å›¾åƒä¸èƒ½å…·æœ‰å¾ˆé«˜çš„åˆ†è¾¨ç‡ã€‚ ä¹‹åï¼Œæ‚¨å¿…é¡»æ ¹æ®è¦è¯†åˆ«çš„æƒ…ç»ª<strong>å°†å›¾åƒåˆ’åˆ†</strong>ä¸ºå‡ ä¸ªç±»åˆ«ï¼Œå¹¶ä¸ºæ¯ç§æƒ…ç»ªåˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤¹ã€‚ ç„¶åï¼Œæ‚¨å¿…é¡»åˆ›å»ºä¸¤ä¸ªè¶…çº§æ–‡ä»¶å¤¹ï¼šè®­ç»ƒå’Œæµ‹è¯•ã€‚ ç¬¬ä¸€ä¸ªæ–‡ä»¶å¤¹åŒ…å«ä¸°å¯Œçš„å›¾åƒï¼Œè¿™æ˜¯æ”¾ç½®å›¾åƒä»¥è®­ç»ƒæ¨¡å‹çš„æ–‡ä»¶å¤¹ï¼Œç¬¬äºŒä¸ªæ–‡ä»¶å¤¹ç”¨äºæµ‹è¯•åˆšåˆšè®­ç»ƒçš„æ¨¡å‹ã€‚ </p> 
     <p>If this operation could be annoying, donâ€™t worry: lots of datasets have a .csv file where there are the <strong>classification already done</strong>! You have only to write a simple script and solve the problem! Here is an example in Python 3.8:</p> 
     <p> å¦‚æœæ­¤æ“ä½œå¯èƒ½å¾ˆçƒ¦äººï¼Œè¯·ä¸è¦æ‹…å¿ƒï¼šè®¸å¤šæ•°æ®é›†éƒ½æœ‰ä¸€ä¸ª.csvæ–‡ä»¶ï¼Œå…¶ä¸­<strong>å·²ç»å®Œæˆ</strong>äº†<strong>åˆ†ç±»</strong> ï¼ æ‚¨åªéœ€è¦ç¼–å†™ä¸€ä¸ªç®€å•çš„è„šæœ¬å³å¯è§£å†³é—®é¢˜ï¼ è¿™æ˜¯Python 3.8ä¸­çš„ç¤ºä¾‹ï¼š </p> 
     <pre><code class="has">import csv<br>import os<br>import sysdef main():<br>    input_path = â€œPATH_OF_YOUR_FOLDERâ€<br>    file_name = â€œFILE_NAME.csvâ€<br>    file = open(file_name,â€râ€)<br>    data = csv.reader(file)<br>    next(data) #this is used to avoid the first line<br>    for info in data:<br>        label_path = os.path.join(input_path, info[-1])<br>        #here you have to consider the structure of .csv filedestination_path = â€œmv â€œ + info[1] + â€œ â€œ + label_path<br>        os.system(destination_path)<br>        print(â€œimages movedâ€)<br>    file.close()if <strong>name</strong> == â€˜<strong>main</strong>â€™ : main()</code></pre> 
     <p>After that, press and hold â€œcontrolâ€ button on the keyboard (if you are using macOS Catalina and Xcode 11) and click on the dock icon of Xcode. You should see a menu called â€œ<strong>Open Developer Tool</strong>â€ and after, <strong>CreateML</strong>; then you click on â€œNew Documentâ€. You should see an interface where you can choose the right template of our MLModel: we will <strong>classify</strong> images, so we choose â€œ<strong>Image Classifier</strong>â€, that is the first template. Next, we will give the name of your project, the name of License, a short description, and where to save the project file.</p> 
     <p> ä¹‹åï¼ŒæŒ‰ä½é”®ç›˜ä¸Šçš„â€œæ§åˆ¶â€æŒ‰é’®(å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯macOS Catalinaå’ŒXcode 11)ï¼Œç„¶åå•å‡»Xcoâ€‹â€‹deçš„åœé å›¾æ ‡ã€‚ æ‚¨åº”è¯¥çœ‹åˆ°ä¸€ä¸ªåä¸ºâ€œ <strong>Open Developer Tool</strong> â€çš„èœå•ï¼Œ <strong>å…¶åæ˜¯CreateML</strong> ï¼› ç„¶åå•å‡»â€œæ–°å»ºæ–‡æ¡£â€ã€‚ æ‚¨åº”è¯¥çœ‹åˆ°ä¸€ä¸ªç•Œé¢ï¼Œæ‚¨å¯ä»¥åœ¨å…¶ä¸­é€‰æ‹©MLModelçš„æ­£ç¡®æ¨¡æ¿ï¼šæˆ‘ä»¬å°†å¯¹å›¾åƒ<strong>è¿›è¡Œåˆ†ç±»</strong> ï¼Œå› æ­¤æˆ‘ä»¬é€‰æ‹©â€œ <strong>Image Classifier</strong> â€ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ¨¡æ¿ã€‚ æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æä¾›æ‚¨çš„é¡¹ç›®åç§°ï¼Œè®¸å¯è¯åç§°ï¼Œç®€çŸ­è¯´æ˜ä»¥åŠé¡¹ç›®æ–‡ä»¶çš„ä¿å­˜ä½ç½®ã€‚ </p> 
     <p>Now we have to select the images for training and testing, and the only thing to do is to<strong> drag</strong> the â€œtrainâ€ folder in the â€œTrainâ€ section and â€œtestâ€ folder in the â€œTestâ€ section. Validation must be set in â€œAutoâ€. Then we have to choose a maximum of <strong>iterations</strong> (600 should be good) and then press â€œStartâ€ at the top of the interface. The other commands in the bottom part are only to edit images for being more useful in the training process, but in this situation, we havenâ€™t needed it.</p> 
     <p> ç°åœ¨ï¼Œæˆ‘ä»¬å¿…é¡»é€‰æ‹©è¦è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•çš„å›¾åƒï¼Œå”¯ä¸€è¦åšçš„å°±æ˜¯<strong>æ‹–åŠ¨</strong> â€œè®­ç»ƒâ€éƒ¨åˆ†ä¸­çš„â€œè®­ç»ƒâ€æ–‡ä»¶å¤¹å’Œâ€œæµ‹è¯•â€éƒ¨åˆ†ä¸­çš„â€œæµ‹è¯•â€æ–‡ä»¶å¤¹ã€‚ éªŒè¯å¿…é¡»åœ¨â€œè‡ªåŠ¨â€ä¸­è®¾ç½®ã€‚ ç„¶åï¼Œæˆ‘ä»¬å¿…é¡»é€‰æ‹©æœ€å¤§<strong>è¿­ä»£æ¬¡æ•°</strong> (600ä¸ªåº”è¯¥æ˜¯å¥½çš„)ï¼Œç„¶ååœ¨ç•Œé¢é¡¶éƒ¨æŒ‰â€œå¼€å§‹â€ã€‚ åº•éƒ¨çš„å…¶ä»–å‘½ä»¤ä»…ç”¨äºç¼–è¾‘å›¾åƒï¼Œä»¥ä¾¿åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æœ‰ç”¨ï¼Œä½†æ˜¯åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸éœ€è¦å®ƒã€‚ </p> 
     <p>After a long time, we have our characteristics about our model and, in the top right, <strong>our model</strong>. You have only to drag this model out of CreateML windows and drop it in an external folder or Desktop.</p> 
     <p> ç»è¿‡å¾ˆé•¿ä¸€æ®µæ—¶é—´ï¼Œæˆ‘ä»¬å¯¹æ¨¡å‹æœ‰äº†è‡ªå·±çš„ç‰¹å¾ï¼Œåœ¨å³ä¸Šè§’ï¼Œ <strong>æˆ‘ä»¬çš„æ¨¡å‹</strong>ä¹Ÿæœ‰äº†ã€‚ æ‚¨åªéœ€å°†æ­¤æ¨¡å‹æ‹–å‡ºCreateMLçª—å£ï¼Œç„¶åå°†å…¶æ”¾åœ¨å¤–éƒ¨æ–‡ä»¶å¤¹æˆ–æ¡Œé¢ä¸­å³å¯ã€‚ </p> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/de/93/qe9Uws3W_o.png" width="5000" height="3313" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </div> 
      </div> 
      <figcaption>
        Photo by 
       <a href="https://unsplash.com/@yancymin?utm_source=medium&amp;utm_medium=referral" rel="noopener nofollow noopener noreferrer" target="_blank">Yancy Min</a> on 
       <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener nofollow noopener noreferrer" target="_blank">Unsplash</a> 
      </figcaption> 
      <figcaption> 
       <a href="https://unsplash.com/@yancymin?utm_source=medium&amp;utm_medium=referral" rel="noopener nofollow noopener noreferrer" target="_blank">Yancy Min</a>åœ¨ 
       <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener nofollow noopener noreferrer" target="_blank">Unsplash</a>ä¸Š 
       <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener nofollow noopener noreferrer" target="_blank">æ‹æ‘„çš„</a>ç…§ç‰‡ 
      </figcaption> 
     </figure> 
     <h3> ç»“åˆä½¿ç”¨CoreMLå’ŒVision <span style="font-weight: bold;">(</span>Using CoreML with Vision<span style="font-weight: bold;">)</span></h3> 
     <p>For creating our software we need two frameworks and an MLModel based on image classification (created before or converted from other models): these frameworks are <strong>Vision</strong> and <strong>CoreML</strong>.</p> 
     <p> ä¸ºäº†åˆ›å»ºæˆ‘ä»¬çš„è½¯ä»¶ï¼Œæˆ‘ä»¬éœ€è¦ä¸¤ä¸ªæ¡†æ¶å’Œä¸€ä¸ªåŸºäºå›¾åƒåˆ†ç±»çš„MLModel(åœ¨å…¶ä»–æ¨¡å‹ä¹‹å‰åˆ›å»ºæˆ–ä»å…¶ä»–æ¨¡å‹è½¬æ¢è€Œæ¥)ï¼šè¿™äº›æ¡†æ¶æ˜¯<strong>Vision</strong>å’Œ<strong>CoreML</strong> ã€‚ </p> 
     <p>We already talked about CoreML, but <strong>whatâ€™s Vision about</strong>?</p> 
     <p> æˆ‘ä»¬å·²ç»è®¨è®ºè¿‡CoreMLï¼Œä½†<strong>æ„¿æ™¯æ˜¯</strong>ä»€ä¹ˆï¼Ÿ </p> 
     <p>Vision permits us to <strong>manipulate</strong> images and videos using <strong>Computer Vision Algorithms</strong> for lots of operations like face and face landmark detection, text detection, barcode recognition, image registration, and general feature tracking. We will use the CoreML interfacing for classifying images.</p> 
     <p> Visionå…è®¸æˆ‘ä»¬ä½¿ç”¨<strong>è®¡ç®—æœºè§†è§‰ç®—æ³•</strong>æ¥<strong>å¤„ç†</strong>å›¾åƒå’Œè§†é¢‘ï¼Œä»¥è¿›è¡Œè®¸å¤šæ“ä½œï¼Œä¾‹å¦‚é¢éƒ¨å’Œé¢éƒ¨ç•Œæ ‡æ£€æµ‹ï¼Œæ–‡æœ¬æ£€æµ‹ï¼Œæ¡å½¢ç è¯†åˆ«ï¼Œå›¾åƒé…å‡†ä»¥åŠä¸€èˆ¬ç‰¹å¾è·Ÿè¸ªã€‚ æˆ‘ä»¬å°†ä½¿ç”¨CoreMLæ¥å£å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ã€‚ </p> 
     <p>To start with this tutorial, first of all, clone <a href="https://gitlab.com/Giovygio97/workshop-start.git" rel="noopener nofollow noopener noreferrer" target="_blank">this repository</a>: in it, there is a simple application (written in Swift 5.2 and compatible for iOS 13.2) with a simple ViewController where there is an UIImageView and a Label. The first is used to show the images we choose to verify emotions, the second is used to identify the emotion and indicate the precision of our image classification.There is also a Greyscale converted because lots of datasets give us grayscale images and, for this reason, image classification is more correct.</p> 
     <p> é¦–å…ˆä»æœ¬æ•™ç¨‹å¼€å§‹ï¼Œå…‹éš†<a href="https://gitlab.com/Giovygio97/workshop-start.git" rel="noopener nofollow noopener noreferrer" target="_blank">è¯¥å­˜å‚¨åº“</a> ï¼šåœ¨å…¶ä¸­ï¼Œæœ‰ä¸€ä¸ªç®€å•çš„åº”ç”¨ç¨‹åº(ç”¨Swift 5.2ç¼–å†™ï¼Œå¹¶ä¸”ä¸iOS 13.2å…¼å®¹)å’Œä¸€ä¸ªç®€å•çš„ViewControllerï¼Œå…¶ä¸­æœ‰ä¸€ä¸ªUIImageViewå’Œä¸€ä¸ªLabelã€‚ ç¬¬ä¸€ä¸ªç”¨äºæ˜¾ç¤ºæˆ‘ä»¬é€‰æ‹©ç”¨æ¥éªŒè¯æƒ…ç»ªçš„å›¾åƒï¼Œç¬¬äºŒä¸ªç”¨äºè¯†åˆ«æƒ…ç»ªå¹¶æŒ‡ç¤ºæˆ‘ä»¬å›¾åƒåˆ†ç±»çš„ç²¾åº¦ã€‚è¿˜æœ‰ä¸€ä¸ªç°åº¦è½¬æ¢ï¼Œå› ä¸ºè®¸å¤šæ•°æ®é›†ä¸ºæˆ‘ä»¬æä¾›äº†ç°åº¦å›¾åƒï¼Œå› æ­¤ï¼Œå›¾åƒåˆ†ç±»æ¯”è¾ƒæ­£ç¡®ã€‚ </p> 
     <p>Now letâ€™s start:</p> 
     <p> ç°åœ¨å¼€å§‹ï¼š </p> 
     <ol><li>Create a new file called â€œPredictionManager.swiftâ€ where we can implement our classification function.<p class="nodelete"></p> åˆ›å»ºä¸€ä¸ªåä¸ºâ€œ PredictionManager.swiftâ€çš„æ–°æ–‡ä»¶ï¼Œåœ¨å…¶ä¸­æˆ‘ä»¬å¯ä»¥å®ç°åˆ†ç±»åŠŸèƒ½ã€‚ </li><li>Save it in the folder of your app.<p class="nodelete"></p> å°†å…¶ä¿å­˜åœ¨åº”ç”¨ç¨‹åºçš„æ–‡ä»¶å¤¹ä¸­ã€‚ </li><li>Import UIKit, CoreML and Vision in your project<p class="nodelete"></p> åœ¨æ‚¨çš„é¡¹ç›®ä¸­å¯¼å…¥UIKitï¼ŒCoreMLå’ŒVision </li><li>Add your model to the project. To do this just drag and drop the .mlmodel file in the project folder opened in Xcode navigator, then select â€œCopy as Groupâ€.<p class="nodelete"></p> å°†æ¨¡å‹æ·»åŠ åˆ°é¡¹ç›®ä¸­ã€‚ ä¸ºæ­¤ï¼Œåªéœ€å°†.mlmodelæ–‡ä»¶æ‹–æ”¾åˆ°Xcodeå¯¼èˆªå™¨ä¸­æ‰“å¼€çš„é¡¹ç›®æ–‡ä»¶å¤¹ä¸­ï¼Œç„¶åé€‰æ‹©â€œå¤åˆ¶ä¸ºç»„â€ã€‚ </li></ol> 
     <p>Now letâ€™s start to write code! ğŸ¥³</p> 
     <p> ç°åœ¨è®©æˆ‘ä»¬å¼€å§‹ç¼–å†™ä»£ç ï¼ ğŸ¥³ </p> 
     <p>First, we create a class, called PredictionManager, with two variables:</p> 
     <p> é¦–å…ˆï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåä¸ºPredictionManagerçš„ç±»ï¼Œå…¶ä¸­åŒ…å«ä¸¤ä¸ªå˜é‡ï¼š </p> 
     <pre><code class="has">var emotionModel: <strong>MLModel</strong><br>var visionModel: <strong>VNCoreMLModel</strong></code></pre> 
     <p>The first variable is the MLModel we consider for our project, instead, the second variable is a Vision Container where we put our MLModel (trained for images) and make operations on it (called â€œVNCoreMLRequestâ€)</p> 
     <p> ç¬¬ä¸€ä¸ªå˜é‡æ˜¯æˆ‘ä»¬ä¸ºé¡¹ç›®è€ƒè™‘çš„MLModelï¼Œç¬¬äºŒä¸ªå˜é‡æ˜¯Visionå®¹å™¨ï¼Œæˆ‘ä»¬åœ¨å…¶ä¸­æ”¾ç½®äº†MLModel(é’ˆå¯¹å›¾åƒè¿›è¡Œè®­ç»ƒ)å¹¶å¯¹å…¶è¿›è¡Œæ“ä½œ(ç§°ä¸ºâ€œ VNCoreMLRequestâ€) </p> 
     <p>After declaration, letâ€™s create the constructor:</p> 
     <p> å£°æ˜åï¼Œè®©æˆ‘ä»¬åˆ›å»ºæ„é€ å‡½æ•°ï¼š </p> 
     <pre><code class="has">init() {<!-- --><br>    self.emotionModel = EmotionClassificator().model<br>    do{<!-- --><br>        self.visionModel = try VNCoreMLModel(for: self.emotionModel)<br>    }catch{<!-- --><br>        fatalError(â€œUnable to create Vision Modelâ€¦â€)<br>    }<br>}</code></pre> 
     <p>Firstly we assigned to the MLModel variable our model (in this case called â€œ<strong>EmotionClassificator</strong>â€, but in general the name of this class is equal to the .mlmodel file name) because every .mlmodel file creates a class called like the model, and this class is usable for every operation with CoreML, but to access to its implementation, you have to open the .mlmodel file and click on the arrow on the right of the Model name.</p> 
     <p> é¦–å…ˆï¼Œæˆ‘ä»¬å°†æ¨¡å‹åˆ†é…ç»™MLModelå˜é‡(åœ¨æœ¬ä¾‹ä¸­ä¸ºâ€œ <strong>EmotionClassificator</strong> â€ï¼Œä½†é€šå¸¸æ­¤ç±»çš„åç§°ç­‰äº.mlmodelæ–‡ä»¶çš„åç§°)ï¼Œå› ä¸ºæ¯ä¸ª.mlmodelæ–‡ä»¶éƒ½ä¼šåˆ›å»ºä¸€ä¸ªç§°ä¸ºæ¨¡å‹çš„ç±»ï¼Œå¹¶ä¸”è¯¥ç±»å¯ç”¨äºCoreMLçš„æ¯ä¸ªæ“ä½œï¼Œä½†æ˜¯è¦è®¿é—®å…¶å®ç°ï¼Œæ‚¨å¿…é¡»æ‰“å¼€.mlmodelæ–‡ä»¶ï¼Œç„¶åå•å‡»Modelåç§°å³ä¾§çš„ç®­å¤´ã€‚ </p> 
     <p>Then we assign to <strong>visionModel</strong> the MLModel if this model is compatible with Vision.</p> 
     <p> ç„¶åï¼Œå¦‚æœæ­¤æ¨¡å‹ä¸Visionå…¼å®¹ï¼Œåˆ™å°†MLModelåˆ†é…ç»™<strong>visionModel</strong> ã€‚ </p> 
     <p>Now we can start with our function:</p> 
     <p> ç°åœ¨æˆ‘ä»¬å¯ä»¥ä»å‡½æ•°å¼€å§‹ï¼š </p> 
     <pre><code class="has">func classification(for image: UIImage, complete: <a href="http://twitter.com/escaping" rel="noopener nofollow noopener noreferrer" target="_blank">@escaping</a> (String) -&gt; Void)</code></pre> 
     <p>For classifying our image we have to use an image (UIImage) and we will have to output a String (here we can have a string with <a href="http://twitter.com/escaping" rel="noopener nofollow noopener noreferrer" target="_blank">@escaping</a> closure, that prevents us to delete information, here in type â€œStringâ€ when function variables are de-allocated).</p> 
     <p> ä¸ºäº†å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ï¼Œæˆ‘ä»¬å¿…é¡»ä½¿ç”¨å›¾åƒ(UIImage)ï¼Œå¹¶ä¸”å¿…é¡»è¾“å‡ºä¸€ä¸ªString(è¿™é‡Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¸¦<a href="http://twitter.com/escaping" rel="noopener nofollow noopener noreferrer" target="_blank">@escaping</a>é—­åŒ…çš„å­—ç¬¦ä¸²ï¼Œä»¥é˜²æ­¢æˆ‘ä»¬åˆ é™¤ä¿¡æ¯ï¼Œæ­¤å¤„çš„å‡½æ•°å˜é‡æ˜¯â€œ Stringâ€ç±»å‹)ã€‚å–æ¶ˆåˆ†é…)ã€‚ </p> 
     <p>Now, the first thing we have to do is the VNCoreMLRequest, to create the request to our MLModel:</p> 
     <p> ç°åœ¨ï¼Œæˆ‘ä»¬è¦åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯VNCoreMLRequestï¼Œä»¥åˆ›å»ºå¯¹MLModelçš„è¯·æ±‚ï¼š </p> 
     <pre><code class="has"><strong>func</strong> classification(for image: UIImage, complete: <a href="http://twitter.com/escaping" rel="noopener nofollow noopener noreferrer" target="_blank">@escaping</a> (String) -&gt; Void){<!-- --><strong>let</strong> request = VNCoreMLRequest(model: self.visionModel {(request,error) in<strong>guard</strong> error == nil else {complete(â€œErrorâ€); <br>        return<br>    }<strong>guard</strong> let results = request.results as [VNClassificationObservation], let firstResult = results.first<strong>else</strong> { <br>            complete(â€œNo Resultsâ€); <br>            return<br>        }<br>        complete(String(format: â€œ%@ %.1f%%â€, firstResult.identifier, firstResult.confidence * 100)) <br>    }<br>}</code></pre> 
     <p>Our <strong>VNCoreMLRequest</strong> needs the <strong>VNCoreMLModel</strong> to operate the requests and then we consider three situations:</p> 
     <p> æˆ‘ä»¬çš„<strong>VNCoreMLRequest</strong>éœ€è¦<strong>VNCoreMLModel</strong>æ¥æ“ä½œè¯·æ±‚ï¼Œç„¶åæˆ‘ä»¬è€ƒè™‘ä¸‰ç§æƒ…å†µï¼š </p> 
     <ul><li><p>the model<strong> isnâ€™t useful</strong> for our purpose;</p><p> è¯¥æ¨¡å‹å¯¹æˆ‘ä»¬çš„ç›®çš„<strong>æ²¡æœ‰ç”¨</strong> ï¼› </p></li><li><p>the entire request (where the results are represented like VNClassificationObservation) <strong>doesnâ€™t give any result;</strong></p><p> æ•´ä¸ªè¯·æ±‚(ç»“æœè¡¨ç¤ºä¸ºVNClassificationObservation) <strong>ä¸ç»™å‡ºä»»ä½•ç»“æœï¼›</strong> </p></li><li><p>choosing the first result (the most precise), we will <strong>print our information </strong>(the classification and the confidence).</p><p> é€‰æ‹©ç¬¬ä¸€ä¸ªç»“æœ(æœ€ç²¾ç¡®çš„ç»“æœ)ï¼Œæˆ‘ä»¬å°†<strong>æ‰“å°æˆ‘ä»¬çš„ä¿¡æ¯</strong> (åˆ†ç±»å’Œç½®ä¿¡åº¦)ã€‚ </p></li></ul> 
     <p>To have more precision, we will <strong>crop</strong> the images to the center:</p> 
     <p> ä¸ºäº†è·å¾—æ›´é«˜çš„ç²¾åº¦ï¼Œæˆ‘ä»¬å°†å›¾åƒ<strong>è£å‰ª</strong>åˆ°ä¸­å¿ƒï¼š </p> 
     <pre><code class="has">request.imageCropAndScaleOption = .centerCrop</code></pre> 
     <p>Now we need a <strong>handler</strong> to handle every request to the VNCoreMLModel, but first, we have to give it an image filtered and optimized for our process: for this reason, we create a CIImage (Core Image) and we will give it a prefixed orientation with <strong>CGImagePropertyOprientation</strong>:</p> 
     <p> ç°åœ¨æˆ‘ä»¬éœ€è¦ä¸€ä¸ª<strong>å¤„ç†ç¨‹åº</strong>æ¥å¤„ç†å¯¹VNCoreMLModelçš„æ¯ä¸ªè¯·æ±‚ï¼Œä½†æ˜¯é¦–å…ˆï¼Œæˆ‘ä»¬å¿…é¡»ä¸ºå®ƒæä¾›ç»è¿‡è¿‡æ»¤å’Œä¼˜åŒ–çš„å›¾åƒï¼Œä»¥ä¾¿é’ˆå¯¹æˆ‘ä»¬çš„æµç¨‹è¿›è¡Œå¤„ç†ï¼šåŸºäºè¿™ä¸ªåŸå› ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªCIImage(æ ¸å¿ƒå›¾åƒ)ï¼Œå¹¶ä¸ºå…¶æŒ‡å®šå‰ç¼€ä¸<strong>CGImagePropertyOprientation</strong> ï¼š </p> 
     <pre><code class="has"><strong>guard</strong> let ciImage = CIImage(image: image) <strong>else</strong> { complete(â€œerror creating imageâ€); return}<strong>let</strong> orientation = CGImagePropertyOrientation(rawValue: UInt32(image.imageOrientation.rawValue))</code></pre> 
     <p>And now itâ€™s time to build the request:</p> 
     <p> ç°åœ¨æ˜¯æ—¶å€™å»ºç«‹è¯·æ±‚äº†ï¼š </p> 
     <pre><code class="has">DispatchQueue.global(qos: .userInitiated).async {<!-- --><strong>let</strong> handler = VNImageRequestHandler(ciImage: ciImage, orientation: orientation!)<strong>do</strong> {<!-- --><br>        try handler.perform([request])<br>    } <strong>catch</strong> {<!-- --><br>        complete(â€œFailed to perform classification.â€)<br>    }}</code></pre> 
     <p>To help our application, we will permit the handler to go in the global area and be activated <strong>only</strong> on the request of the user (when the user chooses the image). This operation will be <strong>asynchronous</strong>, so it will be executed independently from the rest of the app.</p> 
     <p> ä¸ºäº†å¸®åŠ©æˆ‘ä»¬çš„åº”ç”¨ç¨‹åºï¼Œæˆ‘ä»¬å°†å…è®¸å¤„ç†ç¨‹åºè¿›å…¥å…¨å±€åŒºåŸŸï¼Œå¹¶ä¸”<strong>ä»…</strong>åœ¨ç”¨æˆ·è¯·æ±‚æ—¶(å½“ç”¨æˆ·é€‰æ‹©å›¾åƒæ—¶)æ¿€æ´»å¤„ç†ç¨‹åºã€‚ è¯¥æ“ä½œå°†æ˜¯<strong>å¼‚æ­¥çš„</strong> ï¼Œå› æ­¤å®ƒå°†ç‹¬ç«‹äºåº”ç”¨ç¨‹åºçš„å…¶ä½™éƒ¨åˆ†æ‰§è¡Œã€‚ </p> 
     <p>In the end, we will build our handler (using the CIImage created before and the orientation) and try to perform requests created before.</p> 
     <p> æœ€åï¼Œæˆ‘ä»¬å°†æ„å»ºå¤„ç†ç¨‹åº(ä½¿ç”¨ä¹‹å‰åˆ›å»ºçš„CIImageå’Œæ–¹å‘)å¹¶å°è¯•æ‰§è¡Œä¹‹å‰åˆ›å»ºçš„è¯·æ±‚ã€‚ </p> 
     <p>Now we complete our function of classification. Letâ€™s go to call it in the <strong>ViewController</strong>.</p> 
     <p> ç°åœ¨ï¼Œæˆ‘ä»¬å®Œæˆäº†åˆ†ç±»åŠŸèƒ½ã€‚ è®©æˆ‘ä»¬åœ¨<strong>ViewControllerä¸­</strong>è°ƒç”¨å®ƒã€‚ </p> 
     <p>In the Extension of our ViewController, after the dismiss, letâ€™s write this:</p> 
     <p> åœ¨æˆ‘ä»¬çš„ViewControllerçš„æ‰©å±•ä¸­ï¼Œè§£é›‡ä¹‹åï¼Œè®©æˆ‘ä»¬è¿™æ ·å†™ï¼š </p> 
     <pre><code class="has">let monoImage = image.mono</code></pre> 
     <p>Here we convert our image in mono-image, after we will</p> 
     <p> åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†å›¾åƒè½¬æ¢ä¸ºå•å›¾åƒå </p> 
     <pre><code class="has">predictionManager.classification(for: monoImage) { (result) inDispatchQueue.main.async { [weak self] in<br>        self?.predictionLabel.text = result<br>    }}</code></pre> 
     <p>After the classification, the result will be processed in the main thread (DispatchQueue.main.async), and, with a <strong>weak self</strong> we will give the result of our classification.</p> 
     <p> åˆ†ç±»ä¹‹åï¼Œç»“æœå°†åœ¨ä¸»çº¿ç¨‹(DispatchQueue.main.async)ä¸­è¿›è¡Œå¤„ç†ï¼Œå¹¶ä¸”ä½¿ç”¨<strong>å¼±è‡ªèº«ï¼Œ</strong>æˆ‘ä»¬å°†ç»™å‡ºåˆ†ç±»ç»“æœã€‚ </p> 
     <p>Now you can classify emotions! ğŸ¤© What are you waiting for? Try in on your iPhone!</p> 
     <p> ç°åœ¨æ‚¨å¯ä»¥å¯¹æƒ…ç»ªè¿›è¡Œåˆ†ç±»ï¼ youæ‚¨è¿˜åœ¨ç­‰ä»€ä¹ˆï¼Ÿ å°è¯•åœ¨æ‚¨çš„iPhoneä¸Šï¼ </p> 
     <p>For the complete project, <strong>check out</strong> our repository:</p> 
     <p> å¯¹äºå®Œæ•´çš„é¡¹ç›®ï¼Œ <strong>è¯·æŸ¥çœ‹</strong>æˆ‘ä»¬çš„å­˜å‚¨åº“ï¼š </p> 
     <div> 
      <a href="https://gitlab.com/Giovygio97/emotiondetectioncoreml.git" rel="noopener nofollow noopener noreferrer" target="_blank"></a> 
     </div> 
     <h3> å›¢é˜Ÿ-NoSynapses <span style="font-weight: bold;">(</span>The team â€” NoSynapses<span style="font-weight: bold;">)</span></h3> 
     <p><a href="https://www.linkedin.com/in/giovanni-prisco-362335183/" rel="noopener nofollow noopener noreferrer" target="_blank">Giovanni Prisco</a></p> 
     <p> <a href="https://www.linkedin.com/in/giovanni-prisco-362335183/" rel="noopener nofollow noopener noreferrer" target="_blank">ä¹”ç“¦å°¼Â·æ™®é‡Œæ–¯ç§‘(Giovanni Prisco)</a> </p> 
     <p><a href="http://linkedin.com/in/giovanni-di-guida-228287165" rel="noopener nofollow noopener noreferrer" target="_blank">Giovanni Di Guida</a></p> 
     <p> <a href="http://linkedin.com/in/giovanni-di-guida-228287165" rel="noopener nofollow noopener noreferrer" target="_blank">ä¹”å‡¡å°¼Â·è¿ªÂ·å‰è¾¾</a> </p> 
     <p><a href="https://www.linkedin.com/in/iamantonioalfonso/" rel="noopener nofollow noopener noreferrer" target="_blank">Antonio Alfonso</a> (also on <a target="_blank" rel="nofollow noopener noreferrer" href="https://medium.com/@iamantonioalfonso">Medium</a>)</p> 
     <p> <a href="https://www.linkedin.com/in/iamantonioalfonso/" rel="noopener nofollow noopener noreferrer" target="_blank">å®‰ä¸œå°¼å¥¥Â·é˜¿æ–¹ç´¢</a> (ä¹Ÿåœ¨<a target="_blank" rel="nofollow noopener noreferrer" href="https://medium.com/@iamantonioalfonso">Mediumä¸Š</a> ) </p> 
     <p><a href="https://www.linkedin.com/in/simoneserracassano/" rel="noopener nofollow noopener noreferrer" target="_blank">Simone Serra Cassano</a></p> 
     <p> <a href="https://www.linkedin.com/in/simoneserracassano/" rel="noopener nofollow noopener noreferrer" target="_blank">è¥¿è’™å¦®Â·å¡æ‹‰Â·å¡è¨è¯º</a> </p> 
     <p><a href="https://www.linkedin.com/in/vincenzocoppola94/" rel="noopener nofollow noopener noreferrer" target="_blank">Vincenzo Coppola</a></p> 
     <p> <a href="https://www.linkedin.com/in/vincenzocoppola94/" rel="noopener nofollow noopener noreferrer" target="_blank">æ–‡æ£®ä½Â·ç§‘æ³¢æ‹‰</a> </p> 
     <p><a href="http://t.me/Eth3real" rel="noopener nofollow noopener noreferrer" target="_blank">Simone Formisano</a></p> 
     <p> <a href="http://t.me/Eth3real" rel="noopener nofollow noopener noreferrer" target="_blank">è¥¿è’™å¨œÂ·ç¦ç±³è¨è¯º</a> </p> 
    </div> 
   </div> 
  </section> 
 </div> 
 <blockquote> 
  <p>ç¿»è¯‘è‡ª: <a href="https://medium.com/apple-developer-academy-federico-ii/emotion-detection-with-apple-technologies-b782beaa5c44" rel="nofollow">https://medium.com/apple-developer-academy-federico-ii/emotion-detection-with-apple-technologies-b782beaa5c44</a></p> 
 </blockquote> 
 <p>é»‘è‹¹æœæ£€æµ‹</p> 
</article>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ec4b128d5f72f6f044c122743c44786f/" rel="prev">
			<span class="pager__subtitle">Â«&thinsp;Previous</span>
			<p class="pager__title">PHPä½¿ç”¨PHPMailerå‘é€é‚®ä»¶</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/43024e6489592f85a8fb523da8b14bca/" rel="next">
			<span class="pager__subtitle">Next&thinsp;Â»</span>
			<p class="pager__title">outlook æ”¶ä»¶ç®±åˆ†ç»„_å¦‚ä½•åœ¨Macç‰ˆOutlook 2016ä¸­ç¦ç”¨ç»Ÿä¸€æ”¶ä»¶ç®±ï¼ˆå’Œåˆ†ç»„æ–‡ä»¶å¤¹ï¼‰</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 èœé¸Ÿç¨‹åºå‘˜åšå®¢.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>