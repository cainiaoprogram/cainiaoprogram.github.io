<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>机器学习（11）线性回归（1）理论：损失函数(含最小二乘法)、正规方程、梯度下降、回归性能评估（均方差） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="机器学习（11）线性回归（1）理论：损失函数(含最小二乘法)、正规方程、梯度下降、回归性能评估（均方差）" />
<meta property="og:description" content="目录
一、线性回归基础理论
1、定义与公式
2、线性关系（单特征与双特征）
2-1、单特征线性关系
2-2、双特征平面关系
2-3、其他线性模型
二、线性回归的损失和优化原理
1、损失函数
1、定义
2、公式
3、损失函数举例
2、损失函数 -- 最小二乘法
2、优化方法：
2-1、正规方程 1、公式
2、原理
2-2、梯度下降
1、单特征
2、双特征
3、梯度下降过程（单特征）
4、梯度下降过程（双特征）
3、正规方程与梯度下降优缺点对比
三、回归性能评估（均方差）
公式
API
一、线性回归基础理论 1、定义与公式 定义：
线性回归：利用回归方程，对一个或多个自变量（特征值）和因变量（目标值）之间关系进行建模的一种分析方式。
线性关系公式 ：
权重系数：w
偏置：b 2、线性关系（单特征与双特征） 2-1、单特征线性关系 2-2、双特征平面关系 2-3、其他线性模型 （该图是线性模型，但不是线性关系）
二、线性回归的损失和优化 尽可能让假定的参数贴近真实的参数，越贴近，结果越准确。
1、损失函数 1、定义 损失函数（loss function）：是用来估量模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数，通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好。
2、公式 其中，前面的均值函数表示的是经验风险函数，L代表的是损失函数，后面的ΦΦ是正则化项（regularizer）或者叫惩罚项（penalty term），它可以是L1，也可以是L2，或者其他的正则函数。整个式子的目的：找到使目标函数最小时的θ值（损失最小的情况）。
3、损失函数举例 1、log对数损失函数（逻辑回归）
2、平方损失函数（最小二乘法）
3、指数损失函数（Adaboost）
4、Hinge损失函数（SVM）
5、0-1损失函数
6、绝对值损失函数
4、损失函数 -- 最小二乘法 2、优化方法： 优化目的：让损失函数取得最小值。
优化方法：
1、正规方程；
2、梯度下降。
2-1、正规方程 1、公式 2、原理 优点：不需要试错，可以直接取得最小值，比较快捷。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/21eabbac0b7dae85b97c40e6afa0814e/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-09-11T17:30:40+08:00" />
<meta property="article:modified_time" content="2021-09-11T17:30:40+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">机器学习（11）线性回归（1）理论：损失函数(含最小二乘法)、正规方程、梯度下降、回归性能评估（均方差）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E4%B8%80%E3%80%81%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA" rel="nofollow" title="一、线性回归基础理论">一、线性回归基础理论</a></p> 
<p id="1%E3%80%81%E5%AE%9A%E4%B9%89%E4%B8%8E%E5%85%AC%E5%BC%8F-toc" style="margin-left:40px;"><a href="#1%E3%80%81%E5%AE%9A%E4%B9%89%E4%B8%8E%E5%85%AC%E5%BC%8F" rel="nofollow" title="1、定义与公式">1、定义与公式</a></p> 
<p id="2%E3%80%81%E7%BA%BF%E6%80%A7%E5%85%B3%E7%B3%BB%EF%BC%88%E5%8D%95%E7%89%B9%E5%BE%81%E4%B8%8E%E5%8F%8C%E7%89%B9%E5%BE%81%EF%BC%89-toc" style="margin-left:40px;"><a href="#2%E3%80%81%E7%BA%BF%E6%80%A7%E5%85%B3%E7%B3%BB%EF%BC%88%E5%8D%95%E7%89%B9%E5%BE%81%E4%B8%8E%E5%8F%8C%E7%89%B9%E5%BE%81%EF%BC%89" rel="nofollow" title="2、线性关系（单特征与双特征）">2、线性关系（单特征与双特征）</a></p> 
<p id="%E5%8D%95%E7%89%B9%E5%BE%81%E7%BA%BF%E6%80%A7%E5%85%B3%E7%B3%BB%EF%BC%9A%C2%A0-toc" style="margin-left:80px;"><a href="#%E5%8D%95%E7%89%B9%E5%BE%81%E7%BA%BF%E6%80%A7%E5%85%B3%E7%B3%BB%EF%BC%9A%C2%A0" rel="nofollow" title="2-1、单特征线性关系">2-1、单特征线性关系</a></p> 
<p id="%E5%8F%8C%E7%89%B9%E5%BE%81%E5%B9%B3%E9%9D%A2%E5%85%B3%E7%B3%BB%EF%BC%9A%C2%A0-toc" style="margin-left:80px;"><a href="#%E5%8F%8C%E7%89%B9%E5%BE%81%E5%B9%B3%E9%9D%A2%E5%85%B3%E7%B3%BB%EF%BC%9A%C2%A0" rel="nofollow" title="2-2、双特征平面关系">2-2、双特征平面关系</a></p> 
<p id="%E5%85%B6%E4%BB%96%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%EF%BC%9A-toc" style="margin-left:80px;"><a href="#%E5%85%B6%E4%BB%96%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%EF%BC%9A" rel="nofollow" title="2-3、其他线性模型">2-3、其他线性模型</a></p> 
<p id="%E4%BA%8C%E3%80%81%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%92%8C%E4%BC%98%E5%8C%96%E5%8E%9F%E7%90%86-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%92%8C%E4%BC%98%E5%8C%96%E5%8E%9F%E7%90%86" rel="nofollow" title="二、线性回归的损失和优化原理">二、线性回归的损失和优化原理</a></p> 
<p id="1%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-toc" style="margin-left:40px;"><a href="#1%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0" rel="nofollow" title="1、损失函数">1、损失函数</a></p> 
<p id="1%E3%80%81%E5%AE%9A%E4%B9%89-toc" style="margin-left:80px;"><a href="#1%E3%80%81%E5%AE%9A%E4%B9%89" rel="nofollow" title="1、定义">1、定义</a></p> 
<p id="2%E3%80%81%E5%85%AC%E5%BC%8F-toc" style="margin-left:80px;"><a href="#2%E3%80%81%E5%85%AC%E5%BC%8F" rel="nofollow" title="2、公式">2、公式</a></p> 
<p id="3%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%BE%E4%BE%8B-toc" style="margin-left:80px;"><a href="#3%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%BE%E4%BE%8B" rel="nofollow" title="3、损失函数举例">3、损失函数举例</a></p> 
<p id="1%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%2F%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95-toc" style="margin-left:40px;"><a href="#1%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%2F%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95" rel="nofollow" title="2、损失函数 -- 最小二乘法">2、损失函数 -- 最小二乘法</a></p> 
<p id="2%E3%80%81%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%EF%BC%9A-toc" style="margin-left:40px;"><a href="#2%E3%80%81%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%EF%BC%9A" rel="nofollow" title="2、优化方法：">2、优化方法：</a></p> 
<p id="-toc" style="margin-left:80px;"></p> 
<p id="2-1%E3%80%81%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%C2%A0-toc" style="margin-left:40px;"><a href="#2-1%E3%80%81%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%C2%A0" rel="nofollow" title="2-1、正规方程 ">2-1、正规方程 </a></p> 
<p id="1%E3%80%81%E5%85%AC%E5%BC%8F-toc" style="margin-left:80px;"><a href="#1%E3%80%81%E5%85%AC%E5%BC%8F" rel="nofollow" title="1、公式">1、公式</a></p> 
<p id="2%E3%80%81%E5%8E%9F%E7%90%86-toc" style="margin-left:80px;"><a href="#2%E3%80%81%E5%8E%9F%E7%90%86" rel="nofollow" title="2、原理">2、原理</a></p> 
<p id="2-2%E3%80%81%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-toc" style="margin-left:40px;"><a href="#2-2%E3%80%81%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D" rel="nofollow" title="2-2、梯度下降">2-2、梯度下降</a></p> 
<p id="1%E3%80%81%E5%8D%95%E7%89%B9%E5%BE%81-toc" style="margin-left:80px;"><a href="#1%E3%80%81%E5%8D%95%E7%89%B9%E5%BE%81" rel="nofollow" title="1、单特征">1、单特征</a></p> 
<p id="2%E3%80%81%E5%8F%8C%E7%89%B9%E5%BE%81-toc" style="margin-left:80px;"><a href="#2%E3%80%81%E5%8F%8C%E7%89%B9%E5%BE%81" rel="nofollow" title="2、双特征">2、双特征</a></p> 
<p id="3%E3%80%81%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E8%BF%87%E7%A8%8B%EF%BC%88%E5%8D%95%E7%89%B9%E5%BE%81%EF%BC%89-toc" style="margin-left:80px;"><a href="#3%E3%80%81%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E8%BF%87%E7%A8%8B%EF%BC%88%E5%8D%95%E7%89%B9%E5%BE%81%EF%BC%89" rel="nofollow" title="3、梯度下降过程（单特征）">3、梯度下降过程（单特征）</a></p> 
<p id="%C2%A04%E3%80%81%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E8%BF%87%E7%A8%8B%EF%BC%88%E5%8F%8C%E7%89%B9%E5%BE%81%EF%BC%89-toc" style="margin-left:80px;"><a href="#%C2%A04%E3%80%81%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E8%BF%87%E7%A8%8B%EF%BC%88%E5%8F%8C%E7%89%B9%E5%BE%81%EF%BC%89" rel="nofollow" title=" 4、梯度下降过程（双特征）"> 4、梯度下降过程（双特征）</a></p> 
<p id="3%E3%80%81%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E4%B8%8E%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%BC%98%E7%BC%BA%E7%82%B9%E5%AF%B9%E6%AF%94-toc" style="margin-left:40px;"><a href="#3%E3%80%81%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E4%B8%8E%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%BC%98%E7%BC%BA%E7%82%B9%E5%AF%B9%E6%AF%94" rel="nofollow" title="3、正规方程与梯度下降优缺点对比">3、正规方程与梯度下降优缺点对比</a></p> 
<p id="%E4%B8%89%E3%80%81%E5%9B%9E%E5%BD%92%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0%EF%BC%88%E5%9D%87%E6%96%B9%E5%B7%AE%EF%BC%89-toc" style="margin-left:0px;"><a href="#%E4%B8%89%E3%80%81%E5%9B%9E%E5%BD%92%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0%EF%BC%88%E5%9D%87%E6%96%B9%E5%B7%AE%EF%BC%89" rel="nofollow" title="三、回归性能评估（均方差）">三、回归性能评估（均方差）</a></p> 
<p id="%E5%85%AC%E5%BC%8F-toc" style="margin-left:40px;"><a href="#%E5%85%AC%E5%BC%8F" rel="nofollow" title="公式">公式</a></p> 
<p id="API-toc" style="margin-left:40px;"><a href="#API" rel="nofollow" title="API">API</a></p> 
<p id="%E4%B8%89%E3%80%81%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E4%BC%98%E5%8C%96%EF%BC%88%E6%B3%A2%E5%A3%AB%E9%A1%BF%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%EF%BC%89-toc" style="margin-left:0px;"></p> 
<hr id="hr-toc"> 
<h2 id="%E4%B8%80%E3%80%81%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA">一、线性回归基础理论</h2> 
<h3 id="1%E3%80%81%E5%AE%9A%E4%B9%89%E4%B8%8E%E5%85%AC%E5%BC%8F">1、定义与公式</h3> 
<p id="%E5%AE%9A%E4%B9%89%EF%BC%9A"><strong>定义：</strong></p> 
<blockquote> 
 <p><strong>线性回归：利用回归方程，对一个或多个自变量（特征值）和因变量（目标值）之间关系进行建模的一种分析方式。</strong></p> 
</blockquote> 
<p></p> 
<p id="%E7%BA%BF%E6%80%A7%E5%85%B3%E7%B3%BB%E5%85%AC%E5%BC%8F%EF%BC%9A%C2%A0"><strong>线性关系公式 ：</strong></p> 
<blockquote> 
 <p><img alt="" height="126" src="https://images2.imgbox.com/39/7c/ag11fT1G_o.png" width="474"></p> 
 <p> <img alt="" height="59" src="https://images2.imgbox.com/08/04/QRflV3dn_o.png" width="279"></p> 
 <p><strong>权重系数：w</strong></p> 
 <p><strong>偏置：b </strong></p> 
</blockquote> 
<p></p> 
<h3 id="2%E3%80%81%E7%BA%BF%E6%80%A7%E5%85%B3%E7%B3%BB%EF%BC%88%E5%8D%95%E7%89%B9%E5%BE%81%E4%B8%8E%E5%8F%8C%E7%89%B9%E5%BE%81%EF%BC%89">2、线性关系（单特征与双特征）</h3> 
<h4 id="%E5%8D%95%E7%89%B9%E5%BE%81%E7%BA%BF%E6%80%A7%E5%85%B3%E7%B3%BB%EF%BC%9A%C2%A0"><strong>2-1、单特征线性关系</strong></h4> 
<p><img alt="" height="315" src="https://images2.imgbox.com/42/d7/PBlP8I0z_o.png" width="430"></p> 
<h4 id="%E5%8F%8C%E7%89%B9%E5%BE%81%E5%B9%B3%E9%9D%A2%E5%85%B3%E7%B3%BB%EF%BC%9A%C2%A0"><strong>2-2、双特征平面关系</strong></h4> 
<p> <img alt="" height="410" src="https://images2.imgbox.com/9f/9f/gbhxIjLG_o.png" width="434"></p> 
<p></p> 
<h4 id="%E5%85%B6%E4%BB%96%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%EF%BC%9A">2-3、其他线性模型</h4> 
<p><img alt="" height="245" src="https://images2.imgbox.com/e3/6a/gBPexAm2_o.png" width="355"></p> 
<p>（该图是线性模型，但不是线性关系）</p> 
<p> <img alt="" height="99" src="https://images2.imgbox.com/84/9a/a9HmWtsL_o.png" width="375"></p> 
<p></p> 
<p></p> 
<h2 id="%E4%BA%8C%E3%80%81%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%92%8C%E4%BC%98%E5%8C%96%E5%8E%9F%E7%90%86">二、线性回归的损失和优化</h2> 
<blockquote> 
 <p><strong>尽可能让假定的参数贴近真实的参数，越贴近，结果越准确。</strong></p> 
</blockquote> 
<p><img alt="" height="48" src="https://images2.imgbox.com/ca/1e/RkwMk7DF_o.png" width="595"></p> 
<p></p> 
<h3 id="1%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0">1、损失函数</h3> 
<h4 id="1%E3%80%81%E5%AE%9A%E4%B9%89">1、定义</h4> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="color:#000000;">        损失函数（loss function）：</span><span style="color:#000000;">是用来</span><span style="color:#fe2c24;"><strong>估量模型的预测值f(x)与真实值Y的不一致程度</strong></span><span style="color:#000000;">，它是一个非负实值函数，通常使用L(Y, f(x))来表示，</span><span style="color:#fe2c24;"><strong>损失函数越小，模型的鲁棒性就越好</strong></span><span style="color:#000000;">。</span></p> 
<p></p> 
<h4 id="2%E3%80%81%E5%85%AC%E5%BC%8F" style="margin-left:.0001pt;text-align:justify;"><span style="color:#000000;">2、公式</span></h4> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="58" src="https://images2.imgbox.com/10/d9/1E8kEFQg_o.png" width="319"></p> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="color:#000000;">其中，前面的均值函数表示的是经验风险函数，L代表的是损失函数，后面的</span><span style="color:#000000;">Φ</span><span style="color:#000000;">Φ</span><span style="color:#000000;">是正则化项（regularizer）或者叫惩罚项（penalty term），它可以是L1，也可以是L2，或者其他的正则函数。整个式子的目的：</span><span style="color:#fe2c24;"><strong><strong>找到使目标函数最小时的</strong></strong>θ<strong><strong>值（损失最小的情况）</strong></strong></span><span style="color:#000000;">。</span></p> 
<p></p> 
<h4 id="3%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%BE%E4%BE%8B">3、损失函数举例</h4> 
<blockquote> 
 <p><strong>1、log对数损失函数（逻辑回归）</strong></p> 
 <p><strong>2、平方损失函数（最小二乘法）</strong></p> 
 <p><strong>3、指数损失函数（Adaboost）</strong></p> 
 <p><strong>4、Hinge损失函数（SVM）</strong></p> 
 <p><strong>5、0-1损失函数</strong></p> 
 <p><strong>6、绝对值损失函数</strong></p> 
</blockquote> 
<p></p> 
<h4 id="1%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%2F%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95"><strong>4、损失函数 -- 最小二乘法</strong></h4> 
<p> <img alt="" height="162" src="https://images2.imgbox.com/97/5a/CxsxTBDH_o.png" width="504"></p> 
<p></p> 
<h3 id="2%E3%80%81%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%EF%BC%9A">2、优化方法：</h3> 
<blockquote> 
 <p><span style="color:#fe2c24;"><strong>优化目的：</strong></span><span style="color:#fe2c24;"><strong>让损失函数取得最小值。</strong></span></p> 
 <p><span style="color:#fe2c24;"><strong>优化方法：</strong></span></p> 
 <p><span style="color:#fe2c24;"><strong>1、正规方程；</strong></span></p> 
 <p><span style="color:#fe2c24;"><strong>2、梯度下降。</strong></span></p> 
</blockquote> 
<h4></h4> 
<h3 id="2-1%E3%80%81%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%C2%A0">2-1、正规方程 </h3> 
<h4 id="1%E3%80%81%E5%85%AC%E5%BC%8F">1、公式</h4> 
<p><img alt="" height="145" src="https://images2.imgbox.com/3c/e0/by54BqkE_o.png" width="454"></p> 
<p> <img alt="" height="278" src="https://images2.imgbox.com/76/4a/3TK4r9pu_o.png" width="489"></p> 
<p></p> 
<h4 id="2%E3%80%81%E5%8E%9F%E7%90%86">2、原理</h4> 
<p><img alt="" height="96" src="https://images2.imgbox.com/6b/3d/FFIufZrX_o.png" width="227"></p> 
<p></p> 
<blockquote> 
 <p><strong>优点：不需要试错，可以直接取得最小值，比较快捷。</strong></p> 
 <p><strong>缺点：当特征过于复杂时，求解速度太慢且得不到结果。</strong></p> 
 <p><strong>适用于：小数据场景。（梯度下降相对用的更多一些）</strong></p> 
</blockquote> 
<p></p> 
<p></p> 
<h3 id="2-2%E3%80%81%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D">2-2、梯度下降</h3> 
<blockquote> 
 <p><span style="color:#fe2c24;"><strong>机器学习的过程类似于梯度下降：根据上一步的“错误”，不断学习改进，才有了学习能力。</strong></span></p> 
</blockquote> 
<h4 id="1%E3%80%81%E5%8D%95%E7%89%B9%E5%BE%81">1、单特征</h4> 
<p><img alt="" height="332" src="https://images2.imgbox.com/6b/af/AwzAoGKv_o.png" width="479"></p> 
<p>从上面比较高的位置，一点一点移动到最低点： </p> 
<p> <img alt="" height="321" src="https://images2.imgbox.com/45/f9/ix1swuM9_o.png" width="481"></p> 
<h4 id="2%E3%80%81%E5%8F%8C%E7%89%B9%E5%BE%81">2、双特征</h4> 
<p> <img alt="" height="306" src="https://images2.imgbox.com/75/65/8Moj1ctR_o.png" width="490"></p> 
<p></p> 
<blockquote> 
 <p><strong>求出最小的损失值后，它的权重和偏置就是需要求的模型参数</strong>。</p> 
</blockquote> 
<p> <img alt="" height="136" src="https://images2.imgbox.com/b2/bb/W36FALXg_o.png" width="422"></p> 
<p></p> 
<h4 id="3%E3%80%81%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E8%BF%87%E7%A8%8B%EF%BC%88%E5%8D%95%E7%89%B9%E5%BE%81%EF%BC%89">3、梯度下降过程（单特征）</h4> 
<p> <img alt="" height="258" src="https://images2.imgbox.com/eb/a0/OAHtjPfp_o.png" width="305"><img alt="" height="249" src="https://images2.imgbox.com/ed/e4/0kqDfkEO_o.png" width="305"></p> 
<p> <img alt="" height="259" src="https://images2.imgbox.com/00/4b/6imu1k8U_o.png" width="303"><img alt="" height="247" src="https://images2.imgbox.com/ef/b6/d7XbcXzw_o.png" width="304"></p> 
<p></p> 
<h4 id="%C2%A04%E3%80%81%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E8%BF%87%E7%A8%8B%EF%BC%88%E5%8F%8C%E7%89%B9%E5%BE%81%EF%BC%89"> 4、梯度下降过程（双特征）</h4> 
<p><img alt="" height="260" src="https://images2.imgbox.com/07/a7/slh3ZTq4_o.png" width="274"><img alt="" height="267" src="https://images2.imgbox.com/ce/9f/5zatTad3_o.png" width="261"></p> 
<p> <img alt="" height="258" src="https://images2.imgbox.com/1e/71/IfcLzojE_o.png" width="275"></p> 
<p></p> 
<h3 id="3%E3%80%81%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E4%B8%8E%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%BC%98%E7%BC%BA%E7%82%B9%E5%AF%B9%E6%AF%94">3、正规方程与梯度下降优缺点对比</h3> 
<p><img alt="" height="141" src="https://images2.imgbox.com/da/9e/6b4L6pck_o.png" width="483"></p> 
<blockquote> 
 <p><strong>小规模数据：正规方程、岭回归。</strong></p> 
 <p><strong>大规模数据：梯度下降。</strong></p> 
</blockquote> 
<p></p> 
<p></p> 
<h2 id="%E4%B8%89%E3%80%81%E5%9B%9E%E5%BD%92%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0%EF%BC%88%E5%9D%87%E6%96%B9%E5%B7%AE%EF%BC%89">三、回归性能评估（均方差）</h2> 
<blockquote> 
 <p><strong><span style="color:#fe2c24;">回归性能评估方式：求均方差，均方差较小的那个模型效果较好</span>。</strong></p> 
</blockquote> 
<h3 id="%E5%85%AC%E5%BC%8F">公式</h3> 
<p><img alt="" height="88" src="https://images2.imgbox.com/b6/55/lGGwm6Ue_o.png" width="409"></p> 
<p>（y^i为预测值，y为真实值）</p> 
<h3 id="API">API</h3> 
<pre><code class="language-python">sklearn.metrics.mean_squared_error</code></pre> 
<p><img alt="" height="123" src="https://images2.imgbox.com/14/4f/i8ZDkXr0_o.png" width="337"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/82bed24d7e342558b82c718fe19d2eab/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Small Object Detection using Context and Attention</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/83dbff2306668fee3273bab8fb33e181/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">pip3 install mysqlclient 报错 Command “python setup.py egg_info“ failed with error</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>