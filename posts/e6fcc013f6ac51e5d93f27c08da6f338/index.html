<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ResNet-TensorFlow Model Zoo代码理解 - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="ResNet-TensorFlow Model Zoo代码理解" />
<meta property="og:description" content="ResNet自从15年问世以来迅速影响了CNN的发展，主要得益于ResNet的shortcut结构能够避免网络的退化（即传统的CNN随着网络深度的 增加会出现训练误差和测试误差增大的情况）和梯度消失/爆炸现象，使得ResNet能够从网络层数的加深中受益，这也是为什么ResNet 能够做到34层，50层，甚至152层，甚至是1202层的缘故。原文中提供了几个通用的网络结构：
本文分析GitHub上面的TensorFlow提供的官方ResNet的TensorFlow实现程序，讲解网络在TensorFlow中的代码构成。
首先这是对ResNet在Cifar10和Cifar100数据库的一个复现，也就是说TensorFlow官方提供的这一版ResNet程序是用来进行Cifar的分类任务的。代码地址：https://github.com/tensorflow/models
cifar_input.py（https://github.com/tensorflow/models/blob/master/resnet/cifar_input.py） 用来读取Cifar数据库中的图片数据和标注信息的，这里不做过多讲解，如果想要把ResNet改为回归任务或者是训练自己的数据库，则需要对数据输入进行重写（如果是回归任务，那么网络的结构也需要修改（去掉Softmax层）
resnet_model.py（https://github.com/tensorflow/models/blob/master/resnet/resnet_model.py） 构造了ResNet这个类（Class),定义了ResNet的网络结构、loss等。
首先_build_model()函数定义了网络的核心结构，代码如下：
def _build_model(self): &#34;&#34;&#34;Build the core model within the graph.&#34;&#34;&#34; with tf.variable_scope(&#39;init&#39;): #init层将图片的3通道变为16通道feature map输出 x = self._images x = self._conv(&#39;init_conv&#39;, x, 3, 3, 16, self._stride_arr(1)) #3*3的卷积层，16通道输出 strides = [1, 2, 2] #后面两个2的stride用来降采样 activate_before_residual = [True, False, False] if self.hps.use_bottleneck: res_func = self._bottleneck_residual #bottleneck结构，包含三个卷积子层 filters = [16, 64, 128, 256] else: res_func = self._residual #非bottleneck结构（包含两个3*3的卷积子层） filters = [16, 16, 32, 64] # Uncomment the following codes to use w28-10 wide residual network." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/e6fcc013f6ac51e5d93f27c08da6f338/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2017-04-27T14:04:04+08:00" />
<meta property="article:modified_time" content="2017-04-27T14:04:04+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ResNet-TensorFlow Model Zoo代码理解</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>ResNet自从15年问世以来迅速影响了CNN的发展，主要得益于ResNet的shortcut结构能够避免网络的退化（即传统的CNN随着网络深度的 增加会出现训练误差和测试误差增大的情况）和梯度消失/爆炸现象，使得ResNet能够从网络层数的加深中受益，这也是为什么ResNet 能够做到34层，50层，甚至152层，甚至是1202层的缘故。原文中提供了几个通用的网络结构：</p> 
<p><img src="https://images2.imgbox.com/21/5c/UKbXmxSN_o.png" alt="这里写图片描述" title=""></p> 
<p><em>本文分析GitHub上面的TensorFlow提供的官方ResNet的TensorFlow实现程序，讲解网络在TensorFlow中的代码构成。</em></p> 
<p>首先这是对ResNet在Cifar10和Cifar100数据库的一个复现，也就是说TensorFlow官方提供的这一版ResNet程序是用来进行Cifar的分类任务的。代码地址：<a href="https://github.com/tensorflow/models">https://github.com/tensorflow/models</a></p> 
<p>cifar_input.py（<a href="https://github.com/tensorflow/models/blob/master/resnet/cifar_input.py">https://github.com/tensorflow/models/blob/master/resnet/cifar_input.py</a>） <br> 用来读取Cifar数据库中的图片数据和标注信息的，这里不做过多讲解，如果想要把ResNet改为回归任务或者是训练自己的数据库，则需要对数据输入进行重写（如果是回归任务，那么网络的结构也需要修改（去掉Softmax层）</p> 
<p>resnet_model.py（<a href="https://github.com/tensorflow/models/blob/master/resnet/resnet_model.py">https://github.com/tensorflow/models/blob/master/resnet/resnet_model.py</a>） <br> 构造了ResNet这个类（Class),定义了ResNet的网络结构、loss等。</p> 
<p>首先_build_model()函数定义了网络的核心结构，代码如下：</p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_build_model</span><span class="hljs-params">(self)</span>:</span>
    <span class="hljs-string">"""Build the core model within the graph."""</span>
    <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">'init'</span>):    <span class="hljs-comment">#init层将图片的3通道变为16通道feature map输出</span>
      x = self._images
      x = self._conv(<span class="hljs-string">'init_conv'</span>, x, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">16</span>, self._stride_arr(<span class="hljs-number">1</span>))  <span class="hljs-comment">#3*3的卷积层，16通道输出</span>

    strides = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]   <span class="hljs-comment">#后面两个2的stride用来降采样</span>
    activate_before_residual = [<span class="hljs-keyword">True</span>, <span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>]
    <span class="hljs-keyword">if</span> self.hps.use_bottleneck:
      res_func = self._bottleneck_residual    <span class="hljs-comment">#bottleneck结构，包含三个卷积子层</span>
      filters = [<span class="hljs-number">16</span>, <span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">256</span>]
    <span class="hljs-keyword">else</span>:
      res_func = self._residual      <span class="hljs-comment">#非bottleneck结构（包含两个3*3的卷积子层）</span>
      filters = [<span class="hljs-number">16</span>, <span class="hljs-number">16</span>, <span class="hljs-number">32</span>, <span class="hljs-number">64</span>]
      <span class="hljs-comment"># Uncomment the following codes to use w28-10 wide residual network.</span>
      <span class="hljs-comment"># It is more memory efficient than very deep residual network and has</span>
      <span class="hljs-comment"># comparably good performance.</span>
      <span class="hljs-comment"># https://arxiv.org/pdf/1605.07146v1.pdf</span>
      <span class="hljs-comment"># filters = [16, 160, 320, 640]</span>
      <span class="hljs-comment"># Update hps.num_residual_units to 9</span>

    <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">'unit_1_0'</span>):
      x = res_func(x, filters[<span class="hljs-number">0</span>], filters[<span class="hljs-number">1</span>], self._stride_arr(strides[<span class="hljs-number">0</span>]),
                   activate_before_residual[<span class="hljs-number">0</span>])
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> six.moves.range(<span class="hljs-number">1</span>, self.hps.num_residual_units):   <span class="hljs-comment">#可以看到这里num_residual_units决定了一个unit下面包含几个block</span>
      <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">'unit_1_%d'</span> % i):
        x = res_func(x, filters[<span class="hljs-number">1</span>], filters[<span class="hljs-number">1</span>], self._stride_arr(<span class="hljs-number">1</span>), <span class="hljs-keyword">False</span>)

    <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">'unit_2_0'</span>):
      x = res_func(x, filters[<span class="hljs-number">1</span>], filters[<span class="hljs-number">2</span>], self._stride_arr(strides[<span class="hljs-number">1</span>]),
                   activate_before_residual[<span class="hljs-number">1</span>])
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> six.moves.range(<span class="hljs-number">1</span>, self.hps.num_residual_units):
      <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">'unit_2_%d'</span> % i):
        x = res_func(x, filters[<span class="hljs-number">2</span>], filters[<span class="hljs-number">2</span>], self._stride_arr(<span class="hljs-number">1</span>), <span class="hljs-keyword">False</span>)

    <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">'unit_3_0'</span>):
      x = res_func(x, filters[<span class="hljs-number">2</span>], filters[<span class="hljs-number">3</span>], self._stride_arr(strides[<span class="hljs-number">2</span>]),
                   activate_before_residual[<span class="hljs-number">2</span>])
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> six.moves.range(<span class="hljs-number">1</span>, self.hps.num_residual_units):
      <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">'unit_3_%d'</span> % i):
        x = res_func(x, filters[<span class="hljs-number">3</span>], filters[<span class="hljs-number">3</span>], self._stride_arr(<span class="hljs-number">1</span>), <span class="hljs-keyword">False</span>)

    <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">'unit_last'</span>):      <span class="hljs-comment">#卷积完成之后有一个全局Pooling层</span>
      x = self._batch_norm(<span class="hljs-string">'final_bn'</span>, x)
      x = self._relu(x, self.hps.relu_leakiness)
      x = self._global_avg_pool(x)               

    <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">'logit'</span>):
      logits = self._fully_connected(x, self.hps.num_classes)    <span class="hljs-comment">#全连接层输出</span>
      self.predictions = tf.nn.softmax(logits)           <span class="hljs-comment">#Softmax分类输出结果</span>

    <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">'costs'</span>):
      xent = tf.nn.softmax_cross_entropy_with_logits(    <span class="hljs-comment">#损失函数采用的Softmax交叉熵的形式</span>
          logits=logits, labels=self.labels)
      self.cost = tf.reduce_mean(xent, name=<span class="hljs-string">'xent'</span>)
      self.cost += self._decay()             

      tf.summary.scalar(<span class="hljs-string">'cost'</span>, self.cost)      <span class="hljs-comment">#summary收集信息用于tensorboard的可视化显示</span>
</code></pre> 
<p>此外，ResNet类中还采用BN的算法一个weight decay的算法，也都在resnet_model.py中有定义。</p> 
<p>resnet_main.py（<a href="https://github.com/tensorflow/models/blob/master/resnet/resnet_main.py">https://github.com/tensorflow/models/blob/master/resnet/resnet_main.py</a>） 是训练代码和测试代码。训练代码中的 _LearningRateSetterHook类定义了学习率随迭代次数的变化而变化：</p> 
<pre class="prettyprint"><code class=" hljs python"> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">_LearningRateSetterHook</span><span class="hljs-params">(tf.train.SessionRunHook)</span>:</span>
    <span class="hljs-string">"""Sets learning_rate based on global step."""</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">begin</span><span class="hljs-params">(self)</span>:</span>
      self._lrn_rate = <span class="hljs-number">0.1</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">before_run</span><span class="hljs-params">(self, run_context)</span>:</span>
      <span class="hljs-keyword">return</span> tf.train.SessionRunArgs(
          model.global_step,  <span class="hljs-comment"># Asks for global step value.</span>
          feed_dict={model.lrn_rate: self._lrn_rate})  <span class="hljs-comment"># Sets learning rate</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">after_run</span><span class="hljs-params">(self, run_context, run_values)</span>:</span>
      train_step = run_values.results
      <span class="hljs-keyword">if</span> train_step &lt; <span class="hljs-number">40000</span>:
        self._lrn_rate = <span class="hljs-number">0.1</span>
      <span class="hljs-keyword">elif</span> train_step &lt; <span class="hljs-number">60000</span>:
        self._lrn_rate = <span class="hljs-number">0.01</span>
      <span class="hljs-keyword">elif</span> train_step &lt; <span class="hljs-number">80000</span>:
        self._lrn_rate = <span class="hljs-number">0.001</span>
      <span class="hljs-keyword">else</span>:
        self._lrn_rate = <span class="hljs-number">0.0001</span></code></pre> 
<p>主函数中定义了超参数的各项设置：</p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span><span class="hljs-params">(_)</span>:</span>
  <span class="hljs-keyword">if</span> FLAGS.num_gpus == <span class="hljs-number">0</span>:
    dev = <span class="hljs-string">'/cpu:0'</span>
  <span class="hljs-keyword">elif</span> FLAGS.num_gpus == <span class="hljs-number">1</span>:
    dev = <span class="hljs-string">'/gpu:0'</span>
  <span class="hljs-keyword">else</span>:
    <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">'Only support 0 or 1 gpu.'</span>)

  <span class="hljs-keyword">if</span> FLAGS.mode == <span class="hljs-string">'train'</span>:
    batch_size = <span class="hljs-number">128</span>
  <span class="hljs-keyword">elif</span> FLAGS.mode == <span class="hljs-string">'eval'</span>:
    batch_size = <span class="hljs-number">100</span>

  <span class="hljs-keyword">if</span> FLAGS.dataset == <span class="hljs-string">'cifar10'</span>:
    num_classes = <span class="hljs-number">10</span>
  <span class="hljs-keyword">elif</span> FLAGS.dataset == <span class="hljs-string">'cifar100'</span>:
    num_classes = <span class="hljs-number">100</span>

  hps = resnet_model.HParams(batch_size=batch_size,
                             num_classes=num_classes,
                             min_lrn_rate=<span class="hljs-number">0.0001</span>,
                             lrn_rate=<span class="hljs-number">0.1</span>,
                             num_residual_units=<span class="hljs-number">5</span>,
                             use_bottleneck=<span class="hljs-keyword">False</span>,
                             weight_decay_rate=<span class="hljs-number">0.0002</span>,
                             relu_leakiness=<span class="hljs-number">0.1</span>,
                             optimizer=<span class="hljs-string">'mom'</span>)

  <span class="hljs-keyword">with</span> tf.device(dev):
    <span class="hljs-keyword">if</span> FLAGS.mode == <span class="hljs-string">'train'</span>:
      train(hps)
    <span class="hljs-keyword">elif</span> FLAGS.mode == <span class="hljs-string">'eval'</span>:
      evaluate(hps)
</code></pre> 
<p>前面提到过，num_residual_units用来定义一个unit下面的block数量，这里设为5；use_bottleneck=False表示不使用bottleneck结构，即一个block下面只包含两个卷积子层；好了，这里规定num_residual_units和use_bottleneck之后，实际上网络结构就确定了；这个网络长什么样子呢，这里给出一个上面的超参数设置所对用的可视化的网络结构：</p> 
<p><img src="https://images2.imgbox.com/83/09/YQ6YJx0Q_o.jpg" alt="这里写图片描述" title=""></p> 
<p>图中不同填充颜色的方框代表不同的unit，这是一个包含32个参数层（卷积层和全连接层）的网络结构，每一个Block包含conv1和con2两个子层，属于一个浅层的ResNet。</p> 
<p>如果想要设计自己的ResNet用于不同的任务，需要修改训练数据的输入部分以及部分网路结构，最简单的，num_residual_units这个参数就可以控制网络总的深度： <br> 网络层数 = 2*3*num_residual_units+2；（当use_bottleneck = False时）</p> 
<p>此外，use_bottleneck = True时采用的bottleneck结构每一个block包含三个卷积子层，不同于上面的网络结构，按照原文的说法，这种bottleneck结构应该更容易优化和利于网络的加深，原文中给出的50层、101层、152层网络事实上都是采用的这种结构，非bottleneck结构一般来说只在浅层ResNet中使用。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/23294d5ba17bae75016f4a04ed5bb1fe/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">理解密码学中的双线性映射</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/550a0cc2558b95178662b014c5f0ab3b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">eclipse中svn同步失败，cleanup无效</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>