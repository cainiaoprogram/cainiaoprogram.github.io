<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>BP神经网络及python实现（详细） - 菜鸟程序员博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="BP神经网络及python实现（详细）" />
<meta property="og:description" content="目录
一、误差逆传播算法
二、BP算法实现流程图
三、BP神经网络推导
3.1 前向传播
3.2 反向传播
四、Python实现BP神经网络
4.1 激活函数sigmod
4.2 构造三层BP神经网络
4.2.1 BP神经网络初始化
4.2.2 前向传播
4.2.3 反向传播
4.2.4 迭代训练
4.3 算法检验——预测数据
4.4 图解总结
五、运行结果
六、整体总结
七、总代码
写在前面：
本篇文章关于BP神经网络的算法实现相对来说通俗易懂，讲解下至关于函数变量的解释，上至关于BP算法中涉及的前向传播及反向传播的原理、再至其过程中涉及的公式详细推导都有笔者的一些思考笔记。本文也对标准的BP神经网络进行几处改进，目的都是为了提高训练速度。另外，若文章存在一些理论错误请各位不吝赐教，当然，在阅读过程中若有些地方不明白可以在评论区留言！
一、误差逆传播算法 误差逆传播算法的大概流程为：
1、初始化网络中所有连接权和阈值；
2、进入迭代训练：
(1）根据当前参数计算当前样本的输出；
(2）根据下面公式计算输出层神经元的梯度项gj：
（3）根据下面公式计算隐层神经元的梯度项eh：
（4）根据下面公式更新连接权whj、vih与阈值θj、γh：
3、训练完成后输出连接权和阈值确定的多层前馈神经网络；
二、BP算法实现流程图 标准的BP流程图属于单样本训练，假设总共有样本个数为P，其中第i个样本为p，训练次数用q表示，训练完成来源是设置的误差阈值和实际误差对比，达到要求时结束。
左面是标准BP的算法流程，权值调整方法是是基于单样本训练的，但是单样本训练遵循的是只顾眼前的这个数据产生的误差进行调节权值调整，这样的后果是当训练数据很多时，计算量就会急剧增加，导致收敛速度过慢。为了改变这些缺点，我们采用另外一种方法就是在所有样本输入以后，计算网络的总误差，然后根据总误差计算各层的误差信号并调整权值，这种累积的误差的批处理方式称为批（batch）训练或者（epoch）训练。由于批训练遵循了以减小全局误差为目标的“集体主义”原则也就是所有的训练样本。在保证总误差向减小方向变化时，即使训练样本很多，训练时的收敛速度也是很快的。
三、BP神经网络推导 我们只讨论三层的BP神经网络，又称三层感知机。所谓三层，即包括输入层、隐层、输出层。三层感知机的神经元连接形式：
3.1 前向传播 图解：
此处要对三层BP网络中不同符号所代表的意思理解，下面进行符号说明：
符号说明：
输入层、隐层、输出层神经元的个数分别为d、q、l；
xi ：最初第i个神经元的输入；
vih ：输入层第i个神经元和隐层第h个神经元的权值；
αh：隐层第h个神经元的输入；
γh：隐层第h个神经元的阈值；
bh ：隐层第h个神经元的输出；
whj ：隐层第h个神经元与输出层第j个神经元的权值；
βj：输出层第j个神经元的输入；
θj：输出层第j个神经元的阈值；
yj ：输出层第j个神经元的输出； 激活函数f(x)一般有对数几率函数sigmoid、双曲正切函数tanh等等：
3.2 反向传播 在前向传播计算完成后，需要通过误差逆传播算法对误差进行反向传播修改权值和阈值，相关公式推导如下：
下面通过python代码实现上述BP网络的流程。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cainiaoprogram.github.io/posts/aa32c7374bf51959c2c5bdd12bed53a1/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-14T20:34:10+08:00" />
<meta property="article:modified_time" content="2023-10-14T20:34:10+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="菜鸟程序员博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">菜鸟程序员博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">BP神经网络及python实现（详细）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E4%B8%80%E3%80%81%E8%AF%AF%E5%B7%AE%E9%80%86%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81%E8%AF%AF%E5%B7%AE%E9%80%86%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95" rel="nofollow">一、误差逆传播算法</a></p> 
<p id="%E4%BA%8C%E3%80%81BP%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E6%B5%81%E7%A8%8B%E5%9B%BE-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81BP%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E6%B5%81%E7%A8%8B%E5%9B%BE" rel="nofollow">二、BP算法实现流程图</a></p> 
<p id="%E4%B8%89%E3%80%81BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%8E%A8%E5%AF%BC-toc" style="margin-left:0px;"><a href="#%E4%B8%89%E3%80%81BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%8E%A8%E5%AF%BC" rel="nofollow">三、BP神经网络推导</a></p> 
<p id="3.1%20%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD-toc" style="margin-left:40px;"><a href="#3.1%20%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD" rel="nofollow">3.1 前向传播</a></p> 
<p id="3.2%20%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-toc" style="margin-left:40px;"><a href="#3.2%20%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD" rel="nofollow">3.2 反向传播</a></p> 
<p id="%E5%9B%9B%E3%80%81Python%E5%AE%9E%E7%8E%B0BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-toc" style="margin-left:0px;"><a href="#%E5%9B%9B%E3%80%81Python%E5%AE%9E%E7%8E%B0BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" rel="nofollow">四、Python实现BP神经网络</a></p> 
<p id="4.1%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0sigmod-toc" style="margin-left:40px;"><a href="#4.1%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0sigmod" rel="nofollow">4.1 激活函数sigmod</a></p> 
<p id="4.2%20%E6%9E%84%E9%80%A0%E4%B8%89%E5%B1%82BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-toc" style="margin-left:40px;"><a href="#4.2%20%E6%9E%84%E9%80%A0%E4%B8%89%E5%B1%82BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" rel="nofollow">4.2 构造三层BP神经网络</a></p> 
<p id="4.2.1%20BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%9D%E5%A7%8B%E5%8C%96-toc" style="margin-left:80px;"><a href="#4.2.1%20BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%9D%E5%A7%8B%E5%8C%96" rel="nofollow">4.2.1 BP神经网络初始化</a></p> 
<p id="4.2.2%20%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD-toc" style="margin-left:80px;"><a href="#4.2.2%20%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD" rel="nofollow">4.2.2 前向传播</a></p> 
<p id="4.2.3%20%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-toc" style="margin-left:80px;"><a href="#4.2.3%20%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD" rel="nofollow">4.2.3 反向传播</a></p> 
<p id="4.2.4%20%E8%BF%AD%E4%BB%A3%E8%AE%AD%E7%BB%83-toc" style="margin-left:80px;"><a href="#4.2.4%20%E8%BF%AD%E4%BB%A3%E8%AE%AD%E7%BB%83" rel="nofollow">4.2.4 迭代训练</a></p> 
<p id="4.3%20%E7%AE%97%E6%B3%95%E6%A3%80%E9%AA%8C%E2%80%94%E2%80%94%E9%A2%84%E6%B5%8B%E6%95%B0%E6%8D%AE-toc" style="margin-left:40px;"><a href="#4.3%20%E7%AE%97%E6%B3%95%E6%A3%80%E9%AA%8C%E2%80%94%E2%80%94%E9%A2%84%E6%B5%8B%E6%95%B0%E6%8D%AE" rel="nofollow">4.3 算法检验——预测数据</a></p> 
<p id="4.4%20%E5%9B%BE%E8%A7%A3%E6%80%BB%E7%BB%93-toc" style="margin-left:40px;"><a href="#4.4%20%E5%9B%BE%E8%A7%A3%E6%80%BB%E7%BB%93" rel="nofollow">4.4 图解总结</a></p> 
<p id="%E4%BA%94%E3%80%81%E8%BF%90%E8%A1%8C%E7%BB%93%E6%9E%9C-toc" style="margin-left:0px;"><a href="#%E4%BA%94%E3%80%81%E8%BF%90%E8%A1%8C%E7%BB%93%E6%9E%9C" rel="nofollow">五、运行结果</a></p> 
<p id="%E5%85%AD%E3%80%81%E6%95%B4%E4%BD%93%E6%80%BB%E7%BB%93-toc" style="margin-left:0px;"><a href="#%E5%85%AD%E3%80%81%E6%95%B4%E4%BD%93%E6%80%BB%E7%BB%93" rel="nofollow">六、整体总结</a></p> 
<p id="%E4%B8%83%E3%80%81%E6%80%BB%E4%BB%A3%E7%A0%81-toc" style="margin-left:0px;"><a href="#%E4%B8%83%E3%80%81%E6%80%BB%E4%BB%A3%E7%A0%81" rel="nofollow">七、总代码</a></p> 
<hr id="hr-toc"> 
<p></p> 
<p><strong>写在前面：</strong></p> 
<p>        本篇文章关于BP神经网络的算法实现相对来说<span style="background-color:#a2e043;">通俗易懂</span>，讲解下至关于<span style="background-color:#ed7976;">函数变量的解释</span>，上至关于BP算法中涉及的<span style="background-color:#ed7976;">前向传播及反向传播的原理</span>、再至其过程中涉及的<span style="background-color:#ed7976;">公式详细推导</span>都有笔者的一些思考笔记。本文也对标准的BP神经网络进行几处改进，目的都是为了提高训练速度。另外，若文章存在一些理论错误请各位不吝赐教，当然，在阅读过程中若有些地方不明白可以在评论区留言！</p> 
<p></p> 
<h2 id="%E4%B8%80%E3%80%81%E8%AF%AF%E5%B7%AE%E9%80%86%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95">一、误差逆传播算法</h2> 
<p><img alt="" height="481" src="https://images2.imgbox.com/aa/60/qwOSkviJ_o.png" width="864"></p> 
<p>误差逆传播算法的<strong>大概流程</strong>为：</p> 
<p>1、初始化网络中所有<span style="color:#be191c;">连接权</span>和<span style="color:#be191c;">阈值</span>；</p> 
<p>2、进入<strong>迭代训练</strong>：</p> 
<p>        (1）根据当前参数计算当前样本的输出；</p> 
<p>        (2）根据下面公式计算<strong>输出层神经元的梯度项</strong>gj：</p> 
<p class="img-center"><img alt="" height="157" src="https://images2.imgbox.com/10/7c/7B3o9tdT_o.png" width="256"></p> 
<p>        （3）根据下面公式计算<strong>隐层神经元的梯度项</strong>eh：</p> 
<p class="img-center"><img alt="" height="131" src="https://images2.imgbox.com/81/c4/LwKGlWbg_o.png" width="233"></p> 
<p>        （4）根据下面公式<strong>更新</strong><span style="color:#be191c;">连接权</span>whj、vih与<span style="color:#be191c;">阈值</span>θj、γh：</p> 
<p class="img-center"><img alt="" height="191" src="https://images2.imgbox.com/b0/32/UOJ4mLzP_o.png" width="167"></p> 
<p>3、训练完成后<strong>输出</strong><span style="color:#be191c;">连接权</span>和<span style="color:#be191c;">阈值</span>确定的<strong>多层前馈神经网络</strong>；</p> 
<p>     </p> 
<h2 id="%E4%BA%8C%E3%80%81BP%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E6%B5%81%E7%A8%8B%E5%9B%BE">二、BP算法实现流程图</h2> 
<p class="img-center"><img alt="" height="604" src="https://images2.imgbox.com/9b/22/C5b4yWLi_o.png" width="600"></p> 
<p>        <strong>标准的BP</strong>流程图属于<strong>单样本训练</strong>，假设总共有样本个数为P，其中第i个样本为p，训练次数用q表示，训练完成来源是设置的误差阈值和实际误差对比，达到要求时结束。</p> 
<p>        左面是<strong>标准BP的算法流程</strong>，权值调整方法是是<strong>基于单样本训练</strong>的，但是单样本训练遵循的是只顾眼前的这个数据产生的误差进行调节权值调整，这样的后果是<span style="background-color:#ed7976;">当训练数据很多时，计算量就会急剧增加，导致收敛速度过慢</span>。为了改变这些缺点，我们采用另外一种方法就是在所有样本输入以后，<span style="background-color:#a2e043;">计算网络的总误差</span>，然后根据总误差计算各层的误差信号并调整权值，这种累积的误差的批处理方式称为<strong>批（batch）训练</strong>或者（epoch）训练。由于批训练遵循了以减小全局误差为目标的“集体主义”原则也就是所有的训练样本。<span style="background-color:#a2e043;">在保证总误差向减小方向变化时，即使训练样本很多，训练时的收敛速度也是很快的</span>。<br>  </p> 
<h2 id="%E4%B8%89%E3%80%81BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%8E%A8%E5%AF%BC">三、BP神经网络推导</h2> 
<p>        我们只讨论<strong>三层的BP神经网络</strong>，又称<strong>三层感知机</strong>。所谓三层，即包括<strong>输入层、隐层、输出层</strong>。三层感知机的神经元连接形式：</p> 
<p class="img-center"><img alt="" height="221" src="https://images2.imgbox.com/d3/b2/YzX4GxoQ_o.png" width="390"></p> 
<h3 id="3.1%20%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD">3.1 前向传播</h3> 
<p>图解：</p> 
<p class="img-center"><img alt="" height="231" src="https://images2.imgbox.com/ad/41/J1PS31cd_o.png" width="280"></p> 
<p>此处要对三层BP网络中不同符号所代表的意思理解，下面进行符号说明：</p> 
<blockquote> 
 <p><strong>符号说明：</strong></p> 
 <p style="margin-left:0;text-align:justify;">输入层、隐层、输出层<strong>神经元的个数</strong>分别为<strong>d、q、l</strong>；</p> 
 <p style="margin-left:0;text-align:justify;">xi ：最初第i个神经元的<strong>输入</strong>；</p> 
 <p style="margin-left:0;text-align:justify;">vih ：<strong>输入层</strong>第i个神经元和隐层第h个神经元的<strong>权值</strong>；</p> 
 <p style="margin-left:0;text-align:justify;">αh：<strong>隐层</strong>第h个神经元的<strong>输入</strong>；</p> 
 <p style="margin-left:0;text-align:justify;">γh：<strong>隐层</strong>第h个神经元的<strong>阈值</strong>；</p> 
 <p style="margin-left:0;text-align:justify;">bh ：<strong>隐层</strong>第h个神经元的<strong>输出</strong>；</p> 
 <p style="margin-left:0;text-align:justify;">whj ：<strong>隐层</strong>第h个神经元与<strong>输出层</strong>第j个神经元的<strong>权值</strong>；</p> 
 <p style="margin-left:0;text-align:justify;">βj：<strong>输出层</strong>第j个神经元的<strong>输入</strong>；</p> 
 <p style="margin-left:0;text-align:justify;">θj：<strong>输出</strong>层第j个神经元的<strong>阈值</strong>；</p> 
 <p>yj ：<strong>输出层</strong>第j个神经元的<strong>输出</strong>； </p> 
</blockquote> 
<p><strong>激活函数f(x)</strong>一般有<span style="color:#be191c;">对数几率函数sigmoid</span>、<span style="color:#be191c;">双曲正切函数tanh</span>等等：</p> 
<p class="img-center"><img alt="" height="263" src="https://images2.imgbox.com/1e/f8/DPvw5FmN_o.png" width="200"></p> 
<h3 id="3.2%20%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD">3.2 反向传播</h3> 
<p>在前向传播计算完成后，需要通过<strong>误差逆传播算法</strong>对误差进行反向传播<strong>修改<span style="color:#be191c;">权值</span>和<span style="color:#be191c;">阈值</span></strong>，相关公式推导如下：</p> 
<p class="img-center"><img alt="" height="459" src="https://images2.imgbox.com/8f/68/nthcLXam_o.png" width="800"></p> 
<p>下面通过python代码实现上述BP网络的流程。</p> 
<p> </p> 
<h2 id="%E5%9B%9B%E3%80%81Python%E5%AE%9E%E7%8E%B0BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">四、Python实现BP神经网络</h2> 
<h3 id="4.1%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0sigmod">4.1 激活函数sigmod</h3> 
<p>激活函数采用双曲正切函数tanh：</p> 
<pre><code class="language-python">#? 激活函数sigmoid(x)、及其导数DS(x)
import numpy as np

# 双曲正切函数tanh
def sigmoid(x):
    return np.tanh(x)
def DS(x):
    return 1 - (np.tanh(x)) ** 2

# 第90次迭代 误差0.00005</code></pre> 
<h3 id="4.2%20%E6%9E%84%E9%80%A0%E4%B8%89%E5%B1%82BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">4.2 构造三层BP神经网络</h3> 
<h4 id="4.2.1%20BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%9D%E5%A7%8B%E5%8C%96">4.2.1 BP神经网络初始化</h4> 
<pre><code class="language-python">#? 构造3层BP神经网络架构
class BP:
    #? 初始化函数：各层结点数、激活结点、权重矩阵、偏差、动量因子
    def __init__(self,num_in,num_hidden,num_out):
        # 输入层、隐藏层、输出层 的结点数
        self.num_in=num_in+1            # 输入层结点数 并增加一个偏置结点(阈值)
        self.num_hidden=num_hidden+1    # 隐藏层结点数 并增加一个偏置结点(阈值)
        self.num_out=num_out            # 输出层结点数
        # 激活BP神经网络的所有结点（向量）
        self.active_in=np.array([-1.0]*self.num_in)
        self.active_hidden=np.array([-1.0]*self.num_hidden)
        self.active_out=np.array([1.0]*self.num_out)
        # 创建权重矩阵
        self.weight_in=makematrix(self.num_in,self.num_hidden)      # in*hidden 的0矩阵
        self.weight_out=makematrix(self.num_hidden,self.num_out)    # hidden*out的0矩阵
        # 对权重矩阵weight赋初值
        for i in range(self.num_in):        # 对weight_in矩阵赋初值
            for j in range(self.num_hidden):
                self.weight_in[i][j]=random_number(0.1,0.1)
        for i in range(self.num_hidden):    # 对weight_out矩阵赋初值
            for j in range(self.num_out):
                self.weight_out[i][j]=random_number(0.1,0.1)
        # 偏差
        for j in range(self.num_hidden):
            self.weight_in[0][j]=0.1
        for j in range(self.num_out):
            self.weight_out[0][j]=0.1
        
        # 建立动量因子（矩阵）
        self.ci=makematrix(self.num_in,self.num_hidden)     # num_in*num_hidden 矩阵
        self.co=makematrix(self.num_hidden,self.num_out)    # num_hidden*num_out矩阵</code></pre> 
<p><strong>图解：</strong>初始化后各个变量的存在形式：</p> 
<p><img alt="" height="325" src="https://images2.imgbox.com/22/95/HGfC7wxg_o.png" width="700"></p> 
<h4 id="4.2.2%20%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD">4.2.2 前向传播</h4> 
<pre><code class="language-python">#? 构造3层BP神经网络架构
class BP:
    #? 信号正向传播
    def update(self,inputs):
        if len(inputs)!=(self.num_in-1):
            raise ValueError("与输入层结点数不符")
        # 数据输入 输入层
        self.active_in[1:self.num_in]=inputs
        # 数据在隐藏层处理
        self.sum_hidden=np.dot(self.weight_in.T,self.active_in.reshape(-1,1))   # 叉乘
            # .T操作是对于array操作后的数组进行转置操作
            # .reshape(x,y)操作是对于array操作后的数组进行重新排列成一个x*y的矩阵，参数为负数表示无限制，如(-1,1)转换成一列的矩阵
        self.active_hidden=sigmoid(self.sum_hidden) # active_hidden[]是处理完输入数据之后处理，作为输出层的输入数据
        self.active_hidden[0]=-1
        # 数据在输出层处理
        self.sum_out=np.dot(self.weight_out.T,self.active_hidden)
        self.active_out=sigmoid(self.sum_out)
        # 返回输出层结果
        return self.active_out</code></pre> 
<p><strong>图解：</strong>正向传播的过程：计算出active_in、active_hidden、active_out：</p> 
<p class="img-center"><img alt="" height="486" src="https://images2.imgbox.com/66/7a/uCu0VXYJ_o.png" width="700"></p> 
<h4 id="4.2.3%20%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD">4.2.3 反向传播</h4> 
<pre><code class="language-python">#? 构造3层BP神经网络架构
class BP:
    #? 误差反向传播
    def errorbackpropagate(self,targets,lr,m):  # lr 学习效率
        if self.num_out==1:
            targets=[targets]
        if len(targets)!=self.num_out:
            raise ValueError("与输出层结点数不符")
        # 误差
        error=(1/2)*np.dot((targets.reshape(-1,1)-self.active_out).T,
                           (targets.reshape(-1,1)-self.active_out))
        
        # 输出层 误差信号
        self.error_out=(targets.reshape(-1,1)-self.active_out)*DS(self.sum_out) # DS(self.active_out)
        # 隐层 误差信号
        self.error_hidden=np.dot(self.weight_out,self.error_out)*DS(self.sum_hidden)    # DS(self.active_hidden)

        # 更新权值
        # 隐层
        self.weight_out=self.weight_out+lr*np.dot(self.error_out,self.active_hidden.reshape(1,-1)).T+m*self.co
        self.co=lr*np.dot(self.error_out,self.active_hidden.reshape(1,-1)).T
        # 输入层
        self.weight_in=self.weight_in+lr*np.dot(self.error_hidden,self.active_in.reshape(1,-1)).T+m*self.ci
        self.ci=lr*np.dot(self.error_hidden,self.active_in.reshape(1,-1)).T

        return error</code></pre> 
<p><strong>图解：</strong>反向传播计算出error_hidden（即eh）、error_out（即gj）：</p> 
<p class="img-center"><img alt="" height="520" src="https://images2.imgbox.com/e9/f3/xYTdJgTq_o.png" width="500"></p> 
<p>反向传播更新权值，计算出weight_in、weight_out：</p> 
<p class="img-center"><img alt="" height="609" src="https://images2.imgbox.com/01/c4/iyVl3XLk_o.png" width="500"></p> 
<blockquote> 
 <p>更新权值时对标准的BP算法有<strong>改进——增加了动量项ci、co</strong>：</p> 
 <p>        co(t)=co(t) ==&gt; co(t)=co(t)+m*co(t-1)</p> 
 <p>        ci(t)=ci(t) ==&gt; ci(t)=ci(t)+m*ci(t-1)</p> 
 <p>m为动量系数，0&lt;m&lt;1；</p> 
</blockquote> 
<p>        <strong>标准BP算法</strong>在调整权值时，只按t时刻误差的梯度降方向调整，而没有考虑t时刻以前的梯度方向，从而常使训练过程发生振荡，<span style="background-color:#ed7976;">收敛缓慢</span>；</p> 
<p>        <strong>增加动量项</strong>，可以从前一次权值调整量中取出一部分迭加到本次权值调整量中；动量项反映了以前积累的调整经验，对于t时刻的调整起阻尼作用，<span style="background-color:#a2e043;">当误差曲面出现骤然起伏时，可减小振荡趋势，提高训练速度</span>；</p> 
<h4 id="4.2.4%20%E8%BF%AD%E4%BB%A3%E8%AE%AD%E7%BB%83">4.2.4 迭代训练</h4> 
<pre><code class="language-python">#? 构造3层BP神经网络架构
class BP:
    def train(self,pattern,itera=100,lr=0.2,m=0.1):
        for i in range(itera):
            error=0.0   # 每一次迭代将error置0
            for j in pattern:   # j为传入数组的第一维数据   #! *2？(1次迭代里面重复两次？)
                inputs=j[0:self.num_in-1]   # 根据输入层结点的个数确定传入结点值的个数
                targets=j[self.num_in-1:]   # 剩下的结点值作为输出层的值
                self.update(inputs) # 正向传播 更新了active_out
                error=error+self.errorbackpropagate(targets,lr,m)   # 误差反向传播 计算总误差
            if i%10==0:
                print("########################误差 %-.5f ######################第%d次迭代" %(error, i))</code></pre> 
<p>默认迭代训练100次。</p> 
<h3 id="4.3%20%E7%AE%97%E6%B3%95%E6%A3%80%E9%AA%8C%E2%80%94%E2%80%94%E9%A2%84%E6%B5%8B%E6%95%B0%E6%8D%AE">4.3 算法检验——预测数据</h3> 
<pre><code class="language-python">#? 算法检验——预测数据D
# X 输入数据；D 目标数据
X = list(np.arange(-1, 1.1, 0.1))   # -1~1.1 步长0.1增加
D = [-0.96, -0.577, -0.0729, 0.017, -0.641, -0.66, -0.11, 0.1336, -0.201, -0.434, -0.5, 
     -0.393, -0.1647, 0.0988, 0.3072,0.396, 0.3449, 0.1816, -0.0312, -0.2183, -0.3201]
A = X + D   # 数据合并 方便处理
patt = np.array([A] * 2)    # 2*42矩阵  #! 为什么*2？
# 创建神经网络，21个输入节点，13个隐藏层节点，21个输出层节点
bp = BP(21, 13, 21)
# 训练神经网络
bp.train(patt)
# 测试神经网络
d = bp.test(patt)
# 查阅权重值
bp.weights()

import matplotlib.pyplot as plt
plt.plot(X, D, label="source data")  # D为真实值
plt.plot(X, d, label="predict data")  # d为预测值
plt.legend()
plt.show()</code></pre> 
<p style="margin-left:0;text-align:justify;">输入数据为X数据集、预测目标数据为D数据集，经过BP网络后输出的预测数据为d数据集。</p> 
<p style="margin-left:0;text-align:justify;">最后结果输出形式为终端数据呈现、与可视化图像呈现。</p> 
<h3 id="4.4%20%E5%9B%BE%E8%A7%A3%E6%80%BB%E7%BB%93">4.4 图解总结</h3> 
<p>综上图解，各个变量的存在形式如下：</p> 
<p>1、权值矩阵：</p> 
<p class="img-center"><img alt="" height="438" src="https://images2.imgbox.com/e3/67/4aN7YvnQ_o.png" width="500"></p> 
<p>2、激活矩阵：</p> 
<p class="img-center"><img alt="" height="238" src="https://images2.imgbox.com/c1/fc/Qmb6yemi_o.png" width="500"></p> 
<p>3、动量矩阵：</p> 
<p class="img-center"><img alt="" height="225" src="https://images2.imgbox.com/09/5f/z1mhV1xo_o.png" width="300"></p> 
<p></p> 
<h2 id="%E4%BA%94%E3%80%81%E8%BF%90%E8%A1%8C%E7%BB%93%E6%9E%9C">五、运行结果</h2> 
<p><strong>1、运行后在终端输出的一部分结果：</strong></p> 
<p class="img-center"><img alt="" height="330" src="https://images2.imgbox.com/19/44/23EZLo4i_o.png" width="400"></p> 
<p>        “-&gt;”左侧为输入的数据xi，“-&gt;”右侧为这些数据经过三层感知机后的输出结果yj；</p> 
<p class="img-center"><img alt="" height="39" src="https://images2.imgbox.com/ac/a0/JW3INaIk_o.png" width="620"></p> 
<p>        集合D为标准值，可以看到“-&gt;”右侧的预测结果和真实值相差很小，说明预测效果较好；</p> 
<p><strong>2、下面看一下一些迭代后的误差数据：</strong></p> 
<p class="img-center"><img alt="" height="128" src="https://images2.imgbox.com/08/ea/W7eWX7d9_o.png" width="400"></p> 
<p>        我们一共迭代训练了100次，可以看到当训练到90次后，误差已经减小到0.00005，误差极小，也说明预测效果较好；比较这些误差数据发现误差减小的速度很快，说明我们使用改进后的BP算法比较不错；</p> 
<p><strong>3、可视化预测值和真实值的差距：</strong></p> 
<p class="img-center"><img alt="" height="215" src="https://images2.imgbox.com/81/6f/URbv6BsO_o.png" width="300"></p> 
<p>        图中其实有两根线，分别为黄线和蓝线，因为预测效果较好导致蓝线不太明显；<strong>黄线</strong>代表经过三层感知机预测出的数据，<strong>蓝线</strong>代表真实值，两条曲线拟合度很高<span style="background-color:#a2e043;">说明三层感知机训练效果较好</span>；</p> 
<h2 id="%E5%85%AD%E3%80%81%E6%95%B4%E4%BD%93%E6%80%BB%E7%BB%93">六、整体总结</h2> 
<p>1、上述为三层的BP神经网络，即<span style="background-color:#a2e043;">三层感知机</span>；</p> 
<p>2、此BP网络对标准BP网络进行改进：</p> 
<p>（1）标准BP网络属于单样本训练，训练速度满；改进后可以进行<span style="background-color:#a2e043;">批训练</span>，将所有样本输入后<span style="background-color:#a2e043;">计算网络的总误差</span>，并根据总误差调整权值，这样训练时的收敛速度很快；</p> 
<p>（2）标准BP网络在调整权值时只按t时刻误差的梯度降方向调整，而没有考虑t时刻之前的梯度方向，在训练过程中可能会出现振荡，收敛速度慢；改进后<span style="background-color:#a2e043;">添加了动量因子</span>考虑了t时刻之前的梯度变化，<span style="background-color:#a2e043;">可以提高训练速度</span>；<br>  </p> 
<h2 id="%E4%B8%83%E3%80%81%E6%80%BB%E4%BB%A3%E7%A0%81">七、总代码</h2> 
<pre><code class="language-python">#! BP神经网络(误差逆传播算法)
#! 三层BP神经网络/三层感知机

#? 激活函数sigmoid(x)、及其导数DS(x)
import numpy as np

# 双曲正切函数tanh
def sigmoid(x):
    return np.tanh(x)
def DS(x):
    return 1 - (np.tanh(x)) ** 2
# 第90次迭代 误差0.00005

#? 生成区间[a,b]内的随机数
import random
def random_number(a,b):
    return (b-a)*random.random()+a  # random.random()随机生成[0,1)内浮点数

#? 生成一个m*n矩阵，并且设置默认零矩阵
def makematrix(m,n,fill=0.0):
    a = []
    for i in range(m):
        a.append([fill]*n)    # 列表1*n会得到一个新列表，新列表元素为列表1元素重复n次。[fill]*3==[fill fill fill]
    return np.array(a)

#? 构造3层BP神经网络架构
class BP:
    #? 初始化函数：各层结点数、激活结点、权重矩阵、偏差、动量因子
    def __init__(self,num_in,num_hidden,num_out):
        # 输入层、隐藏层、输出层 的结点数
        self.num_in=num_in+1            # 输入层结点数 并增加一个偏置结点(阈值)
        self.num_hidden=num_hidden+1    # 隐藏层结点数 并增加一个偏置结点(阈值)
        self.num_out=num_out            # 输出层结点数
        # 激活BP神经网络的所有结点（向量）
        self.active_in=np.array([-1.0]*self.num_in)
        self.active_hidden=np.array([-1.0]*self.num_hidden)
        self.active_out=np.array([1.0]*self.num_out)
        # 创建权重矩阵
        self.weight_in=makematrix(self.num_in,self.num_hidden)      # in*hidden 的0矩阵
        self.weight_out=makematrix(self.num_hidden,self.num_out)    # hidden*out的0矩阵
        # 对权重矩阵weight赋初值
        for i in range(self.num_in):        # 对weight_in矩阵赋初值
            for j in range(self.num_hidden):
                self.weight_in[i][j]=random_number(0.1,0.1)
        for i in range(self.num_hidden):    # 对weight_out矩阵赋初值
            for j in range(self.num_out):
                self.weight_out[i][j]=random_number(0.1,0.1)
        # 偏差
        for j in range(self.num_hidden):
            self.weight_in[0][j]=0.1
        for j in range(self.num_out):
            self.weight_out[0][j]=0.1
        
        # 建立动量因子（矩阵）
        self.ci=makematrix(self.num_in,self.num_hidden)     # num_in*num_hidden 矩阵
        self.co=makematrix(self.num_hidden,self.num_out)    # num_hidden*num_out矩阵

    #? 信号正向传播
    def update(self,inputs):
        if len(inputs)!=(self.num_in-1):
            raise ValueError("与输入层结点数不符")
        # 数据输入 输入层
        self.active_in[1:self.num_in]=inputs
        # 数据在隐藏层处理
        self.sum_hidden=np.dot(self.weight_in.T,self.active_in.reshape(-1,1))   # 叉乘
            # .T操作是对于array操作后的数组进行转置操作
            # .reshape(x,y)操作是对于array操作后的数组进行重新排列成一个x*y的矩阵，参数为负数表示无限制，如(-1,1)转换成一列的矩阵
        self.active_hidden=sigmoid(self.sum_hidden) # active_hidden[]是处理完输入数据之后处理，作为输出层的输入数据
        self.active_hidden[0]=-1
        # 数据在输出层处理
        self.sum_out=np.dot(self.weight_out.T,self.active_hidden)
        self.active_out=sigmoid(self.sum_out)
        # 返回输出层结果
        return self.active_out
    
    #? 误差反向传播
    def errorbackpropagate(self,targets,lr,m):  # lr 学习效率
        if self.num_out==1:
            targets=[targets]
        if len(targets)!=self.num_out:
            raise ValueError("与输出层结点数不符")
        # 误差
        error=(1/2)*np.dot((targets.reshape(-1,1)-self.active_out).T,
                           (targets.reshape(-1,1)-self.active_out))
        
        # 输出层 误差信号
        self.error_out=(targets.reshape(-1,1)-self.active_out)*DS(self.sum_out) # DS(self.active_out)
        # 隐层 误差信号
        self.error_hidden=np.dot(self.weight_out,self.error_out)*DS(self.sum_hidden)    # DS(self.active_hidden)

        # 更新权值
        # 隐层
        self.weight_out=self.weight_out+lr*np.dot(self.error_out,self.active_hidden.reshape(1,-1)).T+m*self.co
        self.co=lr*np.dot(self.error_out,self.active_hidden.reshape(1,-1)).T
        # 输入层
        self.weight_in=self.weight_in+lr*np.dot(self.error_hidden,self.active_in.reshape(1,-1)).T+m*self.ci
        self.ci=lr*np.dot(self.error_hidden,self.active_in.reshape(1,-1)).T

        return error

    #? 测试
    def test(self,patterns):
        for i in patterns:  # i为传入数组的第一维数据
            print(i[0:self.num_in-1],"-&gt;",self.update(i[0:self.num_in-1]))
        return self.update(i[0:self.num_in-1])  # 返回测试结果，用于作图

    #? 权值
    def weights(self):
        print("输入层的权值：")
        print(self.weight_in)
        print("输出层的权值：")
        print(self.weight_out)
    
    def train(self,pattern,itera=100,lr=0.2,m=0.1):
        for i in range(itera):
            error=0.0   # 每一次迭代将error置0
            for j in pattern:   # j为传入数组的第一维数据
                inputs=j[0:self.num_in-1]   # 根据输入层结点的个数确定传入结点值的个数
                targets=j[self.num_in-1:]   # 剩下的结点值作为输出层的值
                self.update(inputs) # 正向传播 更新了active_out
                error=error+self.errorbackpropagate(targets,lr,m)   # 误差反向传播 计算总误差
            if i%10==0:
                print("########################误差 %-.5f ######################第%d次迭代" %(error, i))


#? 算法检验——预测数据D
# X 输入数据；D 目标数据
X = list(np.arange(-1, 1.1, 0.1))   # -1~1.1 步长0.1增加
D = [-0.96, -0.577, -0.0729, 0.017, -0.641, -0.66, -0.11, 0.1336, -0.201, -0.434, -0.5, 
     -0.393, -0.1647, 0.0988, 0.3072,0.396, 0.3449, 0.1816, -0.0312, -0.2183, -0.3201]
A = X + D   # 数据合并 方便处理
patt = np.array([A] * 2)    # 2*42矩阵
# 创建神经网络，21个输入节点，13个隐藏层节点，21个输出层节点
bp = BP(21, 13, 21)
# 训练神经网络
bp.train(patt)
# 测试神经网络
d = bp.test(patt)
# 查阅权重值
bp.weights()


import matplotlib.pyplot as plt
plt.plot(X, D, label="source data")  # D为真实值
plt.plot(X, d, label="predict data")  # d为预测值
plt.legend()
plt.show()</code></pre> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/756043a5eddd6b3f35ef9c917269964f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">2023全新车型汽车配置参数数据库大全</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/4e9f68308b2626a096b781180b51ad33/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【springboot3.x 记录】spring-cloud-gateway 2022踩坑记录</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 菜鸟程序员博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>